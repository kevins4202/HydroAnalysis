index,text
26290,we use the flexible model coupling technology known as the bespoke framework generator to link established existing modules representing dynamics in the global economy gemini e3 the energy system tiam world the global and regional climate system magicc6 plasim ents and climgen the agricultural system the hydrological system and ecosystems lpjml together in a single integrated assessment modelling iam framework building on the pre existing framework of the community integrated assessment system next we demonstrate the application of the framework to produce policy relevant scientific information we use it to show that when using carbon price mechanisms to induce a transition from a high carbon to a low carbon economy prices can be minimised if policy action is taken early if burden sharing regimes are used and if agriculture is intensified some of the coupled models have been made available for use at a secure and user friendly web portal keywords integrated assessment integrated assessment modelling climate change mitigation carbon price 1 introduction integrated assessment models are increasingly used as tools for projecting scenarios of global change by drawing together information from a variety of disciplines however such models often do not assemble detailed treatments of both the earth system and the global economy within a single framework and often consist of single pieces of software here we describe the assembly and use of a modular integrated assessment framework that is based on the principle of coupling together alternative combinations of modules each implemented at a different institution to produce an enhanced integrated modelling framework warren et al 2013 http ermitage cs man ac uk we couple together state of the art intermediately complex models representing the global economy and social actors within it the physical climate system the energy system the agricultural system the hydrological system and ecosystems this type of integrated assessment modelling is needed in order to study the complex interactions between climate change climate change impacts climate change mitigation and decisions about land use management the work was performed as part of the eu project enhancing robustness and model integration for the assessment of global environmental change ermitage these integrated assessments are now of particular topical interest in view of the recent adoption of the united nations framework on climate change s paris agreement unfccc 2015 by 195 countries most of this framework is incorporated within the community integrated assessment system cias warren et al 2008 whilst some of the coupled models exist independently of cias specifically the coupling between the energy technology model remind and the land use allocation model magpie see table 2 for details the approach is based on the advanced flexible bespoke framework generator which is language independent armstrong et al 2009 the flexible approach allows new modules to be added to the system with minimum disruption for example when climate models are upgraded with new information or when updated modules become available simulating climate impacts in new sectors the approach has created long lasting coupled models available for use in research for the future by drawing together a range of models created at different institutions the co location of many of the models in the same system cias allows for increased easy use of the models in the future at a secure and user friendly web portal 2 methodology the first step in the modelling processes is to determine conceptually the required linkages between models this was achieved through bilateral discussions at workshops which allowed model developers from different disciplines to work together initially the team created a prioritised list of model couplings needed to answer the research questions we have once the list of model couplings had been agreed the team then worked together to determine the scientific requirements of the couplings these requirements included detailing a which are the variables output from one model that are to become the input to another model b are any unit conversions required c is any spatial or temporal aggregation required to allow for differences in the spatial or temporal resolution used in different models d when during the operation of the code should the variables be passed once these requirements had been determined we used the bespoke framework generator version 2 bfg2 to couple models together according to the requirements in a language independent fashion ford et al 2006 warren et al 2008 bfg2 has a simple interface which allows users to automatically create metadata describing model linkages and it continues by using this meta data to automatically generate the coupling code the metadata follows a dcd approach it contains description d information about the variables to be exchanged between the models that are to be coupled specifically the variable names units and temporal and spatial scales composition c information detailing which quantities should be exchanged between the model codes at which times during the running of the code and deployment d information detailing which machines will run the code we initially coupled pairs of models together before moving on to more complex coupled models involving three or more components finally couplings were incorporated into framework of the community integrated assessment system cias fig 1 which allows users to execute the couplings at a user friendly web portal cias warren et al 2008 is a framework that supports and enables the creation and running of integrated assessment models it connects together alternative sets of component models thus one of these sets is broadly equivalent to an integrated assessment model and may be referred to as a coupled model it is flexible and multi modular and enables models to communicate with each other even if they are written in different programming languages or operate on different platforms the cias web portal supports users in running the integrated models it is facilitated by the softiam technology goswami and warren 2012 for each coupling the softiam technology supports a variety of coupling specific features related to the selection of modes of operation changing model parameters selecting variables for output and user management model coupling outputs are stored in a database and can be accessed from the web portal table 1 provides the list of models used and table 2 shows the list of linkages between the modules which we created we used a third key software technology statistical emulation to speed up the run time of some of our model couplings in this approach a model is replaced by a computationally much faster and functionally smoother model emulator derived from a large ensemble of simulations we created emulators for plasim ents holden et al 2014 and also for the simulation of net primary production and crop yields by lpjml oyebamiji et al 2015 the methodologies are described in detail in these references in summary the plasim ents emulator uses singular vector decompositions of the spatiotemporal outputs of a large ensemble of transient 21st century climate simulations considering a wide range of future emissions scenarios the dominant components of the decompositions are fitted as polynomial functions of future forcing and model parameters the approach represents an advance on pattern scaling as it allows us to address non linear spatiotemporal feedbacks and model parametric uncertainty by representing multiple modes of variability the lpjml emulator is constructed in a two stage approach the first stage uses step wise regression to fit crop yields as smooth functions of local climate variables under the assumption that each lpjml grid cell is an independent sample the second stage combines principal component analysis and weighted least squares to allow for bias in predicted spatial patterns correcting for the anticipated residual of the first stage in table 2 coupling sequences in which the models have suffix em refers to emulators of the full codes fig 2 illustrates the emulation of precipitation from plasim ents deriving the emulated precipitation fields required 1 min of cpu time compared to 1 year of computer time required for the full simulation this can result in the loss of representation of more complex processes such as feedbacks and non linearities which might be important thus use of emulators of more complex models allows the statistical as opposed to mechanistic representation of more complex processes than would otherwise be possible within integrated models the statistical emulation needs to be robust in this example the emulated ensemble reproduces the simulated mean field extremely closely in relation to the ensemble variance it is also able to reproduce the pattern of the simulated uncertainty field though somewhat understating its magnitude 3 example couplings 3 1 example 1 plasim entsem climgen lpjmlem in this relatively simple coupling fig 3a measures of global climate change such as temperature are used to drive a pattern scaling module climgen which in turn drives an emulator of a climate change impact model lpjmlem the process begins with the provision of historical and projected global time series of greenhouse gas concentrations to the climate model emulator plasim entsem holden et al 2014 which simulates global climate changes for near surface temperature precipitation and cloud cover on a 5 grid scale the seasonally resolved climate projections are passed to climgen which downscales the data to a 0 5 grid in pattern scaling linear relationships between projected local climate change and projected global mean temperature change are diagnosed directly from outputs of global circulation models these are combined with observed climatological data to create projected fields of climate change here precipitation and temperature at a resolution of 0 5 x 0 5 warren et al 2008 for further detail finally the downscaled climate change projections are used by lpjmlem to project impacts resulting from the studied global climate change scenarios outputs from this coupling are for example gridded projections of crop yields 3 2 example 2 gemini e3 plasim entsem this particular coupling fig 3b has been designed to use the emulator of the climate model plasim entsem to create greenhouse gas emissions constraints for the macro economic model gemini e3 in order to derive climate policy such as a carbon tax scheme that constrains the global annual mean temperature rise occurring between pre industrial times and 2050 to a particular level it is also designed to investigate the impacts of climate changes on heating and cooling demands and the economic consequences thereof since gemini e3 is a time step optimization model it is not feasible to compute endogenously an optimal emissions path with respect to the economy for this reason we have implemented a soft coupling approach in which no optimisation occurs which gives realistic emissions profiles given the anticipated temperature expectations these emissions profiles are used in gemini e3 as an upper bound on the emissions for the assessment of potential climate policies as the number of satisfactory emissions trajectories is potentially unlimited the coupling constrains its search to a subset of trajectories with two functional forms a class of simple linear functions and a class of more complex smooth polynomials for each proposed trajectory plasim entsem can compute a temperature increase and the coupling algorithm selects the one that meets the given warming target for the resulting selected trajectory plasim entsem also provides heating degree days hdd and cooling degree days cdd to gemini e3 this allows gemini e3 to evaluate the impact of climate change on heating and cooling demands and the resultant economic consequences outputs of this coupling are economic measures for each economic region in each time period e g discounted and total welfare permit allocation gdp carbon taxes and the heating and cooling demand 3 3 example 3 plasim entsem climgen lpjmlem gemini e3 this coupling fig 3c is an extension of the plasim entsem gemini e3 one presented in the previous section where the emulator of the agriculture model lpjmlem has been integrated between plasim entsem and gemini e3 in order to evaluate physical and economic consequences of climate change on the agricultural sector for a specified climate policy see previous section for more details plasim entsem sends climate information at the grid cell level temperature precipitation etc to lpjmlem that then predicts agricultural variables such as crop yields changes for irrigated or non irrigated paddy rice maize and temperate cereal and oil crop at a spatial resolution of 0 5 0 5 this information is converted into gemini e3 regions using a conversion key to aggregate the data regionally and then used to analyse the economic impacts of the selected policy or rcp 3 4 example 4 magicc climgen lpjml magpie remind this coupling fig 4 which is implemented off line uses magicc to simulate radiative forcing pathways global mean temperature and co2 time series for the 21st century climgen generates the corresponding 0 5 regular climate change pattern grid for a selected gcm eg gfdl cm2 0 these data are then used to perform climate change impact simulations with the lpjml bio and agrosphere model or alternatively its emulated version lpjmlem see example 1 focused on variables relevant for use as boundary conditions in the subsequent model chain lpjml is set up to provide biophysical inputs to the magpie agro economy and land use allocation model magpie considers the following biophysical constraints on land use patterns per 0 5 grid cell globally from lpjml i changes in freshwater resources defined as changes in runoff from the surface and from below ground and water availability in rivers lakes and reservoirs ii changes in soil and vegetation carbon pools iii changes in potential crop yields of 12 rainfed and irrigated crop types with pasture parameterized in lpjml each determined under condition of 7 different management options iv changes in net irrigation water demand v sowing and harvest date for all irrigated and rainfed crops the first and second of these constraints are examined for potential natural vegetation and the others for both natural and agricultural vegetation all simulations for the relevant constraints iii v above were performed for all 7 management options that can be interpreted as different cropping intensities all runs were made for the two rcps and the gfdl cm2 0 gcm and to separate fertilization and increased water use efficiencies due to enhanced atmospheric co2 concentrations variants were computed in which ambient co2 concentration was held constant after year 2002 all results are only used to estimate potentials irrespective of current land use patterns and management practices as needed for biophysical constraints in the magpie model crop distribution is then calculated by magpie based on the simulated local biophysical potentials to determine crop production and land allocation magpie relies on additional information on bioenergy demand from remind popp et al 2011 remind computes the bioenergy demand based on a biomass supply curve that uses magpie results from a large number of previous model runs klein et al 2014 in return magpie gets from remind data on greenhouse gas prices in the rcp3pd scenarios which imply the presence of climate policies ghg prices represent information on external costs of ghg emitting activities and the urgency of emissions reduction respectively bioenergy is part of a broader technology portfolio that remind uses in order to meet the economies demand on final energy such as transport energy electricity and non electric energy for stationary end uses techno economic parameters investment costs operation maintenance costs fuel costs conversion efficiency etc characterize each conversion technology they essentially determine future technology choice and energy mix major outputs from remind include primary energy consumption co2 emissions fossil fuel prices carbon prices and mitigation costs i e gdp and consumption losses 3 5 example 5 plasim entsem tiamworld the objective of the coupling fig 5 of tiam world and the emulator of plasim entsem is to use regional and seasonal temperature changes obtained from plasim entsem in order to represent the possible heating and cooling adjustments due to climate change indeed the climate module included in tiam world provides only the global average surface temperature increase in essence there is an iterative exchange of data between the two models whereby tiam world sends to the climate emulator a set of total greenhouse gas concentrations for the entire 21st century computed in tiam world and the climate emulator sends to tiam world the seasonal and regional temperatures converted into seasonal heating and cooling degree days hdd cdd for each of the regions of the model plasim ents emulated outputs seasonal mean and variance of temperature at 5 degree resolution were converted to heating and cooling degree days under the assumption that daily temperatures are scattered about the seasonal mean with a normal distribution these data were integrated onto the 16 tiam world regions as a population weighted average the transformation and its validation are described in detail in holden et al 2014 these seasonal and regional degree days are then used to compute new seasonal and regional heating and cooling demands in tiam world the new heating and cooling services result in the endogenous computation of a new supply demand equilibrium the same approach has been used to model 1 the impacts of regional temperature changes on the efficiency and availability of thermal power plants 2 the impacts of regional precipitation changes on hydropower 3 and all the impacts together labriet et al 2015 the coupling can be applied both as a single iteration linkage and as an iterative loop the single iteration linkage feeds into tiam world with hdd and cdd from plasim entsem run once with greenhouse gas concentration provided by tiam world this linkage allows the assessment of the impacts of climate change on energy dynamics related to heating and cooling as well as the possible adjustments on the entire energy system the loop refers to the iterative exchanges of greenhouse gas concentrations and hdd cdd it is needed to assess the possible feedback between the energy and climate systems climate change results in hdd cdd changes which may themselves result in more or less greenhouse emissions 4 illustrative results and discussion we used both the simpler and more advanced couplings to create 21st century scenarios in a harmonized fashion using common or similar datasets for population gdp and land use in particular we used the couplings to explore economic instruments and technical solutions necessary to achieve a transition from a higher to a lower carbon world specifically from the representative concentration pathway rcp6 fujino et al 2006 to that of rcp2 6 van vuuren et al 2011b under the common socioeconomic pathway ssp2 moss et al 2010 this is a question of topical interest in view of the recent adoption of the united nations framework on climate change s paris agreement in which 195 countries emphasized the urgent need to address the significant gap between the aggregate effect of parties mitigation pledges in terms of global annual emissions of greenhouse gases by 2020 and aggregate emission pathways consistent with holding the increase in the global average temperature to well below 2 c above pre industrial levels and pursuing efforts to limit the temperature increase to 1 5 c unfccc 2015 since rcp2 6 is broadly consistent with constraining global average temperature rise to 2 c above pre industrial levels although we do not in this study explore scenarios which reduce temperatures more than this early international assessments such as the ipcc special report on emissions scenarios sres nakicenovic and swart 2000 used self consistent socio economic scenarios characterised by population gdp land use and energy use and greenhouse gas emission pathways over time sres scenarios were based upon an analysis of how demographic social economic environmental and technological aspects of our society might evolve globally in these scenarios two main axes of change were considered a environmental versus economic and b globalisation versus regionalisation of markets and cultures hence the four scenarios may be briefly summarised as a1 global economic a2 regional economic b1 global environmental b2 regional environmental a new process independent of the original sres scenarios has since been established moss et al 2010 this recognises that different socioeconomic pathways might have the same climatic change outcome hence sres scenarios have now been replaced by the representative concentration pathways rcps which were used in the ipcc fifth assessment report ar5 and new shared socio economic pathways ssps van vuuren et al 2011a kriegler et al 2012 ebi et al 2014 in ssps the axes of change are a challenge to mitigation and b challenge to adaptation for example increased population is a challenge to mitigation because energy demand will be higher ssps are based on new set of socio economic data including some trends important in sres such as population and gdp however other data may also be important but most fundamentally there is a change in the way in which the data are used the rcps and ssps have not been designed as a new fully integrated and self consistent set of socio economic and emission scenarios over time but instead offer the potential to mix and match alternative combinations this is undertaken in a framework a matrix that combines climate forcing on one axis as represented by the representative forcing pathways and socio economic conditions represented by the socio economic pathways on the other thus we apply this new methodology in our research firstly we ensured that our model couplings were reasonably harmonised in projecting greenhouse gas emissions associated with the rcp6 pathway and the rcp2 6 pathway fig 6 we used the five couplings above and others to derive policy relevant information the paths of the emissions from the three models gemini e3 remind and tiam world are illustrated in fig 6 alongside reference rcp2 6 and rcp6 trajectories from van vuuren et al 2011a labelled rcp6vv and rcp2 6vv showing that our simulations from all three models are broadly consistent with theirs substantial emissions reductions are needed in order to stabilize the greenhouse gas concentrations in the atmosphere to a level of around 450 ppm co2eq rcp2 6 model couplings including those listed above were used to explore this transition and to create different strategies for or implications of reaching these emission reductions carbon prices policy design energy technologies and climate change impacts were all explored applying the coupling in example 3 the gemini e3 model explores how the carbon price required to achieve the transition depends on the time of onset of climate change mitigation and on burden sharing approaches to climate change policy applying the coupling in example 4 remind simulates the most cost effective way to achieve the emission reductions globally exploring how this changes when the availability of biomass is low other couplings are used to explore the consequences of these emission reductions applying the coupling in example 5 tiam world simulates the consequences of climate change for heating and cooling demand in the two rcp scenarios and finally the coupling in example 2 assesses sea level rise impacts in the two scenarios referring to the coupling in example 3 here the gemini e3 model can be used to explore a set of standard burden sharing approaches babonneau et al 2016 and alternative constraints on the date at which climate policy in the form of the use of a carbon price in international markets is instigated table 3 highlights the key findings the model indicates that large rises in carbon prices are needed to achieve the necessary emission reduction however these are greatly reduced if policy is instigated in 2020 rather than 2030 the importance of early policy action has also been highlighted in other studies which report on the implications of short term emission targets for the cost and feasibility of long term climate goals such as the 2c target for limiting warming luderer et al 2015 riahi et al 2015 rogelj et al 2012 in assessing the transition from rcp6 to rcp2 6 remind selects from a large set of potential energy conversion technologies generating negative emissions by using biomass in combination with carbon capturing and sequestration turns out to be a favourable cost effective option fig 7 left panels the associated carbon price increases from almost 10 tco2 in 2010 to around 220 tco2 in 2050 the meta analysis of recent mitigation studies of clarke et al 2014 identifies a number of studies that demonstrate feasibility of rcp2 6 whilst emphasizing that higher carbon prices and reliance on bioenergy with carbon capture and storage are necessary to achieve this azar et al 2010 hence our results are in line with the findings of many other studies however we also explore the effects of limiting the supply of bioenergy leimbach et al 2016 within both rcp2 6 scenarios low and high biomass potential there is a fast phase out of the coal technologies which are the most carbon intensive fig 7 upper panels importantly while bioenergy and solar are similarly important for the long term energy mix in the rcp2 6 scenario high biomass potential solar energy is the dominant source of energy in the rcp2 6 biolow scenario the high sensitivity of the energy system to the availability of biomass can also be seen in fig 7 lower panel which shows the structure of biomass consumption in the case of sufficient availability of bioenergy it is cost effective to produce biofuels for the transport sector however it is most cost effective to use biomass to produce hydrogen when the biomass potential is low as this technology has comparatively lower emissions furthermore hydrogen has the potential to replace fossil resources in sectors other than transport coupling outputs suggested that carbon prices up to 600 t co2 were needed to achieve the transition to rcp3pd 1 1 rcp3pd corresponds to rcp2 6 and features a peak and decline in radiative forcing if biofuel cropping was minimised in order to reduce competition for land with agricultural crops and preserve natural ecosystems and biodiversity our studies project that reliance on biofuels for mitigation would induce widespread deforestation and other land use change globally consistent with the findings of many other studies e g fargione et al 2008 searchinger et al 2008 popp et al 2012 oppenheimer et al 2014 unless a carbon taxation scheme is used that includes terrestrial carbon consistent with the findings of wise et al 2009 our results indicate that the main response option in land use to climate change mitigation policy is agricultural intensification through investments in yield increasing technological change these are estimated to be 41 72 higher in the policy rcp3pd scenario compared to the bau business as usual rcp6 scenario over the 1995 technology level these are shown in fig 8 the role of agricultural intensification has also been highlighted elsewhere lotze campen et al 2010 tilman et al 2011 smith et al 2013 results obtained with the coupling plasim entsem tiam world coupling example 5 explore the feedback between the climate system and the energy system fig 9 they show that the climate feedback induced by adaptation of the energy system to heating and cooling is found to be insignificant partly because heating and cooling induced changes compensate and partly because they represent a limited share of total final energy consumption however significant changes are observed at regional levels in the reference case rcp6 labriet et al 2013a b in contrast they are negligible in rcp2 6 with smaller temperature changes while the increase in cooling demand is met with electricity the decrease of heating demand results mostly in a decrease in gas consumption this reflects the relatively higher costs of natural gas compared to other energy sources for heating in the longer term the need for power capacity to satisfy additional cooling services and the pressure on electricity demand result in increases in electricity prices for example up to 30 in europe in the mid term and 50 in the long term thus climate change was projected to have minimal effects on heating and cooling demand globally but effects were important regionally especially in europe a coupled plasim ents gemini e3 sequence is also used to analyse the impacts of sea level rise slr in the twenty first century to estimate slr we first use the emulator of the climate model plasim ents to compute the warming profile related to the gemini e3 baseline scenario the temperature increase is used to derive slr using a semi empirical relationship then the physical consequences of slr are computed using gis analysis which are incorporated in gemini e3 see joshi et al 2016 the simulation results suggest that the potential development of future coastal areas is a greater source of uncertainty than the parameters of slr itself in terms of the economic consequences of slr at global level the economic impact of slr could be significant when loss of productive land along with loss of capital and forced displacement of populations are considered furthermore highly urbanised and densely populated coastal areas of south east asia australia and new zealand are likely to suffer significantly if no protective measures are taken hence it is suggested that coastal areas needs to be protected to ameliorate the overall welfare cost across various regions coupled economic and climate models were also exploited in a game theoretical framework to analyse fairness and robustness of the international environmental agreements first we identify a total emission budget over the 2010 2050 period that is compatible with the warming at the end of the century being less than 2 c according to our climate models first results show that an acceptable voluntary burden sharing agreement could be obtained among all groups of countries with a balanced welfare loss below 1 of total discounted household consumption in such an agreement see fig 10 15 3 of the total emission budget of 424gtc is allocated to usa 8 to eu 22 5 to china 7 5 to india 4 8 to russia in a robust solution that prevents potential emissions overshooting in such commitments and takes potential errors arising in the various approximations made in our methodology into consideration the welfare loss rises to 1 8 for each group of countries this analysis has recently been extended see haurie et al 2015 and babonneau et al 2016 5 conclusions a set of coupled models has been developed within an integrated framework that can be used in future research projects involving policy makers and other stakeholders based on the community integrated assessment system the bespoke framework generator and the use of statistical emulators for model coupling we use it to show that when using carbon price mechanisms to induce a transition from a high carbon to a low carbon economy prices can be minimised if policy action is taken early if burden sharing regimes are used and if agriculture is intensified this is of particular relevance owing to the recent adoption of the paris agreement unfccc 2015 the approach has created long lasting coupled models available for future policy relevant research exploration of the robustness of coupled model outputs to uncertainties should form a key part of this future work acknowledgement this work was funded by the eu 7th framework programme project number 265170 enhancing robustness in model integration for the assessment of global environmental change ermitage rfw was also supported by uk natural environment research council advanced fellowship grant no ne f016107 1 software availability table software name bfg2 developer rupert ford contact address and postcode stfc daresbury laboratory warrington wa4 4ad u k tel 44 1925 60 3217 e mail rupert ford stfc ac uk mailto aa manchester ac uk year first available 2005 hardware required none specific software required python2 libxml2 libxslt python lxml2 program language python xslt program size approx 500 kb compressed tar file availability downloadable from http cnc cs manchester ac uk projects bfg php http www cs man ac uk cost free for non commercial use software name softiam developers sudipta goswami santiago de la nava santos rachel warren contact address tyndall centre contact rachel warren tel 44 1603 593912 fax 44 1603 593901 email r warren uea ac uk year first available 2004 hardware required software required bfg2 program language program size availability cost not for sale software name cias developer sudipta goswami santiago de la nava santos matt hyde rachel warren contact address tyndall centre contact rachel warren tel 44 1603 593912 fax 44 1603 593901 e mail r warren uk ac uk year first available 2005 hardware required pc availability some applications are accessible via web portal upon request for password cost not for sale software name plasim entsem developer philip holden neil edwards address environment earth and ecosystems the open university milton keynes uk email philip holden open ac uk neil edwards open ac uk year first available 2012 hardware required none specific software required r program language r program size 152 mb nb mostly input files the code itself is very small availability philip holden open ac uk neil edwards open ac uk cost free for non commercial use software name gemini e3 developer alain bernard and marc vielle contact address and postcode marc vielle epfl enac leure bp2140 station 16 ch 1015 lausanne switzerland tel 41 21 6932031 fax 41 21 6933840 e mail marc vielle epfl ch year first available 1995 hardware required none specific software required gams the general algebraic modeling system program size approx 10 mo availability contact email to enquire for availability cost not for sale software name tiam world developer kanors kanlo eneris contact address and postcode amit kanudia sdf l7b nsez phase ii noida 201305 up india tel 91 9871 488 591 e mail amit kanors com year first available 2005 hardware required pc software required gams solver cplex xpress under windows environment program language gams program size about 1 million row lp availability contact via email cost contact via email software name remind developers nico bauer lavinia baumstark christoph bertram anastasis giannousakis markus haller jerome hilaire david klein marian leimbach antoine levesque gunnar luderer michael lueken ioanna mouratiadou michaja pehl robert pietzcker franziska piontek anselm schultes jessica strefler tino aboumahboub tabare curras alexander körner sylvie ludig jana schwanitzcontact address and postcode potsdam institute for climate impact research p o box 601203 14412 potsdam germany tel 49 331 288 2556 e mail leimbach pik potsdam de year first available 2010 hardware required pc unix machine software required gams conopt solver program language gams program size approx 370 mb availability model description downloadable from http www pik potsdam de research sustainable solutions models remind cost no commercial use software name lpjml vs 3 2 developer pik lpjml team and collaborators contact address and postcode dieter gerten responsible scientist see above tel 49 331 288 2577 fax 49 331 288 2695 e mail gerten pik potsdam de year first available 2007 hardware required none specific unix preferably software required none specific program language c program size approx 1 5 mb tar file source code only availability via http www pik potsdam de research projects cooperations lpjml or via contact email to enquire for availability cost free for non commercial use in case of cooperation agreement software name magpie developer potsdam institute for climate impact research contact alexander popp contact address potsdam institute for climate impact research p o box 601203 14412 potsdam germany tel 49 331 288 2463 e mail popp pik potsdam de year first available 2008 hardware required none specific software required gams conopt cplex solver program language gams r program size approx 30 mb availability upon request cost upon request 
26290,we use the flexible model coupling technology known as the bespoke framework generator to link established existing modules representing dynamics in the global economy gemini e3 the energy system tiam world the global and regional climate system magicc6 plasim ents and climgen the agricultural system the hydrological system and ecosystems lpjml together in a single integrated assessment modelling iam framework building on the pre existing framework of the community integrated assessment system next we demonstrate the application of the framework to produce policy relevant scientific information we use it to show that when using carbon price mechanisms to induce a transition from a high carbon to a low carbon economy prices can be minimised if policy action is taken early if burden sharing regimes are used and if agriculture is intensified some of the coupled models have been made available for use at a secure and user friendly web portal keywords integrated assessment integrated assessment modelling climate change mitigation carbon price 1 introduction integrated assessment models are increasingly used as tools for projecting scenarios of global change by drawing together information from a variety of disciplines however such models often do not assemble detailed treatments of both the earth system and the global economy within a single framework and often consist of single pieces of software here we describe the assembly and use of a modular integrated assessment framework that is based on the principle of coupling together alternative combinations of modules each implemented at a different institution to produce an enhanced integrated modelling framework warren et al 2013 http ermitage cs man ac uk we couple together state of the art intermediately complex models representing the global economy and social actors within it the physical climate system the energy system the agricultural system the hydrological system and ecosystems this type of integrated assessment modelling is needed in order to study the complex interactions between climate change climate change impacts climate change mitigation and decisions about land use management the work was performed as part of the eu project enhancing robustness and model integration for the assessment of global environmental change ermitage these integrated assessments are now of particular topical interest in view of the recent adoption of the united nations framework on climate change s paris agreement unfccc 2015 by 195 countries most of this framework is incorporated within the community integrated assessment system cias warren et al 2008 whilst some of the coupled models exist independently of cias specifically the coupling between the energy technology model remind and the land use allocation model magpie see table 2 for details the approach is based on the advanced flexible bespoke framework generator which is language independent armstrong et al 2009 the flexible approach allows new modules to be added to the system with minimum disruption for example when climate models are upgraded with new information or when updated modules become available simulating climate impacts in new sectors the approach has created long lasting coupled models available for use in research for the future by drawing together a range of models created at different institutions the co location of many of the models in the same system cias allows for increased easy use of the models in the future at a secure and user friendly web portal 2 methodology the first step in the modelling processes is to determine conceptually the required linkages between models this was achieved through bilateral discussions at workshops which allowed model developers from different disciplines to work together initially the team created a prioritised list of model couplings needed to answer the research questions we have once the list of model couplings had been agreed the team then worked together to determine the scientific requirements of the couplings these requirements included detailing a which are the variables output from one model that are to become the input to another model b are any unit conversions required c is any spatial or temporal aggregation required to allow for differences in the spatial or temporal resolution used in different models d when during the operation of the code should the variables be passed once these requirements had been determined we used the bespoke framework generator version 2 bfg2 to couple models together according to the requirements in a language independent fashion ford et al 2006 warren et al 2008 bfg2 has a simple interface which allows users to automatically create metadata describing model linkages and it continues by using this meta data to automatically generate the coupling code the metadata follows a dcd approach it contains description d information about the variables to be exchanged between the models that are to be coupled specifically the variable names units and temporal and spatial scales composition c information detailing which quantities should be exchanged between the model codes at which times during the running of the code and deployment d information detailing which machines will run the code we initially coupled pairs of models together before moving on to more complex coupled models involving three or more components finally couplings were incorporated into framework of the community integrated assessment system cias fig 1 which allows users to execute the couplings at a user friendly web portal cias warren et al 2008 is a framework that supports and enables the creation and running of integrated assessment models it connects together alternative sets of component models thus one of these sets is broadly equivalent to an integrated assessment model and may be referred to as a coupled model it is flexible and multi modular and enables models to communicate with each other even if they are written in different programming languages or operate on different platforms the cias web portal supports users in running the integrated models it is facilitated by the softiam technology goswami and warren 2012 for each coupling the softiam technology supports a variety of coupling specific features related to the selection of modes of operation changing model parameters selecting variables for output and user management model coupling outputs are stored in a database and can be accessed from the web portal table 1 provides the list of models used and table 2 shows the list of linkages between the modules which we created we used a third key software technology statistical emulation to speed up the run time of some of our model couplings in this approach a model is replaced by a computationally much faster and functionally smoother model emulator derived from a large ensemble of simulations we created emulators for plasim ents holden et al 2014 and also for the simulation of net primary production and crop yields by lpjml oyebamiji et al 2015 the methodologies are described in detail in these references in summary the plasim ents emulator uses singular vector decompositions of the spatiotemporal outputs of a large ensemble of transient 21st century climate simulations considering a wide range of future emissions scenarios the dominant components of the decompositions are fitted as polynomial functions of future forcing and model parameters the approach represents an advance on pattern scaling as it allows us to address non linear spatiotemporal feedbacks and model parametric uncertainty by representing multiple modes of variability the lpjml emulator is constructed in a two stage approach the first stage uses step wise regression to fit crop yields as smooth functions of local climate variables under the assumption that each lpjml grid cell is an independent sample the second stage combines principal component analysis and weighted least squares to allow for bias in predicted spatial patterns correcting for the anticipated residual of the first stage in table 2 coupling sequences in which the models have suffix em refers to emulators of the full codes fig 2 illustrates the emulation of precipitation from plasim ents deriving the emulated precipitation fields required 1 min of cpu time compared to 1 year of computer time required for the full simulation this can result in the loss of representation of more complex processes such as feedbacks and non linearities which might be important thus use of emulators of more complex models allows the statistical as opposed to mechanistic representation of more complex processes than would otherwise be possible within integrated models the statistical emulation needs to be robust in this example the emulated ensemble reproduces the simulated mean field extremely closely in relation to the ensemble variance it is also able to reproduce the pattern of the simulated uncertainty field though somewhat understating its magnitude 3 example couplings 3 1 example 1 plasim entsem climgen lpjmlem in this relatively simple coupling fig 3a measures of global climate change such as temperature are used to drive a pattern scaling module climgen which in turn drives an emulator of a climate change impact model lpjmlem the process begins with the provision of historical and projected global time series of greenhouse gas concentrations to the climate model emulator plasim entsem holden et al 2014 which simulates global climate changes for near surface temperature precipitation and cloud cover on a 5 grid scale the seasonally resolved climate projections are passed to climgen which downscales the data to a 0 5 grid in pattern scaling linear relationships between projected local climate change and projected global mean temperature change are diagnosed directly from outputs of global circulation models these are combined with observed climatological data to create projected fields of climate change here precipitation and temperature at a resolution of 0 5 x 0 5 warren et al 2008 for further detail finally the downscaled climate change projections are used by lpjmlem to project impacts resulting from the studied global climate change scenarios outputs from this coupling are for example gridded projections of crop yields 3 2 example 2 gemini e3 plasim entsem this particular coupling fig 3b has been designed to use the emulator of the climate model plasim entsem to create greenhouse gas emissions constraints for the macro economic model gemini e3 in order to derive climate policy such as a carbon tax scheme that constrains the global annual mean temperature rise occurring between pre industrial times and 2050 to a particular level it is also designed to investigate the impacts of climate changes on heating and cooling demands and the economic consequences thereof since gemini e3 is a time step optimization model it is not feasible to compute endogenously an optimal emissions path with respect to the economy for this reason we have implemented a soft coupling approach in which no optimisation occurs which gives realistic emissions profiles given the anticipated temperature expectations these emissions profiles are used in gemini e3 as an upper bound on the emissions for the assessment of potential climate policies as the number of satisfactory emissions trajectories is potentially unlimited the coupling constrains its search to a subset of trajectories with two functional forms a class of simple linear functions and a class of more complex smooth polynomials for each proposed trajectory plasim entsem can compute a temperature increase and the coupling algorithm selects the one that meets the given warming target for the resulting selected trajectory plasim entsem also provides heating degree days hdd and cooling degree days cdd to gemini e3 this allows gemini e3 to evaluate the impact of climate change on heating and cooling demands and the resultant economic consequences outputs of this coupling are economic measures for each economic region in each time period e g discounted and total welfare permit allocation gdp carbon taxes and the heating and cooling demand 3 3 example 3 plasim entsem climgen lpjmlem gemini e3 this coupling fig 3c is an extension of the plasim entsem gemini e3 one presented in the previous section where the emulator of the agriculture model lpjmlem has been integrated between plasim entsem and gemini e3 in order to evaluate physical and economic consequences of climate change on the agricultural sector for a specified climate policy see previous section for more details plasim entsem sends climate information at the grid cell level temperature precipitation etc to lpjmlem that then predicts agricultural variables such as crop yields changes for irrigated or non irrigated paddy rice maize and temperate cereal and oil crop at a spatial resolution of 0 5 0 5 this information is converted into gemini e3 regions using a conversion key to aggregate the data regionally and then used to analyse the economic impacts of the selected policy or rcp 3 4 example 4 magicc climgen lpjml magpie remind this coupling fig 4 which is implemented off line uses magicc to simulate radiative forcing pathways global mean temperature and co2 time series for the 21st century climgen generates the corresponding 0 5 regular climate change pattern grid for a selected gcm eg gfdl cm2 0 these data are then used to perform climate change impact simulations with the lpjml bio and agrosphere model or alternatively its emulated version lpjmlem see example 1 focused on variables relevant for use as boundary conditions in the subsequent model chain lpjml is set up to provide biophysical inputs to the magpie agro economy and land use allocation model magpie considers the following biophysical constraints on land use patterns per 0 5 grid cell globally from lpjml i changes in freshwater resources defined as changes in runoff from the surface and from below ground and water availability in rivers lakes and reservoirs ii changes in soil and vegetation carbon pools iii changes in potential crop yields of 12 rainfed and irrigated crop types with pasture parameterized in lpjml each determined under condition of 7 different management options iv changes in net irrigation water demand v sowing and harvest date for all irrigated and rainfed crops the first and second of these constraints are examined for potential natural vegetation and the others for both natural and agricultural vegetation all simulations for the relevant constraints iii v above were performed for all 7 management options that can be interpreted as different cropping intensities all runs were made for the two rcps and the gfdl cm2 0 gcm and to separate fertilization and increased water use efficiencies due to enhanced atmospheric co2 concentrations variants were computed in which ambient co2 concentration was held constant after year 2002 all results are only used to estimate potentials irrespective of current land use patterns and management practices as needed for biophysical constraints in the magpie model crop distribution is then calculated by magpie based on the simulated local biophysical potentials to determine crop production and land allocation magpie relies on additional information on bioenergy demand from remind popp et al 2011 remind computes the bioenergy demand based on a biomass supply curve that uses magpie results from a large number of previous model runs klein et al 2014 in return magpie gets from remind data on greenhouse gas prices in the rcp3pd scenarios which imply the presence of climate policies ghg prices represent information on external costs of ghg emitting activities and the urgency of emissions reduction respectively bioenergy is part of a broader technology portfolio that remind uses in order to meet the economies demand on final energy such as transport energy electricity and non electric energy for stationary end uses techno economic parameters investment costs operation maintenance costs fuel costs conversion efficiency etc characterize each conversion technology they essentially determine future technology choice and energy mix major outputs from remind include primary energy consumption co2 emissions fossil fuel prices carbon prices and mitigation costs i e gdp and consumption losses 3 5 example 5 plasim entsem tiamworld the objective of the coupling fig 5 of tiam world and the emulator of plasim entsem is to use regional and seasonal temperature changes obtained from plasim entsem in order to represent the possible heating and cooling adjustments due to climate change indeed the climate module included in tiam world provides only the global average surface temperature increase in essence there is an iterative exchange of data between the two models whereby tiam world sends to the climate emulator a set of total greenhouse gas concentrations for the entire 21st century computed in tiam world and the climate emulator sends to tiam world the seasonal and regional temperatures converted into seasonal heating and cooling degree days hdd cdd for each of the regions of the model plasim ents emulated outputs seasonal mean and variance of temperature at 5 degree resolution were converted to heating and cooling degree days under the assumption that daily temperatures are scattered about the seasonal mean with a normal distribution these data were integrated onto the 16 tiam world regions as a population weighted average the transformation and its validation are described in detail in holden et al 2014 these seasonal and regional degree days are then used to compute new seasonal and regional heating and cooling demands in tiam world the new heating and cooling services result in the endogenous computation of a new supply demand equilibrium the same approach has been used to model 1 the impacts of regional temperature changes on the efficiency and availability of thermal power plants 2 the impacts of regional precipitation changes on hydropower 3 and all the impacts together labriet et al 2015 the coupling can be applied both as a single iteration linkage and as an iterative loop the single iteration linkage feeds into tiam world with hdd and cdd from plasim entsem run once with greenhouse gas concentration provided by tiam world this linkage allows the assessment of the impacts of climate change on energy dynamics related to heating and cooling as well as the possible adjustments on the entire energy system the loop refers to the iterative exchanges of greenhouse gas concentrations and hdd cdd it is needed to assess the possible feedback between the energy and climate systems climate change results in hdd cdd changes which may themselves result in more or less greenhouse emissions 4 illustrative results and discussion we used both the simpler and more advanced couplings to create 21st century scenarios in a harmonized fashion using common or similar datasets for population gdp and land use in particular we used the couplings to explore economic instruments and technical solutions necessary to achieve a transition from a higher to a lower carbon world specifically from the representative concentration pathway rcp6 fujino et al 2006 to that of rcp2 6 van vuuren et al 2011b under the common socioeconomic pathway ssp2 moss et al 2010 this is a question of topical interest in view of the recent adoption of the united nations framework on climate change s paris agreement in which 195 countries emphasized the urgent need to address the significant gap between the aggregate effect of parties mitigation pledges in terms of global annual emissions of greenhouse gases by 2020 and aggregate emission pathways consistent with holding the increase in the global average temperature to well below 2 c above pre industrial levels and pursuing efforts to limit the temperature increase to 1 5 c unfccc 2015 since rcp2 6 is broadly consistent with constraining global average temperature rise to 2 c above pre industrial levels although we do not in this study explore scenarios which reduce temperatures more than this early international assessments such as the ipcc special report on emissions scenarios sres nakicenovic and swart 2000 used self consistent socio economic scenarios characterised by population gdp land use and energy use and greenhouse gas emission pathways over time sres scenarios were based upon an analysis of how demographic social economic environmental and technological aspects of our society might evolve globally in these scenarios two main axes of change were considered a environmental versus economic and b globalisation versus regionalisation of markets and cultures hence the four scenarios may be briefly summarised as a1 global economic a2 regional economic b1 global environmental b2 regional environmental a new process independent of the original sres scenarios has since been established moss et al 2010 this recognises that different socioeconomic pathways might have the same climatic change outcome hence sres scenarios have now been replaced by the representative concentration pathways rcps which were used in the ipcc fifth assessment report ar5 and new shared socio economic pathways ssps van vuuren et al 2011a kriegler et al 2012 ebi et al 2014 in ssps the axes of change are a challenge to mitigation and b challenge to adaptation for example increased population is a challenge to mitigation because energy demand will be higher ssps are based on new set of socio economic data including some trends important in sres such as population and gdp however other data may also be important but most fundamentally there is a change in the way in which the data are used the rcps and ssps have not been designed as a new fully integrated and self consistent set of socio economic and emission scenarios over time but instead offer the potential to mix and match alternative combinations this is undertaken in a framework a matrix that combines climate forcing on one axis as represented by the representative forcing pathways and socio economic conditions represented by the socio economic pathways on the other thus we apply this new methodology in our research firstly we ensured that our model couplings were reasonably harmonised in projecting greenhouse gas emissions associated with the rcp6 pathway and the rcp2 6 pathway fig 6 we used the five couplings above and others to derive policy relevant information the paths of the emissions from the three models gemini e3 remind and tiam world are illustrated in fig 6 alongside reference rcp2 6 and rcp6 trajectories from van vuuren et al 2011a labelled rcp6vv and rcp2 6vv showing that our simulations from all three models are broadly consistent with theirs substantial emissions reductions are needed in order to stabilize the greenhouse gas concentrations in the atmosphere to a level of around 450 ppm co2eq rcp2 6 model couplings including those listed above were used to explore this transition and to create different strategies for or implications of reaching these emission reductions carbon prices policy design energy technologies and climate change impacts were all explored applying the coupling in example 3 the gemini e3 model explores how the carbon price required to achieve the transition depends on the time of onset of climate change mitigation and on burden sharing approaches to climate change policy applying the coupling in example 4 remind simulates the most cost effective way to achieve the emission reductions globally exploring how this changes when the availability of biomass is low other couplings are used to explore the consequences of these emission reductions applying the coupling in example 5 tiam world simulates the consequences of climate change for heating and cooling demand in the two rcp scenarios and finally the coupling in example 2 assesses sea level rise impacts in the two scenarios referring to the coupling in example 3 here the gemini e3 model can be used to explore a set of standard burden sharing approaches babonneau et al 2016 and alternative constraints on the date at which climate policy in the form of the use of a carbon price in international markets is instigated table 3 highlights the key findings the model indicates that large rises in carbon prices are needed to achieve the necessary emission reduction however these are greatly reduced if policy is instigated in 2020 rather than 2030 the importance of early policy action has also been highlighted in other studies which report on the implications of short term emission targets for the cost and feasibility of long term climate goals such as the 2c target for limiting warming luderer et al 2015 riahi et al 2015 rogelj et al 2012 in assessing the transition from rcp6 to rcp2 6 remind selects from a large set of potential energy conversion technologies generating negative emissions by using biomass in combination with carbon capturing and sequestration turns out to be a favourable cost effective option fig 7 left panels the associated carbon price increases from almost 10 tco2 in 2010 to around 220 tco2 in 2050 the meta analysis of recent mitigation studies of clarke et al 2014 identifies a number of studies that demonstrate feasibility of rcp2 6 whilst emphasizing that higher carbon prices and reliance on bioenergy with carbon capture and storage are necessary to achieve this azar et al 2010 hence our results are in line with the findings of many other studies however we also explore the effects of limiting the supply of bioenergy leimbach et al 2016 within both rcp2 6 scenarios low and high biomass potential there is a fast phase out of the coal technologies which are the most carbon intensive fig 7 upper panels importantly while bioenergy and solar are similarly important for the long term energy mix in the rcp2 6 scenario high biomass potential solar energy is the dominant source of energy in the rcp2 6 biolow scenario the high sensitivity of the energy system to the availability of biomass can also be seen in fig 7 lower panel which shows the structure of biomass consumption in the case of sufficient availability of bioenergy it is cost effective to produce biofuels for the transport sector however it is most cost effective to use biomass to produce hydrogen when the biomass potential is low as this technology has comparatively lower emissions furthermore hydrogen has the potential to replace fossil resources in sectors other than transport coupling outputs suggested that carbon prices up to 600 t co2 were needed to achieve the transition to rcp3pd 1 1 rcp3pd corresponds to rcp2 6 and features a peak and decline in radiative forcing if biofuel cropping was minimised in order to reduce competition for land with agricultural crops and preserve natural ecosystems and biodiversity our studies project that reliance on biofuels for mitigation would induce widespread deforestation and other land use change globally consistent with the findings of many other studies e g fargione et al 2008 searchinger et al 2008 popp et al 2012 oppenheimer et al 2014 unless a carbon taxation scheme is used that includes terrestrial carbon consistent with the findings of wise et al 2009 our results indicate that the main response option in land use to climate change mitigation policy is agricultural intensification through investments in yield increasing technological change these are estimated to be 41 72 higher in the policy rcp3pd scenario compared to the bau business as usual rcp6 scenario over the 1995 technology level these are shown in fig 8 the role of agricultural intensification has also been highlighted elsewhere lotze campen et al 2010 tilman et al 2011 smith et al 2013 results obtained with the coupling plasim entsem tiam world coupling example 5 explore the feedback between the climate system and the energy system fig 9 they show that the climate feedback induced by adaptation of the energy system to heating and cooling is found to be insignificant partly because heating and cooling induced changes compensate and partly because they represent a limited share of total final energy consumption however significant changes are observed at regional levels in the reference case rcp6 labriet et al 2013a b in contrast they are negligible in rcp2 6 with smaller temperature changes while the increase in cooling demand is met with electricity the decrease of heating demand results mostly in a decrease in gas consumption this reflects the relatively higher costs of natural gas compared to other energy sources for heating in the longer term the need for power capacity to satisfy additional cooling services and the pressure on electricity demand result in increases in electricity prices for example up to 30 in europe in the mid term and 50 in the long term thus climate change was projected to have minimal effects on heating and cooling demand globally but effects were important regionally especially in europe a coupled plasim ents gemini e3 sequence is also used to analyse the impacts of sea level rise slr in the twenty first century to estimate slr we first use the emulator of the climate model plasim ents to compute the warming profile related to the gemini e3 baseline scenario the temperature increase is used to derive slr using a semi empirical relationship then the physical consequences of slr are computed using gis analysis which are incorporated in gemini e3 see joshi et al 2016 the simulation results suggest that the potential development of future coastal areas is a greater source of uncertainty than the parameters of slr itself in terms of the economic consequences of slr at global level the economic impact of slr could be significant when loss of productive land along with loss of capital and forced displacement of populations are considered furthermore highly urbanised and densely populated coastal areas of south east asia australia and new zealand are likely to suffer significantly if no protective measures are taken hence it is suggested that coastal areas needs to be protected to ameliorate the overall welfare cost across various regions coupled economic and climate models were also exploited in a game theoretical framework to analyse fairness and robustness of the international environmental agreements first we identify a total emission budget over the 2010 2050 period that is compatible with the warming at the end of the century being less than 2 c according to our climate models first results show that an acceptable voluntary burden sharing agreement could be obtained among all groups of countries with a balanced welfare loss below 1 of total discounted household consumption in such an agreement see fig 10 15 3 of the total emission budget of 424gtc is allocated to usa 8 to eu 22 5 to china 7 5 to india 4 8 to russia in a robust solution that prevents potential emissions overshooting in such commitments and takes potential errors arising in the various approximations made in our methodology into consideration the welfare loss rises to 1 8 for each group of countries this analysis has recently been extended see haurie et al 2015 and babonneau et al 2016 5 conclusions a set of coupled models has been developed within an integrated framework that can be used in future research projects involving policy makers and other stakeholders based on the community integrated assessment system the bespoke framework generator and the use of statistical emulators for model coupling we use it to show that when using carbon price mechanisms to induce a transition from a high carbon to a low carbon economy prices can be minimised if policy action is taken early if burden sharing regimes are used and if agriculture is intensified this is of particular relevance owing to the recent adoption of the paris agreement unfccc 2015 the approach has created long lasting coupled models available for future policy relevant research exploration of the robustness of coupled model outputs to uncertainties should form a key part of this future work acknowledgement this work was funded by the eu 7th framework programme project number 265170 enhancing robustness in model integration for the assessment of global environmental change ermitage rfw was also supported by uk natural environment research council advanced fellowship grant no ne f016107 1 software availability table software name bfg2 developer rupert ford contact address and postcode stfc daresbury laboratory warrington wa4 4ad u k tel 44 1925 60 3217 e mail rupert ford stfc ac uk mailto aa manchester ac uk year first available 2005 hardware required none specific software required python2 libxml2 libxslt python lxml2 program language python xslt program size approx 500 kb compressed tar file availability downloadable from http cnc cs manchester ac uk projects bfg php http www cs man ac uk cost free for non commercial use software name softiam developers sudipta goswami santiago de la nava santos rachel warren contact address tyndall centre contact rachel warren tel 44 1603 593912 fax 44 1603 593901 email r warren uea ac uk year first available 2004 hardware required software required bfg2 program language program size availability cost not for sale software name cias developer sudipta goswami santiago de la nava santos matt hyde rachel warren contact address tyndall centre contact rachel warren tel 44 1603 593912 fax 44 1603 593901 e mail r warren uk ac uk year first available 2005 hardware required pc availability some applications are accessible via web portal upon request for password cost not for sale software name plasim entsem developer philip holden neil edwards address environment earth and ecosystems the open university milton keynes uk email philip holden open ac uk neil edwards open ac uk year first available 2012 hardware required none specific software required r program language r program size 152 mb nb mostly input files the code itself is very small availability philip holden open ac uk neil edwards open ac uk cost free for non commercial use software name gemini e3 developer alain bernard and marc vielle contact address and postcode marc vielle epfl enac leure bp2140 station 16 ch 1015 lausanne switzerland tel 41 21 6932031 fax 41 21 6933840 e mail marc vielle epfl ch year first available 1995 hardware required none specific software required gams the general algebraic modeling system program size approx 10 mo availability contact email to enquire for availability cost not for sale software name tiam world developer kanors kanlo eneris contact address and postcode amit kanudia sdf l7b nsez phase ii noida 201305 up india tel 91 9871 488 591 e mail amit kanors com year first available 2005 hardware required pc software required gams solver cplex xpress under windows environment program language gams program size about 1 million row lp availability contact via email cost contact via email software name remind developers nico bauer lavinia baumstark christoph bertram anastasis giannousakis markus haller jerome hilaire david klein marian leimbach antoine levesque gunnar luderer michael lueken ioanna mouratiadou michaja pehl robert pietzcker franziska piontek anselm schultes jessica strefler tino aboumahboub tabare curras alexander körner sylvie ludig jana schwanitzcontact address and postcode potsdam institute for climate impact research p o box 601203 14412 potsdam germany tel 49 331 288 2556 e mail leimbach pik potsdam de year first available 2010 hardware required pc unix machine software required gams conopt solver program language gams program size approx 370 mb availability model description downloadable from http www pik potsdam de research sustainable solutions models remind cost no commercial use software name lpjml vs 3 2 developer pik lpjml team and collaborators contact address and postcode dieter gerten responsible scientist see above tel 49 331 288 2577 fax 49 331 288 2695 e mail gerten pik potsdam de year first available 2007 hardware required none specific unix preferably software required none specific program language c program size approx 1 5 mb tar file source code only availability via http www pik potsdam de research projects cooperations lpjml or via contact email to enquire for availability cost free for non commercial use in case of cooperation agreement software name magpie developer potsdam institute for climate impact research contact alexander popp contact address potsdam institute for climate impact research p o box 601203 14412 potsdam germany tel 49 331 288 2463 e mail popp pik potsdam de year first available 2008 hardware required none specific software required gams conopt cplex solver program language gams r program size approx 30 mb availability upon request cost upon request 
26291,eutrophication of inland water is a serious problem in large parts of the world mitigation actions for improved water status are important but often costly to implement therefore tools for estimating the plausible effect of mitigation scenarios are needed to plan which actions are most effective in this paper we implement a web based interactive tool that allows quick exploration of several alternative mitigation scenarios in the paper we motivate and describe the method of deriving the tool from more complex modelling systems we implement tools for sweden and europe based on the hydrological simulation models s hype and e hype s hype is used as one important source of information for sweden s reporting of water status within the european union water framework directive we evaluate the approach by showing that hypothetical changes in load and realistic scenarios have good agreement with full model simulation keywords eutrophication mitigation actions web based software hydrological simulation 1 introduction fresh water is a limited resource and it is important to use it in a sustainable way population pressure changes in lifestyle and climate change add constraints on the use of water for individuals as well as by the society as a whole in europe the european union eu has addressed the issue of water quality within the water framework directive wfd in this directive the member states are commissioned to repeatedly characterize the status of their water resources and potential for improvement based on selected criteria based on this characterization they are also commissioned to make plans for improved water status nutrients are one important factor that determines the water quality human activities such as agriculture and industry increase the nutrient load to surface waters although mitigation actions e g waste water treatment agricultural measures and limitation of phosphorus in detergents have decreased concentrations in many rivers e g grimvall et al 2014 istvánovics and honti 2012 eutrophication is still a problem e g in southern sweden vattenmyndigheterna i samverkan 2014a c characterization of water resources and creation of mitigation plans for improving water status require detailed knowledge of nutrient sources to surface waters and the transport and fate of nutrients in surface water system in sweden the swedish meteorological and hydrological institute smhi is commissioned to make databases of hydrography statistics and water flows and scenario models available as a basis for the reporting to eu this led to the development of the open website vattenweb smhi se making measured and simulated information regarding discharge and simulated information regarding nutrient load available for end users strömbäck et al 2013 there simulated data for water and water quality is provided by the hydrological simulation model s hype strömqvist et al 2012 which is an application of the hype model hydrological predictions for the environment lindström et al 2010 for sweden the s hype model covers all catchments draining sweden by dividing the area into approximately 37000 subbasins several versions of s hype have been released as both the hype model and it s applications are under constant development e g pers et al 2016 lindström 2016 the s hype model was set up for simulation of present conditions as well as for pre industrial background conditions for sweden among the results is high geographical resolution nutrient source apportionment of the load this includes load to surface water gross load load in water flowing to downstream areas net load as well as the relative difference retention for european conditions the hydrological simulation model e hype donnelly et al 2013 2016 is available for simulating nutrient load on the european scale dividing europe into approximately 35000 subbasins results from this model are available at http hypeweb smhi se for each lake and river there is often a large number of plausible actions to reduce the nutrient load and it is of high interest to compare the effect of them despite all the available information on the websites estimating the potential for improving water status and comparing the effect of different mitigation actions is a complex task it may require information on how the local conditions affect actions and how the actions affect the nutrient load in a detailed way this kind of calculations requires expert knowledge and modelling bärlund et al 2007 arheimer et al 2015 and expensive computation one example is the arheimer et al 2015 study of söderköpingsån catchment and the receiving bay which was carried out with expert competence from smhi and several hours of consultancy and meetings simplifying these computations and adding them to the website would make it possible for the end users to test different plausible mitigations scenarios on their own and compare the effect at a downstream location recent development within web technology allows for designing a tool performing simulations and visualizations in the browser walker and chapra 2014 this approach enables implementing an interactive tool that would facilitate learning and exploration similar approaches have been used for management strategy design and decision support tools e g moeseneder et al 2015 greene et al 2015 in this paper we define and implement a web based tool the scenario tool for estimating nutrient change scenarios the main purpose of the tool is to enable exploration of mitigation actions for improved water status thus the goal is to build an interactive tool that is easy to use and understand the tool is based on the subbasin division together with information on retention and of nutrient loads for each subbasin for a specific hype setup in the paper we describe the method for calculations and give a general overview of the implementation we assess assumptions of the method and evaluate the approach for a percentage reduction in load and for advanced realistic scenarios by comparing the downstream nutrient reduction to full scale simulations with the hype model 2 material and methods 2 1 user requirements and main functionality the main challenges during the development of the scenario tool were to design it to be easy to use and understand and allowing fast computations with retained quality of the computed mitigation scenarios it was clear from the beginning that more simple computations than what is implemented in the original hydrological model were needed to obtain sufficient speed as a rerun of hype simulations takes several minutes a prerequisite for a useful end product was user involvement already in the design of the tool therefore the development of the tool was done in close cooperation between hydrological and it specialists and a group of end users using participatory design methods jonsson et al 2011 the group of hydrology experts together with the end users provided knowledge on water quality directives and methods for assessment of water status these experts formed a platform for discussions around concepts and calculations in particular on how to measure and calculate water quality and reference values it was important to discuss and agree on these methods and terms as the experts in many ways define the framework for assessing water quality in sweden experts and users took an active role in the development of the tool and gave input regularly during the development process the users identified many potential situations where they would have benefited from the system in their work situations were ranging from investigation of status and finding causes of bad status in an area through giving an overview of possible actions to more formal uses such as allowing new industrial establishments and plan actions based on allowed total cost the discussions resulted in the decision to build a pedagogical tool that can be used at meetings with stakeholders to discuss possible actions and need for further investigations based on the user input we defined the following main functionality for the tool 1 show the nutrient nitrogen and phosphorus load to the surface water for a particular area of interest the tool should also present the source apportionment and geographical origin of this load 2 presentation and visualisation of results should be easy to understand for stakeholders so that it could be used during a discussion or presentation 3 allow the user to design and compare the effect of plausible scenarios for an area of interest by changing the load in upstream locations 4 fast response times and the possibility to get an overview of scenarios are of high importance 2 2 computational method with the s hype model nutrient mitigation scenarios can be computed by reruns of the model with changed input however such simulations are time consuming and require expert knowledge of s hype the method defined in this paper relies on simplified computations to achieve fast computations this allows a non expert user to explore mitigations scenarios in real time in a web application the method is based on a delineation network for the area of interest yearly means for gross loads to surface waters and surface water retentions in every subbasin of the area a formal definition of the required data is given below and illustrated in fig 1 the hydrological network i e the catchment s division into subbasins and the connections between these based on this information we can for each subbasin subi in the model find a number of subbasins subu1 to subuj that are directly upstream subi bifurcation division of flow into branches bif subji is the fraction of the flow from subbasin j to subbasin i this value is 1 for all subbasins flowing to only one downstream area gross nutrient load gdiff subi as the sum of load for diffuse sources and gpoint subi as the sum of load for point sources for each subbasin subi the gross nutrient load is here defined as the gross load to surface water i e the load reaching the streams and lakes the gross loads are taken from the calculated source apportionment of the original model we used yearly mean values over a longer period to neutralise weather differences examples of diffuse sources are land use and rural household while for instance waste water treatment plants and industries are regarded as point sources the diffuse land use load includes nutrients from overland flow runoff from the soil profile including contribution from groundwater relative retention of the respective nutrients for each subbasin subi we use two different values specific to each subbasin rettot subi the total retention of the subbasin from ditch through streams and lakes which is the retention of diffuse sources and retmain subi the retention in the main stream and lake of subbasin which is the retention of point sources gross nutrient loads are all added to a stream local stream for diffuse source and main stream for point sources and upstream load and are subjected to the respective retention of the subbasin retention in this paper always means the fraction of nutrient lost between the source and the outlet of the subbasin we use retention from a steady state corresponding to average nutrient load for a selected period representing current conditions based on the defined concepts we calculate the net load leaving the outlet of an area as sum of 1 the gross load from diffuse sources reduced with the total retention of the subbasin 2 the gross load from point sources reduced by the retention in the main stream 3 the sum of the net load from all upstream areas flowing into this subbasin reduced by the retention in the main stream in this subbasin mathematically this can be expressed as n s u b i gdif f s u b i 1 retto t s u b i diffuse sources gpoin t subi 1 retmai n s u b i point sources 1 upbasins n s u b u j 1 retmai n s u b i bi f s u b u j i upstream subbasins in this formula upbasins represent the number of upstream basins of subi using this equation the effect of one or more suggested mitigation actions in upstream areas can efficiently be propagated to the outlet of a downstream subbasin 2 3 technical architecture and implementation the tool has been implemented as a web based tool using client server architecture the web server consists of a map server and a database containing information about the hydrological network retentions and nutrient load for each subbasin in the area modelled currently two versions of the tool are available one for sweden http vattenweb smhi se scenario and one for europe http eutrophication eu in the current version of the tool for sweden the retentions gross loads and bifurcation data needed for the tool is imported into the tool from a historical run with s hype2012 4 0 0 for the years 2004 2015 this means that the retention used for the tool setup is calculated based on the long term mean of nutrient load for this period for europe a similar import has been done based on a run with e hype 3 1 1 to achieve a fast interactive tool all necessary computations are performed in the web browser on the client side and implemented using html5 and java script i e similar technology as described in walker and chapra 2014 this results in web based tools where it is easy for the user to move the focus area explore the different sources of load by altering visualisation options and hovering over the graph and finally to assess changes and explore the result at the focus area the interface for the european version of the tool is shown in fig 2 results can also be shared by sharing urls to a special scenario in the tools for the european version of the tools it is also possible to export and import scenarios as excel files 2 4 evaluation of the computational method the computational method used in the scenario tool was evaluated for different aspects of its construction and use the assumption of retention in relative terms as independent of nutrient load and speciation the simplification of using long term mean of nutrient loads and the estimation of realistic mitigation measures to investigate the first point we compared the retentions in two versions of the s hype model with different load the versions were the normal model set up with present condition of nutrient load for sweden and a background model where all anthropogenic sources of nutrient had been removed the computing of background load is based on previously used methods brandt et al 2009 including replacing arable land by pasture and remove point sources as most interesting mitigation actions would aim at reducing anthropogenic sources towards the level of the background model these two model runs would cover the variations in load of realistic mitigation scenarios very well although the total load range is covered variation in nutrient speciation may also affect the retention the retention s dependence on load does not need to be linear and limited by the found range in general there are small differences in relative retention between the two models fig 3 shows the variation in total retention for sweden this implies that the simulated retention in amount is rather linear in respect to the incoming load and that we could expect a good performance of the simplified calculations made by the tool for most areas in sweden there are some exceptions mainly in the south and around the big lakes with larger variation in retention and here we could expect a worse performance of the scenario tool 1 this map includes the glomma area which is part of norway the second point was investigated for selected catchments focus areas by comparing the performance of the simplified computations in the tool with result from simulations of the full s hype with the same changes in nutrient load applied catchments in southern sweden were selected as this part of sweden showed the largest variations in retention the locations of the catchments are shown in fig 4 together with a minimap of each area explaining where we changed the nutrient load i e the application area ronnebyån was selected as an example of a small river and ringsjön as an example of a small lake motala and hjälmaren were selected to test the tool in two of the larger lakes of sweden finally karlstad was selected to test the performance at a bifurcation for both the scenario tool and s hype the load of an upstream point the application area fig 4 was varied by removing 50 of the net load to adding 50 of the net load this range was based on findings in the brandt et al 2009 report that states that the anthropogenic load for nitrogen is 50 of the total load for sweden for our application areas the anthropogenic load varies between 14 and 75 for nitrogen and phosphorus table 1 this corresponds to removing all anthropogenic load based on the sources in the s hype model including atmospheric deposition the scenarios range should thus well cover all realistic scenarios of changes in nutrient load only the load from the application area is reduced increased in the scenarios while additional loads between the application area and the focus area are not changed the usefulness of the tool was tested by reproducing a set of real life scenarios previously defined and applied in arheimer et al 2015 c f section 3 2 the study area is söderköpingsån catchment in sweden a small catchment 882 km2 with 8 lake area draining to a bay of the baltic sea in that study a number of plausible nutrient mitigation scenarios from different societal sectors waste water treatment rural households wetlands and agriculture were defined by the local authorities based on wfd targets the different scenarios were simulated using a full scale hype model to estimate the effect of the actions this hype application was specially adapted to söderköpingsån catchment but based on input data from an earlier s hype version in this study these scenarios were reproduced with the scenario tool where additional information was needed to estimate input to scenarios we used the two publicly available water related sites vattenweb smhi 2017 and viss swedish county administrative board 2016 this approach evaluates whether available information is sufficient as input to the tool and gives a comparison between the results in the tool and the full scale simulations with hype 3 results we start this section by presenting the functionality of the developed tool before discussing the results of computations the tool gives a possibility to easy explore nutrient loads for mitigation scenarios the interface is shown in fig 2 as a first step the user chooses the subbasin for which outlet point results are wanted by clicking on the map the chosen area is marked with a blue flag and a graph with red circles shows the net load from each subbasin upstream of the focus area and the net load at the outlet of the area is shown in the lower right pane the pane shows the total net load and the source apportionment of the net load for the area to get more information the user can manipulate the graph by using the lower left pane for example only showing a specific source of interest for instance agriculture or industry the user can alter the graph to show the contribution from each upstream subbasin to the load at the focus area further information about the areas can be gained by hovering over the circles in the graph if desired the user can also easily move the flag around to investigate other areas or subareas that might be of interest this allows for discussions around the current situation to investigate the effect of one or several plausible mitigation actions or other planned change the user simply clicks on the corresponding circle and adds the deduction or addition of nutrient load in the pop up window the effect of the actions at the focus area will immediately show up in the lower right pane the user can make changes in as many of the upstream subbasins as desired to analyse the combined effects the results presented below are based on the swedish version of the tool analys och scenarioverktyg för övergödning i sötvatten the retentions gross loads and bifurcation data needed for the tool is based on historical run with s hype2012 2 0 for the years 1999 2011 3 1 validity of the computations as explained above the tool does not take into account that retention varies with nutrient load and speciation and with time to understand how this would affect the reliability of the tool s result we compared the results of the tool with full model results from s hype based on realistic assumptions on changes in nutrient load as a first step we compared the retention of two versions of s hype with different nutrient loads fig 3 table 1 this comparison shows that there in general is very small difference in relative retention for most areas that would affect the validation however in the selection of areas fig 4 for validation we included some areas with larger variation in retention where we could expect a worse performance of the scenario tool to investigate the impact of variations in retention and to evaluate the performance of the scenario tool we performed several runs of the s hype model for each selected catchment in the runs we modified the net load corresponding to scenarios where the net load from the selected areas were changed in steps of 25 ranging from 50 to adding 50 of the load the results are shown in table 2 nitrogen and 3 phosphorus for the ringsjön focus area the s hype version used was s hype2012 4 0 0 the net load at the focus area without any modifications differed a little between the tool and the s hype simulation for this area 0 2 the s hype net loads were therefore adjusted before the difference for levels of change in scenario between the tool and the s hype scenarios was calculated as shown in the tables there is almost no difference between the simplified tool and a full s hype run for several of the catchments the exceptions are the lakes hjälmaren and ringsjön where we see a rather large discrepancy between the tool and a full s hype run for phosphorus this was not unexpected since these lakes are among the locations were the retention varied most between the normal s hype and the corresponding background model fig 3 to further investigate the variation we decided to make a more detailed study of the two locations with the largest discrepancies for these points we varied the change in load from 20 to 20 investigating the difference between the scenario tool and s hype for smaller changes table 4 this second evaluation shows that for changes up to 10 of the original net load the scenario tool gives results well in accordance to the full scale s hype model for changes up to 20 the differences are small enough to give a useful first estimation of the change of load at the focus area 3 2 a case study on real life scenarios to test the usefulness of the nutrient scenario tool we applied it to a set of real life scenarios in söderköpingsån catchment fig 5 previously simulated by arheimer et al 2015 the scenarios were grouped into the societal sectors they concern the possible reduction of nutrient load for a societal sector is dependent on the sector s share of the total load for sectors where this is relevant we show this share for the scenario tool and for the hype model used in arheimer et al 2015 study table 5 the difference in source apportionment between the two models is due to an update of nutrient sources between the s hype versions it can be noted that in the scenario tool this information is immediately available when selecting an area and gives important information on how much different sectors contributes to the nutrient load of the area for the societal sector of waste water treatment plants wwtp we evaluated two different scenarios related to improved waste water treatment and a third where the plan was to remove two of the wwtps and instead lead the water to a wwtp in a different subbasin for the first two we have assumed the same proportion of nutrient reductions as in the original study to get a realistic comparison for the third we removed the load from the two waste water treatment plants according to the load in vattenweb the effect of these scenarios resulted in up to 6 reduction of nitrogen and up to 1 5 of the phosphorus load at the focus area table 5 the results of using the scenarios in the tool are very similar to the results in the original experiment the small differences are proportional to the differences in the share of load from wwtp in the two versions of s hype used for the experiments except for nitrogen in scenario 2 the societal sector concerning rural households contains two scenarios one connecting 200 households to a nearby wwtp and a second connecting all rural households the first of these scenarios gives a very limited effect at the focus area both nitrogen and phosphorus is reduced 0 1 0 2 in the second case almost all phosphorus load from rural households is removed and a significant part of the nitrogen load the difference between the tool and the full hype simulation is that arheimer et al 2015 assumed the water treatment was more efficient for phosphorus than for nitrogen while we assumed similarly high efficiency of water treatment for both nitrogen and phosphorus the scenarios in the following sector investigate the effect of constructing wetlands for reducing the amount of nutrients reaching the rivers the original study makes use of hypes wetland routines for calculating the effect of constructing wetlands in our evaluation we decided to instead use the information from viss swedish county administrative board 2016 stating the plausible effect on nutrients load by constructing wetlands viss contains a library of various mitigation measures including several kinds of wetlands where the reduction of nitrogen varies from 200 kg to 675 kg nitrogen per ha and year and the reduction of phosphorus varies from 5 to 65 kg per ha and year we used these ranges for our scenarios for nitrogen the original arheimer et al 2015 wetland experiments had lower reduction than the tool calculated for phosphorus our results gives a large range dependent of the different types of wetlands with different reductions available in viss the reduction from the original experiment is in the middle of this range the reduction of nutrient load from a wetland is highly dependent on the local conditions and actual locations of the wetland arheimer and wittgren 1994 2002 here the setup used by the scenario tool assumes an optimal placement of the wetlands in terms of concentrations of nutrient in water flowing into the wetland and that the full effect of the reduction reaches the outlet of the subbasin the original hype simulation is based on intermediate concentration and placement of the wetland within the subbasin for agriculture we investigated the effect of protection zones and a general reduction of load from agriculture for scenario 8 the scenario tool used information from viss on a reduction of 0 1 kg phosphorus per ha to estimate the effect of protection zones the general reduction scenario is similar to the scenarios in section 3 1 but here the load is reduced for all upstream subbasins with agricultural load also in these cases the reduction in our experiment is very similar to what was calculated by the original simulations to summarize in this evaluation based on realistic mitigation scenarios differences between the tool and a full scale hype simulation are small enough to give useful estimates for selection of scenarios in most case differences based on assumption of the estimated effect of a mitigation action are larger than differences in the computed effect by the different methods moreover the interactive behaviour of the tool gives a clear benefit in realistic applications as it offers a possibility to easily run a large number of different scenarios with different assumptions 4 discussion the nutrient analysis and scenario tool has achieved the main goal for the development it is an easy to use tool that gives an overview over how different nutrient sources in an area contribute to the nutrient load in addition it allows a fast comparison of different scenarios for reducing the nutrient load in the area the presented evaluations show that the simplified computations give comparable result to the full model simulations for most realistic scenarios of nutrient mitigation as such the tool gives a good first overview over plausible nutrient reduction scenarios and their effect however the tool makes only steady state calculations and in many cases this first overview should be followed by more detailed studies with the full model or other method over some selected scenarios this is particularly important when the area has high retention if the expected change is large or if the investments for achieving the effect are high if the mitigation actions are to be introduced step by step or if its effect is building up gradually that could also warrant a more sophisticated method than the scenario tool s static calculations another factor to consider in a detailed study is additional on going changes e g in land use or climate as already noted the tool makes steady state calculations and does not estimate the time for this steady state to be reached in addition the tool does not consider the retention processes dependence on changes in other nutrients because its use of pre calculated constant relative retention conley et al 2009 regarding the severity of eutrophication other factors beside nutrient load affect e g algal blooms and anoxia for instance the limiting nutrient and type affects algae community changing hydrodynamic conditions affect stratification and turnover time light conditions depend on turbidity which can be caused by fish sediment resuspension heisler et al 2008 schwefel et al 2016 bergman et al 1999 fully investigating and understanding the eutrophication problem of an area need to encompass more than the information given by the tool the algorithm in the tool is relatively simple and easy to implement however the method requires input data consisting of subbasin division routing retention and information about nutrient load from each source for each subbasin in this paper we have used data from s hype arheimer et al 2011 strömqvist et al 2012 and e hype donnelly et al 2013 2016 to implement two versions of the tool however in principle any database or hydrological model providing the required background data for the tool can be used to implement the tool the quality of the model providing the input data sets a limitation for the performance of the scenario tool the swedish version of the tool has a higher resolution than the european version as s hype is based on local knowledge of sweden compared to e hype which is mainly based on open data for europe arheimer et al 2012b this means that for especially the european case local conditions might not be captured by the general input about nutrient sources derived from the open data sets the hydrological model is in itself a simplification in hype land use soil type combinations are treated as computational units the water and nutrients leaving such a computational unit include overland flow soil runoff including outflow from shallow groundwater it does not include flow from aquifers as they are calculated separate from soil profile hype and the tool assumes all land use sources enter the local stream at the top of the stream and passes through both local stream and main stream and possibly lakes thus flow in ditches and groundwater flow that actually enters along the stream are treated with the full effect of the simulated river processes like all diffuse sources the calculated result of the model strömquist et al 2012 donnelly et al 2013 shows that this has been a good enough approximation for simulating river nutrient concentrations for the present conditions our validation shows that the estimated reduction in nutrient load at different scenarios has a high agreement to the original model when changes are limited in most cases realistic scenarios are well within these limitations of the tool it should be noted that the full models used in this study are calibrated and validated for present conditions and that it is a topic for future research how well they also capture changed conditions such as a large change in nutrient load arheimer et al 2012a 2015 jomaa et al 2016 or climate capell and olsson 2013 many of the larger differences in retention between the original model and the tool and between the background simulation and the present were found for areas with lakes there the original model may have included simulation of internal loading of phosphorus due to previous high loads this is highly affecting the modelled lakes retention it may even be zero a large reduction in load could in reality create a new internal source when old stocks are starting to be released this scenario is not handled in the present version of the tool because it uses a static retention in addition this is a case where the approximation of independent nutrient may be wrong and possibly affecting the results the user needs to be aware of the limitations of the scenario tool and the underlying model to properly judge when a scenario result is credible the method described in this article concentrates on the calculation and comparison of the downstream effect of performed measures it does not contain information about different measures and their plausible effects this focus of calculations was decided together with swedish end users as they did not want any specific assumptions to be built into the system in addition we saw that the already available viss database contains a lot of information that can be used together with the scenario tool as shown by our evaluation the scenario tool and the viss database complement each other the strength of our tool is the ability to easily compare different measures and their downstream effect together with information on measures from viss or as local knowledge the tool can be used to give a fast estimation on combination of measures or alternative scenarios the lack of coupling between possible measures and their effect and scenarios performed by the tool gives a certain freedom to use the tool practical or realistic limitations of the effect of different measures may be partly obtained from the information of the tool e g size of point source however the user needs to have some understanding of catchment nutrient management to know if a scenario is realistic and if the results are credible for a river with most of its load coming from point sources a large reduction of the load may be reasonable while for a river with most of its load coming from diffuse sources e g a large groundwater store a large reduction of load may be an unrealistic scenario for other measures e g changing manure practise consideration needs to be given to the effect of groundwater retention on the measure before deciding on what the effect of the measure is on the load to surface water we see several possibilities for further development of the tool an important improvement is to increase the information about what sources of nutrient load e g industries population and treatment plants are included in the calculations made by the tool this will increase the understanding of what assumptions the computations are made from and give the end user a possibility to compare the results with local information about an area it is also possible to increase the accuracy of the tool in areas were the retention varies with nutrient loads see fig 3 by making the retention a function of the load this function can be derived from a series of computations with s hype this improvement could for instance give better estimations for lakes with high internal load finally an improvement of the functionality is the possibility to download scenarios as excel sheets providing a more efficient way of experimenting with and document multiple scenarios a first version of this is available in the european version of the tool 5 conclusions in this paper we define a method enabling a web based scenario tool that provides fast and accessible information for an overview of the present situation and plausible scenarios the tool is implemented and openly available for sweden and europe based on data from simulations made by s hype and e hype it enables the user to explore a large number of alternative scenarios to compare the effect of mitigation actions for improved water status our evaluation shows that the results delivered by the tool are in most cases comparable to more expensive full scale simulation and can be used to estimate effect of mitigation actions for realistic scenarios for larger changes especially involving lakes with high retention or high internal load it is possible to improve the estimations made by the tool by basing it on runs with different load however for these cases we recommend to complement the results from the tools with full scale simulations software availability the described tools are available on http vattenweb smhi se scenario and http eutrophication eu the algorithms and interface are implemented in javascript and can be viewed via the applications for the server side code please contact lena stromback smhi se acknowledgements the author contributions all authors have contributed to the design of the method and the tools l s and c p wrote the paper and performed the evaluations j g made the implementation the first swedish version of the tool was developed as part of the work for the swedish agency for marine and water management the project switch on eu fp7 grant agreement no 6035 funded the development of the european version of the tool and the swedish research council project emap 2011 624 has funded research and evaluations needed for completing the paper the authors thank all colleagues that have been involved and contributed to the result this paper by feedback and input in particular we are grateful to henrik spångmyr lotta andersson berit arheimer and niclas hjerdt 
26291,eutrophication of inland water is a serious problem in large parts of the world mitigation actions for improved water status are important but often costly to implement therefore tools for estimating the plausible effect of mitigation scenarios are needed to plan which actions are most effective in this paper we implement a web based interactive tool that allows quick exploration of several alternative mitigation scenarios in the paper we motivate and describe the method of deriving the tool from more complex modelling systems we implement tools for sweden and europe based on the hydrological simulation models s hype and e hype s hype is used as one important source of information for sweden s reporting of water status within the european union water framework directive we evaluate the approach by showing that hypothetical changes in load and realistic scenarios have good agreement with full model simulation keywords eutrophication mitigation actions web based software hydrological simulation 1 introduction fresh water is a limited resource and it is important to use it in a sustainable way population pressure changes in lifestyle and climate change add constraints on the use of water for individuals as well as by the society as a whole in europe the european union eu has addressed the issue of water quality within the water framework directive wfd in this directive the member states are commissioned to repeatedly characterize the status of their water resources and potential for improvement based on selected criteria based on this characterization they are also commissioned to make plans for improved water status nutrients are one important factor that determines the water quality human activities such as agriculture and industry increase the nutrient load to surface waters although mitigation actions e g waste water treatment agricultural measures and limitation of phosphorus in detergents have decreased concentrations in many rivers e g grimvall et al 2014 istvánovics and honti 2012 eutrophication is still a problem e g in southern sweden vattenmyndigheterna i samverkan 2014a c characterization of water resources and creation of mitigation plans for improving water status require detailed knowledge of nutrient sources to surface waters and the transport and fate of nutrients in surface water system in sweden the swedish meteorological and hydrological institute smhi is commissioned to make databases of hydrography statistics and water flows and scenario models available as a basis for the reporting to eu this led to the development of the open website vattenweb smhi se making measured and simulated information regarding discharge and simulated information regarding nutrient load available for end users strömbäck et al 2013 there simulated data for water and water quality is provided by the hydrological simulation model s hype strömqvist et al 2012 which is an application of the hype model hydrological predictions for the environment lindström et al 2010 for sweden the s hype model covers all catchments draining sweden by dividing the area into approximately 37000 subbasins several versions of s hype have been released as both the hype model and it s applications are under constant development e g pers et al 2016 lindström 2016 the s hype model was set up for simulation of present conditions as well as for pre industrial background conditions for sweden among the results is high geographical resolution nutrient source apportionment of the load this includes load to surface water gross load load in water flowing to downstream areas net load as well as the relative difference retention for european conditions the hydrological simulation model e hype donnelly et al 2013 2016 is available for simulating nutrient load on the european scale dividing europe into approximately 35000 subbasins results from this model are available at http hypeweb smhi se for each lake and river there is often a large number of plausible actions to reduce the nutrient load and it is of high interest to compare the effect of them despite all the available information on the websites estimating the potential for improving water status and comparing the effect of different mitigation actions is a complex task it may require information on how the local conditions affect actions and how the actions affect the nutrient load in a detailed way this kind of calculations requires expert knowledge and modelling bärlund et al 2007 arheimer et al 2015 and expensive computation one example is the arheimer et al 2015 study of söderköpingsån catchment and the receiving bay which was carried out with expert competence from smhi and several hours of consultancy and meetings simplifying these computations and adding them to the website would make it possible for the end users to test different plausible mitigations scenarios on their own and compare the effect at a downstream location recent development within web technology allows for designing a tool performing simulations and visualizations in the browser walker and chapra 2014 this approach enables implementing an interactive tool that would facilitate learning and exploration similar approaches have been used for management strategy design and decision support tools e g moeseneder et al 2015 greene et al 2015 in this paper we define and implement a web based tool the scenario tool for estimating nutrient change scenarios the main purpose of the tool is to enable exploration of mitigation actions for improved water status thus the goal is to build an interactive tool that is easy to use and understand the tool is based on the subbasin division together with information on retention and of nutrient loads for each subbasin for a specific hype setup in the paper we describe the method for calculations and give a general overview of the implementation we assess assumptions of the method and evaluate the approach for a percentage reduction in load and for advanced realistic scenarios by comparing the downstream nutrient reduction to full scale simulations with the hype model 2 material and methods 2 1 user requirements and main functionality the main challenges during the development of the scenario tool were to design it to be easy to use and understand and allowing fast computations with retained quality of the computed mitigation scenarios it was clear from the beginning that more simple computations than what is implemented in the original hydrological model were needed to obtain sufficient speed as a rerun of hype simulations takes several minutes a prerequisite for a useful end product was user involvement already in the design of the tool therefore the development of the tool was done in close cooperation between hydrological and it specialists and a group of end users using participatory design methods jonsson et al 2011 the group of hydrology experts together with the end users provided knowledge on water quality directives and methods for assessment of water status these experts formed a platform for discussions around concepts and calculations in particular on how to measure and calculate water quality and reference values it was important to discuss and agree on these methods and terms as the experts in many ways define the framework for assessing water quality in sweden experts and users took an active role in the development of the tool and gave input regularly during the development process the users identified many potential situations where they would have benefited from the system in their work situations were ranging from investigation of status and finding causes of bad status in an area through giving an overview of possible actions to more formal uses such as allowing new industrial establishments and plan actions based on allowed total cost the discussions resulted in the decision to build a pedagogical tool that can be used at meetings with stakeholders to discuss possible actions and need for further investigations based on the user input we defined the following main functionality for the tool 1 show the nutrient nitrogen and phosphorus load to the surface water for a particular area of interest the tool should also present the source apportionment and geographical origin of this load 2 presentation and visualisation of results should be easy to understand for stakeholders so that it could be used during a discussion or presentation 3 allow the user to design and compare the effect of plausible scenarios for an area of interest by changing the load in upstream locations 4 fast response times and the possibility to get an overview of scenarios are of high importance 2 2 computational method with the s hype model nutrient mitigation scenarios can be computed by reruns of the model with changed input however such simulations are time consuming and require expert knowledge of s hype the method defined in this paper relies on simplified computations to achieve fast computations this allows a non expert user to explore mitigations scenarios in real time in a web application the method is based on a delineation network for the area of interest yearly means for gross loads to surface waters and surface water retentions in every subbasin of the area a formal definition of the required data is given below and illustrated in fig 1 the hydrological network i e the catchment s division into subbasins and the connections between these based on this information we can for each subbasin subi in the model find a number of subbasins subu1 to subuj that are directly upstream subi bifurcation division of flow into branches bif subji is the fraction of the flow from subbasin j to subbasin i this value is 1 for all subbasins flowing to only one downstream area gross nutrient load gdiff subi as the sum of load for diffuse sources and gpoint subi as the sum of load for point sources for each subbasin subi the gross nutrient load is here defined as the gross load to surface water i e the load reaching the streams and lakes the gross loads are taken from the calculated source apportionment of the original model we used yearly mean values over a longer period to neutralise weather differences examples of diffuse sources are land use and rural household while for instance waste water treatment plants and industries are regarded as point sources the diffuse land use load includes nutrients from overland flow runoff from the soil profile including contribution from groundwater relative retention of the respective nutrients for each subbasin subi we use two different values specific to each subbasin rettot subi the total retention of the subbasin from ditch through streams and lakes which is the retention of diffuse sources and retmain subi the retention in the main stream and lake of subbasin which is the retention of point sources gross nutrient loads are all added to a stream local stream for diffuse source and main stream for point sources and upstream load and are subjected to the respective retention of the subbasin retention in this paper always means the fraction of nutrient lost between the source and the outlet of the subbasin we use retention from a steady state corresponding to average nutrient load for a selected period representing current conditions based on the defined concepts we calculate the net load leaving the outlet of an area as sum of 1 the gross load from diffuse sources reduced with the total retention of the subbasin 2 the gross load from point sources reduced by the retention in the main stream 3 the sum of the net load from all upstream areas flowing into this subbasin reduced by the retention in the main stream in this subbasin mathematically this can be expressed as n s u b i gdif f s u b i 1 retto t s u b i diffuse sources gpoin t subi 1 retmai n s u b i point sources 1 upbasins n s u b u j 1 retmai n s u b i bi f s u b u j i upstream subbasins in this formula upbasins represent the number of upstream basins of subi using this equation the effect of one or more suggested mitigation actions in upstream areas can efficiently be propagated to the outlet of a downstream subbasin 2 3 technical architecture and implementation the tool has been implemented as a web based tool using client server architecture the web server consists of a map server and a database containing information about the hydrological network retentions and nutrient load for each subbasin in the area modelled currently two versions of the tool are available one for sweden http vattenweb smhi se scenario and one for europe http eutrophication eu in the current version of the tool for sweden the retentions gross loads and bifurcation data needed for the tool is imported into the tool from a historical run with s hype2012 4 0 0 for the years 2004 2015 this means that the retention used for the tool setup is calculated based on the long term mean of nutrient load for this period for europe a similar import has been done based on a run with e hype 3 1 1 to achieve a fast interactive tool all necessary computations are performed in the web browser on the client side and implemented using html5 and java script i e similar technology as described in walker and chapra 2014 this results in web based tools where it is easy for the user to move the focus area explore the different sources of load by altering visualisation options and hovering over the graph and finally to assess changes and explore the result at the focus area the interface for the european version of the tool is shown in fig 2 results can also be shared by sharing urls to a special scenario in the tools for the european version of the tools it is also possible to export and import scenarios as excel files 2 4 evaluation of the computational method the computational method used in the scenario tool was evaluated for different aspects of its construction and use the assumption of retention in relative terms as independent of nutrient load and speciation the simplification of using long term mean of nutrient loads and the estimation of realistic mitigation measures to investigate the first point we compared the retentions in two versions of the s hype model with different load the versions were the normal model set up with present condition of nutrient load for sweden and a background model where all anthropogenic sources of nutrient had been removed the computing of background load is based on previously used methods brandt et al 2009 including replacing arable land by pasture and remove point sources as most interesting mitigation actions would aim at reducing anthropogenic sources towards the level of the background model these two model runs would cover the variations in load of realistic mitigation scenarios very well although the total load range is covered variation in nutrient speciation may also affect the retention the retention s dependence on load does not need to be linear and limited by the found range in general there are small differences in relative retention between the two models fig 3 shows the variation in total retention for sweden this implies that the simulated retention in amount is rather linear in respect to the incoming load and that we could expect a good performance of the simplified calculations made by the tool for most areas in sweden there are some exceptions mainly in the south and around the big lakes with larger variation in retention and here we could expect a worse performance of the scenario tool 1 this map includes the glomma area which is part of norway the second point was investigated for selected catchments focus areas by comparing the performance of the simplified computations in the tool with result from simulations of the full s hype with the same changes in nutrient load applied catchments in southern sweden were selected as this part of sweden showed the largest variations in retention the locations of the catchments are shown in fig 4 together with a minimap of each area explaining where we changed the nutrient load i e the application area ronnebyån was selected as an example of a small river and ringsjön as an example of a small lake motala and hjälmaren were selected to test the tool in two of the larger lakes of sweden finally karlstad was selected to test the performance at a bifurcation for both the scenario tool and s hype the load of an upstream point the application area fig 4 was varied by removing 50 of the net load to adding 50 of the net load this range was based on findings in the brandt et al 2009 report that states that the anthropogenic load for nitrogen is 50 of the total load for sweden for our application areas the anthropogenic load varies between 14 and 75 for nitrogen and phosphorus table 1 this corresponds to removing all anthropogenic load based on the sources in the s hype model including atmospheric deposition the scenarios range should thus well cover all realistic scenarios of changes in nutrient load only the load from the application area is reduced increased in the scenarios while additional loads between the application area and the focus area are not changed the usefulness of the tool was tested by reproducing a set of real life scenarios previously defined and applied in arheimer et al 2015 c f section 3 2 the study area is söderköpingsån catchment in sweden a small catchment 882 km2 with 8 lake area draining to a bay of the baltic sea in that study a number of plausible nutrient mitigation scenarios from different societal sectors waste water treatment rural households wetlands and agriculture were defined by the local authorities based on wfd targets the different scenarios were simulated using a full scale hype model to estimate the effect of the actions this hype application was specially adapted to söderköpingsån catchment but based on input data from an earlier s hype version in this study these scenarios were reproduced with the scenario tool where additional information was needed to estimate input to scenarios we used the two publicly available water related sites vattenweb smhi 2017 and viss swedish county administrative board 2016 this approach evaluates whether available information is sufficient as input to the tool and gives a comparison between the results in the tool and the full scale simulations with hype 3 results we start this section by presenting the functionality of the developed tool before discussing the results of computations the tool gives a possibility to easy explore nutrient loads for mitigation scenarios the interface is shown in fig 2 as a first step the user chooses the subbasin for which outlet point results are wanted by clicking on the map the chosen area is marked with a blue flag and a graph with red circles shows the net load from each subbasin upstream of the focus area and the net load at the outlet of the area is shown in the lower right pane the pane shows the total net load and the source apportionment of the net load for the area to get more information the user can manipulate the graph by using the lower left pane for example only showing a specific source of interest for instance agriculture or industry the user can alter the graph to show the contribution from each upstream subbasin to the load at the focus area further information about the areas can be gained by hovering over the circles in the graph if desired the user can also easily move the flag around to investigate other areas or subareas that might be of interest this allows for discussions around the current situation to investigate the effect of one or several plausible mitigation actions or other planned change the user simply clicks on the corresponding circle and adds the deduction or addition of nutrient load in the pop up window the effect of the actions at the focus area will immediately show up in the lower right pane the user can make changes in as many of the upstream subbasins as desired to analyse the combined effects the results presented below are based on the swedish version of the tool analys och scenarioverktyg för övergödning i sötvatten the retentions gross loads and bifurcation data needed for the tool is based on historical run with s hype2012 2 0 for the years 1999 2011 3 1 validity of the computations as explained above the tool does not take into account that retention varies with nutrient load and speciation and with time to understand how this would affect the reliability of the tool s result we compared the results of the tool with full model results from s hype based on realistic assumptions on changes in nutrient load as a first step we compared the retention of two versions of s hype with different nutrient loads fig 3 table 1 this comparison shows that there in general is very small difference in relative retention for most areas that would affect the validation however in the selection of areas fig 4 for validation we included some areas with larger variation in retention where we could expect a worse performance of the scenario tool to investigate the impact of variations in retention and to evaluate the performance of the scenario tool we performed several runs of the s hype model for each selected catchment in the runs we modified the net load corresponding to scenarios where the net load from the selected areas were changed in steps of 25 ranging from 50 to adding 50 of the load the results are shown in table 2 nitrogen and 3 phosphorus for the ringsjön focus area the s hype version used was s hype2012 4 0 0 the net load at the focus area without any modifications differed a little between the tool and the s hype simulation for this area 0 2 the s hype net loads were therefore adjusted before the difference for levels of change in scenario between the tool and the s hype scenarios was calculated as shown in the tables there is almost no difference between the simplified tool and a full s hype run for several of the catchments the exceptions are the lakes hjälmaren and ringsjön where we see a rather large discrepancy between the tool and a full s hype run for phosphorus this was not unexpected since these lakes are among the locations were the retention varied most between the normal s hype and the corresponding background model fig 3 to further investigate the variation we decided to make a more detailed study of the two locations with the largest discrepancies for these points we varied the change in load from 20 to 20 investigating the difference between the scenario tool and s hype for smaller changes table 4 this second evaluation shows that for changes up to 10 of the original net load the scenario tool gives results well in accordance to the full scale s hype model for changes up to 20 the differences are small enough to give a useful first estimation of the change of load at the focus area 3 2 a case study on real life scenarios to test the usefulness of the nutrient scenario tool we applied it to a set of real life scenarios in söderköpingsån catchment fig 5 previously simulated by arheimer et al 2015 the scenarios were grouped into the societal sectors they concern the possible reduction of nutrient load for a societal sector is dependent on the sector s share of the total load for sectors where this is relevant we show this share for the scenario tool and for the hype model used in arheimer et al 2015 study table 5 the difference in source apportionment between the two models is due to an update of nutrient sources between the s hype versions it can be noted that in the scenario tool this information is immediately available when selecting an area and gives important information on how much different sectors contributes to the nutrient load of the area for the societal sector of waste water treatment plants wwtp we evaluated two different scenarios related to improved waste water treatment and a third where the plan was to remove two of the wwtps and instead lead the water to a wwtp in a different subbasin for the first two we have assumed the same proportion of nutrient reductions as in the original study to get a realistic comparison for the third we removed the load from the two waste water treatment plants according to the load in vattenweb the effect of these scenarios resulted in up to 6 reduction of nitrogen and up to 1 5 of the phosphorus load at the focus area table 5 the results of using the scenarios in the tool are very similar to the results in the original experiment the small differences are proportional to the differences in the share of load from wwtp in the two versions of s hype used for the experiments except for nitrogen in scenario 2 the societal sector concerning rural households contains two scenarios one connecting 200 households to a nearby wwtp and a second connecting all rural households the first of these scenarios gives a very limited effect at the focus area both nitrogen and phosphorus is reduced 0 1 0 2 in the second case almost all phosphorus load from rural households is removed and a significant part of the nitrogen load the difference between the tool and the full hype simulation is that arheimer et al 2015 assumed the water treatment was more efficient for phosphorus than for nitrogen while we assumed similarly high efficiency of water treatment for both nitrogen and phosphorus the scenarios in the following sector investigate the effect of constructing wetlands for reducing the amount of nutrients reaching the rivers the original study makes use of hypes wetland routines for calculating the effect of constructing wetlands in our evaluation we decided to instead use the information from viss swedish county administrative board 2016 stating the plausible effect on nutrients load by constructing wetlands viss contains a library of various mitigation measures including several kinds of wetlands where the reduction of nitrogen varies from 200 kg to 675 kg nitrogen per ha and year and the reduction of phosphorus varies from 5 to 65 kg per ha and year we used these ranges for our scenarios for nitrogen the original arheimer et al 2015 wetland experiments had lower reduction than the tool calculated for phosphorus our results gives a large range dependent of the different types of wetlands with different reductions available in viss the reduction from the original experiment is in the middle of this range the reduction of nutrient load from a wetland is highly dependent on the local conditions and actual locations of the wetland arheimer and wittgren 1994 2002 here the setup used by the scenario tool assumes an optimal placement of the wetlands in terms of concentrations of nutrient in water flowing into the wetland and that the full effect of the reduction reaches the outlet of the subbasin the original hype simulation is based on intermediate concentration and placement of the wetland within the subbasin for agriculture we investigated the effect of protection zones and a general reduction of load from agriculture for scenario 8 the scenario tool used information from viss on a reduction of 0 1 kg phosphorus per ha to estimate the effect of protection zones the general reduction scenario is similar to the scenarios in section 3 1 but here the load is reduced for all upstream subbasins with agricultural load also in these cases the reduction in our experiment is very similar to what was calculated by the original simulations to summarize in this evaluation based on realistic mitigation scenarios differences between the tool and a full scale hype simulation are small enough to give useful estimates for selection of scenarios in most case differences based on assumption of the estimated effect of a mitigation action are larger than differences in the computed effect by the different methods moreover the interactive behaviour of the tool gives a clear benefit in realistic applications as it offers a possibility to easily run a large number of different scenarios with different assumptions 4 discussion the nutrient analysis and scenario tool has achieved the main goal for the development it is an easy to use tool that gives an overview over how different nutrient sources in an area contribute to the nutrient load in addition it allows a fast comparison of different scenarios for reducing the nutrient load in the area the presented evaluations show that the simplified computations give comparable result to the full model simulations for most realistic scenarios of nutrient mitigation as such the tool gives a good first overview over plausible nutrient reduction scenarios and their effect however the tool makes only steady state calculations and in many cases this first overview should be followed by more detailed studies with the full model or other method over some selected scenarios this is particularly important when the area has high retention if the expected change is large or if the investments for achieving the effect are high if the mitigation actions are to be introduced step by step or if its effect is building up gradually that could also warrant a more sophisticated method than the scenario tool s static calculations another factor to consider in a detailed study is additional on going changes e g in land use or climate as already noted the tool makes steady state calculations and does not estimate the time for this steady state to be reached in addition the tool does not consider the retention processes dependence on changes in other nutrients because its use of pre calculated constant relative retention conley et al 2009 regarding the severity of eutrophication other factors beside nutrient load affect e g algal blooms and anoxia for instance the limiting nutrient and type affects algae community changing hydrodynamic conditions affect stratification and turnover time light conditions depend on turbidity which can be caused by fish sediment resuspension heisler et al 2008 schwefel et al 2016 bergman et al 1999 fully investigating and understanding the eutrophication problem of an area need to encompass more than the information given by the tool the algorithm in the tool is relatively simple and easy to implement however the method requires input data consisting of subbasin division routing retention and information about nutrient load from each source for each subbasin in this paper we have used data from s hype arheimer et al 2011 strömqvist et al 2012 and e hype donnelly et al 2013 2016 to implement two versions of the tool however in principle any database or hydrological model providing the required background data for the tool can be used to implement the tool the quality of the model providing the input data sets a limitation for the performance of the scenario tool the swedish version of the tool has a higher resolution than the european version as s hype is based on local knowledge of sweden compared to e hype which is mainly based on open data for europe arheimer et al 2012b this means that for especially the european case local conditions might not be captured by the general input about nutrient sources derived from the open data sets the hydrological model is in itself a simplification in hype land use soil type combinations are treated as computational units the water and nutrients leaving such a computational unit include overland flow soil runoff including outflow from shallow groundwater it does not include flow from aquifers as they are calculated separate from soil profile hype and the tool assumes all land use sources enter the local stream at the top of the stream and passes through both local stream and main stream and possibly lakes thus flow in ditches and groundwater flow that actually enters along the stream are treated with the full effect of the simulated river processes like all diffuse sources the calculated result of the model strömquist et al 2012 donnelly et al 2013 shows that this has been a good enough approximation for simulating river nutrient concentrations for the present conditions our validation shows that the estimated reduction in nutrient load at different scenarios has a high agreement to the original model when changes are limited in most cases realistic scenarios are well within these limitations of the tool it should be noted that the full models used in this study are calibrated and validated for present conditions and that it is a topic for future research how well they also capture changed conditions such as a large change in nutrient load arheimer et al 2012a 2015 jomaa et al 2016 or climate capell and olsson 2013 many of the larger differences in retention between the original model and the tool and between the background simulation and the present were found for areas with lakes there the original model may have included simulation of internal loading of phosphorus due to previous high loads this is highly affecting the modelled lakes retention it may even be zero a large reduction in load could in reality create a new internal source when old stocks are starting to be released this scenario is not handled in the present version of the tool because it uses a static retention in addition this is a case where the approximation of independent nutrient may be wrong and possibly affecting the results the user needs to be aware of the limitations of the scenario tool and the underlying model to properly judge when a scenario result is credible the method described in this article concentrates on the calculation and comparison of the downstream effect of performed measures it does not contain information about different measures and their plausible effects this focus of calculations was decided together with swedish end users as they did not want any specific assumptions to be built into the system in addition we saw that the already available viss database contains a lot of information that can be used together with the scenario tool as shown by our evaluation the scenario tool and the viss database complement each other the strength of our tool is the ability to easily compare different measures and their downstream effect together with information on measures from viss or as local knowledge the tool can be used to give a fast estimation on combination of measures or alternative scenarios the lack of coupling between possible measures and their effect and scenarios performed by the tool gives a certain freedom to use the tool practical or realistic limitations of the effect of different measures may be partly obtained from the information of the tool e g size of point source however the user needs to have some understanding of catchment nutrient management to know if a scenario is realistic and if the results are credible for a river with most of its load coming from point sources a large reduction of the load may be reasonable while for a river with most of its load coming from diffuse sources e g a large groundwater store a large reduction of load may be an unrealistic scenario for other measures e g changing manure practise consideration needs to be given to the effect of groundwater retention on the measure before deciding on what the effect of the measure is on the load to surface water we see several possibilities for further development of the tool an important improvement is to increase the information about what sources of nutrient load e g industries population and treatment plants are included in the calculations made by the tool this will increase the understanding of what assumptions the computations are made from and give the end user a possibility to compare the results with local information about an area it is also possible to increase the accuracy of the tool in areas were the retention varies with nutrient loads see fig 3 by making the retention a function of the load this function can be derived from a series of computations with s hype this improvement could for instance give better estimations for lakes with high internal load finally an improvement of the functionality is the possibility to download scenarios as excel sheets providing a more efficient way of experimenting with and document multiple scenarios a first version of this is available in the european version of the tool 5 conclusions in this paper we define a method enabling a web based scenario tool that provides fast and accessible information for an overview of the present situation and plausible scenarios the tool is implemented and openly available for sweden and europe based on data from simulations made by s hype and e hype it enables the user to explore a large number of alternative scenarios to compare the effect of mitigation actions for improved water status our evaluation shows that the results delivered by the tool are in most cases comparable to more expensive full scale simulation and can be used to estimate effect of mitigation actions for realistic scenarios for larger changes especially involving lakes with high retention or high internal load it is possible to improve the estimations made by the tool by basing it on runs with different load however for these cases we recommend to complement the results from the tools with full scale simulations software availability the described tools are available on http vattenweb smhi se scenario and http eutrophication eu the algorithms and interface are implemented in javascript and can be viewed via the applications for the server side code please contact lena stromback smhi se acknowledgements the author contributions all authors have contributed to the design of the method and the tools l s and c p wrote the paper and performed the evaluations j g made the implementation the first swedish version of the tool was developed as part of the work for the swedish agency for marine and water management the project switch on eu fp7 grant agreement no 6035 funded the development of the european version of the tool and the swedish research council project emap 2011 624 has funded research and evaluations needed for completing the paper the authors thank all colleagues that have been involved and contributed to the result this paper by feedback and input in particular we are grateful to henrik spångmyr lotta andersson berit arheimer and niclas hjerdt 
26292,technology and policy implications of global energy and emissions scenarios can be difficult to analyze because underlying assumptions and drivers of scenarios are rarely made explicit this article documents methods for standardizing emissions scenario results that can be applied to virtually any scenario enabling more meaningful comparisons among scenarios than has been possible in the past this approach uses charts showing the dynamics and effects of emission drivers mitigation technologies and policies applying these methods will enable the policy and research communities to better understand the implications of scenarios and help analysts learn more rapidly as a matter of good practice modelers should create decompositions like the ones put forth in this article policy makers should request them and funders of scenario analysis and sponsors of model comparisons should support the application and further development of such tools keywords greenhouse gas emissions scenarios integrated assessment models climate change decomposition methods 1 introduction after the cop23 meetings in bonn in november 2017 the policy community is increasing its focus on reducing greenhouse gas ghg emissions quickly and efficiently to understand the range of possible solutions scientists have for many years used integrated assessment models which attempt to assess the costs and benefits of climate action many researchers have raised analytical concerns about these models and associated methods ackerman et al 2009 decanio 2003 koomey 2012 2013 pindyck 2013 2017 rosen and guenther 2015 but they provide a systematic way to track the implications of possible scenarios and we ll no doubt need accounting tools like them to explore future scenarios that are potentially promising energy scenarios explore a range of conditions possible in an inherently uncertain future including the responses to policy interventions that could stabilize atmospheric greenhouse gas concentrations riahi et al 2017 because the future is deeply uncertain it is appropriate to analyze any single energy and emissions scenario in the context of other cases such as a family of scenarios generated by a single model wbgu 2004 a set of models generating a common scenario weyant 2004 or a diverse selection of scenarios in the literature hamrin et al 2007 to compare the insights within and between these types of studies an effective and consistent framework for interpretation is essential hummel 2006 unfortunately different practices for reporting scenarios in the literature make it difficult to compare results and infer their meaning policy analysts who are often a primary audience for scenario results then face the challenge of interpreting and evaluating a steady stream of studies based on different models and baseline assumptions the importance of addressing this and related challenges has led to calls for greater transparency disclosure and self examination by the energy modeling community for example koomey et al 2003 and pfenninger 2017 this article describes one technique with minimal data requirements that can be used to illuminate technology and policy implications of global regional or country level energy and emissions scenarios for a policy audience this technique uses driver dashboards of line graphs that make explicit the relative role of key drivers of emissions over time in a particular scenario after reviewing these dashboards a researcher can put the quantitative results of each scenario into context and compare the results to historical trends and to other scenarios presented in the same framework the factors in the first dashboard include population growth economic activity final and primary energy consumption and total fossil carbon dioxide emissions from the energy sector the second dashboard shows ratios of these terms the third dashboard displays total projected energy sector emissions as well as emissions and reductions from biomass sequestration land use industrial non energy co2 emissions and other warming agents supplemental graphs yield additional insight to describe and demonstrate the utility of this interpretive technique this article first reviews a commonly used method for scenario decomposition the kaya identity for the energy sector and then describes why an expanded version of this method is required section 3 describes in more detail methods for creating such an expanded decomposition next the article expands the analysis further to include sources of emissions reductions outside the energy sector it then describes use cases for such decompositions focuses on limitations of the analysis suggests areas of future research and summarizes the conclusions that emerge from this analysis 2 building on a familiar concept the kaya identity the decomposition presented here is adapted and expanded from a well established convention in emissions scenario analysis known as the kaya identity kaya 1989 which is commonly used to decompose drivers of greenhouse gas emissions in the energy sector as many researchers have realized over the years the kaya identity as it was originally introduced is incomplete this section reviews the original kaya identity and how it has been used and altered in the past then modifies and expands the decomposition to correct for the issues in earlier formulations 2 1 overview of the kaya identity the kaya identity illustrates the key drivers for fossil carbon dioxide emissions from the energy sector this identity decomposes carbon emissions as a product of aggregate wealth energy intensity of economic activity and carbon intensity of the energy supplied as originally presented kaya 1989 it reads as shown in equation 1 1 carbon emissions e gnp c e gnp where gnp gross national product a measure of economic activity e gnp primary energy intensity per dollar of gnp and c e carbon intensity of primary energy production professor kaya presented this equation to help understand the implications of history and future scenarios in a simple back of the envelope way in most uses of this equation the order of the terms is switched so that aggregate wealth is first energy intensity is next and carbon dioxide intensity rather than carbon intensity of primary energy supplied is third ipcc 2014 p 368 most analysts split the aggregate wealth term into terms focused on population and economic activity per person which leads to the more familiar four factor kaya identity in equation 2 note different variable abbreviations than in equation 1 2 carbon dioxide emissions p gwp p pe gwp c pe where p is population gwp is gross world product a measure of economic activity pe is primary energy including conversion and energy transmission losses c is total net carbon dioxide emitted from the primary energy resource mix gwp p is the average income per person pe gwp is the primary energy intensity of the economy and c pe is the net carbon dioxide intensity of supplying primary energy in its substance and structure the kaya identity reflects a more general identity that expresses impact i as a product of human population p affluence a and technology t ehrlich and holdren 1971 1972 population is the same in both the kaya and ipat identities gwp person represents affluence and the other two terms characterize technology this formulation implies that a larger number of people with a higher income and more extensive use of certain technologies will have a greater impact on the environment the role of technology can be ambiguous technologies that produce and combust fossil fuels are the primary anthropogenic source of carbon dioxide while technologies for harnessing renewable energy and nuclear power sequestering carbon and improving efficiency can reduce net anthropogenic carbon emissions 2 2 previous uses of the kaya identity in scenario analysis the kaya identity has been widely used with some prominent examples being nakicenovic et al 2000 kawase et al 2006 raupach et al 2007 and hoffert et al 1998 there are so many examples in fact that we don t attempt a comprehensive review here what is interesting is that there s been only modest progress in the use of this identity since 1989 beyond the initial expansion of the identity to split population from gwp per person some researchers have expanded the identity to account for other weaknesses as we discuss below although those efforts always stopped short of what we term our expanded kaya identity for the energy sector presented below to explore the intellectual history of this concept appendix a examines how five generations of reports for the intergovernmental panel on climate change ipcc and related reports have treated the kaya identity the ipcc reports represent the state of scientific understanding about climate change at any time so they are a good marker for evolution in the understanding and use of this concept in analyzing historical data or in evaluating future scenarios in no case did the ipcc reports expand the kaya identity beyond its slightly expanded four factor form as shown in equation 2 above but the assessment reports became more sophisticated over time in how they used the concept and presented the analysis results other researchers have expanded the identity but progress on this front has been piecemeal and halting appendix a also presents the key articles that made progress in expanding the identity over time 2 3 reasons to expand the decomposition 2 3 1 disentangling energy intensity and supply chain losses the most widely used metric for energy intensity of economic activity pe gwp refers to primary energy pe which is the total energy input to the economy from all sources measured as the energy potential in fossil fuels and biomass at the point of extraction grübler et al 2015 the pe gwp metric is sensitive to four types of changes in an energy economy system each of which is affected by specific dynamics outlined with examples in table 1 although the definition of energy intensity as a ratio of primary energy to economic activity pe gwp is widely used in the literature there are long standing arguments for separating final energy to better assess trends in end use demands and to isolate the first effect energy supply losses in table 1 schipper et al 1992 final energy is the energy that is actually delivered to the customer s meter or gas tank and it can include electricity gasoline hydrogen or direct uses of natural gas coal and biomass grübler et al 2015 the special report on emissions scenarios sres reports final energy fe in its detailed appendices allowing a more accurate assessment of the energy intensity of the economy nakicenovic et al 2000 and a few analysts have disaggregated fe gwp and pe fe in previous scenario decomposition studies grübler et al 1993 kawase et al 2006 price et al 2006 following those authors the pe gwp metric can be further disaggregated as shown in equation 3 3 pe gwp fe gwp pe fe where fe gwp is the final energy intensity of economic activity and pe fe is a measure of total energy system supply losses for delivering final energy to users the ratio of primary energy to final energy delivered at an economy wide scale indicates the portion of potential energy lost in the supply chain a value of 1 0 indicates zero conversion losses in delivering final energy to users so this ratio will be greater than 1 0 for all real energy systems in the context of a single technology type e g an electric power plant the ratio of final energy to primary energy fe pe is a metric that captures both the efficiency of conversion and efficiency of energy transport transmission at an aggregate data level however it is not accurate to represent the ratio of primary energy to final energy pe fe as the inverse of the conversion efficiency as is done for example in kawase et al 2006 table 1 lists three different dynamics in an energy system that can affect supply losses and a change in technical efficiency is only one energy supply losses do occur as a result of some inefficiency along the way but the value of the metric itself varies even if conversion efficiencies in the system are held constant changes in the pe fe ratio can result from changes in the balance between fuels supplying a single type of final energy e g natural gas vs coal for electricity generation or the balance between final energy types e g supplying water heating using electricity or natural gas for the sake of precision and clarity this article departs from the system efficiency terminology used by some other authors in favor of the more precise term energy supply loss factor eslf 2 3 2 converting non combustion energy production to primary energy one of the key issues in understanding energy systems is assessing the total energy consumed by the system including all the losses in making energy available to consumers this assessment is complicated because of variations in how different energy sources produce fuels or electricity that allow us to do useful work primary energy is the energy contained in fossil and biomass fuels measured as for example the heat content of coal that goes into a power plant s boiler grübler et al 2015 the difference between primary energy and secondary energy is the conversion loss in converting coal to electricity the secondary energy is the amount of electricity injected into the grid at the busbar measured in kwh which also called net generation after accounting for on site use of electricity to run the plant final energy is the electricity delivered to the customer s meter which is lower than that injected by the power plant into the grid because of transmission and distribution t d losses nuclear hydroelectric solar electric wind power and other non combustion sources of electricity or hydrogen or process heat created using these fuels do not have losses that result in additional emissions like fossil fuel generation does what should define the quantity of primary energy for these sources there is the energy embodied in the nuclear fuel and the solar flux hitting a photovoltaic panel but what does it mean to consume that energy from the perspective of the emissions calculated by the kaya identity to fully account for global energy use in emissions scenarios all non thermal sources of electricity generation hydrogen and process heat have traditionally been assigned a primary energy value based on some measure of the amount of fuel needed to generate equivalent amounts of secondary energy plus the associated transmission and distribution t d losses to transport the secondary energy to the customer s meter this approach assumes that the alternative to the non combustion energy is fossil fuel fired combustion generation for many years this method termed the substitution method was considered in the scenario analysis community to be the customary convention the standard prescription for efficiency of conversion of primary to final energy in electricity generation was a constant 38 6 grübler and nakicenovic 1996 nakicenovic et al 1998 p 90 this convention implies a final to primary energy factor of 9 33 mj kwh kwh measured at the customer s meter for direct heat treated with the substitution method a different efficiency of conversion may be used for example 85 as found in iiasa s global energy assessment iiasa 2012 p 1820 one could also imagine a dynamic substitution approach in which non combustion sources are assigned energy supply chain losses equal to those of the average losses in the combustion part of the energy system as they change over time whereas the original substitution method assumed constant losses over time this alternative method would assess losses as they evolve in the energy systems being modeled that means it would capture the shift from for example older inefficient plants to newer efficient ones this method of imputing average system losses to non combustion sources has some justification when there is a significant amount of final energy delivered by fuel based energy sources a situation that holds now and into the near future for many energy scenarios it also allows for accurate comparison of the contribution of both combustion and non combustion resources to the generation mix there are issues with the substitution approach however even if using the more accurate dynamic version imputing losses for non combustion resources in essence creates fictional primary energy losses that aren t evident in the actual energy supply system if non combustion resources displace combustion sources with real conversion losses those losses are eliminated and primary energy use should go down using the substitution approach masks that contribution in 1998 modelers participating in the landmark special report on emissions scenarios prepared for the intergovernmental panel on climate change adopted an alternative convention for non combustion electricity generation based on the heat content of the electricity power plants delivered to the busbar this convention equates primary energy of electricity generation to the secondary energy at the busbar using a conversion factor of 3 6 mj kwh it then subtracts t d losses to get to final energy the modelers adopted this method termed direct equivalence as their common convention in order to harmonize assumptions and facilitate the comparison of results sres designated nuclear power to be treated with the direct equivalent method along with solar power wind power hydropower geothermal power and other renewable sources of electricity and hydrogen nakicenovic et al 2000 1 1 it is important to clarify that engineering economic models used to produce global energy scenarios do consider the technical efficiency of the engineered systems that harness non thermal renewable resources and nuclear power indeed technical efficiency is a vital characteristic of cost and performance parameters for each technology type however the primary energy data calculated for each technology type by models using sres terms is reported in terms of direct equivalence as described above hundreds of mitigation scenarios based upon the reference scenarios developed for sres have inherited the direct equivalence assumption and aside from cautionary notes buried deep in the sres report itself on pages 216 and 221 and a sidebar treatment in nakicenovic et al 1998 p 90 it has seldom been mentioned in the literature if more direct equivalent sources enter the supply mix primary energy use will decline because conversion losses from combustion are eliminated the substitution approach would instead indicate that total primary energy and losses in the system would change more modestly as a function of the efficiency of combustion plants remaining in the system after existing plants are displaced a counterintuitive result primary energy calculated using the direct equivalent approach correctly characterizes energy system losses over time in the form of the energy supply loss factor scenario modelers who rely heavily on the four factor kaya identity have often compared historical changes in the ratio of primary energy to gwp to the results of model projections failing to distinguish quantitatively between changes attributable to the shift to non combustion direct equivalent resources and those due to changes in final energy intensity one example is loftus et al 2015 which relies on aggregate trends in primary energy to gwp for its otherwise rigorous scenario comparisons another is peters et al 2017 which notes the possibility of splitting out the effects of these two factors in their methods discussion but still shows a graph of historical and projected primary energy use over time their fig 3 even if the scenario modelers understand this distinction our experience is that policy makers can be easily misled by this way of presenting the data thinking that scenarios with large reductions in the ratio of pe to gwp demonstrate significant end use efficiency when in many scenarios significant savings come from increasing penetration of non combustion resources this conceptual confusion is avoided by splitting those two key drivers in our expanded kaya identity and we strongly caution against relying on the ratio of pe to gwp in almost all cases it is also important to note that two of the most important energy data agencies the us energy information administration eia and the international energy agency iea have adopted conventions about primary energy that can lead to confusion eia uses the dynamic substitution method for all non combustion resources with non biomass renewables assigned the annual average conversion efficiency of fossil fuel plants and nuclear power assigned the annual average thermal efficiency of nuclear plants electricity imports are treated using direct equivalence 2 2 https www eia gov tools glossary index php id p iea treats renewables like solar wind and hydro using direct equivalence geothermal energy as a thermal power plant with 10 efficiency and nuclear power with the thermal efficiency of 33 3 3 https www iea org statistics resources balancedefinitions these choices should be reconsidered in light of the wide acceptance of the direct equivalent method in the scenario modeling community and the need to avoid inconsistencies in how primary energy conversions are treated more research is clearly needed on methods for assessing trends in primary energy as nakicenovic et al 1998 p 90 point out the very concept of primary energy becomes increasingly problematic particularly as renewable energy forms gain importance appendix b delves more into the implications of correctly accounting for the convention of direct equivalence 2 3 3 disentangling decarbonization and fossil sequestration historical trends in carbon intensity expressed as a ratio of net carbon emissions c in equation 2 to primary energy pe are unaffected by carbon sequestration because that technology has not yet entered the energy system at any detectable scale however making the distinction between the effects of decarbonization and sequestration is important because many energy scenarios for the 21st century that stabilize global warming at lower levels e g 450 ppm co2 and below imply large scale deployment of carbon sequestration technology koelbl et al 2014 the recent low energy scenario in grübler et al 2018 avoids all use of sequestration but that work is the exception when carbon sequestration cs is present in the system the ratio of net fossil carbon emissions nfc to primary energy pe will produce misleading indications about the carbon intensity of primary energy supply net fossil carbon emissions from the energy sector are less than the total fossil carbon tfc content of primary energy supply by an amount equal to the fossil fuel carbon sequestered csff rather than released to the atmosphere as shown in equation 4 4 nfc tfc cs ff as a result increases in carbon sequestration artificially depress the indicator for carbon intensity in the four factor kaya identity c pe conflating two distinct effects decarbonization a decline in the carbon content of primary energy sources tfc pe and fossil sequestration csff we can express the effect of sequestration as the ratio of net emissions released to the atmosphere after accounting for sequestration and the total fossil carbon dioxide in the primary energy system before sequestration nfc tfc hanaoka et al 2006 and kawase et al 2006 therefore proposed adding a term to the four factor kaya identity that accounts for the effect of sequestration as shown in equation 5 5 nfc pe tfc pe nfc tfc where tfc pe is the total fossil carbon intensity of primary energy nfc tfc is the fraction emitted to the atmosphere and tfc nfc tfc is the sequestration rate or fraction sequestered without this correction whenever the sequestration rate is rising the carbon content based calculation of carbon intensity c pe in equation 2 will overstate the actual decarbonization rate because sequestration masks the carbon content of fossil fuels remaining in the primary energy mix in other words if the c pe indicator is calculated using net emissions after sequestration the decomposition results will be misleading another complexity enters when we consider sequestering carbon from the flue gases of biomass combustion many recent scenarios rely on biomass sequestration to achieve negative emissions fuss et al 2014 kemper 2015 but the flow of carbon in this case is separate from those from fossil fuels biomass sequestration involves extracting carbon from the global biospheric carbon cycle and sequestering it we can think of biomass sequestration as altering the additive term for land use change included in equations 9 and 10 below for clarity we show biomass sequestration as a separate term that is subtracted from the other additive components in the fully expanded decomposition below 4 4 in the past biomass combustion was assumed to be carbon neutral but more recent analysis has shown that assumption to be incorrect for biofuels decicco john m danielle yuqiao liu joonghyeok heo rashmi krishnan angelika kurthen and louise wang 2016 carbon balance effects of u s biofuel production and use climatic change vol 138 no 3 2016 10 01 pp 667 680 https doi org 10 1007 s10584 016 1764 4 which points to the importance of careful accounting for carbon stocks and flows in any scenario where biomass or biofuels play a role 2 3 4 distinguishing fuel switching among fossil fuels from shifts away from fossil fuels an additional refinement of the expanded identity is described by peters et al 2017 in their methods section they advocate and implement further decomposition of the tfc pe term into two terms as shown in equation 6 6 tfc pe pe ff pe tfc pe ff where tfc pe is the total fossil carbon intensity of all primary energy pe ff pe is the share of primary energy from fossil fuels the fossil fuel fraction and tfc pe ff is total fossil carbon intensity of primary energy supplied by fossil fuels this additional detail allows decomposition of changes in tfc pe into the two key drivers of those changes switching from fossil fuels to non combustion alternatives e g from fossil fuels to zero emission energy resources and then fuel switching among fossil fuels e g from coal to natural gas or from high to low emitting oils as described in koomey et al 2016 a subtlety of this treatment of emissions intensity relates to the treatment of combustion of biomass and biofuels the message model following the ipcc reports co2 emissions from the combustion of biomass and biofuels in the agriculture forestry and other land use afolu sector ipcc 2006 indirect emissions of other greenhouse gases like methane are tallied separately and lumped into the other gases category in fig 6 below biomass and biofuels primary energy consumption is contained in our estimate of total primary energy consumption but net direct carbon dioxide emissions from these fuels are captured in the afolu sector along with any offsetting uptake of co2 in the biosphere this is why we call our term for carbon emissions above total fossil carbon and not total energy carbon biomass and biofuels are generally small compared to total primary energy but this accounting convention may become important in certain high biomass scenarios 3 decomposition of emissions in the energy sector improvements to the kaya identity described in the previous section lead to new insights about the way key drivers of carbon dioxide emissions from the energy sector are affected by policy intervention the decomposition applied in this paper incorporates each of these elements which can be summarized as in equation 7 7 c fossil fuels p gwp p fe gwp pe fe pe ff pe tfc pe ff nfc tfc where c fossil fuels represents carbon dioxide co2 emissions from fossil fuels combusted in the energy sector p is population gwp is gross world product measured consistently using purchasing power parity or market exchange rates fe is final energy pe is total primary energy calculated using the direct equivalent deq method as discussed above and in appendix b pe ff is primary energy associated with fossil fuels tfc is total fossil co2 emitted by the primary energy resource mix nfc is net fossil co2 emitted to the atmosphere after accounting for fossil sequestration we summarize the underlying factors e g population gwp final energy primary energy primary energy from fossil fuels and total fossil co2 in a dashboard as in fig 1 and the expanded kaya ratios that comprise c fossil fuels in fig 2 in the first row we show absolute values over time for each of the components in this case for historical data from 1900 to 2014 de stercke 2014 because sequestration is negligible during this period the graphs omit the final term in the expanded identity dotted lines project the paths each driver would have followed if historic rates of change had persisted to 2014 we calculate rates of change for the periods 1900 to 2014 and 1995 to 2014 the latter period showed greater change in the final energy intensity of economic activity than the longer period we also show those historical rates of change on the dashboards for future scenario projections to provide benchmarks against which future scenarios can be compared in figs 1 and 2 there are three different scales at which the dashboard drivers can be plotted absolute value indexed value and rate of change each of these provides different insights into the historical data or as we show later into scenario assumptions and results the first tier in the dashboard shows each key driver by its absolute value constructing a dashboard of key drivers in absolute value terms shows decomposition results in units with physical and economic meaning such as population gwp gwp capita or energy use gwp facilitating cross model comparisons the second tier shows the relative influence of each driver in the reference case and the relative influence of the policy intervention on each driver by plotting an index relative to some base year for figs 1 and 2 that base year is 1900 for graphs characterizing future scenarios below the reference year is 2010 the third and fourth tiers show the growth rate for each factor the instantaneous growth rate of total carbon dioxide emissions is equal to the sum of the growth rates for each key factor as shown in equation 8a for derivation see appendix i 8a d c dt c d p dt p d gwp p dt gwp p d fe gwp dt fe gwp d pe fe dt pe fe d pe ff pe dt pe ff pe d tfc pe ff dt tfc pe ff d nfc tfc dt nfc tfc since we calculate changes over discrete time periods this relationship holds as an approximation 8b δ c c δ p p δ gwp p gwp p δ fe gwp fe gwp δ pe fe pe fe δ pe ff pe pe ff pe δ tfc pe ff tfc pe ff δ nfc tfc nfc tfc for historical data we show the growth rate in the third tier with year by year changes and for the fourth tier we show it with data tallied as a running five year average the annual growth rate shows more variation in the historical data as we expect for future scenario projections we show data averaged over ten year periods because those are the data available in the scenario databases we use 5 5 one subtlety is that for future scenarios we calculate compound annual growth rates over ten year periods then apply those growth rates in each year of the relevant period this allows us to plot trends over the entire analysis period rather than dropping a decade in the beginning or at the end as we would have to do if we showed only one growth rate per decade if scenario output data are available for intervening years then this convention can easily be altered interestingly fig 1 shows that gwp growth is greater in the past two decades than from 1900 to 2014 but the annual rate of increase in final energy consumption is lower in the later period final energy intensity reductions have therefore accelerated in the past two decades we use this same format for scenario projection data to illustrate we show example dashboards for a scenario projection using the message 4 0 model based on results presented in the ampere modeling exercise see appendix h and riahi et al 2015 we chose message as the exemplar in this article because the message modeling team is widely known and respected in the modeling community and because message explicitly treats almost all warming agents in its projections thus allowing us to present a full decomposition to illustrate our calculations and visualizations fricko et al 2017 krey et al 2016 schrattenholzer 1981 fig 3 shows the underlying factors for the message scenario while fig 4 shows the kaya ratios just as for the historical data the figures show the reference scenario using a black solid line and the low emissions intervention scenario as a red line the dashed lines show trends in historical data for comparison as discussed above in the nfc absolute value pane we also show the total effect of biomass ccs plus fossil ccs as a green solid line even though negative emissions from biomass ccs are not counted in the energy sector its deployment is linked to the energy sector as well as to the carbon cycle associated with land use because this emissions reduction option is important in many low emissions scenarios kemper 2015 we add this line to show how it compares to the nfc in the intervention case we also show a line for biomass ccs in the dashboard of additive terms below fig 3 shows some key insights population gwp and carbon intensity of fossil energy are not much affected by the intervention scenario while final energy primary energy fossil fuel fraction and net fossil carbon are all significantly reduced in the intervention case growth rates for all factors in the future scenarios are lower than historical rates gwp growth rates decline over the analysis period but final energy growth slows almost to zero then starts rising again after 2050 causing the fe gwp ratio in fig 4 to break off from the 1995 to 2014 trend line and slow its long and rapid decline this curious behavior is ripe for further investigation and it illustrates the kind of questions that this method of presenting results enables see also appendix c the figures also show the importance of fossil ccs to reducing emissions in the intervention case the nfc pane in fig 3 clearly shows an emissions reduction compared to the tfc pane and the nfc tfc pane in fig 4 shows the importance of that reduction relative to tfc emissions the factors dashboard and the ratios dashboard are a starting point for further investigation now let s dig into additional ways to garner insights from this analysis and presentation approach 4 another window into the expanded kaya results another way to visualize changes in emissions over time is with a bar chart this type of graph shows the compound annual growth rate for each term in the expanded kaya identity over some time period based on equation 8b as shown in fig 5 it can be plotted for the reference case an intervention case or the difference between them giving a quantitative indication of the contributors to emissions growth or reductions in this case over the 2010 to 2100 time period fig 5 shows results for our exemplar message scenarios changes in gwp per capita and final energy per unit of gwp are the two most important drivers of emissions trends in the reference case while reduction of fossil fuel fraction decarbonization of the primary energy supply using non fossil options fossil sequestration and the final energy intensity of economic activity are the three most important components of emission reductions in the intervention case 5 additive elements for non energy emissions the expanded kaya identity in this article addresses direct carbon dioxide emissions from the energy sector now we turn to other sources of greenhouse gas emissions including co2 emissions from industrial processes like cement production co2 emissions from land use changes such as deforestation and non co2 greenhouse gases which include methane nitrous oxide and a set of powerful greenhouse gases containing fluoride manufactured for human use 6 6 the so called f gases include hydrofluorocarbons hfcs perfluorocarbons pfcs and sulfur hexafluoride sf6 all of which are released in extremely small quantities compared to carbon dioxide but the warming potential of each molecule can be as much as five orders of magnitude greater these factors are included in the third dashboard as additional sources of emissions with the non co2 gases being converted to carbon dioxide equivalent total greenhouse gas equivalent emissions c total eq can be expressed as in equation 9 9 c total eq c fossil fuels c industry c land use c non co 2 gases eq cs biomass where c industry represents carbon dioxide emissions from industrial processes non energy uses of fossil fuels that result in emissions such as cement and aluminum production some models combine these emissions with fossil fuel combustion emissions but they should be split out for clarity c land use represents net carbon dioxide emissions from changes in agriculture and land use that are not associated with emissions reductions from biomass ccs this term can also be negative if there is significant reforestation c non co 2 gases eq represents emissions of other greenhouse gases converted to co2 equivalent using relative factors of global warming potential 7 7 we convert emissions of the two major non co2 greenhouse gases methane and nitrous oxides to co2 equivalents using 100 year global warming potentials including climate feedbacks from ipcc 2013 climate change 2013 the physical science basis contribution of working group i to the fifth assessment report of the intergovernmental panel on climate change stocker t f d qin g k plattner m tignor s k allen j boschung a nauels y xia v bex and p m midgley eds cambridge united kingdom and new york ny usa cambridge university press http www climatechange2013 org chapter 8 table 8 7 page 714 https www ipcc ch pdf assessment report ar5 wg1 wg1ar5 chapter08 final pdf for the message analysis we use the model s calculation of the gwp of f gases c s b i o m a s s represents net negative emissions from sequestering carbon emissions associated with biomass combustion in effect such sequestration removes carbon from the biosphere the emissions reductions from this source must be carefully distinguished from those land use changes the guiding principle should be that all energy system and related emissions should be included in one of the additive terms in equation 9 for example indirect emissions from hydroelectric facilities would include co2 from fossil energy used in constructing the plant in the fossil fuels category co2 from cement used in construction in the industry category and methane emissions from decomposition of biomass on flooded land in the non co2 gases category for nuclear power indirect emissions from fossil powered enrichment would fall into the fossil fuels category while co2 from cement used in construction would fall into the industry category substituting c fossil fuels with the expanded kaya identity components from equation 7 we get equation 10 which is what we call our fully expanded decomposition 10 c total eq p gwp p fe gwp pe fe pe ff pe tfc pe ff nfc tfc c industry c land use c non co 2 gases eq cs biomass standard scenario outputs for models with comprehensive coverage of emissions will usually include the data for each additive term in some cases additional calculations or data outputs will be required the fully expanded decomposition of key drivers in fig 6 has all additive elements net fossil energy co2 emissions biomass sequestration land use change industrial non energy co2 emissions and non co2 gas emissions if direct air capture of co2 becomes important in future another pane with those emission reductions can be included in the additive dashboard energy sector emissions reductions still dominate all reductions but each of the additive components has a measurable effect on total greenhouse gas emissions appendix j gives details on the projection of cement emissions which are combined with fossil energy emissions in the message model outputs we derive them instead from van ruijven et al 2016 and subtract from the total of fossil energy and industrial emissions from the message outputs to get fossil fuel energy emissions it is critical to separate biomass ccs from fossil ccs in such attribution graphs to get a clear picture of carbon flows as discussed in section 2 above biomass ccs extracts carbon from the atmosphere and sequesters it while fossil ccs sequesters emissions that otherwise would have reached the atmosphere from the combustion of fossil fuels the importance of ccs is highlighted in fig 7 which gives the full picture of ccs compared to tfc and nfc it includes the energy sector plus biomass ccs but not the other additive terms in fig 6 the reference case tfc is in black and the intervention case tfc is in red nfc which shows the effect of fossil ccs on tfc is represented by the blue solid line and nfc minus biomass ccs is shown by the solid green line the emissions reductions associated with fossil and biomass ccs are shown in the dotted blue and green lines respectively reductions in direct emissions from the energy sector are represented by the difference between the black and red lines in fig 7 then savings from ccs contribute further fossil ccs has a slightly larger cumulative impact than biomass ccs over the analysis period but both achieve reductions of gigatonne scale so are significant by any measure the decline in fossil ccs reductions is significant after 2070 a result that points to the need for further digging into these output results another way to present the full emissions reductions picture is shown in fig 8 this chart shows total ghg equivalent emissions in the reference and intervention cases then attributes savings to each component we ve identified in the energy sector ratios dashboard fig 4 and the additive emissions dashboard fig 6 the non energy emissions results in the additive dashboard come straight from model outputs in almost all cases to decompose the fossil energy related emissions which are part of a multiplicative identity we rely on the ldmi i technique first proposed by ang 2004 this method gives a perfect decomposition no residual term and is relatively simple to apply to multi factor problems in addition the iea uses the lmdi i method to decompose co2 emissions from electricity production iea 2015 creating a credible precedent for our choice population projections are the same in both the reference and intervention cases so no emissions savings accrue from this category in this scenario or in most other scenario exercises bongaarts and neill 2018 the majority of the decline in total fossil carbon is from substituting non combustion resources for fossil energy supply fuel switching among fossil fuels contributes only a nominal amount to emissions reductions in addition substitution on the supply side is the single largest contributor to emissions reductions for the whole scenario the contribution of fossil ccs declines in the latter part of the analysis period as we also saw in fig 7 graphs like fig 8 work well in plotting contributions to emissions reductions but when one or more terms in the identity result in an increase in emissions over time a stacked area graph can t show that easily for example the energy system loss factor eslf sometimes increases during part of the intervention scenario thus increasing emissions during that period in those cases we adjust for that increase in those years by allocating it in proportion to the emissions reductions from the other terms in the fully expanded decomposition in every year the sum of all net emissions reductions will then reflect the difference between the reference case black line and the intervention case emissions red line the sign of each term positive or negative can be most accurately represented in bar charts like those in fig 5 6 benefits and uses of decomposition methods the decomposition tools described in this article offer benefits over the ad hoc methods traditionally used in the scenario analysis community individual modeling groups often develop their own techniques but those cannot be easily applied for cross model comparisons and are normally not as comprehensive as those developed in this article for example most analysts still rely on the four factor kaya identity for analyzing scenario results which can lead to confusion as we describe above there are three main audiences for decomposition methods modelers policy analysts and research funders each of these groups can use decompositions for sanity checking promoting transparency and making valid comparisons 6 1 sanity checking during scenario creation for modelers and secondarily policy analysts and funders who review interim results these tools allow scrutiny to begin much earlier in the research cycle and encourage sanity checking of assumptions and results before publication for example scenarios often rely on heroic assumptions for costs and adoption of new exotic technologies like biomass ccs combined with modest projected changes in fe gwp or renewable energy adoption while not necessarily wrong such inconsistencies in relative technological progress should prompt further digging and analysis hummel 2006 use of these decomposition methods makes that analysis easier and quicker 6 2 promoting transparency for all users decomposition tools allow greater transparency understanding and documentation of methods and results than has heretofore been possible the assumptions and algorithms of these models can be opaque but decomposition methods give a view into the black box that will enable debate about key issues and uncertainties in mitigation scenarios even though the complex algorithms embedded in the models are not as easily subject to outside scrutiny 6 3 making valid comparisons quickly for all users decomposition tools allow better comparisons between history and projections baseline and policy cases and different baseline and policy cases from many modeling shops hummel 2006 for example a substantial divergence of a projection from historical trends or unintuitive discontinuities in the projection can reveal issues with data and methods or lead to new policy insights comparisons among different projections can be done at a glance using our decomposition dashboard and supporting graphs each scenario set can be summarized in a few pages and the results of multiple models can be quickly compared it is now almost never possible to do such comparisons easily using published modeling results but if modelers create decompositions using standard tools in a consistent framework it will become standard practice 7 limitations of the analysis and areas for further research the decomposition of key drivers involves examination of high level aggregate data the focus on global trends can obscure important changes at the regional and sectoral levels as well as technological trends an obvious next step is to apply the dashboard and associated tools to compare recent model runs completed for ipcc s latest assessment report and follow on exercises riahi et al 2017 the tools created for this article can readily be applied to multiple scenarios and we make them freely available at http www koomey com for modelers who want to apply them we are also eager to help modelers implement key scenarios in this framework so we can learn more and improve the tools it is also time for a comprehensive review of the variables included in scenario databases in light of the data needs for the decomposition methods outlined in this article certain key information for most models like the split between fossil biomass and industrial process ccs impacts and the split between fossil energy and industrial process carbon dioxide emissions still require additional digging or assumptions to create these decompositions small changes in the required output data could facilitate more rapid creation of many decomposition analyses appendices d e and f present some specific examples of how we use data from the ampere and ar5 scenario databases and give suggestions for additional data that should be included to make creating full decompositions easier one area that has been insufficiently explored to date is the set of interdependencies between the different terms in multiplicative identities first pointed out for the ipat identity by ehrlich and holdren 1971 and explored in some depth for the four factor kaya identity by nakicenovic et al 2000 the identity implies that each term is independent of the others but in practice that is not a good assumption for example population growth and technological development are both affected by the level of wealth per person and these factors interact if you change one factor the others will also change these complexities are beyond the scope of this analysis but assessing those interactions empirically is a worthwhile focus of additional research starting with assessing correlation coefficients among the terms the decomposition of key drivers presents little information about the energy technologies that are underlying the emissions scenario or more importantly how that portfolio of energy technologies is affected by climate policy intervention a more detailed decomposition of sources of mitigation is required to illuminate those insights at the technology level as explained in hummel 2006 the treatment of biomass and biofuels combustion in ipcc related scenarios lumping these emissions into the afolu sector may be worth revisiting an alternative way to treat emissions from these fuels would be to include the direct carbon emissions from biofuels in the emissions from the energy sector along with the indirect co2 emissions from collection and processing of these fuels which are already tallied there and in the land use sector and to include the uptake of carbon emissions associated with growing biomass for combustion and conversion to biofuels in the land use pane in fig 6 the term for total fossil carbon would then be renamed total energy sector carbon or tec to capture this change the advantage of this treatment is more specificity in which sectors the emissions and uptake occur direct air capture of carbon dioxide has attracted attention recently as a long term option for emissions reductions aps 2011 sanz pérez et al 2016 if this option ever becomes important in mitigation scenarios the additive dashboard fig 6 will need another pane to account for those impacts if the use of synthetic methane porosoff et al 2016 in which carbon is extracted from the atmosphere then combined with hydrogen derived from non fossil sources ever becomes commonplace the direct emissions from using such fuels would also reside in the energy sector while the carbon uptake from creating those fuels would reside in the new direct air capture pane of fig 6 8 conclusions the energy scenario community has struggled for decades with how best to compare and contrast analysis results this article presents one way to solve that problem that builds upon the familiar kaya and ipat identities the graphs developed here show the key drivers of emissions growth and reductions in a standard format using scenario outputs that are widely available we are hopeful that adoption of these methods will result in better understanding of scenario results and more rapid learning in the analysis community than has prevailed to date declaration of interests none authors completed this article on their personal time without support from funding agencies in the public commercial or not for profit sectors acknowledgments this article was based on work first developed and documented in holmes hummel s ph d thesis hummel 2006 for which jonathan koomey and john weyant were two of the committee members koomey and zachary schmidt conducted the writing data collection and expanded analysis of this article with high level guidance and some analytical and writing assistance from holmes hummel and john weyant based on earlier draft materials from hummel s thesis and elsewhere the late stephen schneider was on hummel s dissertation committee and was a co author on an early draft of this article we wish to acknowledge his important contribution to this work in the years before his untimely death in 2010 we are grateful for the suggestions of dr arnulf grübler of iiasa on interdependencies between components in the kaya identity and his sharing of the pfu historical data when it was at an earlier stage of development simon de stercke at iiasa helped us understand the intricacies of the pfu historical data and brainstormed with us on how to apply our decompositions to his historical data and for that interaction we are most thankful we are indebted to amory lovins of rocky mountain institute for suggesting the main title inside the black box and for helpful comments on the manuscript and to professor david victor at uc san diego for putting us in touch with professor kaya to obtain the original kaya identity article which was nowhere to be found online professor kaya deserves our thanks for digging out that article from his archives we are indebted to volker krey of iiasa who helped us understand some of the details of his message modeling results dan sanchez at the energy and resources group at uc berkeley and sabine fuss at the mercator research institute on global commons and climate change gave us helpful insights into the data on biomass ccs and we thank them for sharing their knowledge and experience jason funk at the center for carbon removal gave invaluable guidance on how the ipcc treats biomass and biofuels emissions and matt lucas of that same organization shared with us the latest articles about direct air capture and synthetic methane for which we are grateful we would like to thank professors paul ehrlich of stanford university and john holdren of harvard university for sharing their early work on the ipat identity and explaining some of its subtleties we would also like to express our gratitude to danny cullenward at stanford university who impressed on us the importance of carefully assessing biomass ccs and for his comments on the entire endeavor along the way jim mcmahon gave excellent pre submittal comments on the article and helped us clarify how biomass and biofuels combustion are treated in the scenario decomposition detlef van vuuren shared more details with us about cement emissions in various scenarios and we are grateful for his help appendix a supplementary data the following is the supplementary data related to this article appendicesinsidetheblackbox appendicesinsidetheblackbox appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2018 08 019 
26292,technology and policy implications of global energy and emissions scenarios can be difficult to analyze because underlying assumptions and drivers of scenarios are rarely made explicit this article documents methods for standardizing emissions scenario results that can be applied to virtually any scenario enabling more meaningful comparisons among scenarios than has been possible in the past this approach uses charts showing the dynamics and effects of emission drivers mitigation technologies and policies applying these methods will enable the policy and research communities to better understand the implications of scenarios and help analysts learn more rapidly as a matter of good practice modelers should create decompositions like the ones put forth in this article policy makers should request them and funders of scenario analysis and sponsors of model comparisons should support the application and further development of such tools keywords greenhouse gas emissions scenarios integrated assessment models climate change decomposition methods 1 introduction after the cop23 meetings in bonn in november 2017 the policy community is increasing its focus on reducing greenhouse gas ghg emissions quickly and efficiently to understand the range of possible solutions scientists have for many years used integrated assessment models which attempt to assess the costs and benefits of climate action many researchers have raised analytical concerns about these models and associated methods ackerman et al 2009 decanio 2003 koomey 2012 2013 pindyck 2013 2017 rosen and guenther 2015 but they provide a systematic way to track the implications of possible scenarios and we ll no doubt need accounting tools like them to explore future scenarios that are potentially promising energy scenarios explore a range of conditions possible in an inherently uncertain future including the responses to policy interventions that could stabilize atmospheric greenhouse gas concentrations riahi et al 2017 because the future is deeply uncertain it is appropriate to analyze any single energy and emissions scenario in the context of other cases such as a family of scenarios generated by a single model wbgu 2004 a set of models generating a common scenario weyant 2004 or a diverse selection of scenarios in the literature hamrin et al 2007 to compare the insights within and between these types of studies an effective and consistent framework for interpretation is essential hummel 2006 unfortunately different practices for reporting scenarios in the literature make it difficult to compare results and infer their meaning policy analysts who are often a primary audience for scenario results then face the challenge of interpreting and evaluating a steady stream of studies based on different models and baseline assumptions the importance of addressing this and related challenges has led to calls for greater transparency disclosure and self examination by the energy modeling community for example koomey et al 2003 and pfenninger 2017 this article describes one technique with minimal data requirements that can be used to illuminate technology and policy implications of global regional or country level energy and emissions scenarios for a policy audience this technique uses driver dashboards of line graphs that make explicit the relative role of key drivers of emissions over time in a particular scenario after reviewing these dashboards a researcher can put the quantitative results of each scenario into context and compare the results to historical trends and to other scenarios presented in the same framework the factors in the first dashboard include population growth economic activity final and primary energy consumption and total fossil carbon dioxide emissions from the energy sector the second dashboard shows ratios of these terms the third dashboard displays total projected energy sector emissions as well as emissions and reductions from biomass sequestration land use industrial non energy co2 emissions and other warming agents supplemental graphs yield additional insight to describe and demonstrate the utility of this interpretive technique this article first reviews a commonly used method for scenario decomposition the kaya identity for the energy sector and then describes why an expanded version of this method is required section 3 describes in more detail methods for creating such an expanded decomposition next the article expands the analysis further to include sources of emissions reductions outside the energy sector it then describes use cases for such decompositions focuses on limitations of the analysis suggests areas of future research and summarizes the conclusions that emerge from this analysis 2 building on a familiar concept the kaya identity the decomposition presented here is adapted and expanded from a well established convention in emissions scenario analysis known as the kaya identity kaya 1989 which is commonly used to decompose drivers of greenhouse gas emissions in the energy sector as many researchers have realized over the years the kaya identity as it was originally introduced is incomplete this section reviews the original kaya identity and how it has been used and altered in the past then modifies and expands the decomposition to correct for the issues in earlier formulations 2 1 overview of the kaya identity the kaya identity illustrates the key drivers for fossil carbon dioxide emissions from the energy sector this identity decomposes carbon emissions as a product of aggregate wealth energy intensity of economic activity and carbon intensity of the energy supplied as originally presented kaya 1989 it reads as shown in equation 1 1 carbon emissions e gnp c e gnp where gnp gross national product a measure of economic activity e gnp primary energy intensity per dollar of gnp and c e carbon intensity of primary energy production professor kaya presented this equation to help understand the implications of history and future scenarios in a simple back of the envelope way in most uses of this equation the order of the terms is switched so that aggregate wealth is first energy intensity is next and carbon dioxide intensity rather than carbon intensity of primary energy supplied is third ipcc 2014 p 368 most analysts split the aggregate wealth term into terms focused on population and economic activity per person which leads to the more familiar four factor kaya identity in equation 2 note different variable abbreviations than in equation 1 2 carbon dioxide emissions p gwp p pe gwp c pe where p is population gwp is gross world product a measure of economic activity pe is primary energy including conversion and energy transmission losses c is total net carbon dioxide emitted from the primary energy resource mix gwp p is the average income per person pe gwp is the primary energy intensity of the economy and c pe is the net carbon dioxide intensity of supplying primary energy in its substance and structure the kaya identity reflects a more general identity that expresses impact i as a product of human population p affluence a and technology t ehrlich and holdren 1971 1972 population is the same in both the kaya and ipat identities gwp person represents affluence and the other two terms characterize technology this formulation implies that a larger number of people with a higher income and more extensive use of certain technologies will have a greater impact on the environment the role of technology can be ambiguous technologies that produce and combust fossil fuels are the primary anthropogenic source of carbon dioxide while technologies for harnessing renewable energy and nuclear power sequestering carbon and improving efficiency can reduce net anthropogenic carbon emissions 2 2 previous uses of the kaya identity in scenario analysis the kaya identity has been widely used with some prominent examples being nakicenovic et al 2000 kawase et al 2006 raupach et al 2007 and hoffert et al 1998 there are so many examples in fact that we don t attempt a comprehensive review here what is interesting is that there s been only modest progress in the use of this identity since 1989 beyond the initial expansion of the identity to split population from gwp per person some researchers have expanded the identity to account for other weaknesses as we discuss below although those efforts always stopped short of what we term our expanded kaya identity for the energy sector presented below to explore the intellectual history of this concept appendix a examines how five generations of reports for the intergovernmental panel on climate change ipcc and related reports have treated the kaya identity the ipcc reports represent the state of scientific understanding about climate change at any time so they are a good marker for evolution in the understanding and use of this concept in analyzing historical data or in evaluating future scenarios in no case did the ipcc reports expand the kaya identity beyond its slightly expanded four factor form as shown in equation 2 above but the assessment reports became more sophisticated over time in how they used the concept and presented the analysis results other researchers have expanded the identity but progress on this front has been piecemeal and halting appendix a also presents the key articles that made progress in expanding the identity over time 2 3 reasons to expand the decomposition 2 3 1 disentangling energy intensity and supply chain losses the most widely used metric for energy intensity of economic activity pe gwp refers to primary energy pe which is the total energy input to the economy from all sources measured as the energy potential in fossil fuels and biomass at the point of extraction grübler et al 2015 the pe gwp metric is sensitive to four types of changes in an energy economy system each of which is affected by specific dynamics outlined with examples in table 1 although the definition of energy intensity as a ratio of primary energy to economic activity pe gwp is widely used in the literature there are long standing arguments for separating final energy to better assess trends in end use demands and to isolate the first effect energy supply losses in table 1 schipper et al 1992 final energy is the energy that is actually delivered to the customer s meter or gas tank and it can include electricity gasoline hydrogen or direct uses of natural gas coal and biomass grübler et al 2015 the special report on emissions scenarios sres reports final energy fe in its detailed appendices allowing a more accurate assessment of the energy intensity of the economy nakicenovic et al 2000 and a few analysts have disaggregated fe gwp and pe fe in previous scenario decomposition studies grübler et al 1993 kawase et al 2006 price et al 2006 following those authors the pe gwp metric can be further disaggregated as shown in equation 3 3 pe gwp fe gwp pe fe where fe gwp is the final energy intensity of economic activity and pe fe is a measure of total energy system supply losses for delivering final energy to users the ratio of primary energy to final energy delivered at an economy wide scale indicates the portion of potential energy lost in the supply chain a value of 1 0 indicates zero conversion losses in delivering final energy to users so this ratio will be greater than 1 0 for all real energy systems in the context of a single technology type e g an electric power plant the ratio of final energy to primary energy fe pe is a metric that captures both the efficiency of conversion and efficiency of energy transport transmission at an aggregate data level however it is not accurate to represent the ratio of primary energy to final energy pe fe as the inverse of the conversion efficiency as is done for example in kawase et al 2006 table 1 lists three different dynamics in an energy system that can affect supply losses and a change in technical efficiency is only one energy supply losses do occur as a result of some inefficiency along the way but the value of the metric itself varies even if conversion efficiencies in the system are held constant changes in the pe fe ratio can result from changes in the balance between fuels supplying a single type of final energy e g natural gas vs coal for electricity generation or the balance between final energy types e g supplying water heating using electricity or natural gas for the sake of precision and clarity this article departs from the system efficiency terminology used by some other authors in favor of the more precise term energy supply loss factor eslf 2 3 2 converting non combustion energy production to primary energy one of the key issues in understanding energy systems is assessing the total energy consumed by the system including all the losses in making energy available to consumers this assessment is complicated because of variations in how different energy sources produce fuels or electricity that allow us to do useful work primary energy is the energy contained in fossil and biomass fuels measured as for example the heat content of coal that goes into a power plant s boiler grübler et al 2015 the difference between primary energy and secondary energy is the conversion loss in converting coal to electricity the secondary energy is the amount of electricity injected into the grid at the busbar measured in kwh which also called net generation after accounting for on site use of electricity to run the plant final energy is the electricity delivered to the customer s meter which is lower than that injected by the power plant into the grid because of transmission and distribution t d losses nuclear hydroelectric solar electric wind power and other non combustion sources of electricity or hydrogen or process heat created using these fuels do not have losses that result in additional emissions like fossil fuel generation does what should define the quantity of primary energy for these sources there is the energy embodied in the nuclear fuel and the solar flux hitting a photovoltaic panel but what does it mean to consume that energy from the perspective of the emissions calculated by the kaya identity to fully account for global energy use in emissions scenarios all non thermal sources of electricity generation hydrogen and process heat have traditionally been assigned a primary energy value based on some measure of the amount of fuel needed to generate equivalent amounts of secondary energy plus the associated transmission and distribution t d losses to transport the secondary energy to the customer s meter this approach assumes that the alternative to the non combustion energy is fossil fuel fired combustion generation for many years this method termed the substitution method was considered in the scenario analysis community to be the customary convention the standard prescription for efficiency of conversion of primary to final energy in electricity generation was a constant 38 6 grübler and nakicenovic 1996 nakicenovic et al 1998 p 90 this convention implies a final to primary energy factor of 9 33 mj kwh kwh measured at the customer s meter for direct heat treated with the substitution method a different efficiency of conversion may be used for example 85 as found in iiasa s global energy assessment iiasa 2012 p 1820 one could also imagine a dynamic substitution approach in which non combustion sources are assigned energy supply chain losses equal to those of the average losses in the combustion part of the energy system as they change over time whereas the original substitution method assumed constant losses over time this alternative method would assess losses as they evolve in the energy systems being modeled that means it would capture the shift from for example older inefficient plants to newer efficient ones this method of imputing average system losses to non combustion sources has some justification when there is a significant amount of final energy delivered by fuel based energy sources a situation that holds now and into the near future for many energy scenarios it also allows for accurate comparison of the contribution of both combustion and non combustion resources to the generation mix there are issues with the substitution approach however even if using the more accurate dynamic version imputing losses for non combustion resources in essence creates fictional primary energy losses that aren t evident in the actual energy supply system if non combustion resources displace combustion sources with real conversion losses those losses are eliminated and primary energy use should go down using the substitution approach masks that contribution in 1998 modelers participating in the landmark special report on emissions scenarios prepared for the intergovernmental panel on climate change adopted an alternative convention for non combustion electricity generation based on the heat content of the electricity power plants delivered to the busbar this convention equates primary energy of electricity generation to the secondary energy at the busbar using a conversion factor of 3 6 mj kwh it then subtracts t d losses to get to final energy the modelers adopted this method termed direct equivalence as their common convention in order to harmonize assumptions and facilitate the comparison of results sres designated nuclear power to be treated with the direct equivalent method along with solar power wind power hydropower geothermal power and other renewable sources of electricity and hydrogen nakicenovic et al 2000 1 1 it is important to clarify that engineering economic models used to produce global energy scenarios do consider the technical efficiency of the engineered systems that harness non thermal renewable resources and nuclear power indeed technical efficiency is a vital characteristic of cost and performance parameters for each technology type however the primary energy data calculated for each technology type by models using sres terms is reported in terms of direct equivalence as described above hundreds of mitigation scenarios based upon the reference scenarios developed for sres have inherited the direct equivalence assumption and aside from cautionary notes buried deep in the sres report itself on pages 216 and 221 and a sidebar treatment in nakicenovic et al 1998 p 90 it has seldom been mentioned in the literature if more direct equivalent sources enter the supply mix primary energy use will decline because conversion losses from combustion are eliminated the substitution approach would instead indicate that total primary energy and losses in the system would change more modestly as a function of the efficiency of combustion plants remaining in the system after existing plants are displaced a counterintuitive result primary energy calculated using the direct equivalent approach correctly characterizes energy system losses over time in the form of the energy supply loss factor scenario modelers who rely heavily on the four factor kaya identity have often compared historical changes in the ratio of primary energy to gwp to the results of model projections failing to distinguish quantitatively between changes attributable to the shift to non combustion direct equivalent resources and those due to changes in final energy intensity one example is loftus et al 2015 which relies on aggregate trends in primary energy to gwp for its otherwise rigorous scenario comparisons another is peters et al 2017 which notes the possibility of splitting out the effects of these two factors in their methods discussion but still shows a graph of historical and projected primary energy use over time their fig 3 even if the scenario modelers understand this distinction our experience is that policy makers can be easily misled by this way of presenting the data thinking that scenarios with large reductions in the ratio of pe to gwp demonstrate significant end use efficiency when in many scenarios significant savings come from increasing penetration of non combustion resources this conceptual confusion is avoided by splitting those two key drivers in our expanded kaya identity and we strongly caution against relying on the ratio of pe to gwp in almost all cases it is also important to note that two of the most important energy data agencies the us energy information administration eia and the international energy agency iea have adopted conventions about primary energy that can lead to confusion eia uses the dynamic substitution method for all non combustion resources with non biomass renewables assigned the annual average conversion efficiency of fossil fuel plants and nuclear power assigned the annual average thermal efficiency of nuclear plants electricity imports are treated using direct equivalence 2 2 https www eia gov tools glossary index php id p iea treats renewables like solar wind and hydro using direct equivalence geothermal energy as a thermal power plant with 10 efficiency and nuclear power with the thermal efficiency of 33 3 3 https www iea org statistics resources balancedefinitions these choices should be reconsidered in light of the wide acceptance of the direct equivalent method in the scenario modeling community and the need to avoid inconsistencies in how primary energy conversions are treated more research is clearly needed on methods for assessing trends in primary energy as nakicenovic et al 1998 p 90 point out the very concept of primary energy becomes increasingly problematic particularly as renewable energy forms gain importance appendix b delves more into the implications of correctly accounting for the convention of direct equivalence 2 3 3 disentangling decarbonization and fossil sequestration historical trends in carbon intensity expressed as a ratio of net carbon emissions c in equation 2 to primary energy pe are unaffected by carbon sequestration because that technology has not yet entered the energy system at any detectable scale however making the distinction between the effects of decarbonization and sequestration is important because many energy scenarios for the 21st century that stabilize global warming at lower levels e g 450 ppm co2 and below imply large scale deployment of carbon sequestration technology koelbl et al 2014 the recent low energy scenario in grübler et al 2018 avoids all use of sequestration but that work is the exception when carbon sequestration cs is present in the system the ratio of net fossil carbon emissions nfc to primary energy pe will produce misleading indications about the carbon intensity of primary energy supply net fossil carbon emissions from the energy sector are less than the total fossil carbon tfc content of primary energy supply by an amount equal to the fossil fuel carbon sequestered csff rather than released to the atmosphere as shown in equation 4 4 nfc tfc cs ff as a result increases in carbon sequestration artificially depress the indicator for carbon intensity in the four factor kaya identity c pe conflating two distinct effects decarbonization a decline in the carbon content of primary energy sources tfc pe and fossil sequestration csff we can express the effect of sequestration as the ratio of net emissions released to the atmosphere after accounting for sequestration and the total fossil carbon dioxide in the primary energy system before sequestration nfc tfc hanaoka et al 2006 and kawase et al 2006 therefore proposed adding a term to the four factor kaya identity that accounts for the effect of sequestration as shown in equation 5 5 nfc pe tfc pe nfc tfc where tfc pe is the total fossil carbon intensity of primary energy nfc tfc is the fraction emitted to the atmosphere and tfc nfc tfc is the sequestration rate or fraction sequestered without this correction whenever the sequestration rate is rising the carbon content based calculation of carbon intensity c pe in equation 2 will overstate the actual decarbonization rate because sequestration masks the carbon content of fossil fuels remaining in the primary energy mix in other words if the c pe indicator is calculated using net emissions after sequestration the decomposition results will be misleading another complexity enters when we consider sequestering carbon from the flue gases of biomass combustion many recent scenarios rely on biomass sequestration to achieve negative emissions fuss et al 2014 kemper 2015 but the flow of carbon in this case is separate from those from fossil fuels biomass sequestration involves extracting carbon from the global biospheric carbon cycle and sequestering it we can think of biomass sequestration as altering the additive term for land use change included in equations 9 and 10 below for clarity we show biomass sequestration as a separate term that is subtracted from the other additive components in the fully expanded decomposition below 4 4 in the past biomass combustion was assumed to be carbon neutral but more recent analysis has shown that assumption to be incorrect for biofuels decicco john m danielle yuqiao liu joonghyeok heo rashmi krishnan angelika kurthen and louise wang 2016 carbon balance effects of u s biofuel production and use climatic change vol 138 no 3 2016 10 01 pp 667 680 https doi org 10 1007 s10584 016 1764 4 which points to the importance of careful accounting for carbon stocks and flows in any scenario where biomass or biofuels play a role 2 3 4 distinguishing fuel switching among fossil fuels from shifts away from fossil fuels an additional refinement of the expanded identity is described by peters et al 2017 in their methods section they advocate and implement further decomposition of the tfc pe term into two terms as shown in equation 6 6 tfc pe pe ff pe tfc pe ff where tfc pe is the total fossil carbon intensity of all primary energy pe ff pe is the share of primary energy from fossil fuels the fossil fuel fraction and tfc pe ff is total fossil carbon intensity of primary energy supplied by fossil fuels this additional detail allows decomposition of changes in tfc pe into the two key drivers of those changes switching from fossil fuels to non combustion alternatives e g from fossil fuels to zero emission energy resources and then fuel switching among fossil fuels e g from coal to natural gas or from high to low emitting oils as described in koomey et al 2016 a subtlety of this treatment of emissions intensity relates to the treatment of combustion of biomass and biofuels the message model following the ipcc reports co2 emissions from the combustion of biomass and biofuels in the agriculture forestry and other land use afolu sector ipcc 2006 indirect emissions of other greenhouse gases like methane are tallied separately and lumped into the other gases category in fig 6 below biomass and biofuels primary energy consumption is contained in our estimate of total primary energy consumption but net direct carbon dioxide emissions from these fuels are captured in the afolu sector along with any offsetting uptake of co2 in the biosphere this is why we call our term for carbon emissions above total fossil carbon and not total energy carbon biomass and biofuels are generally small compared to total primary energy but this accounting convention may become important in certain high biomass scenarios 3 decomposition of emissions in the energy sector improvements to the kaya identity described in the previous section lead to new insights about the way key drivers of carbon dioxide emissions from the energy sector are affected by policy intervention the decomposition applied in this paper incorporates each of these elements which can be summarized as in equation 7 7 c fossil fuels p gwp p fe gwp pe fe pe ff pe tfc pe ff nfc tfc where c fossil fuels represents carbon dioxide co2 emissions from fossil fuels combusted in the energy sector p is population gwp is gross world product measured consistently using purchasing power parity or market exchange rates fe is final energy pe is total primary energy calculated using the direct equivalent deq method as discussed above and in appendix b pe ff is primary energy associated with fossil fuels tfc is total fossil co2 emitted by the primary energy resource mix nfc is net fossil co2 emitted to the atmosphere after accounting for fossil sequestration we summarize the underlying factors e g population gwp final energy primary energy primary energy from fossil fuels and total fossil co2 in a dashboard as in fig 1 and the expanded kaya ratios that comprise c fossil fuels in fig 2 in the first row we show absolute values over time for each of the components in this case for historical data from 1900 to 2014 de stercke 2014 because sequestration is negligible during this period the graphs omit the final term in the expanded identity dotted lines project the paths each driver would have followed if historic rates of change had persisted to 2014 we calculate rates of change for the periods 1900 to 2014 and 1995 to 2014 the latter period showed greater change in the final energy intensity of economic activity than the longer period we also show those historical rates of change on the dashboards for future scenario projections to provide benchmarks against which future scenarios can be compared in figs 1 and 2 there are three different scales at which the dashboard drivers can be plotted absolute value indexed value and rate of change each of these provides different insights into the historical data or as we show later into scenario assumptions and results the first tier in the dashboard shows each key driver by its absolute value constructing a dashboard of key drivers in absolute value terms shows decomposition results in units with physical and economic meaning such as population gwp gwp capita or energy use gwp facilitating cross model comparisons the second tier shows the relative influence of each driver in the reference case and the relative influence of the policy intervention on each driver by plotting an index relative to some base year for figs 1 and 2 that base year is 1900 for graphs characterizing future scenarios below the reference year is 2010 the third and fourth tiers show the growth rate for each factor the instantaneous growth rate of total carbon dioxide emissions is equal to the sum of the growth rates for each key factor as shown in equation 8a for derivation see appendix i 8a d c dt c d p dt p d gwp p dt gwp p d fe gwp dt fe gwp d pe fe dt pe fe d pe ff pe dt pe ff pe d tfc pe ff dt tfc pe ff d nfc tfc dt nfc tfc since we calculate changes over discrete time periods this relationship holds as an approximation 8b δ c c δ p p δ gwp p gwp p δ fe gwp fe gwp δ pe fe pe fe δ pe ff pe pe ff pe δ tfc pe ff tfc pe ff δ nfc tfc nfc tfc for historical data we show the growth rate in the third tier with year by year changes and for the fourth tier we show it with data tallied as a running five year average the annual growth rate shows more variation in the historical data as we expect for future scenario projections we show data averaged over ten year periods because those are the data available in the scenario databases we use 5 5 one subtlety is that for future scenarios we calculate compound annual growth rates over ten year periods then apply those growth rates in each year of the relevant period this allows us to plot trends over the entire analysis period rather than dropping a decade in the beginning or at the end as we would have to do if we showed only one growth rate per decade if scenario output data are available for intervening years then this convention can easily be altered interestingly fig 1 shows that gwp growth is greater in the past two decades than from 1900 to 2014 but the annual rate of increase in final energy consumption is lower in the later period final energy intensity reductions have therefore accelerated in the past two decades we use this same format for scenario projection data to illustrate we show example dashboards for a scenario projection using the message 4 0 model based on results presented in the ampere modeling exercise see appendix h and riahi et al 2015 we chose message as the exemplar in this article because the message modeling team is widely known and respected in the modeling community and because message explicitly treats almost all warming agents in its projections thus allowing us to present a full decomposition to illustrate our calculations and visualizations fricko et al 2017 krey et al 2016 schrattenholzer 1981 fig 3 shows the underlying factors for the message scenario while fig 4 shows the kaya ratios just as for the historical data the figures show the reference scenario using a black solid line and the low emissions intervention scenario as a red line the dashed lines show trends in historical data for comparison as discussed above in the nfc absolute value pane we also show the total effect of biomass ccs plus fossil ccs as a green solid line even though negative emissions from biomass ccs are not counted in the energy sector its deployment is linked to the energy sector as well as to the carbon cycle associated with land use because this emissions reduction option is important in many low emissions scenarios kemper 2015 we add this line to show how it compares to the nfc in the intervention case we also show a line for biomass ccs in the dashboard of additive terms below fig 3 shows some key insights population gwp and carbon intensity of fossil energy are not much affected by the intervention scenario while final energy primary energy fossil fuel fraction and net fossil carbon are all significantly reduced in the intervention case growth rates for all factors in the future scenarios are lower than historical rates gwp growth rates decline over the analysis period but final energy growth slows almost to zero then starts rising again after 2050 causing the fe gwp ratio in fig 4 to break off from the 1995 to 2014 trend line and slow its long and rapid decline this curious behavior is ripe for further investigation and it illustrates the kind of questions that this method of presenting results enables see also appendix c the figures also show the importance of fossil ccs to reducing emissions in the intervention case the nfc pane in fig 3 clearly shows an emissions reduction compared to the tfc pane and the nfc tfc pane in fig 4 shows the importance of that reduction relative to tfc emissions the factors dashboard and the ratios dashboard are a starting point for further investigation now let s dig into additional ways to garner insights from this analysis and presentation approach 4 another window into the expanded kaya results another way to visualize changes in emissions over time is with a bar chart this type of graph shows the compound annual growth rate for each term in the expanded kaya identity over some time period based on equation 8b as shown in fig 5 it can be plotted for the reference case an intervention case or the difference between them giving a quantitative indication of the contributors to emissions growth or reductions in this case over the 2010 to 2100 time period fig 5 shows results for our exemplar message scenarios changes in gwp per capita and final energy per unit of gwp are the two most important drivers of emissions trends in the reference case while reduction of fossil fuel fraction decarbonization of the primary energy supply using non fossil options fossil sequestration and the final energy intensity of economic activity are the three most important components of emission reductions in the intervention case 5 additive elements for non energy emissions the expanded kaya identity in this article addresses direct carbon dioxide emissions from the energy sector now we turn to other sources of greenhouse gas emissions including co2 emissions from industrial processes like cement production co2 emissions from land use changes such as deforestation and non co2 greenhouse gases which include methane nitrous oxide and a set of powerful greenhouse gases containing fluoride manufactured for human use 6 6 the so called f gases include hydrofluorocarbons hfcs perfluorocarbons pfcs and sulfur hexafluoride sf6 all of which are released in extremely small quantities compared to carbon dioxide but the warming potential of each molecule can be as much as five orders of magnitude greater these factors are included in the third dashboard as additional sources of emissions with the non co2 gases being converted to carbon dioxide equivalent total greenhouse gas equivalent emissions c total eq can be expressed as in equation 9 9 c total eq c fossil fuels c industry c land use c non co 2 gases eq cs biomass where c industry represents carbon dioxide emissions from industrial processes non energy uses of fossil fuels that result in emissions such as cement and aluminum production some models combine these emissions with fossil fuel combustion emissions but they should be split out for clarity c land use represents net carbon dioxide emissions from changes in agriculture and land use that are not associated with emissions reductions from biomass ccs this term can also be negative if there is significant reforestation c non co 2 gases eq represents emissions of other greenhouse gases converted to co2 equivalent using relative factors of global warming potential 7 7 we convert emissions of the two major non co2 greenhouse gases methane and nitrous oxides to co2 equivalents using 100 year global warming potentials including climate feedbacks from ipcc 2013 climate change 2013 the physical science basis contribution of working group i to the fifth assessment report of the intergovernmental panel on climate change stocker t f d qin g k plattner m tignor s k allen j boschung a nauels y xia v bex and p m midgley eds cambridge united kingdom and new york ny usa cambridge university press http www climatechange2013 org chapter 8 table 8 7 page 714 https www ipcc ch pdf assessment report ar5 wg1 wg1ar5 chapter08 final pdf for the message analysis we use the model s calculation of the gwp of f gases c s b i o m a s s represents net negative emissions from sequestering carbon emissions associated with biomass combustion in effect such sequestration removes carbon from the biosphere the emissions reductions from this source must be carefully distinguished from those land use changes the guiding principle should be that all energy system and related emissions should be included in one of the additive terms in equation 9 for example indirect emissions from hydroelectric facilities would include co2 from fossil energy used in constructing the plant in the fossil fuels category co2 from cement used in construction in the industry category and methane emissions from decomposition of biomass on flooded land in the non co2 gases category for nuclear power indirect emissions from fossil powered enrichment would fall into the fossil fuels category while co2 from cement used in construction would fall into the industry category substituting c fossil fuels with the expanded kaya identity components from equation 7 we get equation 10 which is what we call our fully expanded decomposition 10 c total eq p gwp p fe gwp pe fe pe ff pe tfc pe ff nfc tfc c industry c land use c non co 2 gases eq cs biomass standard scenario outputs for models with comprehensive coverage of emissions will usually include the data for each additive term in some cases additional calculations or data outputs will be required the fully expanded decomposition of key drivers in fig 6 has all additive elements net fossil energy co2 emissions biomass sequestration land use change industrial non energy co2 emissions and non co2 gas emissions if direct air capture of co2 becomes important in future another pane with those emission reductions can be included in the additive dashboard energy sector emissions reductions still dominate all reductions but each of the additive components has a measurable effect on total greenhouse gas emissions appendix j gives details on the projection of cement emissions which are combined with fossil energy emissions in the message model outputs we derive them instead from van ruijven et al 2016 and subtract from the total of fossil energy and industrial emissions from the message outputs to get fossil fuel energy emissions it is critical to separate biomass ccs from fossil ccs in such attribution graphs to get a clear picture of carbon flows as discussed in section 2 above biomass ccs extracts carbon from the atmosphere and sequesters it while fossil ccs sequesters emissions that otherwise would have reached the atmosphere from the combustion of fossil fuels the importance of ccs is highlighted in fig 7 which gives the full picture of ccs compared to tfc and nfc it includes the energy sector plus biomass ccs but not the other additive terms in fig 6 the reference case tfc is in black and the intervention case tfc is in red nfc which shows the effect of fossil ccs on tfc is represented by the blue solid line and nfc minus biomass ccs is shown by the solid green line the emissions reductions associated with fossil and biomass ccs are shown in the dotted blue and green lines respectively reductions in direct emissions from the energy sector are represented by the difference between the black and red lines in fig 7 then savings from ccs contribute further fossil ccs has a slightly larger cumulative impact than biomass ccs over the analysis period but both achieve reductions of gigatonne scale so are significant by any measure the decline in fossil ccs reductions is significant after 2070 a result that points to the need for further digging into these output results another way to present the full emissions reductions picture is shown in fig 8 this chart shows total ghg equivalent emissions in the reference and intervention cases then attributes savings to each component we ve identified in the energy sector ratios dashboard fig 4 and the additive emissions dashboard fig 6 the non energy emissions results in the additive dashboard come straight from model outputs in almost all cases to decompose the fossil energy related emissions which are part of a multiplicative identity we rely on the ldmi i technique first proposed by ang 2004 this method gives a perfect decomposition no residual term and is relatively simple to apply to multi factor problems in addition the iea uses the lmdi i method to decompose co2 emissions from electricity production iea 2015 creating a credible precedent for our choice population projections are the same in both the reference and intervention cases so no emissions savings accrue from this category in this scenario or in most other scenario exercises bongaarts and neill 2018 the majority of the decline in total fossil carbon is from substituting non combustion resources for fossil energy supply fuel switching among fossil fuels contributes only a nominal amount to emissions reductions in addition substitution on the supply side is the single largest contributor to emissions reductions for the whole scenario the contribution of fossil ccs declines in the latter part of the analysis period as we also saw in fig 7 graphs like fig 8 work well in plotting contributions to emissions reductions but when one or more terms in the identity result in an increase in emissions over time a stacked area graph can t show that easily for example the energy system loss factor eslf sometimes increases during part of the intervention scenario thus increasing emissions during that period in those cases we adjust for that increase in those years by allocating it in proportion to the emissions reductions from the other terms in the fully expanded decomposition in every year the sum of all net emissions reductions will then reflect the difference between the reference case black line and the intervention case emissions red line the sign of each term positive or negative can be most accurately represented in bar charts like those in fig 5 6 benefits and uses of decomposition methods the decomposition tools described in this article offer benefits over the ad hoc methods traditionally used in the scenario analysis community individual modeling groups often develop their own techniques but those cannot be easily applied for cross model comparisons and are normally not as comprehensive as those developed in this article for example most analysts still rely on the four factor kaya identity for analyzing scenario results which can lead to confusion as we describe above there are three main audiences for decomposition methods modelers policy analysts and research funders each of these groups can use decompositions for sanity checking promoting transparency and making valid comparisons 6 1 sanity checking during scenario creation for modelers and secondarily policy analysts and funders who review interim results these tools allow scrutiny to begin much earlier in the research cycle and encourage sanity checking of assumptions and results before publication for example scenarios often rely on heroic assumptions for costs and adoption of new exotic technologies like biomass ccs combined with modest projected changes in fe gwp or renewable energy adoption while not necessarily wrong such inconsistencies in relative technological progress should prompt further digging and analysis hummel 2006 use of these decomposition methods makes that analysis easier and quicker 6 2 promoting transparency for all users decomposition tools allow greater transparency understanding and documentation of methods and results than has heretofore been possible the assumptions and algorithms of these models can be opaque but decomposition methods give a view into the black box that will enable debate about key issues and uncertainties in mitigation scenarios even though the complex algorithms embedded in the models are not as easily subject to outside scrutiny 6 3 making valid comparisons quickly for all users decomposition tools allow better comparisons between history and projections baseline and policy cases and different baseline and policy cases from many modeling shops hummel 2006 for example a substantial divergence of a projection from historical trends or unintuitive discontinuities in the projection can reveal issues with data and methods or lead to new policy insights comparisons among different projections can be done at a glance using our decomposition dashboard and supporting graphs each scenario set can be summarized in a few pages and the results of multiple models can be quickly compared it is now almost never possible to do such comparisons easily using published modeling results but if modelers create decompositions using standard tools in a consistent framework it will become standard practice 7 limitations of the analysis and areas for further research the decomposition of key drivers involves examination of high level aggregate data the focus on global trends can obscure important changes at the regional and sectoral levels as well as technological trends an obvious next step is to apply the dashboard and associated tools to compare recent model runs completed for ipcc s latest assessment report and follow on exercises riahi et al 2017 the tools created for this article can readily be applied to multiple scenarios and we make them freely available at http www koomey com for modelers who want to apply them we are also eager to help modelers implement key scenarios in this framework so we can learn more and improve the tools it is also time for a comprehensive review of the variables included in scenario databases in light of the data needs for the decomposition methods outlined in this article certain key information for most models like the split between fossil biomass and industrial process ccs impacts and the split between fossil energy and industrial process carbon dioxide emissions still require additional digging or assumptions to create these decompositions small changes in the required output data could facilitate more rapid creation of many decomposition analyses appendices d e and f present some specific examples of how we use data from the ampere and ar5 scenario databases and give suggestions for additional data that should be included to make creating full decompositions easier one area that has been insufficiently explored to date is the set of interdependencies between the different terms in multiplicative identities first pointed out for the ipat identity by ehrlich and holdren 1971 and explored in some depth for the four factor kaya identity by nakicenovic et al 2000 the identity implies that each term is independent of the others but in practice that is not a good assumption for example population growth and technological development are both affected by the level of wealth per person and these factors interact if you change one factor the others will also change these complexities are beyond the scope of this analysis but assessing those interactions empirically is a worthwhile focus of additional research starting with assessing correlation coefficients among the terms the decomposition of key drivers presents little information about the energy technologies that are underlying the emissions scenario or more importantly how that portfolio of energy technologies is affected by climate policy intervention a more detailed decomposition of sources of mitigation is required to illuminate those insights at the technology level as explained in hummel 2006 the treatment of biomass and biofuels combustion in ipcc related scenarios lumping these emissions into the afolu sector may be worth revisiting an alternative way to treat emissions from these fuels would be to include the direct carbon emissions from biofuels in the emissions from the energy sector along with the indirect co2 emissions from collection and processing of these fuels which are already tallied there and in the land use sector and to include the uptake of carbon emissions associated with growing biomass for combustion and conversion to biofuels in the land use pane in fig 6 the term for total fossil carbon would then be renamed total energy sector carbon or tec to capture this change the advantage of this treatment is more specificity in which sectors the emissions and uptake occur direct air capture of carbon dioxide has attracted attention recently as a long term option for emissions reductions aps 2011 sanz pérez et al 2016 if this option ever becomes important in mitigation scenarios the additive dashboard fig 6 will need another pane to account for those impacts if the use of synthetic methane porosoff et al 2016 in which carbon is extracted from the atmosphere then combined with hydrogen derived from non fossil sources ever becomes commonplace the direct emissions from using such fuels would also reside in the energy sector while the carbon uptake from creating those fuels would reside in the new direct air capture pane of fig 6 8 conclusions the energy scenario community has struggled for decades with how best to compare and contrast analysis results this article presents one way to solve that problem that builds upon the familiar kaya and ipat identities the graphs developed here show the key drivers of emissions growth and reductions in a standard format using scenario outputs that are widely available we are hopeful that adoption of these methods will result in better understanding of scenario results and more rapid learning in the analysis community than has prevailed to date declaration of interests none authors completed this article on their personal time without support from funding agencies in the public commercial or not for profit sectors acknowledgments this article was based on work first developed and documented in holmes hummel s ph d thesis hummel 2006 for which jonathan koomey and john weyant were two of the committee members koomey and zachary schmidt conducted the writing data collection and expanded analysis of this article with high level guidance and some analytical and writing assistance from holmes hummel and john weyant based on earlier draft materials from hummel s thesis and elsewhere the late stephen schneider was on hummel s dissertation committee and was a co author on an early draft of this article we wish to acknowledge his important contribution to this work in the years before his untimely death in 2010 we are grateful for the suggestions of dr arnulf grübler of iiasa on interdependencies between components in the kaya identity and his sharing of the pfu historical data when it was at an earlier stage of development simon de stercke at iiasa helped us understand the intricacies of the pfu historical data and brainstormed with us on how to apply our decompositions to his historical data and for that interaction we are most thankful we are indebted to amory lovins of rocky mountain institute for suggesting the main title inside the black box and for helpful comments on the manuscript and to professor david victor at uc san diego for putting us in touch with professor kaya to obtain the original kaya identity article which was nowhere to be found online professor kaya deserves our thanks for digging out that article from his archives we are indebted to volker krey of iiasa who helped us understand some of the details of his message modeling results dan sanchez at the energy and resources group at uc berkeley and sabine fuss at the mercator research institute on global commons and climate change gave us helpful insights into the data on biomass ccs and we thank them for sharing their knowledge and experience jason funk at the center for carbon removal gave invaluable guidance on how the ipcc treats biomass and biofuels emissions and matt lucas of that same organization shared with us the latest articles about direct air capture and synthetic methane for which we are grateful we would like to thank professors paul ehrlich of stanford university and john holdren of harvard university for sharing their early work on the ipat identity and explaining some of its subtleties we would also like to express our gratitude to danny cullenward at stanford university who impressed on us the importance of carefully assessing biomass ccs and for his comments on the entire endeavor along the way jim mcmahon gave excellent pre submittal comments on the article and helped us clarify how biomass and biofuels combustion are treated in the scenario decomposition detlef van vuuren shared more details with us about cement emissions in various scenarios and we are grateful for his help appendix a supplementary data the following is the supplementary data related to this article appendicesinsidetheblackbox appendicesinsidetheblackbox appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2018 08 019 
26293,dynamical earth and environmental systems models are typically computationally intensive and highly parameterized with many uncertain parameters together these characteristics severely limit the applicability of global sensitivity analysis gsa to high dimensional models because very large numbers of model runs are typically required to achieve convergence and provide a robust assessment paradoxically only 30 percent of gsa applications in the environmental modelling literature have investigated models with more than 20 parameters suggesting that gsa is under utilized on problems for which it should prove most useful we develop a novel grouping strategy based on bootstrap based clustering that enables efficient application of gsa to high dimensional models we also provide a new measure of robustness that assesses gsa stability and convergence for two models having 50 and 111 parameters we show that grouping enabled gsa provides results that are highly robust to sampling variability while converging with a much smaller number of model runs keywords global sensitivity analysis curse of dimensionality computationally intensive simulations factor grouping robustness convergence 1 introduction 1 1 motivation computational models are widely used to understand and simulate the complex physical behaviors of dynamical earth and environmental systems dees bennett et al 2013 provenzale 2014 bathiany et al 2016 by enabling prediction and scenario analysis regarding the quality and quantity of earth s future resources poff et al 2015 guo et al 2016 maier et al 2016 such models have become essential to management and decision making under uncertainty and non stationary conditions however the drive to incorporate our ever growing understanding of underlying system processes and their feedback mechanisms leads to progressively more complex and computationally intensive model formulations with growth in complexity and presumably fidelity it is now not uncommon for dees models to contain hundreds and even thousands of parameters and factors whose values are uncertain and need to be characterized global sensitivity analysis gsa has proven to be an effective means for characterizing the impact and significance of uncertainty in various model components e g parameters initial conditions boundary conditions etc on model behavior and predictions razavi and gupta 2015 most well established gsa techniques are based on one of the following two approaches an analysis of partial derivatives such as in the method of morris e g morris 1991 campolongo et al 2007 sobol and kucherenko 2009 or an analysis of variance such as in the method of sobol e g sobol 1993 homma and saltelli 1996 tarantola et al 2006 however it is not uncommon for methods based on these two approaches to give conflicting assessments and razavi and gupta 2016a b recently showed that both strategies are actually special cases of a more comprehensive variogram based approach that directly accounts for spatial structure of the model response surfaces the new approach entitled variogram analysis of response surfaces vars is a unifying theory that bridges across the derivative and variance based approaches by introducing the notion of perturbation scale haghnegahdar and razavi 2017 gsa techniques are now widely used for a variety of purposes including uncertainty apportionment e g marino et al 2008 borgonovo et al 2012 parameter screening e g trocine and malone 2001 touzani and busby 2014 and diagnostic testing e g saltelli et al 2006 gupta et al 2008 haghnegahdar et al 2017 however two major interrelated challenges limit their application to advanced dees models namely i the curse of dimensionality and ii computational expense the former refers to the fact that as the number of uncertain parameters factors increases the volume of the problem space increases so rapidly that any attempt to investigate and characterize it in a stable robust and statistically sound manner requires an exponentially increasing sample size the latter refers to the typically computationally intensive nature of dees models leading to long run times that together with the former can make any meaningful analysis of such models computationally prohibitive these two challenges are the primary reason why gsa applications reported in the literature are often limited to relatively simple models having smaller numbers of uncertain parameters see section 2 however this is paradoxical to the underlying goals of gsa as a means to facilitate the development understanding and use of more realistic dees models in this regard it is interesting to note that a major goal of gsa commonly stated in the literature is to help reduce the dimensionality of a problem by identifying non influential parameters however as the dimensionality of the problem space grows beyond a few tens of parameters most gsa methods quickly become handicapped by their need for impractically large sample sizes due to the excessively large computational times involved and are therefore unable to provide robust assessments of sensitivity that have an adequate level of confidence razavi and gupta 2016b a further class of sensitivity analysis approaches known as metamodel enabled gsa can substantially reduce the number of model runs needed to estimate sensitivity indices but is mostly restricted to low or moderate dimensionality problems in fact as discussed in razavi et al 2012 about 85 of metamodel applications have been on problems having less than 20 factors in higher dimensions the performance of metamodel enabled gsa algorithms can substantially deteriorate due to over fitting becker 2015 as another potential remedy analysts sometimes use local sensitivity analysis lsa methods that require lower computational demand however it is well known that lsas provide inadequate assessments that can often be misleading saltelli and annoni 2010 in this paper we present an approach that addresses the problem of robust sensitivity assessment for high dimensional problems there are two main aspects to our approach the first aspect is based on the sparsity of factors principle otherwise known as the pareto principle which states that a small subset of factors is often responsible for most of the system output uncertainty box and meyer 1986 to exploit this principle we need a way to identify which of the individual factors have similar properties in terms of their influence on model output variations in doing so we also recognize that when the number of factors is very large the user is typically not interested in an exact ranking of factor importance for example with 100 factors it may not matter whether a given factor has a sensitivity ranking of 49 or 50 instead it may be more profitable to use the available computational budget to reliably categorize factors into a small number of distinct groups for example these could be labeled as strongly influential influential moderately influential weakly influential and non influential the second aspect deals with an essential but often neglected element of gsa which is that of characterizing and improving the robustness of the gsa results this is extremely important given that gsa is a sampling based technique and as such is prone to statistical uncertainty due to sampling variability i e the results will be sensitive to randomness in the selection of the sample hence robustness can be defined as the stability of the gsa results i e the degree of insensitivity to sampling variation in other words lower variability of the results obtained over multiple trials of the algorithm performed with different identically distributed sample sets indicates a higher degree of robustness see e g montgomery 2008 razavi and gupta 2016b developed one of the first techniques to assess the robustness of gsa factor rankings incorrectly termed reliability measure in their paper based on the use of bootstrapping here we extend that approach and integrate it with optimal factor grouping i e the first aspect mentioned above to provide a gsa solution for high dimensional problems that are often intractable with traditional approaches 1 2 objectives and scope the primary goal of this paper is to introduce an automated factor grouping strategy that is based on the integration of clustering with bootstrapping the method is designed to group input factors into a certain number of groups based on information gained during gsa where the resulting groups can be of any size importantly if the number of groups is not pre specified the algorithm efficiently determines an optimal number of groups further to evaluate performance of the grouping based gsa we develop a measure of robustness that provides a way to monitor convergence of the gsa results the overall procedure can be used in conjunction with any gsa technique here we demonstrate the utility of this approach on two high dimensional problems by coupling it with the vars methodology the rest of this paper is structured as follows in section 2 we discuss the curse of dimensionality challenge associated with gsa of high dimensional problems review existing strategies for factor grouping and discuss their limitations in section 3 we introduce the new grouping technique and explain the details of its implementation the two case studies are developed in section 4 and the numerical results and analyses are presented in section 5 finally conclusions and future recommendations appear in section 6 2 review of the literature 2 1 difficulties associated with gsa of high dimensional problems in the vast majority of gsa studies reported in the literature the dimensionality of the problem space has been quite low fig 1 shows a cumulative distribution function cdf of the factor space dimensionality for environmental modelling studies during the period 2007 2017 that reported applications of gsa in 70 of the cases the number of factors was less than or equal to 20 while 85 had less than 40 and almost all 97 had less than 90 factors only very few studies used models with more than 100 factors radomyski et al 2016 used 156 tang et al 2007 used 403 and herman et al 2013 used 1092 so although one of the main goals of gsa is to assist in reducing the dimensionality of a problem by screening out non influential parameters the evidence suggests that it is seldom applied to the high dimensional problems in which it is most needed the reason for this can in part be attributed to the curse of dimensionality which coupled with the high computational costs typically associated with running complex dees models makes application of gsa to high dimensional problems very expensive the curse of dimensionality manifests through the fact that to achieve stable and robust results current gsa techniques require large numbers of sample points i e model runs to be drawn in some representative manner from the factor space while this issue can be partially addressed using optimized sampling algorithms such as progressive latin hypercube sampling plhs sheikholeslami and razavi 2017 and good lattice points glp gong et al 2016 the computational cost remains high song et al 2016 to further synthesize the status quo table 1 provides an overview of recent papers that have used gsa to study higher dimensional models here we have arbitrarily selected only studies where the number of factors was greater than or equal to 40 it is important to note that the sample size in these studies was typically chosen based on available computational budget without considerations of gsa stability and convergence consequently the results are likely to have been highly sensitive to sampling variability i e may not have been robust studies of the convergence behavior of various gsa techniques generally show that factor ranking order of relative sensitivity typically converges more quickly than factor sensitivity indices computed by the gsa perhaps as expected and that factor ranking is more robust to sampling variability benedetti et al 2011 yang 2011 cosenza et al 2013 wang et al 2013 vanrolleghem et al 2015 razavi and gupta 2016b sarrazin et al 2016 overall this fact indicates that if a modeler is interested in factor screening or prioritization rather than in generating accurate estimates for the sensitivity indices the number of required model simulations and hence computational cost can be reduced interestingly vanrolleghem et al 2015 and wang et al 2013 found that for the gsa methods they used sensitivity index convergence rates were typically slowest for the factors having the highest importance while conversely sarrazin et al 2016 nossent et al 2011 and yang 2011 reported that the sensitivity index convergence rates were slowest for factors having lowest importance this lack of agreement in their results suggests that convergence rates may be case specific and depend on the other aspects of the problem at hand including but not limited to sampling variability and the choice of gsa algorithm from this review of the literature table 1 we make three observations i regardless of the gsa method used it is typical for only a small sub group of factors on average 20 to exert significant control over variations in the model outputs a manifestation of the sparsity of factors principle ii historically the grouping of factors into strongly influential versus non influential has typically been done in a subjective and case specific manner the most common approach being to specify a threshold sensitivity value and to group factors based on their sensitivities relative to the threshold value iii when different gsa methods were applied to the same problem while actual rankings varied the factor rankings were typically such that the relative positions of factors in the higher middle or lower parts of the rankings were often quite similar e g cosenza et al 2013 vanuytrecht et al 2014 sheikholeslami et al 2017 in other words different methods generally tended to identify the same groups of factors in terms of the strength of their influence on model response from a practical point of view this suggests that one might focus on whether a factor belongs to a high medium or low influence group rather than on its exact ranking particularly given the demonstrated effects of sampling variability on estimated rankings in high dimensional problems grouping input factors of similar sensitivity can help reduce the effective dimensionality of the factor space this is useful because when the problem has more than 20 factors it can be difficult to analyze the gsa results for extraction of relevant information it seems beneficial therefore to categorize factors into subsets to facilitate interpretation together these issues highlight the need for an effective automated and non subjective strategy for grouping model factors based on information developed during the execution of a gsa algorithm in the next subsection we review the few strategies for grouping that have been proposed in the literature discuss their characteristics and identify shortcomings 2 2 review of existing grouping strategies for gsa despite significant research dedicated to the advancement of gsa there is a surprising paucity of literature on the development of efficient grouping strategies relevant studies have focused mainly on enabling gsa methods to compute an overall sensitivity measure for a pre identified group of factors for example in the variance based sobol method sobol 1993 the set of uncertain factors x can be partitioned into pre specified groups x u and x w where x u x w x and x u x w such that the variance of the response v a r y can be decomposed as v x u v x w v x u x w 1 where v x w v a r x w e x u y x w v a r y is the normalized variance of the conditional expectation that measures the first order effect of x w on the model output this decomposition can be performed for any number of factors saltelli et al 2006 extending this saltelli et al 2008 provided a multi stage factor grouping strategy to perform variance based sensitivity analysis based on the concept presented above in their approach the number of groups is specified a priori and then the members of each group are populated by testing a variety of combinations similarly campolongo et al 2007 modified the morris elementary effects method morris 1991 to work with pre specified groups of factors for which total sensitivity indices can be estimated ciuffo and azevedo 2014 patelli et al 2010 in all the grouping techniques reviewed here the factor grouping scheme is determined before application of gsa based on the nature of the problem physical interpretation previous experience intuition randomized grouping etc another commonly used approach is to group factors based on arbitrary pre specified sensitivity index thresholds eweys et al 2017 hou et al 2015 göhler et al 2013 tang et al 2007 for example in variance based gsa a group of factors can be deemed strongly influential if each member individually contributes at least 10 of the overall model output variance or weakly non influential if each member individually contributes less than 1 of the model variance note that these thresholds are selected rather arbitrarily however in the morris method of elementary effects the definition of thresholds is necessarily case specific because the estimated sensitivity indices can have different ranges of variation depending on the model output and particularities of the case study for screening purposes identifying non influential factors khorashadi zadeh et al 2017 proposed the interesting approach of adding a dummy variable i e a variable known a priori to be fully non influential to the set of factors under investigation factor screening has also been accommodated by applying a statistical testing approach originally introduced by andres 1997 and further extended by sarrazin et al 2016 tang et al 2007 and nossent et al 2011 as a more objective way to achieve factor grouping klepper 1997 used a clustering method to categorize model factors based on the sensitivity analysis results to our knowledge this method is the only method to date that is based on the analysis on a clustering concept it does not however provide an optimal grouping and is prone to uncertainty associated with sampling variability see section 3 1 a major disadvantage with a priori specification of the grouping is that the results can depend strongly on the user s selection of groups a task that can become complicated if not impractical as the number of factors grows overall our review leads us to conclude that an effective grouping strategy must address the following questions as part of the gsa methodology itself i what is an optimal subdivision of model factors into a given number of groups ii what is the optimal number of groups iii to what extent are the grouping results robust regarding the first question the members of a factor group should be as similar in some sense as possible while being as distinct as possible from the members of the other groups to achieve this a metric must be defined that quantifies similarity between input factors according to the manner in which they influence the model outputs regarding the second question removal of subjectivity about the number of groups should be somehow based on the goal of obtaining a maximum level of homogeneity within groups and distinction across groups finally the third question addresses the need for some degree of confidence in the grouping results meaning that the results are not overly sensitive to sampling variability none of the aforementioned grouping strategies addresses these challenges explicitly and in a systematic manner this gap motivated our development of the automated factor grouping method introduced in the next section 3 the proposed factor grouping strategy our proposed method for factor grouping fig 2 combines agglomerative hierarchical clustering with bootstrapping and introduces a new robustness measure that enables an objective assessment of gsa convergence the algorithm consists of five steps details of each are provided in the following subsections 3 1 generating the sensitivity matrix by bootstrapping in the traditional approach to gsa the user runs a model n times using the original sample of n points drawn from factor space and then a vector of sensitivity indices for the d factors is computed using the information provided by these points s 1 d s 1 s d in general the size n of the randomly drawn sample used to estimate s 1 d will be limited and so due to sampling variability we can expect a degree often considerable of statistical uncertainty to be associated with s 1 d the distribution free and easy to implement bootstrap method efron 1979 can be used to estimate the magnitude of that uncertainty davison et al 2003 example applications in the gsa context can be found in razavi and gupta 2016b and archer et al 1997 importantly in bootstrapping the set of bootstrap samples is extracted from the original sample set and thus additional model runs are not necessary we apply the following two steps to implement the bootstrap technique 1 set the number of bootstrap re samplings replicates to r and randomly draw with replacement from original sample set r bootstrapped samples each having the same size of n as the original sample the result is the so called set of r bootstrap samples note that r is selected to be some arbitrarily large number e g 1000 as has been commonly used in the literature 2 for each bootstrap sample i i 1 r in the set recompute the vector of sensitivity indices s i d b s i 1 b s i d b using the gsa method following the above procedure a two dimensional r d bootstrapped sensitivity matrix m containing r bootstrap replicates for the sensitivity indices can be formed as 1 m s 1 1 b s 1 d b s r 1 b s r d b because the matrix m contains information regarding estimation uncertainty it provides valuable information that enables a robust evaluation of how each factor impacts the model outputs note that the values that make up the matrix m should be normalized transformed as discussed in section 3 2 below before proceeding with the analysis 3 2 transforming the distribution of sensitivity indices to be un skewed as shown in fig 3 a when analyzing many factors the distribution of factor sensitivity indices obtained during gsa will usually be heavily skewed to the right this is because as outlined in section 2 typically only a small subset of the parameters of a high dimensional model exerts a strong influence on the model response to reduce bias and to improve discrimination of factor importance across the full range of sensitivities it is helpful to transform the sensitivity indices to be more normalized as shown in fig 3 b this transformation makes it possible to better distinguish factors on the left end of the sensitivity axis such that for example groups that are moderately influential weakly influential and non influential can be easily identified as distinct categories without such transformation if the level of skewness is high there will be a tendency for the factor grouping to become highly biased with a large group at the left end containing non influential to moderately influential factors and smaller groups towards the right containing strongly influential factors leading to conclusions that may not be sufficiently granular to be informative a further reason for normalization is that our proposed grouping scheme divides factors into subsets using a similarity metric based on the euclidean distance consequently if the distribution is highly skewed the euclidean distance and resulting grouping will be strongly affected by large magnitudes e g outliers associated with a very small number of sensitivity indices normalization helps remove any such bias by ensuring that the distance metric assigns appropriate importance to each variable logarithmic square root and arcsine transformations are among the more frequently used methods to normalize data here we used the well known box cox transformation box and cox 1964 that provides a particularly flexible approach encompassing many of the aforementioned transformations logarithm square root reciprocal square root and many others given that the sensitivity indices have a skewed distribution a box cox transformation is applied parametrized by a non negative value λ 2 n s i j b λ s i j b λ 1 λ λ 0 l o g s i j b λ 0 by selection of an appropriate value for λ matrix m is transformed into matrix nm consisting of normalized sensitivity indices n s i j b where i 1 r and j 1 d the distribution of the elements of nm is approximately symmetrical i e having skewness close to zero we used nelder mead simplex direct search optimization lagarias et al 1998 to find an optimal value for λ that maximizes the following log likelihood function sakia 1992 box and cox 1964 3 l l f λ n 2 log σ ˆ λ λ 1 i 1 r j 1 d log n s i j b where σ ˆ λ is the standard deviation of the normalized sensitivity indices for a given λ note that the matrix nm of normalized sensitivity indices will be used in section 3 3 3 3 factor grouping using agglomerative hierarchal clustering having constructed the matrix nm of normalized sensitivity indices factor grouping can proceed with the aid of cluster analysis to identify groups of sensitivity indices whose values are as similar as possible while being as distinct as possible from other groups many clustering algorithms are available including hierarchical clustering k means and density based clustering see tan et al 2006 and hair et al 2006 for more details here we use the popular agglomerative hierarchical clustering method ahc johnson 1967 a bottom up approach that clusters data based on iterative merging of the two closest groups in our implementation ahc starts with d groups by assigning each column of nm each factor to a group having a single element called a leaf at each successive step the two groups that are deemed most similar are joined or agglomerated into a new larger group called a node iterative application of this procedure is continued until all the columns of nm all factors are contained in one single large group called the root the result is a clustering tree commonly referred to as a dendrogram once the dendrogram has been constructed the user can examine the resulting cluster hierarchy and cut the dendrogram at any level cutoff threshold to determine the groups a major advantage of ahc over other clustering techniques such as the k means algorithm is that it does not require a pre specification of the final i e desired number of groups a critical step in implementation of ahc is selection of a metric to quantify the pairwise similarities between factors most available methods are based on euclidean distance such as single linkage sneath 1957 complete linkage sørensen 1948 weighted average linkage sokal and michener 1958 lance and william 1967 centroid lance and william 1967 ward s method ward 1963 etc here we used ward s method because it attempts to both maximize between cluster distances and minimize within cluster distances using a single objective function called merging cost the literature suggests that ward s method typically outperforms other distance metrics and is among the most commonly used techniques see e g hands and everitt 1987 milligan and cooper 1988 ferreira and hitchcock 2009 terada 2013 3 4 a measure of robustness and convergence of gsa given an ability to monitor convergence rates of a gsa experiment the user can improve efficiency by avoiding unnecessary model runs monitoring can be performed through subjective visual inspection of the results e g vanrolleghem et al 2015 or by objective quantitative criteria e g sarrazin et al 2016 yang 2011 razavi and gupta 2016b bear in mind that convergence rates can differ from one gsa algorithm and experiment to another razavi and gupta 2016b developed the first robustness measure for factor ranking incorrectly termed a reliability measure therein based on a bootstrap method by following these steps notations appear in section 3 1 1 based on the vector of sensitivity indices s 1 d s 1 s d obtained by application of gsa to the original sample set compute the vector of the original factor rankings f r 1 d f r 1 f r d 2 using the set of bootstrap based vectors of sensitivity indices s i d b s i 1 b s i d b i 1 r construct the matrix of factor rankings f r b f r 1 1 b f r 1 d b f r r 1 b f r r d b 3 for each column j of f r b j 1 d count the number of times n j that f r i j b f r j for i 1 r 4 the robustness measure for the j th factor is estimated by computing n j r this measure can be any positive number smaller than or equal to 1 where a value equal to 1 indicates that the obtained factor ranking is fully i e 100 robust to sampling variability here we extend the above robustness measure to accommodate factor grouping our proposed measure quantifies the robustness of the factor rankings based on its membership within a factor group after performing a grouping operation on matrix nm model factors can be organized into k groups g 1 g k then we implement the following procedure to calculate the factor grouping based robustness measure for the j th factor 1 identify the group g m that contains the j th factor 2 for all factors in g m based on the original sample set compute the vector of the original factor rankings f r 1 c m g m f r 1 g m f r c m g m where c m is the number of factors in g m 3 for the j th column in f r b count the number of times q j that f r i j b for i 1 2 r is equal to either f r 1 g m f r 2 g m o r f r c m g m 4 compute the grouping based robustness measure for the j th factor by q j r for example assume that four factors of a d dimensional model are clustered into the second group as e g g 2 x 3 x 1 x 8 x 5 and that the corresponding ranks for these factors obtained from the original sample set are f r 1 4 g 2 3 4 5 6 to evaluate the robustness of factor ranking for each factor in g 2 say x 1 we count the number of times q 1 out of r bootstrap replicates that the ranking of x 1 is either 3 4 5 or 6 thus the probability that the rank of x 1 belongs to f r 1 4 g 2 is q 1 r which provides an estimate of the grouped factor ranking robustness by monitoring the convergence behavior of this measure we can terminate the algorithm at any desired level of robustness for that factor given that in high dimensional models it is the general position of the factors in the higher middle or lower parts of the ranking that is of actual interest as opposed to the precise ranking the method developed here is of more practical relevance 3 5 determining an optimal number of groups in this study we used two efficient strategies to determine the optimal number of groups the first known as the elbow method finds the number of groups by analyzing the cluster hierarchy of the dendrogram while the second chooses the number of groups by assessing the robustness measure introduced in section 3 4 3 5 1 an elbow method for finding optimal number of groups to determine the optimal number of groups one can simply examine changes in the merging cost of combining groups across all successive merging steps and select the point at which the merging cost approaches a plateau and thereafter decreases gradually this approach known as the elbow method kodinariya and makwana 2013 is based on the fact that the clustering procedure typically reaches a point elbow point after which it is no longer worth further grouping the factors in other words the merging cost corresponding to this point is good enough and the performance improvement achieved by clustering levels off as the number of groups grows further as shown in fig 4 a the elbow method can be objectively implemented by plotting the merging cost versus the number of groups and finding the elbow point of this curve which is mathematically the point of maximum curvature we apply a widely used heuristic for identifying this point based on the concept of menger curvature satopää et al 2011 which defines the curvature of a triple of points as the curvature of the circle circumscribed about those points thus the elbow point can be simply found by drawing a line from start to end of the curve and then calculating the perpendicular distance from each point to the curve the point that is farthest away from that line maximum perpendicular distance is the elbow point corresponding to the optimal cluster number in the dendrogram this point is analogous to the cutoff threshold as shown in fig 4 b 3 5 2 identifying optimal number of groups based on robustness assessment after generating a dendrogram one can cut it at different cutoff thresholds and evaluate the respective factor grouping based robustness values given a user chosen minimum acceptable robustness value the optimal number of groups can be selected such that the estimates of the robustness values are equal to or greater than the minimum value in other words at each iteration once the dendrogram has been created we cut the dendrogram at different levels to determine the maximum number of groups that guarantees the estimated grouping based robustness of all factors to be higher than the pre specified minimum value an important attribute of this method hereafter called minimum robustness method is that it provides the user with flexibility in selecting the number of groups based on the obtained robustness values however unlike the elbow method the subjectivity involved in finding the optimal number of groups is not completely removed because this method requires the user to specify a minimum acceptable robustness value in general choosing a higher value for this minimum acceptable robustness value will result in a smaller number of groups so the user might find it useful to try several values e g in range 0 50 to 0 95 and pick the one that best suits the objectives of the problem at hand 3 6 the gsa method the grouping strategy developed here is gsa method free and can be implemented in the context of any gsa algorithm including the variance based e g fast cukier et al 1973 and efast saltelli et al 1999 or density based e g δ density borgonovo 2007 and pawn pianosi and wagener 2015 methods in the next section we illustrate its use with the vars approach introduced by razavi and gupta 2016a which has been applied to several real world problems of varying dimensionality and complexity sheikholeslami et al 2017 yassin et al 2017 haghnegahdar and razavi 2017 razavi and gupta 2016b note that vars generates a set of sensitivity indices called ivars integrated variogram across a range of scales that evaluate the rates of variability in model outputs at a range of different perturbation scales the precise implementation of vars used here is the star vars method developed by razavi and gupta 2016b here we use progressive latin hypercube sampling plhs sheikholeslami and razavi 2017 to sequentially locate star centers and star sampling to sample star points in the parameter space it has been shown that plhs outperforms other traditional sampling strategies in terms of a variety of evaluation criteria e g space filling and one dimensional projection properties 4 numerical experiments in the first case study we demonstrate application of the grouping strategy to a theoretical benchmark function in the second case study we apply the method to a real world modelling problem having more than 100 uncertain parameters 4 1 illustration using the sobol g function a commonly used benchmark problem for gsa known as the sobol g function saltelli and sobol 1995 has the following mathematical form y x 1 x 2 x d j 1 d 4 x j 2 a j 1 a j where factors x j j 1 2 d are uniformly distributed over a 0 1 d hypercube and the a j are non negative constants the non linearity and non monotonicity of this function along with the availability of analytical sensitivity indices make it a suitable problem for the study of gsa techniques moreover since the sobol g function is the product of contributions from each input factor the function is non additive and features interactions of all orders archer et al 1997 the strength of contribution of each factor x j to the variability of the response y can be controlled by changing the values of the a j terms when a j is smaller the factor x j becomes more influential and thus factors can be classified in terms of their importance by assigning appropriate values to these coefficients this makes it particularly useful for evaluating the performance of any factor grouping scheme for illustrative purposes the number of factors d was set to 50 and the coefficients were chosen as listed in table 2 to form four groups of factors with differing relative sensitivities to implement star vars the number of star centers was arbitrarily set to 200 and the resolution parameter h was set to 0 05 resulting in a total of 190 200 evaluations of the sobol g function the total number of bootstrap replicates was set to r 1 000 for the sensitivity index we used ivars 50 also referred to as total variogram effect as it encompasses sensitivity analysis information across a comprehensive range of perturbation scales 4 2 modelling case study 4 2 1 model description this case study was adopted from haghnegahdar et al 2017 where the highly parameterized mesh modélisation environmentale surface et hydrologie pietroniro et al 2007 model was calibrated to the nottawasaga river basin in southern ontario canada fig 5 mesh is a semi distributed coupled land surface hydrology modelling system developed by environment and climate change canada eccc for large scale watershed modelling with consideration of lateral and cold region processes in canada mesh treats a watershed as being discretized into grid cells and accounts for within pixel heterogeneity using the concept of grouped response units grus kouwen et al 1993 the gru concept increases computational efficiency by tying most parameters to gru types only in this case study the drainage basin of nearly 2700 km2 is discretized into 20 grid cells with a spatial resolution of 0 1667 15 km the dominant land cover in the area is cropland followed by deciduous forest and grassland the dominant soil type in the area is sand followed by silt and clay loam sixteen gru types are formed by combining land cover and soil types in the region mesh version 1 3 006 was used in this study this case study data model setup etc was included in the vars tool software package more details can be found in haghnegahdar et al 2017 4 2 2 experimental setup in the sensitivity analysis study reported here a total of 111 model parameters were considered most of these parameters are related to land cover and soil classes tied to the gru types out of the existing 16 gru types only parameters corresponding to the top five gru types covering areas greater than 5 were included the rest of parameters are associated with the interflow process initial conditions ponding and channel routing parameter ranges were specified using a combination of expert knowledge previous studies dornes et al 2008 and recommendations in the manual verseghy 2012 taking the available computational budget into consideration 100 star centers were generated with a resolution of 0 1 chosen arbitrarily for implementation of star vars requiring a total of 100 000 model runs this large number of mesh runs was performed using the university of saskatchewan s linux based high performance computing cluster called plato approximately 3 minutes were required to complete a single evaluation of the model on one core of plato if a single cpu core is used the entire set of 100 000 samples will take approximately 6 85 months of computational time to run we used 160 cpu cores the nash sutcliffe coefficient of efficiency ns was used to measure daily model streamflow performance calculated for a period of three years october 2003 to september 2007 following a one year model warmup period that was excluded in the ns calculations sensitivity was assessed using the ivars 50 index of the vars methodology the total number of bootstrap replicates was set to r 1 000 5 results and discussion 5 1 factor grouping results 5 1 1 results for the sobol g function fig 6 a shows the factor grouping dendrogram for the first case study sobol g function obtained by the normalized ivars 50 sensitivity indices applied to fig 6 a the elbow method automatically identified the four groups of factors labeled as g 1 g 2 g 3 g 4 this result is consistent with the groups defined in table 2 for clarity we use lower case g to represent normalized grouping to demonstrate the value of applying a normalizing transformation prior to factor grouping fig 6 b presents the dendrogram obtained using the untransformed ivars 50 sensitivity indicies upper case g is used to represent un normalized grouping without normalization the 9 strongly influential factors of table 2 are clustered into 3 smaller groups as g 1 x 1 g 2 x 4 x 7 and g 3 x 3 x 8 x 2 x 6 x 5 x 9 while the remaining 41 factors are gathered into one big group g 4 overall the proposed factor grouping strategy makes the problem more tractable and results in a significantly higher convergence rate to elaborate consider factors x 1 and x 2 that are by design equally the most influential factors α 1 α 2 0 see table 2 while we would expect x 1 and x 2 to converge to the same sensitivity index value fig 7 shows that convergence has not been achieved after performing 190 200 function evaluations in contrast application of the grouping algorithm fig 6 a quickly clustered x 1 and x 2 into the same group g 1 the group of most influential factors indicating a high convergence rate comparison of fig 6 a and b additionally highlights the importance of normalization because without normalization x 1 and x 2 are clustered into two different groups i e g 1 and g 2 respectively see fig 6 b 5 1 2 effect of sampling variability on factor grouping results robustness of the factor grouping algorithm depends directly on how robust our estimate of the sensitivity matrix is if the gsa algorithm provides sufficiently robust estimates of the sensitivity indices the grouping results can be expected to be robust as well the ahc algorithm that has been used in our proposed grouping strategy is a deterministic algorithm without any random components as a result it provides a unique dendrogram i e clustering for a given set of sensitivity indices until these indices change therefore one can evaluate the impact of sampling variability on factor grouping results by performing multiple trials of sensitivity analysis with different sample sets and comparing the resultant dendrograms however accounting for sampling variability using multiple trials of gsa may be an impractical approach for high dimensional and computationally intensive models such as mesh on the other hand to ensure that our proposed grouping technique operates as expected various testing methods should be implemented during the numerical experiments therefore in this section we investigate the effects of sampling variability on grouping enabled vars algorithm by carrying out several trials of sensitivity analysis initiated using different random seeds we do this only for the sobol g function which is computationally cheaper than mesh by running 40 independent trials of star vars comparing multiple trials of sensitivity analysis through factor grouping requires a method for comparing dendrograms different measures for assessing the degree of similarity association between various classifications i e dendrograms exist including the cophenetic correlation coefficient sokal and rohlf 1962 and baker s index baker 1974 here we used baker s index to investigate how the results of the factor grouping algorithm vary between several replicates of the star vars given a pair of dendrograms baker s index can be calculated by taking two factors model parameters and finding the highest possible level of k number of groups created when cutting the dendrogram for which these two factors belongs to the same tree this procedure is repeated for the same two factors in the other dendrogram overall in a d dimensional model there are d d 1 2 possible combinations of such pairs of factors and accordingly all these numbers can be computed for each of the two dendrograms finally these two sets of numbers are paired with respect to the pairs of factors compared and a rank correlation e g spearman s correlation coefficient between them is calculated galili 2015 accordingly unlike other measures the baker s index is only sensitive to the relative position of branches in the dendrograms and is insensitive to the heights of the branches this important feature makes it suitable for comparing factor grouping results calculation of the baker s indices associated with pairwise comparison of the 40 dendrograms obtained from the gsa of the sobol g function resulted in a 40 by 40 symmetric correlation matrix containing the baker s indices between each dendrogram and the others as shown in fig 8 inspection of these results reveals that almost all runs yielded very similar factor groupings i e the minimum value of baker s index we obtained is 0 991 and the median is 0 999 in fig 8 this is not surprising since it has been shown that the star vars is a robust gsa technique that provides stable results razavi and gupta 2016b and consequently the comparison of the grouping results confirms the robustness of star vars to sampling variability 5 1 3 results for the mesh model the model parameter grouping obtained for the second case study mesh model is shown in fig 9 using the elbow method the 111 mesh model parameters were automatically classified into 7 groups see table a 1 of appendix a these groups are ordered based on their importance i e g 1 contains the most strongly influential parameters while parameters in g 7 are minimally influential the first group g 1 consists of 4 parameters controlling water storage and movement in the soil sdep drn river channel routing wfr22 and snow cover fraction zsnl groups 2 through 6 contain parameters related to soil and vegetation properties control of overland and interflow generation and initial conditions note that the parameters specifying initial conditions including soil initial moisture content thlq1 2 3 temperature tbar1 2 3 and initial surface ponding tpond are all located in g 4 except for initial canopy temperature tcano which is in g 7 least influential this placement of tcano may be due to the fact that model simulations start in october which is not part of the canopy growing season other parameters in g 7 are ponding parameters corresponding to gru number 7 that has small area fractions compared to that of other gru types zplg7 and zpls7 and the manning s coefficient manng used in calculation of the overland flow 5 2 assessment of the convergence and robustness of factor ranking results 5 2 1 results for the sobol g function for the first case study fig 10 a illustrates the convergence behavior of the proposed robustness measure while fig 10 b shows the robustness values based on individual factor ranking computed as described in razavi and gupta 2016b whereas the robustness values estimated based on individual factor rankings actually deteriorates for the first 28 350 evaluations clearly indicating insufficient sample size the factor grouping converges quickly towards probability 1 i e 100 after about only half of that number 15 000 and from 28 350 function evaluations onwards the grouping is stable fig 10 a note also that the robustness measure based on individual factor rankings continue to vary between 0 18 and 1 median 0 55 even for well over 150 000 evaluations 5 2 2 results for the mesh model the robustness assessment results for the second case study are shown in fig 11 a and b after 90 000 function evaluations the estimated factor grouping based robustness values are all higher than 0 50 fig 11 a whereas for individual factor ranking based values they continue to vary between 0 04 and 1 0 fig 11 b for clarity fig 11 c shows the trajectories of the medians of both the individual red dashed line and grouping based blue solid line results notice that the median of the grouping based robustness values is already quite high 0 80 after 20 000 function evaluations and increases rapidly thereafter to be above 0 90 from 50 000 evaluations on finally reaching to 0 99 at 100 000 model runs given there are 111 parameters this means the grouping based approach has achieved greater than 90 robustness in identifying the 55 most sensitive parameters of this model with only 40 000 50 000 model runs in contrast the median of the robustness values for the individual factor ranking based method remains low and is only 0 17 at the termination of the experiment in support of this fig 12 a g shows the trajectories of robustness values for each member of each of the 7 identified groups from most strongly influential to non influential we see clearly that groups 1 fig 12 a and 2 fig 12 b the two most strongly influential and group 7 fig 12 g the least influential have been pretty well established with high robustness after about 50 000 function evaluations in contrast the robustness values for members of the intermediate groups 3 6 fig 12 c f tend to be more variable with some members having high robustness and others having relatively lower robustness values for the latter this indicates that their group membership is less certain and that they may continue to shift between groups as the sampling proceeds nonetheless it is clear that the most and least influential parameter groups are relatively well established quite early in the gsa procedure finally fig 12 h shows together the trajectory of median robustness values for each of the seven groups this plot can be useful for oversight monitoring of the convergence of the factor grouping procedure overall these results clearly illustrate the value of using factor grouping based robustness estimates for monitoring convergence of gsa applied to high dimensional models as indicated in fig 2 when convergence has not yet been achieved the user can iterate by increasing the original sample size using a sequential sampling scheme such as plhs 5 3 comparison of the elbow method and minimum robustness method in finding optimal number of groups fig 13 top panel illustrates the evolution of the optimal number of groups obtained by the elbow method as the star vars algorithm progresses fig 13 a and b the grouping algorithm evolves and discovers groups of parameters that share specific properties in terms of their sensitivity to further investigate the performance of the elbow method the merging costs distance metric versus the number of groups are plotted for the sobol g function fig 13 c and mesh model fig 13 d when the number of function evaluations is 190 200 and 100 000 respectively fig 13 bottom panel confirms the ability of the elbow method in successfully determining the optimal number of groups by finding the elbow point of the curve point of maximum curvature for the sobol g function the elbow method converges rather quickly to an optimal number of groups however for the mesh model the optimal number continues to vary over the range 7 13 for the number of function evaluation examined moreover to examine the performance of the minimum robustness method in determining optimal number of groups we calculated the maximum number of groups that is required to have minimum robustness values of 0 90 for the sobol g function fig 14 a and 0 45 for the mesh model fig 14 b as the number of function evaluations grows the top panel of fig 14 indicates that the number of groups tends to be small for low numbers of function evaluations and tends to increase as the number of function evaluations increases finally histograms of the estimated robustness values are shown in fig 14 c and d for the mesh model for 40 000 and 90 000 function evaluations when the number of groups is equal to 9 and 10 respectively although the minimum desired robustness level was set to 0 45 the estimated robustness value for most of the parameters is greater than 0 60 in fact when the number of function evaluations is 40 000 only two parameters out of 111 parameters have robustness values less than 0 60 while after 90 000 function evaluations except one parameter all the parameters have the robustness values greater than 0 60 6 conclusions and future work global sensitivity analysis gsa is a powerful tool for deepening our understanding of dynamical earth and environmental systems dees providing information helpful for model development parameterization calibration and data acquisition advanced dees models are commonly characterized by large parameter problem spaces and high computational overheads which impede the effective implementation of the modern gsa techniques because an extensive sensitivity analysis often requires a computationally infeasible number of model runs to break down this barrier we have introduced an automated factor grouping strategy that can be used with any gsa algorithm to reliably cluster input factors into groups of different sizes using information gained during the gsa our proposed grouping approach is based on the use of an efficient strategy elbow method and or minimum robustness method to determine the optimal number of groups additionally we developed and tested a new measure of robustness based on factor grouping to monitor and evaluate convergence of the gsa algorithms to illustrate the approach we coupled the proposed factor grouping algorithm with the vars variogram based gsa technique and demonstrated its utility for parameter sensitivity analysis of two high dimensional case studies the 50 parameter sobol g function and the 111 parameter mesh large scale land surface hydrology model for the sobol g function we also assessed the effect of sampling variability on the grouping enabled vars algorithm by running multiple replicates of the algorithm with different original sample sets the results of our experiment reveal the robustness of the grouping strategy combined with vars method for gsa of course to better understand how robustness of the chosen gsa method can affect the robustness of factor grouping results several gsa methods should be tested in conjunction with the proposed grouping strategy overall our results confirm that the strategy of grouping factors can significantly reduce the computational effort required to perform gsa on high dimensional models in the case of our mesh model by as much as 50 this is mainly because factor groupings typically converge faster than factor sensitivity indices a further potential benefit is that the proposed algorithm can provide information useful for reducing the complexity of the problem in the follow up experiments e g model calibration or model order reduction by identifying dominant influential groups of factors that significantly contribute to the variability in the model outputs future work should include an extension of the factor grouping strategy to multi criteria sensitivity analysis of high dimensional models to conduct a comprehensive gsa it is necessary to fully consider the multi output nature of dees models klepper 1997 gupta et al 1998 rosolem et al 2012 haghnegahdar et al 2017 our proposed method for factor grouping can in principle be used in conjunction with multi criteria sensitivity analysis to study parameter properties while recognizing multivariate aspects of the model behavior thereby better supporting model development and understanding one possible option is to employ multi criteria gsa techniques which measure the global contribution of each factor to multivariate outputs through estimating generalized sensitivity indices having generated the sensitivity matrix consisting of these generalized indices the grouping algorithm can use cluster analysis to identify distinct groups of factors as described in section 3 see fig 2 gsa methods such as mogsa multi objective generalized sensitivity analysis method bastidas et al 1999 liu et al 2004 which is an extension of the regional sensitivity analysis and the generalization of the variance based sobol gsa for multivariate outputs lamboni et al 2011 gamboa et al 2014 may be useful in this regard the aforementioned methods apply bootstrapping to ensure statistical robustness of the generalized sensitivity indices and accordingly can yield robust groups of parameters software availability a matlab code for the proposed factor grouping algorithm is included in the vars tool package which is a set of programs for next generation sensitivity and uncertainty analysis the software is freely available for noncommercial use upon request from the authors and can be downloaded from http vars tool com acknowledgments rs gratefully acknowledges the scholarship provided by the school of environment and sustainability through the canada excellence research chair in water security sr has been supported in part by nserc natural sciences and engineering research council of canada discovery grant hvg received partial support from the australian research council through the centre of excellence for climate system science grant ce110001028 appendix a parameters of the mesh model and their corresponding groups the 7 groups of parameters for the mesh model see fig 8 are listed in table a 1 the description of parameters and their feasible ranges can be found in haghnegahdar et al 2017 table a 1 grouping of 111 parameters of the mesh these groups are numbered in order of importance table a 1 group number parameters 1 sdepc wfr22 zsnl3 drnc 2 vpdac zpls4 sdepd rootc sdepg xslpc ratios zsnl4 zsnl1 zplg4 ddenc vpdad lamind vpdag lnz0d 3 claysa3 sandsa2 lamaxc xslpd sandsa1 rsmnc rootg zsnl11 zsnl7 xslpg zplg3 zpls3 zpls1 zplg1 ddend claysi3 sandsi3 lnz0g sandsa3 claysa2 claysa1 qa50c drng vpdbc drnd ddeng 4 lamaxg thlq3 claysi1 sandsi2 sandcl3 qa50d grkfc lnz0c alicc alvcc claysi2 alicg sandcl2 sandcl1 tbar2 psgac thlq1 orgsi3 psgbc thlq2 tbar3 tpond tbar1 cmasc mannc zpond ratiosi qa50g rsmng rsmnd orgsi2 orgsi1 5 ratiocl claycl3 grkfd cmasd orgsa3 orgsa2 orgsa1 orgcl1 orgcl2 claycl2 orgcl3 claycl1 alicd lamaxd alvcg grkfg alvcd vpdbg cmasg 6 zpls11 vpdbd zplg11 psgag psgbg laming psgad psgbd manng rootd 7 zplg7 zpls7 tcano mannd 
26293,dynamical earth and environmental systems models are typically computationally intensive and highly parameterized with many uncertain parameters together these characteristics severely limit the applicability of global sensitivity analysis gsa to high dimensional models because very large numbers of model runs are typically required to achieve convergence and provide a robust assessment paradoxically only 30 percent of gsa applications in the environmental modelling literature have investigated models with more than 20 parameters suggesting that gsa is under utilized on problems for which it should prove most useful we develop a novel grouping strategy based on bootstrap based clustering that enables efficient application of gsa to high dimensional models we also provide a new measure of robustness that assesses gsa stability and convergence for two models having 50 and 111 parameters we show that grouping enabled gsa provides results that are highly robust to sampling variability while converging with a much smaller number of model runs keywords global sensitivity analysis curse of dimensionality computationally intensive simulations factor grouping robustness convergence 1 introduction 1 1 motivation computational models are widely used to understand and simulate the complex physical behaviors of dynamical earth and environmental systems dees bennett et al 2013 provenzale 2014 bathiany et al 2016 by enabling prediction and scenario analysis regarding the quality and quantity of earth s future resources poff et al 2015 guo et al 2016 maier et al 2016 such models have become essential to management and decision making under uncertainty and non stationary conditions however the drive to incorporate our ever growing understanding of underlying system processes and their feedback mechanisms leads to progressively more complex and computationally intensive model formulations with growth in complexity and presumably fidelity it is now not uncommon for dees models to contain hundreds and even thousands of parameters and factors whose values are uncertain and need to be characterized global sensitivity analysis gsa has proven to be an effective means for characterizing the impact and significance of uncertainty in various model components e g parameters initial conditions boundary conditions etc on model behavior and predictions razavi and gupta 2015 most well established gsa techniques are based on one of the following two approaches an analysis of partial derivatives such as in the method of morris e g morris 1991 campolongo et al 2007 sobol and kucherenko 2009 or an analysis of variance such as in the method of sobol e g sobol 1993 homma and saltelli 1996 tarantola et al 2006 however it is not uncommon for methods based on these two approaches to give conflicting assessments and razavi and gupta 2016a b recently showed that both strategies are actually special cases of a more comprehensive variogram based approach that directly accounts for spatial structure of the model response surfaces the new approach entitled variogram analysis of response surfaces vars is a unifying theory that bridges across the derivative and variance based approaches by introducing the notion of perturbation scale haghnegahdar and razavi 2017 gsa techniques are now widely used for a variety of purposes including uncertainty apportionment e g marino et al 2008 borgonovo et al 2012 parameter screening e g trocine and malone 2001 touzani and busby 2014 and diagnostic testing e g saltelli et al 2006 gupta et al 2008 haghnegahdar et al 2017 however two major interrelated challenges limit their application to advanced dees models namely i the curse of dimensionality and ii computational expense the former refers to the fact that as the number of uncertain parameters factors increases the volume of the problem space increases so rapidly that any attempt to investigate and characterize it in a stable robust and statistically sound manner requires an exponentially increasing sample size the latter refers to the typically computationally intensive nature of dees models leading to long run times that together with the former can make any meaningful analysis of such models computationally prohibitive these two challenges are the primary reason why gsa applications reported in the literature are often limited to relatively simple models having smaller numbers of uncertain parameters see section 2 however this is paradoxical to the underlying goals of gsa as a means to facilitate the development understanding and use of more realistic dees models in this regard it is interesting to note that a major goal of gsa commonly stated in the literature is to help reduce the dimensionality of a problem by identifying non influential parameters however as the dimensionality of the problem space grows beyond a few tens of parameters most gsa methods quickly become handicapped by their need for impractically large sample sizes due to the excessively large computational times involved and are therefore unable to provide robust assessments of sensitivity that have an adequate level of confidence razavi and gupta 2016b a further class of sensitivity analysis approaches known as metamodel enabled gsa can substantially reduce the number of model runs needed to estimate sensitivity indices but is mostly restricted to low or moderate dimensionality problems in fact as discussed in razavi et al 2012 about 85 of metamodel applications have been on problems having less than 20 factors in higher dimensions the performance of metamodel enabled gsa algorithms can substantially deteriorate due to over fitting becker 2015 as another potential remedy analysts sometimes use local sensitivity analysis lsa methods that require lower computational demand however it is well known that lsas provide inadequate assessments that can often be misleading saltelli and annoni 2010 in this paper we present an approach that addresses the problem of robust sensitivity assessment for high dimensional problems there are two main aspects to our approach the first aspect is based on the sparsity of factors principle otherwise known as the pareto principle which states that a small subset of factors is often responsible for most of the system output uncertainty box and meyer 1986 to exploit this principle we need a way to identify which of the individual factors have similar properties in terms of their influence on model output variations in doing so we also recognize that when the number of factors is very large the user is typically not interested in an exact ranking of factor importance for example with 100 factors it may not matter whether a given factor has a sensitivity ranking of 49 or 50 instead it may be more profitable to use the available computational budget to reliably categorize factors into a small number of distinct groups for example these could be labeled as strongly influential influential moderately influential weakly influential and non influential the second aspect deals with an essential but often neglected element of gsa which is that of characterizing and improving the robustness of the gsa results this is extremely important given that gsa is a sampling based technique and as such is prone to statistical uncertainty due to sampling variability i e the results will be sensitive to randomness in the selection of the sample hence robustness can be defined as the stability of the gsa results i e the degree of insensitivity to sampling variation in other words lower variability of the results obtained over multiple trials of the algorithm performed with different identically distributed sample sets indicates a higher degree of robustness see e g montgomery 2008 razavi and gupta 2016b developed one of the first techniques to assess the robustness of gsa factor rankings incorrectly termed reliability measure in their paper based on the use of bootstrapping here we extend that approach and integrate it with optimal factor grouping i e the first aspect mentioned above to provide a gsa solution for high dimensional problems that are often intractable with traditional approaches 1 2 objectives and scope the primary goal of this paper is to introduce an automated factor grouping strategy that is based on the integration of clustering with bootstrapping the method is designed to group input factors into a certain number of groups based on information gained during gsa where the resulting groups can be of any size importantly if the number of groups is not pre specified the algorithm efficiently determines an optimal number of groups further to evaluate performance of the grouping based gsa we develop a measure of robustness that provides a way to monitor convergence of the gsa results the overall procedure can be used in conjunction with any gsa technique here we demonstrate the utility of this approach on two high dimensional problems by coupling it with the vars methodology the rest of this paper is structured as follows in section 2 we discuss the curse of dimensionality challenge associated with gsa of high dimensional problems review existing strategies for factor grouping and discuss their limitations in section 3 we introduce the new grouping technique and explain the details of its implementation the two case studies are developed in section 4 and the numerical results and analyses are presented in section 5 finally conclusions and future recommendations appear in section 6 2 review of the literature 2 1 difficulties associated with gsa of high dimensional problems in the vast majority of gsa studies reported in the literature the dimensionality of the problem space has been quite low fig 1 shows a cumulative distribution function cdf of the factor space dimensionality for environmental modelling studies during the period 2007 2017 that reported applications of gsa in 70 of the cases the number of factors was less than or equal to 20 while 85 had less than 40 and almost all 97 had less than 90 factors only very few studies used models with more than 100 factors radomyski et al 2016 used 156 tang et al 2007 used 403 and herman et al 2013 used 1092 so although one of the main goals of gsa is to assist in reducing the dimensionality of a problem by screening out non influential parameters the evidence suggests that it is seldom applied to the high dimensional problems in which it is most needed the reason for this can in part be attributed to the curse of dimensionality which coupled with the high computational costs typically associated with running complex dees models makes application of gsa to high dimensional problems very expensive the curse of dimensionality manifests through the fact that to achieve stable and robust results current gsa techniques require large numbers of sample points i e model runs to be drawn in some representative manner from the factor space while this issue can be partially addressed using optimized sampling algorithms such as progressive latin hypercube sampling plhs sheikholeslami and razavi 2017 and good lattice points glp gong et al 2016 the computational cost remains high song et al 2016 to further synthesize the status quo table 1 provides an overview of recent papers that have used gsa to study higher dimensional models here we have arbitrarily selected only studies where the number of factors was greater than or equal to 40 it is important to note that the sample size in these studies was typically chosen based on available computational budget without considerations of gsa stability and convergence consequently the results are likely to have been highly sensitive to sampling variability i e may not have been robust studies of the convergence behavior of various gsa techniques generally show that factor ranking order of relative sensitivity typically converges more quickly than factor sensitivity indices computed by the gsa perhaps as expected and that factor ranking is more robust to sampling variability benedetti et al 2011 yang 2011 cosenza et al 2013 wang et al 2013 vanrolleghem et al 2015 razavi and gupta 2016b sarrazin et al 2016 overall this fact indicates that if a modeler is interested in factor screening or prioritization rather than in generating accurate estimates for the sensitivity indices the number of required model simulations and hence computational cost can be reduced interestingly vanrolleghem et al 2015 and wang et al 2013 found that for the gsa methods they used sensitivity index convergence rates were typically slowest for the factors having the highest importance while conversely sarrazin et al 2016 nossent et al 2011 and yang 2011 reported that the sensitivity index convergence rates were slowest for factors having lowest importance this lack of agreement in their results suggests that convergence rates may be case specific and depend on the other aspects of the problem at hand including but not limited to sampling variability and the choice of gsa algorithm from this review of the literature table 1 we make three observations i regardless of the gsa method used it is typical for only a small sub group of factors on average 20 to exert significant control over variations in the model outputs a manifestation of the sparsity of factors principle ii historically the grouping of factors into strongly influential versus non influential has typically been done in a subjective and case specific manner the most common approach being to specify a threshold sensitivity value and to group factors based on their sensitivities relative to the threshold value iii when different gsa methods were applied to the same problem while actual rankings varied the factor rankings were typically such that the relative positions of factors in the higher middle or lower parts of the rankings were often quite similar e g cosenza et al 2013 vanuytrecht et al 2014 sheikholeslami et al 2017 in other words different methods generally tended to identify the same groups of factors in terms of the strength of their influence on model response from a practical point of view this suggests that one might focus on whether a factor belongs to a high medium or low influence group rather than on its exact ranking particularly given the demonstrated effects of sampling variability on estimated rankings in high dimensional problems grouping input factors of similar sensitivity can help reduce the effective dimensionality of the factor space this is useful because when the problem has more than 20 factors it can be difficult to analyze the gsa results for extraction of relevant information it seems beneficial therefore to categorize factors into subsets to facilitate interpretation together these issues highlight the need for an effective automated and non subjective strategy for grouping model factors based on information developed during the execution of a gsa algorithm in the next subsection we review the few strategies for grouping that have been proposed in the literature discuss their characteristics and identify shortcomings 2 2 review of existing grouping strategies for gsa despite significant research dedicated to the advancement of gsa there is a surprising paucity of literature on the development of efficient grouping strategies relevant studies have focused mainly on enabling gsa methods to compute an overall sensitivity measure for a pre identified group of factors for example in the variance based sobol method sobol 1993 the set of uncertain factors x can be partitioned into pre specified groups x u and x w where x u x w x and x u x w such that the variance of the response v a r y can be decomposed as v x u v x w v x u x w 1 where v x w v a r x w e x u y x w v a r y is the normalized variance of the conditional expectation that measures the first order effect of x w on the model output this decomposition can be performed for any number of factors saltelli et al 2006 extending this saltelli et al 2008 provided a multi stage factor grouping strategy to perform variance based sensitivity analysis based on the concept presented above in their approach the number of groups is specified a priori and then the members of each group are populated by testing a variety of combinations similarly campolongo et al 2007 modified the morris elementary effects method morris 1991 to work with pre specified groups of factors for which total sensitivity indices can be estimated ciuffo and azevedo 2014 patelli et al 2010 in all the grouping techniques reviewed here the factor grouping scheme is determined before application of gsa based on the nature of the problem physical interpretation previous experience intuition randomized grouping etc another commonly used approach is to group factors based on arbitrary pre specified sensitivity index thresholds eweys et al 2017 hou et al 2015 göhler et al 2013 tang et al 2007 for example in variance based gsa a group of factors can be deemed strongly influential if each member individually contributes at least 10 of the overall model output variance or weakly non influential if each member individually contributes less than 1 of the model variance note that these thresholds are selected rather arbitrarily however in the morris method of elementary effects the definition of thresholds is necessarily case specific because the estimated sensitivity indices can have different ranges of variation depending on the model output and particularities of the case study for screening purposes identifying non influential factors khorashadi zadeh et al 2017 proposed the interesting approach of adding a dummy variable i e a variable known a priori to be fully non influential to the set of factors under investigation factor screening has also been accommodated by applying a statistical testing approach originally introduced by andres 1997 and further extended by sarrazin et al 2016 tang et al 2007 and nossent et al 2011 as a more objective way to achieve factor grouping klepper 1997 used a clustering method to categorize model factors based on the sensitivity analysis results to our knowledge this method is the only method to date that is based on the analysis on a clustering concept it does not however provide an optimal grouping and is prone to uncertainty associated with sampling variability see section 3 1 a major disadvantage with a priori specification of the grouping is that the results can depend strongly on the user s selection of groups a task that can become complicated if not impractical as the number of factors grows overall our review leads us to conclude that an effective grouping strategy must address the following questions as part of the gsa methodology itself i what is an optimal subdivision of model factors into a given number of groups ii what is the optimal number of groups iii to what extent are the grouping results robust regarding the first question the members of a factor group should be as similar in some sense as possible while being as distinct as possible from the members of the other groups to achieve this a metric must be defined that quantifies similarity between input factors according to the manner in which they influence the model outputs regarding the second question removal of subjectivity about the number of groups should be somehow based on the goal of obtaining a maximum level of homogeneity within groups and distinction across groups finally the third question addresses the need for some degree of confidence in the grouping results meaning that the results are not overly sensitive to sampling variability none of the aforementioned grouping strategies addresses these challenges explicitly and in a systematic manner this gap motivated our development of the automated factor grouping method introduced in the next section 3 the proposed factor grouping strategy our proposed method for factor grouping fig 2 combines agglomerative hierarchical clustering with bootstrapping and introduces a new robustness measure that enables an objective assessment of gsa convergence the algorithm consists of five steps details of each are provided in the following subsections 3 1 generating the sensitivity matrix by bootstrapping in the traditional approach to gsa the user runs a model n times using the original sample of n points drawn from factor space and then a vector of sensitivity indices for the d factors is computed using the information provided by these points s 1 d s 1 s d in general the size n of the randomly drawn sample used to estimate s 1 d will be limited and so due to sampling variability we can expect a degree often considerable of statistical uncertainty to be associated with s 1 d the distribution free and easy to implement bootstrap method efron 1979 can be used to estimate the magnitude of that uncertainty davison et al 2003 example applications in the gsa context can be found in razavi and gupta 2016b and archer et al 1997 importantly in bootstrapping the set of bootstrap samples is extracted from the original sample set and thus additional model runs are not necessary we apply the following two steps to implement the bootstrap technique 1 set the number of bootstrap re samplings replicates to r and randomly draw with replacement from original sample set r bootstrapped samples each having the same size of n as the original sample the result is the so called set of r bootstrap samples note that r is selected to be some arbitrarily large number e g 1000 as has been commonly used in the literature 2 for each bootstrap sample i i 1 r in the set recompute the vector of sensitivity indices s i d b s i 1 b s i d b using the gsa method following the above procedure a two dimensional r d bootstrapped sensitivity matrix m containing r bootstrap replicates for the sensitivity indices can be formed as 1 m s 1 1 b s 1 d b s r 1 b s r d b because the matrix m contains information regarding estimation uncertainty it provides valuable information that enables a robust evaluation of how each factor impacts the model outputs note that the values that make up the matrix m should be normalized transformed as discussed in section 3 2 below before proceeding with the analysis 3 2 transforming the distribution of sensitivity indices to be un skewed as shown in fig 3 a when analyzing many factors the distribution of factor sensitivity indices obtained during gsa will usually be heavily skewed to the right this is because as outlined in section 2 typically only a small subset of the parameters of a high dimensional model exerts a strong influence on the model response to reduce bias and to improve discrimination of factor importance across the full range of sensitivities it is helpful to transform the sensitivity indices to be more normalized as shown in fig 3 b this transformation makes it possible to better distinguish factors on the left end of the sensitivity axis such that for example groups that are moderately influential weakly influential and non influential can be easily identified as distinct categories without such transformation if the level of skewness is high there will be a tendency for the factor grouping to become highly biased with a large group at the left end containing non influential to moderately influential factors and smaller groups towards the right containing strongly influential factors leading to conclusions that may not be sufficiently granular to be informative a further reason for normalization is that our proposed grouping scheme divides factors into subsets using a similarity metric based on the euclidean distance consequently if the distribution is highly skewed the euclidean distance and resulting grouping will be strongly affected by large magnitudes e g outliers associated with a very small number of sensitivity indices normalization helps remove any such bias by ensuring that the distance metric assigns appropriate importance to each variable logarithmic square root and arcsine transformations are among the more frequently used methods to normalize data here we used the well known box cox transformation box and cox 1964 that provides a particularly flexible approach encompassing many of the aforementioned transformations logarithm square root reciprocal square root and many others given that the sensitivity indices have a skewed distribution a box cox transformation is applied parametrized by a non negative value λ 2 n s i j b λ s i j b λ 1 λ λ 0 l o g s i j b λ 0 by selection of an appropriate value for λ matrix m is transformed into matrix nm consisting of normalized sensitivity indices n s i j b where i 1 r and j 1 d the distribution of the elements of nm is approximately symmetrical i e having skewness close to zero we used nelder mead simplex direct search optimization lagarias et al 1998 to find an optimal value for λ that maximizes the following log likelihood function sakia 1992 box and cox 1964 3 l l f λ n 2 log σ ˆ λ λ 1 i 1 r j 1 d log n s i j b where σ ˆ λ is the standard deviation of the normalized sensitivity indices for a given λ note that the matrix nm of normalized sensitivity indices will be used in section 3 3 3 3 factor grouping using agglomerative hierarchal clustering having constructed the matrix nm of normalized sensitivity indices factor grouping can proceed with the aid of cluster analysis to identify groups of sensitivity indices whose values are as similar as possible while being as distinct as possible from other groups many clustering algorithms are available including hierarchical clustering k means and density based clustering see tan et al 2006 and hair et al 2006 for more details here we use the popular agglomerative hierarchical clustering method ahc johnson 1967 a bottom up approach that clusters data based on iterative merging of the two closest groups in our implementation ahc starts with d groups by assigning each column of nm each factor to a group having a single element called a leaf at each successive step the two groups that are deemed most similar are joined or agglomerated into a new larger group called a node iterative application of this procedure is continued until all the columns of nm all factors are contained in one single large group called the root the result is a clustering tree commonly referred to as a dendrogram once the dendrogram has been constructed the user can examine the resulting cluster hierarchy and cut the dendrogram at any level cutoff threshold to determine the groups a major advantage of ahc over other clustering techniques such as the k means algorithm is that it does not require a pre specification of the final i e desired number of groups a critical step in implementation of ahc is selection of a metric to quantify the pairwise similarities between factors most available methods are based on euclidean distance such as single linkage sneath 1957 complete linkage sørensen 1948 weighted average linkage sokal and michener 1958 lance and william 1967 centroid lance and william 1967 ward s method ward 1963 etc here we used ward s method because it attempts to both maximize between cluster distances and minimize within cluster distances using a single objective function called merging cost the literature suggests that ward s method typically outperforms other distance metrics and is among the most commonly used techniques see e g hands and everitt 1987 milligan and cooper 1988 ferreira and hitchcock 2009 terada 2013 3 4 a measure of robustness and convergence of gsa given an ability to monitor convergence rates of a gsa experiment the user can improve efficiency by avoiding unnecessary model runs monitoring can be performed through subjective visual inspection of the results e g vanrolleghem et al 2015 or by objective quantitative criteria e g sarrazin et al 2016 yang 2011 razavi and gupta 2016b bear in mind that convergence rates can differ from one gsa algorithm and experiment to another razavi and gupta 2016b developed the first robustness measure for factor ranking incorrectly termed a reliability measure therein based on a bootstrap method by following these steps notations appear in section 3 1 1 based on the vector of sensitivity indices s 1 d s 1 s d obtained by application of gsa to the original sample set compute the vector of the original factor rankings f r 1 d f r 1 f r d 2 using the set of bootstrap based vectors of sensitivity indices s i d b s i 1 b s i d b i 1 r construct the matrix of factor rankings f r b f r 1 1 b f r 1 d b f r r 1 b f r r d b 3 for each column j of f r b j 1 d count the number of times n j that f r i j b f r j for i 1 r 4 the robustness measure for the j th factor is estimated by computing n j r this measure can be any positive number smaller than or equal to 1 where a value equal to 1 indicates that the obtained factor ranking is fully i e 100 robust to sampling variability here we extend the above robustness measure to accommodate factor grouping our proposed measure quantifies the robustness of the factor rankings based on its membership within a factor group after performing a grouping operation on matrix nm model factors can be organized into k groups g 1 g k then we implement the following procedure to calculate the factor grouping based robustness measure for the j th factor 1 identify the group g m that contains the j th factor 2 for all factors in g m based on the original sample set compute the vector of the original factor rankings f r 1 c m g m f r 1 g m f r c m g m where c m is the number of factors in g m 3 for the j th column in f r b count the number of times q j that f r i j b for i 1 2 r is equal to either f r 1 g m f r 2 g m o r f r c m g m 4 compute the grouping based robustness measure for the j th factor by q j r for example assume that four factors of a d dimensional model are clustered into the second group as e g g 2 x 3 x 1 x 8 x 5 and that the corresponding ranks for these factors obtained from the original sample set are f r 1 4 g 2 3 4 5 6 to evaluate the robustness of factor ranking for each factor in g 2 say x 1 we count the number of times q 1 out of r bootstrap replicates that the ranking of x 1 is either 3 4 5 or 6 thus the probability that the rank of x 1 belongs to f r 1 4 g 2 is q 1 r which provides an estimate of the grouped factor ranking robustness by monitoring the convergence behavior of this measure we can terminate the algorithm at any desired level of robustness for that factor given that in high dimensional models it is the general position of the factors in the higher middle or lower parts of the ranking that is of actual interest as opposed to the precise ranking the method developed here is of more practical relevance 3 5 determining an optimal number of groups in this study we used two efficient strategies to determine the optimal number of groups the first known as the elbow method finds the number of groups by analyzing the cluster hierarchy of the dendrogram while the second chooses the number of groups by assessing the robustness measure introduced in section 3 4 3 5 1 an elbow method for finding optimal number of groups to determine the optimal number of groups one can simply examine changes in the merging cost of combining groups across all successive merging steps and select the point at which the merging cost approaches a plateau and thereafter decreases gradually this approach known as the elbow method kodinariya and makwana 2013 is based on the fact that the clustering procedure typically reaches a point elbow point after which it is no longer worth further grouping the factors in other words the merging cost corresponding to this point is good enough and the performance improvement achieved by clustering levels off as the number of groups grows further as shown in fig 4 a the elbow method can be objectively implemented by plotting the merging cost versus the number of groups and finding the elbow point of this curve which is mathematically the point of maximum curvature we apply a widely used heuristic for identifying this point based on the concept of menger curvature satopää et al 2011 which defines the curvature of a triple of points as the curvature of the circle circumscribed about those points thus the elbow point can be simply found by drawing a line from start to end of the curve and then calculating the perpendicular distance from each point to the curve the point that is farthest away from that line maximum perpendicular distance is the elbow point corresponding to the optimal cluster number in the dendrogram this point is analogous to the cutoff threshold as shown in fig 4 b 3 5 2 identifying optimal number of groups based on robustness assessment after generating a dendrogram one can cut it at different cutoff thresholds and evaluate the respective factor grouping based robustness values given a user chosen minimum acceptable robustness value the optimal number of groups can be selected such that the estimates of the robustness values are equal to or greater than the minimum value in other words at each iteration once the dendrogram has been created we cut the dendrogram at different levels to determine the maximum number of groups that guarantees the estimated grouping based robustness of all factors to be higher than the pre specified minimum value an important attribute of this method hereafter called minimum robustness method is that it provides the user with flexibility in selecting the number of groups based on the obtained robustness values however unlike the elbow method the subjectivity involved in finding the optimal number of groups is not completely removed because this method requires the user to specify a minimum acceptable robustness value in general choosing a higher value for this minimum acceptable robustness value will result in a smaller number of groups so the user might find it useful to try several values e g in range 0 50 to 0 95 and pick the one that best suits the objectives of the problem at hand 3 6 the gsa method the grouping strategy developed here is gsa method free and can be implemented in the context of any gsa algorithm including the variance based e g fast cukier et al 1973 and efast saltelli et al 1999 or density based e g δ density borgonovo 2007 and pawn pianosi and wagener 2015 methods in the next section we illustrate its use with the vars approach introduced by razavi and gupta 2016a which has been applied to several real world problems of varying dimensionality and complexity sheikholeslami et al 2017 yassin et al 2017 haghnegahdar and razavi 2017 razavi and gupta 2016b note that vars generates a set of sensitivity indices called ivars integrated variogram across a range of scales that evaluate the rates of variability in model outputs at a range of different perturbation scales the precise implementation of vars used here is the star vars method developed by razavi and gupta 2016b here we use progressive latin hypercube sampling plhs sheikholeslami and razavi 2017 to sequentially locate star centers and star sampling to sample star points in the parameter space it has been shown that plhs outperforms other traditional sampling strategies in terms of a variety of evaluation criteria e g space filling and one dimensional projection properties 4 numerical experiments in the first case study we demonstrate application of the grouping strategy to a theoretical benchmark function in the second case study we apply the method to a real world modelling problem having more than 100 uncertain parameters 4 1 illustration using the sobol g function a commonly used benchmark problem for gsa known as the sobol g function saltelli and sobol 1995 has the following mathematical form y x 1 x 2 x d j 1 d 4 x j 2 a j 1 a j where factors x j j 1 2 d are uniformly distributed over a 0 1 d hypercube and the a j are non negative constants the non linearity and non monotonicity of this function along with the availability of analytical sensitivity indices make it a suitable problem for the study of gsa techniques moreover since the sobol g function is the product of contributions from each input factor the function is non additive and features interactions of all orders archer et al 1997 the strength of contribution of each factor x j to the variability of the response y can be controlled by changing the values of the a j terms when a j is smaller the factor x j becomes more influential and thus factors can be classified in terms of their importance by assigning appropriate values to these coefficients this makes it particularly useful for evaluating the performance of any factor grouping scheme for illustrative purposes the number of factors d was set to 50 and the coefficients were chosen as listed in table 2 to form four groups of factors with differing relative sensitivities to implement star vars the number of star centers was arbitrarily set to 200 and the resolution parameter h was set to 0 05 resulting in a total of 190 200 evaluations of the sobol g function the total number of bootstrap replicates was set to r 1 000 for the sensitivity index we used ivars 50 also referred to as total variogram effect as it encompasses sensitivity analysis information across a comprehensive range of perturbation scales 4 2 modelling case study 4 2 1 model description this case study was adopted from haghnegahdar et al 2017 where the highly parameterized mesh modélisation environmentale surface et hydrologie pietroniro et al 2007 model was calibrated to the nottawasaga river basin in southern ontario canada fig 5 mesh is a semi distributed coupled land surface hydrology modelling system developed by environment and climate change canada eccc for large scale watershed modelling with consideration of lateral and cold region processes in canada mesh treats a watershed as being discretized into grid cells and accounts for within pixel heterogeneity using the concept of grouped response units grus kouwen et al 1993 the gru concept increases computational efficiency by tying most parameters to gru types only in this case study the drainage basin of nearly 2700 km2 is discretized into 20 grid cells with a spatial resolution of 0 1667 15 km the dominant land cover in the area is cropland followed by deciduous forest and grassland the dominant soil type in the area is sand followed by silt and clay loam sixteen gru types are formed by combining land cover and soil types in the region mesh version 1 3 006 was used in this study this case study data model setup etc was included in the vars tool software package more details can be found in haghnegahdar et al 2017 4 2 2 experimental setup in the sensitivity analysis study reported here a total of 111 model parameters were considered most of these parameters are related to land cover and soil classes tied to the gru types out of the existing 16 gru types only parameters corresponding to the top five gru types covering areas greater than 5 were included the rest of parameters are associated with the interflow process initial conditions ponding and channel routing parameter ranges were specified using a combination of expert knowledge previous studies dornes et al 2008 and recommendations in the manual verseghy 2012 taking the available computational budget into consideration 100 star centers were generated with a resolution of 0 1 chosen arbitrarily for implementation of star vars requiring a total of 100 000 model runs this large number of mesh runs was performed using the university of saskatchewan s linux based high performance computing cluster called plato approximately 3 minutes were required to complete a single evaluation of the model on one core of plato if a single cpu core is used the entire set of 100 000 samples will take approximately 6 85 months of computational time to run we used 160 cpu cores the nash sutcliffe coefficient of efficiency ns was used to measure daily model streamflow performance calculated for a period of three years october 2003 to september 2007 following a one year model warmup period that was excluded in the ns calculations sensitivity was assessed using the ivars 50 index of the vars methodology the total number of bootstrap replicates was set to r 1 000 5 results and discussion 5 1 factor grouping results 5 1 1 results for the sobol g function fig 6 a shows the factor grouping dendrogram for the first case study sobol g function obtained by the normalized ivars 50 sensitivity indices applied to fig 6 a the elbow method automatically identified the four groups of factors labeled as g 1 g 2 g 3 g 4 this result is consistent with the groups defined in table 2 for clarity we use lower case g to represent normalized grouping to demonstrate the value of applying a normalizing transformation prior to factor grouping fig 6 b presents the dendrogram obtained using the untransformed ivars 50 sensitivity indicies upper case g is used to represent un normalized grouping without normalization the 9 strongly influential factors of table 2 are clustered into 3 smaller groups as g 1 x 1 g 2 x 4 x 7 and g 3 x 3 x 8 x 2 x 6 x 5 x 9 while the remaining 41 factors are gathered into one big group g 4 overall the proposed factor grouping strategy makes the problem more tractable and results in a significantly higher convergence rate to elaborate consider factors x 1 and x 2 that are by design equally the most influential factors α 1 α 2 0 see table 2 while we would expect x 1 and x 2 to converge to the same sensitivity index value fig 7 shows that convergence has not been achieved after performing 190 200 function evaluations in contrast application of the grouping algorithm fig 6 a quickly clustered x 1 and x 2 into the same group g 1 the group of most influential factors indicating a high convergence rate comparison of fig 6 a and b additionally highlights the importance of normalization because without normalization x 1 and x 2 are clustered into two different groups i e g 1 and g 2 respectively see fig 6 b 5 1 2 effect of sampling variability on factor grouping results robustness of the factor grouping algorithm depends directly on how robust our estimate of the sensitivity matrix is if the gsa algorithm provides sufficiently robust estimates of the sensitivity indices the grouping results can be expected to be robust as well the ahc algorithm that has been used in our proposed grouping strategy is a deterministic algorithm without any random components as a result it provides a unique dendrogram i e clustering for a given set of sensitivity indices until these indices change therefore one can evaluate the impact of sampling variability on factor grouping results by performing multiple trials of sensitivity analysis with different sample sets and comparing the resultant dendrograms however accounting for sampling variability using multiple trials of gsa may be an impractical approach for high dimensional and computationally intensive models such as mesh on the other hand to ensure that our proposed grouping technique operates as expected various testing methods should be implemented during the numerical experiments therefore in this section we investigate the effects of sampling variability on grouping enabled vars algorithm by carrying out several trials of sensitivity analysis initiated using different random seeds we do this only for the sobol g function which is computationally cheaper than mesh by running 40 independent trials of star vars comparing multiple trials of sensitivity analysis through factor grouping requires a method for comparing dendrograms different measures for assessing the degree of similarity association between various classifications i e dendrograms exist including the cophenetic correlation coefficient sokal and rohlf 1962 and baker s index baker 1974 here we used baker s index to investigate how the results of the factor grouping algorithm vary between several replicates of the star vars given a pair of dendrograms baker s index can be calculated by taking two factors model parameters and finding the highest possible level of k number of groups created when cutting the dendrogram for which these two factors belongs to the same tree this procedure is repeated for the same two factors in the other dendrogram overall in a d dimensional model there are d d 1 2 possible combinations of such pairs of factors and accordingly all these numbers can be computed for each of the two dendrograms finally these two sets of numbers are paired with respect to the pairs of factors compared and a rank correlation e g spearman s correlation coefficient between them is calculated galili 2015 accordingly unlike other measures the baker s index is only sensitive to the relative position of branches in the dendrograms and is insensitive to the heights of the branches this important feature makes it suitable for comparing factor grouping results calculation of the baker s indices associated with pairwise comparison of the 40 dendrograms obtained from the gsa of the sobol g function resulted in a 40 by 40 symmetric correlation matrix containing the baker s indices between each dendrogram and the others as shown in fig 8 inspection of these results reveals that almost all runs yielded very similar factor groupings i e the minimum value of baker s index we obtained is 0 991 and the median is 0 999 in fig 8 this is not surprising since it has been shown that the star vars is a robust gsa technique that provides stable results razavi and gupta 2016b and consequently the comparison of the grouping results confirms the robustness of star vars to sampling variability 5 1 3 results for the mesh model the model parameter grouping obtained for the second case study mesh model is shown in fig 9 using the elbow method the 111 mesh model parameters were automatically classified into 7 groups see table a 1 of appendix a these groups are ordered based on their importance i e g 1 contains the most strongly influential parameters while parameters in g 7 are minimally influential the first group g 1 consists of 4 parameters controlling water storage and movement in the soil sdep drn river channel routing wfr22 and snow cover fraction zsnl groups 2 through 6 contain parameters related to soil and vegetation properties control of overland and interflow generation and initial conditions note that the parameters specifying initial conditions including soil initial moisture content thlq1 2 3 temperature tbar1 2 3 and initial surface ponding tpond are all located in g 4 except for initial canopy temperature tcano which is in g 7 least influential this placement of tcano may be due to the fact that model simulations start in october which is not part of the canopy growing season other parameters in g 7 are ponding parameters corresponding to gru number 7 that has small area fractions compared to that of other gru types zplg7 and zpls7 and the manning s coefficient manng used in calculation of the overland flow 5 2 assessment of the convergence and robustness of factor ranking results 5 2 1 results for the sobol g function for the first case study fig 10 a illustrates the convergence behavior of the proposed robustness measure while fig 10 b shows the robustness values based on individual factor ranking computed as described in razavi and gupta 2016b whereas the robustness values estimated based on individual factor rankings actually deteriorates for the first 28 350 evaluations clearly indicating insufficient sample size the factor grouping converges quickly towards probability 1 i e 100 after about only half of that number 15 000 and from 28 350 function evaluations onwards the grouping is stable fig 10 a note also that the robustness measure based on individual factor rankings continue to vary between 0 18 and 1 median 0 55 even for well over 150 000 evaluations 5 2 2 results for the mesh model the robustness assessment results for the second case study are shown in fig 11 a and b after 90 000 function evaluations the estimated factor grouping based robustness values are all higher than 0 50 fig 11 a whereas for individual factor ranking based values they continue to vary between 0 04 and 1 0 fig 11 b for clarity fig 11 c shows the trajectories of the medians of both the individual red dashed line and grouping based blue solid line results notice that the median of the grouping based robustness values is already quite high 0 80 after 20 000 function evaluations and increases rapidly thereafter to be above 0 90 from 50 000 evaluations on finally reaching to 0 99 at 100 000 model runs given there are 111 parameters this means the grouping based approach has achieved greater than 90 robustness in identifying the 55 most sensitive parameters of this model with only 40 000 50 000 model runs in contrast the median of the robustness values for the individual factor ranking based method remains low and is only 0 17 at the termination of the experiment in support of this fig 12 a g shows the trajectories of robustness values for each member of each of the 7 identified groups from most strongly influential to non influential we see clearly that groups 1 fig 12 a and 2 fig 12 b the two most strongly influential and group 7 fig 12 g the least influential have been pretty well established with high robustness after about 50 000 function evaluations in contrast the robustness values for members of the intermediate groups 3 6 fig 12 c f tend to be more variable with some members having high robustness and others having relatively lower robustness values for the latter this indicates that their group membership is less certain and that they may continue to shift between groups as the sampling proceeds nonetheless it is clear that the most and least influential parameter groups are relatively well established quite early in the gsa procedure finally fig 12 h shows together the trajectory of median robustness values for each of the seven groups this plot can be useful for oversight monitoring of the convergence of the factor grouping procedure overall these results clearly illustrate the value of using factor grouping based robustness estimates for monitoring convergence of gsa applied to high dimensional models as indicated in fig 2 when convergence has not yet been achieved the user can iterate by increasing the original sample size using a sequential sampling scheme such as plhs 5 3 comparison of the elbow method and minimum robustness method in finding optimal number of groups fig 13 top panel illustrates the evolution of the optimal number of groups obtained by the elbow method as the star vars algorithm progresses fig 13 a and b the grouping algorithm evolves and discovers groups of parameters that share specific properties in terms of their sensitivity to further investigate the performance of the elbow method the merging costs distance metric versus the number of groups are plotted for the sobol g function fig 13 c and mesh model fig 13 d when the number of function evaluations is 190 200 and 100 000 respectively fig 13 bottom panel confirms the ability of the elbow method in successfully determining the optimal number of groups by finding the elbow point of the curve point of maximum curvature for the sobol g function the elbow method converges rather quickly to an optimal number of groups however for the mesh model the optimal number continues to vary over the range 7 13 for the number of function evaluation examined moreover to examine the performance of the minimum robustness method in determining optimal number of groups we calculated the maximum number of groups that is required to have minimum robustness values of 0 90 for the sobol g function fig 14 a and 0 45 for the mesh model fig 14 b as the number of function evaluations grows the top panel of fig 14 indicates that the number of groups tends to be small for low numbers of function evaluations and tends to increase as the number of function evaluations increases finally histograms of the estimated robustness values are shown in fig 14 c and d for the mesh model for 40 000 and 90 000 function evaluations when the number of groups is equal to 9 and 10 respectively although the minimum desired robustness level was set to 0 45 the estimated robustness value for most of the parameters is greater than 0 60 in fact when the number of function evaluations is 40 000 only two parameters out of 111 parameters have robustness values less than 0 60 while after 90 000 function evaluations except one parameter all the parameters have the robustness values greater than 0 60 6 conclusions and future work global sensitivity analysis gsa is a powerful tool for deepening our understanding of dynamical earth and environmental systems dees providing information helpful for model development parameterization calibration and data acquisition advanced dees models are commonly characterized by large parameter problem spaces and high computational overheads which impede the effective implementation of the modern gsa techniques because an extensive sensitivity analysis often requires a computationally infeasible number of model runs to break down this barrier we have introduced an automated factor grouping strategy that can be used with any gsa algorithm to reliably cluster input factors into groups of different sizes using information gained during the gsa our proposed grouping approach is based on the use of an efficient strategy elbow method and or minimum robustness method to determine the optimal number of groups additionally we developed and tested a new measure of robustness based on factor grouping to monitor and evaluate convergence of the gsa algorithms to illustrate the approach we coupled the proposed factor grouping algorithm with the vars variogram based gsa technique and demonstrated its utility for parameter sensitivity analysis of two high dimensional case studies the 50 parameter sobol g function and the 111 parameter mesh large scale land surface hydrology model for the sobol g function we also assessed the effect of sampling variability on the grouping enabled vars algorithm by running multiple replicates of the algorithm with different original sample sets the results of our experiment reveal the robustness of the grouping strategy combined with vars method for gsa of course to better understand how robustness of the chosen gsa method can affect the robustness of factor grouping results several gsa methods should be tested in conjunction with the proposed grouping strategy overall our results confirm that the strategy of grouping factors can significantly reduce the computational effort required to perform gsa on high dimensional models in the case of our mesh model by as much as 50 this is mainly because factor groupings typically converge faster than factor sensitivity indices a further potential benefit is that the proposed algorithm can provide information useful for reducing the complexity of the problem in the follow up experiments e g model calibration or model order reduction by identifying dominant influential groups of factors that significantly contribute to the variability in the model outputs future work should include an extension of the factor grouping strategy to multi criteria sensitivity analysis of high dimensional models to conduct a comprehensive gsa it is necessary to fully consider the multi output nature of dees models klepper 1997 gupta et al 1998 rosolem et al 2012 haghnegahdar et al 2017 our proposed method for factor grouping can in principle be used in conjunction with multi criteria sensitivity analysis to study parameter properties while recognizing multivariate aspects of the model behavior thereby better supporting model development and understanding one possible option is to employ multi criteria gsa techniques which measure the global contribution of each factor to multivariate outputs through estimating generalized sensitivity indices having generated the sensitivity matrix consisting of these generalized indices the grouping algorithm can use cluster analysis to identify distinct groups of factors as described in section 3 see fig 2 gsa methods such as mogsa multi objective generalized sensitivity analysis method bastidas et al 1999 liu et al 2004 which is an extension of the regional sensitivity analysis and the generalization of the variance based sobol gsa for multivariate outputs lamboni et al 2011 gamboa et al 2014 may be useful in this regard the aforementioned methods apply bootstrapping to ensure statistical robustness of the generalized sensitivity indices and accordingly can yield robust groups of parameters software availability a matlab code for the proposed factor grouping algorithm is included in the vars tool package which is a set of programs for next generation sensitivity and uncertainty analysis the software is freely available for noncommercial use upon request from the authors and can be downloaded from http vars tool com acknowledgments rs gratefully acknowledges the scholarship provided by the school of environment and sustainability through the canada excellence research chair in water security sr has been supported in part by nserc natural sciences and engineering research council of canada discovery grant hvg received partial support from the australian research council through the centre of excellence for climate system science grant ce110001028 appendix a parameters of the mesh model and their corresponding groups the 7 groups of parameters for the mesh model see fig 8 are listed in table a 1 the description of parameters and their feasible ranges can be found in haghnegahdar et al 2017 table a 1 grouping of 111 parameters of the mesh these groups are numbered in order of importance table a 1 group number parameters 1 sdepc wfr22 zsnl3 drnc 2 vpdac zpls4 sdepd rootc sdepg xslpc ratios zsnl4 zsnl1 zplg4 ddenc vpdad lamind vpdag lnz0d 3 claysa3 sandsa2 lamaxc xslpd sandsa1 rsmnc rootg zsnl11 zsnl7 xslpg zplg3 zpls3 zpls1 zplg1 ddend claysi3 sandsi3 lnz0g sandsa3 claysa2 claysa1 qa50c drng vpdbc drnd ddeng 4 lamaxg thlq3 claysi1 sandsi2 sandcl3 qa50d grkfc lnz0c alicc alvcc claysi2 alicg sandcl2 sandcl1 tbar2 psgac thlq1 orgsi3 psgbc thlq2 tbar3 tpond tbar1 cmasc mannc zpond ratiosi qa50g rsmng rsmnd orgsi2 orgsi1 5 ratiocl claycl3 grkfd cmasd orgsa3 orgsa2 orgsa1 orgcl1 orgcl2 claycl2 orgcl3 claycl1 alicd lamaxd alvcg grkfg alvcd vpdbg cmasg 6 zpls11 vpdbd zplg11 psgag psgbg laming psgad psgbd manng rootd 7 zplg7 zpls7 tcano mannd 
26294,ecosystem service es assessments are widely promoted as a tool to support decision makers in ecosystem management and the mapping of es is increasingly supported by the spatial data on ecosystem properties provided by earth observation eo however es assessments are often associated with high levels of uncertainty which affects their credibility we demonstrate how different types of information on es including eo data process models and expert knowledge can be integrated in a bayesian network where the associated uncertainties are quantified the probabilistic approach is used to map the provision and demand of avalanche protection an important regulating service in mountain regions and to identify the key sources of uncertainty the model outputs show high uncertainties mainly due to uncertainties in process modelling our results demonstrate that the potential of eo to improve the accuracy of es assessments cannot be fully utilized without an improved understanding of ecosystem processes keywords ecosystem services earth observation uncertainty bayesian network avalanche protection 1 introduction the ecosystem service es concept is increasingly promoted as a framework to support decision making convention on biological diversity 2010 european commission 2011 in order to improve the management of ecosystems and maintain the services they provide to society daily et al 2009 maes et al 2012 these efforts are supported by the growing body of scientific literature on es assessments schägner et al 2013 schröter et al 2016 and the increasing availability of spatial data particularly through earth observation eo which provides information on a variety of ecosystem properties andrew et al 2014 ayanu et al 2012 however the use of es assessments in planning and decision making remains limited albert et al 2014 es assessments are associated with large uncertainties which are often unreported schägner et al 2013 and different es assessment methods show inconsistent results eigenbrod et al 2010 schulp et al 2014a which may affect their credibility as tools for decision makers andrew et al 2015 ecosystem service assessments combine data on biophysical structures and processes with models of ecosystem function and measures of socio economic value de groot et al 2010 haines young and potschin 2009 modelling the whole es cascade haines young and potschin 2009 comprises not only various types of data and models but also various types of uncertainty ascough et al 2008 on the one hand uncertainty in these assessments stems from the inherent spatial and temporal variability of socio ecological systems regan et al 2002 this type of uncertainty cannot be reduced but should be taken into account in management decisions ascough et al 2008 on the other hand es assessments involve uncertainties that can potentially be reduced such as measurement errors model structure and parameter uncertainties and subjective judgment regan et al 2002 to realistically evaluate the level of confidence in es assessments all these types of uncertainty should be integrated maier et al 2008 and finally also communicated to users moreover understanding how the different sources of uncertainty propagate to the final assessment can help identify knowledge gaps and contribute to more robust decision making neuendorf et al 2018 polasky et al 2011 uusitalo et al 2015 the data most commonly used in es assessments are proxies describing ecosystem structure eigenbrod et al 2010 schägner et al 2013 such as land use land cover lulc costanza et al 1997 troy and wilson 2006 plant functional traits lavorel et al 2011 schirpke et al 2013 or aboveground biomass barredo et al 2008 nelson et al 2009 such data is subject to uncertainty due to limited sample sizes different data collection and processing techniques and sampling biases ascough et al 2008 earth observation eo is expected to reduce these uncertainties as it provides spatially explicit and up to date information on many of these ecosystem properties andrew et al 2014 cord et al 2017 feng et al 2010 so far the eo product most commonly used in es assessments is land cover cord et al 2017 however several studies have highlighted the shortcomings of lulc based es assessments eigenbrod et al 2010 plummer 2009 by combining lulc with other eo products such as ndvi biomass or vegetation density the accuracy of es assessments can potentially be improved andrew et al 2014 nonetheless eo data also contain measurement errors or misclassifications that are often not reported ayanu et al 2012 petrou et al 2015 the valuation of es further depends on proxies of demand for es such as visitor counts or travel cost estimates koetse et al 2015 wolff et al 2015 or social valuation methods such as choice experiments brunner et al 2015 garmendia and gamboa 2012 where subjective judgment plays an important role furthermore when categorical variables such as lulc are used differences in people s definitions of categories lead to linguistic uncertainty regan et al 2002 a wide variety of approaches is used to link proxies of ecosystem structure to ecosystem services lavorel et al 2017 most common are proxy based approaches where expert based look up tables are used to link lulc or habitat types to es provision kienast et al 2009 seppelt et al 2011 more complex approaches combine proxies with spatial analyses e g grêt regamey et al 2014 when sufficient data are available empirical models are used to predict the distribution of ecosystem service providers e g species schulp et al 2014b or to derive the link between ecosystem traits and es e g lavorel et al 2011 while process based models explicitly represent the mechanisms underpinning ecosystem functioning e g lautenbach et al 2013 however uncertainties in model parameters and structure are often not quantified schägner et al 2013 and many es models are unvalidated due to a lack of validation data schulp et al 2014a large discrepancies have been found between lulc based es maps and maps based on process based models eigenbrod et al 2010 highlighting the need to quantify and communicate uncertainties when using es to support decision making carpenter et al 2009 vorstius and spray 2015 in this paper we use a bayesian network bn to model avalanche protection an essential regulating service provided by mountain forests grêt regamey et al 2013 bns can include both expert knowledge and empirical data while their transparent graphical structure facilitates participatory modelling aguilera et al 2011 landuyt et al 2013 therefore bns have been used to address water management ames et al 2005 bacon et al 2002 land use change celio et al 2014 sun and müller 2013 and es modelling gonzalez redin et al 2016 grêt regamey et al 2013 landuyt et al 2013 the probabilistic structure of bns allows the quantification and propagation of uncertainties barton et al 2012 borsuk et al 2004 kelly letcher et al 2013 accounting for uncertainties is particularly relevant when modelling es related to natural hazards where extreme events at the tails of probability distributions are important straub and grêt regamey 2006 we use eo data to model both the provision and demand for avalanche protection and disentangle the effects of data quality and process understanding on uncertainty in the es assessment in addition we demonstrate how knowledge gaps can be identified and discuss how understanding the sources of uncertainty can help improve es assessment methods 2 methods 2 1 bayesian networks bayesian networks are directed probabilistic graphs where nodes represent the variables of the studied system and the links between nodes represent dependencies between them kjaerulff and madsen 2013 underlying the graph is a joint probability distribution p x p x1 xn n i 1 p xi pa xi which consists of a conditional probability distribution p xi pa xi of each node xi for each combination of its parent nodes pa xi states the conditional probabilities are expressed in conditional probability tables cpts or conditional continuous probability distributions the conditional probability of each node can be quantified independently borsuk et al 2004 which allows us to integrate various data and model types uusitalo 2007 and to account for different types of uncertainty evidence on any of the nodes is propagated through the bayesian network and the joint probability distribution is updated by applying bayes theorem p x pa x p x pa x p pa x evidence on input nodes will therefore result in a new updated posterior probability distribution of all other nodes in the network to efficiently perform inference most bayesian network software relies on algorithms such as the junction tree algorithm lauritzen and spiegelhalter 1988 which are limited to discrete or gaussian variables this means that most continuous variables need to be discretized which can lead to a loss of information benjamin fink and reilly 2017 landuyt et al 2013 ropero et al 2013 at the same time using discretized probability distributions means that non normal or even multi modal distributions can more easily be captured myllymäki et al 2002 uusitalo 2007 and non linear relationships can be expressed in cpts since increasing the number of discretization intervals exponentially increases the cpts the discretization is a trade off between accuracy and computational efficiency 2 2 accounting for uncertainty the probabilistic structure of the bayesian network allows us to incorporate uncertainty in the input data of the es model cha and stow 2014 as well as model uncertainties in the links between variables landuyt et al 2013 qian and miltner 2015 the methods to account for different types of uncertainty in the bn are summarized in table 1 when sufficient data is available to estimate the level of natural variability variables in the modelled system are characterized as probability distributions instead of single values for example the probability of heavy snowfall is commonly modelled using a gumbel extreme value distribution salm et al 1990 which we use as the prior probability distribution of max new snow height in the network when input data represents a measured proxy with a known error rate we make the uncertainty explicit by creating separate nodes representing the observed value y and the actual state x of the variable the observation is caused by the actual state not vice versa and defining the structure of the network based on this causality helps to define conditional probabilities we explain this principle on the example of a land cover classification classification errors are commonly expressed in confusion matrices which contain counts of predicted classes for objects where the true class is known e g from ground truth data with rows representing the classes in reality c and columns representing the classes predicted by the classification c based on these counts we can calculate either backward probabilities p x c y c e g the probability that a patch classified as forest is a forest in reality or the forward probabilities p y c x c that a forest patch will be classified as forest the backward probabilities depend on the prior distribution of land cover if we sample ground truth locations in a densely forested landscape it is likely that many of the patches classified as forest will in fact be forested leading to a higher backward probability than if we sample in a sparsely vegetated area however forward probabilities are inherent to the error process in the remote sensing data and the classification algorithm cripps et al 2009 and are therefore consistent over the whole area if we define the classification node y as the child of the actual class x the rows of its cpt correspond to the forward probabilities p y x for continuous variables with a known measurement error rate we similarly define the measurement node y as a child of the actual state of the variable x assuming a normal distribution of errors we can define the conditional probability of y as a normal distribution p y x x n x σ2 where the mean is the value of the actual state x and the standard deviation σ is defined by the measurement error if we have no prior information about the actual state of x a finding on the child y measurement node will then result in a normal distribution p x y y n y σ2 of the parent x actual state bayesian networks can incorporate information about links between variables that is already available in the form of empirical or process based models empirical models typically include information about the error in parameter estimates the model parameters can be included in the bn not as single values but as distributions by specifying equations such as y n β0 σβ0 2 n β1 σβ1 2 x1 n βn σβn 2 xn where x1 xn are the parent nodes of y β0 βn are the corresponding model parameter estimates and σβ0 σβn are the standard errors of the estimates the conditional probability distribution of y can be derived by repeatedly computing the value of y for each combination of its parents with parameter values sampled from the parameter distributions another approach to quantify links between variables is so called parameter learning when data on a child variable and its parents is available an algorithm such as expectation maximisation dempster et al 1977 lauritzen 1995 can be used to estimate the corresponding cpt when information about links between variables is available in the form of process based models the model outputs can be used as an input for parameter learning the uncertainties in the process based model can be captured by learning from monte carlo simulations with varying input parameters ames et al 2005 borsuk et al 2004 cain 2001 kuikka et al 1999 when data is limited and no models are available to quantify links between nodes the cpts can be elicited from experts expert elicitation is frequently used in ecology and risk assessments kuhnert et al 2010 speirs bridge et al 2010 and uncertainty in expert knowledge can be addressed by eliciting probability distributions rather than single values while experiments have shown that experts can more accurately estimate quantiles of a distribution than its mean and variance o hagan 2012 the estimates are often affected by overconfidence kuhnert et al 2010 to limit this problem speirs bridge et al 2010 developed the four point estimation method where experts are asked for the lowest and highest value they would expect the most likely value and their confidence that the true value is within this range metcalf and wallace 2013 we thus obtain information about the quantiles and mode of the distribution as well as its shape which allows us to fit a suitable distribution o hagan 2012 commonly normal distributions are used metcalf and wallace 2013 however in our case the elicited expert estimates for node potential detrainment showed an asymmetrical unimodal distribution so we chose to use a simple triangular distribution johnson 1997 other approaches to quantify uncertainty in expert knowledge involve combining estimates from several experts o neill et al 2008 for a review of expert elicitation methods see kuhnert et al 2010 often expert knowledge is related to qualitative categories rather than quantitative variables for example it may be easier for an expert to estimate the avalanche protection capacity of forests that are either open scattered or dense rather than based on a percentage of crown cover linking such categories to numerical values is associated with a type of linguistic uncertainty vagueness where the delineation between categories is not sharp regan et al 2002 linguistic uncertainty is commonly addressed using fuzzy logic zadeh 1965 zimmerman 1992 where membership functions m y define the level of membership between 0 and 1 in a specific class for continuous values of y for example we define trapezoidal membership functions m y of crown cover y for the classes of forest density x method adapted from petrou et al 2013 see appendix b at the expert defined threshold between a scattered and dense forest y 70 the probability of the forest being classified as dense is 0 5 while a forest with 100 crown cover will certainly be classified as dense p x dense 1 in the language of bayesian networks the membership function corresponds to the probability of the class x given an observation on y p x y y and can be used to populate the corresponding cpt when a class is defined by multiple attributes membership functions can be combined through fuzzy or or and operators zadeh 1965 as used by veitinger et al 2016 to identify potential avalanche release areas there are many methods to quantify uncertainty in the posterior probability distributions of bayesian network model outputs for continuous variables the spread of a distribution is commonly expressed with its second central moment the standard deviation however standard deviation is less informative for skewed distributions landuyt et al 2015 in information theory shannon s entropy shannon 1948 is used to quantify uncertainty in discrete variables h i 1 n p i log 2 p i where pi is the probability of state i and n is the number of states to evaluate uncertainty and compare it between output nodes with different numbers of states we calculate the evenness index hill 1973 of the posterior probability distribution j h hmax where hmax log2 n marcot 2012 the index has values between 0 and 1 where 1 denotes a uniform distribution between all possible states maximum uncertainty and 0 denotes complete certainty that the output node is in a specific state 2 3 sensitivity analysis and flow of information in bayesian network modelling sensitivity analysis is often used to evaluate the influence of variables in the modelled system on the posterior probability distribution of a node of interest marcot 2012 uusitalo 2007 sensitivity to findings can be measured by the reduction in uncertainty e g entropy or variance in the target node due to a finding on another node entropy reduction is expressed by the measure of mutual information kjaerulff and madsen 2013 i x y h x h x y h y h y x y p y x p x y log 2 p x y p x p y where h x is the entropy of x and h x y is the entropy of x after a new finding on y the analysis of sensitivity to findings gives us an indication of which variables in the system have the highest influence on the outcome of the model in addition we use a stepwise sensitivity analysis to visualize the flow of information in the network for each node x we calculate the proportion of its entropy that can be reduced by a finding on each of its parents pa x mi i x pa x h x these relative mutual information values are used as weights for links between nodes in a sankey diagram of the network which is used to identify the most relevant sources of uncertainty in the model when findings are added to the network e g setting the value of node y to state y this alters the probability distributions and sensitivities of other nodes and the sensitivity to node y becomes zero therefore we also perform the stepwise sensitivity analysis for specific combinations of input variables to identify sources of uncertainty under different conditions 2 4 case study avalanche protection we illustrate the approach to quantify uncertainties in eo based es assessments on the example of a regulatory es avalanche protection the case study is located in the region of davos in the eastern part of the swiss alps the principal town davos is a well developed urban and touristic centre located in the central part of the main valley at an elevation of 1500 m above sea level the rest of the main valley and the three side valleys are relatively rural with a few scattered settlements and a landscape still strongly dominated by mountain agriculture snow avalanches are the most common natural hazard in the area kulakowski et al 2011 and mountain forests play a key role in reducing the risk for settlements below through two main functions prevention and detrainment the probability for an avalanche release depends on topography bühler et al 2013 veitinger et al 2016 but is lower in forested areas bebi et al 2009 when an avalanche flows through a forest some of the snow is stopped behind trees detrainment which reduces the mass and velocity of the avalanche feistl et al 2014 teich et al 2014 the anthropogenic value of avalanche protection can be quantified based on the risk to people and buildings planat 2008 previous es valuations indicate that avalanche protection is among the most valuable es in the region of davos grêt regamey et al 2013 we based our avalanche protection model on previous models developed for this es grêt regamey et al 2013 grêt regamey and straub 2006 but extended it to incorporate newly available remote sensing inputs as well as recent developments in modelling forest avalanche interactions the bn structure fig 2 was developed through an iterative process of literature review consultation with experts and testing the behaviour of the network with different input values the bn was constructed in netica norsys 2010 where we also performed sensitivity analyses using the function sensitivity to findings the data available for modelling the avalanche protection service are in situ data on the temporal and spatial distribution of avalanches and remote sensing variables which are proxies for the actual state of the ecosystem we accounted for the spatial variability of the avalanche process by running the bn for each pixel of a 5 m resolution raster of the study area we used input data that describe the spatial patterns of the hazard process under a frequent 30 year and extreme 300 year scenario velocity 30y and velocity 300y where occurrence of both scenarios depends on the probability of heavy snowfall the temporal variability of these events is incorporated through a probability distribution of maximum new snow heights based on long term observations slf 2017 high resolution lidar data august 2015 lms q780 sensor ca 20 points m2 data was processed using lastools isenburg 2016 to derive 1 m resolution digital terrain dtm and canopy height chm models to measure crown cover in forests moeser et al 2014 and terrain roughness sappington et al 2007 and to detect buildings the chm was combined with an aerial cir image august 2013 leica ads 80 0 25 m resolution swisstopo 2013 and a sentinel2 image from may 2016 european space agency 2016 for an object based supervised random forest classification into non forested areas evergreen and deciduous forests fassnacht et al 2016 ground truth data was collected at 110 plots in the valley to train the classification and to estimate the measurement and classification uncertainties in the remote sensing data ecosystem structure and processes were linked to ecosystem functions using fuzzy logic crown cover class release veitinger et al 2016 expert knowledge potential detrainment an empirical model from literature prevention bebi et al 2001 and learning from process based simulation results christen et al 2010 detrainment since the simulation results showed high spatial autocorrelation we did not perform the learning directly in the bn software but fitted a spatial regression model in r pinheiro et al 2017 r core team 2013 and used it to populate the cpt in order to combine both ecosystem functions the total per pixel level of es provision was expressed in the quantity of snow prevented from releasing or stopped which is the es benefit carrier in this case bagstad et al 2013 to quantify the demand for avalanche protection we used a probabilistic risk assessment approach grêt regamey and straub 2006 with values of risk factors as determined by experts for evaluating protection measures against natural hazards bafu 2015 merz et al 1995 at each step the uncertainties are quantified as described in section 2 2 the bn for avalanche protection was applied for the lower dischma one of the side valleys of davos see fig 1 using an application based on the netica api celio et al 2014 we set evidence on nodes where data is available and performed inference for each pixel in a 5 m resolution raster of the study area since the provision and demand for avalanche protection do not occur at the same location and spatial processes could not be modelled in the pixel based bn we quantified provision and demand separately thus we obtained posterior probability distributions of avalanche protection provision and demand for each pixel in order to map the outputs we calculated the per pixel median and evenness index uncertainty of the posterior probability distributions to illustrate the process of inference in the bn we show the joint probability distributions of all variables for some example pixels in appendix d 3 results the process of integrating available data models and knowledge on the avalanche protection service resulted in a bn with 37 nodes and 53 links which is shown in fig 2 the inputs to the model are remote sensing variables and in situ data on avalanches these are linked to intermediate nodes that describe ecosystem structure the natural hazard process and risk assessment the model outputs are posterior probability distributions of the provision expressed in height of snow stopped by the forest and the demand for avalanche protection expressed in chf for forested areas the model predicts a bimodal distribution of es provision with a peak at 0 corresponding to conditions with no avalanche events and another between 0 1 and 0 5 m of snow prevented from releasing and or stopped during avalanches on average areas with a predicted value of provision above 0 have a cv of 110 descriptions of the bn nodes and their states are provided in the supplementary material appendix a as well as examples of posterior probability distributions appendix d for es provision and demand the spatially explicit model output of es provision shows a high spatial heterogeneity fig 3 areas with a high level of avalanche protection provision are the steeper densely forested areas particularly at high elevations where larger avalanche releases are more likely although eo inputs particularly the land cover classification are more uncertain in heterogeneous forests near the upper tree line this pattern is not reflected in the spatial distribution of uncertainty in the provision of the avalanche provision the uncertainty is related to the level of avalanche protection where pixels with high levels of provision show high levels of uncertainty in addition there are many areas with a low predicted value of avalanche protection provision but a high uncertainty indicating that these forests may provide no or only limited avalanche protection under certain infrequent extreme conditions higher levels of certainty are achieved only in areas with a very low or zero level of protection service the factors underlying the spatial distribution of the es were analysed using a sensitivity analysis of the target nodes of the bn provision and demand table 2 provision of avalanche protection is most sensitive to nodes describing the ecosystem functions and the avalanche process the modelled provision is more sensitive to inputs of in situ avalanche data especially the distribution of maximum new snow height than to remote sensing variables of ecosystem structure among these the lidar derived crown cover is most important since inference can run in different directions in a bn the sensitivity analysis also shows indirect influences for example knowledge about potential lethality of avalanches on a specific pixel would increase the knowledge about the potential provision of avalanche protection at that location on the demand side the most influential node is the cost of damage to buildings while the most important input are buildings detected from lidar overall the nodes closer to the target variables have a stronger influence than nodes farther away this is due to uncertainty in the intermediate links for example detecting a building from remote sensing mi 49 6 has a smaller effect on the distribution of demand than certain knowledge of a building s location would mi 60 8 similarly certain knowledge of the actual land cover mi 4 98 would more strongly reduce the uncertainty about provision than the land cover classification does mi 1 42 because there is some uncertainty in the classification in order to understand these relationships in more detail we performed a stepwise sensitivity analysis the results of the stepwise sensitivity analysis are visualized in a sankey diagram fig 4 for each node the thickness of incoming from the left links show how much the entropy on the node can be reduced by findings on preceding nodes mutual information is not additive i e if both parent nodes can reduce the entropy of a child by 50 this does not mean that findings on both parents will result in complete certainty on the child node nonetheless plotting the mi gives an indication of the main sources of uncertainty in the model when the value of mi for all the parents of a node is rather low this means that the node will have a wide probability distribution even when the states of its parents are known implying high uncertainty in the corresponding links if such a node has a large influence on the outcome of the network this indicates a knowledge gap overall the uncertainties related to avalanche processes contribute more to the final uncertainty in es provision than uncertainties about ecosystem structure for example the node release describing whether a pixel is in a potential avalanche release area has an important influence on subsequent nodes in the network but findings on its parents slope roughness measured and curvature can only reduce a small part of its entropy so it is a major source of uncertainty in the model some remote sensing inputs have a strong effect on the knowledge about ecosystem structure gap width and crown cover while others have higher uncertainty e g roughness there is high uncertainty in land cover classification as its mutual information with actual land cover is only 29 however additional information on actual land cover is gained from the crown cover class mi 59 the links from ecosystem structure to the potential provision of es also contain high uncertainty regarding both the potential of a forest to prevent avalanches empirical model based potential prevention and to stop snow during an avalanche expert based potential detrainment however potential detrainment has a relatively low influence on the corresponding ecosystem function process model based detrainment this function is affected more strongly by the avalanche process velocity which in turn is affected by the natural variability in release conditions max new snow height on the demand side appendix c the remote sensing input building detected is rather certain while uncertainty about the total risk is most affected by the natural variability of the avalanche process additionally uncertainty comes from the wide distribution of building types which affect the costs of potential damages and number of people per building and which could not be differentiated in the remote sensing input the sensitivities of the bn change after we enter evidence and are therefore different for each combination of input nodes nonetheless the general pattern remains the same with high uncertainties related to the avalanche processes due to natural variability and model uncertainty furthermore the uncertainty about es provision would be significantly reduced by additional knowledge on avalanche release areas release node examples of sensitivities for posterior probability distributions after input data are added to the network are shown in appendix d for one pixel of es provision and demand 4 discussion 4 1 uncertainties in avalanche protection in this study we used recent developments in eo techniques and natural hazard modelling to assess avalanche protection by forests an important es in mountain regions we integrated eo data empirical and process based models and expert knowledge into a bn while accounting for the uncertainty in each of these components thus we were able to quantify the total uncertainty in the es assessment and evaluate the influence of different sources of uncertainty on the model output although high resolution eo data was available in our study area uncertainties in the es assessment remain high with a coefficient of variation well above 100 while there was some uncertainty in the eo products used these had a limited effect on the final model output the total uncertainty was more strongly affected by the uncertainties regarding avalanche processes particularly the variability of snow heights the probability of avalanche releases which was defined using a fuzzy approach based on expert knowledge veitinger et al 2016 and avalanche velocities and detrainment in forests which were quantified based on a process based model christen et al 2010 these uncertainties can be explained in part by the high natural variability of avalanche hazards related to complex terrain and temporal variability in snow and weather conditions schweizer 2008 in addition currently available avalanche models and expert knowledge are based on limited observational data bühler et al 2009 which contributes to high model uncertainty 4 2 added value of earth observation data by combining different eo inputs high resolution lidar aerial and satellite multispectral images we could include more information at a higher spatial resolution compared to previous es assessments in the region which relied mainly on lulc data grêt regamey et al 2013 we were able to differentiate tree species and measure terrain roughness both of which have an impact on ecosystems potential to provide avalanche protection feistl et al 2014 teich and bebi 2009 the use of eo data enabled us to model the es at a 5 m spatial resolution which allows us to observe the high spatial heterogeneity of es provision in complex terrain and identify individual forest stands that are particularly important for avalanche protection however the eo products used particularly the land cover classification contained considerable uncertainties high error rates are common in classifications for example tree species classifications often report error rates of around 20 fassnacht et al 2016 since errors are propagated to the final model output it is crucial for eo product users that these uncertainties are reported petrou et al 2015 rocchini et al 2010 uncertainties in eo are spatially heterogeneous so they should be reported spatially e g per pixel which can be achieved by using fuzzy classifications petrou et al 2013 or random forest classifiers breiman 2001 that provide a probability distribution of classes for each classified pixel although most es assessments rely on lulc classifications their quality could be further improved by including other eo based ecosystem properties cord et al 2017 for example we were able increase the certainty about actual land cover by including information on lidar based crown cover measurements nonetheless even if errors in eo data could be reduced uncertainties in the avalanche protection assessment would remain high due to natural variability model uncertainty and limited data availability this can be generalized to models of other es where complex socio ecological systems are modelled hou et al 2013 with limited data on es for model calibration and validation landuyt et al 2013 schulp et al 2014a to address this issue eo data should be used not only as a model inputs but also to calibrate validate and update our models of socio ecological systems plummer 2000 for example the data used to validate avalanche models christen et al 2010 veitinger et al 2016 is mostly limited to individual observations in the field detecting avalanches and their release areas from remote sensing bühler et al 2013 2009 could increase the dataset available for model calibration and validation thus reducing model uncertainties 4 3 advantages and limitations of the bayesian network approach a major advantage of bayesian networks as a tool for ecosystem services modelling is their probabilistic nature kelly letcher et al 2013 which allowed us to quantify different types of uncertainty in the es assessment an additional type of uncertainty that we did not explicitly address is structural uncertainty which relates to the selection of variables relevant to the model and the causal relationships between them ascough et al 2008 unlike model parameter uncertainty structural uncertainty is difficult to quantify particularly when validation data is lacking o hagan 2012 so it is often not discussed or only evaluated through expert assessment uusitalo et al 2015 bns can facilitate discussions about model structure with experts through the graphical representation of the variables and causalities in the network barton et al 2012 bromley 2005 landuyt et al 2013 voinov et al 2016 the stepwise sensitivity analysis supports this by visualizing the strength of the causal relationships and identifying nodes with large uncertainties which may indicate that important variables are missing from the model although the spatially explicit bn can capture uncertainties at the level of an individual pixel it is not able to take into account spatial interactions landuyt et al 2015 this is a major limitation in es modelling where spatial mismatches and cross scale effects are common bagstad et al 2013 for example linking es provision to demand would require integrating the provision across service providing areas villa et al 2014 in this case avalanche tracks this requires a spatial analysis that cannot be performed within the bn so information about probability distributions is lost johnson et al 2012 address this issue by using stochastic agent based models to map the flow of ecosystem services in some cases accounting for spatial interactions could also help reduce uncertainties for example a pixel is more likely to be in an avalanche release area if the area is large bühler et al 2013 i e if neighbouring pixels also have a high probability of release such interactions cannot be modelled directly in a bn but could potentially be addressed in two steps by calculating release probabilities for individual pixels and then correcting them based on the number of surrounding pixels with probabilities above a certain threshold however such fuzzy neighbourhood approaches depend on arbitrary threshold probabilities arnot et al 2004 neglecting the full information about pixels probability distributions 4 4 disentangling uncertainties in es assessments by performing the stepwise sensitivity analysis we were able to identify the components of the es model where uncertainties are high and where these uncertainties have a strong impact on the es assessment identifying such knowledge gaps could help define research priorities in the case of avalanche protection the uncertainties with the highest influence on the model output are related to the natural hazard process both in nodes that were quantified through expert knowledge e g the fuzzy definition of avalanche release areas and those based on models e g the process based model used to quantify avalanche velocities and detrainment improved identification of potential avalanche release areas under varying snow conditions bühler et al 2013 veitinger et al 2016 would significantly reduce uncertainties about the es while more sophisticated methods of forest type classification would have only a minor impact on the model output for other es where the underlying processes are better understood e g food production or carbon sequestration improved eo inputs could significantly improve es assessments andrew et al 2014 feng et al 2010 applying the same approach to disentangle uncertainties for other es would also help determine whether some methods of quantifying links between variables systematically produce higher uncertainties e g are expert assessments more uncertain than process based simulations quantifying uncertainties is also important for potential users of es assessments carpenter et al 2009 polasky et al 2011 who face trade offs between model accuracy and time data requirements vorstius and spray 2015 their decisions on which models and data to use require information on the associated uncertainties and how they propagate to the final es maps neuendorf et al 2018 moreover mapping uncertainties can improve model understanding and the credibility of the modelling results grêt regamey et al 2013 and may affect the decision making process kunz et al 2011 maceachren et al 2005 identifying the uncertainties that can be reduced through better models and data as well as understanding the uncertainties that are inherent to the system could lead to more robust decisions about es management ascough et al 2008 brunner et al 2017 acknowledgments this work was supported by the european union s horizon 2020 research and innovation programme ecopotential project grant agreement no 641762 we would like to thank yves bühler marc christen thomas feistl and michaela teich for their support with avalanche modelling and mauro marty christian ginzler lars t waser and fabian e fassnacht for their help with remote sensing methods we also appreciate the valuable discussions with enrico celio sven erik rabe victoria junquera and richard l peters as well as the constructive comments from three anonymous reviewers appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2018 09 005 
26294,ecosystem service es assessments are widely promoted as a tool to support decision makers in ecosystem management and the mapping of es is increasingly supported by the spatial data on ecosystem properties provided by earth observation eo however es assessments are often associated with high levels of uncertainty which affects their credibility we demonstrate how different types of information on es including eo data process models and expert knowledge can be integrated in a bayesian network where the associated uncertainties are quantified the probabilistic approach is used to map the provision and demand of avalanche protection an important regulating service in mountain regions and to identify the key sources of uncertainty the model outputs show high uncertainties mainly due to uncertainties in process modelling our results demonstrate that the potential of eo to improve the accuracy of es assessments cannot be fully utilized without an improved understanding of ecosystem processes keywords ecosystem services earth observation uncertainty bayesian network avalanche protection 1 introduction the ecosystem service es concept is increasingly promoted as a framework to support decision making convention on biological diversity 2010 european commission 2011 in order to improve the management of ecosystems and maintain the services they provide to society daily et al 2009 maes et al 2012 these efforts are supported by the growing body of scientific literature on es assessments schägner et al 2013 schröter et al 2016 and the increasing availability of spatial data particularly through earth observation eo which provides information on a variety of ecosystem properties andrew et al 2014 ayanu et al 2012 however the use of es assessments in planning and decision making remains limited albert et al 2014 es assessments are associated with large uncertainties which are often unreported schägner et al 2013 and different es assessment methods show inconsistent results eigenbrod et al 2010 schulp et al 2014a which may affect their credibility as tools for decision makers andrew et al 2015 ecosystem service assessments combine data on biophysical structures and processes with models of ecosystem function and measures of socio economic value de groot et al 2010 haines young and potschin 2009 modelling the whole es cascade haines young and potschin 2009 comprises not only various types of data and models but also various types of uncertainty ascough et al 2008 on the one hand uncertainty in these assessments stems from the inherent spatial and temporal variability of socio ecological systems regan et al 2002 this type of uncertainty cannot be reduced but should be taken into account in management decisions ascough et al 2008 on the other hand es assessments involve uncertainties that can potentially be reduced such as measurement errors model structure and parameter uncertainties and subjective judgment regan et al 2002 to realistically evaluate the level of confidence in es assessments all these types of uncertainty should be integrated maier et al 2008 and finally also communicated to users moreover understanding how the different sources of uncertainty propagate to the final assessment can help identify knowledge gaps and contribute to more robust decision making neuendorf et al 2018 polasky et al 2011 uusitalo et al 2015 the data most commonly used in es assessments are proxies describing ecosystem structure eigenbrod et al 2010 schägner et al 2013 such as land use land cover lulc costanza et al 1997 troy and wilson 2006 plant functional traits lavorel et al 2011 schirpke et al 2013 or aboveground biomass barredo et al 2008 nelson et al 2009 such data is subject to uncertainty due to limited sample sizes different data collection and processing techniques and sampling biases ascough et al 2008 earth observation eo is expected to reduce these uncertainties as it provides spatially explicit and up to date information on many of these ecosystem properties andrew et al 2014 cord et al 2017 feng et al 2010 so far the eo product most commonly used in es assessments is land cover cord et al 2017 however several studies have highlighted the shortcomings of lulc based es assessments eigenbrod et al 2010 plummer 2009 by combining lulc with other eo products such as ndvi biomass or vegetation density the accuracy of es assessments can potentially be improved andrew et al 2014 nonetheless eo data also contain measurement errors or misclassifications that are often not reported ayanu et al 2012 petrou et al 2015 the valuation of es further depends on proxies of demand for es such as visitor counts or travel cost estimates koetse et al 2015 wolff et al 2015 or social valuation methods such as choice experiments brunner et al 2015 garmendia and gamboa 2012 where subjective judgment plays an important role furthermore when categorical variables such as lulc are used differences in people s definitions of categories lead to linguistic uncertainty regan et al 2002 a wide variety of approaches is used to link proxies of ecosystem structure to ecosystem services lavorel et al 2017 most common are proxy based approaches where expert based look up tables are used to link lulc or habitat types to es provision kienast et al 2009 seppelt et al 2011 more complex approaches combine proxies with spatial analyses e g grêt regamey et al 2014 when sufficient data are available empirical models are used to predict the distribution of ecosystem service providers e g species schulp et al 2014b or to derive the link between ecosystem traits and es e g lavorel et al 2011 while process based models explicitly represent the mechanisms underpinning ecosystem functioning e g lautenbach et al 2013 however uncertainties in model parameters and structure are often not quantified schägner et al 2013 and many es models are unvalidated due to a lack of validation data schulp et al 2014a large discrepancies have been found between lulc based es maps and maps based on process based models eigenbrod et al 2010 highlighting the need to quantify and communicate uncertainties when using es to support decision making carpenter et al 2009 vorstius and spray 2015 in this paper we use a bayesian network bn to model avalanche protection an essential regulating service provided by mountain forests grêt regamey et al 2013 bns can include both expert knowledge and empirical data while their transparent graphical structure facilitates participatory modelling aguilera et al 2011 landuyt et al 2013 therefore bns have been used to address water management ames et al 2005 bacon et al 2002 land use change celio et al 2014 sun and müller 2013 and es modelling gonzalez redin et al 2016 grêt regamey et al 2013 landuyt et al 2013 the probabilistic structure of bns allows the quantification and propagation of uncertainties barton et al 2012 borsuk et al 2004 kelly letcher et al 2013 accounting for uncertainties is particularly relevant when modelling es related to natural hazards where extreme events at the tails of probability distributions are important straub and grêt regamey 2006 we use eo data to model both the provision and demand for avalanche protection and disentangle the effects of data quality and process understanding on uncertainty in the es assessment in addition we demonstrate how knowledge gaps can be identified and discuss how understanding the sources of uncertainty can help improve es assessment methods 2 methods 2 1 bayesian networks bayesian networks are directed probabilistic graphs where nodes represent the variables of the studied system and the links between nodes represent dependencies between them kjaerulff and madsen 2013 underlying the graph is a joint probability distribution p x p x1 xn n i 1 p xi pa xi which consists of a conditional probability distribution p xi pa xi of each node xi for each combination of its parent nodes pa xi states the conditional probabilities are expressed in conditional probability tables cpts or conditional continuous probability distributions the conditional probability of each node can be quantified independently borsuk et al 2004 which allows us to integrate various data and model types uusitalo 2007 and to account for different types of uncertainty evidence on any of the nodes is propagated through the bayesian network and the joint probability distribution is updated by applying bayes theorem p x pa x p x pa x p pa x evidence on input nodes will therefore result in a new updated posterior probability distribution of all other nodes in the network to efficiently perform inference most bayesian network software relies on algorithms such as the junction tree algorithm lauritzen and spiegelhalter 1988 which are limited to discrete or gaussian variables this means that most continuous variables need to be discretized which can lead to a loss of information benjamin fink and reilly 2017 landuyt et al 2013 ropero et al 2013 at the same time using discretized probability distributions means that non normal or even multi modal distributions can more easily be captured myllymäki et al 2002 uusitalo 2007 and non linear relationships can be expressed in cpts since increasing the number of discretization intervals exponentially increases the cpts the discretization is a trade off between accuracy and computational efficiency 2 2 accounting for uncertainty the probabilistic structure of the bayesian network allows us to incorporate uncertainty in the input data of the es model cha and stow 2014 as well as model uncertainties in the links between variables landuyt et al 2013 qian and miltner 2015 the methods to account for different types of uncertainty in the bn are summarized in table 1 when sufficient data is available to estimate the level of natural variability variables in the modelled system are characterized as probability distributions instead of single values for example the probability of heavy snowfall is commonly modelled using a gumbel extreme value distribution salm et al 1990 which we use as the prior probability distribution of max new snow height in the network when input data represents a measured proxy with a known error rate we make the uncertainty explicit by creating separate nodes representing the observed value y and the actual state x of the variable the observation is caused by the actual state not vice versa and defining the structure of the network based on this causality helps to define conditional probabilities we explain this principle on the example of a land cover classification classification errors are commonly expressed in confusion matrices which contain counts of predicted classes for objects where the true class is known e g from ground truth data with rows representing the classes in reality c and columns representing the classes predicted by the classification c based on these counts we can calculate either backward probabilities p x c y c e g the probability that a patch classified as forest is a forest in reality or the forward probabilities p y c x c that a forest patch will be classified as forest the backward probabilities depend on the prior distribution of land cover if we sample ground truth locations in a densely forested landscape it is likely that many of the patches classified as forest will in fact be forested leading to a higher backward probability than if we sample in a sparsely vegetated area however forward probabilities are inherent to the error process in the remote sensing data and the classification algorithm cripps et al 2009 and are therefore consistent over the whole area if we define the classification node y as the child of the actual class x the rows of its cpt correspond to the forward probabilities p y x for continuous variables with a known measurement error rate we similarly define the measurement node y as a child of the actual state of the variable x assuming a normal distribution of errors we can define the conditional probability of y as a normal distribution p y x x n x σ2 where the mean is the value of the actual state x and the standard deviation σ is defined by the measurement error if we have no prior information about the actual state of x a finding on the child y measurement node will then result in a normal distribution p x y y n y σ2 of the parent x actual state bayesian networks can incorporate information about links between variables that is already available in the form of empirical or process based models empirical models typically include information about the error in parameter estimates the model parameters can be included in the bn not as single values but as distributions by specifying equations such as y n β0 σβ0 2 n β1 σβ1 2 x1 n βn σβn 2 xn where x1 xn are the parent nodes of y β0 βn are the corresponding model parameter estimates and σβ0 σβn are the standard errors of the estimates the conditional probability distribution of y can be derived by repeatedly computing the value of y for each combination of its parents with parameter values sampled from the parameter distributions another approach to quantify links between variables is so called parameter learning when data on a child variable and its parents is available an algorithm such as expectation maximisation dempster et al 1977 lauritzen 1995 can be used to estimate the corresponding cpt when information about links between variables is available in the form of process based models the model outputs can be used as an input for parameter learning the uncertainties in the process based model can be captured by learning from monte carlo simulations with varying input parameters ames et al 2005 borsuk et al 2004 cain 2001 kuikka et al 1999 when data is limited and no models are available to quantify links between nodes the cpts can be elicited from experts expert elicitation is frequently used in ecology and risk assessments kuhnert et al 2010 speirs bridge et al 2010 and uncertainty in expert knowledge can be addressed by eliciting probability distributions rather than single values while experiments have shown that experts can more accurately estimate quantiles of a distribution than its mean and variance o hagan 2012 the estimates are often affected by overconfidence kuhnert et al 2010 to limit this problem speirs bridge et al 2010 developed the four point estimation method where experts are asked for the lowest and highest value they would expect the most likely value and their confidence that the true value is within this range metcalf and wallace 2013 we thus obtain information about the quantiles and mode of the distribution as well as its shape which allows us to fit a suitable distribution o hagan 2012 commonly normal distributions are used metcalf and wallace 2013 however in our case the elicited expert estimates for node potential detrainment showed an asymmetrical unimodal distribution so we chose to use a simple triangular distribution johnson 1997 other approaches to quantify uncertainty in expert knowledge involve combining estimates from several experts o neill et al 2008 for a review of expert elicitation methods see kuhnert et al 2010 often expert knowledge is related to qualitative categories rather than quantitative variables for example it may be easier for an expert to estimate the avalanche protection capacity of forests that are either open scattered or dense rather than based on a percentage of crown cover linking such categories to numerical values is associated with a type of linguistic uncertainty vagueness where the delineation between categories is not sharp regan et al 2002 linguistic uncertainty is commonly addressed using fuzzy logic zadeh 1965 zimmerman 1992 where membership functions m y define the level of membership between 0 and 1 in a specific class for continuous values of y for example we define trapezoidal membership functions m y of crown cover y for the classes of forest density x method adapted from petrou et al 2013 see appendix b at the expert defined threshold between a scattered and dense forest y 70 the probability of the forest being classified as dense is 0 5 while a forest with 100 crown cover will certainly be classified as dense p x dense 1 in the language of bayesian networks the membership function corresponds to the probability of the class x given an observation on y p x y y and can be used to populate the corresponding cpt when a class is defined by multiple attributes membership functions can be combined through fuzzy or or and operators zadeh 1965 as used by veitinger et al 2016 to identify potential avalanche release areas there are many methods to quantify uncertainty in the posterior probability distributions of bayesian network model outputs for continuous variables the spread of a distribution is commonly expressed with its second central moment the standard deviation however standard deviation is less informative for skewed distributions landuyt et al 2015 in information theory shannon s entropy shannon 1948 is used to quantify uncertainty in discrete variables h i 1 n p i log 2 p i where pi is the probability of state i and n is the number of states to evaluate uncertainty and compare it between output nodes with different numbers of states we calculate the evenness index hill 1973 of the posterior probability distribution j h hmax where hmax log2 n marcot 2012 the index has values between 0 and 1 where 1 denotes a uniform distribution between all possible states maximum uncertainty and 0 denotes complete certainty that the output node is in a specific state 2 3 sensitivity analysis and flow of information in bayesian network modelling sensitivity analysis is often used to evaluate the influence of variables in the modelled system on the posterior probability distribution of a node of interest marcot 2012 uusitalo 2007 sensitivity to findings can be measured by the reduction in uncertainty e g entropy or variance in the target node due to a finding on another node entropy reduction is expressed by the measure of mutual information kjaerulff and madsen 2013 i x y h x h x y h y h y x y p y x p x y log 2 p x y p x p y where h x is the entropy of x and h x y is the entropy of x after a new finding on y the analysis of sensitivity to findings gives us an indication of which variables in the system have the highest influence on the outcome of the model in addition we use a stepwise sensitivity analysis to visualize the flow of information in the network for each node x we calculate the proportion of its entropy that can be reduced by a finding on each of its parents pa x mi i x pa x h x these relative mutual information values are used as weights for links between nodes in a sankey diagram of the network which is used to identify the most relevant sources of uncertainty in the model when findings are added to the network e g setting the value of node y to state y this alters the probability distributions and sensitivities of other nodes and the sensitivity to node y becomes zero therefore we also perform the stepwise sensitivity analysis for specific combinations of input variables to identify sources of uncertainty under different conditions 2 4 case study avalanche protection we illustrate the approach to quantify uncertainties in eo based es assessments on the example of a regulatory es avalanche protection the case study is located in the region of davos in the eastern part of the swiss alps the principal town davos is a well developed urban and touristic centre located in the central part of the main valley at an elevation of 1500 m above sea level the rest of the main valley and the three side valleys are relatively rural with a few scattered settlements and a landscape still strongly dominated by mountain agriculture snow avalanches are the most common natural hazard in the area kulakowski et al 2011 and mountain forests play a key role in reducing the risk for settlements below through two main functions prevention and detrainment the probability for an avalanche release depends on topography bühler et al 2013 veitinger et al 2016 but is lower in forested areas bebi et al 2009 when an avalanche flows through a forest some of the snow is stopped behind trees detrainment which reduces the mass and velocity of the avalanche feistl et al 2014 teich et al 2014 the anthropogenic value of avalanche protection can be quantified based on the risk to people and buildings planat 2008 previous es valuations indicate that avalanche protection is among the most valuable es in the region of davos grêt regamey et al 2013 we based our avalanche protection model on previous models developed for this es grêt regamey et al 2013 grêt regamey and straub 2006 but extended it to incorporate newly available remote sensing inputs as well as recent developments in modelling forest avalanche interactions the bn structure fig 2 was developed through an iterative process of literature review consultation with experts and testing the behaviour of the network with different input values the bn was constructed in netica norsys 2010 where we also performed sensitivity analyses using the function sensitivity to findings the data available for modelling the avalanche protection service are in situ data on the temporal and spatial distribution of avalanches and remote sensing variables which are proxies for the actual state of the ecosystem we accounted for the spatial variability of the avalanche process by running the bn for each pixel of a 5 m resolution raster of the study area we used input data that describe the spatial patterns of the hazard process under a frequent 30 year and extreme 300 year scenario velocity 30y and velocity 300y where occurrence of both scenarios depends on the probability of heavy snowfall the temporal variability of these events is incorporated through a probability distribution of maximum new snow heights based on long term observations slf 2017 high resolution lidar data august 2015 lms q780 sensor ca 20 points m2 data was processed using lastools isenburg 2016 to derive 1 m resolution digital terrain dtm and canopy height chm models to measure crown cover in forests moeser et al 2014 and terrain roughness sappington et al 2007 and to detect buildings the chm was combined with an aerial cir image august 2013 leica ads 80 0 25 m resolution swisstopo 2013 and a sentinel2 image from may 2016 european space agency 2016 for an object based supervised random forest classification into non forested areas evergreen and deciduous forests fassnacht et al 2016 ground truth data was collected at 110 plots in the valley to train the classification and to estimate the measurement and classification uncertainties in the remote sensing data ecosystem structure and processes were linked to ecosystem functions using fuzzy logic crown cover class release veitinger et al 2016 expert knowledge potential detrainment an empirical model from literature prevention bebi et al 2001 and learning from process based simulation results christen et al 2010 detrainment since the simulation results showed high spatial autocorrelation we did not perform the learning directly in the bn software but fitted a spatial regression model in r pinheiro et al 2017 r core team 2013 and used it to populate the cpt in order to combine both ecosystem functions the total per pixel level of es provision was expressed in the quantity of snow prevented from releasing or stopped which is the es benefit carrier in this case bagstad et al 2013 to quantify the demand for avalanche protection we used a probabilistic risk assessment approach grêt regamey and straub 2006 with values of risk factors as determined by experts for evaluating protection measures against natural hazards bafu 2015 merz et al 1995 at each step the uncertainties are quantified as described in section 2 2 the bn for avalanche protection was applied for the lower dischma one of the side valleys of davos see fig 1 using an application based on the netica api celio et al 2014 we set evidence on nodes where data is available and performed inference for each pixel in a 5 m resolution raster of the study area since the provision and demand for avalanche protection do not occur at the same location and spatial processes could not be modelled in the pixel based bn we quantified provision and demand separately thus we obtained posterior probability distributions of avalanche protection provision and demand for each pixel in order to map the outputs we calculated the per pixel median and evenness index uncertainty of the posterior probability distributions to illustrate the process of inference in the bn we show the joint probability distributions of all variables for some example pixels in appendix d 3 results the process of integrating available data models and knowledge on the avalanche protection service resulted in a bn with 37 nodes and 53 links which is shown in fig 2 the inputs to the model are remote sensing variables and in situ data on avalanches these are linked to intermediate nodes that describe ecosystem structure the natural hazard process and risk assessment the model outputs are posterior probability distributions of the provision expressed in height of snow stopped by the forest and the demand for avalanche protection expressed in chf for forested areas the model predicts a bimodal distribution of es provision with a peak at 0 corresponding to conditions with no avalanche events and another between 0 1 and 0 5 m of snow prevented from releasing and or stopped during avalanches on average areas with a predicted value of provision above 0 have a cv of 110 descriptions of the bn nodes and their states are provided in the supplementary material appendix a as well as examples of posterior probability distributions appendix d for es provision and demand the spatially explicit model output of es provision shows a high spatial heterogeneity fig 3 areas with a high level of avalanche protection provision are the steeper densely forested areas particularly at high elevations where larger avalanche releases are more likely although eo inputs particularly the land cover classification are more uncertain in heterogeneous forests near the upper tree line this pattern is not reflected in the spatial distribution of uncertainty in the provision of the avalanche provision the uncertainty is related to the level of avalanche protection where pixels with high levels of provision show high levels of uncertainty in addition there are many areas with a low predicted value of avalanche protection provision but a high uncertainty indicating that these forests may provide no or only limited avalanche protection under certain infrequent extreme conditions higher levels of certainty are achieved only in areas with a very low or zero level of protection service the factors underlying the spatial distribution of the es were analysed using a sensitivity analysis of the target nodes of the bn provision and demand table 2 provision of avalanche protection is most sensitive to nodes describing the ecosystem functions and the avalanche process the modelled provision is more sensitive to inputs of in situ avalanche data especially the distribution of maximum new snow height than to remote sensing variables of ecosystem structure among these the lidar derived crown cover is most important since inference can run in different directions in a bn the sensitivity analysis also shows indirect influences for example knowledge about potential lethality of avalanches on a specific pixel would increase the knowledge about the potential provision of avalanche protection at that location on the demand side the most influential node is the cost of damage to buildings while the most important input are buildings detected from lidar overall the nodes closer to the target variables have a stronger influence than nodes farther away this is due to uncertainty in the intermediate links for example detecting a building from remote sensing mi 49 6 has a smaller effect on the distribution of demand than certain knowledge of a building s location would mi 60 8 similarly certain knowledge of the actual land cover mi 4 98 would more strongly reduce the uncertainty about provision than the land cover classification does mi 1 42 because there is some uncertainty in the classification in order to understand these relationships in more detail we performed a stepwise sensitivity analysis the results of the stepwise sensitivity analysis are visualized in a sankey diagram fig 4 for each node the thickness of incoming from the left links show how much the entropy on the node can be reduced by findings on preceding nodes mutual information is not additive i e if both parent nodes can reduce the entropy of a child by 50 this does not mean that findings on both parents will result in complete certainty on the child node nonetheless plotting the mi gives an indication of the main sources of uncertainty in the model when the value of mi for all the parents of a node is rather low this means that the node will have a wide probability distribution even when the states of its parents are known implying high uncertainty in the corresponding links if such a node has a large influence on the outcome of the network this indicates a knowledge gap overall the uncertainties related to avalanche processes contribute more to the final uncertainty in es provision than uncertainties about ecosystem structure for example the node release describing whether a pixel is in a potential avalanche release area has an important influence on subsequent nodes in the network but findings on its parents slope roughness measured and curvature can only reduce a small part of its entropy so it is a major source of uncertainty in the model some remote sensing inputs have a strong effect on the knowledge about ecosystem structure gap width and crown cover while others have higher uncertainty e g roughness there is high uncertainty in land cover classification as its mutual information with actual land cover is only 29 however additional information on actual land cover is gained from the crown cover class mi 59 the links from ecosystem structure to the potential provision of es also contain high uncertainty regarding both the potential of a forest to prevent avalanches empirical model based potential prevention and to stop snow during an avalanche expert based potential detrainment however potential detrainment has a relatively low influence on the corresponding ecosystem function process model based detrainment this function is affected more strongly by the avalanche process velocity which in turn is affected by the natural variability in release conditions max new snow height on the demand side appendix c the remote sensing input building detected is rather certain while uncertainty about the total risk is most affected by the natural variability of the avalanche process additionally uncertainty comes from the wide distribution of building types which affect the costs of potential damages and number of people per building and which could not be differentiated in the remote sensing input the sensitivities of the bn change after we enter evidence and are therefore different for each combination of input nodes nonetheless the general pattern remains the same with high uncertainties related to the avalanche processes due to natural variability and model uncertainty furthermore the uncertainty about es provision would be significantly reduced by additional knowledge on avalanche release areas release node examples of sensitivities for posterior probability distributions after input data are added to the network are shown in appendix d for one pixel of es provision and demand 4 discussion 4 1 uncertainties in avalanche protection in this study we used recent developments in eo techniques and natural hazard modelling to assess avalanche protection by forests an important es in mountain regions we integrated eo data empirical and process based models and expert knowledge into a bn while accounting for the uncertainty in each of these components thus we were able to quantify the total uncertainty in the es assessment and evaluate the influence of different sources of uncertainty on the model output although high resolution eo data was available in our study area uncertainties in the es assessment remain high with a coefficient of variation well above 100 while there was some uncertainty in the eo products used these had a limited effect on the final model output the total uncertainty was more strongly affected by the uncertainties regarding avalanche processes particularly the variability of snow heights the probability of avalanche releases which was defined using a fuzzy approach based on expert knowledge veitinger et al 2016 and avalanche velocities and detrainment in forests which were quantified based on a process based model christen et al 2010 these uncertainties can be explained in part by the high natural variability of avalanche hazards related to complex terrain and temporal variability in snow and weather conditions schweizer 2008 in addition currently available avalanche models and expert knowledge are based on limited observational data bühler et al 2009 which contributes to high model uncertainty 4 2 added value of earth observation data by combining different eo inputs high resolution lidar aerial and satellite multispectral images we could include more information at a higher spatial resolution compared to previous es assessments in the region which relied mainly on lulc data grêt regamey et al 2013 we were able to differentiate tree species and measure terrain roughness both of which have an impact on ecosystems potential to provide avalanche protection feistl et al 2014 teich and bebi 2009 the use of eo data enabled us to model the es at a 5 m spatial resolution which allows us to observe the high spatial heterogeneity of es provision in complex terrain and identify individual forest stands that are particularly important for avalanche protection however the eo products used particularly the land cover classification contained considerable uncertainties high error rates are common in classifications for example tree species classifications often report error rates of around 20 fassnacht et al 2016 since errors are propagated to the final model output it is crucial for eo product users that these uncertainties are reported petrou et al 2015 rocchini et al 2010 uncertainties in eo are spatially heterogeneous so they should be reported spatially e g per pixel which can be achieved by using fuzzy classifications petrou et al 2013 or random forest classifiers breiman 2001 that provide a probability distribution of classes for each classified pixel although most es assessments rely on lulc classifications their quality could be further improved by including other eo based ecosystem properties cord et al 2017 for example we were able increase the certainty about actual land cover by including information on lidar based crown cover measurements nonetheless even if errors in eo data could be reduced uncertainties in the avalanche protection assessment would remain high due to natural variability model uncertainty and limited data availability this can be generalized to models of other es where complex socio ecological systems are modelled hou et al 2013 with limited data on es for model calibration and validation landuyt et al 2013 schulp et al 2014a to address this issue eo data should be used not only as a model inputs but also to calibrate validate and update our models of socio ecological systems plummer 2000 for example the data used to validate avalanche models christen et al 2010 veitinger et al 2016 is mostly limited to individual observations in the field detecting avalanches and their release areas from remote sensing bühler et al 2013 2009 could increase the dataset available for model calibration and validation thus reducing model uncertainties 4 3 advantages and limitations of the bayesian network approach a major advantage of bayesian networks as a tool for ecosystem services modelling is their probabilistic nature kelly letcher et al 2013 which allowed us to quantify different types of uncertainty in the es assessment an additional type of uncertainty that we did not explicitly address is structural uncertainty which relates to the selection of variables relevant to the model and the causal relationships between them ascough et al 2008 unlike model parameter uncertainty structural uncertainty is difficult to quantify particularly when validation data is lacking o hagan 2012 so it is often not discussed or only evaluated through expert assessment uusitalo et al 2015 bns can facilitate discussions about model structure with experts through the graphical representation of the variables and causalities in the network barton et al 2012 bromley 2005 landuyt et al 2013 voinov et al 2016 the stepwise sensitivity analysis supports this by visualizing the strength of the causal relationships and identifying nodes with large uncertainties which may indicate that important variables are missing from the model although the spatially explicit bn can capture uncertainties at the level of an individual pixel it is not able to take into account spatial interactions landuyt et al 2015 this is a major limitation in es modelling where spatial mismatches and cross scale effects are common bagstad et al 2013 for example linking es provision to demand would require integrating the provision across service providing areas villa et al 2014 in this case avalanche tracks this requires a spatial analysis that cannot be performed within the bn so information about probability distributions is lost johnson et al 2012 address this issue by using stochastic agent based models to map the flow of ecosystem services in some cases accounting for spatial interactions could also help reduce uncertainties for example a pixel is more likely to be in an avalanche release area if the area is large bühler et al 2013 i e if neighbouring pixels also have a high probability of release such interactions cannot be modelled directly in a bn but could potentially be addressed in two steps by calculating release probabilities for individual pixels and then correcting them based on the number of surrounding pixels with probabilities above a certain threshold however such fuzzy neighbourhood approaches depend on arbitrary threshold probabilities arnot et al 2004 neglecting the full information about pixels probability distributions 4 4 disentangling uncertainties in es assessments by performing the stepwise sensitivity analysis we were able to identify the components of the es model where uncertainties are high and where these uncertainties have a strong impact on the es assessment identifying such knowledge gaps could help define research priorities in the case of avalanche protection the uncertainties with the highest influence on the model output are related to the natural hazard process both in nodes that were quantified through expert knowledge e g the fuzzy definition of avalanche release areas and those based on models e g the process based model used to quantify avalanche velocities and detrainment improved identification of potential avalanche release areas under varying snow conditions bühler et al 2013 veitinger et al 2016 would significantly reduce uncertainties about the es while more sophisticated methods of forest type classification would have only a minor impact on the model output for other es where the underlying processes are better understood e g food production or carbon sequestration improved eo inputs could significantly improve es assessments andrew et al 2014 feng et al 2010 applying the same approach to disentangle uncertainties for other es would also help determine whether some methods of quantifying links between variables systematically produce higher uncertainties e g are expert assessments more uncertain than process based simulations quantifying uncertainties is also important for potential users of es assessments carpenter et al 2009 polasky et al 2011 who face trade offs between model accuracy and time data requirements vorstius and spray 2015 their decisions on which models and data to use require information on the associated uncertainties and how they propagate to the final es maps neuendorf et al 2018 moreover mapping uncertainties can improve model understanding and the credibility of the modelling results grêt regamey et al 2013 and may affect the decision making process kunz et al 2011 maceachren et al 2005 identifying the uncertainties that can be reduced through better models and data as well as understanding the uncertainties that are inherent to the system could lead to more robust decisions about es management ascough et al 2008 brunner et al 2017 acknowledgments this work was supported by the european union s horizon 2020 research and innovation programme ecopotential project grant agreement no 641762 we would like to thank yves bühler marc christen thomas feistl and michaela teich for their support with avalanche modelling and mauro marty christian ginzler lars t waser and fabian e fassnacht for their help with remote sensing methods we also appreciate the valuable discussions with enrico celio sven erik rabe victoria junquera and richard l peters as well as the constructive comments from three anonymous reviewers appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2018 09 005 
