index,text
25910,a system dynamic model smituv system dynamic modeling of infiltration solute transport and root water uptake in vadose zone was developed for simulating transient soil water flow solute transport and root water uptake in crops under water and salinity stress in a multilayered unsaturated soil layer smituv solves the mixed form one dimensional richards equation with a sink term for water transport and root water uptake and advection diffusion equation for solute transport it simulates salinity and water stress on root water uptake as a function of osmotic and matric potential smituv is based on fundamental physical principals and is robust enough to handle different scenarios and what if runs and also has tools that are crucial for growers to understand the salt water soil system the model is designed to be a planning tool for strategic desalination methods and field management measures that optimize irrigation water use for flood irrigated arid region crops 1 introduction irrigation in the arid and semi arid regions is complicated due to the presence of salinity in the soil salinity is caused due to the presence of a high concentration of salts in the soil that reduces the amount of available water for root water uptake by crops the reduction in the root water uptake combined with the effects of drought and other environmental conditions limits the productivity of crop plants by 20 50 of their maximum yield shrivastava kumar 2015 a wide range of salinity stress management strategies are required to overcome such impacts of salinity on crop productivity keeping track of salinity in the soil and its associated reduction in root water uptake transpiration and crop productivity is the first step in understanding the salinity stress modeling the soil water movement root water uptake and solute transport play an important role in assessing the salinity stress and its related impacts on crops šimůnek et al 1996 in addition evaporation and plant transpiration also plays a vital role in the solution composition water and solute distribution in subsurface conditions šimůnek et al 1996 the first approach to understand the complicated relationship of salinity and crop growth was quantified by physically measuring the salt tolerance of various crops in a laboratory condition bernstein 1956 this was followed with separate models on root water extraction using microscopic gardner 1960 molz et al 1968 and macroscopic approaches dutt et al 1972 and salt transport bresler 1973 the first combined model for soil water flow and root water extraction was proposed by nimah and hanks 1973 a comprehensive model combining soil water flow in unsaturated soil root water extraction and solute transport was developed by childs 1975 as an extension to the work by nimah and hanks 1973 later various numerical models for the simulation of 1 dimensional water flow and solute transport were developed most models developed are closed source and monolithic with all the process build in and limited extension capacity these models may be broadly categorized as steady state and transient models the steady state model watsuit rhoades and merrill 1976 divides the root zone to four different zones vertically and assumes the root water extraction to be in the ratios of 40 30 20 10 it has a function of precipitation dissolution based on the presence or absence of caco3 as an option whereas the transient model simulates the continually changing soil water salt effects on evapotranspiration osmotic and matric effects on root water extraction multi component major ion chemistry and transport precipitation dissolution cation exchange carbon dioxide heat production and transportation some of the major transient models are enviro gro pang and letey 1998 saltmed ragab 2002 swap van dam 2000 unsatchem šimůnek et al 1996 suarez and šimůnek 1997 and hydrus šimůnek j huang and van genuchten 1998 šimůnek j van genuchten and šejna 2005 šimůnek m šejna saito sakai and genuchten 2013 vogel et al 1996 the functionality of these models differs in calculation of the stresses in the root water extraction component where saltmed and enviro gro uses an additive function and swap hydrus and unsatchem uses a multiplicative function while integrating the osmotic and matric stress additionally unsatchem calculates the osmotic coefficient using the pitzer equations from the major ion chemistry and incorporates a hydraulic reduction function due to salinity sodicity interactions that further reduces the soil water flow oster letey vaughan wu and qadir 2012 conducted a comprehensive comparison of the simulated results on the yield of forage corn of these models these models are often developed using fortran and require significant expertise to integrate and modify various components on the other hand smituv system dynamic modeling of infiltration solute transport and root water uptake in vadose zone model developed in this study was designed to engage and educate growers and allow for easy modification to the problem at hand with a graphical system dynamic development environment the inherent systems approach allows the user to better understand the system connections for example how salinity hydraulic conductivity and crop uptake are interlinked 1 1 system dynamic approach system dynamics is an approach to understand the nonlinear complex dynamic relationship between systems over time utilizing the feedback complexity of elements in the form of equations using stocks flows internal feedback loops and functions saysel and barlas 2001 sterman 2000 these equations form the overall structure of the model and a well formulated structure can effectively simulate the dynamics of soil water flow root water extraction and solute transport the system dynamics approach has been successfully used for various hydrological and watershed studies keshta et al 2009 ouyang et al 2016 a recent study using the system dynamic approach successfully simulated the infiltration of water in the unsaturated zone using darcy s equation showing the effectiveness of this approach huang et al 2011 in this approach the dynamic relation of the input and its downward or upward movement is simulated based on the system s framework represented by equations and the feedback mechanism that is either reinforcing positive feedback loop or counteracting negative feedback loop huang et al 2011 no studies were found that used the system dynamic approach to simulate the transient combined soil water flow root water extraction and solute transport a system dynamic approach is used to develop an interactive participatory model that can be used by growers and other stakeholders without having to hurdle through the complex options of the already existing software such as hydrus smituv is developed in the isee stella architect software stella software codes in the whole simulation process using a graphical icon based skeletal structure explained in section 2 2 this kind of skeletal structure makes it easier for agricultural researchers or students to understand the iterations involved in the simulation and the dynamic relationship between variables and processes they can easily insert or change a variable at any juncture of the model without even having to know coding since in stella the equation can be directly inputted using variables this enhances the modification or addition of a new module or process in the already existing model to conduct various sensitivity analysis also the model can be presented to the general public in isee exchange website to interact with the smituv model using buttons tables and interactive graphs as discussed in section 6 0 this approach will make smituv a more user friendly than the existing software such as hydrus saltmed etc smituv is developed keeping in mind the common growers and research community and dedicated for future research in the field of applied engineering in agriculture 1 2 model rationale irrigation systems around the world rely on flood irrigation using surface and ground water nurzaman 2017 the irrigation systems have been stressed and adversely affected by the local and global change in climate water scarcity with concomitant pressures from other consumers e g growing cities degrading soil and water quality particularly influx of salinity and degrading rural economic condition corlett and westcott 2013 nurzaman 2017 sheffield et al 2012 these contributing factors can be characterized as complex with many nonlinear feedbacking loops that require a system dynamic approach to develop a mental model for understanding and decoding the system the components of this complex system are dynamically related spatially and temporally the nonlinearity and in some case spatio temporal stochasticity of the relations often means that linear equation cannot be used for representing the interrelation of the components therefore decoding involves numerous variables and complex feedback loops this makes most of the conventional monolithic models developed for field scale irrigation and salinity management complicated requiring extensive expert assistance to be used by growers key stakeholders and with limited education potential after considering a web based wrapper use for other legacy models kumar et al 2015 which are complicated to maintain and do not solve the problem of limited extensibility of the conventional monolithic models an extensible systems dynamic modeling approach was used the solution was to develop a mental model that meets the needs of a grower as well as create multi stakeholder participation that can adapt to the changing perceptions problems of interest and new scientific innovations this model has been developed considering the changing requirement in the future such as groundwater upwelling ag desalination different irrigation techniques global climate change use of fertilizers and pesticides that can be integrated with ease by the other developers or savvy users though designed in isee stella for rapid development and deployment most systems dynamic models have the capability to be converted to a python and module in other languages naimi voinov 2012 for integration with component based modeling systems e g csdms peckham et al 2013 and openmi gregersen et al 2007 and future extensibility 2 model overview the system dynamic model smituv simulates the transient soil water flow root water uptake and solute transport in the unsaturated zone of a flood irrigated field the model can also quantify the effects of root water uptake under salinity and water stress to validate the numerical processes we have compared the results with an established numerical model hydrus it may be noted that smituv is graphical that are easier to interpret and visualize particularly for non expert stakeholders and promotes grower education about the processes 2 1 causal loop diagram smituv uses feedback loops to dynamically relate the systems to each other through reinforcing positive feedback loop represented by and counteracting negative feedback loop represented by loops as shown in fig 1 the first loop loop 1 is water infiltration to the first soil layer that increases the amount of stored water in the first layer this increase in water increases the water pressure head that reduces the infiltration limiting the amount of water being stored the second loop loop 2 is the effect on hydraulic conductivity where the stored water in the soil increases the water content that increases the hydraulic conductivity resulting in higher infiltration rate increasing the stored water in the soil the third loop loop 3 is the extraction of water by crop roots that increases the water pressure head resulting in the build up of water stress limiting the water extracted by roots from the soil layer the fourth loop loop 4 is the effect of salt infiltration that increases the amount of salt in the soil layer resulting in the build up of salinity stress limiting the water extracted by roots from the soil layer the fifth loop loop 5 is the increase in sodicity of the soil that acts in combination with the soil ph reducing the hydraulic conductivity limiting the water storage in the soil 2 2 model approach and structure soil is heterogeneous in nature and a numerical solution of the richards equation is required for simulating the one dimensional flow in unsaturated soil parissopoulos and wheater 1990 the system dynamic approach is used for numerically solving richards equation using a finite difference method a similar technique was used by celia et al 1990 the unsaturated soil hydraulic properties are based on a set of closed form equations van genuchten 1980 and using the capillary model of mualem 1976 root water uptake is modeled using a sink term in richards equation that was first proposed by feddes and zaradny 1978 and later modified to include osmotic stress by van genuchten 1987 solute transport between multi soil layer is simulated by numerically solving the advection diffusion equation for a non reactive and non interactive solute freeze and cherry 1979 using a finite difference method celia et al 1990 the model is developed using isee systems stella architect software a daily time step was used for all simulations due to the binary arithmetic that the computer uses the time step between calculations delta time dt in the model is set at 0 125 that falls in the sequence of 1 2 n i e every 1 8th of a day thus optimizing the computational speed and avoiding round off errors hps inc 2001 fig 2 shows the icon based skeletal structure of smituv it contains rectangular blocks that are the stock variables representing the accumulation of water and solute in soil layers and water in roots the soil water infiltration and root water uptake rate and solute transport flux are simulated using the flow variable symbolized by valves between the stocks variables and equations leading to the formulation of these flow variables are developed using converters symbolized by circles the converters are connected to the flow variables and themselves using connectors symbolized by a line and arrow at the end in the model structure soil layers are divided into three compartments upper middle and bottom layers it has four sectors soil water flow swf simulating the unsaturated soil water flow solute transport st simulating the transport of solute between layers root water uptake rwu simulating extraction of water by roots under water and salinity stress from soil layers and hydraulic reduction h r simulating the reduction in hydraulic conductivity due to sodicity and ph of soil all of the four sectors are interconnected based on various formulations and empirical relationships discussed above the building blocks within a sector can be partially simulated while keeping all other blocks at initial condition allowing to set various combinations of simulation 3 model formulation the smituv model is formulated using the stock flow converter based system dynamics approach to simulate the soil water movement root water extraction and solute transport 3 1 soil water flow sector swf the one dimensional water flow in an unsaturated incompressible porous media is best described using the modified form of richards equation richards 1952 formulated as 1 θ t z k d h d z 1 s where θ is the water content cm3 cm 3 h is the water pressure head capillary suction cm t is time days z is the depth of soil layer cm and s is the sink term cm3 cm 3day 1 representing the root water extraction by the plants the hydraulic conductivity and the capillary suction are calculated using the mualem 1976 and van genuchten 1980 equations that were later upgraded by suarez and šimůnek 1997 to include the effects of soil chemical properties such as sodicity and ph 2 θ h θ r θ s θ r 1 α h n m and 3 r k θ r k s s e 1 2 1 1 s e 1 m m 2 s e 1 r k s s e 1 respectively where 4 m 1 1 n n 1 5 s e θ θ r θ s θ r and where θ r and θ s are the residual and saturated water content cm3 cm 3 respectively k s is the saturated hydraulic conductivity cm day s e is the relative hydraulic conductivity r is the hydraulic reduction function due to the soil chemical properties and n and α cm 1 are the van genuchten 1980 parameter of the soil water retention curve swrc 3 1 1 finite difference approximation the conventional numerical solution of richards equation considers the water balance of an infinitely small soil volume kroes et al 2008 whereas in smituv soil layers are represented by the stock variables and the rate of water movement is represented by the flow variables thus an implicit backward finite difference method is used to solve richards equation transforming it to a discretized form kroes et al 2008 6 θ t 1 δ z i k i 1 2 h i 1 h i δ z i 1 δ z i 2 1 k i 1 2 h i h i 1 δ z i δ z i 1 2 1 s i where the subscript i represents the ith soil layer with values ranging from 1 to 3 and δ z i is the soil compartment thickness the sink term s i representing root water uptake is simulated as a separate sector that flows out from each soil layer 3 1 1 1 boundary conditions the initial and top boundary condition is the flux of water p cm day entering the topsoil layer 7 p r a i n f u l l c m d a y i r r i g a t i o n w a t e r c m d a y the bottom boundary condition is free drainage condition with percolation rate from the bottom layer equal to the hydraulic conductivity of the bottom layer the percolated water is collected to an end stock for quantifying the amount of water drained out of the soil column 3 1 1 2 water flow equations the rate of water entering the upper soil layer has the term k i 1 2 h i 1 h i δ z i 1 δ z i 2 1 in eq 6 replaced by top boundary condition 8 i p k 1 k 2 2 h 1 h 2 z 1 z 2 2 1 the flow equation from the upper to middle soil layer is formulated as 9 t 1 k 1 k 2 2 h 1 h 2 z 1 z 2 2 1 k 2 k 3 2 h 2 h 3 z 2 z 3 2 1 the rate of water flowing from middle to bottom soil layer has the term k i 1 2 h i h i 1 δ z i δ z i 1 2 1 replaced with the bottom boundary condition 10 t 2 k 2 k 3 2 h 2 h 3 z 2 z 3 2 1 k 3 major stock s converters c and flow f variables of swf sector is listed in appendix a 3 2 solute transport sector st the solute transport in the subsurface soil under transient water flow condition is given by freeze and cherry 1979 van dam 2000 11 θ c t q c z z θ d d i f f d c d z where c is the total solute concentration per unit volume of soil g cm3 q is darcy s volumetric flux cm day and d d i f f is the diffusion constant cm2 day given by kroes et al 2008 millington and quirk 1961 12 d d i f f d w θ 7 3 θ s 2 where d w is the solute diffusion coefficient in free water having a value of 2 cm2 day and q is given by darcy s law 13 q k h z 1 3 2 1 finite difference approximation the advection diffusion equation is numerically solved using an explicit central finite difference scheme given by kroes et al 2008 14 c t 1 θ i δ z i q i 1 2 c i 1 2 θ i 1 2 d i 1 2 c i 1 c i 1 2 δ z i 1 δ z i q i 1 2 c i 1 2 θ i 1 2 d i 1 2 c i c i 1 1 2 δ z i δ z i 1 smituv simulates the solute flow as flux and therefore to represent the solute concentration as solute flux left and right hand side of the equation is multiplied with the thickness of the soil compartment δ z i 3 2 1 1 boundary conditions the initial condition is the solute concentration g cm3 that is accumulated on the soil surface from the dissolved salts in rain river and ground water the top boundary condition is the total solute entering the upper soil layer c top calculated by multiplying initial solute concertation with ponded depth the bottom boundary condition represents leaching of solute under gravity equal to the product of solute concentration and darcy s volumetric flux of the bottom soil layer the leached solute is collected to an end stock for quantifying the amount of solute leached out of the soil column 3 2 1 2 solute flux equations the rate of solute entering the upper soil layer has the term 1 θ i δ z i q i 1 2 c i 1 2 θ i 1 2 d i 1 2 c i 1 c i 1 2 δ z i 1 δ z i in eq 14 replaced with the top boundary condition 15 i s c t o p z 1 θ 1 z 1 q 1 q 2 2 c 1 c 2 2 θ 1 θ 2 2 d 1 d 2 2 c 1 c 2 z 1 z 2 2 the rate of solute flowing from upper to bottom soil layer is given by 16 t s 1 z 2 θ 2 z 2 q 1 q 2 2 c 1 c 2 2 q 2 q 3 2 c 2 c 3 2 θ 1 θ 2 2 d 1 d 2 2 c 1 c 2 z 1 z 2 2 θ 2 θ 3 2 d 2 d 3 2 c 2 c 3 z 2 z 3 2 the rate of solute flowing from middle to bottom soil layer has the term 1 θ i δ z i q i 1 2 c i 1 2 θ i 1 2 d i 1 2 c i c i 1 1 2 δ z i δ z i 1 in eq 14 replaced with the bottom boundary condition 17 t s 2 z 3 θ 3 z 3 q 2 q 3 2 c 2 c 3 2 θ 2 θ 3 2 d 2 d 3 2 c 2 c 3 z 2 z 3 2 c 3 q 3 the final output of smituv gives the total solute concentration g cm 3day 1 for the soil layers that is calculated by multiplying the solute flux g cm 2day 1 with the soil compartment thickness major stock s converters c and flow f variables of s t sector are listed in appendix a 3 3 root water uptake rwu the water uptake rate of roots from the soil layer is simulated using the sink term s i from eq 6 the root water uptake is depended on the soil water stress osmotic stress root characteristics and evapotranspiration skaggs et al 2006 a macroscopic approach was adopted in calculating the rwu rate feddes et al 1978 van genuchten 1987 18 s β α h α s t p where β is the normalized root distribution function α h is the water stress response function α s is the salinity stress response function and t p is the potential evapotranspiration in order to account for root water uptake at each soil layer uptake equation is formulated using the flow variable connected to the stock variable representing each soil layer 19 u r i β i α h i α s i t p where i represent the ith soil layer with i equal to 1 2 and 3 on its other end the flow variable is connected to an end stock variable representing the total water stored or extracted by the roots this is important to understand the actual water that has been transpired by the plant under stress major stock s converters c and flow f variables in rwu sector is listed in appendix a 3 3 1 water stress water stress is the reduction of rwu due to the buildup of water pressure head normalized as a water stress response function 0 α h 1 20 α h 1 1 h h 50 3 where h 50 is an empirical constant representing the pressure head at 50 root water extraction having a value of 150 cm taylor and ashcroft 1972 3 3 2 salinity stress salinity stress is the reduction of rwu due to the buildup of osmotic potential by the accumulated salt in root zone normalized as a salinity stress response function 0 α s 1 oster et al 2012 van genuchten 1980 21 α s 1 1 e c e c 50 3 where e c 50 ds m is the threshold value of crop salt tolerance and e c ds m is the total salt concentration tds in the root zone calculated from the s t sector and empirically converted using wallender and tanji 2011 22 640 δ e c d s m t d s m g l 3 4 hydraulic reduction hr the hydraulic reduction is a normalized reduction function 0 r 1 that decreases the effective soil hydraulic conductivity by exchangeable sodium and adverse ph present in the soil high level of exchangeable sodium causes clay dispersion and swelling of the soil mcneal 1968 šimůnek et al 1996 suarez and šimůnek 1997 and soil ph decreases the effective hydraulic conductivity suarez et al 1984 the normalized function is given by 23 r r 1 r 2 where r 1 is the normalized function for exchangeable sodium mcneal 1968 šimůnek et al 1996 suarez and šimůnek 1997 and r 2 for adverse soil ph suarez et al 1984 suarez and šimůnek 1997 respectively 24 r 1 1 c x n 1 c x n where c and n are empirical parameters and x is the swelling factor given by 25 x 3 6 10 4 f m o n t e s p d and 26 d 0 f o r c 0 300 m m o l l 1 356 4 c 0 1 2 1 2 f o r c 0 300 m m o l l 1 and 27 n 1 f o r e s p 25 2 f o r 25 e s p 50 3 f o r e s p 50 and 28 c 35 f o r e s p 25 932 f o r 25 e s p 50 25000 f o r e s p 50 where f m o n t is the weight fraction of montmorillonite in the soil assumed to be 0 1 mcneal 1968 šimůnek et al 1996 d is the adjusted interlayer spacing c 0 is the total salt concentration of the solution mmol l 1 and esp is the exchangeable sodium percentage calculated from the sodium absorption ratio sar given by wallender and tanji 2011 29 s a r n a c a m g 1 2 and 30 e s p 100 e s p k g s a r where k g is the modified gapon selectivity coefficient having a value of 0 015 mmol l 1 2 wallender and tanji 2011 presently smituv does not simulate ion association and chemical speciation and therefore the salt concentrations of sodium na calcium ca and magnesium mg are the total analytical concentrations in mmol l 1 the reduction factor r 2 is calculated using the equation by d l suarez et al 1984 31 r 2 1 p h 6 83 3 46 0 36 p h 6 83 p h 9 3 3 p h 9 3 major stock s converters c and flow f variables in the h r sector are listed in appendix a 4 estimation of model parameters for simulation the simulation of swf and st sectors requires time variable initial boundary condition and soil hydraulic properties to be defined for the model simulation of the rwu sector requires the crop data such as evapotranspiration root distribution and salt tolerance threshold value to be defined these parameters have been estimated for simulation of 365 days in the year 2018 for pecan perennial crop with fully grown root during this period 4 1 soil hydraulic properties the soil properties of a commercially managed field 31 30 32 30 n 106 13 25 49 w in el paso county texas with an area of approximately 16 ha was obtained from web soil survey the soil consists of a distribution of tigua tg 72 glendale gs 12 and harkey hs 16 silty clay loam a weighted average of the soil properties has been taken at each depth of soil as shown in table 1 the air entry pressure α cm 1 and pore size distribution n of the soil water retention curve has been set as 0 0178 cm 1 and 1 30 for the entire soil column as adopted from the experiments of schaap and van genuchten 2006 for silty clay loam soils in the united states 4 2 time variable boundary condition daily rainfall for el paso region was downloaded from climate data online cdo national climatic data center ncdc for 365 days pecan is set to be irrigated every two times in a month from may to october with an irrigation depth of 12 7 cm which roughly corresponds to the irrigation schedule used by growers in the region daily salt load data has been estimated from the annual average observed salts for different water sources in the el paso region table 2 with this data the daily load of salt entering the soil surface is estimated using a weighted average based on the type of water used irrigated water is a combination of 30 groundwater and 70 river water along with incident rainfall later the daily surface solute flux is calculated by multiplying the daily load of salt and rainfall for the year 2018 4 3 root distribution of pecan β pecan is a perennial crop with a fully matured root reaching a span of 300 cm on either side and depth of 80 cm miyamoto 1982 the effective rwu is based on the distribution of roots beneath the soil the normalized root distribution of pecan is as shown in fig 3 4 4 potential transpiration it is the consumptive water use by crops under optimum condition estimated using 32 t p k c e t 0 where k c is crop s coefficient for pecan adopted from the field study by miyamoto 1982 et 0 is the average monthly potential evapotranspiration of el paso obtained from texas et network as shown in table 3 the estimated potential transpiration of pecan is shown in fig 4 5 results the numerical methods of smituv where validated using simulation for mature pecan orchard from two models the reference model hydrus has been widely used and is generally considered a good reference standard note that this assessment of smituv is just to ensure the numerical formulations and may be considered a systematic validation a mature pecan orchard was simulated by both smituv and hydrus for 365 days in 2018 5 1 verification with hydrus model simulation results from smituv are approximate numerical solutions solved using finite difference method in a system dynamic environment with limitations on allowed numerical processes validation of such models particularly the numerical processes in an accurate comprehensive exhaustive efficient and cost effective manner is to compare with a similar mathematical model such as hydrus abelman and patidar 2008 haverkamp et al 1977 shawagfeh and kaya 2004 hydrus solves transient soil water flow solute transport and root water uptake using finite element approximation the soil profile in hydrus has been discretized using 11 nodes with nodes 1 to 4 representing the smituv model parameter of soil hydraulic properties for the upper layer nodes 4 to 7 representing the middle layer and nodes 7 to 11 representing bottom layer it is important to note that hydrus does not account for a hydraulic reduction on hydraulic conductivity j šimůnek m th van genuchte n 2012 and there is no easy way to extend the model therefore the saturated hydraulic conductivity has been reduced to 50 of its actual value for each soil layer to account for hydrus limitation the time variable boundary conditions such as rainfall irrigation water surface solute flux transpiration and root length have been set similar to smituv hydrus was simulated for modules of soil water flow standard solute transport and root water uptake with a simulation duration of 365 days comparisons of the simulation results of smituv and hydrus for cumulative actual root water uptake actual root water uptake rate and root zone solute concentration of pecan is as shown in fig 5 fig 6 and fig 7 smituv results are similar to hydrus demonstrating that smituv accurately simulated the dynamic changes in root water uptake under water and salinity stress 5 2 validation results the accuracy of the results simulated by smituv is evaluated by computing the root mean square error rmse and pearson s correlation coefficient r2 for every ith observation from smituv and hydrus as shown in table 4 for the three simulation results the calculated rmse value varies from 0 104 to 2 810 cm and r2 value from 0 837 to 0 996 the small rmse value and high r2 for all the simulation results show that smituv has accurately predicted results comparable to hydrus the overall performance of the smituv shows that the simulation results are conforming to the mathematical model hydrus 6 model interface a user interface for the model has been developed in stella architect and will be publicly accessible at isee exchange website the interface allows the user to interact with the smituv model using buttons tables and interactive graphs the model can be run from the control center shown in fig 8 that allows the user to set the simulation for the crops by selecting the button for pecan and cotton the run stop and restart buttons allow the user to initiate stop and restart a simulation the blue bar at the bottom allows the user to pan through different timesteps of the simulation window the button model parameters opens the screen for inputting the data for soil properties and constants through an interactive table as shown in fig 9 this screen also allows the importing of climatic data and total analytical salts in the soil required as the initial boundary conditions for simulations restore all values button restores all the values for the variables and constants to the model default value this allows the user to at least have one simulation using the default values the model results can be viewed as a single run and or grouped together for multiple runs the single run results button shows the results plotted against the simulation time for cumulative root water uptake root water uptake rate cumulative soil water storage and cumulative solute concentration as shown in fig 10 similarly the button multiple run results shows the results plotted for multiple runs shown in fig 11 that allows the user to assess the simulation for various scenarios the multiple runs are shown for the runs currently simulated in the smituv model and can be changed in the model as per user requirement 7 discussion smituv demonstrated the applicability of system dynamic models for complex nonlinear and dynamic problems such as simulation of soil water root salt interaction smituv is easily extendable and several other modules are planned the graphical approach of the smituv makes it possible for understanding the relationship between essential components involved in the system that were not understood previously by growers due to the inherent complexity and opaqueness of existing programming language based models it is not necessary for the researchers or collaborators participating in a study to be a modeler all stakeholders may utilize the user interface of smituv for interpretation of the behavior of the system for various conditions and accordingly inform the model developers for better calibration of the model setting up a complex feedback loop mechanism is difficult without the inherent knowledge of programming language and mathematical solvers some of the planned real world validation and grower usages of the model were hampered by the covid19 pandemic situation however supported by various projects in the future smituv will be a significant tool for modeling that involves collaboration from various interdisciplinary researchers to solve the complex and nonlinear nature of systems in the arid southwest region of the united states we expect that model will also be applicable globally where flood irrigation is practiced smituv integrates an emergent field of study involving system dynamics in agricultural systems research though widely adopted in other fields systems dynamic modeling has lacked in agricultural modeling domain this model has showcased the capacity to tackle a complex and dynamic system behavior using simple steps it contains different modules connected to each other dynamically like a symbolized circuit diagram once the diagram is setup in the system dynamic window it will simulate the system behavior correctly this inherent property of the model allows the researchers to integrate any module of interest into the already developed smituv for any additional research studies also researchers have the option to link the results from smituv to other software and applications like any other modeling technique modeling results will be adversely impacted if the variables and equations are formulated based on poor technical knowledge and evaluation the use of predictions from smituv without adequately setting the soil and other parameters can have an adverse impact on the study and deviation from the system s actual behavior therefore smituv needs to be adapted based on a good modeling practices such as model calibration with historical data sensitivity analysis of variables analysis of scenarios comparison with other similar models adhering to local conditions and field experiments and understanding the problem statement however even in cases where detailed data are not available but some generalization may be made there are education benefits in applying smituv with some synthetic simulations stakeholders can understand sensitivity of parameters and the complex relationships it should be noted that currently only flood irrigation methods are implemented thus application to other irrigation scenarios is not correct the key to successful model development is understanding the problem statement as modelers tend to develop unnecessarily complicated large models without understanding the problem type a modeler should keep in mind that the prediction of a system dynamic model is a representation of the systems behavior in real world and its adherence to those representations should be of utmost importance while developing and using the model the model source in isee stella native format and xmile format xml based standard for storing models is posted to github https github com skp703 smituv under open source gnu gplv3 license the code may be modified using stella architect or executed using free software isee player 8 conclusion a system dynamic model smituv was developed for simulating the dynamic changes in soil water infiltration solute transport and root water uptake under water and salinity stress by numerically solving one dimensional richards equation and advection diffusion equation based on finite difference approach smituv simulated results were similar to the finite element method based numerical model hydrus smituv is built on isee stella that has an easily modifiable and understandable development environment allowing a user to easily change the operation and structure of the model to match a specific project requirement unlike the presently available mathematical models smituv does not require any prior knowledge of programming language for model adaptation the simulation results of smituv show that this can be used as an efficient strategic tool for devising field and salinity management measures by stakeholders further smituv reinforces the potential of system dynamic modeling for complex interdisciplinary research studies data availability statement the model source in isee stella native format and xmile format xml based standard for storing models is posted to github https github com skp703 smituv under open source gnu gplv3 license some or all data models or code that support the findings of this study are available from the corresponding author upon reasonable request 1 stella model of simulation 2 daily rainfall crop root growth and solute concentration data declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement the development of the system dynamic model for simulating the one dimensional transient soil water flow solute transport and root water uptake by crops under water and salinity stress was supported by grant no 2015 68007 23130 national institute of food and agriculture nifa project u s department of agriculture usda this work is also supported by the usda nifa hatch project 1022633 appendix a major stock s converters c and flow f variables used in model variable type definition unit a i c soil swelling parameters dimensionless alph i c van genutchen parameter for fitting soil water retention curve swrc cm 1 bi c normalized root distribution function dimensionless c i c soil swelling parameters dimensionless ca c total analytical concentration of calcium mmol l calcium c percentage of calcium dimensionless co i c total salt concentration mmol l d i c soil interlayer spacing dimensionless di c diffusion constant cm2 day dw c diffusion constant of water cm2 day ec50 c crop salt tolerance threshold value ds m eci c salt content at each layer ds m esp i c exchangeable sodium percentage dimensionless etci c potential evapotranspiration cm day evapotranspiration c reference evapotranspiration cm day fm c weight fraction of montmorillonite dimensionless h i c water pressure head cm hm50 c water stress at 50 water extraction h 50 cm hmi c matric stress response function dimensionless hsi c osmotic stress response function dimensionless infiltration transmission rate i f flow equations cm per day k i c hydraulic conductivity cm day kc c consumptive water use dimensionless kg c modified gapon selectivity coefficient mmol l 1 2 ks i c saturated hydraulic conductivity cm day leached water s percolated water cm leaching f leaching rate cm per day m i c van genutchen parameter for fitting swrc dimensionless magnesium c percentage of magnesium dimensionless mass leached solute c amount of solute leached out mg mass solutei c mass of solute mg mg c total analytical concentration of magnesium mmol l n i c van genutchen parameter for fitting swrc dimensionless na c total analytical concentration of sodium mmol l ph i c ph of soil dimensionless ponded c ponded water head cm q i c water content cm3 cm 3 qr i c residual water content cm3 cm 3 qs i c saturated water content or porosity cm3 cm 3 r i c hydraulic reduction function dimensionless r i1 c hydraulic reduction due to salinity dimensionless r i2 c hydraulic reduction due to ph dimensionless root growth c root length cm root watery s root water storage cm sar i c sodium absorption ratio mmol l 1 2 sodium c percentage of sodium dimensionless soil water layer i s soil water storage cm solute infiltration transport i f downward solute flux g cm2 solute layer i s accumulated solute flux g cm2 solute leaching f rate of solute leached out g cm2 solutei c solute concentration at ith soil g cm3 solutei mg l c total salt concentration mg l solutei mmol l c salt concentration from s t sector mmol l tds g cm3 c initial solute concentration g cm3 uptake ratei f rate of water extraction cm per day x i c swelling factor of soil dimensionless z i c soil layer depth cm notation the following symbols are used in this paper c total solute concentration per unit volume of soil c empirical parameters of swelling c0 total salt concentration of the solution ca salt concentrations of calcium ctop total solute entering the upper soil layer d adjusted interlayer spacing ddiff diffusion constant dw solute diffusion coefficient in free water ec total salt concentration ec50 threshold value of crop salt tolerance esp exchangeable sodium percentage et0 average monthly potential evapotranspiration fmont weight fraction of montmorillonite in the soil h water pressure head capillary suction h50 pressure head at 50 root water extraction i infiltration of water is infiltration of solute k hydraulic conductivity k g modified gapon selectivity coefficient kc crop s coefficient for pecan ks saturated hydraulic conductivity mg salt concentrations of magnesium n pore size distribution n empirical parameters of swelling na salt concentrations of sodium p flux of water q darcy s volumetric flux r hydraulic reduction function r1 normalized function for exchangeable sodium r2 normalized function for adverse soil ph r2 pearson s correlation coefficient rmse root mean square error s sink term sar sodium absorption ratio se relative hydraulic conductivity t time t transmission rate of water tds total dissolved solids tp potential evapotranspiration ts transmission rate of solute ur root water uptake rate x swelling factor z depth of soil layer α air entry pressure α h water stress response function α s salinity stress response function β normalized root distribution function δzi soil compartment thickness q water content θr residual water content and θs saturated water content 
25910,a system dynamic model smituv system dynamic modeling of infiltration solute transport and root water uptake in vadose zone was developed for simulating transient soil water flow solute transport and root water uptake in crops under water and salinity stress in a multilayered unsaturated soil layer smituv solves the mixed form one dimensional richards equation with a sink term for water transport and root water uptake and advection diffusion equation for solute transport it simulates salinity and water stress on root water uptake as a function of osmotic and matric potential smituv is based on fundamental physical principals and is robust enough to handle different scenarios and what if runs and also has tools that are crucial for growers to understand the salt water soil system the model is designed to be a planning tool for strategic desalination methods and field management measures that optimize irrigation water use for flood irrigated arid region crops 1 introduction irrigation in the arid and semi arid regions is complicated due to the presence of salinity in the soil salinity is caused due to the presence of a high concentration of salts in the soil that reduces the amount of available water for root water uptake by crops the reduction in the root water uptake combined with the effects of drought and other environmental conditions limits the productivity of crop plants by 20 50 of their maximum yield shrivastava kumar 2015 a wide range of salinity stress management strategies are required to overcome such impacts of salinity on crop productivity keeping track of salinity in the soil and its associated reduction in root water uptake transpiration and crop productivity is the first step in understanding the salinity stress modeling the soil water movement root water uptake and solute transport play an important role in assessing the salinity stress and its related impacts on crops šimůnek et al 1996 in addition evaporation and plant transpiration also plays a vital role in the solution composition water and solute distribution in subsurface conditions šimůnek et al 1996 the first approach to understand the complicated relationship of salinity and crop growth was quantified by physically measuring the salt tolerance of various crops in a laboratory condition bernstein 1956 this was followed with separate models on root water extraction using microscopic gardner 1960 molz et al 1968 and macroscopic approaches dutt et al 1972 and salt transport bresler 1973 the first combined model for soil water flow and root water extraction was proposed by nimah and hanks 1973 a comprehensive model combining soil water flow in unsaturated soil root water extraction and solute transport was developed by childs 1975 as an extension to the work by nimah and hanks 1973 later various numerical models for the simulation of 1 dimensional water flow and solute transport were developed most models developed are closed source and monolithic with all the process build in and limited extension capacity these models may be broadly categorized as steady state and transient models the steady state model watsuit rhoades and merrill 1976 divides the root zone to four different zones vertically and assumes the root water extraction to be in the ratios of 40 30 20 10 it has a function of precipitation dissolution based on the presence or absence of caco3 as an option whereas the transient model simulates the continually changing soil water salt effects on evapotranspiration osmotic and matric effects on root water extraction multi component major ion chemistry and transport precipitation dissolution cation exchange carbon dioxide heat production and transportation some of the major transient models are enviro gro pang and letey 1998 saltmed ragab 2002 swap van dam 2000 unsatchem šimůnek et al 1996 suarez and šimůnek 1997 and hydrus šimůnek j huang and van genuchten 1998 šimůnek j van genuchten and šejna 2005 šimůnek m šejna saito sakai and genuchten 2013 vogel et al 1996 the functionality of these models differs in calculation of the stresses in the root water extraction component where saltmed and enviro gro uses an additive function and swap hydrus and unsatchem uses a multiplicative function while integrating the osmotic and matric stress additionally unsatchem calculates the osmotic coefficient using the pitzer equations from the major ion chemistry and incorporates a hydraulic reduction function due to salinity sodicity interactions that further reduces the soil water flow oster letey vaughan wu and qadir 2012 conducted a comprehensive comparison of the simulated results on the yield of forage corn of these models these models are often developed using fortran and require significant expertise to integrate and modify various components on the other hand smituv system dynamic modeling of infiltration solute transport and root water uptake in vadose zone model developed in this study was designed to engage and educate growers and allow for easy modification to the problem at hand with a graphical system dynamic development environment the inherent systems approach allows the user to better understand the system connections for example how salinity hydraulic conductivity and crop uptake are interlinked 1 1 system dynamic approach system dynamics is an approach to understand the nonlinear complex dynamic relationship between systems over time utilizing the feedback complexity of elements in the form of equations using stocks flows internal feedback loops and functions saysel and barlas 2001 sterman 2000 these equations form the overall structure of the model and a well formulated structure can effectively simulate the dynamics of soil water flow root water extraction and solute transport the system dynamics approach has been successfully used for various hydrological and watershed studies keshta et al 2009 ouyang et al 2016 a recent study using the system dynamic approach successfully simulated the infiltration of water in the unsaturated zone using darcy s equation showing the effectiveness of this approach huang et al 2011 in this approach the dynamic relation of the input and its downward or upward movement is simulated based on the system s framework represented by equations and the feedback mechanism that is either reinforcing positive feedback loop or counteracting negative feedback loop huang et al 2011 no studies were found that used the system dynamic approach to simulate the transient combined soil water flow root water extraction and solute transport a system dynamic approach is used to develop an interactive participatory model that can be used by growers and other stakeholders without having to hurdle through the complex options of the already existing software such as hydrus smituv is developed in the isee stella architect software stella software codes in the whole simulation process using a graphical icon based skeletal structure explained in section 2 2 this kind of skeletal structure makes it easier for agricultural researchers or students to understand the iterations involved in the simulation and the dynamic relationship between variables and processes they can easily insert or change a variable at any juncture of the model without even having to know coding since in stella the equation can be directly inputted using variables this enhances the modification or addition of a new module or process in the already existing model to conduct various sensitivity analysis also the model can be presented to the general public in isee exchange website to interact with the smituv model using buttons tables and interactive graphs as discussed in section 6 0 this approach will make smituv a more user friendly than the existing software such as hydrus saltmed etc smituv is developed keeping in mind the common growers and research community and dedicated for future research in the field of applied engineering in agriculture 1 2 model rationale irrigation systems around the world rely on flood irrigation using surface and ground water nurzaman 2017 the irrigation systems have been stressed and adversely affected by the local and global change in climate water scarcity with concomitant pressures from other consumers e g growing cities degrading soil and water quality particularly influx of salinity and degrading rural economic condition corlett and westcott 2013 nurzaman 2017 sheffield et al 2012 these contributing factors can be characterized as complex with many nonlinear feedbacking loops that require a system dynamic approach to develop a mental model for understanding and decoding the system the components of this complex system are dynamically related spatially and temporally the nonlinearity and in some case spatio temporal stochasticity of the relations often means that linear equation cannot be used for representing the interrelation of the components therefore decoding involves numerous variables and complex feedback loops this makes most of the conventional monolithic models developed for field scale irrigation and salinity management complicated requiring extensive expert assistance to be used by growers key stakeholders and with limited education potential after considering a web based wrapper use for other legacy models kumar et al 2015 which are complicated to maintain and do not solve the problem of limited extensibility of the conventional monolithic models an extensible systems dynamic modeling approach was used the solution was to develop a mental model that meets the needs of a grower as well as create multi stakeholder participation that can adapt to the changing perceptions problems of interest and new scientific innovations this model has been developed considering the changing requirement in the future such as groundwater upwelling ag desalination different irrigation techniques global climate change use of fertilizers and pesticides that can be integrated with ease by the other developers or savvy users though designed in isee stella for rapid development and deployment most systems dynamic models have the capability to be converted to a python and module in other languages naimi voinov 2012 for integration with component based modeling systems e g csdms peckham et al 2013 and openmi gregersen et al 2007 and future extensibility 2 model overview the system dynamic model smituv simulates the transient soil water flow root water uptake and solute transport in the unsaturated zone of a flood irrigated field the model can also quantify the effects of root water uptake under salinity and water stress to validate the numerical processes we have compared the results with an established numerical model hydrus it may be noted that smituv is graphical that are easier to interpret and visualize particularly for non expert stakeholders and promotes grower education about the processes 2 1 causal loop diagram smituv uses feedback loops to dynamically relate the systems to each other through reinforcing positive feedback loop represented by and counteracting negative feedback loop represented by loops as shown in fig 1 the first loop loop 1 is water infiltration to the first soil layer that increases the amount of stored water in the first layer this increase in water increases the water pressure head that reduces the infiltration limiting the amount of water being stored the second loop loop 2 is the effect on hydraulic conductivity where the stored water in the soil increases the water content that increases the hydraulic conductivity resulting in higher infiltration rate increasing the stored water in the soil the third loop loop 3 is the extraction of water by crop roots that increases the water pressure head resulting in the build up of water stress limiting the water extracted by roots from the soil layer the fourth loop loop 4 is the effect of salt infiltration that increases the amount of salt in the soil layer resulting in the build up of salinity stress limiting the water extracted by roots from the soil layer the fifth loop loop 5 is the increase in sodicity of the soil that acts in combination with the soil ph reducing the hydraulic conductivity limiting the water storage in the soil 2 2 model approach and structure soil is heterogeneous in nature and a numerical solution of the richards equation is required for simulating the one dimensional flow in unsaturated soil parissopoulos and wheater 1990 the system dynamic approach is used for numerically solving richards equation using a finite difference method a similar technique was used by celia et al 1990 the unsaturated soil hydraulic properties are based on a set of closed form equations van genuchten 1980 and using the capillary model of mualem 1976 root water uptake is modeled using a sink term in richards equation that was first proposed by feddes and zaradny 1978 and later modified to include osmotic stress by van genuchten 1987 solute transport between multi soil layer is simulated by numerically solving the advection diffusion equation for a non reactive and non interactive solute freeze and cherry 1979 using a finite difference method celia et al 1990 the model is developed using isee systems stella architect software a daily time step was used for all simulations due to the binary arithmetic that the computer uses the time step between calculations delta time dt in the model is set at 0 125 that falls in the sequence of 1 2 n i e every 1 8th of a day thus optimizing the computational speed and avoiding round off errors hps inc 2001 fig 2 shows the icon based skeletal structure of smituv it contains rectangular blocks that are the stock variables representing the accumulation of water and solute in soil layers and water in roots the soil water infiltration and root water uptake rate and solute transport flux are simulated using the flow variable symbolized by valves between the stocks variables and equations leading to the formulation of these flow variables are developed using converters symbolized by circles the converters are connected to the flow variables and themselves using connectors symbolized by a line and arrow at the end in the model structure soil layers are divided into three compartments upper middle and bottom layers it has four sectors soil water flow swf simulating the unsaturated soil water flow solute transport st simulating the transport of solute between layers root water uptake rwu simulating extraction of water by roots under water and salinity stress from soil layers and hydraulic reduction h r simulating the reduction in hydraulic conductivity due to sodicity and ph of soil all of the four sectors are interconnected based on various formulations and empirical relationships discussed above the building blocks within a sector can be partially simulated while keeping all other blocks at initial condition allowing to set various combinations of simulation 3 model formulation the smituv model is formulated using the stock flow converter based system dynamics approach to simulate the soil water movement root water extraction and solute transport 3 1 soil water flow sector swf the one dimensional water flow in an unsaturated incompressible porous media is best described using the modified form of richards equation richards 1952 formulated as 1 θ t z k d h d z 1 s where θ is the water content cm3 cm 3 h is the water pressure head capillary suction cm t is time days z is the depth of soil layer cm and s is the sink term cm3 cm 3day 1 representing the root water extraction by the plants the hydraulic conductivity and the capillary suction are calculated using the mualem 1976 and van genuchten 1980 equations that were later upgraded by suarez and šimůnek 1997 to include the effects of soil chemical properties such as sodicity and ph 2 θ h θ r θ s θ r 1 α h n m and 3 r k θ r k s s e 1 2 1 1 s e 1 m m 2 s e 1 r k s s e 1 respectively where 4 m 1 1 n n 1 5 s e θ θ r θ s θ r and where θ r and θ s are the residual and saturated water content cm3 cm 3 respectively k s is the saturated hydraulic conductivity cm day s e is the relative hydraulic conductivity r is the hydraulic reduction function due to the soil chemical properties and n and α cm 1 are the van genuchten 1980 parameter of the soil water retention curve swrc 3 1 1 finite difference approximation the conventional numerical solution of richards equation considers the water balance of an infinitely small soil volume kroes et al 2008 whereas in smituv soil layers are represented by the stock variables and the rate of water movement is represented by the flow variables thus an implicit backward finite difference method is used to solve richards equation transforming it to a discretized form kroes et al 2008 6 θ t 1 δ z i k i 1 2 h i 1 h i δ z i 1 δ z i 2 1 k i 1 2 h i h i 1 δ z i δ z i 1 2 1 s i where the subscript i represents the ith soil layer with values ranging from 1 to 3 and δ z i is the soil compartment thickness the sink term s i representing root water uptake is simulated as a separate sector that flows out from each soil layer 3 1 1 1 boundary conditions the initial and top boundary condition is the flux of water p cm day entering the topsoil layer 7 p r a i n f u l l c m d a y i r r i g a t i o n w a t e r c m d a y the bottom boundary condition is free drainage condition with percolation rate from the bottom layer equal to the hydraulic conductivity of the bottom layer the percolated water is collected to an end stock for quantifying the amount of water drained out of the soil column 3 1 1 2 water flow equations the rate of water entering the upper soil layer has the term k i 1 2 h i 1 h i δ z i 1 δ z i 2 1 in eq 6 replaced by top boundary condition 8 i p k 1 k 2 2 h 1 h 2 z 1 z 2 2 1 the flow equation from the upper to middle soil layer is formulated as 9 t 1 k 1 k 2 2 h 1 h 2 z 1 z 2 2 1 k 2 k 3 2 h 2 h 3 z 2 z 3 2 1 the rate of water flowing from middle to bottom soil layer has the term k i 1 2 h i h i 1 δ z i δ z i 1 2 1 replaced with the bottom boundary condition 10 t 2 k 2 k 3 2 h 2 h 3 z 2 z 3 2 1 k 3 major stock s converters c and flow f variables of swf sector is listed in appendix a 3 2 solute transport sector st the solute transport in the subsurface soil under transient water flow condition is given by freeze and cherry 1979 van dam 2000 11 θ c t q c z z θ d d i f f d c d z where c is the total solute concentration per unit volume of soil g cm3 q is darcy s volumetric flux cm day and d d i f f is the diffusion constant cm2 day given by kroes et al 2008 millington and quirk 1961 12 d d i f f d w θ 7 3 θ s 2 where d w is the solute diffusion coefficient in free water having a value of 2 cm2 day and q is given by darcy s law 13 q k h z 1 3 2 1 finite difference approximation the advection diffusion equation is numerically solved using an explicit central finite difference scheme given by kroes et al 2008 14 c t 1 θ i δ z i q i 1 2 c i 1 2 θ i 1 2 d i 1 2 c i 1 c i 1 2 δ z i 1 δ z i q i 1 2 c i 1 2 θ i 1 2 d i 1 2 c i c i 1 1 2 δ z i δ z i 1 smituv simulates the solute flow as flux and therefore to represent the solute concentration as solute flux left and right hand side of the equation is multiplied with the thickness of the soil compartment δ z i 3 2 1 1 boundary conditions the initial condition is the solute concentration g cm3 that is accumulated on the soil surface from the dissolved salts in rain river and ground water the top boundary condition is the total solute entering the upper soil layer c top calculated by multiplying initial solute concertation with ponded depth the bottom boundary condition represents leaching of solute under gravity equal to the product of solute concentration and darcy s volumetric flux of the bottom soil layer the leached solute is collected to an end stock for quantifying the amount of solute leached out of the soil column 3 2 1 2 solute flux equations the rate of solute entering the upper soil layer has the term 1 θ i δ z i q i 1 2 c i 1 2 θ i 1 2 d i 1 2 c i 1 c i 1 2 δ z i 1 δ z i in eq 14 replaced with the top boundary condition 15 i s c t o p z 1 θ 1 z 1 q 1 q 2 2 c 1 c 2 2 θ 1 θ 2 2 d 1 d 2 2 c 1 c 2 z 1 z 2 2 the rate of solute flowing from upper to bottom soil layer is given by 16 t s 1 z 2 θ 2 z 2 q 1 q 2 2 c 1 c 2 2 q 2 q 3 2 c 2 c 3 2 θ 1 θ 2 2 d 1 d 2 2 c 1 c 2 z 1 z 2 2 θ 2 θ 3 2 d 2 d 3 2 c 2 c 3 z 2 z 3 2 the rate of solute flowing from middle to bottom soil layer has the term 1 θ i δ z i q i 1 2 c i 1 2 θ i 1 2 d i 1 2 c i c i 1 1 2 δ z i δ z i 1 in eq 14 replaced with the bottom boundary condition 17 t s 2 z 3 θ 3 z 3 q 2 q 3 2 c 2 c 3 2 θ 2 θ 3 2 d 2 d 3 2 c 2 c 3 z 2 z 3 2 c 3 q 3 the final output of smituv gives the total solute concentration g cm 3day 1 for the soil layers that is calculated by multiplying the solute flux g cm 2day 1 with the soil compartment thickness major stock s converters c and flow f variables of s t sector are listed in appendix a 3 3 root water uptake rwu the water uptake rate of roots from the soil layer is simulated using the sink term s i from eq 6 the root water uptake is depended on the soil water stress osmotic stress root characteristics and evapotranspiration skaggs et al 2006 a macroscopic approach was adopted in calculating the rwu rate feddes et al 1978 van genuchten 1987 18 s β α h α s t p where β is the normalized root distribution function α h is the water stress response function α s is the salinity stress response function and t p is the potential evapotranspiration in order to account for root water uptake at each soil layer uptake equation is formulated using the flow variable connected to the stock variable representing each soil layer 19 u r i β i α h i α s i t p where i represent the ith soil layer with i equal to 1 2 and 3 on its other end the flow variable is connected to an end stock variable representing the total water stored or extracted by the roots this is important to understand the actual water that has been transpired by the plant under stress major stock s converters c and flow f variables in rwu sector is listed in appendix a 3 3 1 water stress water stress is the reduction of rwu due to the buildup of water pressure head normalized as a water stress response function 0 α h 1 20 α h 1 1 h h 50 3 where h 50 is an empirical constant representing the pressure head at 50 root water extraction having a value of 150 cm taylor and ashcroft 1972 3 3 2 salinity stress salinity stress is the reduction of rwu due to the buildup of osmotic potential by the accumulated salt in root zone normalized as a salinity stress response function 0 α s 1 oster et al 2012 van genuchten 1980 21 α s 1 1 e c e c 50 3 where e c 50 ds m is the threshold value of crop salt tolerance and e c ds m is the total salt concentration tds in the root zone calculated from the s t sector and empirically converted using wallender and tanji 2011 22 640 δ e c d s m t d s m g l 3 4 hydraulic reduction hr the hydraulic reduction is a normalized reduction function 0 r 1 that decreases the effective soil hydraulic conductivity by exchangeable sodium and adverse ph present in the soil high level of exchangeable sodium causes clay dispersion and swelling of the soil mcneal 1968 šimůnek et al 1996 suarez and šimůnek 1997 and soil ph decreases the effective hydraulic conductivity suarez et al 1984 the normalized function is given by 23 r r 1 r 2 where r 1 is the normalized function for exchangeable sodium mcneal 1968 šimůnek et al 1996 suarez and šimůnek 1997 and r 2 for adverse soil ph suarez et al 1984 suarez and šimůnek 1997 respectively 24 r 1 1 c x n 1 c x n where c and n are empirical parameters and x is the swelling factor given by 25 x 3 6 10 4 f m o n t e s p d and 26 d 0 f o r c 0 300 m m o l l 1 356 4 c 0 1 2 1 2 f o r c 0 300 m m o l l 1 and 27 n 1 f o r e s p 25 2 f o r 25 e s p 50 3 f o r e s p 50 and 28 c 35 f o r e s p 25 932 f o r 25 e s p 50 25000 f o r e s p 50 where f m o n t is the weight fraction of montmorillonite in the soil assumed to be 0 1 mcneal 1968 šimůnek et al 1996 d is the adjusted interlayer spacing c 0 is the total salt concentration of the solution mmol l 1 and esp is the exchangeable sodium percentage calculated from the sodium absorption ratio sar given by wallender and tanji 2011 29 s a r n a c a m g 1 2 and 30 e s p 100 e s p k g s a r where k g is the modified gapon selectivity coefficient having a value of 0 015 mmol l 1 2 wallender and tanji 2011 presently smituv does not simulate ion association and chemical speciation and therefore the salt concentrations of sodium na calcium ca and magnesium mg are the total analytical concentrations in mmol l 1 the reduction factor r 2 is calculated using the equation by d l suarez et al 1984 31 r 2 1 p h 6 83 3 46 0 36 p h 6 83 p h 9 3 3 p h 9 3 major stock s converters c and flow f variables in the h r sector are listed in appendix a 4 estimation of model parameters for simulation the simulation of swf and st sectors requires time variable initial boundary condition and soil hydraulic properties to be defined for the model simulation of the rwu sector requires the crop data such as evapotranspiration root distribution and salt tolerance threshold value to be defined these parameters have been estimated for simulation of 365 days in the year 2018 for pecan perennial crop with fully grown root during this period 4 1 soil hydraulic properties the soil properties of a commercially managed field 31 30 32 30 n 106 13 25 49 w in el paso county texas with an area of approximately 16 ha was obtained from web soil survey the soil consists of a distribution of tigua tg 72 glendale gs 12 and harkey hs 16 silty clay loam a weighted average of the soil properties has been taken at each depth of soil as shown in table 1 the air entry pressure α cm 1 and pore size distribution n of the soil water retention curve has been set as 0 0178 cm 1 and 1 30 for the entire soil column as adopted from the experiments of schaap and van genuchten 2006 for silty clay loam soils in the united states 4 2 time variable boundary condition daily rainfall for el paso region was downloaded from climate data online cdo national climatic data center ncdc for 365 days pecan is set to be irrigated every two times in a month from may to october with an irrigation depth of 12 7 cm which roughly corresponds to the irrigation schedule used by growers in the region daily salt load data has been estimated from the annual average observed salts for different water sources in the el paso region table 2 with this data the daily load of salt entering the soil surface is estimated using a weighted average based on the type of water used irrigated water is a combination of 30 groundwater and 70 river water along with incident rainfall later the daily surface solute flux is calculated by multiplying the daily load of salt and rainfall for the year 2018 4 3 root distribution of pecan β pecan is a perennial crop with a fully matured root reaching a span of 300 cm on either side and depth of 80 cm miyamoto 1982 the effective rwu is based on the distribution of roots beneath the soil the normalized root distribution of pecan is as shown in fig 3 4 4 potential transpiration it is the consumptive water use by crops under optimum condition estimated using 32 t p k c e t 0 where k c is crop s coefficient for pecan adopted from the field study by miyamoto 1982 et 0 is the average monthly potential evapotranspiration of el paso obtained from texas et network as shown in table 3 the estimated potential transpiration of pecan is shown in fig 4 5 results the numerical methods of smituv where validated using simulation for mature pecan orchard from two models the reference model hydrus has been widely used and is generally considered a good reference standard note that this assessment of smituv is just to ensure the numerical formulations and may be considered a systematic validation a mature pecan orchard was simulated by both smituv and hydrus for 365 days in 2018 5 1 verification with hydrus model simulation results from smituv are approximate numerical solutions solved using finite difference method in a system dynamic environment with limitations on allowed numerical processes validation of such models particularly the numerical processes in an accurate comprehensive exhaustive efficient and cost effective manner is to compare with a similar mathematical model such as hydrus abelman and patidar 2008 haverkamp et al 1977 shawagfeh and kaya 2004 hydrus solves transient soil water flow solute transport and root water uptake using finite element approximation the soil profile in hydrus has been discretized using 11 nodes with nodes 1 to 4 representing the smituv model parameter of soil hydraulic properties for the upper layer nodes 4 to 7 representing the middle layer and nodes 7 to 11 representing bottom layer it is important to note that hydrus does not account for a hydraulic reduction on hydraulic conductivity j šimůnek m th van genuchte n 2012 and there is no easy way to extend the model therefore the saturated hydraulic conductivity has been reduced to 50 of its actual value for each soil layer to account for hydrus limitation the time variable boundary conditions such as rainfall irrigation water surface solute flux transpiration and root length have been set similar to smituv hydrus was simulated for modules of soil water flow standard solute transport and root water uptake with a simulation duration of 365 days comparisons of the simulation results of smituv and hydrus for cumulative actual root water uptake actual root water uptake rate and root zone solute concentration of pecan is as shown in fig 5 fig 6 and fig 7 smituv results are similar to hydrus demonstrating that smituv accurately simulated the dynamic changes in root water uptake under water and salinity stress 5 2 validation results the accuracy of the results simulated by smituv is evaluated by computing the root mean square error rmse and pearson s correlation coefficient r2 for every ith observation from smituv and hydrus as shown in table 4 for the three simulation results the calculated rmse value varies from 0 104 to 2 810 cm and r2 value from 0 837 to 0 996 the small rmse value and high r2 for all the simulation results show that smituv has accurately predicted results comparable to hydrus the overall performance of the smituv shows that the simulation results are conforming to the mathematical model hydrus 6 model interface a user interface for the model has been developed in stella architect and will be publicly accessible at isee exchange website the interface allows the user to interact with the smituv model using buttons tables and interactive graphs the model can be run from the control center shown in fig 8 that allows the user to set the simulation for the crops by selecting the button for pecan and cotton the run stop and restart buttons allow the user to initiate stop and restart a simulation the blue bar at the bottom allows the user to pan through different timesteps of the simulation window the button model parameters opens the screen for inputting the data for soil properties and constants through an interactive table as shown in fig 9 this screen also allows the importing of climatic data and total analytical salts in the soil required as the initial boundary conditions for simulations restore all values button restores all the values for the variables and constants to the model default value this allows the user to at least have one simulation using the default values the model results can be viewed as a single run and or grouped together for multiple runs the single run results button shows the results plotted against the simulation time for cumulative root water uptake root water uptake rate cumulative soil water storage and cumulative solute concentration as shown in fig 10 similarly the button multiple run results shows the results plotted for multiple runs shown in fig 11 that allows the user to assess the simulation for various scenarios the multiple runs are shown for the runs currently simulated in the smituv model and can be changed in the model as per user requirement 7 discussion smituv demonstrated the applicability of system dynamic models for complex nonlinear and dynamic problems such as simulation of soil water root salt interaction smituv is easily extendable and several other modules are planned the graphical approach of the smituv makes it possible for understanding the relationship between essential components involved in the system that were not understood previously by growers due to the inherent complexity and opaqueness of existing programming language based models it is not necessary for the researchers or collaborators participating in a study to be a modeler all stakeholders may utilize the user interface of smituv for interpretation of the behavior of the system for various conditions and accordingly inform the model developers for better calibration of the model setting up a complex feedback loop mechanism is difficult without the inherent knowledge of programming language and mathematical solvers some of the planned real world validation and grower usages of the model were hampered by the covid19 pandemic situation however supported by various projects in the future smituv will be a significant tool for modeling that involves collaboration from various interdisciplinary researchers to solve the complex and nonlinear nature of systems in the arid southwest region of the united states we expect that model will also be applicable globally where flood irrigation is practiced smituv integrates an emergent field of study involving system dynamics in agricultural systems research though widely adopted in other fields systems dynamic modeling has lacked in agricultural modeling domain this model has showcased the capacity to tackle a complex and dynamic system behavior using simple steps it contains different modules connected to each other dynamically like a symbolized circuit diagram once the diagram is setup in the system dynamic window it will simulate the system behavior correctly this inherent property of the model allows the researchers to integrate any module of interest into the already developed smituv for any additional research studies also researchers have the option to link the results from smituv to other software and applications like any other modeling technique modeling results will be adversely impacted if the variables and equations are formulated based on poor technical knowledge and evaluation the use of predictions from smituv without adequately setting the soil and other parameters can have an adverse impact on the study and deviation from the system s actual behavior therefore smituv needs to be adapted based on a good modeling practices such as model calibration with historical data sensitivity analysis of variables analysis of scenarios comparison with other similar models adhering to local conditions and field experiments and understanding the problem statement however even in cases where detailed data are not available but some generalization may be made there are education benefits in applying smituv with some synthetic simulations stakeholders can understand sensitivity of parameters and the complex relationships it should be noted that currently only flood irrigation methods are implemented thus application to other irrigation scenarios is not correct the key to successful model development is understanding the problem statement as modelers tend to develop unnecessarily complicated large models without understanding the problem type a modeler should keep in mind that the prediction of a system dynamic model is a representation of the systems behavior in real world and its adherence to those representations should be of utmost importance while developing and using the model the model source in isee stella native format and xmile format xml based standard for storing models is posted to github https github com skp703 smituv under open source gnu gplv3 license the code may be modified using stella architect or executed using free software isee player 8 conclusion a system dynamic model smituv was developed for simulating the dynamic changes in soil water infiltration solute transport and root water uptake under water and salinity stress by numerically solving one dimensional richards equation and advection diffusion equation based on finite difference approach smituv simulated results were similar to the finite element method based numerical model hydrus smituv is built on isee stella that has an easily modifiable and understandable development environment allowing a user to easily change the operation and structure of the model to match a specific project requirement unlike the presently available mathematical models smituv does not require any prior knowledge of programming language for model adaptation the simulation results of smituv show that this can be used as an efficient strategic tool for devising field and salinity management measures by stakeholders further smituv reinforces the potential of system dynamic modeling for complex interdisciplinary research studies data availability statement the model source in isee stella native format and xmile format xml based standard for storing models is posted to github https github com skp703 smituv under open source gnu gplv3 license some or all data models or code that support the findings of this study are available from the corresponding author upon reasonable request 1 stella model of simulation 2 daily rainfall crop root growth and solute concentration data declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement the development of the system dynamic model for simulating the one dimensional transient soil water flow solute transport and root water uptake by crops under water and salinity stress was supported by grant no 2015 68007 23130 national institute of food and agriculture nifa project u s department of agriculture usda this work is also supported by the usda nifa hatch project 1022633 appendix a major stock s converters c and flow f variables used in model variable type definition unit a i c soil swelling parameters dimensionless alph i c van genutchen parameter for fitting soil water retention curve swrc cm 1 bi c normalized root distribution function dimensionless c i c soil swelling parameters dimensionless ca c total analytical concentration of calcium mmol l calcium c percentage of calcium dimensionless co i c total salt concentration mmol l d i c soil interlayer spacing dimensionless di c diffusion constant cm2 day dw c diffusion constant of water cm2 day ec50 c crop salt tolerance threshold value ds m eci c salt content at each layer ds m esp i c exchangeable sodium percentage dimensionless etci c potential evapotranspiration cm day evapotranspiration c reference evapotranspiration cm day fm c weight fraction of montmorillonite dimensionless h i c water pressure head cm hm50 c water stress at 50 water extraction h 50 cm hmi c matric stress response function dimensionless hsi c osmotic stress response function dimensionless infiltration transmission rate i f flow equations cm per day k i c hydraulic conductivity cm day kc c consumptive water use dimensionless kg c modified gapon selectivity coefficient mmol l 1 2 ks i c saturated hydraulic conductivity cm day leached water s percolated water cm leaching f leaching rate cm per day m i c van genutchen parameter for fitting swrc dimensionless magnesium c percentage of magnesium dimensionless mass leached solute c amount of solute leached out mg mass solutei c mass of solute mg mg c total analytical concentration of magnesium mmol l n i c van genutchen parameter for fitting swrc dimensionless na c total analytical concentration of sodium mmol l ph i c ph of soil dimensionless ponded c ponded water head cm q i c water content cm3 cm 3 qr i c residual water content cm3 cm 3 qs i c saturated water content or porosity cm3 cm 3 r i c hydraulic reduction function dimensionless r i1 c hydraulic reduction due to salinity dimensionless r i2 c hydraulic reduction due to ph dimensionless root growth c root length cm root watery s root water storage cm sar i c sodium absorption ratio mmol l 1 2 sodium c percentage of sodium dimensionless soil water layer i s soil water storage cm solute infiltration transport i f downward solute flux g cm2 solute layer i s accumulated solute flux g cm2 solute leaching f rate of solute leached out g cm2 solutei c solute concentration at ith soil g cm3 solutei mg l c total salt concentration mg l solutei mmol l c salt concentration from s t sector mmol l tds g cm3 c initial solute concentration g cm3 uptake ratei f rate of water extraction cm per day x i c swelling factor of soil dimensionless z i c soil layer depth cm notation the following symbols are used in this paper c total solute concentration per unit volume of soil c empirical parameters of swelling c0 total salt concentration of the solution ca salt concentrations of calcium ctop total solute entering the upper soil layer d adjusted interlayer spacing ddiff diffusion constant dw solute diffusion coefficient in free water ec total salt concentration ec50 threshold value of crop salt tolerance esp exchangeable sodium percentage et0 average monthly potential evapotranspiration fmont weight fraction of montmorillonite in the soil h water pressure head capillary suction h50 pressure head at 50 root water extraction i infiltration of water is infiltration of solute k hydraulic conductivity k g modified gapon selectivity coefficient kc crop s coefficient for pecan ks saturated hydraulic conductivity mg salt concentrations of magnesium n pore size distribution n empirical parameters of swelling na salt concentrations of sodium p flux of water q darcy s volumetric flux r hydraulic reduction function r1 normalized function for exchangeable sodium r2 normalized function for adverse soil ph r2 pearson s correlation coefficient rmse root mean square error s sink term sar sodium absorption ratio se relative hydraulic conductivity t time t transmission rate of water tds total dissolved solids tp potential evapotranspiration ts transmission rate of solute ur root water uptake rate x swelling factor z depth of soil layer α air entry pressure α h water stress response function α s salinity stress response function β normalized root distribution function δzi soil compartment thickness q water content θr residual water content and θs saturated water content 
25911,fire safety in urban areas is gaining more attention along with the rapid development of urbanization and the increasing complexity of urban structures many fire spread models have been developed to support emergency decision making however most of them only investigate on single scenarios forest fire or house fire and usually have difficulty in balancing model accuracy and timeliness which makes it hard to apply these models to large scale heterogeneous scenarios in this paper we developed a fire spread model based on the heterogeneous cellular automata model for large scale complex wildland urban interface wui areas the model flexibly integrates a forest fire model and a house fire model based on thermal principles and empirical statistics a software platform is developed based on the fire spread model correspondingly to validate its performance taking a real wui fire occurred in california as a case study we conducted comparative simulation experiments based on our software platform and widely used forest fire simulation software flammap6 the experimental results show that our model can simulate fire spread process with high efficiency strong timeliness and competitive accuracy providing support on emergency decision making of large scale complex fire scenarios keywords fire spread model wui heterogeneous cellular automata model emergency decision making 1 introduction the frequency and intensity of wildland urban interface wui fires have been growing significantly in recent years manzello et al 2018 in 2016 a wui fire in fort mcmurray canada destroyed more than 2400 structures in 2017 forest fires in california killed at least 42 people and damaged more than 7000 buildings in 2018 california suffered a huge loss again because of wui fires which caused more than 80 deaths and the destruction of nearly 19 000 structures in late 2019 a wui fire in australia lasted for almost four months and caused more than 6 million hectares burned forest at least 28 deaths and more than 2000 structures destroyed large scale forest fires can destroy the surface vegetation cause tremendous damage to the local ecological environment and threaten lives of wild animals moreover forest fires have significant impacts on human lives especially in wui areas including direct property losses and health issues caused by air and water pollution to reduce casualties and property losses local emergency management departments need to evacuate people from dangerous areas in time and formulate foreground site incident action plans immediately a fire spread simulation is required to delineate the evacuation area and provide essential information for the evacuation and incident action plans duane aquilue et al 2016 investigated the fire process pattern with different wind parameters and landscape factors sullivan sharples et al 2014 evaluated the impact of topography parameters to the fire spread rate in specific areas rossa 2017 researched the effect of fuel moisture content on the spread rate of forest fires without considering wind or slope according to the fire spreading pattern current fire models can be generally separated into forest fire models and house fire models many existing fire models have shown acceptable accuracy for scenarios that contain one specific scenario fire type such as forest fires house fires etc mell et al 2010 however it is challenging to simulate large scale complex fire scenarios that include various types of potential burning units with different burning properties such as complex urban fires that contain both forest fires and house fires mell et al 2010 amato et al 2018 greg winter 2000 this issue is significant and urgent because there exists a high probability that forest fires spread to periphery urban areas and directly threaten people s property and lives rehm and evans 2013 sirca et al 2017 we think the lack of models for large scale complex urban fires might be due to the challenge to integrate these two fire scenarios into one system since they adopt different fire spread algorithms for fire simulation analysis and deduction in recent years fire spread research in complex scenarios is gaining momentum especially in wui areas where both land and communities are at risk of wildfires mell et al 2010 amato et al 2018 rehm and evans 2013 sirca et al 2017 however the fire spread models for wui areas are seldom studied and no existing model can meet the need of establishing an integrated fire simulation system it is urgent and essential to generate a fire spread modelling system for large scale complex urban areas containing forest and urban houses our research goal is to develop a heterogeneous cellular automata fire spread model and corresponding software for large scale complex urban areas the model can perform real time fire spread simulation and analysis with a low computational cost the software system can provide support for emergency management departments to make informed decisions on evacuation arrangement and firefighting organization ensuring timely and orderly evacuation of people in dangerous areas and minimizing the impact of fire on the environment the remainder of this paper is organized as follows related work is presented in section 2 the design and development of the fire spread model and software are explained in section 3 section 4 elaborates a detailed wui fire spread simulation and compares the experiment result with the real fire case section 5 provides discussion and conclusion 2 related work forest fire spread models mainly include empirical models probability models physical models and cellular automata models in the past the fire spread process was often simulated using empirical models based on environmental temperature humidity wind speed and terrain curry and fons 1940 proposed an empirical model which was the first time adopting mathematical concepts to describe the fire spread process they implemented actual ignition experiments to obtain fire spread parameters an empirical equation of fire spread was proposed after further analysis of fire spread rates normally empirical models only work well in some specific scenarios for different fire scenarios they may require different empirical models since they have different environmental parameters duane et al 2016 sullivan et al 2014 cruz et al 2018 probability models adopt mathematical probabilistic methods to statistically analyze and describe the fire spread process arthur 1967 dexter and williams 1976 noble et al 2006 vines 1969 a probability model is established by analyzing various data in forest fire spread and provides a reliable probability formula to predict the future development of the fire situation the advantage of probability models is users can predict the fire spread in a quite simple and easy way however both empirical and probability models are not physical based and can t well adapt to external environmental changes physical models are based on the thermodynamic analysis of the combustion process noble et al 2006 scott and burgan 2005 wu et al 2011 a pure physical model often contains a highly complex calculation process and requires high computing power so most of the current physical based forest fire models are semi empirical physical based models another important drawback of physical models is their need for accurate and physically coherent initial and boundary conditions cellular automata models are gaining momentum in recent years since their simple structure and low computational complexity when simulating forest fires li et al 2017 ohgai et al 2007 trucchia d andrea et al 2020 cellular automata models are established based on traditional forest fire spread models nowadays many studies have adopted artificial intelligence methods in optimizing fire spread models some fire spread models have been improved by combining with machine learning algorithms it has been demonstrated that machine learning algorithms can improve several parts in fire spread models such as ignition points estimation and burning paths prediction cortez and morais 2007 rodrigues and riva 2014 o connor et al 2017 as for the research of fire spread models in urban areas hamada first proposed an urban fire spread model for this kind of houses in 1951 which was constructed based on empirically assigned variables such as wind speed wind direction the spacing between houses and house type namba and yasuno 1986 revised hamada urban fire model and proposed a statistical empirical urban fire model this revised urban fire model was based on historical fire cases occurred in japan and adopted several statistical parameters to improve the traditional hamada model lots of physical based urban fire spread models were developed by analyzing physical energy law of the fire spread energy transfer mechanism and thermodynamic theory cousins et al 2002 himoto and tanaka 2000 2003 2008 lee and davidson 2008 physical based fire spread models are more accurate than traditional empirical and statistical models however it is challenging and costly to implement a physical based fire spread model because of its high complexity it also has the potential to procrastinate prediction when simulating large scale fires in urban areas using physically based fire models cellular automata models on low rise houses with wooden structures have been developed rapidly in recent years multiple heterogeneous cellular automata models have been developed based on the primitive cellular automata model with considering the impact of different house shapes and wooden structures ohgai et al 2007 li et al 2017 zhao 2011 high rise houses with cement structures are widely distributed in the core area of modern cities current research mainly focuses on the fire spread process among multiple floors in a single story building the commonly used methods of simulating the fire spread process in a single story building include epidemic theory tat 2002 random walk theory ramachandran 1985 mandelbrot 1964 percolation processes men shikov molchanov et al 1988 and probabilistic networks platt et al 1994 ling and williamson 1985 berlin 1980 these methods all set each room or compartment in the building as a calculation unit in which the state and diffusion of the fire are calculated and analyzed through a probability formula the fire spread simulation among multiple floors is usually physically based the fire spread trend is determined by calculated fire heat thermal radiation intensity wind speed and wind direction himoto and tanaka 2003 2008 cheng and hadjisophocleous 2009 2011 chen et al 2011 recently more attention has been paid on the fire spread modelling in wui areas mell et al 2010 amato et al 2018 rehm and evans 2013 sirca et al 2017 this scenario mainly adopts physical based models for fire spread simulation the type of burning materials trees grasses bushes and houses plays a significant role and their burning characteristics are often acquired from experiments other objects like roads rivers and barren land which act as the fire resistant element are also considered and denote proper traits the simulation processes for this scenario are often complicated and time consuming as they implement physical based models mell et al 2010 rehm and evans 2013 sirca et al 2017 these methods are not practical for large area simulation because they have a relatively high requirement on the computer hardware and related equipment in general existing fire spread models have shown acceptable performance in simulating the fire spread process in specific scenarios of forest areas house areas or wui areas however most of them are unable to include all these scenarios in the same platform and calculate large scale complex urban fire process in a short time our goal is to address this lack and the contributions of this work include 1 we propose a heterogeneous cellular automata model to simulate fire spread in the large scale complex wui scenario and elaborate the fire spread rules between forest and urban area based on thermal principle and empirical statistics 2 we optimize the fire spread algorithm in every single scenario and take fire resistant elements into consideration 3 comparative experiments are conducted based on our model and farsite with the reference of a real wui fire in getty california the experimental results have demonstrated that our model can generate timely and easily accessible fire spread predictions with comparable accuracy 3 model description 3 1 cellular definition and segmentation a wui area contains large forest areas urban areas fire resistant element areas road water wasteland etc to include computational units with different attributes at the same time we introduce a heterogeneous cellular automata model under a unified integration framework different types of cells represent different elements in the complex scenario in fig 1 the objects on the map are categorized into three main areas forest urban and fire resistant elements the forest area is divided into grids of the same size each grid contains a status attribute that changes according to its transferring law regular division may separate the individual house into patches in the urban area resulting in inconsistent status thus each house is considered as an independent computing unit with an irregular shape the fire resistant area includes nonflammable elements that can prevent a fire from spreading and is divided into regular cells as the forest area 3 2 cellular automata model of fire spread 3 2 1 fire spread in forest areas as mentioned in 3 1 the forest area is divided into square grids of uniform size in the cellular automata model in fig 2 the fire spread process in the forest area starts at the central burning cell i j and then spreads in eight moore neighbor directions r n r ne r e r se r s r sw r w and r nw the fire spread rate in each direction is calculated respectively through eq 1 by considering vegetation types terrain and meteorological factors mao 1993 in eq 1 r 0 is the initial fire spread rate calculated with temperature t wind scale v and relative humidity r h r is the fire spread rate adjusted by wind coefficient k w fuel coefficient k s and slope coefficient k f 1 r r 0 k w k s k f a n d r 0 0 03 t 0 05 v 0 01 100 r h 0 3 the cell status in the forest area can be defined in five types based on the different stages across combustion life cycle shown as table 1 the first step of the simulation is to manually assign the ignition area by switching the status of ignition cells from s f0 to s f1 then the status of each cell in the simulation area changes according to the following rules 1 s f0 s f1 when the eight moore neighbor cells of a s f0 cell i j contains a s f2 cell the status of s f0 cell i j will be updated by eq 2 and eq 3 2 c i j t δ t c i j t r i 1 j 1 t r i 1 j 1 t r i 1 j 1 t r i 1 j 1 t δ t 2 l r i j 1 t r i 1 j t r i 1 j t r i j 1 t δ t l δ t k l r m a x 3 s t a t u s o f c e l l i j s f 0 c i j t 1 0 s f 1 c i j t 1 0 where δ t is time step which is calculated by cell length l max forest fire spread rate r m a x and adjustment coefficient k a bigger k results in higher calculation efficiency but worse accuracy rui et al 2017 demonstrate that the analog graph is very close to a circle with less efficiency loss when k equals to 0 125 c i j t is a continuous value which is used to represent the combustion status of s f0 cell i j at time t when c i j t is greater than or equal to 1 0 the status of cell i j will be updated from s f0 to s f1 2 s f1 s f2 instead of changing sf1 to sf2 after a duration of δ t as rui et al 2017 we consider the fire spreading in the inner cell as eq 2 shows δ t is determined by the max forest fire spread rate of each cell indicating that δ t may be a relatively small number however the local square cell has comparatively large side length and normally consumes longer time than δ t to burn out the inner cell and grow into s f2 the inner slope and fuel are assumed the same thus the wind vector and the way of ignition are key factors to influence the inner fire spread as fig 3 shows as shown in fig 3 a the inner fire spread equally in all moore neighbor directions when the wind speed is 0 m s the balance breaks when the wind speed is greater than 0 m s the fire spread rate tends to the direction with maximum wind speed when the s f1 cell is manually ignited initial condition of the simulation as shown in fig 3 c the direction of inner fire spread r in is simplified to be same with the wind direction and the value of r in is calculated by eq 1 when the s f1 cell is ignited by a neighbor s f2 cell the fire spread direction is set as the direction that has the greatest impact on the s f1 cell as fig 3 d shows the wind blows from west to east mainly suppressing r w then r sw and finally r s thus we take the direction of r s as the inner fire spread direction after conducting the value and direction of r in an appropriate spread length r is assigned to calculate the burning time from s f1 to s f2 fig 4 shows the burning area generated respectively with r l 2 r l π r l 2 under the condition of no wind table 2 shows the fire spread statistics with different r the spread area is only 78 5 of the single forest cell area s q when r l 2 100 of s q when r l π and 157 1 of s q when r l 2 given that s f1 cell cannot spread fire outside and only s f2 cell can fig 4 a lefts 21 5 unburned fig 4 c burns out of the cell and 14 3 of its four moore neighbor cells by comparison fig 4 b is more reasonable overall we adopt r l π as the spread length and the time from s f1 to s f2 δ t 1 equals to l π r i n 3 s f2 s f3 when an s f2 cell is surrounded by cells with a status of s f2 s f3 s f4 or incombustible cells the cell will update its status into s f3 in the next δ t and lose the ability to ignite other objects 4 s f3 s f4 s f3 indicates that the fire in the forest cell is turning to extinguish after the next δ t the cell status is updated to full extinguish s f4 3 2 2 fire spread in urban area the combustion status and properties of urban house fires are more complicated than forest fires table 3 lists six independent states of house fires corresponding to different stages of house fires development a house can spread fire outside when its cell status is s u3 or s u4 according to historic statistics zhao 2011 points out that the influence range of a burning house is an approximate ellipse as shown in table 4 the house is simplified as a square cell wind vector is the core factor that determines the shape of the spread influence ellipse the direction of the major axis is consistent with wind direction the lengths of the axes are calculated with the wind speed given that the wind speed v rarely goes over 15 m s in the urban areas we constrain the value of v from 0 to 15 the length factor of the house cell d is conducted by house area a following eq 4 4 d a 2 when the influence range of the burning house cell m is set other house cells contained in or intersected with the range will be influenced by the thermal radiation and heat convection of m the ignition probability of unburned house n p m n is calculated with eq 5 zhao 2011 5 p m n p t n p w p s n p a m n where p t n represents the probabilistic contribution of house materials as shown in table 5 p w represents the contribution of weather factors as shown in table 6 p s n represents the impact of the status of the burning house m as mentioned in table 3 only a house cell with s u3 or s u4 can ignite other inflammable objects when the cell status is s u3 p s n is set to 0 4 when the cell status is s u4 p s n equals to 1 for other cell status p s n is set to 0 p a m n which equals to a m n a n represents the area proportion of the influenced house n a m n is the area where influence range and house cell n intersect while a n is the area of house cell n shown as fig 5 when a fire spreads in urban area an unburned house cell n is generally influenced by one or more than one burning houses thus the ignition probability i n of the house cell n is calculated by eq 6 6 i n 1 i 0 m n 1 p m m i n where m represents the index collection of burned houses and m n represents the size of collection m p m m i n is the probability of the unburned house n ignited by a burned house m m i i n is the total ignition probability of the unburned house n a random number p r is selected from the uniform distribution u 0 1 if i n is less than p r the house cell n cannot be ignited otherwise the house cell n is ignited and its status transforms from s u0 to s u1 the fire spread rules in urban area and status transformation from s u0 to s u1 have been discussed above other status transformation rules need to be defined as well the development of a house fire is affected by many factors zhao 2011 simplified this process to be only time related based on fire experts experiences table 7 shows the time of status transformation for different house materials we randomly choose an integer t r as the transform time from each time range 3 2 3 fire spread in wui area the fire spread process in wui areas is more complex than in single forest or urban areas the main obstacle appears on the spread rule in the interface of wildland and urban in this study we combine and optimize the fire spread model in single scenarios and define the fire spread rules in the interface the behavior of interface fire spread includes two types from house to forest from forest to house in the cellular automata model for houses unburned houses are mainly affected by the thermal radiation and thermal convection of the burning house although there are differences in cell status between forest and house the physical principle of ignition is almost the same the combustible objects can be ignited when the intensity of heat flow reaches critical ignition himoto and tanaka 2003 the heat release rate h r r of the burning house is determined by the number of combustibles combustible materials ventilation conditions etc once a burning house steps into full development status h r r is relatively large a lot of research on h r r of burning houses shows that h r r per unit ranges in 197 1014 k w m 2 and the burning duration usually keeps above 100 s as shown in table 8 yannan wang 2014 wu et al 2016 besides research on the ignition characteristics of several common types of wood under different radiant heat fluxes found that the critical heat flux of different wood locals between 23 k w m 2 and 27 k w m 2 shen et al 2007 yang et al 2010 when the radiant heat flux reaches 50 k w m 2 the ignition time is less than 50 s when the radiant heat flux reaches 70 k w m 2 the ignition time is less than 15 s the radiant heat flux from a burning house cell in s u3 or s u4 can far outweigh than 70 k w m 2 which indicates that the forest woods within the influence range can be ignited in a short time fig 6 depicts the scenario of the fire spreads from house to forest another complex scenario is fire spreading from forest to houses the cellular automata model for forest areas is constrained by a spread rule that forest fires spread only between neighbor forest cells real forest fires normally release strong flames and tremendous heat with great potential to ignite houses at a certain distance in wui areas inspired by the house cellular automata model we adopted the thermal physics theory and the forest fire influence ellipse to establish a probability model for forest fires spreading to the neighbor houses the heat release rate of forest fires can be defined by the release heat intensity i s heat release per unit area per unit time k w m 2 wang et al 1996 made forest fire experiments and got the statistics of i s as table 9 shows from table 9 we found the i s of forest fires is relatively higher than that of house fires so the influence range of forest fires is bigger than house fires to acquire the quantitative results of forest fire influence range zarate et al 2008 researched on the max ignition distance of forest fires and concluded the safe distance of wood houses is 36 m when there is a surface fire and 54 m when there is a crown fire to validate and improve existing theoretical and empirical models of crown fire behavior stocks et al 2004 found the spread rate of crown fires ranges from 15 0 to 70 0 m m i n a database of active crown fire shows that the spread rate of 44 crown fires in canada ranges from 10 7 to 107 0 m m i n while the spread rate of 14 crown fires in the us ranges from 13 7 to 80 5 m m i n alexander and cruz 2006 here we take the average minimum spread rate of crown fires 13 1 m m i n as the limit to determine whether a forest fire evolves into a crown fire given that the head fire is stronger than the rear fire we add an adjustment coefficient k s f for surface fires and k c f for crown fires to the influence ellipse parameter a according to table 4 when the wind speed is 15 m s the maximum a equals to the sum of safe distance and d 2 thus k s f is 3 0 and k c f is 4 5 the maximum influence ranges of forest fires and ellipses of surface fires and crown fires are shown in tables 10 12 when an unburned house locates in the forest influence range we adopt eq 4 to calculate the ignition probability p m n given that p s n is determined by the burning cell and the fire intensity of surface fire is relatively similar to the s u3 house cell and the fire intensity of crowns fire is more intense than surface fires and similar to the s u4 house cell therefore we set p s n 0 4 for a surface fire and p s n 1 0 for a crown fire fig 7 shows the scenario of fire spreads from forest cells to house cells 3 2 4 fire resistant elements fire resistant elements are common in wui area including non flammable objects like artificial elements road wasteland green firebreak and so on and natural elements lake river and so on fire resistant elements destroy the continuity of combustion space and prevent fire from spreading further our fire spread model considers non flammable areas such as the wasteland around houses and forest shown as the white areas in fig 1 therefore the model mentioned in 3 2 3 can be applied to deal with fire resistant elements fig 8 shows how fire spreads in scenarios with fire resistant elements whether the fire can spread through fire resistant elements depends on the heat release intensity of the burning cell which can be simulated as the influence range using our cellular automaton model 3 3 model design 3 3 1 software components the software platform is developed based on typical browser service architecture the backend services such as the heterogeneous cellular automata model is developed in java programming language while the user interface for request response interactions and results visualization is developed in javascript the software architecture shown as fig 9 contains the wui fire spread model a data support component a web service interface component and a web visualization component the data support component is responsible for data storage and management in the wui fire simulation such as vegetation types terrain meteorological data etc the wui fire spread model is the kernel component of the software and simulates the fire spread process in complex wui scenarios the service interface component is used to accept fire simulation requests from users parse key simulation parameters and activate the wui fire spread model after the model simulation is completed the service interface component packs up the result and send it back to users as a response the web visualization component is developed for visualizing simulation results and providing support for emergency commanders to analyze fire risks and make rescue decisions 3 3 2 model diagram fig 10 elaborates the algorithm flow of our heterogeneous cellular automata model for wui fire spread first the model is activated by a user request it extracts required simulation parameters from the request including simulation time length t e n d ignition coordinates i c and meteorological data the model then checks whether i c locates within fire resistant elements based on its vegetation type if true the simulation stops with error messages otherwise the initial ignition area is flammable and set secondly if i c locates in forest areas the status of the ignition cell is updated from sf0 to sf1 as only when the forest cell is in sf2 status can spread outside we further turn it to sf2 and set δ t 1 as the initial value of t t o t a l if i c locates in urban areas the status of ignition cell is updated to su3 and the initial value of t t o t a l equals to the sum of t 12 and t 23 the house fire algorithm requires an extra parameter t u p d a t e to control the frequency that calculates the probability of unburned houses or forest get ignited the house fire spread algorithm is activated when the cumulative time of the house fire t c o u n t t u p d a t e the time of forest fire model and house fire model should be unified to be of the same dimension in the wui fire spread scenario the time of forest fire model is preferentially set as time base since δ t is far less than t u p d a t e detailed fire spread rules and status transformations are described in 3 2 in the end when t t o t a l t e n d the simulation is completed with the results of sequential fire data are returned to the user 4 experiments to demonstrate the practicality and reliability of our model we simulated a real wui fire event called getty fire which occurred in california us on october 28 2019 and compared its performance with flammap6 a widely used forest fire simulation software 4 1 experimental area getty fire broke out at 01 34 00 on october 28th 2019 it first spread in forest areas and then affected urban areas the detailed fire statistics can be found on the website of los angeles fire department lafd 2020 fig 11 shows our experimental area 4 2 experimental data we use the same experimental data to simulate getty fire through our model and flammap6 respectively flammap6 is a fire analysis software developed by u s forest service usfs it integrates two forest fire simulation models flammap and farsite usfs 2020a b c with the advantage of simulating wildfire growth and behavior with detailed meteorological data farsite is widely used by u s federal and state land management agencies like u s forest service national park service and so on usfs 2020 in this study we chose farsite as the comparative fire spread model the inputs of farsite include data related to fuels and wind field generated by windninja usfs 2020 it also requires a special landscape file consisting of terrain fire behavior fuel models forest canopy cover forest canopy height forest canopy base height and forest canopy bulk density comparing to farsite the inputs required by our model are relatively simple including vegetation type terrain meteorological data and house data the required data are listed in table 13 landfire is a shared program between usfs and u s department of the interior usdi aiming to provide landscape scale geospatial products to support cross boundary planning management and operations landfire 2020 most of the data required for the experiments can be found in landfire as shown in table 14 the meteorological data were obtained through windninja and mesowest utah 2020 windninja can automatically search and download the meteorological data at nearest station kvny based on the landscape file then we used mesowest to compensate for the lack of relative humidity data in windninja fig 12 shows the meteorological data after the fire broke out from it we can see that the diurnal variation of temperature was between 10 c and 25 c with a high temperature zone occurred from the 12th hour to the 17th hour the relative humidity was lower in the first 18 h and then increased rapidly above 35 the wind speed was higher in the first 12 h and then gradually decreased the wind direction ranged 0 30 in the first 8 h 330 360 in the following 10 h and 240 360 or 0 60 in the last 13 h from these meteorological parameters we conclude that the fire spreads rapidly in the first 15 h and the wind lead the fire to the south of the ignition the house and road vector data come from an open source database openstreetmap 2020 because openstreetmap provides real time updates it may cause minor inconsistency with vegetation type data increasing the deviation between simulation and reality fig 13 shows the vector data downloaded from openstreetmap most houses on the map are single and wooden the fire ignition location and fire perimeter data were obtained from geomac which is an online wildfire map application developed by the united states geological survey usgs and aims to provide locations and boundaries of america historical fires usgs 2020 as shown in fig 14 the fire ignition located at 34 5 47 n 118 28 53 w and the fire perimeter were updated three times october 29th october 30th and november 2nd the minor differences in fire perimeters at different times indicates that the fire was almost completely controlled on or before october 29th in this case we set the final record of the fire perimeter on november 2nd as the reference fire perimeter both the experimental data and simulation results were uniformly projected to the same coordinate system resulting in a slight geospatial inconsistency of burned acres between the original and projected data lafd reported that getty fire had spread 618 acres in the first 13 h accounting for 83 of final burned area the fire forces almost constrained the fire at 20 00 on october 28th then the fire gradually extinguished with little change in the fire perimeter to fully capture the whole fire spread process we set the simulation time to 20 h from 1 34 to 21 34 we found that the reference fire perimeter was about the same as when the fire was almost under control which means that a valid simulation should result in a fire perimeter similar with the reference fire perimeter around 13 h after the fire broke out 4 3 results and discussion we simulated the same grey fire event separately using our model and farsite the simulation results and time consumption were compared to assess the performance and accuracy of our model all the comparative experiments adopted the same computational architecture a 4 core 2 30 ghz inter r core tm i7 10510u laptop with 16 gb of ram table 15 lists the cpu time consumed for different simulation durations when using our model and farsite respectively from it we can see that our model runs much faster than farsite it only takes about 4 s even when the simulation duration is 24 h for each simulation duration farsite consumes nearly 20 times as long as our model long range fire spotting modelling is one of the advantages of farsite however fire spotting is nonlinear and stochastic albini 1979 fernandez pello 2017 kaur and pagnini 2016 trucchia et al 2019 the difference between simulated burning area and observations increased rapidly when farsite integrated its fire spotting algorithm moreover there is no fire spotting record available for grey fire overall neither farsite nor our model incorporates fire spotting algorithm in this study since flammap6 only supports simulations on the hour instead of simulating from 1 34 to 21 34 we averaged the simulation results of two separate experiments 1 00 21 00 farsite1 and 2 00 22 00 farsite2 fig 15 a shows the fire perimeter changes within 600 min after the simulation started from fig 15 b we can see that the fire perimeter of farsite1 and farsite2 are almost the same fig 16 shows the simulation results of our model the simulation time is from 1 34 to 21 34 as shown in fig 16 b the burned area simulated from farsite and our model both increases linearly while the lafd observed burned area increases rapidly from 120 to 240 min then grows slowly until it stops around 800 min the simulated burned area of both models exceeds lafd observations around 600 min the area difference between simulation results and observations may be caused by several reasons first the meteorological data in getty fire area may be inconsistent with the meteorological station kvny which is 12 km away from the fire in particular the difference in wind direction may be the reason for the significant difference between simulated results and observations in the northern area secondly the dynamical change of vegetation types leads to the uncertainty of fuel distribution and affects simulation results third the observed fire perimeters from geomac were recorded through gps measurement and remote sensing imagery interpretation which can be greatly affected by gps accuracy image resolution and image quality affected by clouds dust fog wind and bad weather condition moreover there were 1165 firefighters 170 fire engines and 11 helicopters in total taking part in the fire rescue in getty fire significantly slowing the fire spread process however we didn t include fire rescue forces in both our current model and farsite overall our model can generate fire perimeter results similar to geomac records with much less time than farsite another advantage of our model over farsite is that it can simulate fire spread in wui scenarios rather than just in forest areas fig 17 shows the fire spread process simulated by our model we found that getty fire was greatly affected by north and northeast wind and spread fast in the south area after 5 h the fire reached the wui area and ignited several houses fig 17 k shows the statistics of burned houses simulated by our model and from lafd records in the first 4 h the fire only spread in the forest area our model results show that the fire began to spread in the wui area and ignite some houses after 4 h lafd records show that at least five houses were burned in around 420 min while our model predicted that at least 8 houses were ignited during the same period after 480 min the difference in the number of burned houses between our model and geomac records became bigger and our model predicted more houses get burned the main reason for the difference is the fire rescue forces as mentioned before lafd reported that as least 470 firefighters had participated in the fire rescue within 171 min after the fire broke out however the strong wind in the early period made it difficult to contain the fire even the number of firefighters increased to more than 600 lafd 2020 therefore some houses still got burned with the help of fire rescue forces and weather conditions the houses were better protected and only 25 houses were ignited eventually since we didn t consider fire rescue forces in our model the simulated number of burned houses was more than geomac records 5 conclusions this paper proposes a fire spread heterogeneous cellular automata model and elaborates it in different large scale scenarios including forest areas urban areas and wui areas a software platform is developed to simulate the fire spread process under different climatic and territorial variations a typical wui fire event occurred in getty california is simulated with our model and farsite respectively the simulation results and time consumption of the two models are compared with observed fire data from geomac the comparison results show that our model can generate fire spread predictions with acceptable accuracy short running time and easily accessible data providing better support to emergency decision making for large scale fire spread in wui areas our future work will consider other core factors that impact the fire spread process the first factor is the long range fire spotting phenomenon fire spotting is nonlinear and stochastic it is still challenging to use current existing fire spotting algorithms to get fire spread predictions with acceptable accuracy and efficiency in large scale scenarios the second factor is fire resistant elements such as manual rescue forces and natural fire resistant elements fire resistant elements become more and more non negligible in disaster prevention and mitigation it s necessary to include them in fire spread modelling quantitatively for better fire control and protection the last factor is the reliability of empirical models empirical models may not be able to produce promising simulation results in various wui scenarios more fire events need to be recorded and fully utilized through data mining method such as machine learning to optimize empirical models and improve fire spreading prediction furthermore we have provided here only one case study to serve as a proof of reliability for using our model in simulating complex wui fire scenarios more cases are needed to fully validate our model declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this paper is supported by the science technology and innovation commission of shenzhen city china grant number jcyj20180508152055235 the ministry of science and technology of china grant number 2016yfc0803107 and the science and technology department of guangdong province china grant number 2019b111104001 appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104895 
25911,fire safety in urban areas is gaining more attention along with the rapid development of urbanization and the increasing complexity of urban structures many fire spread models have been developed to support emergency decision making however most of them only investigate on single scenarios forest fire or house fire and usually have difficulty in balancing model accuracy and timeliness which makes it hard to apply these models to large scale heterogeneous scenarios in this paper we developed a fire spread model based on the heterogeneous cellular automata model for large scale complex wildland urban interface wui areas the model flexibly integrates a forest fire model and a house fire model based on thermal principles and empirical statistics a software platform is developed based on the fire spread model correspondingly to validate its performance taking a real wui fire occurred in california as a case study we conducted comparative simulation experiments based on our software platform and widely used forest fire simulation software flammap6 the experimental results show that our model can simulate fire spread process with high efficiency strong timeliness and competitive accuracy providing support on emergency decision making of large scale complex fire scenarios keywords fire spread model wui heterogeneous cellular automata model emergency decision making 1 introduction the frequency and intensity of wildland urban interface wui fires have been growing significantly in recent years manzello et al 2018 in 2016 a wui fire in fort mcmurray canada destroyed more than 2400 structures in 2017 forest fires in california killed at least 42 people and damaged more than 7000 buildings in 2018 california suffered a huge loss again because of wui fires which caused more than 80 deaths and the destruction of nearly 19 000 structures in late 2019 a wui fire in australia lasted for almost four months and caused more than 6 million hectares burned forest at least 28 deaths and more than 2000 structures destroyed large scale forest fires can destroy the surface vegetation cause tremendous damage to the local ecological environment and threaten lives of wild animals moreover forest fires have significant impacts on human lives especially in wui areas including direct property losses and health issues caused by air and water pollution to reduce casualties and property losses local emergency management departments need to evacuate people from dangerous areas in time and formulate foreground site incident action plans immediately a fire spread simulation is required to delineate the evacuation area and provide essential information for the evacuation and incident action plans duane aquilue et al 2016 investigated the fire process pattern with different wind parameters and landscape factors sullivan sharples et al 2014 evaluated the impact of topography parameters to the fire spread rate in specific areas rossa 2017 researched the effect of fuel moisture content on the spread rate of forest fires without considering wind or slope according to the fire spreading pattern current fire models can be generally separated into forest fire models and house fire models many existing fire models have shown acceptable accuracy for scenarios that contain one specific scenario fire type such as forest fires house fires etc mell et al 2010 however it is challenging to simulate large scale complex fire scenarios that include various types of potential burning units with different burning properties such as complex urban fires that contain both forest fires and house fires mell et al 2010 amato et al 2018 greg winter 2000 this issue is significant and urgent because there exists a high probability that forest fires spread to periphery urban areas and directly threaten people s property and lives rehm and evans 2013 sirca et al 2017 we think the lack of models for large scale complex urban fires might be due to the challenge to integrate these two fire scenarios into one system since they adopt different fire spread algorithms for fire simulation analysis and deduction in recent years fire spread research in complex scenarios is gaining momentum especially in wui areas where both land and communities are at risk of wildfires mell et al 2010 amato et al 2018 rehm and evans 2013 sirca et al 2017 however the fire spread models for wui areas are seldom studied and no existing model can meet the need of establishing an integrated fire simulation system it is urgent and essential to generate a fire spread modelling system for large scale complex urban areas containing forest and urban houses our research goal is to develop a heterogeneous cellular automata fire spread model and corresponding software for large scale complex urban areas the model can perform real time fire spread simulation and analysis with a low computational cost the software system can provide support for emergency management departments to make informed decisions on evacuation arrangement and firefighting organization ensuring timely and orderly evacuation of people in dangerous areas and minimizing the impact of fire on the environment the remainder of this paper is organized as follows related work is presented in section 2 the design and development of the fire spread model and software are explained in section 3 section 4 elaborates a detailed wui fire spread simulation and compares the experiment result with the real fire case section 5 provides discussion and conclusion 2 related work forest fire spread models mainly include empirical models probability models physical models and cellular automata models in the past the fire spread process was often simulated using empirical models based on environmental temperature humidity wind speed and terrain curry and fons 1940 proposed an empirical model which was the first time adopting mathematical concepts to describe the fire spread process they implemented actual ignition experiments to obtain fire spread parameters an empirical equation of fire spread was proposed after further analysis of fire spread rates normally empirical models only work well in some specific scenarios for different fire scenarios they may require different empirical models since they have different environmental parameters duane et al 2016 sullivan et al 2014 cruz et al 2018 probability models adopt mathematical probabilistic methods to statistically analyze and describe the fire spread process arthur 1967 dexter and williams 1976 noble et al 2006 vines 1969 a probability model is established by analyzing various data in forest fire spread and provides a reliable probability formula to predict the future development of the fire situation the advantage of probability models is users can predict the fire spread in a quite simple and easy way however both empirical and probability models are not physical based and can t well adapt to external environmental changes physical models are based on the thermodynamic analysis of the combustion process noble et al 2006 scott and burgan 2005 wu et al 2011 a pure physical model often contains a highly complex calculation process and requires high computing power so most of the current physical based forest fire models are semi empirical physical based models another important drawback of physical models is their need for accurate and physically coherent initial and boundary conditions cellular automata models are gaining momentum in recent years since their simple structure and low computational complexity when simulating forest fires li et al 2017 ohgai et al 2007 trucchia d andrea et al 2020 cellular automata models are established based on traditional forest fire spread models nowadays many studies have adopted artificial intelligence methods in optimizing fire spread models some fire spread models have been improved by combining with machine learning algorithms it has been demonstrated that machine learning algorithms can improve several parts in fire spread models such as ignition points estimation and burning paths prediction cortez and morais 2007 rodrigues and riva 2014 o connor et al 2017 as for the research of fire spread models in urban areas hamada first proposed an urban fire spread model for this kind of houses in 1951 which was constructed based on empirically assigned variables such as wind speed wind direction the spacing between houses and house type namba and yasuno 1986 revised hamada urban fire model and proposed a statistical empirical urban fire model this revised urban fire model was based on historical fire cases occurred in japan and adopted several statistical parameters to improve the traditional hamada model lots of physical based urban fire spread models were developed by analyzing physical energy law of the fire spread energy transfer mechanism and thermodynamic theory cousins et al 2002 himoto and tanaka 2000 2003 2008 lee and davidson 2008 physical based fire spread models are more accurate than traditional empirical and statistical models however it is challenging and costly to implement a physical based fire spread model because of its high complexity it also has the potential to procrastinate prediction when simulating large scale fires in urban areas using physically based fire models cellular automata models on low rise houses with wooden structures have been developed rapidly in recent years multiple heterogeneous cellular automata models have been developed based on the primitive cellular automata model with considering the impact of different house shapes and wooden structures ohgai et al 2007 li et al 2017 zhao 2011 high rise houses with cement structures are widely distributed in the core area of modern cities current research mainly focuses on the fire spread process among multiple floors in a single story building the commonly used methods of simulating the fire spread process in a single story building include epidemic theory tat 2002 random walk theory ramachandran 1985 mandelbrot 1964 percolation processes men shikov molchanov et al 1988 and probabilistic networks platt et al 1994 ling and williamson 1985 berlin 1980 these methods all set each room or compartment in the building as a calculation unit in which the state and diffusion of the fire are calculated and analyzed through a probability formula the fire spread simulation among multiple floors is usually physically based the fire spread trend is determined by calculated fire heat thermal radiation intensity wind speed and wind direction himoto and tanaka 2003 2008 cheng and hadjisophocleous 2009 2011 chen et al 2011 recently more attention has been paid on the fire spread modelling in wui areas mell et al 2010 amato et al 2018 rehm and evans 2013 sirca et al 2017 this scenario mainly adopts physical based models for fire spread simulation the type of burning materials trees grasses bushes and houses plays a significant role and their burning characteristics are often acquired from experiments other objects like roads rivers and barren land which act as the fire resistant element are also considered and denote proper traits the simulation processes for this scenario are often complicated and time consuming as they implement physical based models mell et al 2010 rehm and evans 2013 sirca et al 2017 these methods are not practical for large area simulation because they have a relatively high requirement on the computer hardware and related equipment in general existing fire spread models have shown acceptable performance in simulating the fire spread process in specific scenarios of forest areas house areas or wui areas however most of them are unable to include all these scenarios in the same platform and calculate large scale complex urban fire process in a short time our goal is to address this lack and the contributions of this work include 1 we propose a heterogeneous cellular automata model to simulate fire spread in the large scale complex wui scenario and elaborate the fire spread rules between forest and urban area based on thermal principle and empirical statistics 2 we optimize the fire spread algorithm in every single scenario and take fire resistant elements into consideration 3 comparative experiments are conducted based on our model and farsite with the reference of a real wui fire in getty california the experimental results have demonstrated that our model can generate timely and easily accessible fire spread predictions with comparable accuracy 3 model description 3 1 cellular definition and segmentation a wui area contains large forest areas urban areas fire resistant element areas road water wasteland etc to include computational units with different attributes at the same time we introduce a heterogeneous cellular automata model under a unified integration framework different types of cells represent different elements in the complex scenario in fig 1 the objects on the map are categorized into three main areas forest urban and fire resistant elements the forest area is divided into grids of the same size each grid contains a status attribute that changes according to its transferring law regular division may separate the individual house into patches in the urban area resulting in inconsistent status thus each house is considered as an independent computing unit with an irregular shape the fire resistant area includes nonflammable elements that can prevent a fire from spreading and is divided into regular cells as the forest area 3 2 cellular automata model of fire spread 3 2 1 fire spread in forest areas as mentioned in 3 1 the forest area is divided into square grids of uniform size in the cellular automata model in fig 2 the fire spread process in the forest area starts at the central burning cell i j and then spreads in eight moore neighbor directions r n r ne r e r se r s r sw r w and r nw the fire spread rate in each direction is calculated respectively through eq 1 by considering vegetation types terrain and meteorological factors mao 1993 in eq 1 r 0 is the initial fire spread rate calculated with temperature t wind scale v and relative humidity r h r is the fire spread rate adjusted by wind coefficient k w fuel coefficient k s and slope coefficient k f 1 r r 0 k w k s k f a n d r 0 0 03 t 0 05 v 0 01 100 r h 0 3 the cell status in the forest area can be defined in five types based on the different stages across combustion life cycle shown as table 1 the first step of the simulation is to manually assign the ignition area by switching the status of ignition cells from s f0 to s f1 then the status of each cell in the simulation area changes according to the following rules 1 s f0 s f1 when the eight moore neighbor cells of a s f0 cell i j contains a s f2 cell the status of s f0 cell i j will be updated by eq 2 and eq 3 2 c i j t δ t c i j t r i 1 j 1 t r i 1 j 1 t r i 1 j 1 t r i 1 j 1 t δ t 2 l r i j 1 t r i 1 j t r i 1 j t r i j 1 t δ t l δ t k l r m a x 3 s t a t u s o f c e l l i j s f 0 c i j t 1 0 s f 1 c i j t 1 0 where δ t is time step which is calculated by cell length l max forest fire spread rate r m a x and adjustment coefficient k a bigger k results in higher calculation efficiency but worse accuracy rui et al 2017 demonstrate that the analog graph is very close to a circle with less efficiency loss when k equals to 0 125 c i j t is a continuous value which is used to represent the combustion status of s f0 cell i j at time t when c i j t is greater than or equal to 1 0 the status of cell i j will be updated from s f0 to s f1 2 s f1 s f2 instead of changing sf1 to sf2 after a duration of δ t as rui et al 2017 we consider the fire spreading in the inner cell as eq 2 shows δ t is determined by the max forest fire spread rate of each cell indicating that δ t may be a relatively small number however the local square cell has comparatively large side length and normally consumes longer time than δ t to burn out the inner cell and grow into s f2 the inner slope and fuel are assumed the same thus the wind vector and the way of ignition are key factors to influence the inner fire spread as fig 3 shows as shown in fig 3 a the inner fire spread equally in all moore neighbor directions when the wind speed is 0 m s the balance breaks when the wind speed is greater than 0 m s the fire spread rate tends to the direction with maximum wind speed when the s f1 cell is manually ignited initial condition of the simulation as shown in fig 3 c the direction of inner fire spread r in is simplified to be same with the wind direction and the value of r in is calculated by eq 1 when the s f1 cell is ignited by a neighbor s f2 cell the fire spread direction is set as the direction that has the greatest impact on the s f1 cell as fig 3 d shows the wind blows from west to east mainly suppressing r w then r sw and finally r s thus we take the direction of r s as the inner fire spread direction after conducting the value and direction of r in an appropriate spread length r is assigned to calculate the burning time from s f1 to s f2 fig 4 shows the burning area generated respectively with r l 2 r l π r l 2 under the condition of no wind table 2 shows the fire spread statistics with different r the spread area is only 78 5 of the single forest cell area s q when r l 2 100 of s q when r l π and 157 1 of s q when r l 2 given that s f1 cell cannot spread fire outside and only s f2 cell can fig 4 a lefts 21 5 unburned fig 4 c burns out of the cell and 14 3 of its four moore neighbor cells by comparison fig 4 b is more reasonable overall we adopt r l π as the spread length and the time from s f1 to s f2 δ t 1 equals to l π r i n 3 s f2 s f3 when an s f2 cell is surrounded by cells with a status of s f2 s f3 s f4 or incombustible cells the cell will update its status into s f3 in the next δ t and lose the ability to ignite other objects 4 s f3 s f4 s f3 indicates that the fire in the forest cell is turning to extinguish after the next δ t the cell status is updated to full extinguish s f4 3 2 2 fire spread in urban area the combustion status and properties of urban house fires are more complicated than forest fires table 3 lists six independent states of house fires corresponding to different stages of house fires development a house can spread fire outside when its cell status is s u3 or s u4 according to historic statistics zhao 2011 points out that the influence range of a burning house is an approximate ellipse as shown in table 4 the house is simplified as a square cell wind vector is the core factor that determines the shape of the spread influence ellipse the direction of the major axis is consistent with wind direction the lengths of the axes are calculated with the wind speed given that the wind speed v rarely goes over 15 m s in the urban areas we constrain the value of v from 0 to 15 the length factor of the house cell d is conducted by house area a following eq 4 4 d a 2 when the influence range of the burning house cell m is set other house cells contained in or intersected with the range will be influenced by the thermal radiation and heat convection of m the ignition probability of unburned house n p m n is calculated with eq 5 zhao 2011 5 p m n p t n p w p s n p a m n where p t n represents the probabilistic contribution of house materials as shown in table 5 p w represents the contribution of weather factors as shown in table 6 p s n represents the impact of the status of the burning house m as mentioned in table 3 only a house cell with s u3 or s u4 can ignite other inflammable objects when the cell status is s u3 p s n is set to 0 4 when the cell status is s u4 p s n equals to 1 for other cell status p s n is set to 0 p a m n which equals to a m n a n represents the area proportion of the influenced house n a m n is the area where influence range and house cell n intersect while a n is the area of house cell n shown as fig 5 when a fire spreads in urban area an unburned house cell n is generally influenced by one or more than one burning houses thus the ignition probability i n of the house cell n is calculated by eq 6 6 i n 1 i 0 m n 1 p m m i n where m represents the index collection of burned houses and m n represents the size of collection m p m m i n is the probability of the unburned house n ignited by a burned house m m i i n is the total ignition probability of the unburned house n a random number p r is selected from the uniform distribution u 0 1 if i n is less than p r the house cell n cannot be ignited otherwise the house cell n is ignited and its status transforms from s u0 to s u1 the fire spread rules in urban area and status transformation from s u0 to s u1 have been discussed above other status transformation rules need to be defined as well the development of a house fire is affected by many factors zhao 2011 simplified this process to be only time related based on fire experts experiences table 7 shows the time of status transformation for different house materials we randomly choose an integer t r as the transform time from each time range 3 2 3 fire spread in wui area the fire spread process in wui areas is more complex than in single forest or urban areas the main obstacle appears on the spread rule in the interface of wildland and urban in this study we combine and optimize the fire spread model in single scenarios and define the fire spread rules in the interface the behavior of interface fire spread includes two types from house to forest from forest to house in the cellular automata model for houses unburned houses are mainly affected by the thermal radiation and thermal convection of the burning house although there are differences in cell status between forest and house the physical principle of ignition is almost the same the combustible objects can be ignited when the intensity of heat flow reaches critical ignition himoto and tanaka 2003 the heat release rate h r r of the burning house is determined by the number of combustibles combustible materials ventilation conditions etc once a burning house steps into full development status h r r is relatively large a lot of research on h r r of burning houses shows that h r r per unit ranges in 197 1014 k w m 2 and the burning duration usually keeps above 100 s as shown in table 8 yannan wang 2014 wu et al 2016 besides research on the ignition characteristics of several common types of wood under different radiant heat fluxes found that the critical heat flux of different wood locals between 23 k w m 2 and 27 k w m 2 shen et al 2007 yang et al 2010 when the radiant heat flux reaches 50 k w m 2 the ignition time is less than 50 s when the radiant heat flux reaches 70 k w m 2 the ignition time is less than 15 s the radiant heat flux from a burning house cell in s u3 or s u4 can far outweigh than 70 k w m 2 which indicates that the forest woods within the influence range can be ignited in a short time fig 6 depicts the scenario of the fire spreads from house to forest another complex scenario is fire spreading from forest to houses the cellular automata model for forest areas is constrained by a spread rule that forest fires spread only between neighbor forest cells real forest fires normally release strong flames and tremendous heat with great potential to ignite houses at a certain distance in wui areas inspired by the house cellular automata model we adopted the thermal physics theory and the forest fire influence ellipse to establish a probability model for forest fires spreading to the neighbor houses the heat release rate of forest fires can be defined by the release heat intensity i s heat release per unit area per unit time k w m 2 wang et al 1996 made forest fire experiments and got the statistics of i s as table 9 shows from table 9 we found the i s of forest fires is relatively higher than that of house fires so the influence range of forest fires is bigger than house fires to acquire the quantitative results of forest fire influence range zarate et al 2008 researched on the max ignition distance of forest fires and concluded the safe distance of wood houses is 36 m when there is a surface fire and 54 m when there is a crown fire to validate and improve existing theoretical and empirical models of crown fire behavior stocks et al 2004 found the spread rate of crown fires ranges from 15 0 to 70 0 m m i n a database of active crown fire shows that the spread rate of 44 crown fires in canada ranges from 10 7 to 107 0 m m i n while the spread rate of 14 crown fires in the us ranges from 13 7 to 80 5 m m i n alexander and cruz 2006 here we take the average minimum spread rate of crown fires 13 1 m m i n as the limit to determine whether a forest fire evolves into a crown fire given that the head fire is stronger than the rear fire we add an adjustment coefficient k s f for surface fires and k c f for crown fires to the influence ellipse parameter a according to table 4 when the wind speed is 15 m s the maximum a equals to the sum of safe distance and d 2 thus k s f is 3 0 and k c f is 4 5 the maximum influence ranges of forest fires and ellipses of surface fires and crown fires are shown in tables 10 12 when an unburned house locates in the forest influence range we adopt eq 4 to calculate the ignition probability p m n given that p s n is determined by the burning cell and the fire intensity of surface fire is relatively similar to the s u3 house cell and the fire intensity of crowns fire is more intense than surface fires and similar to the s u4 house cell therefore we set p s n 0 4 for a surface fire and p s n 1 0 for a crown fire fig 7 shows the scenario of fire spreads from forest cells to house cells 3 2 4 fire resistant elements fire resistant elements are common in wui area including non flammable objects like artificial elements road wasteland green firebreak and so on and natural elements lake river and so on fire resistant elements destroy the continuity of combustion space and prevent fire from spreading further our fire spread model considers non flammable areas such as the wasteland around houses and forest shown as the white areas in fig 1 therefore the model mentioned in 3 2 3 can be applied to deal with fire resistant elements fig 8 shows how fire spreads in scenarios with fire resistant elements whether the fire can spread through fire resistant elements depends on the heat release intensity of the burning cell which can be simulated as the influence range using our cellular automaton model 3 3 model design 3 3 1 software components the software platform is developed based on typical browser service architecture the backend services such as the heterogeneous cellular automata model is developed in java programming language while the user interface for request response interactions and results visualization is developed in javascript the software architecture shown as fig 9 contains the wui fire spread model a data support component a web service interface component and a web visualization component the data support component is responsible for data storage and management in the wui fire simulation such as vegetation types terrain meteorological data etc the wui fire spread model is the kernel component of the software and simulates the fire spread process in complex wui scenarios the service interface component is used to accept fire simulation requests from users parse key simulation parameters and activate the wui fire spread model after the model simulation is completed the service interface component packs up the result and send it back to users as a response the web visualization component is developed for visualizing simulation results and providing support for emergency commanders to analyze fire risks and make rescue decisions 3 3 2 model diagram fig 10 elaborates the algorithm flow of our heterogeneous cellular automata model for wui fire spread first the model is activated by a user request it extracts required simulation parameters from the request including simulation time length t e n d ignition coordinates i c and meteorological data the model then checks whether i c locates within fire resistant elements based on its vegetation type if true the simulation stops with error messages otherwise the initial ignition area is flammable and set secondly if i c locates in forest areas the status of the ignition cell is updated from sf0 to sf1 as only when the forest cell is in sf2 status can spread outside we further turn it to sf2 and set δ t 1 as the initial value of t t o t a l if i c locates in urban areas the status of ignition cell is updated to su3 and the initial value of t t o t a l equals to the sum of t 12 and t 23 the house fire algorithm requires an extra parameter t u p d a t e to control the frequency that calculates the probability of unburned houses or forest get ignited the house fire spread algorithm is activated when the cumulative time of the house fire t c o u n t t u p d a t e the time of forest fire model and house fire model should be unified to be of the same dimension in the wui fire spread scenario the time of forest fire model is preferentially set as time base since δ t is far less than t u p d a t e detailed fire spread rules and status transformations are described in 3 2 in the end when t t o t a l t e n d the simulation is completed with the results of sequential fire data are returned to the user 4 experiments to demonstrate the practicality and reliability of our model we simulated a real wui fire event called getty fire which occurred in california us on october 28 2019 and compared its performance with flammap6 a widely used forest fire simulation software 4 1 experimental area getty fire broke out at 01 34 00 on october 28th 2019 it first spread in forest areas and then affected urban areas the detailed fire statistics can be found on the website of los angeles fire department lafd 2020 fig 11 shows our experimental area 4 2 experimental data we use the same experimental data to simulate getty fire through our model and flammap6 respectively flammap6 is a fire analysis software developed by u s forest service usfs it integrates two forest fire simulation models flammap and farsite usfs 2020a b c with the advantage of simulating wildfire growth and behavior with detailed meteorological data farsite is widely used by u s federal and state land management agencies like u s forest service national park service and so on usfs 2020 in this study we chose farsite as the comparative fire spread model the inputs of farsite include data related to fuels and wind field generated by windninja usfs 2020 it also requires a special landscape file consisting of terrain fire behavior fuel models forest canopy cover forest canopy height forest canopy base height and forest canopy bulk density comparing to farsite the inputs required by our model are relatively simple including vegetation type terrain meteorological data and house data the required data are listed in table 13 landfire is a shared program between usfs and u s department of the interior usdi aiming to provide landscape scale geospatial products to support cross boundary planning management and operations landfire 2020 most of the data required for the experiments can be found in landfire as shown in table 14 the meteorological data were obtained through windninja and mesowest utah 2020 windninja can automatically search and download the meteorological data at nearest station kvny based on the landscape file then we used mesowest to compensate for the lack of relative humidity data in windninja fig 12 shows the meteorological data after the fire broke out from it we can see that the diurnal variation of temperature was between 10 c and 25 c with a high temperature zone occurred from the 12th hour to the 17th hour the relative humidity was lower in the first 18 h and then increased rapidly above 35 the wind speed was higher in the first 12 h and then gradually decreased the wind direction ranged 0 30 in the first 8 h 330 360 in the following 10 h and 240 360 or 0 60 in the last 13 h from these meteorological parameters we conclude that the fire spreads rapidly in the first 15 h and the wind lead the fire to the south of the ignition the house and road vector data come from an open source database openstreetmap 2020 because openstreetmap provides real time updates it may cause minor inconsistency with vegetation type data increasing the deviation between simulation and reality fig 13 shows the vector data downloaded from openstreetmap most houses on the map are single and wooden the fire ignition location and fire perimeter data were obtained from geomac which is an online wildfire map application developed by the united states geological survey usgs and aims to provide locations and boundaries of america historical fires usgs 2020 as shown in fig 14 the fire ignition located at 34 5 47 n 118 28 53 w and the fire perimeter were updated three times october 29th october 30th and november 2nd the minor differences in fire perimeters at different times indicates that the fire was almost completely controlled on or before october 29th in this case we set the final record of the fire perimeter on november 2nd as the reference fire perimeter both the experimental data and simulation results were uniformly projected to the same coordinate system resulting in a slight geospatial inconsistency of burned acres between the original and projected data lafd reported that getty fire had spread 618 acres in the first 13 h accounting for 83 of final burned area the fire forces almost constrained the fire at 20 00 on october 28th then the fire gradually extinguished with little change in the fire perimeter to fully capture the whole fire spread process we set the simulation time to 20 h from 1 34 to 21 34 we found that the reference fire perimeter was about the same as when the fire was almost under control which means that a valid simulation should result in a fire perimeter similar with the reference fire perimeter around 13 h after the fire broke out 4 3 results and discussion we simulated the same grey fire event separately using our model and farsite the simulation results and time consumption were compared to assess the performance and accuracy of our model all the comparative experiments adopted the same computational architecture a 4 core 2 30 ghz inter r core tm i7 10510u laptop with 16 gb of ram table 15 lists the cpu time consumed for different simulation durations when using our model and farsite respectively from it we can see that our model runs much faster than farsite it only takes about 4 s even when the simulation duration is 24 h for each simulation duration farsite consumes nearly 20 times as long as our model long range fire spotting modelling is one of the advantages of farsite however fire spotting is nonlinear and stochastic albini 1979 fernandez pello 2017 kaur and pagnini 2016 trucchia et al 2019 the difference between simulated burning area and observations increased rapidly when farsite integrated its fire spotting algorithm moreover there is no fire spotting record available for grey fire overall neither farsite nor our model incorporates fire spotting algorithm in this study since flammap6 only supports simulations on the hour instead of simulating from 1 34 to 21 34 we averaged the simulation results of two separate experiments 1 00 21 00 farsite1 and 2 00 22 00 farsite2 fig 15 a shows the fire perimeter changes within 600 min after the simulation started from fig 15 b we can see that the fire perimeter of farsite1 and farsite2 are almost the same fig 16 shows the simulation results of our model the simulation time is from 1 34 to 21 34 as shown in fig 16 b the burned area simulated from farsite and our model both increases linearly while the lafd observed burned area increases rapidly from 120 to 240 min then grows slowly until it stops around 800 min the simulated burned area of both models exceeds lafd observations around 600 min the area difference between simulation results and observations may be caused by several reasons first the meteorological data in getty fire area may be inconsistent with the meteorological station kvny which is 12 km away from the fire in particular the difference in wind direction may be the reason for the significant difference between simulated results and observations in the northern area secondly the dynamical change of vegetation types leads to the uncertainty of fuel distribution and affects simulation results third the observed fire perimeters from geomac were recorded through gps measurement and remote sensing imagery interpretation which can be greatly affected by gps accuracy image resolution and image quality affected by clouds dust fog wind and bad weather condition moreover there were 1165 firefighters 170 fire engines and 11 helicopters in total taking part in the fire rescue in getty fire significantly slowing the fire spread process however we didn t include fire rescue forces in both our current model and farsite overall our model can generate fire perimeter results similar to geomac records with much less time than farsite another advantage of our model over farsite is that it can simulate fire spread in wui scenarios rather than just in forest areas fig 17 shows the fire spread process simulated by our model we found that getty fire was greatly affected by north and northeast wind and spread fast in the south area after 5 h the fire reached the wui area and ignited several houses fig 17 k shows the statistics of burned houses simulated by our model and from lafd records in the first 4 h the fire only spread in the forest area our model results show that the fire began to spread in the wui area and ignite some houses after 4 h lafd records show that at least five houses were burned in around 420 min while our model predicted that at least 8 houses were ignited during the same period after 480 min the difference in the number of burned houses between our model and geomac records became bigger and our model predicted more houses get burned the main reason for the difference is the fire rescue forces as mentioned before lafd reported that as least 470 firefighters had participated in the fire rescue within 171 min after the fire broke out however the strong wind in the early period made it difficult to contain the fire even the number of firefighters increased to more than 600 lafd 2020 therefore some houses still got burned with the help of fire rescue forces and weather conditions the houses were better protected and only 25 houses were ignited eventually since we didn t consider fire rescue forces in our model the simulated number of burned houses was more than geomac records 5 conclusions this paper proposes a fire spread heterogeneous cellular automata model and elaborates it in different large scale scenarios including forest areas urban areas and wui areas a software platform is developed to simulate the fire spread process under different climatic and territorial variations a typical wui fire event occurred in getty california is simulated with our model and farsite respectively the simulation results and time consumption of the two models are compared with observed fire data from geomac the comparison results show that our model can generate fire spread predictions with acceptable accuracy short running time and easily accessible data providing better support to emergency decision making for large scale fire spread in wui areas our future work will consider other core factors that impact the fire spread process the first factor is the long range fire spotting phenomenon fire spotting is nonlinear and stochastic it is still challenging to use current existing fire spotting algorithms to get fire spread predictions with acceptable accuracy and efficiency in large scale scenarios the second factor is fire resistant elements such as manual rescue forces and natural fire resistant elements fire resistant elements become more and more non negligible in disaster prevention and mitigation it s necessary to include them in fire spread modelling quantitatively for better fire control and protection the last factor is the reliability of empirical models empirical models may not be able to produce promising simulation results in various wui scenarios more fire events need to be recorded and fully utilized through data mining method such as machine learning to optimize empirical models and improve fire spreading prediction furthermore we have provided here only one case study to serve as a proof of reliability for using our model in simulating complex wui fire scenarios more cases are needed to fully validate our model declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this paper is supported by the science technology and innovation commission of shenzhen city china grant number jcyj20180508152055235 the ministry of science and technology of china grant number 2016yfc0803107 and the science and technology department of guangdong province china grant number 2019b111104001 appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104895 
25912,there are limitations to traditional visualization solutions regarding real time 3d visualization of time varying and large volume 3d gridded oceanographic data in a web environment we adopted the open source visualization technologies to implement a browser based 3d visualization framework the developed 3d visualization interfaces provide users 3dgis experiences on a virtual globe and simultaneously provide efficient 3d volume rendering and enriched interactive volume analysis our experiments suggest that the well designed cesium and plotly js api allow researchers to easily establish 3d visualization applications while avoiding the requirements of intensive programming and computations the case study conducted shows that the proposed methods is a feasible alternative web based 3d visualization solution which provides a faster rendering speed high visual effects and on the fly 3d visualization of oceanographic data due to its open source architecture and the simplicity of the adopted technologies the visualization framework can be easily customized to visualize other scientific data with few modifications keywords 3d visualization webgl volume rendering cesium virtual globe plotly 1 introduction to understand the complex characteristics and actual state of an ocean environment the physical state observations and accurate ocean forecasting systems are required during the past few decades with the rapid advancement of numerical modeling methods and computational power the oceanographic community has widely adopted ocean forecasting systems to provide short term simulations and predictions of the characteristics of a water mass such as the sea temperature salinity surface elevations and currents ellsworth et al 2017 oliveira et al 2020 from an application perspective the results of an ocean forecasting system need to be easily understood however ocean simulation results are typically time varying multi variable multidimensional and large in volume thereby critical challenges for oceanographers and stakeholders to effectively understand and utilize such datasets graphical representations of complicated datasets on 2d 3d displays provide insightful ways to acquire qualitative understanding of the data billen et al 2008 brooks and whalley 2008 djurcilov et al 2002 li et al 2011 therefore with an increasing use of ocean forecasting services an effective and efficient visualization framework is becoming an extremely important aspect of an ocean forecasting system and is playing an increasingly important role in a better expression and understanding of forecasts aiding the exploration and decision making in actual operational fields höllt et al 2014 visualization contributes to the extraction and identification of the necessary information from large volumes of spatial temporal data which has been became a popular topic in various fields numerous studies have highlighted the value of visualization in environmental science geology meteorology and hydrology blower et al 2013 cox et al 2013 criollo et al 2019 hunter et al 2016 li and wang 2017 liang et al 2014 mcdonald et al 2019 mao et al 2019 rajib et al 2016 sitterson et al 2020 sun et al 2019 2020 wang et al 2013 zhang et al 2019b similarly many efforts are also being made within the field of oceanographic science in this study we focus on the 3d visualization of simulation results in a web environment and for detailed reviews of numerous ocean data visualization techniques readers can refer to a study by xie et al 2019 ocean forecasting systems are generally implemented in a three dimensional 3d computational space and their outputs are typically 3d gridded geospatial data hence data visualization and analysis inevitably require a 3d or stereoscopic display environment therefore numerous previous studies have focused on investigating 3d visualization techniques and algorithms there are some freely available 3d geoscience visualization and analysis toolkits that give users the ability to view and analyze geoscience data in 2d 2 5d and 3d graphs fashion such as voreen meyer spradow et al 2009 vapor li et al 2019 paraview ahrens et al 2005 met 3d rautenhaus et al 2015 and integrated data viewer idv unidata 2015 these visualization packages were traditionally developed based on opengl graphic library using c and java however these existing visualization tools have been commonly developed as standalone software programs which involve practical limitations in terms of their sharing capabilities and collaborative scientiﬁc analysis li and wang 2017 liu et al 2015b users tend to access process and apply real time data visualization in a web environment to address these trends considerable efforts have been devoted to the coupling of gis with web based visualization technologies for the development of webgis applications that can provide online mapping services and interactive tools for the dissemination visualization and analysis of spatial datasets however most conventional webgis solutions typically use two dimensional 2d thematic maps to illustrate the spatiotemporal distribution thus lacking the capability to analyze and display marine environmental factors in a stereoscopic environment su et al 2016 the recent development of the web graphics library webgl technique has provided an option to expand the capacity of interactive 3d visualization in a web browser because of the features of plug in free cross web browsers chrome firefox internet explorer and gpu programming rendering webgl is thus eminently suitable for solving the 3d visualization problem in a web environment hunter et al 2016 liu et al 2019 resch et al 2014 in recent years applications of webgl based 3d visualization have substantially increased in several different fields with the emergence of webgl based virtual globe technology a new way to display geospatial data in a 3d virtual globe environment has become available liu et al 2019 su et al 2016 zhang et al 2019a virtual globe based 3d visualization e g google earth nasa world wind and cesium offers researchers a simpler alternative to traditional gis software which allows researchers to interact with geospatial data in a true 3d virtual environment and has been identified as an ideal visualization environment by integrating global geospatial data goodchild et al 2012 liu et al 2015b wang et al 2013 yu and gong 2012 zhu et al 2019 although a virtual globe is ideal for representing terrestrial earth surface features turk et al 2010 such as meteorological data there is an essential drawback regarding the lack of a subsurface visualization ability in oceanographic data visualization the direct rendering certain of the phenomenon or internal characteristics of the oceans on the surface of a globe is unsuitable volume rendering technology has been adopted to address this limitation volume rendering is a visualization method that can be used to explore the inner structure of 3d objects and is capable of displaying the full distribution of 3d scalar data concurrently liu et al 2015b volume rendering technology has been at the forefront of research in the field of visualization and has been extensively studied in various domains despite some valuable studies conducted over the last few years on oceanographic data volume rendering du et al 2015 liu et al 2017 iwasaki et al 2003 zhang et al 2019a there is still a need to continuously improve the visualization techniques for a better rendering and analysis of such oceanographic datasets in a web environment the challenges in visualization and interactive analysis of large volume and time varying 3d gridded oceanographic datasets in a web environment have motivated our design and implementation of a novel browser based 3d visualization framework for ocean simulation outputs this visualization framework extends the virtual globe using an embedded volume rendering engine which provides both surface and subsurface visualization of a water mass with different display modes the efforts involved in this study are mainly focused on the development and implementation of an interactive and user friendly web based 3d visualization interface through open source technologies after studying the capabilities of state of the art open source visualization technologies cesium virtual globe plotly heatmap and moving particle technologies were adopted in the development of web application herein we also demonstrate the proposed visualization framework through a case study it should be noted that this visualization framework requires high density and regular 3d gridded data for accurate and better visualization in a browser supporting webgl therefore any ocean forecasting system that generates a 3d structured grid model outputs stored in a netcdf network common data form format is suitable for this visualization framework the remainder of the paper is organized as follows section 2 describes the methods adopted for the development and implementation of the visualization framework in detail section 3 presents the results of the web based 3d visualization applications section 4 discusses the proposed visualization framework based on the results finally section 5 summarizes the conclusion 2 methods 2 1 the architecture of visualization framework a conceptual architecture of the visualization framework is visually illustrated in fig 1 like most web application solutions the presented visualization framework is a server client system organized in a three layer architecture namely a client side application layer a server side logical layer and a data source layer the functions of which are described below 1 client side application layer this is a rich web interface that includes the graphical user interface gui and the main 3d rendering canvas offering a user interaction for data access interactive visualization and analysis the front end interfaces were developed under the lightweight progressive vue js framework using html5 javascript and cascade style sheet css in a visual studio code environment the webgl based cesium virtual globe ploy heatmap and echarts libraries were integrated into the vue js framework to create a 3d virtual globe heatmaps moving particles and volume rendering on html5 canvas 2 server side logical layer a web server along with a database server and a data processing tool constitute the integrated server side components a the web server is a middleware layer that hosts all specially developed web services supplying system functionalities these back end web services interact with the database server for data discovery processing access and publication which were written in the event driven based runtime environment node js apache http server was deployed on the web server for handling the http requests and responses b a database server is used to store both the raw and processed data collected from the ocean forecasting system gauge stations and online geospatial data sources in our case the datasets obtained are heterogeneous and therefore the storage strategies also differ database server runs mysql open source dbms to manage observations according to a schema that allows searching and retrieval based on several fine grained criteria other datasets are stored directly in the filesystem c on the server side instead of processing raw data on demand simulation results are preprocessed to improve the overall response time a netcdf2json data processing tool provides the capability to automatically parse the outputs based on the metadata and then restructure the grid values into a compatible format so that they can be retrieved and loaded onto a web page for further rendering and computation the netcdf2json tool was coded in the c based on netcdf api using the microsoft visual platform 3 data source layer data involved in this visualization framework including ocean observational and forecasting data and online public gis database the autonomous sensors mounted on ocean observational systems collect in situ data with specified intervals and transmit the sensor data to the data server the ocean forecasting systems are achieved through a high performance computing system hpc which allow the forecasting systems to produce spatiotemporally high resolution forecasts in particular a scheduler fetching subsystem initiates the fetching scripts daily for pushing the ocean forecasts to a specified workspace in the data server archive through ftp besides the base maps e g satellite imagery used in this visualization framework can be obtained from the open geospatial consortium s web map service wms such as google maps esri arcgis online or openstreetmap 2 2 pre processing of model outputs the simulation results can generally be saved in the common data format netcdf network common data form which can efficiently store and manage multidimensional grid data because javascript has no capacity to directly interact with a netcdf file a portion of the work presented in this study involves an effort to develop a data conversion tool netcdf2json that makes the simulation results more readable and writable in web friendly formats before being loaded and displayed on a web interface in this study json javascript object notation a standard format of a javascript exchange file commonly used on the internet was adopted for storing the processed data a typical netcdf based model output is grid based multi variable and multidimensional to improve the efficiency of the data transmission over the web and apply data rendering in a client side web interface high dimensional netcdf data are divided into a series of smaller json subsets each json file was created corresponds to the grid values at time steps with a single depth the netcdf and the generated json based files can be described as d v 1 v 2 v m v t d v 1 v 1 i j v 2 v 2 i j v m v m i j and pos x x ij y y ij where d indicates a netcdf dataset including m variables and v t d defines a collection of values for m variables in one time and depth dimension denoting the model prediction value at the t th time step of the d th depth at each grid point in particular the missing or invalid data are flagged given a 9999 value and formatted into the data structure furthermore to minimize the number of data calculations on the web client side the tool pre calculates the vector magnitude λ and direction θ according the u and v values after the computations are finished they are then deposited into v t d data storage in addition pos denotes the latitude and longitude coordinates of the model grid this simplistic data structure and organized approach effectively facilitates the query loading and rendering of volumetric datasets when client side requests to load a dataset the server only needs sequential file reads and a query of the neighborhood relationships between grids this increases the i o performance when reading the files from a disk because it minimizes the query and allows prefetching and therefore the client can obtain the data from the server in much less time and contribute to an improved rendering speed and efficiency on the web client side the tool was designed as an event driven multi tasking and multi threaded application which means that the processing workflow can be concurrently invoked to process multiple files simultaneously qin and lin 2017 the data conversion workflow does not involve a user interaction with an automated discovery parsing and restructuring of the simulation results and thus significantly reduces the time spent by users on frequently repeated data processing each day using a manual method upon fetching new outputs from the ocean forecasting system the tool automatically detects the newly available netcdf files and formulates a scheduler to conduct a format conversion accordingly the information regarding the current status of the multiple data processing steps is indicated through progress bars and label tips the metadata of the data processing are logged in the log files as an extensible markup language xml file 2 3 ocean forecasts visualization on virtual globe two recent studies have already evaluated both the advantages and drawbacks of the currently available virtual globes compared to other virtual globes studies have demonstrated that cesium is the optimum approach for implementing a 3d visualization on a virtual globe hunter et al 2016 müller et al 2016 the virtual globe rendering engine cesium is an open source javascript library for interactive visualization of 3d globes and 2d maps in a web browser as a web based 3d virtual globe cesium uses webgl for hardware accelerated graphics which provides fast and scalable cross platform cross browser and plugin free 3d rendering functionality furthermore cesium uses geographic coordinate latitude longitude on the world geodetic system of 1984 wgs84 datum that provides a wgs84 spatial reference for data visualization and thus multi source geospatial data can be easily integrated on the sphere moreover cesium also offers fundamental gis functionalities that can be easily incorporated in web interfaces li and wang 2017 hunter et al 2016 müller et al 2016 in this study the cesium version 1 56 https cesium com cesiumjs is adopted in the development of a fully customizable 3dgis interface as shown in fig 2 traditionally the dissemination of geospatial data with online map services initially requires the creation of gis layers i e raster or vector layers and relies on the map servers to render the gis layers in web friendly formats e g png kml gml and geojson finally publishing the data as standardized web services wms services for display in the web application s map view swain et al 2016 however this popular solution for map publishing is a tiered and somewhat inflexible client server architecture epitropou et al 2016 and presents numerous difficulties and an ineffective implementation in publishing time varying and large volume datasets in this study we present a new solution for geospatial data rendering in a web application independent of map servers and map services as an alternative solution dynamic heatmap and moving particle technologies have been adopted to display scalar and vector data on the cesium virtual globe respectively a heatmap is a graphical representation of two dimensional data latitude longitude and value where individual values are illustrated with a discrete number of color levels heatmap has been widely used as a common visualization method in many scientific fields epitropou et al 2016 fernandez et al 2017 kalo et al 2020 kumatani et al 2016 liu et al 2015a moumtzidou et al 2014 in this study heatmap js was used to render scalar data as heatmap images overlaying on the cesium virtual globe heatmap js is an open source lightweight simple and fast solution for heatmaps generation and is available from https www patrick wied at static heatmapjs we adopted a value based heatmap approach to display the spatial distribution of scalar data compared with the commonly used density based approach a value based heatmap that ignores the density of the measurements in a given area is more suitable for accurate rendering of scalar data kalo et al 2020 several steps were followed for the construction of value based heatmaps over a geographical domain 1 canvas initialization and data preparation according to the pos json file a regular mesh and boundary of the grid points are calculated and generated to initialize a canvas in a cesium virtual globe for plotting the heatmaps the scalar data are then loaded from the v t d datasets and the maximum and minimum values are calculated for further processing it should be noted that surface islands and seafloor terrains physically contained in a model s computational domain that are not a part of the model s output value labeled with 9999 in the corresponding grid should be treated as uncovered heatmap areas 2 influence calculation the influence of the alpha transparency at the map point can be calculated as a v m a x v v m a x v m i n where v max and v min are the maximum and minimum values of the scalar data and v represents the value of the scalar data a linear gradient circle is then constructed and painted using alpha transparency on the canvas according to the data point coordinates the radius of the linear gradient circle is adjustable and represents the influence radius r 3 mapping the grayscale is superimposed on the overlapping areas owing to the superimposition of the grayscale the more the linear gradient circles are crossed the larger the grayscale which indicates the hotter area the heatmap technology uses the renderer pixel alpha channel overlay rule as the influence overlay mode by default the formula for a general renderer overlay rule is a 0 a 1 a 2 a 1 a 2 255 where a 0 is the alpha value of the superimposed pixel and a 1 and a 2 are the alpha values of the superimposed pixel a heatmap is generated by rendering the pixels of heatmap into different colors according to its superimposed transparency alpha value more specifically the render color ribbon has a total of 0 255 ribbon values the transparency value is multiplied by 255 to get the color value corresponding to the transparency simultaneously a legend is created to show the gradient colors indicating the correspondence between the grid value and color on the map 4 enhancement of the rendering effects to create a smooth and continuous visualization heatmap circles should overlay in the most efficient way possible kalo et al 2020 we need to adjust the radius of the circle through comparative testing and validation of the alternatives to find an optimal value for blending the visualization of the circular heatmap elements vector data such as the current speed and direction can be visualized on the virtual globe in the form of static arrow symbols and dynamic moving particles or streamlines compared with static symbolized markers moving particles or streamlines can efficiently illustrate the instantaneous current characteristics and elucidate the laws of current movements this study adopted the moving particle approach to vividly visualize the currents although some similar visualization applications are available such as earth nullschool https earth nullschool net current wind surface level and windy https www windy com none of them appear to be open source and thus they cannot be expanded and or adapted for further application by third parties to create a moving particle engine in the cesium virtual globe a regular high resolution mesh covering the study area according to user defined columns and rows needs to be constructed next a bilinear interpolation is applied to calculate the direction and magnitude of the current vector at point based on the v t d datasets and the values are then assigned as the attribute of that particular mesh node finally once the interpolated surface has been generated a function randomly places particles onto the canvas at random points each particle moves in a direction and at a velocity dictated by the interpolated surface rayman 2019 following this visualization method we simulate the continuous movement of the particles based on fading existing particle trails and drawing new particle trails which stretch and move within the space to convey the dynamic flow process of the currents to achieve a better display effect the density of the moving particles is automatically adjusted at different levels using an appropriate map view corresponding to the scale of the map in addition the web application provides a toolbox containing a set of both common e g zoom in out pan rotation and flying and customized toolsets e g location bookmarks graphic tools distance area measure and on the fly mouse moving for value picking moreover for a better understanding of the temporal and spatial changes of the ocean parameters the interface provides an animation tool that can be easily used to conduct an animated visualization users can interactively slide the time steps with a time slider and use a play pause button to view the heatmaps or particles in a time lapse animation visualization these additional tools and functions make the presented web application more versatile in terms of 3dgis functions than a pure rendering engine in addition to providing mechanisms of scalar and vector displays we also developed plotting tools for a time series analysis the plotting capabilities are implemented using the open source javascript based echarts 3 0 http echarts baidu com interactive charting and visualization library when clicking on the heatmaps at any location on the virtual globe the associated time series of the predictions are extracted on demand from the server side and a time series is displayed through line graphs within a pop up window as shown in fig 3 b the information derived from these plots provides supply information on the identification of the main temporal trends and patterns of the ocean parameters which is highly useful in identifying the characteristics of the predictions the plotting tool also provides the option of exporting the time series as a comma separated value csv file in addition it allows plotting interaction such as a tooltip display of values as well as the ability to zoom in to specific sections of the charts moreover we also integrated the observations collected by gauge stations into this visualization framework the interface displays the gauge stations spatially overlaid onto the virtual globe and retrieves and displays those latest observations and metadata the plotting tool also provides the ability to automatically compare simulated values with in situ sensor data for comparison the plotting tool is initiated to extract the time series predictions from the heatmaps and load observations from the server side mysql database if sensor data are available for the same time period data are added to the charts for model validation this function provides users a convenient way to validate and diagnose the simulations against real observations for an evaluation of the prediction accuracy 2 4 volume rendering of ocean forecasts as an extension of the previously presented 3d virtual globe application we developed a 3d cubic volume rendering environment volumeviewer as shown in fig 4 in this study the interactive volumeviewer interface has been customized using plotly js an advanced and freely available graphical javascript library the plotly js is built on top of d3 js and stack gl and is a third party high performance 2d and 3d graphic library https plot ly javascript plotly js renders 3d volumes as stacks of translucent images using texture slicing algorithm and provides numerous interactive 3d visualization functions among the most commonly used volume rendering methods ray casting splatting shear warp and texture slicing as a direct volume rendering technique the texture slicing method is sufficiently fast allowing real time volume rendering to be achieved for large amounts of scalar data owing to the improvements in gpu computing liang et al 2014 beyer et al 2015 ohno and kageyama 2007 however few studies have mentioned the use of plotly technology for conducting 3d volume rendering of oceanographic datasets the main process of 3d volume rendering using plotly js can apply the following steps 1 to set up the rendering environment a basic 3d cube needs to first be constructed and some basic rendering properties need to be defined the 3d cube determines the x y and z coordinates and grid lines according to the latitude longitude and depth of aoi area of interest notably the simulation results generally cover large geographic areas where the vertical axis is generally lower in altitudinal range than the other axes it is therefore necessary to exaggerate the depth vertically to make the visualization clearer 2 considering the fact that certain ocean phenomena and processes can be better expressed and understood based on the high resolution topography and bathymetry of deep oceans liu et al 2017 it is necessary to incorporate the topography of aoi into a basic 3d cube early rendering any spatially referenced scalar or vector data over the topography 3 the client side requests grid datasets from the remote server and organizes them into objects ready to be plotted using the plotly js library in particular the datasets are cached on the client side web browser and rendered as a whole this reduces the rendering batches and allows the user to change the rendering properties on the fly e g color scheme and opacity without re requesting the data from the server side which can further increase the smoothness visual effects and the efficiency and interactivity next the plotting functions provided by plotly js are utilized to apply data rendering in the form of a 3d volume 3d slice and 3d vector the actual visualization codes are relatively simple compared with previous visualization development kits they focus solely on the definitions required to describe the internal data structure for the rendering functions and these functions read the grid structures and data values and render them in different graphics volumetric data usually refer to a 3d distribution of scalar data which are passed in an isosurface manner i e x y z and u value arrays of the coordinates and intensity each array has the same length the x y and z arrays are used to construct a mesh grid that determines the shape of the u array the plotly js volume lib renders a 3d volume as stacks of translucent textures with vertically stacked layer images thus the functions of 3d volume plots are used to draw a volume trace between the iso min and iso max values with coordinates given by four one dimensional arrays containing the value x y and z of every vertex of a uniform or non uniform 3 d grid and to show the scalar data represented as colored cubes polity js volume traces 2753 2018 similar to heatmap visualization volume rendering is also applied on a per time step basis ensuring that individual layers can be interactively displayed or hidden by selecting a specified time dimension step furthermore because spatial and temporal volumetric data are difficult to analyze through the use of a single type of visual representation we also provide a volume slice visualization and cross sectional analysis to assist with the scientific interpretation of oceanic forecasting data in this customized 3d volume environment volume slices are used to visualize 3d volumetric data by displaying one or more profiles that can inspect the data characteristics within the volumetric data and provide more insights into the 3d structure and its evolution the volumeviewer utilizes 3d surface plots to generate slices by cutting a plane section from the rendered volumetric data through operations using the slicing tool users are able to interactively generate vertical and horizontal section slices by selecting a location along the horizontal latitude or longitude direction and vertical depth direction axes because all rendered volumetric data are stored in memory and are locally cached on the client side a significant increase in the efficiency of the computations and the loading speed of a section slice location is achieved and all cross sections can be dynamically generated and rendered on the fly 3d cone plots are used to express the orientation and magnitude of ocean currents in the form of 3d directional arrows in the volumeviewer all 3d volumetric data 3d cross sections and 3d scatter plots arrows of various variables and 2 5d terrain surfaces can be simultaneously displayed in the same scene such features enable the identification of unknown processes and insights from the complex spatial relationships between various variables that may have a close underlying correlation with each other in addition the volumeviewer also provides a data ﬁltering function for displaying partial data based on the interest value this capability facilitates a dynamic visualization of the regions of interest of the volumetric data in this way the visualization is particularly useful when a user intends to display or highlight the desired features similarly users can interactively set the opacity gradient factor of the volume rendering to express the inner characteristics when setting the opacity of the surface it should be noted that when using high opacity values an overlay of multiple transparent surfaces may not be perfectly sorted in depth by the webgl api plotly js volume traces 2753 2018 further the volumeviewer can apply customized interactions including a click and drag motion to zoom into a small region of the rendered volumetric data which allows investigating the inner features in more detail moreover as the mouse pointer moving over the graphics a value picking tool enables querying the data values and spatial position any locations within the dataset s domain with real time feedback finally similar to common gis operations the volumeviewer implements various interactive functions to enable intuitive data exploration including zoom in out pan rotate and clearing or re rendering the scene 3 results to demonstrate the proposed approaches and evaluate the applicability and efficiency of the developed visualization framework a case study was presented in this section the experiments were performed in the chrome browser on a client side computer with a 3 40 ghz 8 core intel core i7 4770 cpu 16 gb of ram and an nvidia geforce gtx 760 graphics card 3 1 data description in this case study we illustrated this presented visualization framework using real world simulation results that were provided by the operational ocean forecasting system running on hpc at the south china sea institute of oceanology scsio the operational ocean forecasting system is a 3 d structured grid finite volume numerical model for providing 5 days forecasts of sea temperature salinity currents u and v and water level the model covers an area of the south china sea between 18 n and 20 n and 118 e 120 e and generates outputs with 1 60 horizontal resolutions 101 depth levels and 1 h time intervals the output forms a 120 120 101 120 174 528 000 data of each variable and constitutes about 4 gigabyte floating point data in addition base maps high resolution satellite imagery overlaid on the virtual globe were obtained from google maps and a high resolution seabed topography with 15 arc second intervals rendered in the volumeviewer were downloaded from bodc british oceanographic data centre database https www bodc ac uk data hosted data systems gebco gridded bathymetry data observational data obtained from the buoys were used to validate the simulation results and calculate the model errors 3 2 visualization application results launching the web application the interface displays the cesium virtual globe incorporated with several menus and toolsets as shown in fig 2 fig 3 shows an example visualization of heatmaps salinity and temperature distribution and moving particles currents on a cesium virtual globe fig 4 shows the main interface of the volume rendering application fig 5 shows a comprehensive display of sea temperature salinity and currents with time stamp march 27 2020 09 00 gmt through different visualization modes 4 discussion the above experiments demonstrated that the use of a value based heatmap and moving particle technologies integrated with a cesium virtual globe is an informative approach for fast and interactive visualization of scalar and vector data the adopted technologies can effectively display time varying multi variable multidimensional and large volume geospatial datasets in a web browser avoiding the need for complicated and time consuming geo processing tasks on the server side such as the conversion and organization of data in the gis layers the creation of thematic maps the deployment of a gis sever and publishing map services and the management and updating of map services at the same time it can fully take advantage of additional gis functionalities particularly on the fly 3d spatial interactivities and analytics provided by cesium virtual globe in addition the automated workflow technique in the visualization framework will simplify the daily activities of tedious model outputs processing tasks moreover as noted previously a visual exploration of a dataset to identify features or processes of scientific interest requires an interactive environment and real time visualizations that can be modified on the fly billen et al 2008 the presented experiments demonstrated that the visualization framework provides numerous high performance interactive functionalities that facilitate users to interact with data and derive insightful processes from large volumetric datasets more specifically the rendering speed is considered a typical challenge when visualizing large volumes of oceanographic data in a web application the case study showed that the display speed in the rendering of 3d volumetric scalar data over millions of data points can satisfy the user requirements likewise the newly created cross sections can be viewed immediately similarly there is no evident time delay when playing an animation of the heatmaps and moving particles in cesium virtual globe furthermore this scenario also demonstrated that the cesium virtual globe and the volumeviewer enable users to achieve a fast interaction such as the rotation zooming and panning of volumetric data these experiments suggest that the presented visualization framework can be used to implement dynamic heatmaps moving particles and volume rendering of massive ocean data with an immediate interactive response using popular graphic cards on the web client side in particular plotly js has been proven to be an efficient graphic library for achieving a real time 3d visualization and visual analysis of large amounts of volumetric data being completely independent of a high performance computer to obtain an acceptable performance however most previous investigations in this field have indicated that 3d visualization is generally a difficult task that requires considerable programming and is inefficient at displaying large volume and high dimensional data in a web environment by contrast our experiments suggest that the well designed cesium api and plotly js api allow researchers to easily establish efficient and effective 3d visualization applications in a web environment while avoiding the requirements for intensive programming and computations the simplicity of these technologies can potentially enable researchers in various fields to adopt the proposed methods for web based 3d visualization applications the presented easy to use visualization framework may not sufficiently versatile to meet all user requirements and it is not the intention of this study to replace existing standalone visualization toolkits however this study has proven that a visualization framework coupled with webgl based cesium virtual globe and plotly js map engine is a simple and feasible alternative solution for establishing an efficient and effective 3d interactive visualization and analysis environment through online spherical volume rendering and cubic volume rendering modes in particular because all of the technologies involved in this study are open source or free frameworks and none of the software or plug ins need to be installed on a client side computer it enables the visualization framework to be easily customized and or adapted to other applications by interested parties in summary compared with other freely available 3d geoscience visualization toolkits mentioned in the introduction the visualization framework demonstrated in this study has the following advantages firstly it was designed as a browser server tiered architecture and therefore the software applications are hosted by service providers and made available to users via the internet and there is nothing software and plug ins needs to be downloaded and installed what is more certain of these freely available visualization toolkits generally do not display data within a geo referenced environment lacking the ability to integrate various geospatial data sources liang et al 2014 by contrast the presented visualization framework provides users with an interactive 3dgis experience in the cesium virtual globe while at the same time offering efficient volume rendering and enriched interactive volume analysis in the volumeviewer in addition previous studies have demonstrated that the freely available software realized small scale data visualizations in the regional scene zhang et al 2019b while the developed visualization software can realize real time volume rendering for large scale data on the web client side in particular the developed visualization interfaces is user friendly with simplified operations while certain of the freely available 3d visualization toolkits are designed for general purposes and difficult to learn and use need additional and time consuming processing steps for the presentation of 3d maps finally due to its open source architecture and the simplicity of these adopted technologies the proposed visualization framework can be easily expanded and or adapted for visualizing other scientific data with few modifications despite these advances the visualization framework also remains certain limitations on the one hand it limits its adoption to irregular 3d gridded data besides it is also necessary to extend the ability of supporting many other file formats such as hdf hierarchical data format and grib gridded binary most of all it should enhance the data analysis and 3d dynamic rendering functionalities such as the multiple 3d isosurface analysis and visualization computation and visualization of 3d streamlines and trajectories from flow fields 5 conclusion in this study with the adoption of open source and webgl based cesium virtual globe plotly js heatmap moving particle and echarts visualization technologies we have developed an interactive and user friendly web based 3d visualization application with a simplified implementation the application allows users to view regular 3d grid data in a variety of representations including 1 d 2 d 2 5 d and 3 d static or dynamic visualization the results of a case study demonstrate that the proposed visualization framework provides a faster rendering speed high visual effects and on the fly 3d visualization of 3d gridded oceanic forecasting data along with greater interactivity and explorative functionalities and therefore satisfies the general visualization requirements of oceanic forecasting data enabling the comprehension of complicated oceanographic phenomena although we focused on the visualization and analysis of oceanic forecasting data due to its open source architecture and the simplicity of the adopted technologies we believe that the visualization framework can be easily customized or adapted to visualize other scientific data with few modifications 6 software availability software name the web based 3d visualization system for ocean forecasts version 1 0 year of first available 2019 developers rufu qin bin feng yusheng zhou and zhounan xu contact rufu qin qinrufu tongji edu cn 86 21 65989834 software required web browsers that support webgl programming language html5 javascript node js ms visual studio 2015 net c availability the source code can be accessed via github https github com qinrufu web based 3d visualization of oceanic forecasting data declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work has been funded by the national natural science foundation of china nsfc grant no 61701487 the strategic priority research program of chinese academy of sciences grant no xda13030102 and the youth innovation promotion association cas the authors would like to thank elsevier language editing services for their language modification appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104908 
25912,there are limitations to traditional visualization solutions regarding real time 3d visualization of time varying and large volume 3d gridded oceanographic data in a web environment we adopted the open source visualization technologies to implement a browser based 3d visualization framework the developed 3d visualization interfaces provide users 3dgis experiences on a virtual globe and simultaneously provide efficient 3d volume rendering and enriched interactive volume analysis our experiments suggest that the well designed cesium and plotly js api allow researchers to easily establish 3d visualization applications while avoiding the requirements of intensive programming and computations the case study conducted shows that the proposed methods is a feasible alternative web based 3d visualization solution which provides a faster rendering speed high visual effects and on the fly 3d visualization of oceanographic data due to its open source architecture and the simplicity of the adopted technologies the visualization framework can be easily customized to visualize other scientific data with few modifications keywords 3d visualization webgl volume rendering cesium virtual globe plotly 1 introduction to understand the complex characteristics and actual state of an ocean environment the physical state observations and accurate ocean forecasting systems are required during the past few decades with the rapid advancement of numerical modeling methods and computational power the oceanographic community has widely adopted ocean forecasting systems to provide short term simulations and predictions of the characteristics of a water mass such as the sea temperature salinity surface elevations and currents ellsworth et al 2017 oliveira et al 2020 from an application perspective the results of an ocean forecasting system need to be easily understood however ocean simulation results are typically time varying multi variable multidimensional and large in volume thereby critical challenges for oceanographers and stakeholders to effectively understand and utilize such datasets graphical representations of complicated datasets on 2d 3d displays provide insightful ways to acquire qualitative understanding of the data billen et al 2008 brooks and whalley 2008 djurcilov et al 2002 li et al 2011 therefore with an increasing use of ocean forecasting services an effective and efficient visualization framework is becoming an extremely important aspect of an ocean forecasting system and is playing an increasingly important role in a better expression and understanding of forecasts aiding the exploration and decision making in actual operational fields höllt et al 2014 visualization contributes to the extraction and identification of the necessary information from large volumes of spatial temporal data which has been became a popular topic in various fields numerous studies have highlighted the value of visualization in environmental science geology meteorology and hydrology blower et al 2013 cox et al 2013 criollo et al 2019 hunter et al 2016 li and wang 2017 liang et al 2014 mcdonald et al 2019 mao et al 2019 rajib et al 2016 sitterson et al 2020 sun et al 2019 2020 wang et al 2013 zhang et al 2019b similarly many efforts are also being made within the field of oceanographic science in this study we focus on the 3d visualization of simulation results in a web environment and for detailed reviews of numerous ocean data visualization techniques readers can refer to a study by xie et al 2019 ocean forecasting systems are generally implemented in a three dimensional 3d computational space and their outputs are typically 3d gridded geospatial data hence data visualization and analysis inevitably require a 3d or stereoscopic display environment therefore numerous previous studies have focused on investigating 3d visualization techniques and algorithms there are some freely available 3d geoscience visualization and analysis toolkits that give users the ability to view and analyze geoscience data in 2d 2 5d and 3d graphs fashion such as voreen meyer spradow et al 2009 vapor li et al 2019 paraview ahrens et al 2005 met 3d rautenhaus et al 2015 and integrated data viewer idv unidata 2015 these visualization packages were traditionally developed based on opengl graphic library using c and java however these existing visualization tools have been commonly developed as standalone software programs which involve practical limitations in terms of their sharing capabilities and collaborative scientiﬁc analysis li and wang 2017 liu et al 2015b users tend to access process and apply real time data visualization in a web environment to address these trends considerable efforts have been devoted to the coupling of gis with web based visualization technologies for the development of webgis applications that can provide online mapping services and interactive tools for the dissemination visualization and analysis of spatial datasets however most conventional webgis solutions typically use two dimensional 2d thematic maps to illustrate the spatiotemporal distribution thus lacking the capability to analyze and display marine environmental factors in a stereoscopic environment su et al 2016 the recent development of the web graphics library webgl technique has provided an option to expand the capacity of interactive 3d visualization in a web browser because of the features of plug in free cross web browsers chrome firefox internet explorer and gpu programming rendering webgl is thus eminently suitable for solving the 3d visualization problem in a web environment hunter et al 2016 liu et al 2019 resch et al 2014 in recent years applications of webgl based 3d visualization have substantially increased in several different fields with the emergence of webgl based virtual globe technology a new way to display geospatial data in a 3d virtual globe environment has become available liu et al 2019 su et al 2016 zhang et al 2019a virtual globe based 3d visualization e g google earth nasa world wind and cesium offers researchers a simpler alternative to traditional gis software which allows researchers to interact with geospatial data in a true 3d virtual environment and has been identified as an ideal visualization environment by integrating global geospatial data goodchild et al 2012 liu et al 2015b wang et al 2013 yu and gong 2012 zhu et al 2019 although a virtual globe is ideal for representing terrestrial earth surface features turk et al 2010 such as meteorological data there is an essential drawback regarding the lack of a subsurface visualization ability in oceanographic data visualization the direct rendering certain of the phenomenon or internal characteristics of the oceans on the surface of a globe is unsuitable volume rendering technology has been adopted to address this limitation volume rendering is a visualization method that can be used to explore the inner structure of 3d objects and is capable of displaying the full distribution of 3d scalar data concurrently liu et al 2015b volume rendering technology has been at the forefront of research in the field of visualization and has been extensively studied in various domains despite some valuable studies conducted over the last few years on oceanographic data volume rendering du et al 2015 liu et al 2017 iwasaki et al 2003 zhang et al 2019a there is still a need to continuously improve the visualization techniques for a better rendering and analysis of such oceanographic datasets in a web environment the challenges in visualization and interactive analysis of large volume and time varying 3d gridded oceanographic datasets in a web environment have motivated our design and implementation of a novel browser based 3d visualization framework for ocean simulation outputs this visualization framework extends the virtual globe using an embedded volume rendering engine which provides both surface and subsurface visualization of a water mass with different display modes the efforts involved in this study are mainly focused on the development and implementation of an interactive and user friendly web based 3d visualization interface through open source technologies after studying the capabilities of state of the art open source visualization technologies cesium virtual globe plotly heatmap and moving particle technologies were adopted in the development of web application herein we also demonstrate the proposed visualization framework through a case study it should be noted that this visualization framework requires high density and regular 3d gridded data for accurate and better visualization in a browser supporting webgl therefore any ocean forecasting system that generates a 3d structured grid model outputs stored in a netcdf network common data form format is suitable for this visualization framework the remainder of the paper is organized as follows section 2 describes the methods adopted for the development and implementation of the visualization framework in detail section 3 presents the results of the web based 3d visualization applications section 4 discusses the proposed visualization framework based on the results finally section 5 summarizes the conclusion 2 methods 2 1 the architecture of visualization framework a conceptual architecture of the visualization framework is visually illustrated in fig 1 like most web application solutions the presented visualization framework is a server client system organized in a three layer architecture namely a client side application layer a server side logical layer and a data source layer the functions of which are described below 1 client side application layer this is a rich web interface that includes the graphical user interface gui and the main 3d rendering canvas offering a user interaction for data access interactive visualization and analysis the front end interfaces were developed under the lightweight progressive vue js framework using html5 javascript and cascade style sheet css in a visual studio code environment the webgl based cesium virtual globe ploy heatmap and echarts libraries were integrated into the vue js framework to create a 3d virtual globe heatmaps moving particles and volume rendering on html5 canvas 2 server side logical layer a web server along with a database server and a data processing tool constitute the integrated server side components a the web server is a middleware layer that hosts all specially developed web services supplying system functionalities these back end web services interact with the database server for data discovery processing access and publication which were written in the event driven based runtime environment node js apache http server was deployed on the web server for handling the http requests and responses b a database server is used to store both the raw and processed data collected from the ocean forecasting system gauge stations and online geospatial data sources in our case the datasets obtained are heterogeneous and therefore the storage strategies also differ database server runs mysql open source dbms to manage observations according to a schema that allows searching and retrieval based on several fine grained criteria other datasets are stored directly in the filesystem c on the server side instead of processing raw data on demand simulation results are preprocessed to improve the overall response time a netcdf2json data processing tool provides the capability to automatically parse the outputs based on the metadata and then restructure the grid values into a compatible format so that they can be retrieved and loaded onto a web page for further rendering and computation the netcdf2json tool was coded in the c based on netcdf api using the microsoft visual platform 3 data source layer data involved in this visualization framework including ocean observational and forecasting data and online public gis database the autonomous sensors mounted on ocean observational systems collect in situ data with specified intervals and transmit the sensor data to the data server the ocean forecasting systems are achieved through a high performance computing system hpc which allow the forecasting systems to produce spatiotemporally high resolution forecasts in particular a scheduler fetching subsystem initiates the fetching scripts daily for pushing the ocean forecasts to a specified workspace in the data server archive through ftp besides the base maps e g satellite imagery used in this visualization framework can be obtained from the open geospatial consortium s web map service wms such as google maps esri arcgis online or openstreetmap 2 2 pre processing of model outputs the simulation results can generally be saved in the common data format netcdf network common data form which can efficiently store and manage multidimensional grid data because javascript has no capacity to directly interact with a netcdf file a portion of the work presented in this study involves an effort to develop a data conversion tool netcdf2json that makes the simulation results more readable and writable in web friendly formats before being loaded and displayed on a web interface in this study json javascript object notation a standard format of a javascript exchange file commonly used on the internet was adopted for storing the processed data a typical netcdf based model output is grid based multi variable and multidimensional to improve the efficiency of the data transmission over the web and apply data rendering in a client side web interface high dimensional netcdf data are divided into a series of smaller json subsets each json file was created corresponds to the grid values at time steps with a single depth the netcdf and the generated json based files can be described as d v 1 v 2 v m v t d v 1 v 1 i j v 2 v 2 i j v m v m i j and pos x x ij y y ij where d indicates a netcdf dataset including m variables and v t d defines a collection of values for m variables in one time and depth dimension denoting the model prediction value at the t th time step of the d th depth at each grid point in particular the missing or invalid data are flagged given a 9999 value and formatted into the data structure furthermore to minimize the number of data calculations on the web client side the tool pre calculates the vector magnitude λ and direction θ according the u and v values after the computations are finished they are then deposited into v t d data storage in addition pos denotes the latitude and longitude coordinates of the model grid this simplistic data structure and organized approach effectively facilitates the query loading and rendering of volumetric datasets when client side requests to load a dataset the server only needs sequential file reads and a query of the neighborhood relationships between grids this increases the i o performance when reading the files from a disk because it minimizes the query and allows prefetching and therefore the client can obtain the data from the server in much less time and contribute to an improved rendering speed and efficiency on the web client side the tool was designed as an event driven multi tasking and multi threaded application which means that the processing workflow can be concurrently invoked to process multiple files simultaneously qin and lin 2017 the data conversion workflow does not involve a user interaction with an automated discovery parsing and restructuring of the simulation results and thus significantly reduces the time spent by users on frequently repeated data processing each day using a manual method upon fetching new outputs from the ocean forecasting system the tool automatically detects the newly available netcdf files and formulates a scheduler to conduct a format conversion accordingly the information regarding the current status of the multiple data processing steps is indicated through progress bars and label tips the metadata of the data processing are logged in the log files as an extensible markup language xml file 2 3 ocean forecasts visualization on virtual globe two recent studies have already evaluated both the advantages and drawbacks of the currently available virtual globes compared to other virtual globes studies have demonstrated that cesium is the optimum approach for implementing a 3d visualization on a virtual globe hunter et al 2016 müller et al 2016 the virtual globe rendering engine cesium is an open source javascript library for interactive visualization of 3d globes and 2d maps in a web browser as a web based 3d virtual globe cesium uses webgl for hardware accelerated graphics which provides fast and scalable cross platform cross browser and plugin free 3d rendering functionality furthermore cesium uses geographic coordinate latitude longitude on the world geodetic system of 1984 wgs84 datum that provides a wgs84 spatial reference for data visualization and thus multi source geospatial data can be easily integrated on the sphere moreover cesium also offers fundamental gis functionalities that can be easily incorporated in web interfaces li and wang 2017 hunter et al 2016 müller et al 2016 in this study the cesium version 1 56 https cesium com cesiumjs is adopted in the development of a fully customizable 3dgis interface as shown in fig 2 traditionally the dissemination of geospatial data with online map services initially requires the creation of gis layers i e raster or vector layers and relies on the map servers to render the gis layers in web friendly formats e g png kml gml and geojson finally publishing the data as standardized web services wms services for display in the web application s map view swain et al 2016 however this popular solution for map publishing is a tiered and somewhat inflexible client server architecture epitropou et al 2016 and presents numerous difficulties and an ineffective implementation in publishing time varying and large volume datasets in this study we present a new solution for geospatial data rendering in a web application independent of map servers and map services as an alternative solution dynamic heatmap and moving particle technologies have been adopted to display scalar and vector data on the cesium virtual globe respectively a heatmap is a graphical representation of two dimensional data latitude longitude and value where individual values are illustrated with a discrete number of color levels heatmap has been widely used as a common visualization method in many scientific fields epitropou et al 2016 fernandez et al 2017 kalo et al 2020 kumatani et al 2016 liu et al 2015a moumtzidou et al 2014 in this study heatmap js was used to render scalar data as heatmap images overlaying on the cesium virtual globe heatmap js is an open source lightweight simple and fast solution for heatmaps generation and is available from https www patrick wied at static heatmapjs we adopted a value based heatmap approach to display the spatial distribution of scalar data compared with the commonly used density based approach a value based heatmap that ignores the density of the measurements in a given area is more suitable for accurate rendering of scalar data kalo et al 2020 several steps were followed for the construction of value based heatmaps over a geographical domain 1 canvas initialization and data preparation according to the pos json file a regular mesh and boundary of the grid points are calculated and generated to initialize a canvas in a cesium virtual globe for plotting the heatmaps the scalar data are then loaded from the v t d datasets and the maximum and minimum values are calculated for further processing it should be noted that surface islands and seafloor terrains physically contained in a model s computational domain that are not a part of the model s output value labeled with 9999 in the corresponding grid should be treated as uncovered heatmap areas 2 influence calculation the influence of the alpha transparency at the map point can be calculated as a v m a x v v m a x v m i n where v max and v min are the maximum and minimum values of the scalar data and v represents the value of the scalar data a linear gradient circle is then constructed and painted using alpha transparency on the canvas according to the data point coordinates the radius of the linear gradient circle is adjustable and represents the influence radius r 3 mapping the grayscale is superimposed on the overlapping areas owing to the superimposition of the grayscale the more the linear gradient circles are crossed the larger the grayscale which indicates the hotter area the heatmap technology uses the renderer pixel alpha channel overlay rule as the influence overlay mode by default the formula for a general renderer overlay rule is a 0 a 1 a 2 a 1 a 2 255 where a 0 is the alpha value of the superimposed pixel and a 1 and a 2 are the alpha values of the superimposed pixel a heatmap is generated by rendering the pixels of heatmap into different colors according to its superimposed transparency alpha value more specifically the render color ribbon has a total of 0 255 ribbon values the transparency value is multiplied by 255 to get the color value corresponding to the transparency simultaneously a legend is created to show the gradient colors indicating the correspondence between the grid value and color on the map 4 enhancement of the rendering effects to create a smooth and continuous visualization heatmap circles should overlay in the most efficient way possible kalo et al 2020 we need to adjust the radius of the circle through comparative testing and validation of the alternatives to find an optimal value for blending the visualization of the circular heatmap elements vector data such as the current speed and direction can be visualized on the virtual globe in the form of static arrow symbols and dynamic moving particles or streamlines compared with static symbolized markers moving particles or streamlines can efficiently illustrate the instantaneous current characteristics and elucidate the laws of current movements this study adopted the moving particle approach to vividly visualize the currents although some similar visualization applications are available such as earth nullschool https earth nullschool net current wind surface level and windy https www windy com none of them appear to be open source and thus they cannot be expanded and or adapted for further application by third parties to create a moving particle engine in the cesium virtual globe a regular high resolution mesh covering the study area according to user defined columns and rows needs to be constructed next a bilinear interpolation is applied to calculate the direction and magnitude of the current vector at point based on the v t d datasets and the values are then assigned as the attribute of that particular mesh node finally once the interpolated surface has been generated a function randomly places particles onto the canvas at random points each particle moves in a direction and at a velocity dictated by the interpolated surface rayman 2019 following this visualization method we simulate the continuous movement of the particles based on fading existing particle trails and drawing new particle trails which stretch and move within the space to convey the dynamic flow process of the currents to achieve a better display effect the density of the moving particles is automatically adjusted at different levels using an appropriate map view corresponding to the scale of the map in addition the web application provides a toolbox containing a set of both common e g zoom in out pan rotation and flying and customized toolsets e g location bookmarks graphic tools distance area measure and on the fly mouse moving for value picking moreover for a better understanding of the temporal and spatial changes of the ocean parameters the interface provides an animation tool that can be easily used to conduct an animated visualization users can interactively slide the time steps with a time slider and use a play pause button to view the heatmaps or particles in a time lapse animation visualization these additional tools and functions make the presented web application more versatile in terms of 3dgis functions than a pure rendering engine in addition to providing mechanisms of scalar and vector displays we also developed plotting tools for a time series analysis the plotting capabilities are implemented using the open source javascript based echarts 3 0 http echarts baidu com interactive charting and visualization library when clicking on the heatmaps at any location on the virtual globe the associated time series of the predictions are extracted on demand from the server side and a time series is displayed through line graphs within a pop up window as shown in fig 3 b the information derived from these plots provides supply information on the identification of the main temporal trends and patterns of the ocean parameters which is highly useful in identifying the characteristics of the predictions the plotting tool also provides the option of exporting the time series as a comma separated value csv file in addition it allows plotting interaction such as a tooltip display of values as well as the ability to zoom in to specific sections of the charts moreover we also integrated the observations collected by gauge stations into this visualization framework the interface displays the gauge stations spatially overlaid onto the virtual globe and retrieves and displays those latest observations and metadata the plotting tool also provides the ability to automatically compare simulated values with in situ sensor data for comparison the plotting tool is initiated to extract the time series predictions from the heatmaps and load observations from the server side mysql database if sensor data are available for the same time period data are added to the charts for model validation this function provides users a convenient way to validate and diagnose the simulations against real observations for an evaluation of the prediction accuracy 2 4 volume rendering of ocean forecasts as an extension of the previously presented 3d virtual globe application we developed a 3d cubic volume rendering environment volumeviewer as shown in fig 4 in this study the interactive volumeviewer interface has been customized using plotly js an advanced and freely available graphical javascript library the plotly js is built on top of d3 js and stack gl and is a third party high performance 2d and 3d graphic library https plot ly javascript plotly js renders 3d volumes as stacks of translucent images using texture slicing algorithm and provides numerous interactive 3d visualization functions among the most commonly used volume rendering methods ray casting splatting shear warp and texture slicing as a direct volume rendering technique the texture slicing method is sufficiently fast allowing real time volume rendering to be achieved for large amounts of scalar data owing to the improvements in gpu computing liang et al 2014 beyer et al 2015 ohno and kageyama 2007 however few studies have mentioned the use of plotly technology for conducting 3d volume rendering of oceanographic datasets the main process of 3d volume rendering using plotly js can apply the following steps 1 to set up the rendering environment a basic 3d cube needs to first be constructed and some basic rendering properties need to be defined the 3d cube determines the x y and z coordinates and grid lines according to the latitude longitude and depth of aoi area of interest notably the simulation results generally cover large geographic areas where the vertical axis is generally lower in altitudinal range than the other axes it is therefore necessary to exaggerate the depth vertically to make the visualization clearer 2 considering the fact that certain ocean phenomena and processes can be better expressed and understood based on the high resolution topography and bathymetry of deep oceans liu et al 2017 it is necessary to incorporate the topography of aoi into a basic 3d cube early rendering any spatially referenced scalar or vector data over the topography 3 the client side requests grid datasets from the remote server and organizes them into objects ready to be plotted using the plotly js library in particular the datasets are cached on the client side web browser and rendered as a whole this reduces the rendering batches and allows the user to change the rendering properties on the fly e g color scheme and opacity without re requesting the data from the server side which can further increase the smoothness visual effects and the efficiency and interactivity next the plotting functions provided by plotly js are utilized to apply data rendering in the form of a 3d volume 3d slice and 3d vector the actual visualization codes are relatively simple compared with previous visualization development kits they focus solely on the definitions required to describe the internal data structure for the rendering functions and these functions read the grid structures and data values and render them in different graphics volumetric data usually refer to a 3d distribution of scalar data which are passed in an isosurface manner i e x y z and u value arrays of the coordinates and intensity each array has the same length the x y and z arrays are used to construct a mesh grid that determines the shape of the u array the plotly js volume lib renders a 3d volume as stacks of translucent textures with vertically stacked layer images thus the functions of 3d volume plots are used to draw a volume trace between the iso min and iso max values with coordinates given by four one dimensional arrays containing the value x y and z of every vertex of a uniform or non uniform 3 d grid and to show the scalar data represented as colored cubes polity js volume traces 2753 2018 similar to heatmap visualization volume rendering is also applied on a per time step basis ensuring that individual layers can be interactively displayed or hidden by selecting a specified time dimension step furthermore because spatial and temporal volumetric data are difficult to analyze through the use of a single type of visual representation we also provide a volume slice visualization and cross sectional analysis to assist with the scientific interpretation of oceanic forecasting data in this customized 3d volume environment volume slices are used to visualize 3d volumetric data by displaying one or more profiles that can inspect the data characteristics within the volumetric data and provide more insights into the 3d structure and its evolution the volumeviewer utilizes 3d surface plots to generate slices by cutting a plane section from the rendered volumetric data through operations using the slicing tool users are able to interactively generate vertical and horizontal section slices by selecting a location along the horizontal latitude or longitude direction and vertical depth direction axes because all rendered volumetric data are stored in memory and are locally cached on the client side a significant increase in the efficiency of the computations and the loading speed of a section slice location is achieved and all cross sections can be dynamically generated and rendered on the fly 3d cone plots are used to express the orientation and magnitude of ocean currents in the form of 3d directional arrows in the volumeviewer all 3d volumetric data 3d cross sections and 3d scatter plots arrows of various variables and 2 5d terrain surfaces can be simultaneously displayed in the same scene such features enable the identification of unknown processes and insights from the complex spatial relationships between various variables that may have a close underlying correlation with each other in addition the volumeviewer also provides a data ﬁltering function for displaying partial data based on the interest value this capability facilitates a dynamic visualization of the regions of interest of the volumetric data in this way the visualization is particularly useful when a user intends to display or highlight the desired features similarly users can interactively set the opacity gradient factor of the volume rendering to express the inner characteristics when setting the opacity of the surface it should be noted that when using high opacity values an overlay of multiple transparent surfaces may not be perfectly sorted in depth by the webgl api plotly js volume traces 2753 2018 further the volumeviewer can apply customized interactions including a click and drag motion to zoom into a small region of the rendered volumetric data which allows investigating the inner features in more detail moreover as the mouse pointer moving over the graphics a value picking tool enables querying the data values and spatial position any locations within the dataset s domain with real time feedback finally similar to common gis operations the volumeviewer implements various interactive functions to enable intuitive data exploration including zoom in out pan rotate and clearing or re rendering the scene 3 results to demonstrate the proposed approaches and evaluate the applicability and efficiency of the developed visualization framework a case study was presented in this section the experiments were performed in the chrome browser on a client side computer with a 3 40 ghz 8 core intel core i7 4770 cpu 16 gb of ram and an nvidia geforce gtx 760 graphics card 3 1 data description in this case study we illustrated this presented visualization framework using real world simulation results that were provided by the operational ocean forecasting system running on hpc at the south china sea institute of oceanology scsio the operational ocean forecasting system is a 3 d structured grid finite volume numerical model for providing 5 days forecasts of sea temperature salinity currents u and v and water level the model covers an area of the south china sea between 18 n and 20 n and 118 e 120 e and generates outputs with 1 60 horizontal resolutions 101 depth levels and 1 h time intervals the output forms a 120 120 101 120 174 528 000 data of each variable and constitutes about 4 gigabyte floating point data in addition base maps high resolution satellite imagery overlaid on the virtual globe were obtained from google maps and a high resolution seabed topography with 15 arc second intervals rendered in the volumeviewer were downloaded from bodc british oceanographic data centre database https www bodc ac uk data hosted data systems gebco gridded bathymetry data observational data obtained from the buoys were used to validate the simulation results and calculate the model errors 3 2 visualization application results launching the web application the interface displays the cesium virtual globe incorporated with several menus and toolsets as shown in fig 2 fig 3 shows an example visualization of heatmaps salinity and temperature distribution and moving particles currents on a cesium virtual globe fig 4 shows the main interface of the volume rendering application fig 5 shows a comprehensive display of sea temperature salinity and currents with time stamp march 27 2020 09 00 gmt through different visualization modes 4 discussion the above experiments demonstrated that the use of a value based heatmap and moving particle technologies integrated with a cesium virtual globe is an informative approach for fast and interactive visualization of scalar and vector data the adopted technologies can effectively display time varying multi variable multidimensional and large volume geospatial datasets in a web browser avoiding the need for complicated and time consuming geo processing tasks on the server side such as the conversion and organization of data in the gis layers the creation of thematic maps the deployment of a gis sever and publishing map services and the management and updating of map services at the same time it can fully take advantage of additional gis functionalities particularly on the fly 3d spatial interactivities and analytics provided by cesium virtual globe in addition the automated workflow technique in the visualization framework will simplify the daily activities of tedious model outputs processing tasks moreover as noted previously a visual exploration of a dataset to identify features or processes of scientific interest requires an interactive environment and real time visualizations that can be modified on the fly billen et al 2008 the presented experiments demonstrated that the visualization framework provides numerous high performance interactive functionalities that facilitate users to interact with data and derive insightful processes from large volumetric datasets more specifically the rendering speed is considered a typical challenge when visualizing large volumes of oceanographic data in a web application the case study showed that the display speed in the rendering of 3d volumetric scalar data over millions of data points can satisfy the user requirements likewise the newly created cross sections can be viewed immediately similarly there is no evident time delay when playing an animation of the heatmaps and moving particles in cesium virtual globe furthermore this scenario also demonstrated that the cesium virtual globe and the volumeviewer enable users to achieve a fast interaction such as the rotation zooming and panning of volumetric data these experiments suggest that the presented visualization framework can be used to implement dynamic heatmaps moving particles and volume rendering of massive ocean data with an immediate interactive response using popular graphic cards on the web client side in particular plotly js has been proven to be an efficient graphic library for achieving a real time 3d visualization and visual analysis of large amounts of volumetric data being completely independent of a high performance computer to obtain an acceptable performance however most previous investigations in this field have indicated that 3d visualization is generally a difficult task that requires considerable programming and is inefficient at displaying large volume and high dimensional data in a web environment by contrast our experiments suggest that the well designed cesium api and plotly js api allow researchers to easily establish efficient and effective 3d visualization applications in a web environment while avoiding the requirements for intensive programming and computations the simplicity of these technologies can potentially enable researchers in various fields to adopt the proposed methods for web based 3d visualization applications the presented easy to use visualization framework may not sufficiently versatile to meet all user requirements and it is not the intention of this study to replace existing standalone visualization toolkits however this study has proven that a visualization framework coupled with webgl based cesium virtual globe and plotly js map engine is a simple and feasible alternative solution for establishing an efficient and effective 3d interactive visualization and analysis environment through online spherical volume rendering and cubic volume rendering modes in particular because all of the technologies involved in this study are open source or free frameworks and none of the software or plug ins need to be installed on a client side computer it enables the visualization framework to be easily customized and or adapted to other applications by interested parties in summary compared with other freely available 3d geoscience visualization toolkits mentioned in the introduction the visualization framework demonstrated in this study has the following advantages firstly it was designed as a browser server tiered architecture and therefore the software applications are hosted by service providers and made available to users via the internet and there is nothing software and plug ins needs to be downloaded and installed what is more certain of these freely available visualization toolkits generally do not display data within a geo referenced environment lacking the ability to integrate various geospatial data sources liang et al 2014 by contrast the presented visualization framework provides users with an interactive 3dgis experience in the cesium virtual globe while at the same time offering efficient volume rendering and enriched interactive volume analysis in the volumeviewer in addition previous studies have demonstrated that the freely available software realized small scale data visualizations in the regional scene zhang et al 2019b while the developed visualization software can realize real time volume rendering for large scale data on the web client side in particular the developed visualization interfaces is user friendly with simplified operations while certain of the freely available 3d visualization toolkits are designed for general purposes and difficult to learn and use need additional and time consuming processing steps for the presentation of 3d maps finally due to its open source architecture and the simplicity of these adopted technologies the proposed visualization framework can be easily expanded and or adapted for visualizing other scientific data with few modifications despite these advances the visualization framework also remains certain limitations on the one hand it limits its adoption to irregular 3d gridded data besides it is also necessary to extend the ability of supporting many other file formats such as hdf hierarchical data format and grib gridded binary most of all it should enhance the data analysis and 3d dynamic rendering functionalities such as the multiple 3d isosurface analysis and visualization computation and visualization of 3d streamlines and trajectories from flow fields 5 conclusion in this study with the adoption of open source and webgl based cesium virtual globe plotly js heatmap moving particle and echarts visualization technologies we have developed an interactive and user friendly web based 3d visualization application with a simplified implementation the application allows users to view regular 3d grid data in a variety of representations including 1 d 2 d 2 5 d and 3 d static or dynamic visualization the results of a case study demonstrate that the proposed visualization framework provides a faster rendering speed high visual effects and on the fly 3d visualization of 3d gridded oceanic forecasting data along with greater interactivity and explorative functionalities and therefore satisfies the general visualization requirements of oceanic forecasting data enabling the comprehension of complicated oceanographic phenomena although we focused on the visualization and analysis of oceanic forecasting data due to its open source architecture and the simplicity of the adopted technologies we believe that the visualization framework can be easily customized or adapted to visualize other scientific data with few modifications 6 software availability software name the web based 3d visualization system for ocean forecasts version 1 0 year of first available 2019 developers rufu qin bin feng yusheng zhou and zhounan xu contact rufu qin qinrufu tongji edu cn 86 21 65989834 software required web browsers that support webgl programming language html5 javascript node js ms visual studio 2015 net c availability the source code can be accessed via github https github com qinrufu web based 3d visualization of oceanic forecasting data declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work has been funded by the national natural science foundation of china nsfc grant no 61701487 the strategic priority research program of chinese academy of sciences grant no xda13030102 and the youth innovation promotion association cas the authors would like to thank elsevier language editing services for their language modification appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104908 
25913,this paper presents sherpa city a web application to assess the potential of traffic measures to abate no2 air pollution in cities the application is developed by the joint research centre it is freely available https integrated assessment jrc ec europa eu and allows the user to perform a fast screening of possible no2 abatement measures addressing traffic in european cities sherpa city results depend on the quality of the default input data it is therefore important to stress that the sherpa city default traffic flows emission factors fleet composition road network topology no2 pollution from other sources and meteorological data are based on eu wide datasets that may not always represent perfectly a particular local situation this is why the sherpa city allows the default data to be substituted by local data to better reflect local features this tool must be considered as a first step in exploring options to abate no2 air pollution through transport measures the final decisions should be based wherever possible on full scale modelling studies incorporating local knowledge graphical abstract image 1 keywords no2 traffic low emission zone lez 1 introduction many cities in europe still suffer from poor air quality one of the major causes besides particulate matter pm is nitrogen dioxide no2 it has a direct negative impact on health and an indirect one through the formation of fine secondary particulate matter pm2 5 and ozone o3 who europe 2013 though the situation has improved over the last 30 years air pollution remains the single largest environmental health risk in europe according to the world health organization who 2018 both the ambient air quality directives ec 2008 and the who air quality guidelines aqg who 2005 prescribe a limit value of 40 μg m3 for no2 as an annual average ambient concentration in their latest air quality in europe report eea 2019 the european environmental agency indicates that many european cities still exceed this limit value for both no2 and pm2 5 in 2017 around 10 of all eu28 air quality monitoring stations reported exceedances of this limit value eea 2019 thunis et al 2018 with about 86 of them occurring at traffic stations i e stations placed close to major roads this is to be expected as traffic is a major source of nox which forms no2 when reacting with o3 in the atmosphere to respond to this air pollution challenge cities often take measures to reduce air pollution in areas with high traffic densities see pisoni et al 2019 for a review of such measures such measures are sometimes required to comply with the aaqd and include access restrictions to limit transport emissions in cities the eu commission s ec 2014 study on a european city pass for low emission zones lezs showed that there is a patchwork of lez approaches and rules applied throughout the eu the effectiveness of lezs depends strongly on the way they are implemented which emission standards are allowed for which vehicle categories restrictions are in place the type of access control and particularly the number of exemptions granted studies like font et al 2019 proof that vehicle emission regulation combined with other traffic policies can have a positive impact on air quality in paris london and other cities the authors analysed trends in pm and no2 they found that the introduction of euro v emission limits for trucks combined with a lez had a positive impact in london however they also detected that more stringent nox emission limits for euro 5 diesel cars had no positive impact because these cars performed worse on the road a consequence of high emission factors in real driving conditions to correctly estimate the impact of a lez and other traffic measures modelling tools are helpful many tools are available jensen et al 2017 oettl and uhrner 2011 lefebvre et al 2013 yeganeh et al 2018 rahman et al 2017 sokhi et al 2008 to model yearly daily and hourly particulate matter and nitrogen dioxide concentrations at street level taking into account urban topography emission performance of vehicles the composition of the vehicle fleet the daily activity patterns and background pollution these tools can be used to perform ex ante evaluations to understand the impact of a lez on air quality however they require appropriate it infrastructure and the availability of detailed input data as a result in many cases municipalities wishing to put into place access restrictions do not have access to a simple tool to estimate the effects of such measures on air quality before they are applied thus they have no way to evaluate access restrictions neither geographically nor as a function of emission performance to overcome this limitation the joint research centre jrc developed the sherpa city tool this simplified screening tool mimics a gaussian pollutant dispersion model but with a much shorter calculation time it can therefore be used to evaluate the impact of traffic management measures leading to reductions of emissions at the source 2 methods the methodology behind the sherpa city web application consists of the following steps 1 selection of a modelling domain of maximum 100 km2 within one of the eu countries eu27 ch uk no the application provides the full road network for the specific domain as well as the corresponding traffic levels annual average daily traffic aadt available in the database 2 implementation of desired infrastructure changes e g add or close roads different traffic flows and definition of zones where specific traffic policy will apply 3 selection of the vehicle fleet default emission factors per country year and road type both historical and forecasts are available to support this choice 4 definition of traffic measures that modify the fleet composition and or traffic flows road specific emission factors are calculated per road type and per domain zones 5 combination of the road network and emission factors to generate gridded traffic emissions at high resolution 6 application of atmospheric dispersion kernels masey et al 2018 to convert traffic emissions into contributions to the annual average concentration this kernel is location dependent 7 combination of the local traffic contribution with contributions from other emission sources within the domain as well as from all sources outside the domain 8 conversion from nox to no2 concentrations in the following paragraphs these steps and how they are implemented in sherpa city are explained in detail 2 1 road network and traffic data the sherpa city application uses traffic data from opentransportmap otm jedlička et al 2015 2016 bib jedlička et al 2015 bib jedlička et al 2016 and network information from openstreetmap osm openstreetmap contributors 2020 in particular results from the omnitrans model 1 1 https www dat nl en solutions transport modelling omnitrans are used these result are represented by a road network with these attributes traffic volume and capacity that are expressed using the concept of annual average daily traffic aadt it is however not possible to use this dataset as such because the sum of the annual vehicle kilometres per country does not match the national totals neither for traffic volume nor for capacity the annual vehicle kilometers per country of otm are on average 37 sd 24 different smaller than the official ones reported i e by gains iiasa 2020 the traffic volume based on road capacity on the contrary in general overestimates the gains data by 4 5 times sd 3 because in reality roads are not used all the time at full capacity hence we decided to scale the otm data to the gains national total annual vehicle kilometres in the case of traffic volume some gap filling is necessary for example no traffic is allocated by otm on osm fourth class roads small urban roads or in a few nuts3 regions traffic is missing on other roads to generate traffic on these roads a correlation between the functional road class and the traffic volume is used there is indeed a downward linear trend between the road class and the logarithm of the otm traffic volume this means that the traffic volume decreases with a constant percentage from one class to the next the average decrease across all countries is 64 from one class to another this correlation is extrapolated to the fourth class roads for each country in a next step the aadt on all roads is scaled up with the same factor to match the gains national totals this road network is available as the default in sherpa city it is also possible to choose the scaled road capacity if the user judges that this dataset is closer to reality if neither of the two dataset is good enough the user can edit the existing dataset or upload his own data a comparison between the default traffic dataset and local data is presented in the case study for madrid shown in the next sections of this manuscript 2 2 road transport emissions besides the traffic flow the fleet composition and emission factors are also needed to calculate emissions on each road segment the fleet composition is required in terms of kilometers driven for each segment of the fleet i e vehicle categories passenger cars vans trucks busses mopeds and motorcycles and sub segments of these categories size fuel and emission standard the fleet composition for sherpa city is acquired from emisia https www emisia com vehicle numbers kilometers driven and corresponding emissions are provided according to the copert methodology ntziachristos et al 2018 however sherpa city uses an aggregated version of the copert classification of the vehicle fleet with only 77 categories instead of 325 this makes it easier to configure a fleet and still allows for modelling typical traffic restrictions the user can control the following fleet categories passenger cars the technologies considered are gasoline diesel lpg cng hybrid and electricity all euro norms are considered pre euro euro 1 to 6 and euro 6c and euro 6 d for diesel cars the data are aggregated at vehicle size vans only diesel and gasoline are considered for all euro norms up to 6c trucks only diesel is considered with distinction between euro norms from pre euro to euro vi busses diesel and cng are considered with distinction between euro norms from pre euro to euro vi motorcycles this is one single category without distinction between sizes or norms mopeds this is one single category without distinction between sizes or norms emisia also provides the share of vehicle kilometers of each vehicle category per road type e g the share of trucks is bigger on highways than on urban roads with this information the emission factors of specific vehicle classes can be weighted by their respective kilometers driven to obtain a fleet emission factor per road type for the past years until 2016 the numbers are based on national statistics while for future years 2020 and 2025 predictions are made with the emisia s sybil software this model predicts fleet composition starting from the last data year it takes into account scrappage second hand imports and sales of new cars fig 1 shows the share of passenger cars complying with each euro norm per country in 2020 the sherpa city application assumes that the local fleet mix corresponds to the national one this is however not always the case as differences between urban and rural fleets or between fleets in different regions within the same country can be substantial if better estimates are available they can be uploaded by the user an example file is available in the fleet configuration module of the web application the sherpa city web application has a fleet configuration module that allows to reduce the mileage or completely ban a part of the fleet when a subcategory is banned e g passenger cars using diesel fuel or passenger cars compliant with euro 3 or older the vehicle kilometers of the remaining categories are scaled up to the kilometers of that respective category it is assumed that the activity of a banned subcategory will be replaced by the remaining allowed vehicles in that category if the user wants to model a reduction of the kilometers driven in a category on top of a fleet composition change this must be introduced separately on the other hand if a whole category is banned its vehicle kilometers are not moved to other categories this means that removing all trucks will not increase the kilometres driven by busses cars etc the final step in creating emissions consists in converting the road network into a raster the intersection between grid cells and roads determines the level of emissions in each cell the network is finally converted in the local utm projection at a spatial resolution of 20 m 2 3 dispersion kernels to calculate annual average concentrations dispersion kernels are used a dispersion kernel provides information on the average annual concentration of a pollutant nox or pm2 5 around a normalized emission source 1 kg h was chosen kernels covering europe are pre calculated over a 0 2 latitude and 0 5 longitude resolution grid between 35 and 70 north and between 15 west and 35 east resulting in 14 000 kernels for a project designed in a particular city the closest kernel is then chosen kernels are calculated with the gaussian dispersion model ifdm lefebvre et al 2013 with hourly wind speed and temperature data as input concentrations around a unit source of 1 kg h within a domain of 4 by 4 km are calculated at a resolution of 20 by 20 m the ifdm hourly output concentrations are then averaged over the year the shape of a dispersion kernel depends on the local meteorology at locations characterized by higher wind speeds polluted air is dispersed more efficiently and concentrations are therefore lower fig 2 shows the maximum concentration obtained for each kernel high values for the kernel are found in mountain areas while lower concentrations are found at sea or in coastal locations where higher wind speeds spread out air pollution more efficiently to calculate the traffic contribution to the concentration for each cell the source kernel is multiplied by the strength of the emissions in that cell these scaled kernels are then summed over the entire domain 2 4 nox emissions from other sources the background to obtain the total concentrations background levels must be considered we use a simulation from a chemistry transport model ctm to obtain this information the following options are available an emep simpson et al 2012 run for the year 2015 this run gives a concentration value for every cell within the domain at a resolution of 0 1 by 0 1 this corresponds roughly to 11 km latitude and 7 km longitude a chimere mailler et al 2017 run for the year 2010 this run gives a concentration value for every cell within the domain at a resolution of 0 0625 by 0 125 this corresponds roughly to 7 km latitude and 7 km longitude regardless of the selected option the ctm provides one value for no and no2 that is interpolated to the centre of the city sherpa domain this concentration also includes the contribution of local traffic to avoid double counting the average concentration due to local sources the result of the kernel approach is subtracted from the ctm value then the high resolution local contribution is added to this corrected ctm concentration in the case of no2 this correction is carried out on the nox concentrations in a final step explained in the next paragraph the no2 concentration is calculated from the nox concentration 2 5 from annual average nox to no2 sherpa city does not calculate the annual average no2 directly first the annual average nox concentration is calculated as a combination of a corrected ctm background and the local contribution see previous section then the no2 concentration is calculated using a correlation between the no2 fraction and the total nox concentration as a function of the overall nox emissions this approximation is justified because close to the source and shortly after the emission nox behaves as an inert gas therefore its dispersion is well predicted with a gaussian dispersion model and the local contribution to the concentration is proportional to the nox emissions the reactions that remove nox forming nitrates and secondary pm are much slower and accounted for by the ctm the only important reactions close to the emissions release point are the reaction between ozone and nitrogen monoxide and the photolysis of nitrogen dioxide these reactions do not change the total amount of nox 1 n o 2 o 2 h ν n o o 3 when solar radiation shifts the equilibrium to the right of the aforementioned equation ozone is formed in the absence of solar radiation the equilibrium shifts to the left and no2 is formed the resulting annual average no2 fraction therefore depends on local conditions like the concentration of ozone and other oxidants solar radiation and the presence of volatile organic carbons voc the trend is similar everywhere when nox concentrations are low the no2 fraction increases to 100 when nox concentrations are high the no2 fraction levels off in this ozone limited regime not all the no can be oxidized to no2 this means that a reduction of nox emissions has a smaller impact on no2 concentrations when the nox concentrations are high so this effect is of interest when planning measures to reduce pollution romberg et al 1996 and bächlin et al 2008 proposed an empirical correlation approximating this behaviour with a formula like equation 2 2 f n o 2 n o 2 a n o x a a n o x a b c n o x a and n o 2 a are the annual average nox and no2 respectively romberg et al 1996 proposed a 103 b 130 and c 0 005 bächlin et al 2008 proposed a 29 b 35 and c 0 217 jurado et al 2020 compared both formulas with measurements in france and found that both models perform similarly with respective deviations of 9 1 and 10 2 a third model by derwent and middleton 1996 was shown to perform slightly better but requires hourly data and cannot be used for this application fig 3 shows the romberg and bächlin correlations applied to all annual average data available in eea air quality statistics eea 2020 the correlations follows the trend but are not accurate therefor sherpa city uses a correlation with adjusted coefficients so that the background ctm value in the domain lies on the curve this adapted correlation will be used to calculate the no2 concentration due to changing nox emissions 3 the web application the sherpa city web application implements all the aforementioned steps it is accessible through a login page https integrated assessment jrc ec europa eu sherpacity the use of sherpa city is free and registration can be done by selecting register on the login page or directly through the page https aqm jrc ec europa eu register aspx users will receive an e mail by the site administrator granting access to the application the steps to set up a simulation in the sherpa city web application accessible are creating a new project and selecting national average default fleet compositions from previous or for future years fleets for all eu27 countries uk switzerland and norway are available selecting the domain study area for the simulations the maximum size is 100 km2 creating zones or modifying network and traffic data this step can be carried out through the web application or by uploading user data this step allows the user to create the zones where traffic measures defined in the fleet configurations see below can be applied it also allows the user to modify the default traffic data e g by modifying traffic flow for specific roads in the domain and network e g by removing or adding streets this can be done through the web application or by uploading the user s own shape files creating new fleet configurations that modify the default fleet in terms of a traffic volumes by vehicle type b allowed vehicles by type fuel or euro standard c type of road where the measures are applied fleets can be defined at any moment of the workflow before the simulation of a scenario and applied in any project in combination with the default project fleet a fleet configuration becomes then a real fleet e g a user can create a fleet configuration without trucks this configuration can be combined in a project with i e the bulgarian fleet of 2016 or the portuguese one of 2025 simulating the basecase and other scenarios is the core of the sherpa city web application after the simulation of the basecase the user can define scenarios to compare results the scenarios are defined within a project by selecting a specific fleet configuration to be applied to each zone visualizing results of the simulation is the final step both concentrations and emissions can be visualized as gridded values average values and in table or chart format results can be exported in raster format each step is described in more detail in the manual that can be found on the tool website the import of shape files and other known issues are reported in the annex of the manual during the workflow the user can visualize and update the process status by clicking on the verify process status and update process status buttons at the top right of the page also an offline version of the tool is available on github at https github com bd77 sherpa city this offline version is also used to implement the case study presented in the next section 4 case study madrid central to illustrate how the sherpa city web tool works an application has been implemented to study the madrid central case madrid introduced a low emission zone called madrid central on november 30 2018 in which only specific vehicles could enter a classification of vehicles according to their environmental impact was introduced by the transport ministry zero emissions vehicles mainly electrical vehicles eco vehicles hybrid natural gas lpg vehicles type c vehicles gasoline passenger cars and vans complying with euro 4 iv 5 v o 6 vi diesel cars vans and trucks complying with euro 6 vi type b vehicles gasoline passenger cars and vans complying with euro 3 iii diesel cars vans and trucks complying with euro 4 iv and 5 v type a vehicles all vehicles not contained in the previous classes vehicles are identified with a sticker in the lez vehicles of type a are not allowed these are mainly gasoline vehicles build before the year 2000 and diesel cars older than 2005 vehicles of class b and c can access to park eco and zero emission vehicles can circulate without restrictions more details are available on the website of the city of madrid ayuntamiento de madrid 2020 in this paragraph first the available input data are described and then few scenarios are run to evaluate the impact of the lez on no2 concentrations 4 1 traffic and fleet data traffic counts are available for 2018 and 2019 at about 2000 locations in madrid these were used to calibrate a traffic model providing traffic flows on all roads in the madrid area these local data are compared with the sherpa city default dataset table 1 shows the vehicle kilometers driven aadt that is to say annual average daily traffic in the metropolitan area split in two areas the madrid central lez and the rest of the area outside the lez according to the traffic data of madrid the traffic in the lez represents only 1 2 of the traffic in the metropolitan area the default datasets provided in sherpa city give 1 5 and 0 8 for the otm traffic volume and otm road capacity respectively fig 4 shows maps of the three datasets on the same scale the map on the left are the data provided by madrid the map in the middle are the otm trafficvol data there are clear differences especially on the m30 highway surrounding the city and on major roads in the centre the otm data underestimate the traffic on some parts of the highway and overestimate the traffic on some major roads in the centre the otm capacity dataset smears out the traffic evenly all over the domain in the next section the local madrid data will be used because of their higher quality in comparison to the default data 4 2 scenarios the aim of this sherpa city application is to understand the impact of the madrid lez on no2 concentrations to disentangle the impact from the lez and traffic changes the following scenarios were calculated considering 2018 local traffic data allowing all the fleet to travel everywhere 2018 local traffic data with only authorized fleet in the madrid central lez 2019 local traffic data allowing all the fleet to travel everywhere 2019 local traffic data with only authorized fleet in the madrid central lez in this way it is possible to differentiate the impact caused by traffic changes independent from the lez from the impact caused by the lez implementation itself an additional test is done in which we completely remove traffic from the lez to see which could be the most extreme air quality improvement i e keeping fixed the area of reduction but reducing to zero the allowed fleet 4 3 concentration results in this section we show the results of the sherpa city application it is important to stress that results are shown both for nox output of the sherpa city and no2 as obtained applying the nox to no conversion described in previous sections as an example fig 5 shows the no2 concentration in madrid in 2018 as predicted by sherpa city the maps shows how sherpa city capture the spatial trends of no2 higher on main roads lower when distant from roads with a reasonable agreement with observations see points and values on the map table 2 shows the average over the lez results obtained for nox and no2 in particular showing the basecase the case regulated by the lez banning the a type vehicles and the extreme case with no traffic on the lez looking at both years 2018 and 2019 in table 2 it is clear how the nox and no2 reductions are quite limited for the mean values going from the basecase to the lez scenario only a more aggressive lez going to a very small allowed traffic in the lez would reduce i e no2 in 2019 from 23 9 to 19 9 μg m3 while lez simulated case stops at 23 6 μg m3 this small impact is due to the small amount of aadt reduced with the lez table 1 looking at the maximum values instead of the mean values again over the lez the impact is higher i e moving for 2019 from 54 5 basecase to 41 4 no traffic μg m3 however one should remember that lez are done usually not only for air quality but more in general to improve quality of life reducting noise accidents allowing for more walking biking so the results of this analysis with sherpa city is a very partial view of the issue at stake representing only one of the indicators to be considered in addition to this the lez discussed here is designed in such a way that only a very small fraction of the city transport is affected stronger impacts would require more ambitious lez configurations particularly in terms of spatial extension 5 discussion and conclusions in this paper we have presented a novel user friendly web application sherpa city to assess traffic measures in cities in particular sherpa city can be used to evaluate the impact of low emission zones lez on no2 and pm concentrations in comparison to other existing approaches it can be used with limited effort using default data to test the impact of mobility measures on air quality however it is advisable to replace the sherpa city default data with local data that likely reflect better the local traffic and fleet composition or at least to check for a given study domain if the default data respect real local conditions also a case study on madrid has been presented with a focus on the evaluation of a low emission zone implemented few years ago results using locally provided data show that the lez affecting only a small portion of the total aadt of madrid had in reality a small impact on no2 average concentrations and a bit higher for maximum values summarizing we think sherpa city can be used as a screening tool to check ex ante or ex post the impact of lez on pollutant levels at a very fine spatial resolution the main advantages of using sherpa city are the availability of road networks with default traffic data fleet data and emission factors covering the whole eu the possibility to select domains and zones for traffic policies at a fine spatial resolution over a detailed map the possibility to define traffic measures in terms of fleet composition and traffic volumes the calculation of contributions from other sources than traffic from a full air quality model the possibility to compute scenario concentrations to be compared with a baseline scenario applying the default values in sherpa city can give an approximation of the real impacts for the various eu city so introducing local data on traffic fleet mix and background concentrations is recommended in order to achieve a better correspondence of the model results with real local conditions software availability the sherpa city tool can be accessed at https integrated assessment jrc ec europa eu after registration free of charge the source code for the case study shown in this paper is available at https github com bd77 sherpa city disclaimer the views expressed in this paper are purely those of the authors and may not under any circumstances be regarded as an official position of the european commission declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper 
25913,this paper presents sherpa city a web application to assess the potential of traffic measures to abate no2 air pollution in cities the application is developed by the joint research centre it is freely available https integrated assessment jrc ec europa eu and allows the user to perform a fast screening of possible no2 abatement measures addressing traffic in european cities sherpa city results depend on the quality of the default input data it is therefore important to stress that the sherpa city default traffic flows emission factors fleet composition road network topology no2 pollution from other sources and meteorological data are based on eu wide datasets that may not always represent perfectly a particular local situation this is why the sherpa city allows the default data to be substituted by local data to better reflect local features this tool must be considered as a first step in exploring options to abate no2 air pollution through transport measures the final decisions should be based wherever possible on full scale modelling studies incorporating local knowledge graphical abstract image 1 keywords no2 traffic low emission zone lez 1 introduction many cities in europe still suffer from poor air quality one of the major causes besides particulate matter pm is nitrogen dioxide no2 it has a direct negative impact on health and an indirect one through the formation of fine secondary particulate matter pm2 5 and ozone o3 who europe 2013 though the situation has improved over the last 30 years air pollution remains the single largest environmental health risk in europe according to the world health organization who 2018 both the ambient air quality directives ec 2008 and the who air quality guidelines aqg who 2005 prescribe a limit value of 40 μg m3 for no2 as an annual average ambient concentration in their latest air quality in europe report eea 2019 the european environmental agency indicates that many european cities still exceed this limit value for both no2 and pm2 5 in 2017 around 10 of all eu28 air quality monitoring stations reported exceedances of this limit value eea 2019 thunis et al 2018 with about 86 of them occurring at traffic stations i e stations placed close to major roads this is to be expected as traffic is a major source of nox which forms no2 when reacting with o3 in the atmosphere to respond to this air pollution challenge cities often take measures to reduce air pollution in areas with high traffic densities see pisoni et al 2019 for a review of such measures such measures are sometimes required to comply with the aaqd and include access restrictions to limit transport emissions in cities the eu commission s ec 2014 study on a european city pass for low emission zones lezs showed that there is a patchwork of lez approaches and rules applied throughout the eu the effectiveness of lezs depends strongly on the way they are implemented which emission standards are allowed for which vehicle categories restrictions are in place the type of access control and particularly the number of exemptions granted studies like font et al 2019 proof that vehicle emission regulation combined with other traffic policies can have a positive impact on air quality in paris london and other cities the authors analysed trends in pm and no2 they found that the introduction of euro v emission limits for trucks combined with a lez had a positive impact in london however they also detected that more stringent nox emission limits for euro 5 diesel cars had no positive impact because these cars performed worse on the road a consequence of high emission factors in real driving conditions to correctly estimate the impact of a lez and other traffic measures modelling tools are helpful many tools are available jensen et al 2017 oettl and uhrner 2011 lefebvre et al 2013 yeganeh et al 2018 rahman et al 2017 sokhi et al 2008 to model yearly daily and hourly particulate matter and nitrogen dioxide concentrations at street level taking into account urban topography emission performance of vehicles the composition of the vehicle fleet the daily activity patterns and background pollution these tools can be used to perform ex ante evaluations to understand the impact of a lez on air quality however they require appropriate it infrastructure and the availability of detailed input data as a result in many cases municipalities wishing to put into place access restrictions do not have access to a simple tool to estimate the effects of such measures on air quality before they are applied thus they have no way to evaluate access restrictions neither geographically nor as a function of emission performance to overcome this limitation the joint research centre jrc developed the sherpa city tool this simplified screening tool mimics a gaussian pollutant dispersion model but with a much shorter calculation time it can therefore be used to evaluate the impact of traffic management measures leading to reductions of emissions at the source 2 methods the methodology behind the sherpa city web application consists of the following steps 1 selection of a modelling domain of maximum 100 km2 within one of the eu countries eu27 ch uk no the application provides the full road network for the specific domain as well as the corresponding traffic levels annual average daily traffic aadt available in the database 2 implementation of desired infrastructure changes e g add or close roads different traffic flows and definition of zones where specific traffic policy will apply 3 selection of the vehicle fleet default emission factors per country year and road type both historical and forecasts are available to support this choice 4 definition of traffic measures that modify the fleet composition and or traffic flows road specific emission factors are calculated per road type and per domain zones 5 combination of the road network and emission factors to generate gridded traffic emissions at high resolution 6 application of atmospheric dispersion kernels masey et al 2018 to convert traffic emissions into contributions to the annual average concentration this kernel is location dependent 7 combination of the local traffic contribution with contributions from other emission sources within the domain as well as from all sources outside the domain 8 conversion from nox to no2 concentrations in the following paragraphs these steps and how they are implemented in sherpa city are explained in detail 2 1 road network and traffic data the sherpa city application uses traffic data from opentransportmap otm jedlička et al 2015 2016 bib jedlička et al 2015 bib jedlička et al 2016 and network information from openstreetmap osm openstreetmap contributors 2020 in particular results from the omnitrans model 1 1 https www dat nl en solutions transport modelling omnitrans are used these result are represented by a road network with these attributes traffic volume and capacity that are expressed using the concept of annual average daily traffic aadt it is however not possible to use this dataset as such because the sum of the annual vehicle kilometres per country does not match the national totals neither for traffic volume nor for capacity the annual vehicle kilometers per country of otm are on average 37 sd 24 different smaller than the official ones reported i e by gains iiasa 2020 the traffic volume based on road capacity on the contrary in general overestimates the gains data by 4 5 times sd 3 because in reality roads are not used all the time at full capacity hence we decided to scale the otm data to the gains national total annual vehicle kilometres in the case of traffic volume some gap filling is necessary for example no traffic is allocated by otm on osm fourth class roads small urban roads or in a few nuts3 regions traffic is missing on other roads to generate traffic on these roads a correlation between the functional road class and the traffic volume is used there is indeed a downward linear trend between the road class and the logarithm of the otm traffic volume this means that the traffic volume decreases with a constant percentage from one class to the next the average decrease across all countries is 64 from one class to another this correlation is extrapolated to the fourth class roads for each country in a next step the aadt on all roads is scaled up with the same factor to match the gains national totals this road network is available as the default in sherpa city it is also possible to choose the scaled road capacity if the user judges that this dataset is closer to reality if neither of the two dataset is good enough the user can edit the existing dataset or upload his own data a comparison between the default traffic dataset and local data is presented in the case study for madrid shown in the next sections of this manuscript 2 2 road transport emissions besides the traffic flow the fleet composition and emission factors are also needed to calculate emissions on each road segment the fleet composition is required in terms of kilometers driven for each segment of the fleet i e vehicle categories passenger cars vans trucks busses mopeds and motorcycles and sub segments of these categories size fuel and emission standard the fleet composition for sherpa city is acquired from emisia https www emisia com vehicle numbers kilometers driven and corresponding emissions are provided according to the copert methodology ntziachristos et al 2018 however sherpa city uses an aggregated version of the copert classification of the vehicle fleet with only 77 categories instead of 325 this makes it easier to configure a fleet and still allows for modelling typical traffic restrictions the user can control the following fleet categories passenger cars the technologies considered are gasoline diesel lpg cng hybrid and electricity all euro norms are considered pre euro euro 1 to 6 and euro 6c and euro 6 d for diesel cars the data are aggregated at vehicle size vans only diesel and gasoline are considered for all euro norms up to 6c trucks only diesel is considered with distinction between euro norms from pre euro to euro vi busses diesel and cng are considered with distinction between euro norms from pre euro to euro vi motorcycles this is one single category without distinction between sizes or norms mopeds this is one single category without distinction between sizes or norms emisia also provides the share of vehicle kilometers of each vehicle category per road type e g the share of trucks is bigger on highways than on urban roads with this information the emission factors of specific vehicle classes can be weighted by their respective kilometers driven to obtain a fleet emission factor per road type for the past years until 2016 the numbers are based on national statistics while for future years 2020 and 2025 predictions are made with the emisia s sybil software this model predicts fleet composition starting from the last data year it takes into account scrappage second hand imports and sales of new cars fig 1 shows the share of passenger cars complying with each euro norm per country in 2020 the sherpa city application assumes that the local fleet mix corresponds to the national one this is however not always the case as differences between urban and rural fleets or between fleets in different regions within the same country can be substantial if better estimates are available they can be uploaded by the user an example file is available in the fleet configuration module of the web application the sherpa city web application has a fleet configuration module that allows to reduce the mileage or completely ban a part of the fleet when a subcategory is banned e g passenger cars using diesel fuel or passenger cars compliant with euro 3 or older the vehicle kilometers of the remaining categories are scaled up to the kilometers of that respective category it is assumed that the activity of a banned subcategory will be replaced by the remaining allowed vehicles in that category if the user wants to model a reduction of the kilometers driven in a category on top of a fleet composition change this must be introduced separately on the other hand if a whole category is banned its vehicle kilometers are not moved to other categories this means that removing all trucks will not increase the kilometres driven by busses cars etc the final step in creating emissions consists in converting the road network into a raster the intersection between grid cells and roads determines the level of emissions in each cell the network is finally converted in the local utm projection at a spatial resolution of 20 m 2 3 dispersion kernels to calculate annual average concentrations dispersion kernels are used a dispersion kernel provides information on the average annual concentration of a pollutant nox or pm2 5 around a normalized emission source 1 kg h was chosen kernels covering europe are pre calculated over a 0 2 latitude and 0 5 longitude resolution grid between 35 and 70 north and between 15 west and 35 east resulting in 14 000 kernels for a project designed in a particular city the closest kernel is then chosen kernels are calculated with the gaussian dispersion model ifdm lefebvre et al 2013 with hourly wind speed and temperature data as input concentrations around a unit source of 1 kg h within a domain of 4 by 4 km are calculated at a resolution of 20 by 20 m the ifdm hourly output concentrations are then averaged over the year the shape of a dispersion kernel depends on the local meteorology at locations characterized by higher wind speeds polluted air is dispersed more efficiently and concentrations are therefore lower fig 2 shows the maximum concentration obtained for each kernel high values for the kernel are found in mountain areas while lower concentrations are found at sea or in coastal locations where higher wind speeds spread out air pollution more efficiently to calculate the traffic contribution to the concentration for each cell the source kernel is multiplied by the strength of the emissions in that cell these scaled kernels are then summed over the entire domain 2 4 nox emissions from other sources the background to obtain the total concentrations background levels must be considered we use a simulation from a chemistry transport model ctm to obtain this information the following options are available an emep simpson et al 2012 run for the year 2015 this run gives a concentration value for every cell within the domain at a resolution of 0 1 by 0 1 this corresponds roughly to 11 km latitude and 7 km longitude a chimere mailler et al 2017 run for the year 2010 this run gives a concentration value for every cell within the domain at a resolution of 0 0625 by 0 125 this corresponds roughly to 7 km latitude and 7 km longitude regardless of the selected option the ctm provides one value for no and no2 that is interpolated to the centre of the city sherpa domain this concentration also includes the contribution of local traffic to avoid double counting the average concentration due to local sources the result of the kernel approach is subtracted from the ctm value then the high resolution local contribution is added to this corrected ctm concentration in the case of no2 this correction is carried out on the nox concentrations in a final step explained in the next paragraph the no2 concentration is calculated from the nox concentration 2 5 from annual average nox to no2 sherpa city does not calculate the annual average no2 directly first the annual average nox concentration is calculated as a combination of a corrected ctm background and the local contribution see previous section then the no2 concentration is calculated using a correlation between the no2 fraction and the total nox concentration as a function of the overall nox emissions this approximation is justified because close to the source and shortly after the emission nox behaves as an inert gas therefore its dispersion is well predicted with a gaussian dispersion model and the local contribution to the concentration is proportional to the nox emissions the reactions that remove nox forming nitrates and secondary pm are much slower and accounted for by the ctm the only important reactions close to the emissions release point are the reaction between ozone and nitrogen monoxide and the photolysis of nitrogen dioxide these reactions do not change the total amount of nox 1 n o 2 o 2 h ν n o o 3 when solar radiation shifts the equilibrium to the right of the aforementioned equation ozone is formed in the absence of solar radiation the equilibrium shifts to the left and no2 is formed the resulting annual average no2 fraction therefore depends on local conditions like the concentration of ozone and other oxidants solar radiation and the presence of volatile organic carbons voc the trend is similar everywhere when nox concentrations are low the no2 fraction increases to 100 when nox concentrations are high the no2 fraction levels off in this ozone limited regime not all the no can be oxidized to no2 this means that a reduction of nox emissions has a smaller impact on no2 concentrations when the nox concentrations are high so this effect is of interest when planning measures to reduce pollution romberg et al 1996 and bächlin et al 2008 proposed an empirical correlation approximating this behaviour with a formula like equation 2 2 f n o 2 n o 2 a n o x a a n o x a b c n o x a and n o 2 a are the annual average nox and no2 respectively romberg et al 1996 proposed a 103 b 130 and c 0 005 bächlin et al 2008 proposed a 29 b 35 and c 0 217 jurado et al 2020 compared both formulas with measurements in france and found that both models perform similarly with respective deviations of 9 1 and 10 2 a third model by derwent and middleton 1996 was shown to perform slightly better but requires hourly data and cannot be used for this application fig 3 shows the romberg and bächlin correlations applied to all annual average data available in eea air quality statistics eea 2020 the correlations follows the trend but are not accurate therefor sherpa city uses a correlation with adjusted coefficients so that the background ctm value in the domain lies on the curve this adapted correlation will be used to calculate the no2 concentration due to changing nox emissions 3 the web application the sherpa city web application implements all the aforementioned steps it is accessible through a login page https integrated assessment jrc ec europa eu sherpacity the use of sherpa city is free and registration can be done by selecting register on the login page or directly through the page https aqm jrc ec europa eu register aspx users will receive an e mail by the site administrator granting access to the application the steps to set up a simulation in the sherpa city web application accessible are creating a new project and selecting national average default fleet compositions from previous or for future years fleets for all eu27 countries uk switzerland and norway are available selecting the domain study area for the simulations the maximum size is 100 km2 creating zones or modifying network and traffic data this step can be carried out through the web application or by uploading user data this step allows the user to create the zones where traffic measures defined in the fleet configurations see below can be applied it also allows the user to modify the default traffic data e g by modifying traffic flow for specific roads in the domain and network e g by removing or adding streets this can be done through the web application or by uploading the user s own shape files creating new fleet configurations that modify the default fleet in terms of a traffic volumes by vehicle type b allowed vehicles by type fuel or euro standard c type of road where the measures are applied fleets can be defined at any moment of the workflow before the simulation of a scenario and applied in any project in combination with the default project fleet a fleet configuration becomes then a real fleet e g a user can create a fleet configuration without trucks this configuration can be combined in a project with i e the bulgarian fleet of 2016 or the portuguese one of 2025 simulating the basecase and other scenarios is the core of the sherpa city web application after the simulation of the basecase the user can define scenarios to compare results the scenarios are defined within a project by selecting a specific fleet configuration to be applied to each zone visualizing results of the simulation is the final step both concentrations and emissions can be visualized as gridded values average values and in table or chart format results can be exported in raster format each step is described in more detail in the manual that can be found on the tool website the import of shape files and other known issues are reported in the annex of the manual during the workflow the user can visualize and update the process status by clicking on the verify process status and update process status buttons at the top right of the page also an offline version of the tool is available on github at https github com bd77 sherpa city this offline version is also used to implement the case study presented in the next section 4 case study madrid central to illustrate how the sherpa city web tool works an application has been implemented to study the madrid central case madrid introduced a low emission zone called madrid central on november 30 2018 in which only specific vehicles could enter a classification of vehicles according to their environmental impact was introduced by the transport ministry zero emissions vehicles mainly electrical vehicles eco vehicles hybrid natural gas lpg vehicles type c vehicles gasoline passenger cars and vans complying with euro 4 iv 5 v o 6 vi diesel cars vans and trucks complying with euro 6 vi type b vehicles gasoline passenger cars and vans complying with euro 3 iii diesel cars vans and trucks complying with euro 4 iv and 5 v type a vehicles all vehicles not contained in the previous classes vehicles are identified with a sticker in the lez vehicles of type a are not allowed these are mainly gasoline vehicles build before the year 2000 and diesel cars older than 2005 vehicles of class b and c can access to park eco and zero emission vehicles can circulate without restrictions more details are available on the website of the city of madrid ayuntamiento de madrid 2020 in this paragraph first the available input data are described and then few scenarios are run to evaluate the impact of the lez on no2 concentrations 4 1 traffic and fleet data traffic counts are available for 2018 and 2019 at about 2000 locations in madrid these were used to calibrate a traffic model providing traffic flows on all roads in the madrid area these local data are compared with the sherpa city default dataset table 1 shows the vehicle kilometers driven aadt that is to say annual average daily traffic in the metropolitan area split in two areas the madrid central lez and the rest of the area outside the lez according to the traffic data of madrid the traffic in the lez represents only 1 2 of the traffic in the metropolitan area the default datasets provided in sherpa city give 1 5 and 0 8 for the otm traffic volume and otm road capacity respectively fig 4 shows maps of the three datasets on the same scale the map on the left are the data provided by madrid the map in the middle are the otm trafficvol data there are clear differences especially on the m30 highway surrounding the city and on major roads in the centre the otm data underestimate the traffic on some parts of the highway and overestimate the traffic on some major roads in the centre the otm capacity dataset smears out the traffic evenly all over the domain in the next section the local madrid data will be used because of their higher quality in comparison to the default data 4 2 scenarios the aim of this sherpa city application is to understand the impact of the madrid lez on no2 concentrations to disentangle the impact from the lez and traffic changes the following scenarios were calculated considering 2018 local traffic data allowing all the fleet to travel everywhere 2018 local traffic data with only authorized fleet in the madrid central lez 2019 local traffic data allowing all the fleet to travel everywhere 2019 local traffic data with only authorized fleet in the madrid central lez in this way it is possible to differentiate the impact caused by traffic changes independent from the lez from the impact caused by the lez implementation itself an additional test is done in which we completely remove traffic from the lez to see which could be the most extreme air quality improvement i e keeping fixed the area of reduction but reducing to zero the allowed fleet 4 3 concentration results in this section we show the results of the sherpa city application it is important to stress that results are shown both for nox output of the sherpa city and no2 as obtained applying the nox to no conversion described in previous sections as an example fig 5 shows the no2 concentration in madrid in 2018 as predicted by sherpa city the maps shows how sherpa city capture the spatial trends of no2 higher on main roads lower when distant from roads with a reasonable agreement with observations see points and values on the map table 2 shows the average over the lez results obtained for nox and no2 in particular showing the basecase the case regulated by the lez banning the a type vehicles and the extreme case with no traffic on the lez looking at both years 2018 and 2019 in table 2 it is clear how the nox and no2 reductions are quite limited for the mean values going from the basecase to the lez scenario only a more aggressive lez going to a very small allowed traffic in the lez would reduce i e no2 in 2019 from 23 9 to 19 9 μg m3 while lez simulated case stops at 23 6 μg m3 this small impact is due to the small amount of aadt reduced with the lez table 1 looking at the maximum values instead of the mean values again over the lez the impact is higher i e moving for 2019 from 54 5 basecase to 41 4 no traffic μg m3 however one should remember that lez are done usually not only for air quality but more in general to improve quality of life reducting noise accidents allowing for more walking biking so the results of this analysis with sherpa city is a very partial view of the issue at stake representing only one of the indicators to be considered in addition to this the lez discussed here is designed in such a way that only a very small fraction of the city transport is affected stronger impacts would require more ambitious lez configurations particularly in terms of spatial extension 5 discussion and conclusions in this paper we have presented a novel user friendly web application sherpa city to assess traffic measures in cities in particular sherpa city can be used to evaluate the impact of low emission zones lez on no2 and pm concentrations in comparison to other existing approaches it can be used with limited effort using default data to test the impact of mobility measures on air quality however it is advisable to replace the sherpa city default data with local data that likely reflect better the local traffic and fleet composition or at least to check for a given study domain if the default data respect real local conditions also a case study on madrid has been presented with a focus on the evaluation of a low emission zone implemented few years ago results using locally provided data show that the lez affecting only a small portion of the total aadt of madrid had in reality a small impact on no2 average concentrations and a bit higher for maximum values summarizing we think sherpa city can be used as a screening tool to check ex ante or ex post the impact of lez on pollutant levels at a very fine spatial resolution the main advantages of using sherpa city are the availability of road networks with default traffic data fleet data and emission factors covering the whole eu the possibility to select domains and zones for traffic policies at a fine spatial resolution over a detailed map the possibility to define traffic measures in terms of fleet composition and traffic volumes the calculation of contributions from other sources than traffic from a full air quality model the possibility to compute scenario concentrations to be compared with a baseline scenario applying the default values in sherpa city can give an approximation of the real impacts for the various eu city so introducing local data on traffic fleet mix and background concentrations is recommended in order to achieve a better correspondence of the model results with real local conditions software availability the sherpa city tool can be accessed at https integrated assessment jrc ec europa eu after registration free of charge the source code for the case study shown in this paper is available at https github com bd77 sherpa city disclaimer the views expressed in this paper are purely those of the authors and may not under any circumstances be regarded as an official position of the european commission declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper 
25914,this work presents an open source tool to predict natural system responses by transforming the frequency spectrum of predictor variables to create a response that better resembles observations the r package namely wavelet system prediction wasp is based on a discrete wavelet transform dwt based variance transformation method we further introduce the maximal overlap dwt modwt based variance transformation which allows the method to be used in forecasting applications we also develop the method to include an unbiased estimator that mitigates the well known issue of edge effects in wavelet transforms the predictive model in the method is a k nearest neighbor knn approach the main functionalities of the software include 1 transforming the system predictors 2 identifying significant predictors corresponding to the response 3 predicting target response using the knn results of predicting sustained drought anomalies across australia show clear improvements in predictive skill compared to the use of untransformed predictors keywords wavelet system prediction r package maximal overlap discrete wavelet transform unbiased estimator 1 introduction a regression model describes the relationship between a system response and a finite set of predictor variables using an assumed modelling form linear or nonlinear approaches can range from simple regression models using a range of physically justifiable predictor variables hertig and tramblay 2017 to those where more complex transformations including rotations are adopted jiang et al 2019 ndehedehe et al 2016 differing spectral attributes in a response and a system predictor can complicate specifying the system prediction model we present here an approach that addresses this difficulty by optimally transforming each predictor variable to better characterise the spectrum of the response being modelled the underlying idea behind the approach here is to improve the modelling of natural systems where the potential predictor variables vary at time scales that differ from those of the plausible response for example in hydrology daily precipitation is used to predict catchment streamflow however attenuation from catchment storage means that at short time scales the variability of streamflow is substantially dampened compared to rainfall thus conventional regression modelling approaches can have difficulties in characterising this differing variability and formulating a relationship rashid and beecham 2019 although the approach is generic and can be used for any natural system model our specific focus is on hydro climatological systems an example of such a prediction problem is the need to assess changes to natural systems due to climate change in this case earth system models and or general circulation models gcms provide predictors that can be used to model future changes in hydrological variables alternatives to transfer the modelling problem into the frequency domain include methods such as fourier and wavelet transforms our approach uses wavelet theory to formulate an optimal model of the system to improve the assessment of changes into the future the wavelet transform wt is adopted in the approach to avoid loss of temporal information when transforming to the frequency domain using a fourier transform daubechies 1990 strang 1996 torrence and compo 1998 the wt can decompose the original time series into separate large scale slowly changing low frequency and fine scale rapidly changing details high frequency time series a number of models based on frequency domain analysis have been proposed recently to simulate and predict the variability in the response fahimi yaseen el shafie 2017 nguyen et al 2019 quilty et al 2019 rashid et al 2018 sang 2013 most of those applications directly use the decomposed time series to forecast the target response quilty and adamowski 2018 rashid et al 2016 jiang sharma and johnson 2020 proposed a new approach by using the decomposed time series to reconstruct a new set of predictor variables that can explain maximal information in the response they showed that this approach can significantly improve the performance of the regression model when applied firstly to synthetic data and a drought index downscaling case study at fifteen rainfall gauges in sydney australia however the original method is limited to prediction problems where the future state of the predictors is known which is due to the mathematical properties of the discrete wavelet transform dwt if a forecasting model is required then dwt is not suitable because this wavelet transform requires future information which is not available in a forecasting setting to predict the target response du et al 2017 quilty and adamowski 2018 to address this issue other wavelet transformations can be used to implement the variance transformation including the maximal overlap dwt modwt and à trous algorithm at in this case dwt can be replaced with modwt or at which have no dependence on future information nason 2008 quilty and adamowski 2018 therefore we have included the modwt and at based variance transformation into the wasp r package alternatively when considering climate change projections the dwt forecasting problem is overcome because reliable future projections of the predictors are available from gcms although the target response is unknown or its projection is not reliable as shown by rashid et al 2018 and fowler et al 2007 another issue in using wavelet based methods in real world applications is the edge effects resulting mainly due to limited sample sizes also known as the error due to the boundary condition that is associated with wavelet decompositions including wavelet and scaling coefficients percival and walden 2000 however there are ways to reduce the effects of boundary bias in wavelet transformations an estimator excluding the boundary coefficients is regarded as an unbiased wavelet variance estimator cornish et al 2006 this logic can be applied to the proposed variance transformation method which leads to an unbiased variance transformation thus the methodological contributions of this study are to 1 generalise the wavelet based variance transformation method to allow it to be applied in forecasting problems and 2 develop an unbiased variance transformation this substantially broadens the application of the proposed method across a wide range of systems beyond the simplified illustrations in jiang et al 2020 the approach outlined above is embodied in the wavelet system prediction wasp r package and it consists of three key functions the first function finds the optimal variance transformation for each predictor variable of interest reconstructing a new predictor with complimentary spectral attributes to the predictand the second function identifies significant reconstructed predictors using partial informational correlation pic pic is used to measure the dependence between a given response and the reconstructed new predictor conditioned to pre existing predictor s sharma 2000 sharma and mehrotra 2014 the last function is the predictive model which is a k nearest neighbor knn estimator using a kernel regression function lall and sharma 1996 sharma et al 1997 an additional contribution here is that the knn estimator has been modified to better allow for extrapolation in this study we implement the modwt based and unbiased variance transformation in the r package wasp and evaluate it on a large scale of hydro climatological system for instance sustained droughts are natural hazards associated with a range of climatic factors such as low precipitation and high temperatures and potential evapotranspiration sheffield 2011 these climatic factors are in turn affected by large scale climate teleconnections which vary over periods of years to decades e g el niño southern oscillation and india ocean dipole mishra and singh 2010 as well as long term trends from anthropogenic climate change dai 2013 sheffield and wood 2008 thus drought is a result of the interactions of a large number of variables all of which have very different spectral properties here the variance transformation method is demonstrated by modelling and predicting sustained drought anomalies for australia as represented by the standardized precipitation index spi 2 methodology 2 1 modwt based variance transformation in this section we first introduce the original dwt based variance transformation and then extend it to include the modwt based variance transformation full details and derivation of the variance transformation are provided in jiang et al 2020 a summary of the important steps is provided here consider a set of n paired centred i e with mean of zero observations of the predictor variable x and the response variable y i e x 0 y 0 x n 1 y n 1 first the signal x is decomposed into a vector of coefficients matrix w d 1 d j a j with a dimension of n 1 using the dwt the coefficients matrix is then reconstructed into a matrix of frequency components r d 1 d j a j and the associated variance structure of these sub time series is given by i σ d 1 σ d j σ a j t percival and walden 2000 this is so called multiresolution analysis mra here j is the highest decomposition level which will be further discussed in the section of unbiased variance transformation the property of dwt ensures that the sum of the variance of the sub time series equals the variance of the original time series which means j 1 j 1 i j 2 1 n 1 x t x σ x 2 accordingly x can be written as a matrix multiplication x r ˆ i with the standardized reconstruction matrix r ˆ d ˆ 1 d ˆ j a ˆ j the variance transformation is achieved by reconstructing a new predictor x with variance structure α similar to the corresponding response in the frequency domain they can be written as 1 x r ˆ α α σ x c ˆ where c ˆ is the normalized covariance of the variable set y r ˆ and the covariance c has the form of 2 c 1 n 1 y t r ˆ s y d ˆ 1 s y d ˆ j s y a ˆ j essentially the reconstructed new predictor x is obtained by redistributing the variance in its spectrum and it has the same total variance as the original predictor x all potential predictors will be reconstructed with this operation and a reconstructed new set of predictors is then used for predictor selection and response prediction assuming that the variance transformed predictor is used to predict the associated response with simple linear regression we can derive the theoretical optimal prediction accuracy as measured by root mean square error rmse 3 r m s e min n 1 n σ y 2 c 2 where σ y denotes the standard deviation of the response y the method originally proposed by jiang et al 2020 requires both additive decomposition i e mra and variance decomposition i e energy based decomposition to extend the method to consider forecasting problems the dwt can be replaced by wavelet approaches that do not include future time steps such as modwt and at however for the above derivation to be valid then the new wavelet approaches need to also fulfill the requirement for additive and variance decomposition both modwt and at fulfill these two requirements only when the haar wavelet filter equivalent to daubechies 1 db1 or d2 is adopted when the haar wavelet filter is used modwt and at are equivalent i e lead to the same decomposed frequency components therefore for forecasting applications wasp has been extended to include modwt with the haar wavelet filter as the basis for the variance transformation there is a potential risk that the spectrum of the variables of interest cannot be characterised well because the wavelet filter is limited to the haar wavelet filter however the logic can be applied to both modwt and at when other wavelet filters are adopted as they provide additional ways to characterise the spectrum of variables of interest another advantage of using modwt is that there is no restriction on the dyadic sample size briefly modwt decomposes the original time series x into a n j 1 matrix of wavelet and scaling coefficients w d 1 d j a j and the associated standard deviation matrix is given by i σ d 1 σ d j σ a j t modwt also ensures variance decomposition which means j 1 j 1 i j 2 1 n 1 x t x σ x 2 this provides a way to investigate and transform the variance structure of the coefficients matrix w directly as a result using the covariance c of the variable set y w the variance transformed x can be obtained given by the equation 4 x w α α σ x c ˆ where w is the standardized coefficients matrix w it is noted that the coefficients matrix w decomposed from dwt has the dimension of n 1 while the coefficients matrix w from modwt has a dimension of n j 1 expect for the independence on future information this is another reason the coefficients matrix of modwt can be directly used for variance transformation 2 2 unbiased variance transformation the second methodological contribution of this study is to solve the issue of boundary bias in applying wavelet based methods boundary related issues are due to sample size the choice of decomposition level as well as wavelet filter table 1 summarizes the size of the boundary effects for both types of wavelet transforms as shown in the table the number of non boundary coefficients depends on the sample size n the decomposition level j and the width of wavelet filter l the multiresolution analysis of dwt is affected at the beginning and the end of the sub time series while modwt is only affected at the start of the decomposed components it is clear that shorter wavelet filter longer time series or lower decomposition level leads to a smaller number of boundary coefficients in wavelet theory the exclusion of boundary coefficients in wavelet variance estimation is called unbiased estimator cornish et al 2006 there is a smaller difference between biased and unbiased estimates when fewer boundary coefficients need to be excluded here we propose to adopt the unbiased variance transformation by computing the covariance using only the non boundary coefficients as follows 5 c 1 n 1 y t r ˆ where the asterisk implies the unbiased value r ˆ or w is the standardized matrix excluding boundary coefficients and c is a vector of unbiased covariance it is worth noting that the unbiased estimator can only be computed for some decomposition levels however the nature of variance transformation requires greater decomposition levels thus we still use the biased estimator whenever the unbiased estimator is not available the introduction of unbiased variance transformation is not likely to change the model performance substantially when a shorter wavelet filter is used and larger sample size is available 2 3 partial informational correlation the wavelet based variance transformation approach adopts pic which takes the partial dependence between predictors and the response into account to identify significant in this case variance transformed predictors a short description of the logic behind pic is presented here and readers are referred to sharma 2000 galelli et al 2014 and sharma and mehrotra 2014 for additional details as well as to sharma et al 2016 for the software known as npred needed to estimate the pic the partial information pi is based on information theory and measures the dependence between the response y and a potential predictor x of the response given pre existing predictor s z thus a sample estimate of p i y x z is written as 6 p i ˆ y x z 1 n i 1 n log e f y z x z y i x i z i f y z y i z i f x z x i z i where y i and x i is the i th bivariate sample data pair in a sample of size n y z and x z are partial response and partial independent variable which represent the residual information in variables y and x when the effect of pre existing predictor s z has been taken into account f y z y i z f x z x i z and f y z x z y i x i z are the respective marginal and joint probability densities using kernel density estimation the pi can be scaled to the range from 0 to 1 which is introduced as the pic 7 p i c ˆ 1 exp 2 p i ˆ thus the pic is a generic measure of conditional dependence where 0 represents no dependence and 1 represents perfect dependence a measure of statistical significance for the pic is also required 8 t p i c m 1 p i c 2 where t follows the student s t distribution with m n l degrees of freedom with n being the number of observations and l the number of conditioning variables this is used for the stopping criterion when selecting the significant predictor variable s given a certain significance level p we used p 0 1 in the case study when the estimated pic is smaller than an associated threshold pic p for all the remaining partial predictors the selection process will be terminated 2 4 modified k nearest neighbor regression estimator selecting a predictive model is generally based on the nature of the modelling system as well as the modeler s experience regression methods have been widely solved by using the parametric least squares estimator approach non parametric models can also be used with the advantage that fewer assumptions about the distribution of the population are required in this study the nonparametric knn method was used for prediction the key to the knn method is to find the closest observations to x in the training dataset to form y ˆ specifically the knn fit for y ˆ is defined as follows 9 y ˆ x 1 k x i n k x y i where n k x is the neighbourhood of x defined by the k closest points x i in the training sample friedman et al 2001 the closeness is a distance metric which can be defined by euclidean distance as well as a range of alternate distance metrics weinberger et al 2006 mehrotra and sharma 2006a argue for the use of a weighted euclidean distance using a discrete kernel k i with weights estimated based on the partial importance each predictor exerts on the response in this current study a linear extrapolation of the associated response based on the covariance of the predictor response dataset was implemented this is required because when considering climate change projections future predictor values may exceed the range of the observed data over which the knn model is trained this subtle modification is based on the kernel regression as described by sharma et al 1997 10 y ˆ x 1 k i x i n k x y i s x y t s x x 1 x i x k i 1 i i 1 k 1 i where s xy and s xx represent the covariance matrix for the variable set x y and x x respectively 3 wasp r package structure 3 1 details of the software fig 1 is a flowchart of the proposed method showing the general process that is required for the variance transformation technique this algorithm is implemented in the r library wasp software a detailed help file for each function and test data are provided in the package as well in summary the r package consists of built in functions for variance transformation operation for calibration dwt vt modwt vt and at vt and validation dwt vt val modwt vt val and at vt val based on dwt modwt and at respectively the option of unbiased variance transformation for each variance transformation method is included in these functions with flag c biased unbiased and the modified knn regression predictive model knn there are several supplementary functions including padding function padding which extends the data to provide a dyadic sample size for the dwt based variance transformation and three synthetic data generator functions used in jiang et al 2020 each of these codes come with associated help files that provide guidance on their use as described in the following section datasets from the drought prediction case study are provided in the package and all the results reported in this paper are reproducible using rmarkdown provided in the vignettes of this r package fig 2 is a screenshot of the sequence of r commands illustrating the usage of the wasp package to transform the potential predictors see figure s 1 in the supporting material for an example of predictor variables before and after variance transformation corresponding to the response identify the significant predictors and predict the associated response modwt is adopted as the basis of wavelet transform in this case study since we are using observed data to predict target response and thus there is no dependence on future information all codes and data in the package are open source it should be noted that when applying this method to forecast a future response we assume that the conditional dependence between the predictor variables and the response remains unchanged into the future thus the covariance between the response and wavelet decompositions of predictor variables from historical data is used for future predictions as well as the fitted predictive model to check the validity of this assumption we use cross validation by partitioning the historical data into four complementary subsets one subset is used as the validation set while other subsets are used as the calibration set the results presented hereafter are cross validated results for the entire period the rationale for using cross validation is that we can have a better assessment of the model performance with independent datasets mehrotra and sharma 2006b nguyen et al 2019 it is important to note however that in the context of anthropogenic climate change the range of future changes will likely exceed those observed in the past so the cross validation is not a perfect test of our stationarity assumption for the predictor response dependence structure 3 2 prediction of standardized precipitation index over australia the wasp package was applied to predict the spi using various climate indicators over australia to assess the impact the variance transformation makes we adopted climate indicators used in previous prediction of sustained hydrologic anomalies using the spi rashid et al 2020 and further expanded this dataset by including additional climate drivers strongly influencing australia climate cai and cowan 2013 kirono et al 2010 murphy and timbal 2008 table 2 lists the details of the climate indices considered in this study the monthly anomalies of nino3 4 pdo and dmi are derived from monthly sea surface temperature sst values of hadley centre global ice and sea surface temperature hadisst datasets rayner et al 2003 sam is calculated using sea level pressure slp from noaa earth system research laboratory s physical sciences division psd the australian water availability project awap gridded monthly rainfall metadata is obtained from the bureau of meteorology jones et al 2009 and is regarded as observations the rainfall data was re gridded to 2 5 2 5 over australia using weighted area average and the spi for 12 month and 36 month periods spi12 spi36 is calculated mckee et al 1993 note that grid cells where more than 25 of rainfall values are zero or missing are removed from the calculations due to data reliability concerns spinoni et al 2014 as described previously we split the data into four equal subsets for cross validation the study period was 1910 2009 first of all significant climate indicators were identified at each rainfall grid over australia using pic from the set of four variance transformed climate indices in fig 3 the most significant drivers i e the most frequently selected predictor in the pic process among the four cross validation subsets for both spi12 and spi36 are shown in addition four randomly chosen grids that are used to examine the results in detail in this study are highlighted with grid index numbers in red color refer to figure s 2 in the supporting material for the complete overview of grid index over australia table 3 summarizes the most significant climatic driver selection using both original and variance transformed vt climate indices see figure s 3 in the supporting material for the selection results using original climate indices as expected based on previous research drought in australia is significantly influenced by el niño southern oscillation enso cai et al 2011 fierro and leslie 2013 pui et al 2012 westra and sharma 2010 for spi12 most grids 83 are influenced by enso and more than half of grids 89 are sensitive to enso for spi36 on the other hand the selection results using original climate indices are similar i e enso is the main climatic driver of australia rainfall with less grids affected by enso particularly for spi36 one interesting observation is that there are several grids where no climate drivers are identified as useful for prediction if the original untransformed climate indices are used because of the discrepancies in the temporal scale of the response and the potential predictors this demonstrates the advantage of variance transformation technique in selecting predictor variables jiang et al 2020 another interesting outcome is that the use of variance transformation leads to a reduced selection of non pacific variability indicators such as dmi and sam in the resulting model with these variables being relegated to second or higher order predictors in the ensuing model fig 4 a and c present the density plots of observed predicted and predicted with variance transformation spi at the four randomly sampled grids it is clear that the probability distributions of predicted spi using variance transformed predictors are closer to observed spi in the sampled grids its closeness can also be measured by the pdf skill scores perkins et al 2007 which are shown in fig 4 b and d the value of a pdf skill score ranges between 0 and 1 and 1 represents a perfect match these results suggest that the wavelet based approach can capture sustained drought wet anomalies well a close look at the selection results in table 4 provides more information about the benefits of the proposed method first additional climate indices can be selected which is likely to result in considerable improvements at all grids second even when the same predictor variables are selected as the case in grid 94 for both spi12 and spi36 the variance transformation leads to improved characterisation of sustained anomalies only a small improvement is observed at grid 142 for spi12 after applying variance transformation because at this location reasonably good skill was obtained from the original predictors in fig 5 a and c the improvement in pdf skill scores percentage relative to non wavelet models for both spi12 and spi36 over australia is presented the wavelet based method provides improvements at around 99 and 97 of grids for spi12 and spi36 respectively grids with white areas represent grids with missing data located in the central and western desert of australia jones et al 2009 while grids with black dots refer to locations with no improvements after variance transformation is used further scatterplots in fig 5 b and d provide the magnitude of pdf skill scores at all grids over australia and the model using the proposed variance transformation technique outperforms the reference model using original climate indices the biggest improvements tending to occur for locations that had lower skill with the non wavelet model consistent with the results discussed above for grid 142 it is noted that the improvements in prediction performance of spi36 are larger than for spi12 which results from possibly identifying and characterising one of the known major drivers of droughts in australia i e enso using variance transformed climate indices what we have shown here represents the results of the modwt based biased variance transformation with the results using the unbiased estimator being given in figure s 4 of the supporting material in addition boxplots in fig 6 compare the model performance between approaches using biased and unbiased estimator first the unbiased variance transformation does show improved prediction accuracy with all grids presenting improvements while with the biased variance transformation there are several grids perform worse than the reference model second the unbiased variance transformation shows better mean statistics in both drought indices with greater improvements in spi36 there is no significant difference in the two which is due to the fact that the haar wavelet filter has been used and large sample size is available in this case study meanwhile we have also done the experiments under two folds cross validation with varying wavelet filter length seen in table 5 the results of pdf skill scores show that first the unbiased variance transformation approach outperform its alternative in both mean and median statistics second larger differences between two estimators are observed when we adopt wider wavelet filters in both mean and median statistics given the similar standard deviation sd across all grids it should be noted that there is an exception when using d8 for spi12 median and spi36 mean prediction the difference of statistic gets smaller which is likely due to the violation of additive decomposition when other wavelet filters are adopted however the results we show here confirm the argument that modwt or at can be applied as the basis for variance transformation even when wavelet filters other than the haar are adopted 4 summary and conclusions the open source wasp r package contains the codes sample datasets and help files for natural system prediction we introduce the modwt based variance transformation which resolves the issues of future dependence moreover the boundary related bias is addressed using a newly proposed unbiased variance transformation both improvements have broadened the application of wavelet based variance transformation method the use of the wavelet based variance transformation technique is demonstrated by predicting a drought index over australia using various climate indices but the logic represents a generic approach not limited to modelling hydro climatological systems alone this approach has shown substantial improvements in predictive accuracy especially in systems where the response and plausible predictor variables have large differences in their spectrums it is worth noting that this method provides a way to predict a target response in a complex system without making assumptions and simplifications including characterising the form of the underlying model that relates the two this is implicitly undertaken by the variance transformation technique thereby formulating a transformed predictor that can be expected to have a concurrent relationship with the response ensuring improvement of predictivity in the complex system however the proposed approach has its inherent limitations and should be applied with care first the boundary related bias is a curse thus the selection of wavelet family and the length of filters should be realistic given the nature of the physical phenomenon with varying data length bakshi 1999 maheswaran and khosa 2012 percival and walden 2000 torrence and compo 1998 in addition the rule of thumb of the decomposition level by kaiser 2010 is preferred such that the variance transformation is done across the entire spectrum of the predictor variables jiang et al 2020 lastly while the logic presented here focusses on the modelling of a single response extensions to modelling multiple responses are possible future extensions of the proposed logic will illustrate how we can extend the approach here to multiple response variables while keeping the dimensionality of the predictive system small enough to maintain robustness in predictions software availability the open source r package wasp is available for download from the following website http www hydrology unsw edu au software wasp and results in this work are fully reproducible through the rmarkdown in the vignettes of this r package source codes are available along with help files and example datasets used to generate the outcomes reported declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this research was funded by the australian research council linkage grant lp150100548 and crown lands water division department of industry nsw australia monthly nino3 4 pdo and dmi are derived from monthly sst values of hadley centre global ice and seal surface temperature hadisst dataset sam is calculated using slp from noaa earth system research laboratory s physical sciences division psd gridded rainfall data is obtained from the australian water availability project awap led by the bureau of meteorology assistance from raj mehrotra in preparing the datasets used over australia and instructions from jingwan li on building r library are gratefully acknowledged we are also thankful to two anonymous referees professor holger maier and the editor for their constructive comments appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104907 
25914,this work presents an open source tool to predict natural system responses by transforming the frequency spectrum of predictor variables to create a response that better resembles observations the r package namely wavelet system prediction wasp is based on a discrete wavelet transform dwt based variance transformation method we further introduce the maximal overlap dwt modwt based variance transformation which allows the method to be used in forecasting applications we also develop the method to include an unbiased estimator that mitigates the well known issue of edge effects in wavelet transforms the predictive model in the method is a k nearest neighbor knn approach the main functionalities of the software include 1 transforming the system predictors 2 identifying significant predictors corresponding to the response 3 predicting target response using the knn results of predicting sustained drought anomalies across australia show clear improvements in predictive skill compared to the use of untransformed predictors keywords wavelet system prediction r package maximal overlap discrete wavelet transform unbiased estimator 1 introduction a regression model describes the relationship between a system response and a finite set of predictor variables using an assumed modelling form linear or nonlinear approaches can range from simple regression models using a range of physically justifiable predictor variables hertig and tramblay 2017 to those where more complex transformations including rotations are adopted jiang et al 2019 ndehedehe et al 2016 differing spectral attributes in a response and a system predictor can complicate specifying the system prediction model we present here an approach that addresses this difficulty by optimally transforming each predictor variable to better characterise the spectrum of the response being modelled the underlying idea behind the approach here is to improve the modelling of natural systems where the potential predictor variables vary at time scales that differ from those of the plausible response for example in hydrology daily precipitation is used to predict catchment streamflow however attenuation from catchment storage means that at short time scales the variability of streamflow is substantially dampened compared to rainfall thus conventional regression modelling approaches can have difficulties in characterising this differing variability and formulating a relationship rashid and beecham 2019 although the approach is generic and can be used for any natural system model our specific focus is on hydro climatological systems an example of such a prediction problem is the need to assess changes to natural systems due to climate change in this case earth system models and or general circulation models gcms provide predictors that can be used to model future changes in hydrological variables alternatives to transfer the modelling problem into the frequency domain include methods such as fourier and wavelet transforms our approach uses wavelet theory to formulate an optimal model of the system to improve the assessment of changes into the future the wavelet transform wt is adopted in the approach to avoid loss of temporal information when transforming to the frequency domain using a fourier transform daubechies 1990 strang 1996 torrence and compo 1998 the wt can decompose the original time series into separate large scale slowly changing low frequency and fine scale rapidly changing details high frequency time series a number of models based on frequency domain analysis have been proposed recently to simulate and predict the variability in the response fahimi yaseen el shafie 2017 nguyen et al 2019 quilty et al 2019 rashid et al 2018 sang 2013 most of those applications directly use the decomposed time series to forecast the target response quilty and adamowski 2018 rashid et al 2016 jiang sharma and johnson 2020 proposed a new approach by using the decomposed time series to reconstruct a new set of predictor variables that can explain maximal information in the response they showed that this approach can significantly improve the performance of the regression model when applied firstly to synthetic data and a drought index downscaling case study at fifteen rainfall gauges in sydney australia however the original method is limited to prediction problems where the future state of the predictors is known which is due to the mathematical properties of the discrete wavelet transform dwt if a forecasting model is required then dwt is not suitable because this wavelet transform requires future information which is not available in a forecasting setting to predict the target response du et al 2017 quilty and adamowski 2018 to address this issue other wavelet transformations can be used to implement the variance transformation including the maximal overlap dwt modwt and à trous algorithm at in this case dwt can be replaced with modwt or at which have no dependence on future information nason 2008 quilty and adamowski 2018 therefore we have included the modwt and at based variance transformation into the wasp r package alternatively when considering climate change projections the dwt forecasting problem is overcome because reliable future projections of the predictors are available from gcms although the target response is unknown or its projection is not reliable as shown by rashid et al 2018 and fowler et al 2007 another issue in using wavelet based methods in real world applications is the edge effects resulting mainly due to limited sample sizes also known as the error due to the boundary condition that is associated with wavelet decompositions including wavelet and scaling coefficients percival and walden 2000 however there are ways to reduce the effects of boundary bias in wavelet transformations an estimator excluding the boundary coefficients is regarded as an unbiased wavelet variance estimator cornish et al 2006 this logic can be applied to the proposed variance transformation method which leads to an unbiased variance transformation thus the methodological contributions of this study are to 1 generalise the wavelet based variance transformation method to allow it to be applied in forecasting problems and 2 develop an unbiased variance transformation this substantially broadens the application of the proposed method across a wide range of systems beyond the simplified illustrations in jiang et al 2020 the approach outlined above is embodied in the wavelet system prediction wasp r package and it consists of three key functions the first function finds the optimal variance transformation for each predictor variable of interest reconstructing a new predictor with complimentary spectral attributes to the predictand the second function identifies significant reconstructed predictors using partial informational correlation pic pic is used to measure the dependence between a given response and the reconstructed new predictor conditioned to pre existing predictor s sharma 2000 sharma and mehrotra 2014 the last function is the predictive model which is a k nearest neighbor knn estimator using a kernel regression function lall and sharma 1996 sharma et al 1997 an additional contribution here is that the knn estimator has been modified to better allow for extrapolation in this study we implement the modwt based and unbiased variance transformation in the r package wasp and evaluate it on a large scale of hydro climatological system for instance sustained droughts are natural hazards associated with a range of climatic factors such as low precipitation and high temperatures and potential evapotranspiration sheffield 2011 these climatic factors are in turn affected by large scale climate teleconnections which vary over periods of years to decades e g el niño southern oscillation and india ocean dipole mishra and singh 2010 as well as long term trends from anthropogenic climate change dai 2013 sheffield and wood 2008 thus drought is a result of the interactions of a large number of variables all of which have very different spectral properties here the variance transformation method is demonstrated by modelling and predicting sustained drought anomalies for australia as represented by the standardized precipitation index spi 2 methodology 2 1 modwt based variance transformation in this section we first introduce the original dwt based variance transformation and then extend it to include the modwt based variance transformation full details and derivation of the variance transformation are provided in jiang et al 2020 a summary of the important steps is provided here consider a set of n paired centred i e with mean of zero observations of the predictor variable x and the response variable y i e x 0 y 0 x n 1 y n 1 first the signal x is decomposed into a vector of coefficients matrix w d 1 d j a j with a dimension of n 1 using the dwt the coefficients matrix is then reconstructed into a matrix of frequency components r d 1 d j a j and the associated variance structure of these sub time series is given by i σ d 1 σ d j σ a j t percival and walden 2000 this is so called multiresolution analysis mra here j is the highest decomposition level which will be further discussed in the section of unbiased variance transformation the property of dwt ensures that the sum of the variance of the sub time series equals the variance of the original time series which means j 1 j 1 i j 2 1 n 1 x t x σ x 2 accordingly x can be written as a matrix multiplication x r ˆ i with the standardized reconstruction matrix r ˆ d ˆ 1 d ˆ j a ˆ j the variance transformation is achieved by reconstructing a new predictor x with variance structure α similar to the corresponding response in the frequency domain they can be written as 1 x r ˆ α α σ x c ˆ where c ˆ is the normalized covariance of the variable set y r ˆ and the covariance c has the form of 2 c 1 n 1 y t r ˆ s y d ˆ 1 s y d ˆ j s y a ˆ j essentially the reconstructed new predictor x is obtained by redistributing the variance in its spectrum and it has the same total variance as the original predictor x all potential predictors will be reconstructed with this operation and a reconstructed new set of predictors is then used for predictor selection and response prediction assuming that the variance transformed predictor is used to predict the associated response with simple linear regression we can derive the theoretical optimal prediction accuracy as measured by root mean square error rmse 3 r m s e min n 1 n σ y 2 c 2 where σ y denotes the standard deviation of the response y the method originally proposed by jiang et al 2020 requires both additive decomposition i e mra and variance decomposition i e energy based decomposition to extend the method to consider forecasting problems the dwt can be replaced by wavelet approaches that do not include future time steps such as modwt and at however for the above derivation to be valid then the new wavelet approaches need to also fulfill the requirement for additive and variance decomposition both modwt and at fulfill these two requirements only when the haar wavelet filter equivalent to daubechies 1 db1 or d2 is adopted when the haar wavelet filter is used modwt and at are equivalent i e lead to the same decomposed frequency components therefore for forecasting applications wasp has been extended to include modwt with the haar wavelet filter as the basis for the variance transformation there is a potential risk that the spectrum of the variables of interest cannot be characterised well because the wavelet filter is limited to the haar wavelet filter however the logic can be applied to both modwt and at when other wavelet filters are adopted as they provide additional ways to characterise the spectrum of variables of interest another advantage of using modwt is that there is no restriction on the dyadic sample size briefly modwt decomposes the original time series x into a n j 1 matrix of wavelet and scaling coefficients w d 1 d j a j and the associated standard deviation matrix is given by i σ d 1 σ d j σ a j t modwt also ensures variance decomposition which means j 1 j 1 i j 2 1 n 1 x t x σ x 2 this provides a way to investigate and transform the variance structure of the coefficients matrix w directly as a result using the covariance c of the variable set y w the variance transformed x can be obtained given by the equation 4 x w α α σ x c ˆ where w is the standardized coefficients matrix w it is noted that the coefficients matrix w decomposed from dwt has the dimension of n 1 while the coefficients matrix w from modwt has a dimension of n j 1 expect for the independence on future information this is another reason the coefficients matrix of modwt can be directly used for variance transformation 2 2 unbiased variance transformation the second methodological contribution of this study is to solve the issue of boundary bias in applying wavelet based methods boundary related issues are due to sample size the choice of decomposition level as well as wavelet filter table 1 summarizes the size of the boundary effects for both types of wavelet transforms as shown in the table the number of non boundary coefficients depends on the sample size n the decomposition level j and the width of wavelet filter l the multiresolution analysis of dwt is affected at the beginning and the end of the sub time series while modwt is only affected at the start of the decomposed components it is clear that shorter wavelet filter longer time series or lower decomposition level leads to a smaller number of boundary coefficients in wavelet theory the exclusion of boundary coefficients in wavelet variance estimation is called unbiased estimator cornish et al 2006 there is a smaller difference between biased and unbiased estimates when fewer boundary coefficients need to be excluded here we propose to adopt the unbiased variance transformation by computing the covariance using only the non boundary coefficients as follows 5 c 1 n 1 y t r ˆ where the asterisk implies the unbiased value r ˆ or w is the standardized matrix excluding boundary coefficients and c is a vector of unbiased covariance it is worth noting that the unbiased estimator can only be computed for some decomposition levels however the nature of variance transformation requires greater decomposition levels thus we still use the biased estimator whenever the unbiased estimator is not available the introduction of unbiased variance transformation is not likely to change the model performance substantially when a shorter wavelet filter is used and larger sample size is available 2 3 partial informational correlation the wavelet based variance transformation approach adopts pic which takes the partial dependence between predictors and the response into account to identify significant in this case variance transformed predictors a short description of the logic behind pic is presented here and readers are referred to sharma 2000 galelli et al 2014 and sharma and mehrotra 2014 for additional details as well as to sharma et al 2016 for the software known as npred needed to estimate the pic the partial information pi is based on information theory and measures the dependence between the response y and a potential predictor x of the response given pre existing predictor s z thus a sample estimate of p i y x z is written as 6 p i ˆ y x z 1 n i 1 n log e f y z x z y i x i z i f y z y i z i f x z x i z i where y i and x i is the i th bivariate sample data pair in a sample of size n y z and x z are partial response and partial independent variable which represent the residual information in variables y and x when the effect of pre existing predictor s z has been taken into account f y z y i z f x z x i z and f y z x z y i x i z are the respective marginal and joint probability densities using kernel density estimation the pi can be scaled to the range from 0 to 1 which is introduced as the pic 7 p i c ˆ 1 exp 2 p i ˆ thus the pic is a generic measure of conditional dependence where 0 represents no dependence and 1 represents perfect dependence a measure of statistical significance for the pic is also required 8 t p i c m 1 p i c 2 where t follows the student s t distribution with m n l degrees of freedom with n being the number of observations and l the number of conditioning variables this is used for the stopping criterion when selecting the significant predictor variable s given a certain significance level p we used p 0 1 in the case study when the estimated pic is smaller than an associated threshold pic p for all the remaining partial predictors the selection process will be terminated 2 4 modified k nearest neighbor regression estimator selecting a predictive model is generally based on the nature of the modelling system as well as the modeler s experience regression methods have been widely solved by using the parametric least squares estimator approach non parametric models can also be used with the advantage that fewer assumptions about the distribution of the population are required in this study the nonparametric knn method was used for prediction the key to the knn method is to find the closest observations to x in the training dataset to form y ˆ specifically the knn fit for y ˆ is defined as follows 9 y ˆ x 1 k x i n k x y i where n k x is the neighbourhood of x defined by the k closest points x i in the training sample friedman et al 2001 the closeness is a distance metric which can be defined by euclidean distance as well as a range of alternate distance metrics weinberger et al 2006 mehrotra and sharma 2006a argue for the use of a weighted euclidean distance using a discrete kernel k i with weights estimated based on the partial importance each predictor exerts on the response in this current study a linear extrapolation of the associated response based on the covariance of the predictor response dataset was implemented this is required because when considering climate change projections future predictor values may exceed the range of the observed data over which the knn model is trained this subtle modification is based on the kernel regression as described by sharma et al 1997 10 y ˆ x 1 k i x i n k x y i s x y t s x x 1 x i x k i 1 i i 1 k 1 i where s xy and s xx represent the covariance matrix for the variable set x y and x x respectively 3 wasp r package structure 3 1 details of the software fig 1 is a flowchart of the proposed method showing the general process that is required for the variance transformation technique this algorithm is implemented in the r library wasp software a detailed help file for each function and test data are provided in the package as well in summary the r package consists of built in functions for variance transformation operation for calibration dwt vt modwt vt and at vt and validation dwt vt val modwt vt val and at vt val based on dwt modwt and at respectively the option of unbiased variance transformation for each variance transformation method is included in these functions with flag c biased unbiased and the modified knn regression predictive model knn there are several supplementary functions including padding function padding which extends the data to provide a dyadic sample size for the dwt based variance transformation and three synthetic data generator functions used in jiang et al 2020 each of these codes come with associated help files that provide guidance on their use as described in the following section datasets from the drought prediction case study are provided in the package and all the results reported in this paper are reproducible using rmarkdown provided in the vignettes of this r package fig 2 is a screenshot of the sequence of r commands illustrating the usage of the wasp package to transform the potential predictors see figure s 1 in the supporting material for an example of predictor variables before and after variance transformation corresponding to the response identify the significant predictors and predict the associated response modwt is adopted as the basis of wavelet transform in this case study since we are using observed data to predict target response and thus there is no dependence on future information all codes and data in the package are open source it should be noted that when applying this method to forecast a future response we assume that the conditional dependence between the predictor variables and the response remains unchanged into the future thus the covariance between the response and wavelet decompositions of predictor variables from historical data is used for future predictions as well as the fitted predictive model to check the validity of this assumption we use cross validation by partitioning the historical data into four complementary subsets one subset is used as the validation set while other subsets are used as the calibration set the results presented hereafter are cross validated results for the entire period the rationale for using cross validation is that we can have a better assessment of the model performance with independent datasets mehrotra and sharma 2006b nguyen et al 2019 it is important to note however that in the context of anthropogenic climate change the range of future changes will likely exceed those observed in the past so the cross validation is not a perfect test of our stationarity assumption for the predictor response dependence structure 3 2 prediction of standardized precipitation index over australia the wasp package was applied to predict the spi using various climate indicators over australia to assess the impact the variance transformation makes we adopted climate indicators used in previous prediction of sustained hydrologic anomalies using the spi rashid et al 2020 and further expanded this dataset by including additional climate drivers strongly influencing australia climate cai and cowan 2013 kirono et al 2010 murphy and timbal 2008 table 2 lists the details of the climate indices considered in this study the monthly anomalies of nino3 4 pdo and dmi are derived from monthly sea surface temperature sst values of hadley centre global ice and sea surface temperature hadisst datasets rayner et al 2003 sam is calculated using sea level pressure slp from noaa earth system research laboratory s physical sciences division psd the australian water availability project awap gridded monthly rainfall metadata is obtained from the bureau of meteorology jones et al 2009 and is regarded as observations the rainfall data was re gridded to 2 5 2 5 over australia using weighted area average and the spi for 12 month and 36 month periods spi12 spi36 is calculated mckee et al 1993 note that grid cells where more than 25 of rainfall values are zero or missing are removed from the calculations due to data reliability concerns spinoni et al 2014 as described previously we split the data into four equal subsets for cross validation the study period was 1910 2009 first of all significant climate indicators were identified at each rainfall grid over australia using pic from the set of four variance transformed climate indices in fig 3 the most significant drivers i e the most frequently selected predictor in the pic process among the four cross validation subsets for both spi12 and spi36 are shown in addition four randomly chosen grids that are used to examine the results in detail in this study are highlighted with grid index numbers in red color refer to figure s 2 in the supporting material for the complete overview of grid index over australia table 3 summarizes the most significant climatic driver selection using both original and variance transformed vt climate indices see figure s 3 in the supporting material for the selection results using original climate indices as expected based on previous research drought in australia is significantly influenced by el niño southern oscillation enso cai et al 2011 fierro and leslie 2013 pui et al 2012 westra and sharma 2010 for spi12 most grids 83 are influenced by enso and more than half of grids 89 are sensitive to enso for spi36 on the other hand the selection results using original climate indices are similar i e enso is the main climatic driver of australia rainfall with less grids affected by enso particularly for spi36 one interesting observation is that there are several grids where no climate drivers are identified as useful for prediction if the original untransformed climate indices are used because of the discrepancies in the temporal scale of the response and the potential predictors this demonstrates the advantage of variance transformation technique in selecting predictor variables jiang et al 2020 another interesting outcome is that the use of variance transformation leads to a reduced selection of non pacific variability indicators such as dmi and sam in the resulting model with these variables being relegated to second or higher order predictors in the ensuing model fig 4 a and c present the density plots of observed predicted and predicted with variance transformation spi at the four randomly sampled grids it is clear that the probability distributions of predicted spi using variance transformed predictors are closer to observed spi in the sampled grids its closeness can also be measured by the pdf skill scores perkins et al 2007 which are shown in fig 4 b and d the value of a pdf skill score ranges between 0 and 1 and 1 represents a perfect match these results suggest that the wavelet based approach can capture sustained drought wet anomalies well a close look at the selection results in table 4 provides more information about the benefits of the proposed method first additional climate indices can be selected which is likely to result in considerable improvements at all grids second even when the same predictor variables are selected as the case in grid 94 for both spi12 and spi36 the variance transformation leads to improved characterisation of sustained anomalies only a small improvement is observed at grid 142 for spi12 after applying variance transformation because at this location reasonably good skill was obtained from the original predictors in fig 5 a and c the improvement in pdf skill scores percentage relative to non wavelet models for both spi12 and spi36 over australia is presented the wavelet based method provides improvements at around 99 and 97 of grids for spi12 and spi36 respectively grids with white areas represent grids with missing data located in the central and western desert of australia jones et al 2009 while grids with black dots refer to locations with no improvements after variance transformation is used further scatterplots in fig 5 b and d provide the magnitude of pdf skill scores at all grids over australia and the model using the proposed variance transformation technique outperforms the reference model using original climate indices the biggest improvements tending to occur for locations that had lower skill with the non wavelet model consistent with the results discussed above for grid 142 it is noted that the improvements in prediction performance of spi36 are larger than for spi12 which results from possibly identifying and characterising one of the known major drivers of droughts in australia i e enso using variance transformed climate indices what we have shown here represents the results of the modwt based biased variance transformation with the results using the unbiased estimator being given in figure s 4 of the supporting material in addition boxplots in fig 6 compare the model performance between approaches using biased and unbiased estimator first the unbiased variance transformation does show improved prediction accuracy with all grids presenting improvements while with the biased variance transformation there are several grids perform worse than the reference model second the unbiased variance transformation shows better mean statistics in both drought indices with greater improvements in spi36 there is no significant difference in the two which is due to the fact that the haar wavelet filter has been used and large sample size is available in this case study meanwhile we have also done the experiments under two folds cross validation with varying wavelet filter length seen in table 5 the results of pdf skill scores show that first the unbiased variance transformation approach outperform its alternative in both mean and median statistics second larger differences between two estimators are observed when we adopt wider wavelet filters in both mean and median statistics given the similar standard deviation sd across all grids it should be noted that there is an exception when using d8 for spi12 median and spi36 mean prediction the difference of statistic gets smaller which is likely due to the violation of additive decomposition when other wavelet filters are adopted however the results we show here confirm the argument that modwt or at can be applied as the basis for variance transformation even when wavelet filters other than the haar are adopted 4 summary and conclusions the open source wasp r package contains the codes sample datasets and help files for natural system prediction we introduce the modwt based variance transformation which resolves the issues of future dependence moreover the boundary related bias is addressed using a newly proposed unbiased variance transformation both improvements have broadened the application of wavelet based variance transformation method the use of the wavelet based variance transformation technique is demonstrated by predicting a drought index over australia using various climate indices but the logic represents a generic approach not limited to modelling hydro climatological systems alone this approach has shown substantial improvements in predictive accuracy especially in systems where the response and plausible predictor variables have large differences in their spectrums it is worth noting that this method provides a way to predict a target response in a complex system without making assumptions and simplifications including characterising the form of the underlying model that relates the two this is implicitly undertaken by the variance transformation technique thereby formulating a transformed predictor that can be expected to have a concurrent relationship with the response ensuring improvement of predictivity in the complex system however the proposed approach has its inherent limitations and should be applied with care first the boundary related bias is a curse thus the selection of wavelet family and the length of filters should be realistic given the nature of the physical phenomenon with varying data length bakshi 1999 maheswaran and khosa 2012 percival and walden 2000 torrence and compo 1998 in addition the rule of thumb of the decomposition level by kaiser 2010 is preferred such that the variance transformation is done across the entire spectrum of the predictor variables jiang et al 2020 lastly while the logic presented here focusses on the modelling of a single response extensions to modelling multiple responses are possible future extensions of the proposed logic will illustrate how we can extend the approach here to multiple response variables while keeping the dimensionality of the predictive system small enough to maintain robustness in predictions software availability the open source r package wasp is available for download from the following website http www hydrology unsw edu au software wasp and results in this work are fully reproducible through the rmarkdown in the vignettes of this r package source codes are available along with help files and example datasets used to generate the outcomes reported declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this research was funded by the australian research council linkage grant lp150100548 and crown lands water division department of industry nsw australia monthly nino3 4 pdo and dmi are derived from monthly sst values of hadley centre global ice and seal surface temperature hadisst dataset sam is calculated using slp from noaa earth system research laboratory s physical sciences division psd gridded rainfall data is obtained from the australian water availability project awap led by the bureau of meteorology assistance from raj mehrotra in preparing the datasets used over australia and instructions from jingwan li on building r library are gratefully acknowledged we are also thankful to two anonymous referees professor holger maier and the editor for their constructive comments appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104907 
