index,text
25855,diagnostic testing is an oft recommended use of sensitivity analysis to assess correctness or plausibility of model behavior in this paper we demonstrate the use of sensitivity analysis as a complementary first pass software test for the validation of model behavior typical testing processes rely on comparing model outputs to results known to be correct such approaches are limited to specific model configurations and require that correct results be known in advance property based sensitivity analysis pbsa examines model properties in terms of the behavior of parameter sensitivities to provide a line of evidence that the expected conceptual relationships between model factors and their effects are present unanticipated results can indicate issues to be corrected the pbsa approach is also scalable as it can complement existing testing practices and be applied in conjunction with global sensitivity methods that can reuse existing model evaluations or are otherwise independent of the sampling scheme keywords integrated development cycle diagnostic testing sensitivity analysis parameter sensitivity parameter inactivity integrated environmental model 1 introduction integrated environmental models iems are often developed to inform policy and management processes in the problem realm of socio environmental systems ses such integrated models account for multiple sectoral influences and their interactions including the biophysical e g hydrological climate ecological and agriculture and socio economic processes e g human drivers economy market policy and legislative interactions multiple models both purpose built and pre existing i e legacy models kelly letcher et al 2013 are often coupled to represent this system of systems typical iem development conceptualizes an iterative cyclic process in which an interdisciplinary team of teams collaborates to appropriately represent the interactions across the ses being modeled hamilton et al 2015 little et al 2019 the development process is such that the suite of models that constitute an iem and their coupling are in a state of flux with each undergoing a separate iterative development cycle changes to one model component may necessitate changes in another and there will be emergent behaviors that arise only when models are integrated the modeler s responsible for integrating the disparate models involved is the foundation for ensuring that the constituent models and the resulting iem are both technically and conceptually sound lest usability of the iem and confidence in the results be compromised voinov and shugart 2013 compounding matters is the fact that iems are increasingly being operated at grander scales in terms of the number of systems represented the breadth of researchers and interest groups involved and consequently the required computational infrastructure budget and time available elsawah et al 2020 little et al 2019 the resulting iem may have hundreds possibly even thousands of parameters models external to each discipline or sectoral component are often treated as black or at best gray boxes given the spread of domain specific knowledge required to understand in full technical detail the models representing the system of systems consequently no single person is likely to have a full and complete understanding of the models involved given the complex and complicated context of ses modeling and the pace at which iem development occurs the cost of correcting errors that may inadvertently creep in may increase as time progresses boehm 1986 the associated opportunity cost may be substantial and so it is desirable for any issue to be identified and corrected as early in the development cycle as possible continuous and repeated testing of the models and their integration therefore plays an important part in the model development cycle in the development of software testing is leveraged to gain confidence that the underlying code is working as intended and continues to do so throughout the rapid pace of iterative development danglot et al 2020 a failing test then falsifies the assumption that the software is working correctly researchers in the field of sensitivity analysis sa have independently arrived at the idea of diagnostic evaluation estimated sensitivities of parameters are used to provide some validation that the model behavior is in line with expectations campolongo et al 2011 gupta et al 2008 pianosi et al 2016 such diagnostic approaches have been recognized as vital for maximizing the capabilities of mathematical models rabitz 1989 due to the computational demands of iems results from diagnostic sa may be effective as once off analyses yet take an excessive amount of time relative to the computational time and budget available for continuous testing purposes given the context of rapid iteration and high complexity of iem development there is a need for a diagnostic process that aids in the quick and early identification of issues throughout the integrated modeling process in this paper we showcase how a simple and computationally inexpensive sa based on one at a time oat sensitivity analyses applied in the frame of software testing can be a complementary strategy in identifying model implementation and integration issues early in the modeling cycle the approach which we refer to as property based sensitivity analysis pbsa can help expose issues in the course of building or integrating models by exploiting expected and unexpected sensitivity of parameters these are used as indicators to confirm the expected model behavior in areas of parameter space with known model behaviors in the following sections we briefly introduce software testing practices contextualized by the integrated model development context section 2 and explore its conceptual linkages with diagnostic sensitivity analysis section 3 and 4 we then provide an illustrative example in section 5 using the campaspe integrated model cim an integrated model developed to explore sustainable water management futures within an agricultural setting in the lower campaspe catchment of victoria australia iwanaga et al 2020a we then conclude in section 6 with a discussion on directions for future research 2 software testing in integrated model development computational models are software in that they are implemented as code and are run on computers although there are clear similarities perhaps even identicalities between software and model development model testing and development practices that are common in software production may not be readily adopted crouch et al 2013 hutton et al 2016 sletholt et al 2012 in fact publications have been retracted in the past for errors that software testing practices would have assisted in identifying ahalt et al 2014 bhandari neupane et al 2019 kanewala and bieman 2014 in this section we briefly introduce the concept of unit testing and the practice of property based testing it has long been recognized that issues are easier and cheaper to address if they are identified earlier in the development process levin et al 2019 mossalam 2018 a core aim of software testing is to reduce the time taken to reach a stable working piece of software in this case a model by aiding in the identification of issues as early as possible in the development workflow see fig 1 developers write code to ensure the correct functionality of other code to accomplish this aim such code are referred to collectively as tests a common type of software test is referred to as a unit test as it tests an arbitrary but preferably small unit of code against a specific known result sarma et al 2016 unit tests support the development process by providing indications that the model is working in line with expectations frequent re running of these tests e g after every change shorten the time between changes to the code and identification of issues thereby smoothing the model development cycle one issue is that identifying the correct behavior to test may be challenging in cases where the effects of model interactions may not be fully understood as in the iem context running of tests can be automated verweij et al 2010 such that a collection of unit and property based tests could then form a regression and or integration test suite regression tests help alert developers to the unintentional re introduction of issues that may have been previously addressed during model development huizinga and kolawa 2007 yoo and harman 2012 integration tests are those intended to ensure that the combined operation of multiple functions e g model coupling is both technically and conceptually sound and may also be continuously applied throughout the modeling process danglot et al 2020 laukkanen et al 2017 testing can uncover bugs or other issues that are show stopping high priority issues that render further work inadvisable without them being addressed from a bayesian perspective the more tests that are available covering more of the codebase and the conditions of their use the more confident modelers can be in the correct functionality of the model davidson pilon 2016 although there is some evidence that software testing practices are being adopted within the computational sciences hannay et al 2009 sarma et al 2016 sletholt et al 2012 to what extent is difficult to ascertain given the weak albeit strengthening norms requiring the provision of model code hutton et al 2016 adoption of software development practices such as testing is likely to be low given recent literature that encourage their adoption software development practices in general are also acknowledged to play a part in resolving issues with replicability and reproducibility of studies in environmental science and the computational sciences ahalt et al 2014 easterbrook 2014 gray and marwick 2019 hut et al 2017 one possible reason for the sparsity of reported software testing is the lack of formal software development training for researchers hannay et al 2009 and the reliance on mathematical or statistical rigor in model implementation there is also an element of trust involved due to the variety of disciplines found within iem development as constituent models and their components are taken to function correctly in the unintegrated context they are assumed to be correct in the integrated context regardless of the reasons the consistent application of tests for environmental model quality assurance appears to still be in its infancy the subsequent possibility of technical complications influencing model results referred to as technical uncertainty walker et al 2003 or as a consequence of conceptual mismatches across disciplinary specialists appears to be largely ignored 2 1 practical considerations of computational budget it is important to recognize and consider the computational costs involved in the diagnostic context as every computational work is subject to a budget arising from the intertwined concerns of available time computational power and monetary cost these concerns are collectively referred to here as the computational budget a hypothetical context wherein a model integrator performs tests on a typical desktop computer is described here to service the argument although dedicated infrastructure may be available e g distributed or cloud based platforms they too would be constrained by the same or at least similar considerations regarding their computational budget as iems are often time consuming to evaluate model diagnostics may be scheduled to run overnight on a desktop computer e g 5 p m to 9 a m or 16 h current typical development machines have 4 cores if the model is estimated to take on average an hour to run then 64 model evaluations may be conducted in the available time in practice model runtimes should be expected to be variable and computational performance is unlikely to scale linearly with the number of cores due to the computational overheads involved a rule of thumb to arrive at an estimate of runtime is given by 1 c t 1 r where c is the number of cores available t is the time available in hours and r is the estimated model run time in hours assumed here to be t it is common practice to inflate the runtime estimate r by some degree e g by 10 based on prior empirical knowledge of the model s computational performance and requirements overestimating the runtime ensures that model evaluations complete within the defined available time given the variability of model runtime and computational overhead such considerations are also important in cases where cloud based infrastructure is adopted as such services may charge by a unit of time e g per minute description of the terms used throughout the paper are provided in table 1 for ease of reference running of tests can be structured such that they are run from the simplest and least time consuming to the most complicated and computationally intensive failure of a simple test may then negate the need to run a more computationally intensive test in some cases failure of any single test may preclude the necessity of running any other tests as the model has been shown to have issues or at least allow for a more targeted diagnostic to occur structuring tests in this manner aids in conserving available computational budget 2 2 example unit and property based testing box 1 shows an example unit test implemented in the python programming language with the pytest framework krekel et al 2004 for an example linear function case 1 in li et al 2010 2 y x 1 x 2 x 3 x 4 x 5 this simple example illustrates unit tests that protect modelers from changes inadvertent or otherwise that may introduce errors that would otherwise go unnoticed but only for a specific known result one disadvantage of unit testing is the need for such specificities to be known and for tests to be written for each while requirements may be known in advance particularly in business oriented software development it is less likely in research modeling contexts and even less likely where the complex interactions between models are involved as in iems while it is possible to test that a known correct model output has not changed such a test does not apply to new model configurations as is common when integrating existing models to counter this limitation modelers may adopt a property based testing approach fink and bishop 1997 wherein the expected behavioral aspect of the software model is tested rather than a specific known output as with the regular unit testing approach sets of inputs to feed into the model would be automatically generated in a property based testing approach property based testing was perhaps popularized by the quickcheck tool for the haskell programming language claessen and hughes 2000 which sparked the development of similar tooling for other programming languages such testing frameworks can assist in determining the properties of failing tests themselves helping to identify specific cases in which the model does not behave as expected löscher and sagonas 2017 to give a specific example one such test could serve to ensure a zero or positive valued output is obtained i e 0 in cases where the sum of positive inputs is greater than the absolute sum of negative inputs as this is an expected property of the model box 2 depicts an implementation of such a property based test along with its output indicating that the test failed as the model does not produce the expected behavior code for these examples are provided in iwanaga 2020 on examination we see that the model was incorrectly implemented see box 3 but crucially in a way the previous unit test shown in box 1 would still pass the results illustrated here should not be taken to mean that property based testing supersedes unit testing as both are useful and can be leveraged in tandem to inform the level of confidence in the model implementation modelers may find that property based testing is somewhat analogous to pattern oriented modeling grimm 2005 grimm and railsback 2012 although the focus of the latter is on model construction and calibration there is a conceptual similarity in that both pattern oriented and property based approaches evaluate model accuracy against known or desired behavioral properties rather than evaluating against a single point of truth i e a benchmark failure of a model to adhere to expected behavior then invalidates the assumption that the model is functioning correctly it is therefore useful to test against a broad range of expected behavioral patterns as models are modified and coupled and to do so frequently throughout the modeling cycle 3 sa in the evaluation process sensitivity analysis sa can play multiple roles in the model evaluation process a common use of sa is to screen and rank factors parameters and input variables according to their influence on model outputs razavi et al 2020 saltelli et al 2008 sa may also be used to analyze the bounds and uncertainties of a model s parameters and its predictions and is valuable in assessing model identifiability guillaume et al 2019 shin et al 2015 model sensitivities have also been assessed as part of a diagnostic evaluation procedure to aid in verifying models and their structure gupta et al 2008 pianosi et al 2016 sieber and uhlenbrook 2005 typical applications of diagnostic sa concern themselves with the identification of model components or parameters that explain or should explain differences between simulated and observed system behavior gupta et al 2008 reiter 1987 saltelli et al 2004 diagnostic sa typically assumes that model development is complete rarely is it framed as an approach to test and validate model behavior throughout the model development cycle a key consideration in the selection of an sa method or methods is its appropriateness for the intended aim constrained by the available computational budget screening and ranking parameters for example requires substantially fewer model runs to accomplish compared to obtaining estimates of parameter sensitivities herman et al 2013 sarrazin et al 2016 screening for parameters on which to conduct further analysis is a common practice that aids in conserving computational budget cuntz et al 2015 mai and cuntz 2020 fixing the resultant insensitive parameters constraints the number of parameter combinations to be run for later global sa or bayesian uncertainty analysis the trade off is a risk that fixing parameters may introduce large errors in the quantities of interest in the following subsections we describe typical sa approaches and their suitability in the diagnostic context for context brief descriptions of the terms used are provided in table 2 3 1 sensitivity analysis methods in typical local sensitivity analysis lsa each model parameter is assigned a best guess baseline value and then changed perturbed by setting to some pre selected value or multiplying by some proportion and then returned to their baseline value whilst others remain fixed campolongo et al 2011 the derivative is calculated for each change and the process repeated for each parameter one after the other giving it its name one at a time oat any changes to the model output are thus attributable to the parameter that was perturbed such approaches are defined as local as they are only capable of providing indications of sensitivity at specific points in parameter space in contrast to global sensitivity analysis gsa lsa cannot provide indications of interactions between parameters and their effect on model outputs saltelli et al 2019 wagener and pianosi 2019 the oat approach described here is referred to as a pure oat to distinguish it from other global approaches which may also vary parameters one at a time there are other approaches to sa that do not rely on oat variance based methods are a commonly used class of gsa which involve the perturbation of parameters all at a time douglas smith et al 2020 although more appropriate for parameter sensitivity estimation compared to pure oat variance based approaches can be difficult to apply for early diagnosis of model issues where large numbers of parameters and long runtimes are involved sufficient samples are needed to obtain accurate sensitivity estimates and this can increase exponentially with the number of parameters involved there is however no universally applicable rule that provides a reliable estimation of the number of samples required which changes from method to method sampling regime the number of parameters the model itself and its quantities of predictive interest wagener and pianosi 2019 pure oat is unsuitable for comprehensive analysis of sensitivities in complex models with non linear behavior as wider areas of parameter space must be explored to capture global indications of parameter interactions razavi and gupta 2015 saltelli and annoni 2010 yang 2011 despite these shortcomings oat remains prevalent in model assessment against published advice ferretti et al 2016 although the situation does appear to be slowly improving douglas smith et al 2020 one clear advantage that oat has exploited in the pbsa approach of this paper is its conceptual and computational simplicity relative to other methods the morris method morris 1991 is an extension of the oat approach vanrolleghem et al 2015 being capable of providing adequate indications of sensitivity for a variety of purposes in the context of complex nonlinear models sun et al 2012 the morris method changes parameter values one at a time and so is sometimes referred to as morris one at a time but does so in a stepwise manner without dependence on nominal values through a process known as trajectory sampling unlike the pure oat approach parameter values are not reset to their original start points and instead are kept until all parameters have been modified the process is repeated n times so that the total number of model evaluations is n n p 1 where usually n p or less norton 2009 thus the number of model evaluations increases quadratically with the number of parameters unless n p in which case the increase is linear the sensitivity index produced by the morris method indicates the relative change in the quantity of interest regarding the changed parameter value the average elementary effect μ the average absolute change in parameter value which accounts for the effect negative values may have denoted as μ and its standard deviation σ which indicates interaction and non linear effects a high σ indicates that a parameter is interacting with others braddock and schreider 2006 pianosi et al 2016 saltelli et al 2008 the morris method is often recommended for screening and ranking purposes cuntz et al 2015 saltelli and annoni 2010 as it requires fewer model runs to arrive at an acceptable parameter rank or screening conclusion compared to other common sa approaches see for example braddock and schreider 2006 cuntz et al 2015 herman et al 2013 sun et al 2012 an alternative to the morris approach is the application of oat with a radial design wherein the pure oat approach is repeatedly applied around different start points campolongo et al 2011 in this radial approach referred to as r oat from hereon the model is evaluated n p 1 times where n is the number of repetitions it is noted here that r oat transforms oat from a local to global sa when n 1 thus r oat is equivalent to pure oat when n 1 and the total number of model evaluations is the same as with the morris method unlike the morris method however r oat does not require a specific sampling scheme and can leverage existing schemes such as latin hypercube sobol sequences or even simple monte carlo to gain an indication of variance based indices campolongo et al 2011 pianosi et al 2016 r oat is particularly appealing within the iem context due to its simplicity and scalability leading to its application being relevant throughout the model development life cycle as suggested previously by campolongo et al 2011 a collection of sa results can be built up in stages where and when necessary smaller samples for diagnostic purposes can be built on with additional samples added for screening and ranking larger samples can be used to obtain an indication of global effects via variance based indices assuming no implementation or integration errors are identified there are alternatives to variance based approaches such as moment independent also known as density based approaches from which usable indicators can be obtained with a reduced number of samples relative to variance based approaches the pawn method pianosi and wagener 2015 2018 for example was found to be able to identify parameters of significance with 10 of the samples needed by the sobol method for a 26 parameter hydrological model 200 compared to 2000 samples zadeh et al 2017 with the pawn and sobol methods a dummy parameter can be used to obtain an indication of insensitive parameters a dummy parameter is an inactive factor that does not have any influence on the behavior of the model i e it is completely insensitive parameters that are awarded a sensitivity rank equal to or less than the dummy parameter are assumed to be insensitive the focus therein however is on assessing parameter sensitivities rather than expected model behavior the use of emulators which approximate the model response surface with an abstract formalism is one oft suggested approach to resolving issues of computational complexity and runtime e g yang et al 2018 and could in principle be used to speed up testing developing emulators however requires sufficient areas of parameter space to be represented the time taken to obtain the necessary samples for a complex model is typically prohibitive in the context of the model development cycle by the time the emulator is ready the model is likely to have undergone significant changes such that the emulator represents an obsolete version a further consideration is that many methods require that the response surface have a level of smoothness for it to be approximated and that the parameterization of the original model is not exceedingly high oakley and o hagan 2004 sudret 2008 the above criteria are often not met in the case of iems the error in emulators also needs to be evaluated prior to use as emulation of an iem with conceptual or implementation issues renders any subsequent uses of the emulator beyond diagnostic tests inappropriate making their development too costly for the sole purpose of obtaining indicative results 3 2 example diagnostic sa to provide an illustrative if simplistic example of diagnostic sa within the development cycle a hypothetical model developer could apply the morris method morris 1991 to gain an indication of the behavior of the incorrectly implemented model introduced above see box 2 and 3 the morris sensitivity index indicates the relative change in the quantity of interest regarding the changed parameter value μ the average absolute change in parameter value μ which accounts for the effect negative values may have and its standard deviation σ which indicates interaction and non linear effects campolongo et al 2011 the method as implemented in the salib sensitivity analysis library herman and usher 2017 package for python is used here for demonstration purposes which applies the improved sampling method introduced in ruano et al 2012 relevant code for this example may be found in iwanaga 2020 for the example linear function eq 2 two properties are expected first the effect of each parameter is expected to be positive given the quantity of interest is the sum of all inputs second the contribution of parameters to the quantity of interest is expected to be equal again due to the linear nature of the model although the second property is satisfied the results indicate that x 5 is having a negative effect due to the erroneous implementation fig 2 depicts this unexpected result for the erroneously implemented example function see also eq 2 and box 3 in the software testing paradigm diagnostic sa is a form of property based test as the model property i e its sensitivities are being investigated and evaluated although modelers usually apply this in a more manual manner through the visualization and qualitative assessment of results diagnostic sa may be an effective complement to traditional software development tests particularly in complex integrated modeling contexts as the correct functioning of code in isolation does not necessarily imply conceptually correct integrated model behavior voinov and shugart 2013 one barrier to the adoption of diagnostic sa is the reported lack of norms around investigating model sensitivities saltelli et al 2019 use of sa in general is reportedly low with the complexity and lack of understanding of recommended sa techniques being one suggested reason for the lack of uptake ferretti et al 2016 saltelli et al 2019 there may also be resistance towards the adoption of the new and unfamiliar known as the status quo bias samuelson and zeckhauser 1988 as hypothesized in ferretti et al 2016 4 parameter in sensitivity as a property to test the iem development context can involve multiple modeling paradigms disciplinary sectoral knowledge and feature the adoption of multiple technologies including different computational infrastructure and programming languages hannay et al 2009 hut et al 2017 hutton et al 2016 sletholt et al 2012 as mentioned in the introduction this leads to a situation in which no single modeler has a full and complete understanding of the models involved given the complex development context of iems one fundamentally important property that iem and other model developers can target for indicative assessment is the inappropriate sensitivity of parameters known to have cross system influences the principal idea here is that parameter sensitivities are generally a robust property of model behavior that provide indications of correct model implementation and integration parameter activity or inactivity i e complete insensitivity is a property that will remain invariant even as the model itself changes and evolves through the model development cycle and as the precise model outputs change thus diagnostic sa applied as a form of property based test to regions of parameter space in which model behavior is expected to be sensitive or insensitive can then provide early confidence that other more computationally demanding processes can proceed without issue for iems conceptual analysis of the relationships between the models can be invoked to identify parameters to test an example is provided in section 5 quantitative assessment of sa results within the automated testing process could alert modelers to unintended changes that unknowingly affect model applications such tests may also guard against issues of technical uncertainty specifically computational infrastructure uncertainty as model behavior may differ under different computational contexts bhandari neupane et al 2019 iwanaga et al 2020a walker et al 2003 4 1 testing for inactivity with property based sa to provide a concrete example with the earlier example function another error is introduced perhaps in the process of correcting the earlier implementation issue shown in box 4 which cancels out the effect of parameter x 5 diagnostic sa results with the morris method are shown in fig 3 highlighting the issue for modelers to investigate this simplistic example is intended to illustrate the concept a more expansive example is provided in section 5 although the morris method is applied in this specific diagnostic case the same conclusion of inactivity can be reached with a purely one at a time oat analysis with p 1 model evaluations in the worst case i e n 6 for the example function as indicated in section 2 1 identified issues with the model implementation or integration through the failure of a test at any point may negate the need to run further tests failure of oat to produce expected results negates the need to apply other more computationally intensive tests and sa for this reason the total number of evaluations to invalidate the model may be less than p 1 hypothetically it may not be necessary to test all parameters as in cases where only a certain subset of parameters denoted as s where s p is expected to influence model interactions in such cases inappropriate model behavior could be determined with n 2 s 1 model evaluations unexpected results should be investigated before any further analyses proceed use of a purely oat approach is heavily discouraged in the literature saltelli et al 2019 saltelli and annoni 2010 and it is suggested as appropriate here only because of the expectation that other forms of tests and sa will be applied after property based sa tests pass it is stressed here that relying on oat for purposes outside this first pass diagnostic context is not encouraged in the case of iem development property based tests that focus on parameters that influence model interactions can be a computationally effective approach to obtaining a first pass indication of correct conceptual and technical model integration 4 2 example property based sa for parameter inactivity we devise two property based testing strategies for the quick first pass identification of model integration issues the first is a form of oat referred to as extremity testing and the other follows a more usual sensitivity analysis approach using r oat as noted previously failure of these tests indicates the presence of issues that should be further investigated prior to the application of more computationally expensive diagnostics e g a global sensitivity analysis or operationalization of the model both approaches require that the conceptual relationships between parameters at their extremes and the targeted qois are known beforehand with extremity testing the model is run just twice i e n 2 one run is to be conducted with the targeted parameters perturbed to their lower extremes and the other run with parameters set to their upper an example is shown in box 5 in practice any sufficiently large perturbation should suffice and the upper and lower extremes are suggested here for conceptual simplicity and ease of application under usual applications of sa extremity testing comes with a risk of type i and ii error a false positive or negative due to non monotonicity the approach is applicable in this specific case as the conceptual relationship between qois and parameters is an a priori expectation diagnostics are being carried out in a restricted local area of parameter space where sensitivities are expected to exist the primary concern is to determine whether the effect can be identified before the application of the model and more rigorous analysis such as with gsa note that diagnostics may also be carried out in regions of parameter space that are known to produce no effect wherein larger than expected i e non zero sensitivities can also indicate an issue pbsa in this case using r oat can be useful in identifying the conditions in which unexpected behavior occurs thereby helping to avoid a potentially time consuming debugging exercise two requirements can then be set for a gsa method to be a practical complement in the iem development context it would be desirable for any samples to be reusable in a later gsa if results are found to be acceptable another requirement is that the time taken to conduct such analyses should not exceed the available computational budget for such analyses to be timely and useful the illustrative examples provided in earlier sections showcase a diagnostic approach from both software development and sa perspectives in these examples however the hypothetical modeler has sufficient understanding of the model and its implementation details to apply and evaluate results from both tests and diagnostic sa in the context of integrated environmental modeling this may be a luxury rather than a given due to the aforementioned interdisciplinary nature of iem development iwanaga et al 2021 knapen et al 2013 in section 5 we describe the case of the campaspe integrated model and the usefulness of pbsa with extremity testing as an indication of valid model integration 5 an example with the campaspe integrated model the campaspe integrated model cim iwanaga et al 2018 2020a is a hydro environmental economic model used to explore water management options the cim is highly complex featuring interactions between six non linear component models each representing a specific system it can be considered a system of systems model in which a representation of the socio environmental system is built up from multiple independent and interacting constituent models little et al 2019 in the hypothetical development context individual model developers are disciplinarily diverse with their own traditions practices and preferred modeling approaches a common language and perspective of the modeling being conducted may still be developing macleod and nagatsu 2018 thomas and mcdonagh 2013 modelers may also be geographically spread inducing delays in communication that increase the risk of inadvertent errors being introduced to reflect this interdisciplinary context the model is treated here as a gray box for the purpose of the example modelers involved in the integration of constituent models may have working knowledge of the represented system and the operation of each model e g implementation and usage but are not necessarily disciplinary specialists themselves thus the primary concern in the initial stage is to gain confidence that operation of the iem is both conceptually and technically sound by testing the assumptions associated with the conceptual understanding iwanaga et al 2020a wilson et al 2017 falsifying the assumption that the model is integrated correctly also helps to preserve available computational budget in the development iems the relationships between all parameters and qois may not be fully known because of the complex model interactions that occur parameter activity inactivity may be a proxy that indicates correct model integration testing for the obvious behavior i e change of poi have flow on effects that should affect the qoi and continual confirmation that the behavior is present throughout the development cycle is valuable in that errors or conceptual mismatches could be highlighted and corrected earlier in the modeling cycle in other cases the conceptual understanding that the model integrator has may not be complete and so the testing process could be helpful in improving modelers understanding of the iem new knowledge or model configurations may invalidate previously obvious assumed behavior in which case the tests serve to alert modelers to a change in context change in context should subsequently be documented and the relevant tests updated to reflect this new understanding in this example the model integrator is principally focused on the policy surface water hydrology and farming system models however the entire model also includes representations of climate groundwater and ecology an example of the initially known interactions between the constituent models of interest is provided in fig 4 further description of the cim may be found in iwanaga et al 2020a interactions between all models affect the main quantity of interest selected here that is the long term surface water allocation index which indicates the average volume of water made available to water users over the simulation period the cim has 53 parameters which may all be varied runtime of the model is variable depending on the scenario being run but typically takes 30 minutes the influence of a single poi irrigation efficiency is investigated in this hypothetical context the parameter relates to the efficiency of water application for pipe and riser irrigations a common irrigation mode available to farmers in the campaspe region we consider the effect on modeled long term average surface water allocations rendering the poi static simulates an inadvertent change that introduced an implementation or integration error interactions between models are consequently inappropriately represented specific details of the poi are provided in table 3 irrigation efficiency relates to the proportion of water that reaches a crop s root zone the higher the efficiency rating the less water that is wasted or lost from a farmer s perspective to evaporation run off or deep drainage e g aquifer recharge hence the more efficient an irrigation system the less water required to maintain crop productivity for a given spatial area and the less water extracted from the dam water is allocated each year to farms by the policy model excessive use of water by farmers in one year can reduce farm water availability in subsequent years making efficient irrigations desirable given the available computational budget as contextualized in section 2 1 overnight execution of tests between 5pm and 9am i e 16 h would allow 128 model evaluations on a currently typical 4 core machine using eq 1 above assuming a consistent 30 min runtime a lesser number should be selected to ensure model runs resolve within the available time as runtime should not be expected to be consistent as explained in section 2 1 relevant to the point here is the application of the morris method for a realm model as reported in braddock and schreider 2006 the realm model is similar to the cim in terms of geographic region targeting the neighboring goulburn catchment and its use in water allocation modeling computational considerations which constrained the number of available model samples are highlighted therein use of cloud based testing infrastructure is ignored for the purpose of illustration and may potentially be cost prohibitive depending on the project budget it is unlikely that indicative results would be obtained with gsa development and use of emulators are similarly precluded given their requirement for sufficient areas of parameter space to be represented which is not possible within the allotted time 5 1 extremity testing results to demonstrate the diagnostic use of oat interactions between the surface water and policy models are initially deactivated such that dam level calculations never account for the volume of water used by farms therefore pipe and riser irrigation efficiency the poi will not directly influence the long term surface water allocation index the qoi functionally the deactivated parameter is equivalent to a dummy parameter in this example an extremity test is applied by those integrating the models the model integrators all 53 parameters are perturbed between their lowest and highest values with a cost of n 2 model runs unexpected results e g no change or smaller or larger than expected change indicate issues which modelers should investigate grouping parameters by their parent model components could also give at least an indication of which model in the iem the issue stems from the example results show that significantly higher volumes of water are allocated in the deactivated case much more than what would be expected under normal circumstances as shown in fig 5 the reason is that the farm water orders are never considered and the dam is never depleted such errors may inadvertently creep in during model development and integration examples include misunderstanding of the model interoperation e g what outputs from one model relate to an input to another implementation error e g a bug in a model or technical issues e g different compilers producing different machine code it is acknowledged here that the presented approach is viable in cases where the rough order of magnitude effect is known the results additionally indicate that the qoi will be affected even if the poi is completely deactivated suggesting that the qoi is affected by other factors thus the conceptual understanding depicted in fig 4 is not complete there are other factors which influence the qoi example code data and figures presented in this paper are provided as supplementary material via the open science framework see iwanaga 2020 iwanaga et al 2020b further description of other model analyses conducted on the cim can be found in iwanaga et al 2020a a single parameter can be targeted i e s 1 either after the above issue has been identified and further confirmation is desired or where the relative change from perturbing all parameters is unknown in this specific case any perturbation of the poi should be sufficient as the behavioral property being tested for is the presence of change in the qoi when all other parameters are set to their nominal best guess values an oat test with a further two model runs is thus applied to the poi illustrated in fig 6 because the interaction between models is disabled the results show no change in long term surface water allocation it is re emphasized here that the diagnostic property based test targets areas of parameter space for which the poi and qoi are expected to be sensitive and that diagnostic applications of sa should be conducted alongside other testing processes 5 2 a global approach to property based sa in this example parameter activity inactivity is used as a proxy to indicate correct model integration the relationship between the poi and qoi could be tested using r oat and morris to confirm the presence of some sensitivity across parameter space the r oat and morris methods are applied here given that they are known to provide reliable indications with fewer samples compared to other gsas as noted in section 2 1 samples were generated by producing n p 1 parameter sets such that n points in parameter space were sampled based on the targeting distribution as shown in fig 5 other factors may influence the qoi and so a non zero sensitivity value is to be expected given that these gsa approaches report the average effect with parameter interactions for this reason we adapt the dummy threshold approach from zadeh et al 2017 wherein a parameter is considered insensitive if the reported sensitivity value is comparable to the sensitivities reported for the dummy parameter in this case we apply such a threshold to indicate an unexpected lack of activity an activity threshold an activity threshold of 0 1 is empirically set for this example a value lower than expected sensitivities for the parameter in question for the available number of samples but higher than typical sensitivity thresholds e g 0 05 sarrazin et al 2016 as the poi is expected to be active its reported sensitivities should be above this threshold and values lower than the threshold indicate a cause for concern testing for the property of parameter activity in this manner is more robust compared to searching for absolute inactivity as computational precision error compounded as the models within the iem continually interact may introduce variability in results dunford et al 2015 unexpected interactions based on modelers current understanding of model interactions may also cause non zero sensitivities such tests could be incorporated as part of an automated test suite existing property based testing frameworks could also be leveraged to aid in pinpointing areas of parameter space wherein errors of concern occur e g löscher and sagonas 2017 the number of repetitions possible under the hypothetical 16 h time limit i e 5pm to 9am is n 2 i e total number of possible model runs is n 108 given that a model run takes roughly 30 min performing an additional repetition n 162 when n 3 would exceed the available time limit taking over 20 h we take 540 model evaluations i e n 10 purely to illustrate response of μ in this example indicative confirmation that the model is not behaving as expected could be obtained with n 2 using r oat n 108 see the disabled case in fig 7 in general how low n can be depends on the parameter and model context and some initial experimentation is likely required similar results may be obtained with the morris method fig 8 in the disabled case although in the active case concrete confirmation does not occur until n 9 i e n 486 the results for both i e no sensitivity at n 54 for r oat and insufficient activity until n 486 for morris suggest that the sampling scheme plays an important role in the efficacy of gsa methods for diagnostic purposes the diagnostic context severely limits the number of samples that could be obtained in a timely manner and for this reason other gsa methods were not wholly considered preliminary results are included in appendix a for the saltelli 2002 easi plischke 2010 and dmim plischke et al 2013 methods which indicate the unreliability of gsa methods at such low sample sizes these results indicate potential issues to be overcome if these methods are to be applied for diagnostic purposes in the context of rapid iterative model development and testing in cases where no issues are identified it is desirable for obtained samples to be reused to conserve computational budget 6 discussion and conclusions this paper outlined a role sa can play in software testing practices in the iem development process specifically local oat analyses coupled with r oat and or morris can provide a first pass indication of the correctness of technical and conceptual integration of constituent models particularly in terms of checking the effects of active inactive parameters on expected model behavior including such property based diagnostics as part of an automated test suite can aid in conserving a limited computational budget which is often desirable even in cases where there is an abundance of computational time available with the campaspe integrated model used as an example the lack of an expected relationship between a poi and qoi could be identified using extremity parameter value testing with just n 2 runs when an activity threshold is applied although global sensitivity analysis methods can be computationally demanding the use of r oat is shown to be a computationally efficient approach to assessing expected behavior although the morris method was able to identify inactivity of the parameter of interest it required more model runs to do so at least in the presented example case it is emphasized here that for the purpose of integrated model testing the number of parameters to be perturbed could be further reduced in all approaches described through qualitative assessment that identifies which parameters would have system level or inter system implications targeting these parameters or organizing these into groups such that the number of perturbations is much fewer than the total number of model parameters would reduce the overall computational effort involved to gain an indicative result additionally a failing test may negate the need to conduct further diagnostics as the assumption that the model is operating correctly is falsified thereby aiding in conserving computational budget more complete analyses could follow in cases where no property based sa tests fail in a full property based testing approach the framework applied would generate a random set of inputs and iteratively narrow the parameter space to specific areas that cause unexpected model behavior löscher and sagonas 2017 such tests could be augmented to use gsa methods that require comparatively limited number of samples such as r oat use of given data methods such as pawn pianosi and wagener 2015 hdmr li et al 2002 or methods with flexible sampling requirements such as star vars razavi et al 2019 could also be explored to identify potential advantages and limitations e g puy et al 2020 these given data methods may be more suitable in the iem context due to their ability to leverage available samples and may also be used to complement any diagnostic analyses conducted towards a comprehensive gsa mora et al 2019 the use of dummy parameters in combination with extremity testing under conditions in which model parameters or targeted subset of parameters are known to be active could also be explored alternate oat based global analyses that are potentially more efficient for obtaining indications of parameter interaction e g borgonovo 2010 may also be beneficial there are many approaches to validating computational models model developers can adopt a mix of testing practices from both software engineering and statistical mathematical analysis to cover the range of issues that may occur during model development ideally modelers would not restrict themselves to techniques found in one discipline over the other there are however barriers to the adoption of this hybrid approach for one it requires the technical knowledge and capacity of modelers to develop and maintain tests including the application of relevant sa techniques it is demonstrated here that a diagnostic property based testing approach with sa methods is a useful pragmatic and computationally efficient approach to providing a line of evidence that the model parameters are in fact having an expected effect in the iem development context any single model may itself require teams of domain specialists to fully understand and no single person can be expected to grasp all aspects especially in cases where legacy models are adopted assessing the expected behavioral properties of a model could be leveraged to reduce the time taken to identify and correct model implementation and integration errors declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors acknowledge support from the australian government research training program takuya iwanaga anu hilda john endowment fund takuya iwanaga xifu sun and barry croke china scholarship council qian wang and the australian research council discovery early career researcher award joseph guillaume project no de190100317 the campaspe integrated model was developed as part of the murray darling basin authority s partnership with the national centre for groundwater research and training ncgrt under contract no md2594 the authors would like to thank the reviewers for their comments and insight which greatly improved the quality and clarity of the paper appendix a here we showcase some preliminary results with three global sensitivity analysis approaches namely saltelli 2010 easi 2010 and dmim 2013 unreliable results are obtained for all approaches at these relatively low sample sizes easi and dmim are given data approaches for which morris samples are used for the saltelli method only first order indices s 1 are shown using the approach described in saltelli et al 2010 estimated with a cost of n p 2 in practice total first and second order indices may be estimated at a cost of n 2 p 2 runs saltelli 2002 the results for the saltelli analysis include negative values fig 9 which indicate an insufficient number of samples saltelli 2008 sharifi et al 2019 which is to be expected given the known high sampling requirements of sobol based approaches razavi and gupta 2015 fig 9 negative first order sensitivity values s1 from the saltelli method analysis for both inactive and active cases fig 9 the easi analysis technique indicates that an effect is occurring when interactions are disabled i e type i error as shown in fig 10 while the easi approach does not require a specific sampling scheme plischke et al 2013 the results produced may be sensitive to the sampling approach very little difference was found between disabled and enabled cases when results were obtained with morris sampling see fig 11 in both cases easi was unable to distinguish the lack of effect of an inactive parameter at these low sample sizes fig 10 first order effect s1 of irrigation efficiency on surface water allocations using oat samples easi analysis indicates the parameter is sensitive where model interactions are disabled fig 10 fig 11 example of easi analysis on results taken with morris sampling results of first order sensitivities s1 appear near identical indicating larger morris samples are necessary for a distinction to be made with easi note that the y axis for the top panel is in log scale fig 11 dmim was unable determine the lack of influence from the poi see fig 12 and fig 13 with similar issues fig 12 dmim analysis on oat samples fig 12 fig 13 dmim analysis on morris samples fig 13 when compared to the results of oat based gsa morris and r oat in the main text it appears that methods which indirectly estimate first order sensitivity while varying multiple parameters at once do not correctly identify inactive parameters at least at the given sample sizes furthermore a significantly larger number of model evaluations may be required to achieve convergence which may exceed the available time for diagnostic testing particularly in the case of complex and highly parameterized iems 
25855,diagnostic testing is an oft recommended use of sensitivity analysis to assess correctness or plausibility of model behavior in this paper we demonstrate the use of sensitivity analysis as a complementary first pass software test for the validation of model behavior typical testing processes rely on comparing model outputs to results known to be correct such approaches are limited to specific model configurations and require that correct results be known in advance property based sensitivity analysis pbsa examines model properties in terms of the behavior of parameter sensitivities to provide a line of evidence that the expected conceptual relationships between model factors and their effects are present unanticipated results can indicate issues to be corrected the pbsa approach is also scalable as it can complement existing testing practices and be applied in conjunction with global sensitivity methods that can reuse existing model evaluations or are otherwise independent of the sampling scheme keywords integrated development cycle diagnostic testing sensitivity analysis parameter sensitivity parameter inactivity integrated environmental model 1 introduction integrated environmental models iems are often developed to inform policy and management processes in the problem realm of socio environmental systems ses such integrated models account for multiple sectoral influences and their interactions including the biophysical e g hydrological climate ecological and agriculture and socio economic processes e g human drivers economy market policy and legislative interactions multiple models both purpose built and pre existing i e legacy models kelly letcher et al 2013 are often coupled to represent this system of systems typical iem development conceptualizes an iterative cyclic process in which an interdisciplinary team of teams collaborates to appropriately represent the interactions across the ses being modeled hamilton et al 2015 little et al 2019 the development process is such that the suite of models that constitute an iem and their coupling are in a state of flux with each undergoing a separate iterative development cycle changes to one model component may necessitate changes in another and there will be emergent behaviors that arise only when models are integrated the modeler s responsible for integrating the disparate models involved is the foundation for ensuring that the constituent models and the resulting iem are both technically and conceptually sound lest usability of the iem and confidence in the results be compromised voinov and shugart 2013 compounding matters is the fact that iems are increasingly being operated at grander scales in terms of the number of systems represented the breadth of researchers and interest groups involved and consequently the required computational infrastructure budget and time available elsawah et al 2020 little et al 2019 the resulting iem may have hundreds possibly even thousands of parameters models external to each discipline or sectoral component are often treated as black or at best gray boxes given the spread of domain specific knowledge required to understand in full technical detail the models representing the system of systems consequently no single person is likely to have a full and complete understanding of the models involved given the complex and complicated context of ses modeling and the pace at which iem development occurs the cost of correcting errors that may inadvertently creep in may increase as time progresses boehm 1986 the associated opportunity cost may be substantial and so it is desirable for any issue to be identified and corrected as early in the development cycle as possible continuous and repeated testing of the models and their integration therefore plays an important part in the model development cycle in the development of software testing is leveraged to gain confidence that the underlying code is working as intended and continues to do so throughout the rapid pace of iterative development danglot et al 2020 a failing test then falsifies the assumption that the software is working correctly researchers in the field of sensitivity analysis sa have independently arrived at the idea of diagnostic evaluation estimated sensitivities of parameters are used to provide some validation that the model behavior is in line with expectations campolongo et al 2011 gupta et al 2008 pianosi et al 2016 such diagnostic approaches have been recognized as vital for maximizing the capabilities of mathematical models rabitz 1989 due to the computational demands of iems results from diagnostic sa may be effective as once off analyses yet take an excessive amount of time relative to the computational time and budget available for continuous testing purposes given the context of rapid iteration and high complexity of iem development there is a need for a diagnostic process that aids in the quick and early identification of issues throughout the integrated modeling process in this paper we showcase how a simple and computationally inexpensive sa based on one at a time oat sensitivity analyses applied in the frame of software testing can be a complementary strategy in identifying model implementation and integration issues early in the modeling cycle the approach which we refer to as property based sensitivity analysis pbsa can help expose issues in the course of building or integrating models by exploiting expected and unexpected sensitivity of parameters these are used as indicators to confirm the expected model behavior in areas of parameter space with known model behaviors in the following sections we briefly introduce software testing practices contextualized by the integrated model development context section 2 and explore its conceptual linkages with diagnostic sensitivity analysis section 3 and 4 we then provide an illustrative example in section 5 using the campaspe integrated model cim an integrated model developed to explore sustainable water management futures within an agricultural setting in the lower campaspe catchment of victoria australia iwanaga et al 2020a we then conclude in section 6 with a discussion on directions for future research 2 software testing in integrated model development computational models are software in that they are implemented as code and are run on computers although there are clear similarities perhaps even identicalities between software and model development model testing and development practices that are common in software production may not be readily adopted crouch et al 2013 hutton et al 2016 sletholt et al 2012 in fact publications have been retracted in the past for errors that software testing practices would have assisted in identifying ahalt et al 2014 bhandari neupane et al 2019 kanewala and bieman 2014 in this section we briefly introduce the concept of unit testing and the practice of property based testing it has long been recognized that issues are easier and cheaper to address if they are identified earlier in the development process levin et al 2019 mossalam 2018 a core aim of software testing is to reduce the time taken to reach a stable working piece of software in this case a model by aiding in the identification of issues as early as possible in the development workflow see fig 1 developers write code to ensure the correct functionality of other code to accomplish this aim such code are referred to collectively as tests a common type of software test is referred to as a unit test as it tests an arbitrary but preferably small unit of code against a specific known result sarma et al 2016 unit tests support the development process by providing indications that the model is working in line with expectations frequent re running of these tests e g after every change shorten the time between changes to the code and identification of issues thereby smoothing the model development cycle one issue is that identifying the correct behavior to test may be challenging in cases where the effects of model interactions may not be fully understood as in the iem context running of tests can be automated verweij et al 2010 such that a collection of unit and property based tests could then form a regression and or integration test suite regression tests help alert developers to the unintentional re introduction of issues that may have been previously addressed during model development huizinga and kolawa 2007 yoo and harman 2012 integration tests are those intended to ensure that the combined operation of multiple functions e g model coupling is both technically and conceptually sound and may also be continuously applied throughout the modeling process danglot et al 2020 laukkanen et al 2017 testing can uncover bugs or other issues that are show stopping high priority issues that render further work inadvisable without them being addressed from a bayesian perspective the more tests that are available covering more of the codebase and the conditions of their use the more confident modelers can be in the correct functionality of the model davidson pilon 2016 although there is some evidence that software testing practices are being adopted within the computational sciences hannay et al 2009 sarma et al 2016 sletholt et al 2012 to what extent is difficult to ascertain given the weak albeit strengthening norms requiring the provision of model code hutton et al 2016 adoption of software development practices such as testing is likely to be low given recent literature that encourage their adoption software development practices in general are also acknowledged to play a part in resolving issues with replicability and reproducibility of studies in environmental science and the computational sciences ahalt et al 2014 easterbrook 2014 gray and marwick 2019 hut et al 2017 one possible reason for the sparsity of reported software testing is the lack of formal software development training for researchers hannay et al 2009 and the reliance on mathematical or statistical rigor in model implementation there is also an element of trust involved due to the variety of disciplines found within iem development as constituent models and their components are taken to function correctly in the unintegrated context they are assumed to be correct in the integrated context regardless of the reasons the consistent application of tests for environmental model quality assurance appears to still be in its infancy the subsequent possibility of technical complications influencing model results referred to as technical uncertainty walker et al 2003 or as a consequence of conceptual mismatches across disciplinary specialists appears to be largely ignored 2 1 practical considerations of computational budget it is important to recognize and consider the computational costs involved in the diagnostic context as every computational work is subject to a budget arising from the intertwined concerns of available time computational power and monetary cost these concerns are collectively referred to here as the computational budget a hypothetical context wherein a model integrator performs tests on a typical desktop computer is described here to service the argument although dedicated infrastructure may be available e g distributed or cloud based platforms they too would be constrained by the same or at least similar considerations regarding their computational budget as iems are often time consuming to evaluate model diagnostics may be scheduled to run overnight on a desktop computer e g 5 p m to 9 a m or 16 h current typical development machines have 4 cores if the model is estimated to take on average an hour to run then 64 model evaluations may be conducted in the available time in practice model runtimes should be expected to be variable and computational performance is unlikely to scale linearly with the number of cores due to the computational overheads involved a rule of thumb to arrive at an estimate of runtime is given by 1 c t 1 r where c is the number of cores available t is the time available in hours and r is the estimated model run time in hours assumed here to be t it is common practice to inflate the runtime estimate r by some degree e g by 10 based on prior empirical knowledge of the model s computational performance and requirements overestimating the runtime ensures that model evaluations complete within the defined available time given the variability of model runtime and computational overhead such considerations are also important in cases where cloud based infrastructure is adopted as such services may charge by a unit of time e g per minute description of the terms used throughout the paper are provided in table 1 for ease of reference running of tests can be structured such that they are run from the simplest and least time consuming to the most complicated and computationally intensive failure of a simple test may then negate the need to run a more computationally intensive test in some cases failure of any single test may preclude the necessity of running any other tests as the model has been shown to have issues or at least allow for a more targeted diagnostic to occur structuring tests in this manner aids in conserving available computational budget 2 2 example unit and property based testing box 1 shows an example unit test implemented in the python programming language with the pytest framework krekel et al 2004 for an example linear function case 1 in li et al 2010 2 y x 1 x 2 x 3 x 4 x 5 this simple example illustrates unit tests that protect modelers from changes inadvertent or otherwise that may introduce errors that would otherwise go unnoticed but only for a specific known result one disadvantage of unit testing is the need for such specificities to be known and for tests to be written for each while requirements may be known in advance particularly in business oriented software development it is less likely in research modeling contexts and even less likely where the complex interactions between models are involved as in iems while it is possible to test that a known correct model output has not changed such a test does not apply to new model configurations as is common when integrating existing models to counter this limitation modelers may adopt a property based testing approach fink and bishop 1997 wherein the expected behavioral aspect of the software model is tested rather than a specific known output as with the regular unit testing approach sets of inputs to feed into the model would be automatically generated in a property based testing approach property based testing was perhaps popularized by the quickcheck tool for the haskell programming language claessen and hughes 2000 which sparked the development of similar tooling for other programming languages such testing frameworks can assist in determining the properties of failing tests themselves helping to identify specific cases in which the model does not behave as expected löscher and sagonas 2017 to give a specific example one such test could serve to ensure a zero or positive valued output is obtained i e 0 in cases where the sum of positive inputs is greater than the absolute sum of negative inputs as this is an expected property of the model box 2 depicts an implementation of such a property based test along with its output indicating that the test failed as the model does not produce the expected behavior code for these examples are provided in iwanaga 2020 on examination we see that the model was incorrectly implemented see box 3 but crucially in a way the previous unit test shown in box 1 would still pass the results illustrated here should not be taken to mean that property based testing supersedes unit testing as both are useful and can be leveraged in tandem to inform the level of confidence in the model implementation modelers may find that property based testing is somewhat analogous to pattern oriented modeling grimm 2005 grimm and railsback 2012 although the focus of the latter is on model construction and calibration there is a conceptual similarity in that both pattern oriented and property based approaches evaluate model accuracy against known or desired behavioral properties rather than evaluating against a single point of truth i e a benchmark failure of a model to adhere to expected behavior then invalidates the assumption that the model is functioning correctly it is therefore useful to test against a broad range of expected behavioral patterns as models are modified and coupled and to do so frequently throughout the modeling cycle 3 sa in the evaluation process sensitivity analysis sa can play multiple roles in the model evaluation process a common use of sa is to screen and rank factors parameters and input variables according to their influence on model outputs razavi et al 2020 saltelli et al 2008 sa may also be used to analyze the bounds and uncertainties of a model s parameters and its predictions and is valuable in assessing model identifiability guillaume et al 2019 shin et al 2015 model sensitivities have also been assessed as part of a diagnostic evaluation procedure to aid in verifying models and their structure gupta et al 2008 pianosi et al 2016 sieber and uhlenbrook 2005 typical applications of diagnostic sa concern themselves with the identification of model components or parameters that explain or should explain differences between simulated and observed system behavior gupta et al 2008 reiter 1987 saltelli et al 2004 diagnostic sa typically assumes that model development is complete rarely is it framed as an approach to test and validate model behavior throughout the model development cycle a key consideration in the selection of an sa method or methods is its appropriateness for the intended aim constrained by the available computational budget screening and ranking parameters for example requires substantially fewer model runs to accomplish compared to obtaining estimates of parameter sensitivities herman et al 2013 sarrazin et al 2016 screening for parameters on which to conduct further analysis is a common practice that aids in conserving computational budget cuntz et al 2015 mai and cuntz 2020 fixing the resultant insensitive parameters constraints the number of parameter combinations to be run for later global sa or bayesian uncertainty analysis the trade off is a risk that fixing parameters may introduce large errors in the quantities of interest in the following subsections we describe typical sa approaches and their suitability in the diagnostic context for context brief descriptions of the terms used are provided in table 2 3 1 sensitivity analysis methods in typical local sensitivity analysis lsa each model parameter is assigned a best guess baseline value and then changed perturbed by setting to some pre selected value or multiplying by some proportion and then returned to their baseline value whilst others remain fixed campolongo et al 2011 the derivative is calculated for each change and the process repeated for each parameter one after the other giving it its name one at a time oat any changes to the model output are thus attributable to the parameter that was perturbed such approaches are defined as local as they are only capable of providing indications of sensitivity at specific points in parameter space in contrast to global sensitivity analysis gsa lsa cannot provide indications of interactions between parameters and their effect on model outputs saltelli et al 2019 wagener and pianosi 2019 the oat approach described here is referred to as a pure oat to distinguish it from other global approaches which may also vary parameters one at a time there are other approaches to sa that do not rely on oat variance based methods are a commonly used class of gsa which involve the perturbation of parameters all at a time douglas smith et al 2020 although more appropriate for parameter sensitivity estimation compared to pure oat variance based approaches can be difficult to apply for early diagnosis of model issues where large numbers of parameters and long runtimes are involved sufficient samples are needed to obtain accurate sensitivity estimates and this can increase exponentially with the number of parameters involved there is however no universally applicable rule that provides a reliable estimation of the number of samples required which changes from method to method sampling regime the number of parameters the model itself and its quantities of predictive interest wagener and pianosi 2019 pure oat is unsuitable for comprehensive analysis of sensitivities in complex models with non linear behavior as wider areas of parameter space must be explored to capture global indications of parameter interactions razavi and gupta 2015 saltelli and annoni 2010 yang 2011 despite these shortcomings oat remains prevalent in model assessment against published advice ferretti et al 2016 although the situation does appear to be slowly improving douglas smith et al 2020 one clear advantage that oat has exploited in the pbsa approach of this paper is its conceptual and computational simplicity relative to other methods the morris method morris 1991 is an extension of the oat approach vanrolleghem et al 2015 being capable of providing adequate indications of sensitivity for a variety of purposes in the context of complex nonlinear models sun et al 2012 the morris method changes parameter values one at a time and so is sometimes referred to as morris one at a time but does so in a stepwise manner without dependence on nominal values through a process known as trajectory sampling unlike the pure oat approach parameter values are not reset to their original start points and instead are kept until all parameters have been modified the process is repeated n times so that the total number of model evaluations is n n p 1 where usually n p or less norton 2009 thus the number of model evaluations increases quadratically with the number of parameters unless n p in which case the increase is linear the sensitivity index produced by the morris method indicates the relative change in the quantity of interest regarding the changed parameter value the average elementary effect μ the average absolute change in parameter value which accounts for the effect negative values may have denoted as μ and its standard deviation σ which indicates interaction and non linear effects a high σ indicates that a parameter is interacting with others braddock and schreider 2006 pianosi et al 2016 saltelli et al 2008 the morris method is often recommended for screening and ranking purposes cuntz et al 2015 saltelli and annoni 2010 as it requires fewer model runs to arrive at an acceptable parameter rank or screening conclusion compared to other common sa approaches see for example braddock and schreider 2006 cuntz et al 2015 herman et al 2013 sun et al 2012 an alternative to the morris approach is the application of oat with a radial design wherein the pure oat approach is repeatedly applied around different start points campolongo et al 2011 in this radial approach referred to as r oat from hereon the model is evaluated n p 1 times where n is the number of repetitions it is noted here that r oat transforms oat from a local to global sa when n 1 thus r oat is equivalent to pure oat when n 1 and the total number of model evaluations is the same as with the morris method unlike the morris method however r oat does not require a specific sampling scheme and can leverage existing schemes such as latin hypercube sobol sequences or even simple monte carlo to gain an indication of variance based indices campolongo et al 2011 pianosi et al 2016 r oat is particularly appealing within the iem context due to its simplicity and scalability leading to its application being relevant throughout the model development life cycle as suggested previously by campolongo et al 2011 a collection of sa results can be built up in stages where and when necessary smaller samples for diagnostic purposes can be built on with additional samples added for screening and ranking larger samples can be used to obtain an indication of global effects via variance based indices assuming no implementation or integration errors are identified there are alternatives to variance based approaches such as moment independent also known as density based approaches from which usable indicators can be obtained with a reduced number of samples relative to variance based approaches the pawn method pianosi and wagener 2015 2018 for example was found to be able to identify parameters of significance with 10 of the samples needed by the sobol method for a 26 parameter hydrological model 200 compared to 2000 samples zadeh et al 2017 with the pawn and sobol methods a dummy parameter can be used to obtain an indication of insensitive parameters a dummy parameter is an inactive factor that does not have any influence on the behavior of the model i e it is completely insensitive parameters that are awarded a sensitivity rank equal to or less than the dummy parameter are assumed to be insensitive the focus therein however is on assessing parameter sensitivities rather than expected model behavior the use of emulators which approximate the model response surface with an abstract formalism is one oft suggested approach to resolving issues of computational complexity and runtime e g yang et al 2018 and could in principle be used to speed up testing developing emulators however requires sufficient areas of parameter space to be represented the time taken to obtain the necessary samples for a complex model is typically prohibitive in the context of the model development cycle by the time the emulator is ready the model is likely to have undergone significant changes such that the emulator represents an obsolete version a further consideration is that many methods require that the response surface have a level of smoothness for it to be approximated and that the parameterization of the original model is not exceedingly high oakley and o hagan 2004 sudret 2008 the above criteria are often not met in the case of iems the error in emulators also needs to be evaluated prior to use as emulation of an iem with conceptual or implementation issues renders any subsequent uses of the emulator beyond diagnostic tests inappropriate making their development too costly for the sole purpose of obtaining indicative results 3 2 example diagnostic sa to provide an illustrative if simplistic example of diagnostic sa within the development cycle a hypothetical model developer could apply the morris method morris 1991 to gain an indication of the behavior of the incorrectly implemented model introduced above see box 2 and 3 the morris sensitivity index indicates the relative change in the quantity of interest regarding the changed parameter value μ the average absolute change in parameter value μ which accounts for the effect negative values may have and its standard deviation σ which indicates interaction and non linear effects campolongo et al 2011 the method as implemented in the salib sensitivity analysis library herman and usher 2017 package for python is used here for demonstration purposes which applies the improved sampling method introduced in ruano et al 2012 relevant code for this example may be found in iwanaga 2020 for the example linear function eq 2 two properties are expected first the effect of each parameter is expected to be positive given the quantity of interest is the sum of all inputs second the contribution of parameters to the quantity of interest is expected to be equal again due to the linear nature of the model although the second property is satisfied the results indicate that x 5 is having a negative effect due to the erroneous implementation fig 2 depicts this unexpected result for the erroneously implemented example function see also eq 2 and box 3 in the software testing paradigm diagnostic sa is a form of property based test as the model property i e its sensitivities are being investigated and evaluated although modelers usually apply this in a more manual manner through the visualization and qualitative assessment of results diagnostic sa may be an effective complement to traditional software development tests particularly in complex integrated modeling contexts as the correct functioning of code in isolation does not necessarily imply conceptually correct integrated model behavior voinov and shugart 2013 one barrier to the adoption of diagnostic sa is the reported lack of norms around investigating model sensitivities saltelli et al 2019 use of sa in general is reportedly low with the complexity and lack of understanding of recommended sa techniques being one suggested reason for the lack of uptake ferretti et al 2016 saltelli et al 2019 there may also be resistance towards the adoption of the new and unfamiliar known as the status quo bias samuelson and zeckhauser 1988 as hypothesized in ferretti et al 2016 4 parameter in sensitivity as a property to test the iem development context can involve multiple modeling paradigms disciplinary sectoral knowledge and feature the adoption of multiple technologies including different computational infrastructure and programming languages hannay et al 2009 hut et al 2017 hutton et al 2016 sletholt et al 2012 as mentioned in the introduction this leads to a situation in which no single modeler has a full and complete understanding of the models involved given the complex development context of iems one fundamentally important property that iem and other model developers can target for indicative assessment is the inappropriate sensitivity of parameters known to have cross system influences the principal idea here is that parameter sensitivities are generally a robust property of model behavior that provide indications of correct model implementation and integration parameter activity or inactivity i e complete insensitivity is a property that will remain invariant even as the model itself changes and evolves through the model development cycle and as the precise model outputs change thus diagnostic sa applied as a form of property based test to regions of parameter space in which model behavior is expected to be sensitive or insensitive can then provide early confidence that other more computationally demanding processes can proceed without issue for iems conceptual analysis of the relationships between the models can be invoked to identify parameters to test an example is provided in section 5 quantitative assessment of sa results within the automated testing process could alert modelers to unintended changes that unknowingly affect model applications such tests may also guard against issues of technical uncertainty specifically computational infrastructure uncertainty as model behavior may differ under different computational contexts bhandari neupane et al 2019 iwanaga et al 2020a walker et al 2003 4 1 testing for inactivity with property based sa to provide a concrete example with the earlier example function another error is introduced perhaps in the process of correcting the earlier implementation issue shown in box 4 which cancels out the effect of parameter x 5 diagnostic sa results with the morris method are shown in fig 3 highlighting the issue for modelers to investigate this simplistic example is intended to illustrate the concept a more expansive example is provided in section 5 although the morris method is applied in this specific diagnostic case the same conclusion of inactivity can be reached with a purely one at a time oat analysis with p 1 model evaluations in the worst case i e n 6 for the example function as indicated in section 2 1 identified issues with the model implementation or integration through the failure of a test at any point may negate the need to run further tests failure of oat to produce expected results negates the need to apply other more computationally intensive tests and sa for this reason the total number of evaluations to invalidate the model may be less than p 1 hypothetically it may not be necessary to test all parameters as in cases where only a certain subset of parameters denoted as s where s p is expected to influence model interactions in such cases inappropriate model behavior could be determined with n 2 s 1 model evaluations unexpected results should be investigated before any further analyses proceed use of a purely oat approach is heavily discouraged in the literature saltelli et al 2019 saltelli and annoni 2010 and it is suggested as appropriate here only because of the expectation that other forms of tests and sa will be applied after property based sa tests pass it is stressed here that relying on oat for purposes outside this first pass diagnostic context is not encouraged in the case of iem development property based tests that focus on parameters that influence model interactions can be a computationally effective approach to obtaining a first pass indication of correct conceptual and technical model integration 4 2 example property based sa for parameter inactivity we devise two property based testing strategies for the quick first pass identification of model integration issues the first is a form of oat referred to as extremity testing and the other follows a more usual sensitivity analysis approach using r oat as noted previously failure of these tests indicates the presence of issues that should be further investigated prior to the application of more computationally expensive diagnostics e g a global sensitivity analysis or operationalization of the model both approaches require that the conceptual relationships between parameters at their extremes and the targeted qois are known beforehand with extremity testing the model is run just twice i e n 2 one run is to be conducted with the targeted parameters perturbed to their lower extremes and the other run with parameters set to their upper an example is shown in box 5 in practice any sufficiently large perturbation should suffice and the upper and lower extremes are suggested here for conceptual simplicity and ease of application under usual applications of sa extremity testing comes with a risk of type i and ii error a false positive or negative due to non monotonicity the approach is applicable in this specific case as the conceptual relationship between qois and parameters is an a priori expectation diagnostics are being carried out in a restricted local area of parameter space where sensitivities are expected to exist the primary concern is to determine whether the effect can be identified before the application of the model and more rigorous analysis such as with gsa note that diagnostics may also be carried out in regions of parameter space that are known to produce no effect wherein larger than expected i e non zero sensitivities can also indicate an issue pbsa in this case using r oat can be useful in identifying the conditions in which unexpected behavior occurs thereby helping to avoid a potentially time consuming debugging exercise two requirements can then be set for a gsa method to be a practical complement in the iem development context it would be desirable for any samples to be reusable in a later gsa if results are found to be acceptable another requirement is that the time taken to conduct such analyses should not exceed the available computational budget for such analyses to be timely and useful the illustrative examples provided in earlier sections showcase a diagnostic approach from both software development and sa perspectives in these examples however the hypothetical modeler has sufficient understanding of the model and its implementation details to apply and evaluate results from both tests and diagnostic sa in the context of integrated environmental modeling this may be a luxury rather than a given due to the aforementioned interdisciplinary nature of iem development iwanaga et al 2021 knapen et al 2013 in section 5 we describe the case of the campaspe integrated model and the usefulness of pbsa with extremity testing as an indication of valid model integration 5 an example with the campaspe integrated model the campaspe integrated model cim iwanaga et al 2018 2020a is a hydro environmental economic model used to explore water management options the cim is highly complex featuring interactions between six non linear component models each representing a specific system it can be considered a system of systems model in which a representation of the socio environmental system is built up from multiple independent and interacting constituent models little et al 2019 in the hypothetical development context individual model developers are disciplinarily diverse with their own traditions practices and preferred modeling approaches a common language and perspective of the modeling being conducted may still be developing macleod and nagatsu 2018 thomas and mcdonagh 2013 modelers may also be geographically spread inducing delays in communication that increase the risk of inadvertent errors being introduced to reflect this interdisciplinary context the model is treated here as a gray box for the purpose of the example modelers involved in the integration of constituent models may have working knowledge of the represented system and the operation of each model e g implementation and usage but are not necessarily disciplinary specialists themselves thus the primary concern in the initial stage is to gain confidence that operation of the iem is both conceptually and technically sound by testing the assumptions associated with the conceptual understanding iwanaga et al 2020a wilson et al 2017 falsifying the assumption that the model is integrated correctly also helps to preserve available computational budget in the development iems the relationships between all parameters and qois may not be fully known because of the complex model interactions that occur parameter activity inactivity may be a proxy that indicates correct model integration testing for the obvious behavior i e change of poi have flow on effects that should affect the qoi and continual confirmation that the behavior is present throughout the development cycle is valuable in that errors or conceptual mismatches could be highlighted and corrected earlier in the modeling cycle in other cases the conceptual understanding that the model integrator has may not be complete and so the testing process could be helpful in improving modelers understanding of the iem new knowledge or model configurations may invalidate previously obvious assumed behavior in which case the tests serve to alert modelers to a change in context change in context should subsequently be documented and the relevant tests updated to reflect this new understanding in this example the model integrator is principally focused on the policy surface water hydrology and farming system models however the entire model also includes representations of climate groundwater and ecology an example of the initially known interactions between the constituent models of interest is provided in fig 4 further description of the cim may be found in iwanaga et al 2020a interactions between all models affect the main quantity of interest selected here that is the long term surface water allocation index which indicates the average volume of water made available to water users over the simulation period the cim has 53 parameters which may all be varied runtime of the model is variable depending on the scenario being run but typically takes 30 minutes the influence of a single poi irrigation efficiency is investigated in this hypothetical context the parameter relates to the efficiency of water application for pipe and riser irrigations a common irrigation mode available to farmers in the campaspe region we consider the effect on modeled long term average surface water allocations rendering the poi static simulates an inadvertent change that introduced an implementation or integration error interactions between models are consequently inappropriately represented specific details of the poi are provided in table 3 irrigation efficiency relates to the proportion of water that reaches a crop s root zone the higher the efficiency rating the less water that is wasted or lost from a farmer s perspective to evaporation run off or deep drainage e g aquifer recharge hence the more efficient an irrigation system the less water required to maintain crop productivity for a given spatial area and the less water extracted from the dam water is allocated each year to farms by the policy model excessive use of water by farmers in one year can reduce farm water availability in subsequent years making efficient irrigations desirable given the available computational budget as contextualized in section 2 1 overnight execution of tests between 5pm and 9am i e 16 h would allow 128 model evaluations on a currently typical 4 core machine using eq 1 above assuming a consistent 30 min runtime a lesser number should be selected to ensure model runs resolve within the available time as runtime should not be expected to be consistent as explained in section 2 1 relevant to the point here is the application of the morris method for a realm model as reported in braddock and schreider 2006 the realm model is similar to the cim in terms of geographic region targeting the neighboring goulburn catchment and its use in water allocation modeling computational considerations which constrained the number of available model samples are highlighted therein use of cloud based testing infrastructure is ignored for the purpose of illustration and may potentially be cost prohibitive depending on the project budget it is unlikely that indicative results would be obtained with gsa development and use of emulators are similarly precluded given their requirement for sufficient areas of parameter space to be represented which is not possible within the allotted time 5 1 extremity testing results to demonstrate the diagnostic use of oat interactions between the surface water and policy models are initially deactivated such that dam level calculations never account for the volume of water used by farms therefore pipe and riser irrigation efficiency the poi will not directly influence the long term surface water allocation index the qoi functionally the deactivated parameter is equivalent to a dummy parameter in this example an extremity test is applied by those integrating the models the model integrators all 53 parameters are perturbed between their lowest and highest values with a cost of n 2 model runs unexpected results e g no change or smaller or larger than expected change indicate issues which modelers should investigate grouping parameters by their parent model components could also give at least an indication of which model in the iem the issue stems from the example results show that significantly higher volumes of water are allocated in the deactivated case much more than what would be expected under normal circumstances as shown in fig 5 the reason is that the farm water orders are never considered and the dam is never depleted such errors may inadvertently creep in during model development and integration examples include misunderstanding of the model interoperation e g what outputs from one model relate to an input to another implementation error e g a bug in a model or technical issues e g different compilers producing different machine code it is acknowledged here that the presented approach is viable in cases where the rough order of magnitude effect is known the results additionally indicate that the qoi will be affected even if the poi is completely deactivated suggesting that the qoi is affected by other factors thus the conceptual understanding depicted in fig 4 is not complete there are other factors which influence the qoi example code data and figures presented in this paper are provided as supplementary material via the open science framework see iwanaga 2020 iwanaga et al 2020b further description of other model analyses conducted on the cim can be found in iwanaga et al 2020a a single parameter can be targeted i e s 1 either after the above issue has been identified and further confirmation is desired or where the relative change from perturbing all parameters is unknown in this specific case any perturbation of the poi should be sufficient as the behavioral property being tested for is the presence of change in the qoi when all other parameters are set to their nominal best guess values an oat test with a further two model runs is thus applied to the poi illustrated in fig 6 because the interaction between models is disabled the results show no change in long term surface water allocation it is re emphasized here that the diagnostic property based test targets areas of parameter space for which the poi and qoi are expected to be sensitive and that diagnostic applications of sa should be conducted alongside other testing processes 5 2 a global approach to property based sa in this example parameter activity inactivity is used as a proxy to indicate correct model integration the relationship between the poi and qoi could be tested using r oat and morris to confirm the presence of some sensitivity across parameter space the r oat and morris methods are applied here given that they are known to provide reliable indications with fewer samples compared to other gsas as noted in section 2 1 samples were generated by producing n p 1 parameter sets such that n points in parameter space were sampled based on the targeting distribution as shown in fig 5 other factors may influence the qoi and so a non zero sensitivity value is to be expected given that these gsa approaches report the average effect with parameter interactions for this reason we adapt the dummy threshold approach from zadeh et al 2017 wherein a parameter is considered insensitive if the reported sensitivity value is comparable to the sensitivities reported for the dummy parameter in this case we apply such a threshold to indicate an unexpected lack of activity an activity threshold an activity threshold of 0 1 is empirically set for this example a value lower than expected sensitivities for the parameter in question for the available number of samples but higher than typical sensitivity thresholds e g 0 05 sarrazin et al 2016 as the poi is expected to be active its reported sensitivities should be above this threshold and values lower than the threshold indicate a cause for concern testing for the property of parameter activity in this manner is more robust compared to searching for absolute inactivity as computational precision error compounded as the models within the iem continually interact may introduce variability in results dunford et al 2015 unexpected interactions based on modelers current understanding of model interactions may also cause non zero sensitivities such tests could be incorporated as part of an automated test suite existing property based testing frameworks could also be leveraged to aid in pinpointing areas of parameter space wherein errors of concern occur e g löscher and sagonas 2017 the number of repetitions possible under the hypothetical 16 h time limit i e 5pm to 9am is n 2 i e total number of possible model runs is n 108 given that a model run takes roughly 30 min performing an additional repetition n 162 when n 3 would exceed the available time limit taking over 20 h we take 540 model evaluations i e n 10 purely to illustrate response of μ in this example indicative confirmation that the model is not behaving as expected could be obtained with n 2 using r oat n 108 see the disabled case in fig 7 in general how low n can be depends on the parameter and model context and some initial experimentation is likely required similar results may be obtained with the morris method fig 8 in the disabled case although in the active case concrete confirmation does not occur until n 9 i e n 486 the results for both i e no sensitivity at n 54 for r oat and insufficient activity until n 486 for morris suggest that the sampling scheme plays an important role in the efficacy of gsa methods for diagnostic purposes the diagnostic context severely limits the number of samples that could be obtained in a timely manner and for this reason other gsa methods were not wholly considered preliminary results are included in appendix a for the saltelli 2002 easi plischke 2010 and dmim plischke et al 2013 methods which indicate the unreliability of gsa methods at such low sample sizes these results indicate potential issues to be overcome if these methods are to be applied for diagnostic purposes in the context of rapid iterative model development and testing in cases where no issues are identified it is desirable for obtained samples to be reused to conserve computational budget 6 discussion and conclusions this paper outlined a role sa can play in software testing practices in the iem development process specifically local oat analyses coupled with r oat and or morris can provide a first pass indication of the correctness of technical and conceptual integration of constituent models particularly in terms of checking the effects of active inactive parameters on expected model behavior including such property based diagnostics as part of an automated test suite can aid in conserving a limited computational budget which is often desirable even in cases where there is an abundance of computational time available with the campaspe integrated model used as an example the lack of an expected relationship between a poi and qoi could be identified using extremity parameter value testing with just n 2 runs when an activity threshold is applied although global sensitivity analysis methods can be computationally demanding the use of r oat is shown to be a computationally efficient approach to assessing expected behavior although the morris method was able to identify inactivity of the parameter of interest it required more model runs to do so at least in the presented example case it is emphasized here that for the purpose of integrated model testing the number of parameters to be perturbed could be further reduced in all approaches described through qualitative assessment that identifies which parameters would have system level or inter system implications targeting these parameters or organizing these into groups such that the number of perturbations is much fewer than the total number of model parameters would reduce the overall computational effort involved to gain an indicative result additionally a failing test may negate the need to conduct further diagnostics as the assumption that the model is operating correctly is falsified thereby aiding in conserving computational budget more complete analyses could follow in cases where no property based sa tests fail in a full property based testing approach the framework applied would generate a random set of inputs and iteratively narrow the parameter space to specific areas that cause unexpected model behavior löscher and sagonas 2017 such tests could be augmented to use gsa methods that require comparatively limited number of samples such as r oat use of given data methods such as pawn pianosi and wagener 2015 hdmr li et al 2002 or methods with flexible sampling requirements such as star vars razavi et al 2019 could also be explored to identify potential advantages and limitations e g puy et al 2020 these given data methods may be more suitable in the iem context due to their ability to leverage available samples and may also be used to complement any diagnostic analyses conducted towards a comprehensive gsa mora et al 2019 the use of dummy parameters in combination with extremity testing under conditions in which model parameters or targeted subset of parameters are known to be active could also be explored alternate oat based global analyses that are potentially more efficient for obtaining indications of parameter interaction e g borgonovo 2010 may also be beneficial there are many approaches to validating computational models model developers can adopt a mix of testing practices from both software engineering and statistical mathematical analysis to cover the range of issues that may occur during model development ideally modelers would not restrict themselves to techniques found in one discipline over the other there are however barriers to the adoption of this hybrid approach for one it requires the technical knowledge and capacity of modelers to develop and maintain tests including the application of relevant sa techniques it is demonstrated here that a diagnostic property based testing approach with sa methods is a useful pragmatic and computationally efficient approach to providing a line of evidence that the model parameters are in fact having an expected effect in the iem development context any single model may itself require teams of domain specialists to fully understand and no single person can be expected to grasp all aspects especially in cases where legacy models are adopted assessing the expected behavioral properties of a model could be leveraged to reduce the time taken to identify and correct model implementation and integration errors declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors acknowledge support from the australian government research training program takuya iwanaga anu hilda john endowment fund takuya iwanaga xifu sun and barry croke china scholarship council qian wang and the australian research council discovery early career researcher award joseph guillaume project no de190100317 the campaspe integrated model was developed as part of the murray darling basin authority s partnership with the national centre for groundwater research and training ncgrt under contract no md2594 the authors would like to thank the reviewers for their comments and insight which greatly improved the quality and clarity of the paper appendix a here we showcase some preliminary results with three global sensitivity analysis approaches namely saltelli 2010 easi 2010 and dmim 2013 unreliable results are obtained for all approaches at these relatively low sample sizes easi and dmim are given data approaches for which morris samples are used for the saltelli method only first order indices s 1 are shown using the approach described in saltelli et al 2010 estimated with a cost of n p 2 in practice total first and second order indices may be estimated at a cost of n 2 p 2 runs saltelli 2002 the results for the saltelli analysis include negative values fig 9 which indicate an insufficient number of samples saltelli 2008 sharifi et al 2019 which is to be expected given the known high sampling requirements of sobol based approaches razavi and gupta 2015 fig 9 negative first order sensitivity values s1 from the saltelli method analysis for both inactive and active cases fig 9 the easi analysis technique indicates that an effect is occurring when interactions are disabled i e type i error as shown in fig 10 while the easi approach does not require a specific sampling scheme plischke et al 2013 the results produced may be sensitive to the sampling approach very little difference was found between disabled and enabled cases when results were obtained with morris sampling see fig 11 in both cases easi was unable to distinguish the lack of effect of an inactive parameter at these low sample sizes fig 10 first order effect s1 of irrigation efficiency on surface water allocations using oat samples easi analysis indicates the parameter is sensitive where model interactions are disabled fig 10 fig 11 example of easi analysis on results taken with morris sampling results of first order sensitivities s1 appear near identical indicating larger morris samples are necessary for a distinction to be made with easi note that the y axis for the top panel is in log scale fig 11 dmim was unable determine the lack of influence from the poi see fig 12 and fig 13 with similar issues fig 12 dmim analysis on oat samples fig 12 fig 13 dmim analysis on morris samples fig 13 when compared to the results of oat based gsa morris and r oat in the main text it appears that methods which indirectly estimate first order sensitivity while varying multiple parameters at once do not correctly identify inactive parameters at least at the given sample sizes furthermore a significantly larger number of model evaluations may be required to achieve convergence which may exceed the available time for diagnostic testing particularly in the case of complex and highly parameterized iems 
25856,numerical models are essential tools for understanding the complex and dynamic nature of the natural environment the ability to evaluate how well these models represent reality is critical in their use and future development this study presents a combination of changepoint analysis and fuzzy logic to assess the ability of numerical models to capture local scale temporal events seen in observations the fuzzy union based metric factors in uncertainty of the changepoint location to calculate individual similarity scores between the numerical model and reality for each changepoint in the observed record the application of the method is demonstrated through a case study on a high resolution model dataset which was able to pick up observed changepoints in temperature records over greenland to varying degrees of success the case study is presented using the datalabs framework a cloud based collaborative platform which simplifies access to complex statistical methods for environmental science applications keywords changepoints fuzzy logic data science uncertainty evaluation framework 1 introduction the natural environment is a complex system that evolves through time in response to drivers such as climate change economic change and social change ipcc 2018 schröter et al 2005 to understand the wide range of feedbacks and interactions involved in the earth system numerical models of varying complexities and computational requirements are becoming increasingly relied upon recent advances in high powered computing have resulted in models that are capable of running at finer spatial and temporal resolutions and or include more processes and thus better represent the dynamic natural environment collins et al 2011 gutjahr et al 2019 hu et al 2018 savage et al 2013 swart et al 2019 these developments are particularly important as many environmental processes are local in nature and exhibit high spatial variability e g air pollution episodes localised heavy rainfall or ice sheet melt therefore in theory finer resolution models should be able to better capture this variability than their coarser scale counterparts furthermore finer scale models are able to provide high resolution predictions of future environmental change under a warming climate however with this enhanced capability comes increased scrutiny of uncertainty in the model structure parameters and outputs beven 2006 and how these uncertainties are communicated to model users developers and ultimately decision makers this increasing need to quantify uncertainty in outputs along with the rapid rise in the volume and variety of big data in environmental science has resulted in increasingly complex datasets from which scientists wish to answer key questions the field of data science provides potential solutions to extract information from ever growing complex environmental datasets tso et al 2020 along with providing the ability to drive new scientific insight and better constrain uncertainties hollaway et al 2018 however utilisation of such techniques often requires experts from different domains to work together in an open and transparent way therefore there is a need to facilitate such collaborative efforts in order to use complex statistical methods to answer environmental science challenges an example of which is the evaluation of complex numerical models typically numerical models are evaluated against observations from a variety of different sources e g sensor networks or satellite data with global metrics often employed to assess how well the model captures the overall behaviour of the system gleckler et al 2008 pincus et al 2008 these integrated quantities often include the coefficient of determination r2 which assesses how well a numerical model represents the overall variance seen in the observations or the root mean square error rmse which evaluates the overall magnitude of the forecast errors by the numerical model whilst providing a good overall summary of model performance these metrics do have limitations in their use for example if the numerical model consistently over under predicts the observations it can still return a high r2 krause et al 2005 therefore it is often used in conjunction with other metrics such as rmse however this itself is scale dependant and therefore cannot be used to compare different model outputs furthermore many processes in the natural environment are local in both space and time and therefore a good numerical model performance using integrated metrics does not necessarily translate to the model being a good predictor at the local scale examples of such local scale events can be seen in long term atmospheric records over varying temporal scales these can range from abrupt shifts in the long term statistical properties of air temperature e g the rapid rise in global mean temperature in the late 1990s ipcc 2018 through to shorter term shifts associated with seasonal variability therefore if a numerical model is to be classed as suitably representing the reality of the natural environment it should be able to capture variability at a range of spatial and temporal scales with an acceptable degree of accuracy in recent years there has been a move towards incorporating advanced statistical techniques into climate model evaluation for example the analysis of extreme events leeson et al 2018 to date however no previous studies have focussed on the degree of accuracy to which models capture the timing of changepoints in the long term temperature record this study presents a new approach to numerical model evaluation that utilises changepoint analysis detection of shifts in the statistical properties of time series data to assess the ability of a model derived time series to capture different modes of temporal variability seen in the observed record changepoints are first identified in the modelled and observed time series then their locations together with an estimate of uncertainty calculated through bootstrap samples are used in combination with fuzzy logic to develop a metric which captures the degree to which the location of modelled changepoints agrees with those identified in the observations the method is then demonstrated using a case study of its application to a high resolution model reanalysis dataset designed to simulate the climate of the greenland ice sheet 2 materials and methods 2 1 changepoint detection in discrete time series changepoint detection is essentially a statistical method that is used to estimate the point or points in a time series where there is an abrupt shift in its statistical properties such as the mean variance or both conditional on an assumed model eckley et al 2011 for a discrete time series of ordered data y 1 n y 1 y 2 y n the optimal location and number of changepoints m are identified based on a chosen cost function and a penalty to avoid over fitting the number and locations of the changepoints in this study are identified using the pruned exact linear time pelt algorithm killick et al 2012 more detail on pelt can be found in killick et al 2012 but in short the algorithm performs an exact search of the time series and considers all possible combinations for any number of changepoints up to a maximum specified using a minimum segment length here the modified bayesian information criterion mbic zhang and siegmund 2007 is used in combination with pelt to detect the optimal number of changepoints in the time series this helps reduce the identification of short segments as the mbic penalty balances the overall fit against the length of each segment 2 2 estimating confidence intervals on changepoints in order estimate the uncertainty in the locations of the changepoints identified using pelt confidence intervals cis are constructed these are calculated as follows 1 for changepoint location k i isolate the segments to its left y k i 1 1 y k i and right y k i 1 y k i 1 and generate a bootstrap sample of each segment separately giving y k i 1 1 y k i and y k i 1 y k i 1 respectively 2 combine the output from step 1 and treat as a time series with a single changepoint this is the series defined by y k i 1 1 y k i 1 with length l i 3 estimate the location of the changepoint for this sample using the same approach used to calculate the original m changepoints 4 repeat steps 1 to 3 n times 5 calculate the 2 5 and 97 5 percentiles of the n bootstrap samples giving the lower and upper confidence interval for the changepoint location 6 repeat steps 1 5 for each of the m changepoints in the time series this will provide 95 confidence intervals for each changepoint identified by the pelt algorithm if another confidence interval range e g 90 or 99 is required the percentiles set in step 5 can be changed accordingly 2 3 comparing changepoints between 2 time series in order to compare the timing of changepoints between two time series and take account of the uncertainty in the estimation of the changepoint locations represented by the constructed cis a new metric is proposed that is based around fuzzy logic matthé et al 2006 meyer and hornik 2009 zadeh 1965 here the aim is to evaluate whether the numerical models are able to reproduce changepoints in the observational record and thus demonstrate the model s ability to capture key processes in the environmental system in their outputs note therefore that the changepoints and associated cis in the observed record are assumed to be the truth and are used as the benchmark with which to evaluate the model the observed and modelled changepoints are converted into triangular fuzzy numbers centred on the changepoint location with the corresponding upper and lower confidence intervals as boundaries a normalised similarity score is computed between each observed and climate model fuzzy pairs with a score of 0 indicating no similarity and a score of 1 indicating perfect similarity fig 1 in this case the measure is the jaccard similarity score which is the ratio between the fuzzy intersection i e the area containing membership to both fuzzy numbers and the fuzzy union i e the area containing membership of either of the fuzzy numbers being compared of each pair the performance of the climate model at each observed changepoint is recorded as the model changepoint that returns the highest similarity or 0 if no points show similarity if more than one modelled changepoint shows similarity with an observed the one returning the highest score will be associated with that observed changepoint if there are more modelled changepoints than observed some will not be included in this pairwise comparison conversely if there are more observed changepoints than model changepoints not every observed will have a match this approach ensures that each model changepoint is only associated with one observed the similarity scores are then summed across the total number of observed changepoints to give an overall score for the given climate model at that site and normalised to unity by dividing by the total number of observed changepoints thus allowing comparison of performance across different sites the total score thus ranges from 0 no observed changepoints captured to 1 all observed changepoints captured perfectly it should be noted that if the timing of the changepoint is captured perfectly by the climate model but the cis differ the normalised score will be lower than 1 as different cis indicate that the statistical representations of the two time series are different in addition the proportion of climate model changepoints that are observed changepoints is calculated as well as the proportion of observed changepoints that are captured using the above criteria here an observed changepoint is classed as captured if there is intersection with the ci of a climate model changepoint these summary metrics provide an overall view of how well the climate model captures the statistical properties of the observed dataset in terms of its marginal behaviour in time with the individual similarity scores highlighting which local in time events are particularly well or poorly captured by the model missing data in the observed time series are treated as missing at random and therefore are ignored in the analysis for consistency where there are missing data in the observed time series the corresponding data is removed from the numerical model time series in order to ensure a sufficient sample size for processing for changepoints a minimum threshold of 100 complete timesteps of data was assigned with any segment not meeting this threshold discarded from the analysis 2 4 case study application to the evaluation of a high resolution model reanalysis product over the greenland ice sheet the era5 dataset is a state of the art global reanalysis product developed by the european centre for medium range weather forecasting ecmwf which provides a detailed record of global atmospheric conditions from 1979 through to present day hersbach et al 2020 the dataset is based around the integrated forecasting system ifs and utilises data assimilation techniques using observations and satellite data to produce the final product representing the best state of global meteorological conditions at hourly intervals and 31 km horizontal resolution the era5 dataset supersedes the era interim reanalysis dataset dee et al 2011 which operates at lower temporal 6 hourly and spatial 79 km resolutions and has been commonly used to force regional climate models including over greenland fettweis et al 2013 2017 given the high temporal and spatial resolution of era5 along with the use of data assimilation of observations it is reasonable to expect that era5 should perform well in terms of accurately representing local scale temporal variability when compared to the observed record the changepoint evaluation method described above is applied to era5 air temperature time series data in order to evaluate its performance against observations from automatic weather stations awss from the greenland climate network gc net steffen et al 1996 this network of 18 awss provides amongst other key meteorological variables long term hourly temperature records from the mid 1990s through to the present day for this work hourly data is aggregated to daily mean temperature for the 14 awss with the most complete records for the period 2000 through to 2017 these time series are then analysed using the approach presented above and the locations of changepoints established as the changepoint detection algorithm is conditional on the assumed statistical model used to represent the temperature time series if the underlying data structure is poorly understood erroneous results can be produced beaulieu and killick 2018 the daily temperature time series in this study exhibit seasonality and auto correlation between days which must be accounted for in the fitting of the changepoint algorithm in this case pre screening of the data revealed a first order auto regressive model ar1 fitted to the data enabled the best detection of changepoints in the time series therefore an ar1 model is fitted to the data prior to the changepoint analysis pelt is then used on the residuals of the ar1 time series to identify changepoint locations based on a change in variance over time the same process is then applied to the model time series and the ability to capture the observed changepoint evaluated as the focus of this study is to develop a new approach to evaluating the ability of climate models to pick up local scale changes in the statistical properties of an observed temperature time series a marginal approach in space is taken and the changepoint analysis is applied to each of the 14 gc net aws sites independently for this case study all analysis were conducted using r version 3 5 3 r core team 2019 and executed within a jupyter notebook version 2 3 1 of the changepoint r package killick and eckley 2014 killick et al 2019 was used to detect changepoint locations using pelt killick et al 2012 and version 1 0 18 of the sets package meyer and hornik 2009 was used to calculate the fuzzy based evaluation scores 2 5 implementation of the method into the datalabs framework in order to champion open science collaboration and ease access to complex statistical methods for environmental science applications the method presented here is implemented into the datalabs framework hollaway et al 2020 these tools sit in a cloud based computational environment that can scale in resources depending on the complexity and volume of data that is required to be processed furthermore datalabs can provide access to analytical methods at different levels of abstraction ranging from raw code to a graphical user interface that drives the workflow this can foster collaboration between scientists of different areas of expertise in an open and transparent environment seen as a key advancement in the field of data science 3 results 3 1 identification of changepoints in the observed time series a summary of the changepoint locations estimated using pelt between 2000 and 2017 inclusive is shown in fig 2 in general most of the stations show 2 changepoints in each calendar year with the first typically falling during the spring march to may and the second falling in late summer early autumn august to october from the pelt fit not shown it is clear that the time series tends to be more variable during the winter months and less so during summer this allows physical inference to be made from the timing of the changepoints which potentially correspond to the onset and end of the ice melt season in a given year overall where there is data available periods of missing data are highlighted in grey in fig 2 this pattern of changepoint timing holds for most stations the exception is summit which returns no estimated changepoints for the period 2009 2011 during the estimation of the changepoint locations using the bootstrap approach described above the mean temperatures of each segment are also calculated the resulting warmest segments at each site red shading in fig 2 indicate that the 2012 summer season as inferred from the changepoint locations is warmest at 4 of the 14 measurements stations it is known that 2012 was particularly high melt year on greenland nghiem et al 2012 and thus this is a potential key event to focus on for the evaluation of the numerical model 3 2 evaluation of era5 in terms of capturing observed changepoints the changepoint analysis is repeated on the era5 temperature time series for the grid cells corresponding to each aws site to identify changepoint locations and associated confidence intervals as per the observations each location is treated independently in space the new fuzzy logic based metric is then used to evaluate the model s ability to capture the observed changepoints at each station fig 3 overall era5 captures the timing of the changepoints in the observations with varying degrees of accuracy returning overall similarity scores that range from 0 17 at jar2 to a best performance of 0 51 at summit this indicates that there are fairly significant differences between the timing of the changepoints seen in observations and corresponding estimated changepoints in the era5 simulated time series furthermore lower scores tend to occur at sites where the percentage of observed changepoints captured by era5 is low i e no intersection at all between the modelled and observed changepoint confidence intervals or the percentage of model changepoints that are true changepoints is also low fig 3 a closer look at the individual changepoint evaluation scores corresponding to each changepoint in the observations time series at the 2 best performing nasa u and summit and the 2 worst performing sites nasa se and jar2 provides further information to inform interpretation of the overall performance metric fig 4 at nasa u and summit era5 tends to produce high scores for capturing each individual changepoint particularly in the latter half of the record 2012 onwards where similarity scores of 0 65 or higher are frequently returned at summit from 2012 to 2017 the timing of the observed changepoint is captured perfectly on 4 occasions out of 11 and within 2 days on 4 other occasions resulting in similarity scores of 0 65 1 0 here the range in scores is driven by differences in the confidence intervals of the observed and climate model changepoints at nasu u for the same period the timing of observed changepoints is captured perfectly on 3 occasions out of 7 for the remaining 4 occasions in this period the model misses the timing by 2 10 days including failing to capture the observed changepoint in 2017 at all i e returns a similarity score of 0 again the range in similarity scores at nasa u for 2012 2017 0 18 0 82 for non zero similarity scores is also driven by differences in the confidence intervals era5 tends to capture a similar proportion of the observed changepoints at each site fig 3 resulting in the higher overall metrics when normalised across all changepoints 0 51 at summit and 0 44 at nasa u at nasa se and jar2 era5 captures far fewer of the observed changepoints and returns lower similarity scores when they are captured fig 4 at nasa se similarity scores of 0 45 0 82 are returned for 2015 2017 inclusive where the climate model captures the timing of the observed changepoints either exactly or within one day despite this similarity scores of less than 1 0 are due to differences in the confidence intervals overlap however from 2003 to 2006 7 changepoints are estimated in the observed time series with era5 failing to estimate any at all conversely at jar2 with the exception of 2010 era5 and observed changepoint pairs occur in most years fig 4 however the model tends to return lower similarity scores 0 03 0 75 with era5 failing to capture the timing of the observed changepoints by between 2 and 16 days overall despite capturing a smaller proportion of observed changepoints 17 out of 31 at nasa se than at jar2 14 out of 22 the generally higher similarity scores at nase se leads to a slightly better overall performance 0 19 compared to 0 17 at jar2 3 3 focus on capturing key events the 2012 warm year as highlighted in section 3 1 the changepoint analysis on the 14 aws temperature time series identified the summer months of 2012 as the warmest on average at 4 of the stations furthermore previous studies have highlighted that 2012 was an unusually warm year and one of extreme melt over the greenland ice sheet hanna et al 2014 nghiem et al 2012 as such this provides an ideal localised in time event to evaluate how well era5 captures the timings of these changepoints this can be done by critiquing the similarity scores at sites where the changepoint locations in the observations dataset could be interpreted as the start and end of the summer months table 1 the timing of the summer months is captured best at southdome fig 5 d with the model capturing the start of the season evaluation score of 0 712 better than the end 0 365 here the start of the season is captured perfectly with the non perfect similarity score being driven by the difference in confidence intervals lower panel fig 5d era5 estimates the end of the season as being 15 days later than the observations however given the relatively large uncertainty in the changepoint location there is a degree of overlap in the confidence intervals resulting in the similarity score of 0 365 the timing of the summer season is captured fully i e era5 returns non zero evaluation scores for both the start and the end at 4 of the remaining 5 sites swisscamp fig 5a dye2 fig 5b saddle fig 5c and nasa se fig 5e overall the timing of the start of the summer season is captured better than the end at dye2 0 528 0 045 for the start end saddle 0 170 0 098 and nasa se 0 101 0 003 with only dye2 capturing the timing perfectly the reverse is seen at swisscamp where the end of the season is captured very well 0 719 with the timing captured perfectly and the non prefect similarity score again driven by the overlap in the confidence intervals the start of the season is only just captured returns a non zero score of 0 016 estimating it to be about a month later than the observations fig 5a at the remaining sites era5 performs poorly either not estimating any changepoints at all for 2012 at nasa e not shown in fig 5 or failing to capture the start of the summer at all at jar2 the model estimating the changepoint around a month earlier than the observations with no overlap of the confidence intervals the end of the summer at jar2 is also captured poorly with era5 estimating the changepoint around 10 days earlier than the observations returning a low similarity score 0 106 the uncertainty in the changepoint locations as signified by the confidence intervals indicates that the underlying statistical representation of the time series from era5 differs to that of the observations at these sites leading to the discrepancies in the timings of the changepoints for example at jar2 at the start of summer the model displays much larger uncertainty fig 5f in the changepoint location indicating that the variance of the time series differs greatly to that of the observations this could indicate the methods used to produce the reanalysis time series potentially fail to capture the variability seen in reality on this particular event and location furthermore this also suggests that the summer season is offset a month earlier in the era5 model and could have implications for ice melt should the reanalysis product be used to drive other regional models simulating ice dynamics during the calculations of the confidence intervals for the changepoint locations the bootstrap samples were also used to estimate the corresponding segment means and associated confidence intervals given the uncertainty in the changepoint locations these were used to compute similarity scores using a similar fuzzy union approach to the changepoint evaluation table 1 of the sites that capture the summer season only 2 return non zero similarity scores for the segment means swisscamp and southdome at southdome despite capturing the timing of the season well era5 does not estimate the mean temperature well only returning a score of 0 112 in contrast at swisscamp the similarity score for the mean is slightly higher 0 125 however era5 fails to capture the start of the season well 4 discussion the method presented in this study provides a new approach for assessing numerical models by evaluating how well they capture observed local scale temporal events i e changepoints the fuzzy logic based metric also factors in the uncertainty in the changepoint locations represented by bootstrapping confidence intervals in both datasets into the evaluation the application of the method is demonstrated in the evaluation of the era5 reanalysis dataset using temperature data from the gc net monitoring network in greenland 4 1 understanding numerical model performance at capturing local scale temporal events using the normalised summary metric era5 returns overall similarity scores ranging from 0 17 to 0 51 fig 3 when averaged across all observed changepoints at a given site as similarity scores are also calculated for each individual changepoint these can give an indication of potential reasons for the model performing poorly at sites where era5 performs the best summit 0 51 and nasa u 0 44 the model tends to score 0 65 or higher for many changepoints in the latter half of the record this indicates that era5 not only captures the timing of the changepoints well but there is also strong overlap in the confidence intervals between the changepoints of each time series strong overlap indicates that accounting for uncertainty in the changepoint location there is little difference in the statistical properties of the time series in this case variance suggesting era5 is representing reality well for that particular event at sites where era5 performs poorly e g nasa se the model despite returning reasonably high scores when it does capture changepoints fails to capture the majority of changepoints at the start of the record leading to an overall low similarity score interestingly when compared across all sites the model tends to perform better at sites located in the dry snow zone of the ice sheet where there is very little melting and poorly at the ablation zone sites swisscamp jar and jar2 where there tends to be the most melt leeson et al 2018 which plays an important role in the surface mass balance of the ice sheet therefore as reanalysis datasets are often used to drive detailed regional climate models that include detailed representations of melt processes e g mar fettweis et al 2017 the failure to capture local scale events by datasets such as era5 could propagate through the model chain therefore consistency across sites in either the geography or timing of poor performance can be used to aid development efforts as to potential processes and feedbacks in the era5 model that require further investigation this offers potential advantages over using global metrics to evaluate model performance a comparison with evaluation using the traditional integrated metrics yields another advantage of utilising this local event based method as part of the model evaluation workflow table 2 shows the associated r2 for each site indicating that era5 generally captures the general temperature trends well r2 of 0 65 0 97 overall the sites where the r2 indicates good performance tend to also perform better at capturing the local scale events however there are some notable exceptions at jar 0 88 r2 and jar2 0 72 r2 that return good performance on the global metrics but produce some of the lowest scores using the changepoint metric 0 22 and 0 17 respectively therefore the good performance across the record does not translate to the ability to capture local scale events well this agrees with previous work where fidelity at global scales does not always translate to finer scale events when using complex computationally heavy models medley et al 2013 the individual similarity scores can also evaluate how well the model captures key events that are known to be important in this case using the changepoint locations 2012 was identified as a potential anomalous warm year with era5 able to capture this with mixed results the summer season is captured at 5 of the 7 sites that recorded changepoints that could be inferred as the start and end of the summer season with either missed changepoints or poor capturing of timing leading to poor performance at the other 2 sites further to this as the overall similarity scores are normalised across the record if there is a known event that is of critical importance for the model to capture greater weightings can be applied when the overall score is calculated as this method enables focus on localised events in time it could be used to flag other potential events in this case summer seasons that could also be anomalous and are critical for the climate model to capture the changepoint analysis used in this study suggests the 2012 summer season is warmest over the 20 year record of available data across a large number of sites which corresponds to one of the largest melt years in history over greenland hanna et al 2014 nghiem et al 2012 therefore the method could be run on longer term data records to highlight other extreme summer years in the past and provide further constraints on model evaluation the method utilised here can use changing patterns in the statistical properties of the data to detect the onset of particular events e g in this case summer season this can provide a more robust constraint on how well the model captures local scale temporal changes rather than simply looking at annual time series or arbitrary definitions of a season in this study the method was applied to detect changepoints on a seasonal scale however with the availability of long term records it could be adapted to critique the data for longer term changes this could be combined with using the technique to evaluate multiple climate models of different complexities and resolutions to assess whether the incorporation of more processes leads to better model fidelity at capturing local scale temporal events and would constitute a natural follow on study to this work 4 2 issues to consider despite the advantages this new model evaluation method offers there are some issues that need to be considered in its application particularly when using highly seasonal environmental data typically datasets from the environmental domain exhibit high levels of seasonality are non stationary and can exhibit high levels of auto correlation changepoint detection algorithms can be sensitive to all of these things and lead to the identification of spurious changepoints in the time series beaulieu et al 2012 beaulieu and killick 2018 and overfitting of the algorithm this can often make it difficult to make any inference as to the physical cause of the changepoint e g onset of a particular season or a change in instrument calibration or location therefore due to the changepoint algorithm being conditional on the underlying model that is specified for the data ar1 in this study if the underlying data structure is not adequately understood the method could produce erroneous results as discussed in beaulieu and killick 2018 the gc net temperature data and corresponding model data in this work were investigated for seasonality and autocorrelation prior to fitting of pelt and it was concluded that an ar1 model was the best model to fit to the data however not all environmental datasets would be suitable for fitting an ar1 model and therefore some prior exploration of the data is required finally the amount of missing data in the time series can also impact the ability to detect changepoints and result in issues of overfitting of pelt the method presented here allows specification of the minimum length of continuous data for the time series to have for pelt to be applied to detect changepoints in this case it was set at 100 days to focus on seasonal changes however this setting can be varied dependent on the temporal scale of features which want to be investigated 4 3 contribution to data science solutions to environmental science challenges to facilitate access to the complex workflow of integrating statistical i e changepoint analysis and process based models i e era5 for a range of users with different expertise the analytical workflow presented here is implemented into the datalabs framework these cloud based tools provide a consistent and coherent environment for scientists from different backgrounds e g environmental scientists statisticians and computer scientists to come together and collaborate in the development of novel methods furthermore through visualisation dashboards such as rshiny chang et al 2018 users can run complicated analytical methods without having to access complex code fawcett 2018 slater et al 2019 this enables the dissemination of results to a wide range of user abstractions the method presented above sits in a modular series of r markdown allaire et al 2018 notebooks that perform the data extraction the changepoint analysis itself and calculation of the fuzzy based evaluation metric finally an rshiny application sits over the r code in the notebooks which enables the user to explore the performance of the model at each site an example of the application is available with this manuscript the open and transparent nature of the datalabs enables the method to meet fair findable accessible interoperable and reusable wilkinson et al 2016 standard recommended for scientific data this allows users to understand the assumptions made in the execution of the workflow and enable reproducibility of the method and adaptability to datasets from other domains the user is also able to tailor the lab to bring in a different changepoint algorithm or indeed combine the analysis with other approaches that critique model performance e g the lab could be updated to also evaluate the climate models using extreme value theory as has been done previously by leeson et al 2018 this can serve as a key tool in facilitating the use of data science methods to tackle some of environmental sciences grand challenges blair et al 2019 5 conclusions a new approach to numerical model evaluation has been developed by utilising a combination of changepoint analysis using the pelt algorithm developed by killick et al 2012 and fuzzy logic to assess the ability of climate models to capture key events seen in the observed record uncertainty in the changepoint locations are used in combination with a fuzzy union based metric to assign individual similarity scores to each changepoint in the observations time series to measure how well the numerical model captures that particular changepoint this allows focus of the model evaluation to be placed on local scale temporal events and quantify whether strong performance using global integrated quantities translates to the local scale in addition the method can be used to identify common events that indicate good or poor performance highlighting potential areas to focus further model development on this was demonstrated through a case study using a regional climate model which was able to pick up observed changepoints in temperature records over greenland to varying degrees of success in order to facilitate access to data science and statistical approaches for environmental scientists the method has also been incorporated into the datalabs framework this allows users to interact in a collaborative way utilising the method standalone porting it to other datasets or combining it with other approaches e g extreme value theory leeson et al 2018 toulemonde et al 2015 for a more robust model evaluation exercise this helps provide a collaborative platform to tackle environmental data sciences grand challenges blair et al 2019 code and data availability software name fuzzy changepoint application to evaluate numerical model ability to capture important shifts in environmental time series hardware requirements pc system requirements windows linux program language r program size 60 kb licence ogl v3 available at the nerc environmental information data centre eidc https doi org 10 5285 49d04d55 90a7 4106 b8fe 2e75aba228e4 hollaway 2021 the r code to run the fuzzy changepoint based analysis case study presented in this paper is available as either a jupyter or r markdown notebook and is available on github https github com mjhollaway fuzzy cpt eval the accompanying r shiny application is available at the following url https dsne fuzzycpteval datalabs ceh ac uk the gc net weather station data are publically available for download from http cires1 colorado edu steffen gcnet the era5 reanalysis dataset is available for download from the copernicus climate change service c3s climate data store at https cds climate copernicus eu search text era5 type dataset declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work was funded by the uk engineering and physical sciences research council epsrc data science for the natural environment dsne project grant no ep r01860x 1 this work was also supported by the uk status change and projections of the environment project a national capability award funded by the uk natural environmental research council nerc ne r016429 1 additional funding for the datalabs came through a nerc capital bid as part of the environmental data services the authors would like to thank iain walmsley and oladimeji awe for assistance in deploying the shiny application in datalabs 
25856,numerical models are essential tools for understanding the complex and dynamic nature of the natural environment the ability to evaluate how well these models represent reality is critical in their use and future development this study presents a combination of changepoint analysis and fuzzy logic to assess the ability of numerical models to capture local scale temporal events seen in observations the fuzzy union based metric factors in uncertainty of the changepoint location to calculate individual similarity scores between the numerical model and reality for each changepoint in the observed record the application of the method is demonstrated through a case study on a high resolution model dataset which was able to pick up observed changepoints in temperature records over greenland to varying degrees of success the case study is presented using the datalabs framework a cloud based collaborative platform which simplifies access to complex statistical methods for environmental science applications keywords changepoints fuzzy logic data science uncertainty evaluation framework 1 introduction the natural environment is a complex system that evolves through time in response to drivers such as climate change economic change and social change ipcc 2018 schröter et al 2005 to understand the wide range of feedbacks and interactions involved in the earth system numerical models of varying complexities and computational requirements are becoming increasingly relied upon recent advances in high powered computing have resulted in models that are capable of running at finer spatial and temporal resolutions and or include more processes and thus better represent the dynamic natural environment collins et al 2011 gutjahr et al 2019 hu et al 2018 savage et al 2013 swart et al 2019 these developments are particularly important as many environmental processes are local in nature and exhibit high spatial variability e g air pollution episodes localised heavy rainfall or ice sheet melt therefore in theory finer resolution models should be able to better capture this variability than their coarser scale counterparts furthermore finer scale models are able to provide high resolution predictions of future environmental change under a warming climate however with this enhanced capability comes increased scrutiny of uncertainty in the model structure parameters and outputs beven 2006 and how these uncertainties are communicated to model users developers and ultimately decision makers this increasing need to quantify uncertainty in outputs along with the rapid rise in the volume and variety of big data in environmental science has resulted in increasingly complex datasets from which scientists wish to answer key questions the field of data science provides potential solutions to extract information from ever growing complex environmental datasets tso et al 2020 along with providing the ability to drive new scientific insight and better constrain uncertainties hollaway et al 2018 however utilisation of such techniques often requires experts from different domains to work together in an open and transparent way therefore there is a need to facilitate such collaborative efforts in order to use complex statistical methods to answer environmental science challenges an example of which is the evaluation of complex numerical models typically numerical models are evaluated against observations from a variety of different sources e g sensor networks or satellite data with global metrics often employed to assess how well the model captures the overall behaviour of the system gleckler et al 2008 pincus et al 2008 these integrated quantities often include the coefficient of determination r2 which assesses how well a numerical model represents the overall variance seen in the observations or the root mean square error rmse which evaluates the overall magnitude of the forecast errors by the numerical model whilst providing a good overall summary of model performance these metrics do have limitations in their use for example if the numerical model consistently over under predicts the observations it can still return a high r2 krause et al 2005 therefore it is often used in conjunction with other metrics such as rmse however this itself is scale dependant and therefore cannot be used to compare different model outputs furthermore many processes in the natural environment are local in both space and time and therefore a good numerical model performance using integrated metrics does not necessarily translate to the model being a good predictor at the local scale examples of such local scale events can be seen in long term atmospheric records over varying temporal scales these can range from abrupt shifts in the long term statistical properties of air temperature e g the rapid rise in global mean temperature in the late 1990s ipcc 2018 through to shorter term shifts associated with seasonal variability therefore if a numerical model is to be classed as suitably representing the reality of the natural environment it should be able to capture variability at a range of spatial and temporal scales with an acceptable degree of accuracy in recent years there has been a move towards incorporating advanced statistical techniques into climate model evaluation for example the analysis of extreme events leeson et al 2018 to date however no previous studies have focussed on the degree of accuracy to which models capture the timing of changepoints in the long term temperature record this study presents a new approach to numerical model evaluation that utilises changepoint analysis detection of shifts in the statistical properties of time series data to assess the ability of a model derived time series to capture different modes of temporal variability seen in the observed record changepoints are first identified in the modelled and observed time series then their locations together with an estimate of uncertainty calculated through bootstrap samples are used in combination with fuzzy logic to develop a metric which captures the degree to which the location of modelled changepoints agrees with those identified in the observations the method is then demonstrated using a case study of its application to a high resolution model reanalysis dataset designed to simulate the climate of the greenland ice sheet 2 materials and methods 2 1 changepoint detection in discrete time series changepoint detection is essentially a statistical method that is used to estimate the point or points in a time series where there is an abrupt shift in its statistical properties such as the mean variance or both conditional on an assumed model eckley et al 2011 for a discrete time series of ordered data y 1 n y 1 y 2 y n the optimal location and number of changepoints m are identified based on a chosen cost function and a penalty to avoid over fitting the number and locations of the changepoints in this study are identified using the pruned exact linear time pelt algorithm killick et al 2012 more detail on pelt can be found in killick et al 2012 but in short the algorithm performs an exact search of the time series and considers all possible combinations for any number of changepoints up to a maximum specified using a minimum segment length here the modified bayesian information criterion mbic zhang and siegmund 2007 is used in combination with pelt to detect the optimal number of changepoints in the time series this helps reduce the identification of short segments as the mbic penalty balances the overall fit against the length of each segment 2 2 estimating confidence intervals on changepoints in order estimate the uncertainty in the locations of the changepoints identified using pelt confidence intervals cis are constructed these are calculated as follows 1 for changepoint location k i isolate the segments to its left y k i 1 1 y k i and right y k i 1 y k i 1 and generate a bootstrap sample of each segment separately giving y k i 1 1 y k i and y k i 1 y k i 1 respectively 2 combine the output from step 1 and treat as a time series with a single changepoint this is the series defined by y k i 1 1 y k i 1 with length l i 3 estimate the location of the changepoint for this sample using the same approach used to calculate the original m changepoints 4 repeat steps 1 to 3 n times 5 calculate the 2 5 and 97 5 percentiles of the n bootstrap samples giving the lower and upper confidence interval for the changepoint location 6 repeat steps 1 5 for each of the m changepoints in the time series this will provide 95 confidence intervals for each changepoint identified by the pelt algorithm if another confidence interval range e g 90 or 99 is required the percentiles set in step 5 can be changed accordingly 2 3 comparing changepoints between 2 time series in order to compare the timing of changepoints between two time series and take account of the uncertainty in the estimation of the changepoint locations represented by the constructed cis a new metric is proposed that is based around fuzzy logic matthé et al 2006 meyer and hornik 2009 zadeh 1965 here the aim is to evaluate whether the numerical models are able to reproduce changepoints in the observational record and thus demonstrate the model s ability to capture key processes in the environmental system in their outputs note therefore that the changepoints and associated cis in the observed record are assumed to be the truth and are used as the benchmark with which to evaluate the model the observed and modelled changepoints are converted into triangular fuzzy numbers centred on the changepoint location with the corresponding upper and lower confidence intervals as boundaries a normalised similarity score is computed between each observed and climate model fuzzy pairs with a score of 0 indicating no similarity and a score of 1 indicating perfect similarity fig 1 in this case the measure is the jaccard similarity score which is the ratio between the fuzzy intersection i e the area containing membership to both fuzzy numbers and the fuzzy union i e the area containing membership of either of the fuzzy numbers being compared of each pair the performance of the climate model at each observed changepoint is recorded as the model changepoint that returns the highest similarity or 0 if no points show similarity if more than one modelled changepoint shows similarity with an observed the one returning the highest score will be associated with that observed changepoint if there are more modelled changepoints than observed some will not be included in this pairwise comparison conversely if there are more observed changepoints than model changepoints not every observed will have a match this approach ensures that each model changepoint is only associated with one observed the similarity scores are then summed across the total number of observed changepoints to give an overall score for the given climate model at that site and normalised to unity by dividing by the total number of observed changepoints thus allowing comparison of performance across different sites the total score thus ranges from 0 no observed changepoints captured to 1 all observed changepoints captured perfectly it should be noted that if the timing of the changepoint is captured perfectly by the climate model but the cis differ the normalised score will be lower than 1 as different cis indicate that the statistical representations of the two time series are different in addition the proportion of climate model changepoints that are observed changepoints is calculated as well as the proportion of observed changepoints that are captured using the above criteria here an observed changepoint is classed as captured if there is intersection with the ci of a climate model changepoint these summary metrics provide an overall view of how well the climate model captures the statistical properties of the observed dataset in terms of its marginal behaviour in time with the individual similarity scores highlighting which local in time events are particularly well or poorly captured by the model missing data in the observed time series are treated as missing at random and therefore are ignored in the analysis for consistency where there are missing data in the observed time series the corresponding data is removed from the numerical model time series in order to ensure a sufficient sample size for processing for changepoints a minimum threshold of 100 complete timesteps of data was assigned with any segment not meeting this threshold discarded from the analysis 2 4 case study application to the evaluation of a high resolution model reanalysis product over the greenland ice sheet the era5 dataset is a state of the art global reanalysis product developed by the european centre for medium range weather forecasting ecmwf which provides a detailed record of global atmospheric conditions from 1979 through to present day hersbach et al 2020 the dataset is based around the integrated forecasting system ifs and utilises data assimilation techniques using observations and satellite data to produce the final product representing the best state of global meteorological conditions at hourly intervals and 31 km horizontal resolution the era5 dataset supersedes the era interim reanalysis dataset dee et al 2011 which operates at lower temporal 6 hourly and spatial 79 km resolutions and has been commonly used to force regional climate models including over greenland fettweis et al 2013 2017 given the high temporal and spatial resolution of era5 along with the use of data assimilation of observations it is reasonable to expect that era5 should perform well in terms of accurately representing local scale temporal variability when compared to the observed record the changepoint evaluation method described above is applied to era5 air temperature time series data in order to evaluate its performance against observations from automatic weather stations awss from the greenland climate network gc net steffen et al 1996 this network of 18 awss provides amongst other key meteorological variables long term hourly temperature records from the mid 1990s through to the present day for this work hourly data is aggregated to daily mean temperature for the 14 awss with the most complete records for the period 2000 through to 2017 these time series are then analysed using the approach presented above and the locations of changepoints established as the changepoint detection algorithm is conditional on the assumed statistical model used to represent the temperature time series if the underlying data structure is poorly understood erroneous results can be produced beaulieu and killick 2018 the daily temperature time series in this study exhibit seasonality and auto correlation between days which must be accounted for in the fitting of the changepoint algorithm in this case pre screening of the data revealed a first order auto regressive model ar1 fitted to the data enabled the best detection of changepoints in the time series therefore an ar1 model is fitted to the data prior to the changepoint analysis pelt is then used on the residuals of the ar1 time series to identify changepoint locations based on a change in variance over time the same process is then applied to the model time series and the ability to capture the observed changepoint evaluated as the focus of this study is to develop a new approach to evaluating the ability of climate models to pick up local scale changes in the statistical properties of an observed temperature time series a marginal approach in space is taken and the changepoint analysis is applied to each of the 14 gc net aws sites independently for this case study all analysis were conducted using r version 3 5 3 r core team 2019 and executed within a jupyter notebook version 2 3 1 of the changepoint r package killick and eckley 2014 killick et al 2019 was used to detect changepoint locations using pelt killick et al 2012 and version 1 0 18 of the sets package meyer and hornik 2009 was used to calculate the fuzzy based evaluation scores 2 5 implementation of the method into the datalabs framework in order to champion open science collaboration and ease access to complex statistical methods for environmental science applications the method presented here is implemented into the datalabs framework hollaway et al 2020 these tools sit in a cloud based computational environment that can scale in resources depending on the complexity and volume of data that is required to be processed furthermore datalabs can provide access to analytical methods at different levels of abstraction ranging from raw code to a graphical user interface that drives the workflow this can foster collaboration between scientists of different areas of expertise in an open and transparent environment seen as a key advancement in the field of data science 3 results 3 1 identification of changepoints in the observed time series a summary of the changepoint locations estimated using pelt between 2000 and 2017 inclusive is shown in fig 2 in general most of the stations show 2 changepoints in each calendar year with the first typically falling during the spring march to may and the second falling in late summer early autumn august to october from the pelt fit not shown it is clear that the time series tends to be more variable during the winter months and less so during summer this allows physical inference to be made from the timing of the changepoints which potentially correspond to the onset and end of the ice melt season in a given year overall where there is data available periods of missing data are highlighted in grey in fig 2 this pattern of changepoint timing holds for most stations the exception is summit which returns no estimated changepoints for the period 2009 2011 during the estimation of the changepoint locations using the bootstrap approach described above the mean temperatures of each segment are also calculated the resulting warmest segments at each site red shading in fig 2 indicate that the 2012 summer season as inferred from the changepoint locations is warmest at 4 of the 14 measurements stations it is known that 2012 was particularly high melt year on greenland nghiem et al 2012 and thus this is a potential key event to focus on for the evaluation of the numerical model 3 2 evaluation of era5 in terms of capturing observed changepoints the changepoint analysis is repeated on the era5 temperature time series for the grid cells corresponding to each aws site to identify changepoint locations and associated confidence intervals as per the observations each location is treated independently in space the new fuzzy logic based metric is then used to evaluate the model s ability to capture the observed changepoints at each station fig 3 overall era5 captures the timing of the changepoints in the observations with varying degrees of accuracy returning overall similarity scores that range from 0 17 at jar2 to a best performance of 0 51 at summit this indicates that there are fairly significant differences between the timing of the changepoints seen in observations and corresponding estimated changepoints in the era5 simulated time series furthermore lower scores tend to occur at sites where the percentage of observed changepoints captured by era5 is low i e no intersection at all between the modelled and observed changepoint confidence intervals or the percentage of model changepoints that are true changepoints is also low fig 3 a closer look at the individual changepoint evaluation scores corresponding to each changepoint in the observations time series at the 2 best performing nasa u and summit and the 2 worst performing sites nasa se and jar2 provides further information to inform interpretation of the overall performance metric fig 4 at nasa u and summit era5 tends to produce high scores for capturing each individual changepoint particularly in the latter half of the record 2012 onwards where similarity scores of 0 65 or higher are frequently returned at summit from 2012 to 2017 the timing of the observed changepoint is captured perfectly on 4 occasions out of 11 and within 2 days on 4 other occasions resulting in similarity scores of 0 65 1 0 here the range in scores is driven by differences in the confidence intervals of the observed and climate model changepoints at nasu u for the same period the timing of observed changepoints is captured perfectly on 3 occasions out of 7 for the remaining 4 occasions in this period the model misses the timing by 2 10 days including failing to capture the observed changepoint in 2017 at all i e returns a similarity score of 0 again the range in similarity scores at nasa u for 2012 2017 0 18 0 82 for non zero similarity scores is also driven by differences in the confidence intervals era5 tends to capture a similar proportion of the observed changepoints at each site fig 3 resulting in the higher overall metrics when normalised across all changepoints 0 51 at summit and 0 44 at nasa u at nasa se and jar2 era5 captures far fewer of the observed changepoints and returns lower similarity scores when they are captured fig 4 at nasa se similarity scores of 0 45 0 82 are returned for 2015 2017 inclusive where the climate model captures the timing of the observed changepoints either exactly or within one day despite this similarity scores of less than 1 0 are due to differences in the confidence intervals overlap however from 2003 to 2006 7 changepoints are estimated in the observed time series with era5 failing to estimate any at all conversely at jar2 with the exception of 2010 era5 and observed changepoint pairs occur in most years fig 4 however the model tends to return lower similarity scores 0 03 0 75 with era5 failing to capture the timing of the observed changepoints by between 2 and 16 days overall despite capturing a smaller proportion of observed changepoints 17 out of 31 at nasa se than at jar2 14 out of 22 the generally higher similarity scores at nase se leads to a slightly better overall performance 0 19 compared to 0 17 at jar2 3 3 focus on capturing key events the 2012 warm year as highlighted in section 3 1 the changepoint analysis on the 14 aws temperature time series identified the summer months of 2012 as the warmest on average at 4 of the stations furthermore previous studies have highlighted that 2012 was an unusually warm year and one of extreme melt over the greenland ice sheet hanna et al 2014 nghiem et al 2012 as such this provides an ideal localised in time event to evaluate how well era5 captures the timings of these changepoints this can be done by critiquing the similarity scores at sites where the changepoint locations in the observations dataset could be interpreted as the start and end of the summer months table 1 the timing of the summer months is captured best at southdome fig 5 d with the model capturing the start of the season evaluation score of 0 712 better than the end 0 365 here the start of the season is captured perfectly with the non perfect similarity score being driven by the difference in confidence intervals lower panel fig 5d era5 estimates the end of the season as being 15 days later than the observations however given the relatively large uncertainty in the changepoint location there is a degree of overlap in the confidence intervals resulting in the similarity score of 0 365 the timing of the summer season is captured fully i e era5 returns non zero evaluation scores for both the start and the end at 4 of the remaining 5 sites swisscamp fig 5a dye2 fig 5b saddle fig 5c and nasa se fig 5e overall the timing of the start of the summer season is captured better than the end at dye2 0 528 0 045 for the start end saddle 0 170 0 098 and nasa se 0 101 0 003 with only dye2 capturing the timing perfectly the reverse is seen at swisscamp where the end of the season is captured very well 0 719 with the timing captured perfectly and the non prefect similarity score again driven by the overlap in the confidence intervals the start of the season is only just captured returns a non zero score of 0 016 estimating it to be about a month later than the observations fig 5a at the remaining sites era5 performs poorly either not estimating any changepoints at all for 2012 at nasa e not shown in fig 5 or failing to capture the start of the summer at all at jar2 the model estimating the changepoint around a month earlier than the observations with no overlap of the confidence intervals the end of the summer at jar2 is also captured poorly with era5 estimating the changepoint around 10 days earlier than the observations returning a low similarity score 0 106 the uncertainty in the changepoint locations as signified by the confidence intervals indicates that the underlying statistical representation of the time series from era5 differs to that of the observations at these sites leading to the discrepancies in the timings of the changepoints for example at jar2 at the start of summer the model displays much larger uncertainty fig 5f in the changepoint location indicating that the variance of the time series differs greatly to that of the observations this could indicate the methods used to produce the reanalysis time series potentially fail to capture the variability seen in reality on this particular event and location furthermore this also suggests that the summer season is offset a month earlier in the era5 model and could have implications for ice melt should the reanalysis product be used to drive other regional models simulating ice dynamics during the calculations of the confidence intervals for the changepoint locations the bootstrap samples were also used to estimate the corresponding segment means and associated confidence intervals given the uncertainty in the changepoint locations these were used to compute similarity scores using a similar fuzzy union approach to the changepoint evaluation table 1 of the sites that capture the summer season only 2 return non zero similarity scores for the segment means swisscamp and southdome at southdome despite capturing the timing of the season well era5 does not estimate the mean temperature well only returning a score of 0 112 in contrast at swisscamp the similarity score for the mean is slightly higher 0 125 however era5 fails to capture the start of the season well 4 discussion the method presented in this study provides a new approach for assessing numerical models by evaluating how well they capture observed local scale temporal events i e changepoints the fuzzy logic based metric also factors in the uncertainty in the changepoint locations represented by bootstrapping confidence intervals in both datasets into the evaluation the application of the method is demonstrated in the evaluation of the era5 reanalysis dataset using temperature data from the gc net monitoring network in greenland 4 1 understanding numerical model performance at capturing local scale temporal events using the normalised summary metric era5 returns overall similarity scores ranging from 0 17 to 0 51 fig 3 when averaged across all observed changepoints at a given site as similarity scores are also calculated for each individual changepoint these can give an indication of potential reasons for the model performing poorly at sites where era5 performs the best summit 0 51 and nasa u 0 44 the model tends to score 0 65 or higher for many changepoints in the latter half of the record this indicates that era5 not only captures the timing of the changepoints well but there is also strong overlap in the confidence intervals between the changepoints of each time series strong overlap indicates that accounting for uncertainty in the changepoint location there is little difference in the statistical properties of the time series in this case variance suggesting era5 is representing reality well for that particular event at sites where era5 performs poorly e g nasa se the model despite returning reasonably high scores when it does capture changepoints fails to capture the majority of changepoints at the start of the record leading to an overall low similarity score interestingly when compared across all sites the model tends to perform better at sites located in the dry snow zone of the ice sheet where there is very little melting and poorly at the ablation zone sites swisscamp jar and jar2 where there tends to be the most melt leeson et al 2018 which plays an important role in the surface mass balance of the ice sheet therefore as reanalysis datasets are often used to drive detailed regional climate models that include detailed representations of melt processes e g mar fettweis et al 2017 the failure to capture local scale events by datasets such as era5 could propagate through the model chain therefore consistency across sites in either the geography or timing of poor performance can be used to aid development efforts as to potential processes and feedbacks in the era5 model that require further investigation this offers potential advantages over using global metrics to evaluate model performance a comparison with evaluation using the traditional integrated metrics yields another advantage of utilising this local event based method as part of the model evaluation workflow table 2 shows the associated r2 for each site indicating that era5 generally captures the general temperature trends well r2 of 0 65 0 97 overall the sites where the r2 indicates good performance tend to also perform better at capturing the local scale events however there are some notable exceptions at jar 0 88 r2 and jar2 0 72 r2 that return good performance on the global metrics but produce some of the lowest scores using the changepoint metric 0 22 and 0 17 respectively therefore the good performance across the record does not translate to the ability to capture local scale events well this agrees with previous work where fidelity at global scales does not always translate to finer scale events when using complex computationally heavy models medley et al 2013 the individual similarity scores can also evaluate how well the model captures key events that are known to be important in this case using the changepoint locations 2012 was identified as a potential anomalous warm year with era5 able to capture this with mixed results the summer season is captured at 5 of the 7 sites that recorded changepoints that could be inferred as the start and end of the summer season with either missed changepoints or poor capturing of timing leading to poor performance at the other 2 sites further to this as the overall similarity scores are normalised across the record if there is a known event that is of critical importance for the model to capture greater weightings can be applied when the overall score is calculated as this method enables focus on localised events in time it could be used to flag other potential events in this case summer seasons that could also be anomalous and are critical for the climate model to capture the changepoint analysis used in this study suggests the 2012 summer season is warmest over the 20 year record of available data across a large number of sites which corresponds to one of the largest melt years in history over greenland hanna et al 2014 nghiem et al 2012 therefore the method could be run on longer term data records to highlight other extreme summer years in the past and provide further constraints on model evaluation the method utilised here can use changing patterns in the statistical properties of the data to detect the onset of particular events e g in this case summer season this can provide a more robust constraint on how well the model captures local scale temporal changes rather than simply looking at annual time series or arbitrary definitions of a season in this study the method was applied to detect changepoints on a seasonal scale however with the availability of long term records it could be adapted to critique the data for longer term changes this could be combined with using the technique to evaluate multiple climate models of different complexities and resolutions to assess whether the incorporation of more processes leads to better model fidelity at capturing local scale temporal events and would constitute a natural follow on study to this work 4 2 issues to consider despite the advantages this new model evaluation method offers there are some issues that need to be considered in its application particularly when using highly seasonal environmental data typically datasets from the environmental domain exhibit high levels of seasonality are non stationary and can exhibit high levels of auto correlation changepoint detection algorithms can be sensitive to all of these things and lead to the identification of spurious changepoints in the time series beaulieu et al 2012 beaulieu and killick 2018 and overfitting of the algorithm this can often make it difficult to make any inference as to the physical cause of the changepoint e g onset of a particular season or a change in instrument calibration or location therefore due to the changepoint algorithm being conditional on the underlying model that is specified for the data ar1 in this study if the underlying data structure is not adequately understood the method could produce erroneous results as discussed in beaulieu and killick 2018 the gc net temperature data and corresponding model data in this work were investigated for seasonality and autocorrelation prior to fitting of pelt and it was concluded that an ar1 model was the best model to fit to the data however not all environmental datasets would be suitable for fitting an ar1 model and therefore some prior exploration of the data is required finally the amount of missing data in the time series can also impact the ability to detect changepoints and result in issues of overfitting of pelt the method presented here allows specification of the minimum length of continuous data for the time series to have for pelt to be applied to detect changepoints in this case it was set at 100 days to focus on seasonal changes however this setting can be varied dependent on the temporal scale of features which want to be investigated 4 3 contribution to data science solutions to environmental science challenges to facilitate access to the complex workflow of integrating statistical i e changepoint analysis and process based models i e era5 for a range of users with different expertise the analytical workflow presented here is implemented into the datalabs framework these cloud based tools provide a consistent and coherent environment for scientists from different backgrounds e g environmental scientists statisticians and computer scientists to come together and collaborate in the development of novel methods furthermore through visualisation dashboards such as rshiny chang et al 2018 users can run complicated analytical methods without having to access complex code fawcett 2018 slater et al 2019 this enables the dissemination of results to a wide range of user abstractions the method presented above sits in a modular series of r markdown allaire et al 2018 notebooks that perform the data extraction the changepoint analysis itself and calculation of the fuzzy based evaluation metric finally an rshiny application sits over the r code in the notebooks which enables the user to explore the performance of the model at each site an example of the application is available with this manuscript the open and transparent nature of the datalabs enables the method to meet fair findable accessible interoperable and reusable wilkinson et al 2016 standard recommended for scientific data this allows users to understand the assumptions made in the execution of the workflow and enable reproducibility of the method and adaptability to datasets from other domains the user is also able to tailor the lab to bring in a different changepoint algorithm or indeed combine the analysis with other approaches that critique model performance e g the lab could be updated to also evaluate the climate models using extreme value theory as has been done previously by leeson et al 2018 this can serve as a key tool in facilitating the use of data science methods to tackle some of environmental sciences grand challenges blair et al 2019 5 conclusions a new approach to numerical model evaluation has been developed by utilising a combination of changepoint analysis using the pelt algorithm developed by killick et al 2012 and fuzzy logic to assess the ability of climate models to capture key events seen in the observed record uncertainty in the changepoint locations are used in combination with a fuzzy union based metric to assign individual similarity scores to each changepoint in the observations time series to measure how well the numerical model captures that particular changepoint this allows focus of the model evaluation to be placed on local scale temporal events and quantify whether strong performance using global integrated quantities translates to the local scale in addition the method can be used to identify common events that indicate good or poor performance highlighting potential areas to focus further model development on this was demonstrated through a case study using a regional climate model which was able to pick up observed changepoints in temperature records over greenland to varying degrees of success in order to facilitate access to data science and statistical approaches for environmental scientists the method has also been incorporated into the datalabs framework this allows users to interact in a collaborative way utilising the method standalone porting it to other datasets or combining it with other approaches e g extreme value theory leeson et al 2018 toulemonde et al 2015 for a more robust model evaluation exercise this helps provide a collaborative platform to tackle environmental data sciences grand challenges blair et al 2019 code and data availability software name fuzzy changepoint application to evaluate numerical model ability to capture important shifts in environmental time series hardware requirements pc system requirements windows linux program language r program size 60 kb licence ogl v3 available at the nerc environmental information data centre eidc https doi org 10 5285 49d04d55 90a7 4106 b8fe 2e75aba228e4 hollaway 2021 the r code to run the fuzzy changepoint based analysis case study presented in this paper is available as either a jupyter or r markdown notebook and is available on github https github com mjhollaway fuzzy cpt eval the accompanying r shiny application is available at the following url https dsne fuzzycpteval datalabs ceh ac uk the gc net weather station data are publically available for download from http cires1 colorado edu steffen gcnet the era5 reanalysis dataset is available for download from the copernicus climate change service c3s climate data store at https cds climate copernicus eu search text era5 type dataset declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work was funded by the uk engineering and physical sciences research council epsrc data science for the natural environment dsne project grant no ep r01860x 1 this work was also supported by the uk status change and projections of the environment project a national capability award funded by the uk natural environmental research council nerc ne r016429 1 additional funding for the datalabs came through a nerc capital bid as part of the environmental data services the authors would like to thank iain walmsley and oladimeji awe for assistance in deploying the shiny application in datalabs 
25857,although global circulation models gcms have been used for the reconstruction of precipitation for selected geological time slices there is a lack of a coherent set of precipitation models for the mesozoic cenozoic period the last 250 million years there has been dramatic climate change during this time period capturing a supercontinent hothouse climate and continental breakup and dispersal associated with successive greenhouse and ice house climate periods we present an approach that links climate sensitive sedimentary deposits such as coal evaporites and glacial deposits to a global plate model reconstructed paleo elevation maps and high resolution gcms via bayesian machine learning we model the joint distribution of climate sensitive sediments and annual precipitation through geological time and use the dependency between sediments and precipitation to improve the model s predictive accuracy our approach provides a set of 13 data driven global paleo precipitation maps between 14 and 249 ma capturing major changes in long term annual rainfall patterns as a function of plate tectonics paleo elevation and climate change at a low computational cost keywords paleo climate gaussian process bayesian methods forecasting precipitation 1 introduction palaeoclimatology refers to the study or reconstruction of ancient climates crowley and north 1991 bradley 1999 often linked to the goal of understanding the current climate and its potential future trajectories hansen and sato 2012 the two primary variables used to define climate are temperature and precipitation we focus on reconstructing the long term history of precipitation which is reflected in the geological record of climate sensitive sedimentary deposits boucot et al 2013a such a reconstruction involves several challenges first observational data constraining precipitation over geological time spans covering millions of years are sparse both temporally and spatially boucot et al 2013a second the information from observational data must be fused together with knowledge of the geophysical processes in a logically consistent statistical framework or model birchfield et al 1981 crowley 1988 glancy et al 1993 patzkowsky et al 1991 mcgehee and lehman 2012 stocker et al 1992 phipps et al 2013 ritz et al 2011 wang and mysak 2000 contreras et al 2019 arıkan 2015 sellwood and valdes 2006 third the data is often noisy and becomes increasingly uncertain the further we go back in time mann and rutherford 2002 steiger et al 2014 mcintyre and mckitrick 2009 these characteristics increase levels of uncertainty about ancient climates which must be accurately quantified for meaningful inference using the data and the model parameters the evolution of precipitation through geological time can be modelled using fully coupled global circulation models gcms e g herold et al 2011 lunt et al 2017 baatsen et al 2020 however a single model of this type for an individual geological time slice typically takes several months to run on a high performance computer this limits the usefulness of this approach to develop models over geologic time in addition the preparation of initial and boundary conditions for such models is time consuming only a limited number of geological time slices has been explored given the enormous computational resources for construction of a single model using gcms some models focused on past hothouse climates such as those in parts of the miocene herold et al 2011 and eocene baatsen et al 2020 periods a major challenge in this area of research is developing improved methods to quantify climate model uncertainty combining climate proxies with bayesian inference is seen as having great potential for assessing uncertainties and directly linking climate proxies with climate simulations lunt et al 2017 although machine learning methods have been used in data driven earth system science reichstein et al 2019 by coupling physical process models with observational data the development of this field is still in its infancy to our knowledge a machine learning approach has not been used for the reconstruction of the distribution of climate belts and precipitation over geological time this knowledge gap motivates our approach to fuse gcm precipitation simulations with proxies from the geological record to estimate paleo precipitation back to the assembly of the supercontinent pangea over 200 million years ago the prediction or reconstruction of precipitation through geological time is essential for understanding the evolution of earth s climate continental erosion and sedimentation as well as the associated carbon cycle our work builds on previous studies investigating the connection between climate sensitive lithologies and paleogeography paleoclimate and paleoenvironments ziegler et al 2003 recently cao et al 2019 reconstructed the most extensive available global scale compilation of lithologic indicators of climate belts including coals evaporites and glacial deposits boucot et al 2013a and tested the sensitivity of their latitudinal distributions to the uneven distribution of continental areas through time using a purely data driven approach they were able to evaluate how the paleo latitudinal distributions of climate sensitive lithologies have changed through geological time in response to plate motions mountain building biological evolution and paleoclimate conditions given lithological indicators and additional global precipitation gcm simulations we can extend this approach to use the same climate sensitive indicators to develop estimates of paleo precipitation bayesian inference has been used extensively for the estimation and quantification of uncertainty in paleoclimate reconstructions haslett et al 2006 li et al 2010 tingley and huybers 2010 carson et al 2018 ilvonen et al 2016 the models for these reconstructions are often expressed in a hierarchical form the top of the hierarchy specifies the model for observational data known as the likelihood and the next layers in the hierarchy describe the latent processes which give rise to these observations the latent processes have unknown parameters associated with them and the bottom layers of the hierarchy specify the prior distributions of these parameters tingley et al 2012 having specified a likelihood function and priors inference regarding the quantities of interest proceeds via the posterior distribution obtaining this posterior distribution often involves performing a high dimensional integration which has no closed form analytic solution as a result this distribution is usually approximated in two ways 1 sampling methods such as markov chain monte carlo mcmc hastings 1970 geman and geman 1984 gelfand and smith 1990 chandra et al 2019 scalzo et al 2019 pall et al 2020 and 2 variational methods which provides more computationally efficient approximations jordan et al 1999 wainwright and jordan 2008 our goal is to reconstruct precipitation across the globe for selected time frames taken from the miocene 14 million years ago to 249 million years back in time we present a bayesian framework for inference to jointly model the predictive distributions of both precipitation and climate sensitive deposits and place a gaussian process gp prior over the spatial models for paleoclimate reconstruction our framework employs mcmc via gibbs sampling for estimating the relevant posterior distributions such as the gp model hyperparameters as well as the joint predictive posterior distributions of the precipitation and deposits we note that we have a spatiotemporal missing data problem where we have missing deposits lithologies although it is common practice to impute missing values such an approach ignores the uncertainty associated with these imputed values to address this we model the joint distribution of lithologies and precipitation and treat the missing values as unknown parameters we account for the uncertainty surrounding the missing values by integrating or marginalising over all possible values using the mcmc framework 2 background the bayesian paradigm is a logically consistent framework to combine different pieces of information to make inference about unknown quantities via the posterior distribution information from expert opinion or prior knowledge can be incorporated via the prior distribution while information from data is incorporated via the likelihood function sen and stoffa 1996 the choice of likelihood function depends upon the type of observational data and assumptions regarding how the data is generated from the latent processes the latent processes may be simple and deterministic such as how the density of particular geological structures below the surface may impact gravity measurements made above the surface reid et al 2013 scalzo et al 2019 the latent processes may be deterministic yet complex such as the impact of environmental conditions on the development coralgal assemblages in a vertical reef drilled core pall et al 2020 the latent processes may also be stochastic such as earthquake fault rupture monterrubio velasco et al 2019 the prior distribution of the parameters which govern these processes should be informed by the physical characteristics regarding the problem such as precipitation values that must be positive and have a realistic upper limit chandra et al 2019 gaussian process priors gpp play an important role in the bayesian analysis of spatiotemporal data and have been used extensively for over 30 years they are an established method for geostatistics that focuses on spatial or spatiotemporal datasets diggle et al 1998 chiles and delfiner 2009 a gaussian process is used as a prior over an unknown function such a prior has many advantages first and most importantly it avoids specifying any parametric form for the function such as assuming the function is linear polynomial or a power function second it assumes that the posterior mean of the function changes smoothly where the degree of smoothness is estimated from the data so that over fitting is rarely problem third uncertainty intervals for the posterior distribution of the unknown function are easily obtained from the iterates of the mcmc scheme lastly it provides a means of obtaining estimates of the function and the corresponding uncertainties between observed data points kriging is an example of a gp regression which is more familiar to geologists bernardo et al 1998 gp regression has been used in many applications in a recent application marchant et al 2018 a gp regression was incorporated into a larger model to determine spatial demographic insights for criminology the dependency between crime rate and demographic indicators was assumed to be linear while the dependency between spatial location and crime rate after controlling for demographic information was assumed to follow a gp the advantage of this approach is that it provides a flexible fit to the data via the gp while allowing for inference to be made around the impact of demographic indicators on crime in this paper we use this approach in modelling paleolithic precipitation the relationship between precipitation and geographic variables including lithologies can be modelled via linear regression while the relationship between precipitation and spatial location can be modelled via a gp 3 data this paper provides reconstruction of paleo precipitation for selected timeslices between 14 and 249 million years ago ma as shown in table 1 the data used to do this can be grouped into three categories the first category is lithological indicators the second is precipitation simulations and the third is paleo coastlines and continental paleo elevation 3 1 lithological indicators we group available lithological indicators into three groups following boucot et al 2013a humid environments are represented by coal deposits and sediments with palm mangrove and crocodilian fossils arid environments are represented by evaporites calcrete bauxite laterite kaolinite and oolitic ironstones glacial environments are represented by tillite dropstones and glendonites we refer to these three types of lithologies as coal evaporite and glacial respectively we note that we do not have all of these lithological indicators across the reconstruction timeslices the lithologies depend on the climate conditions of the time slices and we have a significant amount of missing information as shown in table 1 in our analysis the climate sensitive geological indicators are reconstructed using a recent plate model matthews et al 2016 paired with a paleomagnetic reference frame as employed by cao et al 2019 in order to produce data on the presence of lithology types for reconstruction timeslices from 14 ma to 249 ma we note that we do not have all of these lithological indicators across the geological time period of interest the lithologies depend on the climate conditions of the time frame and we have a lot of missing information which is evident from our sparse dataset as shown in table 1 3 2 precipitation simulation the paleoclimate proxy data are complemented by global precipitation simulations from gcms for the mid miocene herold et al 2011 and late eocene baatsen et al 2020 epochs hence we use simulated precipitation datasets and proxy data from the mid miocene 14 ma and the late eocene 38 ma for model training and validation respectively the precipitation estimates at these two times are extracted for the sites at which time overlapping paleo climate proxies are available from boucot et al 2013b figs 1 and 2 presents the precipitation estimates from the mid miocene and late eocene epochs herold et al 2011 using the data from the mid miocene and eocene as training data the goal is to provide precipitation estimates for the test data namely those epochs without gcm simulated precipitation these precipitation data of the miocene and eocene are available on the spatial characteristics of sites across the globe at a grid of 2 5 resolution in longitude the latitude spacing is 2 5 at the equator and increases toward the poles to preserve the grid area for 13 reconstruction times following cao et al 2019 3 3 paleo coastlines and paleo elevation the paleogeographic maps of cao et al 2019 subdivide various timeslices in the geological past into different qualitative classes the majority of continental areas above sea level are classified as lowlands with the remainder of the continents above sea level subareas classified as mountains a further class that of ice sheets also exists for some cenozoic timeslices but is not relevant for our analysis since no data points are available for antarctica during these times we use a simple approach to convert these maps of qualitative paleoenvironment into a quantitative representation of paleotopography through time ziegler et al 1985 first we assign uniform elevation values to different classes we assigned 200 meters m for lowland areas to which we added an additional 1500 m of elevation for mountain ranges 3000 m for non collisional orogenic belts along the western margins of the americas and 4500 m for the collisional orogen between india and eurasia to smooth the transition between mountains lowlands and regions of the continents below sea level we applied a 1 buffer which is approximately 110 km km around the mountain regions to remove the lowland elevations we replaced them with elevation values by linear interpolation between the surrounding mountain and lowlands elevations finally we smoothed the grids with a 400 km wavelength low pass gaussian filter our analysis is further underpinned by paleo coastlines from the globalpaleo environments of cao et al 2019 back to 249 ma paleo coastlines provide information such as the distance to shoreline relative to a location of interest and orientation given by direction to the nearest paleo shoreline clockwise from north in degrees figs 3 and 4 present maps of paleo elevation which are used in our proposed model for precipitation estimation in developing our model we only use the spatial characteristics of the sites above sea level in a given age and thus the number of observations changes with each reconstruction time the time dependent site specific model parameters include distance to the nearest paleo shoreline orientation given by direction to the nearest paleo shoreline clockwise from north in degrees given as cosine and the paleo elevation given by meters we denote the number of sites with geological indicators for each time t to be n t for t 1 t where the subscript t refers to the reconstruction time and t 13 is the total number of times table 1 provides details for the number of observations regarding the spatial characteristics for each reconstruction time ma the number of observations on the deposit type coal evaporites and glaciers and other associated deposits for humid arid and glacial climates denoted by n d t and the number of precipitation estimates available for that era 4 methodology in order to avoid underestimating the uncertainty surrounding imputing the missing lithologies we model the joint posterior distribution of lithologies and precipitation hence we estimate the marginal posterior distribution of lithologies given the data and then model the conditional posterior distribution of precipitation the following sections provide further details of the approach 4 1 model and priors we wish to model the joint distribution of sedimentary deposits referred to as deposits from hereon denoted by z and precipitation denoted by y across the globe for a number of reconstruction times in the past conditional on some data we denote this distribution as p z y data the joint distribution of z and y conditional on the data can be decomposed into the posterior marginal distribution of deposits multiplied by the conditional distribution of precipitation here the conditioning is on the deposits as well as the data and we have p z y data p z data p y z data we define the observed precipitation and deposits to be y obs and z obs respectively where y obs y 1 obs y t obs with y t o b s y 1 t obs y n t t obs and y s t obs is the observed median annual precipitation at site s in time t for t 1 t the reconstruction times corresponding to the time indicator t are given in table 1 let z obs z 1 obs z t obs with z t obs z 1 t obs z n t t obs with z s t obs z 1 s t obs z d s t obs where z d s t obs 1 if a deposit of type d was observed at location s in time t and z d s t obs 0 otherwise for d 1 d and d is the number of deposit types we also have data available on the spatial characteristics of the site denoted collectively by x such as the latitude l a t longitude l o n elevation given in meters the shortest distance to paleo shoreline d 2 s given in kilometers and orientation to nearest paleo shoreline corresponding to d 2 s given by a n g l e cosine we transformed the variables so that the assumption of a linear relationship between the predictors and response with normally distributed errors it is convenient from a computational and estimation point of view to assume that the noise has a gaussian distribution in order for this assumption to be valid it is common practice to transform the data we take the cube root transformation of the median annual precipitation data precipitation 3 based on previous research stidd 1953 sanso and guenni 2000 we also take the square root to transform elevation and d 2 s and hence use them as variables hereafter we note that we do not use longitude however it is included indirectly by using the distance to coastlines this controls aridity in continental interiors and by using the a n g l e cosine of this distance to distinguish between the effect of eastern versus western coasts on precipitation the idea for using the coastal orientation originates from the need to distinguish between continental locations close to a western or eastern shoreline as these react differently to adjacent oceans climate and precipitation is moderated by the proximity to a western or eastern ocean because of the effect of the rotation of the earth and the coriolis force on ocean circulation and where upwelling of deep cold water occurs i e preferentially along west facing coastlines making these coastal regions cooler and reducing precipitation relative to their eastern counterparts the array x is defined as x x 1 x t where x t x 1 t x n t t with x s t x 1 s t x k s t and x k s t is the value of covariate k at location s in time t for t 1 t s 1 n t and k 1 k where k is the number of covariates in developing our proposed framework for modelling p z y data it is useful to look at the relationship between the spatial characteristics of a site and the precipitation and deposit type if recorded at that site fig 5 shows scatter plots of the predictor variables a elevation b d 2 s and c a n g l e cosine versus precipitation 3 we show the data for the miocene in blue and eocene in red the figure shows that we can fit a linear regression line for each predictor variable for each period we now outline our model for p z data and p y z data the data consists of the observed precipitation y obs the observed deposits z obs and the spatial characteristics given by x in defining our model for p z y data it will be useful to define subsets of the data x z x given x z l a t l o n and x y x given x y l a t elevation d 2 s a n g l e used to predict z and y respectively 4 1 1 model for deposits in this section we describe our model for p z data there are many other locations for which there are topographical observations than there are locations for there are deposit observations see table 1 thus our goal is to develop a flexible model for each time t to predict the probability of the presence of a deposit type d at a given location conditional on the location of the observed deposits given by z obs and spatial characteristics x z we assume that the spatial dependence among deposits of type d in time t is induced by a gaussian process g p prior wahba 1990 with mean function u α d t with u u 1 u n t and u s 1 l a t s l o n s for s 1 n t and α d t α 0 d t α 1 d t α 2 d t and covariance matrix τ d t 2 ω t we use the reproducing kernel hilbert space defined by a two dimensional thinplate gaussian process prior to construct ω t expressed as a linear combination of basis functions as described in wood et al 2013 nychka 1988 to achieve computational feasibility we truncate the basis expansion to the first p d e p 60 basis vectors where the subscript d e p denotes the number of basis functions used in our model for deposits we obtain the eigenvalue decomposition of ω t q t r t q t where q t is a n t p d e p matrix of the first p d e p eigen vectors r t is a p d e p p d e p diagonal matrix with entries corresponding to the first 60 eigenvalues and define w t q t r t 1 2 φ d t n 0 τ d t 2 i p d e p so that w t φ d t is approximately distributed g p 0 τ d t 2 ω t we model the probability of deposit d occurring at a location s and time t by 1 pr z d s t 1 x s t z ψ u s α d t w s t φ d t where ψ is the standard normal cumulative distribution function known as a probit link function and w s t is the s t h row of w t the probabilities in equation 1 are formulated as probit models so that the sampling scheme of albert and chib 1993 can be used to obtain draws of α d t and φ d t from the appropriate conditional posterior distribution as described in section 4 2 4 1 2 model for precipitation conditional on deposits modelling the dependency of precipitation on latitude and longitude across such a large time horizon is problematic due to plate tectonics and changing boundaries of the continents accordingly we do not include longitude since latitude plays the major role in climate conditions deutsch et al 2008 therefore our set of spatial characteristics x y used to predict precipitation and use only l a t elevation d 2 s and a n g l e as predictors additionally we condition on the type of deposits which are present at the site as these contain information regarding precipitation for example the locations where evaporites are present are likely to have lower than average precipitation while those where coal deposits are found are likely to have higher than average precipitation in our model p y z z x y we assume that the observed precipitation in time t at location s denoted by y s t conditional on observations in x s t y and z s t arise from a true signal plus some noise we obtain a non parametric yet parsimonious estimate of the dependence between precipitation and latitude by assuming this dependence is induced by a g p prior with mean μ l a t and covariance matrix λ 2 σ constructed using a low rank approximation to the reproducing kernel hilbert space defined by an integrated wiener process wood et al 2013 expressed as a linear combination of basis functions to achieve parsimony we truncate the basis expansion to the first p p r e c i p 10 basis vectors only where the subscript p r e c i p refers to precipitation we obtain the eigenvalue decomposition of σ q r q and define l q r 1 2 δ n 0 λ i 10 as in section 4 1 1 then the function which maps latitude to precipitation f l a t μ l a t l δ is approximately distributed as g p μ l a t λ σ note that we assume this function varies only with latitude and is constant across all reconstruction times this assumption is justified for the time range investigated here cao et al 2019 found that the latitudinal dependence of climate sensitive lithologies differs for timeslices before 300 ma when compared to timeslices after 300 ma where the palaeo latitudinal distribution of climate proxies particularly coal has remained similar to formally define the model let y s t be the cube root of precipitation at location s at time t our model is 2 y t 1 n t β 0 z t β z x t y β x l δ e s t with e s t n 0 σ 2 where x t y x 1 t y x n t t y x s t y l a t s e l e v a t i o n s t d 2 s s t a n g l e s t for s 1 n t β x β l a t β d 2 s β e l e v a t i o n β a n g l e and β z β c o a l β e v a p β g l a c let β β 0 β z β x be the regression coefficients of variables which comprise the linear component of the mean of y t and let h t be the matrix formed by formed by concatenating the vector 1 n t with the matrices z t x t y and so that h t 1 n t z t x t y we can decompose the mean of y t into its linear and non linear components and write equation 2 as 3 y t h t β l δ e s t our full model is now y t β h t l t δ σ 2 n h t β l t δ σ 2 i n t β n 0 c β i δ n 0 λ 2 i p p r e c z d t u w α φ s 1 n t b e φ u s α d t w s φ d t α d t n 0 c α i 3 φ d t n 0 τ d t 2 i p d e p σ cauchy λ cauchy 4 τ d t cauchy for d 1 3 t 1 t and where cauchy is the folded cauchy distribution which are commonly used prior for variance parameters with justifications given in gelmanet al 2006 a graphical representation of the model appears in fig 6 4 2 estimation via mcmc sampling scheme our goal is to estimate the joint predictive distribution of precipitation and deposits across time and space we take a bayesian approach and use the joint posterior distribution p y z data and estimate the respective parameters via mcmc sampling scheme as mentioned earlier the data consists of the observed values of precipitation and deposits y obs z obs as well as the observed factors x we only have observations on precipitation for the miocene t 1 and eocene t 3 there is no precipitation data available for other timeslices and the predictive distribution p y z data is given by 5 p y z data p y z θ data p θ data d θ where θ is a vector of unknown parameters which are needed to prescribe the joint posterior p y z data in this context the unknown parameters are θ β δ a φ σ 2 t λ where t τ 1 2 τ t 2 with τ t 2 τ 1 t 2 τ 2 t 2 τ 3 t 2 a α d 1 α d n t with α d t α 0 d t α 1 d t α 2 d t and φ φ d 1 φ d n t with φ d t φ 1 d t φ p d e p d t for t 1 t and d 1 2 3 as stated in section 4 1 1 the deposit data is sparse in space and time and the missing values are part of the parameter space contained in θ we define the indicator variable γ s t 1 to denote if the type of deposit is recorded at site s at time t and γ s t 0 otherwise we further define the set s 1 t s γ s t 1 to be the collection of sites for which deposit information is recorded and define s 0 t similarly the matrix z t is rearranged so that z t z s 1 t z s 0 t and n t i 1 n t γ s t is the number of sites for which deposit information is available at time t we approximate equation 5 by p y z data 1 j j 1 j p y s t θ j data where θ j are draws from the posterior distribution p θ data for j 1 j we use mcmc sampling to estimate the parameters in the model and to facilitate the sampling scheme we introduce the latent variables v d t v d 1 t v d n t t for t 1 t and d 1 2 3 as shown below v d s t z d s t 1 n r u s α d t w s t φ d t 1 v d s t z d s t 0 n r u s α d t w s t φ d t 1 where the subscripts r and r denote that the distribution has support only on the positive and negative real number line respectively then conditional on v d s t s the regression coefficients α d t and φ d t have distributions which are normally distributed the mcmc sampling scheme for a given number of iterations n iterates proceeds as follows initialize σ 2 t λ a φ z s 0 σ 2 0 t 0 λ 0 a 0 φ 0 z s 0 0 then for j 1 n iterates 1 for t 1 t and d 1 d a draw v d t j from p v d t z s 1 t z s 0 t j 1 u t w t α d t j 1 φ d t j 1 b draw α d t j and φ d t j jointly from p α d t φ d t u t w t v d t j τ d t 2 j 1 note that conditional on v d t α d t and φ d t are independent of z s 1 t and z s 0 t c draw τ d t 2 j from p τ d t 2 φ d t j d draw z s 0 t j from p z s 0 t u t w t v d t j α d t j φ d t j e form h t j 1 n t z s 1 t z s 0 t j x t y 2 draw β j and δ j from p β δ y h t j σ 2 j 1 λ j 1 3 draw σ 2 j from p σ 2 y h t j β j δ j 4 draw λ 2 j from p λ 2 δ j 5 repeat we note that the proposed framework assumes that the dependency between precipitation and covariates is constant over time the temporal structure is induced only because the values of these covariates change across time this simplification is made because there is only precipitation data from two epochs i e the mid miocene and the late eocene which is insufficient to directly estimate any temporal structure in precipitation 5 results 5 1 reconstruction of deposits recall that we are modelling the joint distribution p z y data p z data p y z data fig 7 panels a j presents the marginal posterior probability of a coal deposit across the globe for various reconstruction timeslices namely p z 1 t data for t 1 10 for 14 ma t 1 panel a to 182 ma t 10 panel j figs 8 and 9 are analogous plots for evaporites and glacial deposits namely p z 2 t data and p z 3 t data respectively fig 10 presents the results for all three types of deposits for 242 ma t 11 to 249 ma t 13 timeslices respectively note that the actual observations in the training data is given by the orange spots on the grid and the rest are estimations table 2 gives the misclassification rates for the deposits coal evaporite and glacial for the respective reconstruction times each time was treated independently and 90 percent of data was used for training and the remaining 10 for testing we note that in some cases the misclassification rate is not applicable n a for glacial deposits where we have no observations as shown in table 1 fig 11 shows uncertainty estimates surrounding the posterior probability of deposits for the first and the last timeslices only namely 14ma t 1 and 249 ma t 13 panels a and b show results for coal deposits panels c and d for evaporites and panels e and f for glacial deposits as expected the uncertainty is highest in locations for which no deposit information was recorded and lowest in those locations where deposit information was available in addition the uncertainty is higher for earlier timeslices than later ones 5 2 paleo precipitation estimation one of the most important drivers of precipitation is latitude deutsch et al 2008 since it has major effect on climate figs 12 and 13 plot latitude versus precipitation for the miocene t 1 and the eocene t 3 respectively the observations are colour coded according to deposit information black for coal red for evaporites yellow for glacial and blue for not available na it is clear from these figures that the relationship between latitude and precipitation is not only non linear it is also not well approximated by parametric nonlinear forms such as polynomials to investigate the relationship between latitude and precipitation we assume the dependence is induced by a g p prior as discussed in section 4 1 2 we provide plots of the 95 credible intervals for the posterior median of precipitation by the model assuming this prior and using latitude as the only covariate shaded function regions in figs 12 and 13 one of the model assumptions is that the relationship between latitude and precipitation is constant across all epochs figs 12 and 13 show that this assumption is reasonable both plots show local minima at 30 o n s and local maxima at about 55 o n s fig 12 shows the global maxima of median precipitation occurs at the equator however it is interesting to note that this maximum is shifted north by about 5 o for the eocene data in estimating the time varying posterior distribution of precipitation spatial data such as d 2 s e l e v a t i o n l a t i t u d e and paleo shoreline orientation a n g l e and geological data such as the deposit information from all epochs are used as well as precipitation data from both the miocene and eocene we account for the uncertainty in the missing deposit information by integrating over all possible values where the integration is with respect to the posterior distribution of the deposits figs 14 and 15 show the posterior median annual precipitation for time slices t 1 10 and t 11 13 respectively figs 16 and 17 show the posterior uncertainty surrounding the annual precipitation for time slices t 1 10 and t 11 13 respectively the uncertainty is computed by the difference of 5th and 95th percentile in predictions fig 18 shows the posterior distribution of the regression coefficients contained in β in equation 3 as expected the presence of a coal deposit is positively related to precipitation while the presence of an evaporite deposit is negatively related to precipitation our predicted precipitation maps figs 14 and 15 illustrate how the distribution of continents and oceans combined with latitude reflecting paleo temperature gradients control precipitation moving back in time from the late 14 ma to the early cenozoic 61 ma changing latitudinal positions of africa results in northern africa becoming wetter while central africa south of the equator becomes progressive drier fig 14 in contrast there is less of a change in broad precipitation patterns observed in the americas because their latitudinal positions have changed little during the cenozoic period india moves through the wet equatorial belt in the early mid cenozoic resulting in intense precipitation throughout most of the sub continent at 38 and 51 ma while at 61 ma most of india is located south of this belt leading to a progressive drying that intensifies in the cretaceous fig 14 as australia moves south back in geological time gradually closing the southeast indian ocean its southern half experiences a greening due to its movement into southern hemisphere mid latitudes this is marked by moderate rainfall reflecting eastward traveling depressions and fronts that yield rain throughout the year fig 14 throughout the cretaceous period patterns of annual precipitation are predicted to change more significantly back in time as the south atlantic closes creating a large landmass by joining south america and africa as seen at 129 ma fig 14 the elimination of western african coasts and the creation of a vast continental interior region at 129 ma results in a marked change in precipitation moderating the precipitation highs seen later at 101 and 77 ma along the coast of western africa between what is now nigeria and senegal fig 14 the large equatorial tethys gateway that existed in the early cretaceous between africa south america and eurasia results in an increase in precipitation along the northeastern coastline of africa seen at 129 ma moving back in time from the late into the early cretaceous india experiences a greening again as it moves into the southern mid latitude belt of moderate rainfall fig 14 moving into the jurassic a large southern continent gondwana forms whose interior is relatively dry but whose northern and southern portions straddle high to medium rainfall belts as visible at 154 ma fig 14 the early jurassic 182 ma exhibits a reduction in northern gondwana rainfall compared to the late jurassic 154 ma reflecting the elimination of central atlantic coastlines by closing the central north atlantic ocean and forming the supercontinent pangea which persists throughout the triassic 219 249 ma fig 15 pangea in the triassic shows a relatively stable precipitation pattern with two prominent rainfall lows centered around 35 north and south with an equatorial high and moderate mid latitude highs fig 15 southern hemisphere mid latitude rainfall is most intense in the late triassic at 219 ma while the early triassic is somewhat drier in gondwana s southern pangea s interior and along the southern perimeter of pangea fig 15 the main difficulty in comparing our results with other published work i e climate simulations is that precipitation as output of climate simulations is not frequently made available in addition the timeframes for which these models are run would not generally coincide with the timeframes for which we have predictions however we used the eocene simulation dataset as a test set where the training model was based on miocene data only we show the accuracy of the predictions by the root mean squared error rmse shown in table 3 and fig 19 the rmse takes into account the actual and predicted values for miocene and eocene precipitation m yr we notice that the rmse values become somewhat poorer for the eocene when compared to the miocene and from the histogram in fig 19 it is clear that the range of predictions for the eocene range from 0 25 to 2 5 while the actual data range from 0 to 4 5 hence the model does not seem to do well for extreme cases i e anomalously high precipitation values 6 discussion 6 1 mesozoic era the quality of our precipitation maps can be compared to published reconstructions and models of paleo climate belts and rainfall our oldest map centered on 249 ma fig 16c represents the earliest part of the triassic period the triassic is known for the driest climate in the last 500 million years driven by a large contiguous land area a small area of tropical ocean and expansive deserts lacking vegetation and a number of other factors hay and wold 1998 the precipitation model fig 16 agrees very well with a recent climate simulation for this time montenegro et al 2011 we have two prominent regions north and south of the equator with minimal precipitation leading to evaporation exceeding precipitation montenegro et al 2011 and the widespread occurrence of evaporites fig 8 our maps reveal a number of changes in the spatial extent of the equatorial humid belt while pangea was assembled in the triassic period roughly from 249 to 200 ma fig 16a and b and throughout pangea s initial breakup in the jurassic period postdating 200 ma but it is difficult to verify our models with other independent climate reconstructions as mid late triassic and jurassic climate models are sparse the cretaceous period commencing around 145 ma has been the subject of a number of climate models and proxy reconstructions e g ziegler et al 1987 bush and philander 1997 a cretaceous simulation of global precipitation bush and philander 1997 suggests that it was approximately 10 percent greater than at present with the only region of reduced precipitation occurring in southern central eurasia such a pattern of pronounced low latitude rainfall especially in regions that are deserts today such as northern africa while southern central eurasia is relatively dry is well reflected in our cretaceous precipitation reconstructions fig 15f h the existence of an equatorial seaway in the cretaceous period produces a large body of warm tropical water much unlike pangea s configuration leading to a relatively wet northern africa northern south america and southeast asia 6 2 cenozoic era the early cenozoic fig 15 d and e still resembles the cretaceous in terms of the persisting presence of an equatorial seaway relatively warm climate and a distribution of precipitation with only minor changes relative to the cretaceous a major change can be seen in the late eocene 38 ma fig 1 5c a period approaching the initial formation of antarctic ice sheets in the oligocene at 34 ma associated with gradual global cooling zachos et al 2001 this transition is associated with reduced global precipitation fig 15 c as compared to the relatively wet cretaceous and early cenozoic climate the initial expansion of antarctic ice sheets in the early oligocene is followed by a transient warming period climaxing at the mid miocene climate optimum this warming is associated with increased equatorial precipitation relative to 38 ma fig 15 a and b however the continuing dispersal of the continents following the breakup of pangea leads to northern africa moving out of the low latitude region characterised by high precipitation this results in a distinct reduction in rainfall in northern africa at 14 ma fig 15 a both the eocene and miocene precipitation reconstructions are well constrained via the two gcms we are using to train our model 7 conclusions and future work we present a bayesian machine learning framework for modelling the joint posterior distribution of precipitation and deposit presence across the global for 14 timeslices ranging from 14 ma to 249 ma in this challenging spatiotemporal paleoclimate reconstruction problem the posterior distribution accounts for the uncertainty in the missing deposit information by integrating over possible deposit locations using mcmc sampling to perform the required multidimensional integration our work represents one of the first attempts to couple physical process models with observational data in a fully probabilistic framework our precipitation maps can be used as inputs for surface process models that depend on rainfall through time as a boundary condition driving erosion and landscape evolution our reconstructions of climate sensitive lithologies provide a link between changes in long term climate and forest cover and also open the opportunity for a more differentiated reconstruction of different soil types including calcrete and laterite in the future data and software data and open source software from this research is available online 1 1 https github com earthbyte paleoclimate reconstruction all the model outputs precipitation and predicted lithology grids with separate uncertainty grids are made available on github and included with the paper 2 2 https github com earthbyte paleoclimate reconstruction tree master reconstruction prediction results depositsprecip declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement we would like to thank madhura killedar sebastian haan and david kohn from the sydney informatics hub of the university of sydney for providing informatics support we would like to thank simon williams and john cannon for providing the paleo elevation dataset r dietmar muller acknowledges support from the australian research council through grant ih130200012 s cripps and r chandra acknowledge support from the australian research council through grant ic190100031 appendix table 4 provides a summary of key terms used in the paper table 4 summary of key terms table 4 term description prior prior probability distribution expresses belief expert or prior knowledge about a quantity before taking into account evidence or data likelihood function a probabilistic measure of goodness of fit that considers data and model output posterior posterior probability distribution of an unknown quality is determined after taking into account prior and likelihood mcmc markov chain monte carlo used for sampling the posterior using likelihood and prior distribution lithology geological indicators such as coal evaporates and glacial deposits epoch geological time span in millions of years ma distance to shoreline gives the distance from the grid where the deposit was found to the nearest shoreline gaussian process model statistical model based on gaussian process precipitation condensation of atmospheric water vapor from clouds that includes rainfall and snow which are measured in meters per annum m a impute to estimate the value of a missing datapoint using statistical inference 
25857,although global circulation models gcms have been used for the reconstruction of precipitation for selected geological time slices there is a lack of a coherent set of precipitation models for the mesozoic cenozoic period the last 250 million years there has been dramatic climate change during this time period capturing a supercontinent hothouse climate and continental breakup and dispersal associated with successive greenhouse and ice house climate periods we present an approach that links climate sensitive sedimentary deposits such as coal evaporites and glacial deposits to a global plate model reconstructed paleo elevation maps and high resolution gcms via bayesian machine learning we model the joint distribution of climate sensitive sediments and annual precipitation through geological time and use the dependency between sediments and precipitation to improve the model s predictive accuracy our approach provides a set of 13 data driven global paleo precipitation maps between 14 and 249 ma capturing major changes in long term annual rainfall patterns as a function of plate tectonics paleo elevation and climate change at a low computational cost keywords paleo climate gaussian process bayesian methods forecasting precipitation 1 introduction palaeoclimatology refers to the study or reconstruction of ancient climates crowley and north 1991 bradley 1999 often linked to the goal of understanding the current climate and its potential future trajectories hansen and sato 2012 the two primary variables used to define climate are temperature and precipitation we focus on reconstructing the long term history of precipitation which is reflected in the geological record of climate sensitive sedimentary deposits boucot et al 2013a such a reconstruction involves several challenges first observational data constraining precipitation over geological time spans covering millions of years are sparse both temporally and spatially boucot et al 2013a second the information from observational data must be fused together with knowledge of the geophysical processes in a logically consistent statistical framework or model birchfield et al 1981 crowley 1988 glancy et al 1993 patzkowsky et al 1991 mcgehee and lehman 2012 stocker et al 1992 phipps et al 2013 ritz et al 2011 wang and mysak 2000 contreras et al 2019 arıkan 2015 sellwood and valdes 2006 third the data is often noisy and becomes increasingly uncertain the further we go back in time mann and rutherford 2002 steiger et al 2014 mcintyre and mckitrick 2009 these characteristics increase levels of uncertainty about ancient climates which must be accurately quantified for meaningful inference using the data and the model parameters the evolution of precipitation through geological time can be modelled using fully coupled global circulation models gcms e g herold et al 2011 lunt et al 2017 baatsen et al 2020 however a single model of this type for an individual geological time slice typically takes several months to run on a high performance computer this limits the usefulness of this approach to develop models over geologic time in addition the preparation of initial and boundary conditions for such models is time consuming only a limited number of geological time slices has been explored given the enormous computational resources for construction of a single model using gcms some models focused on past hothouse climates such as those in parts of the miocene herold et al 2011 and eocene baatsen et al 2020 periods a major challenge in this area of research is developing improved methods to quantify climate model uncertainty combining climate proxies with bayesian inference is seen as having great potential for assessing uncertainties and directly linking climate proxies with climate simulations lunt et al 2017 although machine learning methods have been used in data driven earth system science reichstein et al 2019 by coupling physical process models with observational data the development of this field is still in its infancy to our knowledge a machine learning approach has not been used for the reconstruction of the distribution of climate belts and precipitation over geological time this knowledge gap motivates our approach to fuse gcm precipitation simulations with proxies from the geological record to estimate paleo precipitation back to the assembly of the supercontinent pangea over 200 million years ago the prediction or reconstruction of precipitation through geological time is essential for understanding the evolution of earth s climate continental erosion and sedimentation as well as the associated carbon cycle our work builds on previous studies investigating the connection between climate sensitive lithologies and paleogeography paleoclimate and paleoenvironments ziegler et al 2003 recently cao et al 2019 reconstructed the most extensive available global scale compilation of lithologic indicators of climate belts including coals evaporites and glacial deposits boucot et al 2013a and tested the sensitivity of their latitudinal distributions to the uneven distribution of continental areas through time using a purely data driven approach they were able to evaluate how the paleo latitudinal distributions of climate sensitive lithologies have changed through geological time in response to plate motions mountain building biological evolution and paleoclimate conditions given lithological indicators and additional global precipitation gcm simulations we can extend this approach to use the same climate sensitive indicators to develop estimates of paleo precipitation bayesian inference has been used extensively for the estimation and quantification of uncertainty in paleoclimate reconstructions haslett et al 2006 li et al 2010 tingley and huybers 2010 carson et al 2018 ilvonen et al 2016 the models for these reconstructions are often expressed in a hierarchical form the top of the hierarchy specifies the model for observational data known as the likelihood and the next layers in the hierarchy describe the latent processes which give rise to these observations the latent processes have unknown parameters associated with them and the bottom layers of the hierarchy specify the prior distributions of these parameters tingley et al 2012 having specified a likelihood function and priors inference regarding the quantities of interest proceeds via the posterior distribution obtaining this posterior distribution often involves performing a high dimensional integration which has no closed form analytic solution as a result this distribution is usually approximated in two ways 1 sampling methods such as markov chain monte carlo mcmc hastings 1970 geman and geman 1984 gelfand and smith 1990 chandra et al 2019 scalzo et al 2019 pall et al 2020 and 2 variational methods which provides more computationally efficient approximations jordan et al 1999 wainwright and jordan 2008 our goal is to reconstruct precipitation across the globe for selected time frames taken from the miocene 14 million years ago to 249 million years back in time we present a bayesian framework for inference to jointly model the predictive distributions of both precipitation and climate sensitive deposits and place a gaussian process gp prior over the spatial models for paleoclimate reconstruction our framework employs mcmc via gibbs sampling for estimating the relevant posterior distributions such as the gp model hyperparameters as well as the joint predictive posterior distributions of the precipitation and deposits we note that we have a spatiotemporal missing data problem where we have missing deposits lithologies although it is common practice to impute missing values such an approach ignores the uncertainty associated with these imputed values to address this we model the joint distribution of lithologies and precipitation and treat the missing values as unknown parameters we account for the uncertainty surrounding the missing values by integrating or marginalising over all possible values using the mcmc framework 2 background the bayesian paradigm is a logically consistent framework to combine different pieces of information to make inference about unknown quantities via the posterior distribution information from expert opinion or prior knowledge can be incorporated via the prior distribution while information from data is incorporated via the likelihood function sen and stoffa 1996 the choice of likelihood function depends upon the type of observational data and assumptions regarding how the data is generated from the latent processes the latent processes may be simple and deterministic such as how the density of particular geological structures below the surface may impact gravity measurements made above the surface reid et al 2013 scalzo et al 2019 the latent processes may be deterministic yet complex such as the impact of environmental conditions on the development coralgal assemblages in a vertical reef drilled core pall et al 2020 the latent processes may also be stochastic such as earthquake fault rupture monterrubio velasco et al 2019 the prior distribution of the parameters which govern these processes should be informed by the physical characteristics regarding the problem such as precipitation values that must be positive and have a realistic upper limit chandra et al 2019 gaussian process priors gpp play an important role in the bayesian analysis of spatiotemporal data and have been used extensively for over 30 years they are an established method for geostatistics that focuses on spatial or spatiotemporal datasets diggle et al 1998 chiles and delfiner 2009 a gaussian process is used as a prior over an unknown function such a prior has many advantages first and most importantly it avoids specifying any parametric form for the function such as assuming the function is linear polynomial or a power function second it assumes that the posterior mean of the function changes smoothly where the degree of smoothness is estimated from the data so that over fitting is rarely problem third uncertainty intervals for the posterior distribution of the unknown function are easily obtained from the iterates of the mcmc scheme lastly it provides a means of obtaining estimates of the function and the corresponding uncertainties between observed data points kriging is an example of a gp regression which is more familiar to geologists bernardo et al 1998 gp regression has been used in many applications in a recent application marchant et al 2018 a gp regression was incorporated into a larger model to determine spatial demographic insights for criminology the dependency between crime rate and demographic indicators was assumed to be linear while the dependency between spatial location and crime rate after controlling for demographic information was assumed to follow a gp the advantage of this approach is that it provides a flexible fit to the data via the gp while allowing for inference to be made around the impact of demographic indicators on crime in this paper we use this approach in modelling paleolithic precipitation the relationship between precipitation and geographic variables including lithologies can be modelled via linear regression while the relationship between precipitation and spatial location can be modelled via a gp 3 data this paper provides reconstruction of paleo precipitation for selected timeslices between 14 and 249 million years ago ma as shown in table 1 the data used to do this can be grouped into three categories the first category is lithological indicators the second is precipitation simulations and the third is paleo coastlines and continental paleo elevation 3 1 lithological indicators we group available lithological indicators into three groups following boucot et al 2013a humid environments are represented by coal deposits and sediments with palm mangrove and crocodilian fossils arid environments are represented by evaporites calcrete bauxite laterite kaolinite and oolitic ironstones glacial environments are represented by tillite dropstones and glendonites we refer to these three types of lithologies as coal evaporite and glacial respectively we note that we do not have all of these lithological indicators across the reconstruction timeslices the lithologies depend on the climate conditions of the time slices and we have a significant amount of missing information as shown in table 1 in our analysis the climate sensitive geological indicators are reconstructed using a recent plate model matthews et al 2016 paired with a paleomagnetic reference frame as employed by cao et al 2019 in order to produce data on the presence of lithology types for reconstruction timeslices from 14 ma to 249 ma we note that we do not have all of these lithological indicators across the geological time period of interest the lithologies depend on the climate conditions of the time frame and we have a lot of missing information which is evident from our sparse dataset as shown in table 1 3 2 precipitation simulation the paleoclimate proxy data are complemented by global precipitation simulations from gcms for the mid miocene herold et al 2011 and late eocene baatsen et al 2020 epochs hence we use simulated precipitation datasets and proxy data from the mid miocene 14 ma and the late eocene 38 ma for model training and validation respectively the precipitation estimates at these two times are extracted for the sites at which time overlapping paleo climate proxies are available from boucot et al 2013b figs 1 and 2 presents the precipitation estimates from the mid miocene and late eocene epochs herold et al 2011 using the data from the mid miocene and eocene as training data the goal is to provide precipitation estimates for the test data namely those epochs without gcm simulated precipitation these precipitation data of the miocene and eocene are available on the spatial characteristics of sites across the globe at a grid of 2 5 resolution in longitude the latitude spacing is 2 5 at the equator and increases toward the poles to preserve the grid area for 13 reconstruction times following cao et al 2019 3 3 paleo coastlines and paleo elevation the paleogeographic maps of cao et al 2019 subdivide various timeslices in the geological past into different qualitative classes the majority of continental areas above sea level are classified as lowlands with the remainder of the continents above sea level subareas classified as mountains a further class that of ice sheets also exists for some cenozoic timeslices but is not relevant for our analysis since no data points are available for antarctica during these times we use a simple approach to convert these maps of qualitative paleoenvironment into a quantitative representation of paleotopography through time ziegler et al 1985 first we assign uniform elevation values to different classes we assigned 200 meters m for lowland areas to which we added an additional 1500 m of elevation for mountain ranges 3000 m for non collisional orogenic belts along the western margins of the americas and 4500 m for the collisional orogen between india and eurasia to smooth the transition between mountains lowlands and regions of the continents below sea level we applied a 1 buffer which is approximately 110 km km around the mountain regions to remove the lowland elevations we replaced them with elevation values by linear interpolation between the surrounding mountain and lowlands elevations finally we smoothed the grids with a 400 km wavelength low pass gaussian filter our analysis is further underpinned by paleo coastlines from the globalpaleo environments of cao et al 2019 back to 249 ma paleo coastlines provide information such as the distance to shoreline relative to a location of interest and orientation given by direction to the nearest paleo shoreline clockwise from north in degrees figs 3 and 4 present maps of paleo elevation which are used in our proposed model for precipitation estimation in developing our model we only use the spatial characteristics of the sites above sea level in a given age and thus the number of observations changes with each reconstruction time the time dependent site specific model parameters include distance to the nearest paleo shoreline orientation given by direction to the nearest paleo shoreline clockwise from north in degrees given as cosine and the paleo elevation given by meters we denote the number of sites with geological indicators for each time t to be n t for t 1 t where the subscript t refers to the reconstruction time and t 13 is the total number of times table 1 provides details for the number of observations regarding the spatial characteristics for each reconstruction time ma the number of observations on the deposit type coal evaporites and glaciers and other associated deposits for humid arid and glacial climates denoted by n d t and the number of precipitation estimates available for that era 4 methodology in order to avoid underestimating the uncertainty surrounding imputing the missing lithologies we model the joint posterior distribution of lithologies and precipitation hence we estimate the marginal posterior distribution of lithologies given the data and then model the conditional posterior distribution of precipitation the following sections provide further details of the approach 4 1 model and priors we wish to model the joint distribution of sedimentary deposits referred to as deposits from hereon denoted by z and precipitation denoted by y across the globe for a number of reconstruction times in the past conditional on some data we denote this distribution as p z y data the joint distribution of z and y conditional on the data can be decomposed into the posterior marginal distribution of deposits multiplied by the conditional distribution of precipitation here the conditioning is on the deposits as well as the data and we have p z y data p z data p y z data we define the observed precipitation and deposits to be y obs and z obs respectively where y obs y 1 obs y t obs with y t o b s y 1 t obs y n t t obs and y s t obs is the observed median annual precipitation at site s in time t for t 1 t the reconstruction times corresponding to the time indicator t are given in table 1 let z obs z 1 obs z t obs with z t obs z 1 t obs z n t t obs with z s t obs z 1 s t obs z d s t obs where z d s t obs 1 if a deposit of type d was observed at location s in time t and z d s t obs 0 otherwise for d 1 d and d is the number of deposit types we also have data available on the spatial characteristics of the site denoted collectively by x such as the latitude l a t longitude l o n elevation given in meters the shortest distance to paleo shoreline d 2 s given in kilometers and orientation to nearest paleo shoreline corresponding to d 2 s given by a n g l e cosine we transformed the variables so that the assumption of a linear relationship between the predictors and response with normally distributed errors it is convenient from a computational and estimation point of view to assume that the noise has a gaussian distribution in order for this assumption to be valid it is common practice to transform the data we take the cube root transformation of the median annual precipitation data precipitation 3 based on previous research stidd 1953 sanso and guenni 2000 we also take the square root to transform elevation and d 2 s and hence use them as variables hereafter we note that we do not use longitude however it is included indirectly by using the distance to coastlines this controls aridity in continental interiors and by using the a n g l e cosine of this distance to distinguish between the effect of eastern versus western coasts on precipitation the idea for using the coastal orientation originates from the need to distinguish between continental locations close to a western or eastern shoreline as these react differently to adjacent oceans climate and precipitation is moderated by the proximity to a western or eastern ocean because of the effect of the rotation of the earth and the coriolis force on ocean circulation and where upwelling of deep cold water occurs i e preferentially along west facing coastlines making these coastal regions cooler and reducing precipitation relative to their eastern counterparts the array x is defined as x x 1 x t where x t x 1 t x n t t with x s t x 1 s t x k s t and x k s t is the value of covariate k at location s in time t for t 1 t s 1 n t and k 1 k where k is the number of covariates in developing our proposed framework for modelling p z y data it is useful to look at the relationship between the spatial characteristics of a site and the precipitation and deposit type if recorded at that site fig 5 shows scatter plots of the predictor variables a elevation b d 2 s and c a n g l e cosine versus precipitation 3 we show the data for the miocene in blue and eocene in red the figure shows that we can fit a linear regression line for each predictor variable for each period we now outline our model for p z data and p y z data the data consists of the observed precipitation y obs the observed deposits z obs and the spatial characteristics given by x in defining our model for p z y data it will be useful to define subsets of the data x z x given x z l a t l o n and x y x given x y l a t elevation d 2 s a n g l e used to predict z and y respectively 4 1 1 model for deposits in this section we describe our model for p z data there are many other locations for which there are topographical observations than there are locations for there are deposit observations see table 1 thus our goal is to develop a flexible model for each time t to predict the probability of the presence of a deposit type d at a given location conditional on the location of the observed deposits given by z obs and spatial characteristics x z we assume that the spatial dependence among deposits of type d in time t is induced by a gaussian process g p prior wahba 1990 with mean function u α d t with u u 1 u n t and u s 1 l a t s l o n s for s 1 n t and α d t α 0 d t α 1 d t α 2 d t and covariance matrix τ d t 2 ω t we use the reproducing kernel hilbert space defined by a two dimensional thinplate gaussian process prior to construct ω t expressed as a linear combination of basis functions as described in wood et al 2013 nychka 1988 to achieve computational feasibility we truncate the basis expansion to the first p d e p 60 basis vectors where the subscript d e p denotes the number of basis functions used in our model for deposits we obtain the eigenvalue decomposition of ω t q t r t q t where q t is a n t p d e p matrix of the first p d e p eigen vectors r t is a p d e p p d e p diagonal matrix with entries corresponding to the first 60 eigenvalues and define w t q t r t 1 2 φ d t n 0 τ d t 2 i p d e p so that w t φ d t is approximately distributed g p 0 τ d t 2 ω t we model the probability of deposit d occurring at a location s and time t by 1 pr z d s t 1 x s t z ψ u s α d t w s t φ d t where ψ is the standard normal cumulative distribution function known as a probit link function and w s t is the s t h row of w t the probabilities in equation 1 are formulated as probit models so that the sampling scheme of albert and chib 1993 can be used to obtain draws of α d t and φ d t from the appropriate conditional posterior distribution as described in section 4 2 4 1 2 model for precipitation conditional on deposits modelling the dependency of precipitation on latitude and longitude across such a large time horizon is problematic due to plate tectonics and changing boundaries of the continents accordingly we do not include longitude since latitude plays the major role in climate conditions deutsch et al 2008 therefore our set of spatial characteristics x y used to predict precipitation and use only l a t elevation d 2 s and a n g l e as predictors additionally we condition on the type of deposits which are present at the site as these contain information regarding precipitation for example the locations where evaporites are present are likely to have lower than average precipitation while those where coal deposits are found are likely to have higher than average precipitation in our model p y z z x y we assume that the observed precipitation in time t at location s denoted by y s t conditional on observations in x s t y and z s t arise from a true signal plus some noise we obtain a non parametric yet parsimonious estimate of the dependence between precipitation and latitude by assuming this dependence is induced by a g p prior with mean μ l a t and covariance matrix λ 2 σ constructed using a low rank approximation to the reproducing kernel hilbert space defined by an integrated wiener process wood et al 2013 expressed as a linear combination of basis functions to achieve parsimony we truncate the basis expansion to the first p p r e c i p 10 basis vectors only where the subscript p r e c i p refers to precipitation we obtain the eigenvalue decomposition of σ q r q and define l q r 1 2 δ n 0 λ i 10 as in section 4 1 1 then the function which maps latitude to precipitation f l a t μ l a t l δ is approximately distributed as g p μ l a t λ σ note that we assume this function varies only with latitude and is constant across all reconstruction times this assumption is justified for the time range investigated here cao et al 2019 found that the latitudinal dependence of climate sensitive lithologies differs for timeslices before 300 ma when compared to timeslices after 300 ma where the palaeo latitudinal distribution of climate proxies particularly coal has remained similar to formally define the model let y s t be the cube root of precipitation at location s at time t our model is 2 y t 1 n t β 0 z t β z x t y β x l δ e s t with e s t n 0 σ 2 where x t y x 1 t y x n t t y x s t y l a t s e l e v a t i o n s t d 2 s s t a n g l e s t for s 1 n t β x β l a t β d 2 s β e l e v a t i o n β a n g l e and β z β c o a l β e v a p β g l a c let β β 0 β z β x be the regression coefficients of variables which comprise the linear component of the mean of y t and let h t be the matrix formed by formed by concatenating the vector 1 n t with the matrices z t x t y and so that h t 1 n t z t x t y we can decompose the mean of y t into its linear and non linear components and write equation 2 as 3 y t h t β l δ e s t our full model is now y t β h t l t δ σ 2 n h t β l t δ σ 2 i n t β n 0 c β i δ n 0 λ 2 i p p r e c z d t u w α φ s 1 n t b e φ u s α d t w s φ d t α d t n 0 c α i 3 φ d t n 0 τ d t 2 i p d e p σ cauchy λ cauchy 4 τ d t cauchy for d 1 3 t 1 t and where cauchy is the folded cauchy distribution which are commonly used prior for variance parameters with justifications given in gelmanet al 2006 a graphical representation of the model appears in fig 6 4 2 estimation via mcmc sampling scheme our goal is to estimate the joint predictive distribution of precipitation and deposits across time and space we take a bayesian approach and use the joint posterior distribution p y z data and estimate the respective parameters via mcmc sampling scheme as mentioned earlier the data consists of the observed values of precipitation and deposits y obs z obs as well as the observed factors x we only have observations on precipitation for the miocene t 1 and eocene t 3 there is no precipitation data available for other timeslices and the predictive distribution p y z data is given by 5 p y z data p y z θ data p θ data d θ where θ is a vector of unknown parameters which are needed to prescribe the joint posterior p y z data in this context the unknown parameters are θ β δ a φ σ 2 t λ where t τ 1 2 τ t 2 with τ t 2 τ 1 t 2 τ 2 t 2 τ 3 t 2 a α d 1 α d n t with α d t α 0 d t α 1 d t α 2 d t and φ φ d 1 φ d n t with φ d t φ 1 d t φ p d e p d t for t 1 t and d 1 2 3 as stated in section 4 1 1 the deposit data is sparse in space and time and the missing values are part of the parameter space contained in θ we define the indicator variable γ s t 1 to denote if the type of deposit is recorded at site s at time t and γ s t 0 otherwise we further define the set s 1 t s γ s t 1 to be the collection of sites for which deposit information is recorded and define s 0 t similarly the matrix z t is rearranged so that z t z s 1 t z s 0 t and n t i 1 n t γ s t is the number of sites for which deposit information is available at time t we approximate equation 5 by p y z data 1 j j 1 j p y s t θ j data where θ j are draws from the posterior distribution p θ data for j 1 j we use mcmc sampling to estimate the parameters in the model and to facilitate the sampling scheme we introduce the latent variables v d t v d 1 t v d n t t for t 1 t and d 1 2 3 as shown below v d s t z d s t 1 n r u s α d t w s t φ d t 1 v d s t z d s t 0 n r u s α d t w s t φ d t 1 where the subscripts r and r denote that the distribution has support only on the positive and negative real number line respectively then conditional on v d s t s the regression coefficients α d t and φ d t have distributions which are normally distributed the mcmc sampling scheme for a given number of iterations n iterates proceeds as follows initialize σ 2 t λ a φ z s 0 σ 2 0 t 0 λ 0 a 0 φ 0 z s 0 0 then for j 1 n iterates 1 for t 1 t and d 1 d a draw v d t j from p v d t z s 1 t z s 0 t j 1 u t w t α d t j 1 φ d t j 1 b draw α d t j and φ d t j jointly from p α d t φ d t u t w t v d t j τ d t 2 j 1 note that conditional on v d t α d t and φ d t are independent of z s 1 t and z s 0 t c draw τ d t 2 j from p τ d t 2 φ d t j d draw z s 0 t j from p z s 0 t u t w t v d t j α d t j φ d t j e form h t j 1 n t z s 1 t z s 0 t j x t y 2 draw β j and δ j from p β δ y h t j σ 2 j 1 λ j 1 3 draw σ 2 j from p σ 2 y h t j β j δ j 4 draw λ 2 j from p λ 2 δ j 5 repeat we note that the proposed framework assumes that the dependency between precipitation and covariates is constant over time the temporal structure is induced only because the values of these covariates change across time this simplification is made because there is only precipitation data from two epochs i e the mid miocene and the late eocene which is insufficient to directly estimate any temporal structure in precipitation 5 results 5 1 reconstruction of deposits recall that we are modelling the joint distribution p z y data p z data p y z data fig 7 panels a j presents the marginal posterior probability of a coal deposit across the globe for various reconstruction timeslices namely p z 1 t data for t 1 10 for 14 ma t 1 panel a to 182 ma t 10 panel j figs 8 and 9 are analogous plots for evaporites and glacial deposits namely p z 2 t data and p z 3 t data respectively fig 10 presents the results for all three types of deposits for 242 ma t 11 to 249 ma t 13 timeslices respectively note that the actual observations in the training data is given by the orange spots on the grid and the rest are estimations table 2 gives the misclassification rates for the deposits coal evaporite and glacial for the respective reconstruction times each time was treated independently and 90 percent of data was used for training and the remaining 10 for testing we note that in some cases the misclassification rate is not applicable n a for glacial deposits where we have no observations as shown in table 1 fig 11 shows uncertainty estimates surrounding the posterior probability of deposits for the first and the last timeslices only namely 14ma t 1 and 249 ma t 13 panels a and b show results for coal deposits panels c and d for evaporites and panels e and f for glacial deposits as expected the uncertainty is highest in locations for which no deposit information was recorded and lowest in those locations where deposit information was available in addition the uncertainty is higher for earlier timeslices than later ones 5 2 paleo precipitation estimation one of the most important drivers of precipitation is latitude deutsch et al 2008 since it has major effect on climate figs 12 and 13 plot latitude versus precipitation for the miocene t 1 and the eocene t 3 respectively the observations are colour coded according to deposit information black for coal red for evaporites yellow for glacial and blue for not available na it is clear from these figures that the relationship between latitude and precipitation is not only non linear it is also not well approximated by parametric nonlinear forms such as polynomials to investigate the relationship between latitude and precipitation we assume the dependence is induced by a g p prior as discussed in section 4 1 2 we provide plots of the 95 credible intervals for the posterior median of precipitation by the model assuming this prior and using latitude as the only covariate shaded function regions in figs 12 and 13 one of the model assumptions is that the relationship between latitude and precipitation is constant across all epochs figs 12 and 13 show that this assumption is reasonable both plots show local minima at 30 o n s and local maxima at about 55 o n s fig 12 shows the global maxima of median precipitation occurs at the equator however it is interesting to note that this maximum is shifted north by about 5 o for the eocene data in estimating the time varying posterior distribution of precipitation spatial data such as d 2 s e l e v a t i o n l a t i t u d e and paleo shoreline orientation a n g l e and geological data such as the deposit information from all epochs are used as well as precipitation data from both the miocene and eocene we account for the uncertainty in the missing deposit information by integrating over all possible values where the integration is with respect to the posterior distribution of the deposits figs 14 and 15 show the posterior median annual precipitation for time slices t 1 10 and t 11 13 respectively figs 16 and 17 show the posterior uncertainty surrounding the annual precipitation for time slices t 1 10 and t 11 13 respectively the uncertainty is computed by the difference of 5th and 95th percentile in predictions fig 18 shows the posterior distribution of the regression coefficients contained in β in equation 3 as expected the presence of a coal deposit is positively related to precipitation while the presence of an evaporite deposit is negatively related to precipitation our predicted precipitation maps figs 14 and 15 illustrate how the distribution of continents and oceans combined with latitude reflecting paleo temperature gradients control precipitation moving back in time from the late 14 ma to the early cenozoic 61 ma changing latitudinal positions of africa results in northern africa becoming wetter while central africa south of the equator becomes progressive drier fig 14 in contrast there is less of a change in broad precipitation patterns observed in the americas because their latitudinal positions have changed little during the cenozoic period india moves through the wet equatorial belt in the early mid cenozoic resulting in intense precipitation throughout most of the sub continent at 38 and 51 ma while at 61 ma most of india is located south of this belt leading to a progressive drying that intensifies in the cretaceous fig 14 as australia moves south back in geological time gradually closing the southeast indian ocean its southern half experiences a greening due to its movement into southern hemisphere mid latitudes this is marked by moderate rainfall reflecting eastward traveling depressions and fronts that yield rain throughout the year fig 14 throughout the cretaceous period patterns of annual precipitation are predicted to change more significantly back in time as the south atlantic closes creating a large landmass by joining south america and africa as seen at 129 ma fig 14 the elimination of western african coasts and the creation of a vast continental interior region at 129 ma results in a marked change in precipitation moderating the precipitation highs seen later at 101 and 77 ma along the coast of western africa between what is now nigeria and senegal fig 14 the large equatorial tethys gateway that existed in the early cretaceous between africa south america and eurasia results in an increase in precipitation along the northeastern coastline of africa seen at 129 ma moving back in time from the late into the early cretaceous india experiences a greening again as it moves into the southern mid latitude belt of moderate rainfall fig 14 moving into the jurassic a large southern continent gondwana forms whose interior is relatively dry but whose northern and southern portions straddle high to medium rainfall belts as visible at 154 ma fig 14 the early jurassic 182 ma exhibits a reduction in northern gondwana rainfall compared to the late jurassic 154 ma reflecting the elimination of central atlantic coastlines by closing the central north atlantic ocean and forming the supercontinent pangea which persists throughout the triassic 219 249 ma fig 15 pangea in the triassic shows a relatively stable precipitation pattern with two prominent rainfall lows centered around 35 north and south with an equatorial high and moderate mid latitude highs fig 15 southern hemisphere mid latitude rainfall is most intense in the late triassic at 219 ma while the early triassic is somewhat drier in gondwana s southern pangea s interior and along the southern perimeter of pangea fig 15 the main difficulty in comparing our results with other published work i e climate simulations is that precipitation as output of climate simulations is not frequently made available in addition the timeframes for which these models are run would not generally coincide with the timeframes for which we have predictions however we used the eocene simulation dataset as a test set where the training model was based on miocene data only we show the accuracy of the predictions by the root mean squared error rmse shown in table 3 and fig 19 the rmse takes into account the actual and predicted values for miocene and eocene precipitation m yr we notice that the rmse values become somewhat poorer for the eocene when compared to the miocene and from the histogram in fig 19 it is clear that the range of predictions for the eocene range from 0 25 to 2 5 while the actual data range from 0 to 4 5 hence the model does not seem to do well for extreme cases i e anomalously high precipitation values 6 discussion 6 1 mesozoic era the quality of our precipitation maps can be compared to published reconstructions and models of paleo climate belts and rainfall our oldest map centered on 249 ma fig 16c represents the earliest part of the triassic period the triassic is known for the driest climate in the last 500 million years driven by a large contiguous land area a small area of tropical ocean and expansive deserts lacking vegetation and a number of other factors hay and wold 1998 the precipitation model fig 16 agrees very well with a recent climate simulation for this time montenegro et al 2011 we have two prominent regions north and south of the equator with minimal precipitation leading to evaporation exceeding precipitation montenegro et al 2011 and the widespread occurrence of evaporites fig 8 our maps reveal a number of changes in the spatial extent of the equatorial humid belt while pangea was assembled in the triassic period roughly from 249 to 200 ma fig 16a and b and throughout pangea s initial breakup in the jurassic period postdating 200 ma but it is difficult to verify our models with other independent climate reconstructions as mid late triassic and jurassic climate models are sparse the cretaceous period commencing around 145 ma has been the subject of a number of climate models and proxy reconstructions e g ziegler et al 1987 bush and philander 1997 a cretaceous simulation of global precipitation bush and philander 1997 suggests that it was approximately 10 percent greater than at present with the only region of reduced precipitation occurring in southern central eurasia such a pattern of pronounced low latitude rainfall especially in regions that are deserts today such as northern africa while southern central eurasia is relatively dry is well reflected in our cretaceous precipitation reconstructions fig 15f h the existence of an equatorial seaway in the cretaceous period produces a large body of warm tropical water much unlike pangea s configuration leading to a relatively wet northern africa northern south america and southeast asia 6 2 cenozoic era the early cenozoic fig 15 d and e still resembles the cretaceous in terms of the persisting presence of an equatorial seaway relatively warm climate and a distribution of precipitation with only minor changes relative to the cretaceous a major change can be seen in the late eocene 38 ma fig 1 5c a period approaching the initial formation of antarctic ice sheets in the oligocene at 34 ma associated with gradual global cooling zachos et al 2001 this transition is associated with reduced global precipitation fig 15 c as compared to the relatively wet cretaceous and early cenozoic climate the initial expansion of antarctic ice sheets in the early oligocene is followed by a transient warming period climaxing at the mid miocene climate optimum this warming is associated with increased equatorial precipitation relative to 38 ma fig 15 a and b however the continuing dispersal of the continents following the breakup of pangea leads to northern africa moving out of the low latitude region characterised by high precipitation this results in a distinct reduction in rainfall in northern africa at 14 ma fig 15 a both the eocene and miocene precipitation reconstructions are well constrained via the two gcms we are using to train our model 7 conclusions and future work we present a bayesian machine learning framework for modelling the joint posterior distribution of precipitation and deposit presence across the global for 14 timeslices ranging from 14 ma to 249 ma in this challenging spatiotemporal paleoclimate reconstruction problem the posterior distribution accounts for the uncertainty in the missing deposit information by integrating over possible deposit locations using mcmc sampling to perform the required multidimensional integration our work represents one of the first attempts to couple physical process models with observational data in a fully probabilistic framework our precipitation maps can be used as inputs for surface process models that depend on rainfall through time as a boundary condition driving erosion and landscape evolution our reconstructions of climate sensitive lithologies provide a link between changes in long term climate and forest cover and also open the opportunity for a more differentiated reconstruction of different soil types including calcrete and laterite in the future data and software data and open source software from this research is available online 1 1 https github com earthbyte paleoclimate reconstruction all the model outputs precipitation and predicted lithology grids with separate uncertainty grids are made available on github and included with the paper 2 2 https github com earthbyte paleoclimate reconstruction tree master reconstruction prediction results depositsprecip declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement we would like to thank madhura killedar sebastian haan and david kohn from the sydney informatics hub of the university of sydney for providing informatics support we would like to thank simon williams and john cannon for providing the paleo elevation dataset r dietmar muller acknowledges support from the australian research council through grant ih130200012 s cripps and r chandra acknowledge support from the australian research council through grant ic190100031 appendix table 4 provides a summary of key terms used in the paper table 4 summary of key terms table 4 term description prior prior probability distribution expresses belief expert or prior knowledge about a quantity before taking into account evidence or data likelihood function a probabilistic measure of goodness of fit that considers data and model output posterior posterior probability distribution of an unknown quality is determined after taking into account prior and likelihood mcmc markov chain monte carlo used for sampling the posterior using likelihood and prior distribution lithology geological indicators such as coal evaporates and glacial deposits epoch geological time span in millions of years ma distance to shoreline gives the distance from the grid where the deposit was found to the nearest shoreline gaussian process model statistical model based on gaussian process precipitation condensation of atmospheric water vapor from clouds that includes rainfall and snow which are measured in meters per annum m a impute to estimate the value of a missing datapoint using statistical inference 
25858,increasing global demand for agricultural commodities has driven local land use cover change lucc and agricultural production across brazil during the 21st century modelling tools are needed to help understand the range of possible outcomes due to these telecoupled global to local relationships given future political economic and environmental uncertainties here we present crafty brazil a lucc model representing production of multiple agricultural commodities that accounts for spatially explicit e g land access and temporally contingent e g agricultural debt processes of importance across our nearly four million km2 brazilian study area we calibrate the model calibration for 2001 2018 and run tests and scenarios about commodity demand agricultural yields climate change and policy decisions for 2019 2035 results indicate greater confidence in modelled time series than spatial allocation we discuss how our approach might be best understood to be agency based rather than agent based and highlight questions more and less appropriate for this approach keywords land use cover telecoupling brazil crafty simulation agent based 1 introduction it is now well understood that local land use cover changes in many regions of the world are influenced by international demand for agricultural commodities and that socio ecological systems are often telecoupled over great distances liu et al 2013 2018 for example increased chinese demand for soybean over the last several decades has contributed to increased production in brazil making the country a key soybean production region in the global food system and driving change in local land use silva et al 2017 sun et al 2017 increased production has been achieved through a combination of expansion of agricultural land increases in yields and changes in farming practices including the development of a double crop system with maize predominantly as a second crop improvements in yields have come through improved seed varieties including genetic modification increases in agricultural inputs fertilisers pesticides machinery and economies of scale wesz 2016 these changes have come in tandem with significant economic changes in the brazilian farming system meaning that many farmers are faced by tough economic decisions to ensure the future viability of their businesses silva et al 2020 future uncertainty is further exacerbated by the spectre of climate change which may bring increased frequency of drought during the second crop and other conditions unfavourable to consistent production from year to year heinemann et al 2017 hampf et al 2020 to create a tool for examining the range of possible outcomes given such a range of drivers and uncertainties we set out to develop a spatially explicit land use cover change model capable of representing both production and associated land use of multiple agricultural commodities that could be subsequently linked to a system dynamics model of global trade see millington et al 2017 the telecoupling framework within which we developed our model emphasises the importance of agents and flows as drivers of change in coupled human natural systems that are linked across long distances to represent agency in land use and agricultural production decision making we use the previously developed competition for resources between agent functional types crafty modelling framework murray rust et al 2014 adapting it to improve representation of spatially explicit e g land access and temporally contingent e g agricultural debt processes of importance in our brazilian study area the crafty framework has been designed specifically with the intention of simulating broad scale land use change over large spatial extents national to continental for example blanco et al 2017 parameterised crafty to examine ecosystem services and decision making under scenarios of climate change for the entire land area of sweden over many decades brown et al 2019 used crafty to simulate land use change across the entire european union to investigate land manager behaviour at the continental scale in work similar to that presented here investigating the telecoupled effects of global food commodity trade between china and brazil on land use dou et al 2019 2020 developed a bespoke agent based model whereas the aims of that approach were to understand land use impacts at a fine scale for a single municipality our work aims to understand land use across much broader extents and hence the use of crafty is more appropriate millington et al 2017 in this paper we present the description and first results from the application of the crafty framework to simulate land use cover change over several decades for ten brazilian states an implementation we call crafty brazil we provide an overview of the endogenous and exogenous processes represented the data used to parameterize and calibrate the model and results from using crafty brazil to simulate scenarios of future change importantly we developed and tested crafty brazil using empirically grounded data in a fashion that has not previously been achieved for other applications of the crafty framework this empirically grounded approach aims to ensure internal model consistency but also identify key areas of uncertainty the model structure and results presented here are independent of the system dynamics model based on warner et al 2013 we ultimately intend to couple crafty brazil with however by examining scenarios of commodity demand agricultural yields climate change and policy decisions over land use rights we are able to both identify important uncertainties in the model but also shed light on important processes influencing land use change in brazil subsequently we reflect on what we have learned from this initial use of the model and discuss future directions in which this work should proceed 2 methods 2 1 study area and data crafty brazil was designed with the intention of subsequently connecting this spatial explicit model of land use cover change with a system dynamics model representing the international trade of three primary agricultural commodities soybean maize and beef with this focus our study area was designated as the 10 states in brazil that have been the dominant producers of soybean and maize in recent years and for which pasture area is widespread fig 1 our aim to simulate this region of 3 850 000 km2 for several decades required compromises on model spatial and temporal resolution to ensure feasible execution times while representing sufficient variation to explore system dynamics thus the model operates on a raster grid at a 5 km spatial resolution for a total of 162 026 simulated cells and we aggregate results to municipality level for our study area mean and median municipality areas are 1 040 and 398 km2 respectively these resolutions are appropriate given the available data needed to calibrate the model which comes from a variety of sources and with a range of original resolutions including many aggregated at municipality level table 1 furthermore the 5 km resolution is comparable to other applications of crafty across large spatial extents e g 1 km for blanco et al 2017 and 10 arcminute approx 18 km at the equator for brown et al 2019 the model runs with an annual timestep and we calibrate using data for 2001 2018 for model initialization and calibration we use land use cover luc data from the mapbiomas project version 4 mapbiomas 2018 original 30 m data were resampled to 5 km modal pixel class see millington 2019 and luc classes were reclassified from 27 classes to four cropland pasture nature and other appendix a these luc classes are associated with the production of services by different agent functional types as described below section 2 2 1 cropland represents land used to produce soybean maize and other crops e g rice sugarcane while pasture is assumed to represent all land area used for beef production nature represents vegetation cover not used for agriculture i e crops or pasture processes of natural vegetation dynamics are not explicitly represented although see use of nature capital in section 2 2 1 below but simulated land that is no longer needed for human use reverts to the nature class and nature cells can be converted to all other classes finally other represents land covers such as urban and inland water bodies and is assumed to not change in simulations that project into the future given uncertainty in the mapbiomas data and the multiple alternative ways those data might be re classified we examined several re classifications of the mapbiomas v4 0 data and used commodity planted area data from ibge to disaggregate ambiguous classes the overall accuracy for the mapbiomas v4 0 data for the period 2001 2018 at the level 2 classification is estimated to be 88 with the grassland class consistently the most poorly classified of all classes high rates of natural forest and pasture commission error see mapbiomas 2019b on comparing the implied beef pasture yields for luc classifications including excluding grassland in our pasture class we find a closer match with observed yields when grassland is included see millington 2019 the mapbiomas class mosaic of agriculture and pasture is also problematic given that our focus is on distinguishing between pasture and cropland to address this we used planted area data ibge 2019 to allocate pixels in the mapbiomas mosaic of agriculture and pasture class into either cropland or pasture classes see millington 2019 2 2 model description 2 2 1 services agents and capitals the crafty framework represents the production of land services by agent functional types afts through production functions for multiple capitals available in each cell representing a land unit murray rust et al 2014 cobb douglas production functions are used 1 p s c c i λ c a where λ c a is a weighting factor specific to capital c and aft a and p s is the productivity for service s in abstract production units later converted to kg for agricultural commodity services competition between afts is represented by calculating competitiveness using a utility function that accounts for production and marginal demand for each service in each timestep the competitiveness of afts is calculated for cells based on current capital values which crafty assumes are scaled 0 to 1 and service demands specified in abstract production units which in our case represent demand for agricultural commodities in domestic and international markets the aft with the greatest competitiveness is allocated to that cell subject to the giving in threshold of the currently occupying agent land may also be abandoned from agent use if competitiveness falls below an aft s giving up threshold for full details on the crafty framework see murray rust et al 2014 given our focus on soybean maize and beef crafty brazil represents provision of the following services soybean maize beef other crops ocrops nature and other the nature service represents the value of land not under human influence while other represents all other land not in other classes e g urban water the afts represented are soybean producers maize producers soybean maize double crop producers producing both services beef producers other crop producers and other land managers table 2 we developed these afts based on expert judgement and discussion with stakeholders in the study area based on the key services being simulated in the model and the key variations in strategies e g single vs double crop that currently exist a nature aft is also required for cells not under control of any other aft and representing land not under direct human land use regardless of whether that is primary or secondary vegetation representing double crop farmers as an individual aft is important as land managers have implemented the practice of sowing and planting a soybean crop early in the season followed by a late season crop usually maize the double cropping system of soybean and maize was initiated in brazil in the late 1990s as raising maize following soybean harvest provides protective cover for soils and the system eventually improves soil quality silva et al 2017 initial yields using this method were low but since the early 2000s many improvements have been made in management practices such as no tillage agriculture nitrogen biological fixation serviced by soybeans hybrid and gmo maize varieties which boosted yields for the second crop consequently this double cropping practice has grown through the 21st century pushed more by economic demand than by a desire for improved soil management silva et al 2017 although this system provides potential for both agronomic and economic gains over single crop systems the second crop growth is exposed to the risk of late season drought and consequent production losses brunini et al 2001 gonçalves et al 2002 for soybean the double cropping system has pushed production into a very short growing season forcing producers to adopt short cycle gmo varieties about 90 days from planting to harvest that are impacting soybean yields oliveira neto 2017 we represent other crops and other land managers as grouped afts that likely contain multiple management strategies because our focus here is on the production of soybean maize and beef 2 2 2 exogenous processes we use multiple crafty capitals to represent the numerous influences on the production of services table 3 for example important determinants of agricultural production in brazil have been found to include human capital technology generation and dissemination climate conditions and transport networks and land access pereira 2012 rada 2013 all capitals vary spatially across the study area except for tech capitals which are spatially uniform because these represent improvements in technology that lead to broad scale yield improvements through time including due to improved seed varieties machinery and fertilisers that are available widely values for capitals are provided exogenously i e from ancillary data sources except for the three access nature soybean maize other crops capitals and the conservation capital which are calculated endogenously i e during a simulation run from the dynamic spatial configuration of afts all endogenous and many exogenous capitals are updated in each timestep i e annually although some are updated less frequently see table 3 either because of data availability e g transport network or because the process they represent does not occur on an annual basis e g the soybean moratorium occurred in a given year see below all scripts to create files for initializing and updating capital values are available online millington 2020a we discuss exogenous capitals in the remainder of this section and endogenous capitals and representation of other processes in the following section section 2 2 3 the moisture capitals are derived from the monthly mean temperature and precipitation variables from the cru ts v 4 03 high resolution gridded datasets see harris et al 2020 and represent the role of climate on agricultural production to understand climatic limitations associated with plant growth and agricultural production we used the dryness index to describe the relation between water deficit and potential evapotranspiration pereira and pruitt 2004 both obtained from the thornthwaite and matter 1955 climatic water balance as implemented by victoria et al 2007 this index represents the water deficit in percentage of potential evapotranspiration and is calculated by the equation 2 di 100 def pet where di is the dryness index def is the water deficit and pet is potential evapotranspiration see table 2 in victoria et al 2007 and millington 2020a for full definition we calculate mean monthly di for two different growing seasons oct mar and jan jun to calculate two sets of moisture capital values to represent climate influence on single vs double crop production the use of these moisture capitals also allows us to investigate the possible influence of climate change on agricultural productivity in simulations of alternative future scenarios see section 2 3 below transport infrastructure is a key variable influencing the spatial distribution and volume of agricultural production related to land conversion e g soares filho et al 2006 weinhold and reis 2008 and both imports of agricultural inputs and exports of commodities to markets rada 2013 the transport capital uses data on the national road network dnit 2019 with locations and operating years of ports antaq 2019 to derive a spatial cost surface at a broad scale this cost surface weights the quality of transport infrastructure such that paved roads present lower cost than unpaved roads for access to land see victoria et al 2021 the use of the transport capital allows us to investigate the possible influence of future alternative infrastructure development on land use change and agricultural productivity at a broad scale the land value capital is used to represent the incentive for agents to convert nature land to agriculture in places where agricultural potential and infrastructure as represented by the previous capitals are poor but which have been converted e g based on mapbiomas lulcc between 1985 and 2018 in these frontier regions land prices are lower compared to other developed regions of southern brazil reflecting the challenges of making a profit from agricultural production in frontier lands these lower land prices provide an incentive to those willing to take a risk on developing land for grain production assuming that future improvements e g logistics infrastructure in the region will improve yields to pay off the risk previous studies have shown that as the agricultural sector develops in a given region land prices tend to increase alongside infrastructure and social standards rezende 2002 ferro and castro 2013 martinelli et al 2017 to develop this capital we used data from ieg fnp 2017 to represent the relative cost of land for new development as noted above improvement in agricultural productivity yields through time due to advances in technological resources such as improved machinery and seed varieties e g pereira 2012 are represented using the tech capitals because these capitals represent the aggregation of multiple sources of improvements in yield we calibrate their values by combining our observed land use cover data with commodity production data to calculate yields that are internally consistent within the model see millington 2020b the four protection capitals represent areas of land that cannot be used for soybean maize beef or other crops this exclusion may be because an area is designated as national or state parks indigenous lands or because of a policy that excludes production of a given commodity e g soybean moratorium for example the protection soybean capital is used in calibration runs section 2 3 to represent the soybean moratorium policy implemented in 2006 to discourage deforestation and limiting the market for soybean grown on deforested lands gibbs et al 2015 dou et al 2018 these capitals therefore also allow examination of simulation runs that implement similar policies finally the other capital is used to drive a high probability of cells being in the other land cover category e g urban and water based on observed land cover change for calibration and potentially for representation of future expansion of this land type in the crafty modelling framework demand for services is provided exogenously rather than incorporated as an endogenous process furthermore this demand is specified in the same abstract units used to represent services production section 2 2 1 above as our focus here is on soybean maize and beef we derive the abstract demand for these services from real production units kg while for other crops other and nature we derive demand based on land area ha see millington 2020b 2 2 3 endogenous processes processes driven by the spatial configuration or historical contingency of land resources and agent actions are represented in the model endogenously by updating cell or agent states dynamically during a simulation dependent on their circumstances in each timestep specifically we represent the spatial agglomeration effect of agricultural economies the tendency of land conversion to be spatially contagious vegetation regeneration processes and producer debt representing these processes required additions to the crafty source code see millington 2020c the importance of spatial proximity for driving land use change due to efficiencies afforded by agglomeration economies is well known e g fujita and krugman 1995 porter 2000 and has been shown to be important in brazil vera diaz et al 2008 garrett et al 2013 picoli et al 2020 to represent this in the model at a local level the access soy maize and access ocrops capitals are updated in each cell in each timestep during simulation runs based on whether one of these agent types is present within the eight neighbouring cells moore neighbourhood with value 0 05 if the target agent type is not present 0 95 if the agent type is present and 1 0 if the cell is occupied by the specified aft similarly conversion of natural land for agriculture is known to be well modelled as a contagious process of spread from existing cultivated areas at the edge of natural lands e g rosa et al 2013 to represent spatial access to natural land at a local level the access nature capital is updated in each cell in each timestep during simulation runs based on the adjacency of nature and non nature land covers for any given cell if the moore neighbourhood is composed entirely of nature cells the capital takes a value of 1 0 if between 1 and 7 cells in the moore neighbourhood are nature cells the cell takes a value of 0 75 and finally a nature access capital value of 0 0 is taken if all neighbouring cells are non nature through time we represent regeneration of natural vegetation following land abandonment by modifying conservation capital cell values during a simulation based both on the time since last human disturbance but also the type of disturbance several recent studies support the hypothesis that forest regeneration rate is related to intensity of previous land use mesquita et al 2015 jakovac et al 2015 martines ramos et al 2016 and here we assume that the rate of regeneration is faster following extensive pasture land use than intensive crop soybean maize land uses hence for cells in a simulation run that have never been disturbed i e have always had a nature land cover the conservation capital will have value 1 0 the conservation capital value is reduced to 0 4 if converted from nature to pasture and to 0 0 if converted to one of the other non nature land covers following abandonment of a non nature use conservation capital is increased by a value of 0 01 each timestep the final endogenous process we represent is the accrual and repayment of debt by those producers changing land uses new producers often need to take out loans to pay for land new machinery seed and other start up costs producers can be trapped into activities needed to earn profits to make repayments e g silva et al 2020 and changes in land use are unlikely during the repayment period to represent this inertia following conversion we prevent new agricultural agents from changing land use until the debt is repaid debt is measured in years the number of years to pay off the debt and is specified for transitions as shown in appendix b 2 3 calibration testing and scenarios previous implementations of the crafty framework for modelling real world regions have calibrated model parameters using methods that ensure internal consistency and produce expected system trajectories but without comparing model outputs to empirical observations e g blanco et al 2017 brown et al 2019 in contrast here we use empirical data for land cover use and agricultural commodity production table 1 to parameterize aft production functions and capital conversions e g from climate dryness index to moisture capitals identifying values that reproduce trends and patterns observed over the period 2001 2018 this approach aims to both ensure internal consistency but also identify key areas of uncertainty in the model and is one that has not been employed in previous crafty modelling applications understanding this uncertainty is important and useful for assessing model outputs for scenarios that project future land cover use and agricultural production here we compare simulated land cover use and agricultural production to observed data for the same variables aggregated for the entire study area we also compare observed and simulated municipality level proportions of land cover use for snap shots in time 5 year intervals final calibrated production function values are shown in appendix c and capital conversions are shown in millington 2020a once calibrated we test the model to examine how exogenous processes influence simulated land use cover and agricultural production specifically we test for changes in commodity demand and capitals associated with agricultural yields and climate change table 4 to understand the relative importance of these inputs on land use and production we vary each by the same proportion 20 of 2018 values over 2019 2035 holding all other values constant at 2018 values for demand we also examine tests in which demand for all services except for the nature service are varied to examine the impacts of changes in non nature demand comparing outputs from all of these tests including to a simulation that holds all values constant helps us to better understand the drivers and dynamics in the model and identify most important drivers of future change finally we examine future change in land use and production under alternative scenarios of demand yield climate change and protected land table 4 scenario input values for commodity demand and yields are derived from projections by the brazilian ministério da agricultura pecuária e abastecimento mapa 2020 as these projections are only to 2030 we use proportional change as indicated in the projections for 2019 2030 then mean proportional change over that period for 2031 2035 see appendix d for climate change scenarios we use regionally downscaled projections of temperature and precipitation from the wcrp coordinated regional downscaling experiment cordex our chosen driving model was had gem2 as this model has been shown to have better agreement with observed rainfall in atlantic forest caatinga and cerrado biomes than other models although biases do remain rosolem et al 2018 we use monthly data for daily minimum near surface air temperature ta s min daily maximum near surface air temperature tasmax and precipitation pr from ensemble r1i1p1 and cordex region sam44 downscaling realisation v3 projections were for representative concentration pathways rcp4 5 and rcp 8 5 and were accessed via the esgf ceda project ceda 2019 scripts that convert these data to moisture capitals values are available in millington 2020a finally we also examine a scenario that includes removal of protected area designations in addition to other changes a possibility given the environmental policy direction the current brazilian federal government has taken recently e g abessa et al 2019 3 results 3 1 calibration results from model calibration indicate that crafty brazil is able to reproduce observed time series of total area and production for the entire study area but performs less well in terms of spatial allocation across the study area observed trends of decreasing pasture and nature area combined with increases in cropland area are reproduced well with small year to year variation fig 2 a the general trends of increases in production of soybean and maize are reproduced although much of the large inter annual variability in production is not captured fig 2b interestingly there is also a slight lag in the rate of increase in maize production 2011 2014 and dramatic decreases in recent years are not captured correspondence between the two sets of time series can also be noted for example with the under estimation of cropland area 2003 2005 linked to under estimation of soybean production in these years although time series of observed trends are reasonably well reproduced there are some disparities between observed and modelled locations of land use cover fig 3 for example the model tends to locate more cropland in the north east of the study area bahia state than has been observed with correspondingly less pasture than observed in this area conversely in the central part of the study area são paulo state the model produces more pasture than observed at the expense of cropland nature is reasonably well modelled across the study area although with some over estimation at the expense of cropland in the north west of the study area mato grosso state while we see generally consistent variation from observations in the simulated time series accuracy in spatial allocation of land use seems to deteriorate through time for example while the modal land use cover was incorrectly modelled for 9 8 of municipalities in 2009 this had risen to 16 4 by 2018 3 2 testing assuming constant 2018 conditions into the future const scenario fig 4 all land covers remain in a steady state with the exception of double cropping and maize dc replacing maize as former is more competitive hence soybean production continues to rise while maize production declines slightly results for tests examining change in commodity demand fig 4a and b show that decreases in demand whether for all services or only for non nature services result in decreases in production of services and decreases in agricultural area with commensurate increases in nature area all change is either 20 or 20 relative to starting conditions these changes are generally larger than changes observed in outputs from tests that examine increases in demand this can be seen spatially in maps for tests with decreased demand fig 5 which indicate much less change than increased demand tests compared to the constant test and with greatest differences in the north east and south west of the study area differing trends exist between increased demand scenarios that consider all services vs non nature services when demand across all services increases by 2035 nature production and land cover is greater than the initial 2018 state and beef pasture are lower however when demand increases only for non nature services the reverse situation arises by 2035 nature production and land cover is less than the initial 2018 state and beef pasture production land cover are greater although there is a difference in trend the final change is less than the change in input i e 20 regarding the shape of production timelines in demand scenarios initial production is below demandand so rises to meet that demand in year 2022 demand decreases to a value similar to that actually being produced thereafter demand continues to decrease and is at a value that can be met by existing land use consequently agricultural land is abandoned and nature land area increases more rapidly yield scenarios fig 4c have similar sensitivity to demand except for nature and other crops which don t have changes in yield inputs however maize and soy have greater changes in yield tests than demand tests for constant demans production is greater with greater yields and lower with lower yields there is minimal change in nature area for decreases in yield as relative pressure on land for all uses is high but large increase in nature area for increased yields as less land is needed to meet demands production of soy and maize in the yield increase test rise until 2025 when greater yields mean demand can be met high yields relative to demand from 2023 mean that this is the point at which the rate of abandonment and growth of nature land increases outputs are least sensitive to changes in climate inputs particularly for land area for climate scenarios all change in land area is 20 except for maize which as for all other scenarios decreases due to replacement by soy maize double cropping limited change in land area means that there is very little difference between spatial distribution of luc in climate tests compared to the constant state agricultural service production increases decreases for increases decreases in the moisture capital respectively as would be expected by the relationships encoded in the model as would be expected maps of spatial change fig 5 indicate greatest change for tests in which timeseries fig 4 indicate greatest aggregate change i e nature land area increases commensurate with decreases in agricultural land area maps show the location of these changes are focused in the north east and south west of the study area with shifts from pasture to nature in the former and from agriculture to nature in the latter 3 3 scenarios the results for scenarios figs 4e and 6 exhibit combinations of the trends and patterns seen in the other scenarios varying individual driving factors all three scenarios result in increased overall production through time in response to improving yields and increased demand greatest increases in production are found in the ext np scenario although corresponding increases in cropland and pasture land are not spatially confined to formerly protected areas e g blue red shades for cropland nature respectively in fig 6 are found across the entire study area however although inter annual variation in land area outputs for scenarios is similar to that for tests i e relatively low with smooth transitions it is much greater for production outputs greater inter annual variation in production for scenario simulations is a result of variation in precipitation and temperature from gcm outputs which directly influence moisture capitals and therefore agricultural service provision of particular note is the large drop in production for ext and ext np scenarios based on gcm output for rcp85 in the final year of the simulation the result of a deep and widespread decrease in annual precipitation in gcm outputs this drop in production does not produce a commensurate change in land cover in the same year as there is a lag in agent decision making e g to abandon land the lag in decision making can be seen in the noticeable decrease in pasture and increase in nature land covers indicating pasture abandonment simulated in 2034 for the bau scenario this abandonment of pasture is the result of consecutive years of low precipitation which also caused a period of relatively low soybean and maize production for 2030 2033 compared to 2025 2030 although with no effect on cropland cover 4 discussion and conclusions 4 1 calibrating crafty this paper represents one of the first attempts to calibrate the crafty land use cover modelling framework against observed data such an approach has not been used in the past often because comprehensive data describing capitals demand and spatial distribution of land use are unavailable at the broad national to continental scales crafty is designed for brown et al 2019 other approaches to calibrate crafty have used stability checks with baseline inputs to ensure outputs do not deviate or oscillate wildly from expected behaviour e g blanco et al 2017 similar to our constant conditions scenario or to ensure sensible outputs are produced when starting from a blank map of null land use e g brown et al 2019 as described in section 2 1 we have utilised multiple empirical data sets to enable our approach but have also needed to make assumptions about how data represent different processes for example commodity demands in any given year are difficult to assess and for our calibration here we have assumed continuous market clearing such that observed 2001 2018 production perfectly met demand in each year the value of such a strong but simple assumption during calibration is that it enables clear understanding of the meaning of commodity demand in future scenarios and matches data that can be readily produced by the system dynamics model of global trade based on warner et al 2013 that we plan to link with crafty brazil in the hybrid model produced demand will be modelled endogenously by the system dynamics model adding further variation to crafty brazil inputs that will need to be appropriately assessed e g via sensitivity analyses our calibration of crafty brazil was more successful in reproducing observed aggregate land cover and production values than for the spatial distribution of variables figs 2 and 3 a situation similar to previous applications of crafty blanco et al 2017 brown et al 2019 in particular results from our calibration show pasture outcompeting cropland in the centre of the study area and vice versa in the north east neither of which were observed historically this is surprising as the north east of the study area is relatively marginal for cropland uses while the conditions further south and centrally are better it seems that our calibration allows the marginal utility of the beef service to increase at a rate faster than cropland services pushing the latter to less productive land such issues are likely further exacerbated by uncertainty in the land use cover maps against which we calibrate our model see section 2 1 although the mapbiomas data are the best available the uncertainty in classifying grassland and mosaics of agriculture and pasture into the classes required for crafty brazil may also contribute some level of error in our calibration fig 3 4 2 tests and scenarios tests of the model using standard differences in model inputs 20 of 2018 values shows that production is insensitive to inputs outputs vary by 20 or 20 but that land cover change is more sensitive some change is 20 all tests result in large 20 decreases in maize land area with smaller but also often large changes in soy due to large increases in double cropping area this shift away from maize only land use is to be expected both due to intended model logic and observed and expected empirical shifts to double cropping systems more obvious in the time series of land cover fig 4 are the large simulated shifts in nature in tests that represent decreased demand dem all decr and dem nnat decr in fig 4a and b or increasing yield yield incr in fig 4c these shifts are due to abandonment of agricultural land crops and pasture which in our model logic then becomes nature abandonment in these tests is driven by decreased demand in agricultural services in the case of the demand tests or a decreased pressure on land for agriculture due to increasing yields which in turn means less land needed to meet the same demand in the case of the yield incr test land cover in other tests is relatively insensitive because of competition for land between services in the dem nnat incr test for example constant 2018 yields means that commodity production never reaches the required demand in these scenarios hence production and land cover time series differ little from those seen in the constant test as production is already at its limit at the start of simulations given the calibrated yield values in contrast when yields increase through time yield incr test less land is needed for pasture land to meet beef demand and much is abandoned reverting to nature land cover yield increases in pasture and crops produce what previous studies defined as land sparing where the increased volume of production per land unit i e agricultural intensification leads to a decrease in cropland or at least alleviating the pressure for cropland expansion angelsen and kaimowitz 2001 hertel et al 2014 in this test production of all products is able to meet the constant 2018 demand because of the high yields resulting in production timeseries that flatline scenarios were designed to enable examination of both the effects of variations in multiple input factors and potential alternative futures for example for all three scenarios we see inter annual variability in soybean and maize production due to climate but with differing overall trend due to variation between scenarios in yield and demand a combined pattern we do not see in the tests the ext np scenario results in greatest agricultural production and lowest nature land area as the ability to farm formerly protected areas plus decreased demand for nature land allow greatest shifts in land from nature to cropland and pasture however increases in cropland and pasture land in the ext np scenario are not spatially confined to formerly protected areas as might have been expected this is likely because these protected areas have relatively limited infrastructure which is considered invariant through time in the ext np scenario with many of the indigenous and park lands some distance from the core agricultural production areas and much pasture and other non protected land available for conversion processes also represented in the model even under the scenarios we examine there is little pressure on the current protected areas however this is a general trend for the entire study area which may be different in particular local realities i e conservation areas that already suffer land use pressure furthermore if restrictions on land use in protected areas really are relaxed e g abessa et al 2019 we might expect improvements in infrastructure e g road building which may in turn lead to a positive feedback and greater exploitation of these areas over the longer term e g weinhold and reis 2008 furthermore in our scenarios demand for nature is specified as an overall percentage change to reflect possible trajectories of policy or socio economic change that value ecosystem demand for ecosystem services as implicitly provided by our nature service is difficult to estimate carpenter et al 2009 hayha et al 2015 brown et al 2017 this is reflected in the fact that while projections of future demand for agricultural commodities are regularly generated by formal government institutions e g mapa 2020 aligned projections of demand for ecosystem services are not common aligned projections of agricultural e g soybeans and non agricultural e g carbon sequestration land benefits would improve our ability to model future scenarios particularly with respect to demand for the nature service comparing results for scenarios with those for tests highlights qualitative differences in spatio temporal variation tests used simple temporally uniform and spatially invariant rates of change based on observed values whereas scenarios used precipitation and temperature outputs from gcms to provide moisture capitals values which have much greater inter annual variability fig 4e the qualitative differences in input time series demonstrates a strong influence of climate inputs on production outputs but not on land cover change the inter annual variability in production in our modelling is not sufficient for land cover change to occur through abandonment as discussed by silva et al 2020 but many fine details of farm level financing that may be vital for individual farm viability are not represented in this model and so we cannot conclusively argue that land cover change would not occur under the climate projections we have examined spatially land change is generally diffuse but with some focused regions of change tests that produce large increases in nature area indicate greatest decreases in pasture and cropland in north east and south west regions respectively where these are initially in 2018 widespread these are prone to greatest decreases as pasture land in the north east areas are the most marginal with historically low stocking rates e g dias filho 2014 whereas the south west is initially dominated by dense cropland and so has most to lose furthermore this is also the region that was most poorly modelled during calibration and as above section 4 1 we suspect land classification challenges confusion between pasture grassland and pasture agriculture mosaic in the mapbiomas input data mapbiomas 2019b also play a role here for similar reasons spatial change is quite dispersed across the study area but with intense change in a focused region in the north east of the study area in bahia state again with switches from pasture to nature 4 3 agency based modelling the tradeoffs necessary in spatial agent based modelling of land use systems have been well identified in the literature in particular with respect to a perceived spectrum from empirically grounded and complicated models to theoretically focused and simple models e g o sullivan et al 2015 sun et al 2016 here our approach has been to build on the theoretical structure provided by the crafty framework and remain relatively conceptually simple but also incorporating empirical data where possible to ground our application for brazil in doing so this version of crafty brazil limited the number of agent functional types and services represented eight services aggregated to four land use cover types and yet this still required the representation of a greater number of capitals 15 and processes multiple including the accrual of debt than we had initially expected a limited number of afts may seem to produce what sayer 1992 p 138 termed a chaotic conception a group of agent types that artificially lump together the unrelated and the inessential and inadequately represent differences between real world actors that are needed to reproduce empirical events however given that our model is implemented at a spatial resolution of 5 km crafty brazil does not represent individual actors as individuated agents but instead aggregates actors across space into grid cells within which human agency is represented by an aft this representation means crafty brazil should be thought of as an agency based model representing the behaviour of aggregate human actions rather than an agent based model representing individual actors activities e g such as that developed by dou et al 2019 furthermore working with relatively coarse aft representations aligns with the relatively coarse spatial representation that inherently lumps multiple real world actors together the aggregation of 27 land types defined at 30 m spatial resolution to four types at 5 km appendix a is robust given that the original classification was hierarchical that we have aligned our reclassification on that hierarchy and that we made further analyses of variability see section 2 1 and millington 2019 although appropriate for the scales we are working at the combination of this agency approach with constraints of the crafty framework presents challenges to representing some processes that influence land decision making of individual actors in our study area for example the design of the crafty framework to ensure computational efficiency means that the history of simulated agents is not retained and that agents cannot anticipate change beyond the next time step we modified the crafty source code to enable some coarse representation of temporally contingent processes i e the debt that farmers incur when setting up a new farm section 2 2 on the agency of multiple aggregated actors however the agency approach combined with the difficulty of tracking history and representing planning strategies presents a challenge for representing the processes that trap producers in cycles of debt and investment silva et al 2020 this combination also limits our ability to understand possible vulnerabilities and responses of producers to temporal e g inter annual variability in climate or other exogenous factors as highlighted above such questions cannot be examined without incorporating representation of history and planning and or working at finer aggregations and scales such that individual actors are represented by individuated agents for example readers considering their own agent based modelling projects whether focused on land use or other environmental issues might learn from this example about aligning scale and detail of conceptualisation in particular we suggest readers compare our broad scale agency based approach for modelling soy and maize to the finer scale and explicitly agent based approached taken by dou et al 2019 to consider for themselves the advantages and disadvantages of the different approaches at the different scales our modelling will continue to focus on broader scale issues as we dynamically couple crafty brazil to a system dynamics model to create a hybrid simulation model for examining the land use impacts of telecoupled global trade millington et al 2017 liu et al 2018 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements we are grateful to numerous colleagues for ideas and understanding gained from discussion on the subjects presented here these include but are not limited to steve peterson jem woods hang xiong yue dou and jack liu plus several local stakeholders in brazil we are grateful to paul mccord for his early work with mapbiomas data thanks also to several anonymous reviewers for their comments which helped improve the manuscript appendix a mapbiomas v4 0 reclassification code and description are from mapbiomas reclassification is used here note that cropland are further disaggregated into soy maize and other crops using planted area data scripts used to resample reclassify and disaggregated the mapbiomas data are available in millington 2019 code description reclassification 1 forest formations nature 1 1 natural forest formations nature 1 1 1 dense forest nature 1 1 2 open forest nature 1 1 3 mangrove nature 1 2 forest plantations nature 2 non forest natural formations nature 2 1 non forest formations in wetlands nature 2 2 grassland pasture nature in protected areas 2 3 salt flat nature 2 4 rocky outcrop other 2 5 other non forest natural formations other 3 farming cropland 3 1 pasture pasture 3 2 agriculture cropland 3 2 1 annual and perennial crop cropland 3 2 2 semi perennial crop cropland 3 3 mosaic of agriculture and pasture cropland 4 non vegetated areas other 4 1 beaches and dunes other 4 2 urban infrastructure other 4 3 mining other 4 4 other non vegetated area other 5 water bodies other 5 1 river lake and ocean other 5 2 aquaculture other 6 not observed other appendix b debt debt incurred by agents following land use change units are years previous land use cropland agent pasture agent nature or other 5 3 soybean maize or other crops 3 3 double cropping 0 3 pasture 4 na appendix c production functions capitals agent functional types and their production weighting factors for each service a soybean aft soybean maize nature ocrops other beef moisture main 0 8 0 0 0 0 0 moisture second 0 0 0 0 0 0 transport 0 5 0 0 0 0 0 land value 0 0 0 0 0 0 conservation 0 0 0 0 0 0 tech soy maize 0 8 0 0 0 0 0 tech pasture 0 0 0 0 0 0 other 0 0 0 0 0 0 protection soy 1 0 0 0 0 0 protection maize 0 0 0 0 0 0 protection beef 0 0 0 0 0 0 protection ocrop 0 0 0 0 0 0 access nature 1 0 0 0 0 0 access soy maize 0 4 0 0 0 0 0 access ocrop 0 0 0 0 0 0 production 1 0 0 0 0 0 b maize aft soybean maize nature ocrops other beef moisture main 0 0 8 0 0 0 0 moisture second 0 0 0 0 0 0 transport 0 0 5 0 0 0 0 land value 0 0 0 0 0 0 conservation 0 0 0 0 0 0 tech soy maize 0 0 8 0 0 0 0 tech pasture 0 0 0 0 0 0 other 0 0 0 0 0 0 protection soy 0 0 0 0 0 0 protection maize 0 1 0 0 0 0 protection beef 0 0 0 0 0 0 protection ocrop 0 0 0 0 0 0 access nature 0 1 0 0 0 0 access soy maize 0 0 4 0 0 0 0 access ocrop 0 0 0 0 0 0 production 0 1 0 0 0 0 c double crop aft soybean maize nature ocrops other beef moisture main 0 8 0 0 0 0 0 moisture second 0 0 5 0 0 0 0 transport 0 5 0 5 0 0 0 0 land value 0 0 0 0 0 0 conservation 0 0 0 0 0 0 tech soy maize 0 8 0 8 0 0 0 0 tech pasture 0 0 0 0 0 0 other 0 0 0 0 0 0 protection soy 1 0 0 0 0 0 protection maize 0 1 0 0 0 0 protection beef 0 0 0 0 0 0 protection ocrop 0 0 0 0 0 0 access nature 1 1 0 0 0 0 access soy maize 0 4 0 4 0 0 0 0 access ocrop 0 0 0 0 0 0 production 0 8 0 75 0 0 0 0 d nature aft soybean maize nature ocrops other beef moisture main 0 0 0 0 0 0 moisture second 0 0 0 0 0 0 transport 0 0 0 0 0 0 land value 0 0 1 0 0 0 conservation 0 0 1 0 0 0 tech soy maize 0 0 0 0 0 0 tech pasture 0 0 0 0 0 0 other 0 0 0 0 0 0 protection soy 0 0 0 0 0 0 protection maize 0 0 0 0 0 0 protection beef 0 0 0 0 0 0 protection ocrop 0 0 0 0 0 0 access nature 0 0 0 0 0 0 access soy maize 0 0 0 0 0 0 access ocrop 0 0 0 0 0 0 production 0 0 1 0 0 0 e other crops aft soybean maize nature ocrops other beef moisture main 0 0 0 0 8 0 0 moisture second 0 0 0 0 0 0 transport 0 0 0 0 5 0 0 land value 0 0 0 0 0 0 conservation 0 0 0 0 0 0 tech soy maize 0 0 0 0 0 0 tech pasture 0 0 0 0 0 0 other 0 0 0 0 0 0 protection soy 0 0 0 0 0 0 protection maize 0 0 0 0 0 0 protection beef 0 0 0 0 0 0 protection ocrop 0 0 0 1 0 0 access nature 0 0 0 0 0 0 access soy maize 0 0 0 0 0 0 access ocrop 0 0 0 1 0 0 production 0 0 0 1 0 0 f other aft soybean maize nature ocrops other beef moisture main 0 0 0 0 0 0 moisture second 0 0 0 0 0 0 transport 0 0 0 0 0 0 land value 0 0 0 0 0 0 conservation 0 0 0 0 0 0 tech soy maize 0 0 0 0 0 0 tech pasture 0 0 0 0 0 0 other 0 0 0 0 1 0 protection soy 0 0 0 0 1 0 protection maize 0 0 0 0 1 0 protection beef 0 0 0 0 1 0 protection ocrop 0 0 0 0 1 0 access nature 0 0 0 0 0 0 access soy maize 0 0 0 0 0 0 access ocrop 0 0 0 0 0 0 production 0 0 0 0 1 0 g pasture aft soybean maize nature ocrops other beef moisture main 0 0 0 0 0 0 2 moisture second 0 0 0 0 0 0 transport 0 0 0 0 0 0 5 land value 0 0 0 0 0 0 conservation 0 0 0 0 0 0 tech soy maize 0 0 0 0 0 0 tech pasture 0 0 0 0 0 1 other 0 0 0 0 0 0 protection soy 0 0 0 0 0 0 protection maize 0 0 0 0 0 0 protection beef 0 0 0 0 0 1 protection ocrop 0 0 0 0 0 0 access nature 0 0 0 0 0 0 2 access soy maize 0 0 0 0 0 0 access ocrop 0 0 0 0 0 0 production 0 0 0 0 0 0 85 appendix d demand and yield projections values are annual change derived from mapa 2020 used in scenarios specified in table 4 a demand soy and maize beef year standard upper standard upper 2019 2 19 3 67 1 53 3 14 2020 2 19 3 67 1 53 3 14 2021 1 34 3 67 2 89 7 81 2022 3 15 5 41 0 30 2 46 2023 2 86 4 43 1 52 3 58 2024 2 68 4 09 5 26 6 50 2025 2 67 3 82 2 14 0 69 2026 2 51 3 41 0 08 0 87 2027 2 44 3 22 3 85 4 26 2028 2 38 3 06 2 07 2 76 2029 2 32 2 89 1 09 0 01 2030 2 25 2 75 3 37 3 84 2031 2 20 2 65 1 50 3 14 2032 2 15 2 55 1 50 3 14 2033 2 10 2 45 1 50 3 14 2034 2 05 2 35 1 50 3 14 2035 2 00 2 25 1 50 3 14 b yield year soy and maize beef standard upper standard upper 2019 0 78 1 66 1 52 2 67 2020 0 78 1 66 1 52 2 67 2021 1 51 1 66 1 66 5 50 2022 1 23 2 26 1 52 4 25 2023 1 10 2 14 1 08 3 26 2024 1 10 1 88 3 75 5 12 2025 1 05 1 72 1 97 1 22 2026 1 02 1 58 1 68 1 76 2027 0 99 1 47 1 71 1 77 2028 0 96 1 37 1 90 1 90 2029 0 93 1 28 0 53 0 79 2030 0 91 1 21 3 39 3 59 2031 0 89 1 16 1 50 2 67 2032 0 87 1 11 1 50 2 67 2033 0 85 1 06 1 50 2 67 2034 0 83 1 01 1 50 2 67 2035 0 81 0 96 1 50 2 67 software and data availability code for both the simulation model and our data analysis is freely available online we refer to the relevant github repositories in the text at the appropriate points the model can be deployed via docker using lane and millington 2021 also see victoria et al 2021 author contributions conceptualization all authors software jm and vk formal analysis jm rbs and dcv data curation jm rbs and dcv writing original draft preparation jm writing review editing jm dcv rbs and mb funding this work was supported by the uk natural environment research council under grant reference ne m021335 1 we gratefully acknowledge the funding support by the são paulo research foundation fapesp processes 14 50628 9 15 25892 7 and 18 08200 2 
25858,increasing global demand for agricultural commodities has driven local land use cover change lucc and agricultural production across brazil during the 21st century modelling tools are needed to help understand the range of possible outcomes due to these telecoupled global to local relationships given future political economic and environmental uncertainties here we present crafty brazil a lucc model representing production of multiple agricultural commodities that accounts for spatially explicit e g land access and temporally contingent e g agricultural debt processes of importance across our nearly four million km2 brazilian study area we calibrate the model calibration for 2001 2018 and run tests and scenarios about commodity demand agricultural yields climate change and policy decisions for 2019 2035 results indicate greater confidence in modelled time series than spatial allocation we discuss how our approach might be best understood to be agency based rather than agent based and highlight questions more and less appropriate for this approach keywords land use cover telecoupling brazil crafty simulation agent based 1 introduction it is now well understood that local land use cover changes in many regions of the world are influenced by international demand for agricultural commodities and that socio ecological systems are often telecoupled over great distances liu et al 2013 2018 for example increased chinese demand for soybean over the last several decades has contributed to increased production in brazil making the country a key soybean production region in the global food system and driving change in local land use silva et al 2017 sun et al 2017 increased production has been achieved through a combination of expansion of agricultural land increases in yields and changes in farming practices including the development of a double crop system with maize predominantly as a second crop improvements in yields have come through improved seed varieties including genetic modification increases in agricultural inputs fertilisers pesticides machinery and economies of scale wesz 2016 these changes have come in tandem with significant economic changes in the brazilian farming system meaning that many farmers are faced by tough economic decisions to ensure the future viability of their businesses silva et al 2020 future uncertainty is further exacerbated by the spectre of climate change which may bring increased frequency of drought during the second crop and other conditions unfavourable to consistent production from year to year heinemann et al 2017 hampf et al 2020 to create a tool for examining the range of possible outcomes given such a range of drivers and uncertainties we set out to develop a spatially explicit land use cover change model capable of representing both production and associated land use of multiple agricultural commodities that could be subsequently linked to a system dynamics model of global trade see millington et al 2017 the telecoupling framework within which we developed our model emphasises the importance of agents and flows as drivers of change in coupled human natural systems that are linked across long distances to represent agency in land use and agricultural production decision making we use the previously developed competition for resources between agent functional types crafty modelling framework murray rust et al 2014 adapting it to improve representation of spatially explicit e g land access and temporally contingent e g agricultural debt processes of importance in our brazilian study area the crafty framework has been designed specifically with the intention of simulating broad scale land use change over large spatial extents national to continental for example blanco et al 2017 parameterised crafty to examine ecosystem services and decision making under scenarios of climate change for the entire land area of sweden over many decades brown et al 2019 used crafty to simulate land use change across the entire european union to investigate land manager behaviour at the continental scale in work similar to that presented here investigating the telecoupled effects of global food commodity trade between china and brazil on land use dou et al 2019 2020 developed a bespoke agent based model whereas the aims of that approach were to understand land use impacts at a fine scale for a single municipality our work aims to understand land use across much broader extents and hence the use of crafty is more appropriate millington et al 2017 in this paper we present the description and first results from the application of the crafty framework to simulate land use cover change over several decades for ten brazilian states an implementation we call crafty brazil we provide an overview of the endogenous and exogenous processes represented the data used to parameterize and calibrate the model and results from using crafty brazil to simulate scenarios of future change importantly we developed and tested crafty brazil using empirically grounded data in a fashion that has not previously been achieved for other applications of the crafty framework this empirically grounded approach aims to ensure internal model consistency but also identify key areas of uncertainty the model structure and results presented here are independent of the system dynamics model based on warner et al 2013 we ultimately intend to couple crafty brazil with however by examining scenarios of commodity demand agricultural yields climate change and policy decisions over land use rights we are able to both identify important uncertainties in the model but also shed light on important processes influencing land use change in brazil subsequently we reflect on what we have learned from this initial use of the model and discuss future directions in which this work should proceed 2 methods 2 1 study area and data crafty brazil was designed with the intention of subsequently connecting this spatial explicit model of land use cover change with a system dynamics model representing the international trade of three primary agricultural commodities soybean maize and beef with this focus our study area was designated as the 10 states in brazil that have been the dominant producers of soybean and maize in recent years and for which pasture area is widespread fig 1 our aim to simulate this region of 3 850 000 km2 for several decades required compromises on model spatial and temporal resolution to ensure feasible execution times while representing sufficient variation to explore system dynamics thus the model operates on a raster grid at a 5 km spatial resolution for a total of 162 026 simulated cells and we aggregate results to municipality level for our study area mean and median municipality areas are 1 040 and 398 km2 respectively these resolutions are appropriate given the available data needed to calibrate the model which comes from a variety of sources and with a range of original resolutions including many aggregated at municipality level table 1 furthermore the 5 km resolution is comparable to other applications of crafty across large spatial extents e g 1 km for blanco et al 2017 and 10 arcminute approx 18 km at the equator for brown et al 2019 the model runs with an annual timestep and we calibrate using data for 2001 2018 for model initialization and calibration we use land use cover luc data from the mapbiomas project version 4 mapbiomas 2018 original 30 m data were resampled to 5 km modal pixel class see millington 2019 and luc classes were reclassified from 27 classes to four cropland pasture nature and other appendix a these luc classes are associated with the production of services by different agent functional types as described below section 2 2 1 cropland represents land used to produce soybean maize and other crops e g rice sugarcane while pasture is assumed to represent all land area used for beef production nature represents vegetation cover not used for agriculture i e crops or pasture processes of natural vegetation dynamics are not explicitly represented although see use of nature capital in section 2 2 1 below but simulated land that is no longer needed for human use reverts to the nature class and nature cells can be converted to all other classes finally other represents land covers such as urban and inland water bodies and is assumed to not change in simulations that project into the future given uncertainty in the mapbiomas data and the multiple alternative ways those data might be re classified we examined several re classifications of the mapbiomas v4 0 data and used commodity planted area data from ibge to disaggregate ambiguous classes the overall accuracy for the mapbiomas v4 0 data for the period 2001 2018 at the level 2 classification is estimated to be 88 with the grassland class consistently the most poorly classified of all classes high rates of natural forest and pasture commission error see mapbiomas 2019b on comparing the implied beef pasture yields for luc classifications including excluding grassland in our pasture class we find a closer match with observed yields when grassland is included see millington 2019 the mapbiomas class mosaic of agriculture and pasture is also problematic given that our focus is on distinguishing between pasture and cropland to address this we used planted area data ibge 2019 to allocate pixels in the mapbiomas mosaic of agriculture and pasture class into either cropland or pasture classes see millington 2019 2 2 model description 2 2 1 services agents and capitals the crafty framework represents the production of land services by agent functional types afts through production functions for multiple capitals available in each cell representing a land unit murray rust et al 2014 cobb douglas production functions are used 1 p s c c i λ c a where λ c a is a weighting factor specific to capital c and aft a and p s is the productivity for service s in abstract production units later converted to kg for agricultural commodity services competition between afts is represented by calculating competitiveness using a utility function that accounts for production and marginal demand for each service in each timestep the competitiveness of afts is calculated for cells based on current capital values which crafty assumes are scaled 0 to 1 and service demands specified in abstract production units which in our case represent demand for agricultural commodities in domestic and international markets the aft with the greatest competitiveness is allocated to that cell subject to the giving in threshold of the currently occupying agent land may also be abandoned from agent use if competitiveness falls below an aft s giving up threshold for full details on the crafty framework see murray rust et al 2014 given our focus on soybean maize and beef crafty brazil represents provision of the following services soybean maize beef other crops ocrops nature and other the nature service represents the value of land not under human influence while other represents all other land not in other classes e g urban water the afts represented are soybean producers maize producers soybean maize double crop producers producing both services beef producers other crop producers and other land managers table 2 we developed these afts based on expert judgement and discussion with stakeholders in the study area based on the key services being simulated in the model and the key variations in strategies e g single vs double crop that currently exist a nature aft is also required for cells not under control of any other aft and representing land not under direct human land use regardless of whether that is primary or secondary vegetation representing double crop farmers as an individual aft is important as land managers have implemented the practice of sowing and planting a soybean crop early in the season followed by a late season crop usually maize the double cropping system of soybean and maize was initiated in brazil in the late 1990s as raising maize following soybean harvest provides protective cover for soils and the system eventually improves soil quality silva et al 2017 initial yields using this method were low but since the early 2000s many improvements have been made in management practices such as no tillage agriculture nitrogen biological fixation serviced by soybeans hybrid and gmo maize varieties which boosted yields for the second crop consequently this double cropping practice has grown through the 21st century pushed more by economic demand than by a desire for improved soil management silva et al 2017 although this system provides potential for both agronomic and economic gains over single crop systems the second crop growth is exposed to the risk of late season drought and consequent production losses brunini et al 2001 gonçalves et al 2002 for soybean the double cropping system has pushed production into a very short growing season forcing producers to adopt short cycle gmo varieties about 90 days from planting to harvest that are impacting soybean yields oliveira neto 2017 we represent other crops and other land managers as grouped afts that likely contain multiple management strategies because our focus here is on the production of soybean maize and beef 2 2 2 exogenous processes we use multiple crafty capitals to represent the numerous influences on the production of services table 3 for example important determinants of agricultural production in brazil have been found to include human capital technology generation and dissemination climate conditions and transport networks and land access pereira 2012 rada 2013 all capitals vary spatially across the study area except for tech capitals which are spatially uniform because these represent improvements in technology that lead to broad scale yield improvements through time including due to improved seed varieties machinery and fertilisers that are available widely values for capitals are provided exogenously i e from ancillary data sources except for the three access nature soybean maize other crops capitals and the conservation capital which are calculated endogenously i e during a simulation run from the dynamic spatial configuration of afts all endogenous and many exogenous capitals are updated in each timestep i e annually although some are updated less frequently see table 3 either because of data availability e g transport network or because the process they represent does not occur on an annual basis e g the soybean moratorium occurred in a given year see below all scripts to create files for initializing and updating capital values are available online millington 2020a we discuss exogenous capitals in the remainder of this section and endogenous capitals and representation of other processes in the following section section 2 2 3 the moisture capitals are derived from the monthly mean temperature and precipitation variables from the cru ts v 4 03 high resolution gridded datasets see harris et al 2020 and represent the role of climate on agricultural production to understand climatic limitations associated with plant growth and agricultural production we used the dryness index to describe the relation between water deficit and potential evapotranspiration pereira and pruitt 2004 both obtained from the thornthwaite and matter 1955 climatic water balance as implemented by victoria et al 2007 this index represents the water deficit in percentage of potential evapotranspiration and is calculated by the equation 2 di 100 def pet where di is the dryness index def is the water deficit and pet is potential evapotranspiration see table 2 in victoria et al 2007 and millington 2020a for full definition we calculate mean monthly di for two different growing seasons oct mar and jan jun to calculate two sets of moisture capital values to represent climate influence on single vs double crop production the use of these moisture capitals also allows us to investigate the possible influence of climate change on agricultural productivity in simulations of alternative future scenarios see section 2 3 below transport infrastructure is a key variable influencing the spatial distribution and volume of agricultural production related to land conversion e g soares filho et al 2006 weinhold and reis 2008 and both imports of agricultural inputs and exports of commodities to markets rada 2013 the transport capital uses data on the national road network dnit 2019 with locations and operating years of ports antaq 2019 to derive a spatial cost surface at a broad scale this cost surface weights the quality of transport infrastructure such that paved roads present lower cost than unpaved roads for access to land see victoria et al 2021 the use of the transport capital allows us to investigate the possible influence of future alternative infrastructure development on land use change and agricultural productivity at a broad scale the land value capital is used to represent the incentive for agents to convert nature land to agriculture in places where agricultural potential and infrastructure as represented by the previous capitals are poor but which have been converted e g based on mapbiomas lulcc between 1985 and 2018 in these frontier regions land prices are lower compared to other developed regions of southern brazil reflecting the challenges of making a profit from agricultural production in frontier lands these lower land prices provide an incentive to those willing to take a risk on developing land for grain production assuming that future improvements e g logistics infrastructure in the region will improve yields to pay off the risk previous studies have shown that as the agricultural sector develops in a given region land prices tend to increase alongside infrastructure and social standards rezende 2002 ferro and castro 2013 martinelli et al 2017 to develop this capital we used data from ieg fnp 2017 to represent the relative cost of land for new development as noted above improvement in agricultural productivity yields through time due to advances in technological resources such as improved machinery and seed varieties e g pereira 2012 are represented using the tech capitals because these capitals represent the aggregation of multiple sources of improvements in yield we calibrate their values by combining our observed land use cover data with commodity production data to calculate yields that are internally consistent within the model see millington 2020b the four protection capitals represent areas of land that cannot be used for soybean maize beef or other crops this exclusion may be because an area is designated as national or state parks indigenous lands or because of a policy that excludes production of a given commodity e g soybean moratorium for example the protection soybean capital is used in calibration runs section 2 3 to represent the soybean moratorium policy implemented in 2006 to discourage deforestation and limiting the market for soybean grown on deforested lands gibbs et al 2015 dou et al 2018 these capitals therefore also allow examination of simulation runs that implement similar policies finally the other capital is used to drive a high probability of cells being in the other land cover category e g urban and water based on observed land cover change for calibration and potentially for representation of future expansion of this land type in the crafty modelling framework demand for services is provided exogenously rather than incorporated as an endogenous process furthermore this demand is specified in the same abstract units used to represent services production section 2 2 1 above as our focus here is on soybean maize and beef we derive the abstract demand for these services from real production units kg while for other crops other and nature we derive demand based on land area ha see millington 2020b 2 2 3 endogenous processes processes driven by the spatial configuration or historical contingency of land resources and agent actions are represented in the model endogenously by updating cell or agent states dynamically during a simulation dependent on their circumstances in each timestep specifically we represent the spatial agglomeration effect of agricultural economies the tendency of land conversion to be spatially contagious vegetation regeneration processes and producer debt representing these processes required additions to the crafty source code see millington 2020c the importance of spatial proximity for driving land use change due to efficiencies afforded by agglomeration economies is well known e g fujita and krugman 1995 porter 2000 and has been shown to be important in brazil vera diaz et al 2008 garrett et al 2013 picoli et al 2020 to represent this in the model at a local level the access soy maize and access ocrops capitals are updated in each cell in each timestep during simulation runs based on whether one of these agent types is present within the eight neighbouring cells moore neighbourhood with value 0 05 if the target agent type is not present 0 95 if the agent type is present and 1 0 if the cell is occupied by the specified aft similarly conversion of natural land for agriculture is known to be well modelled as a contagious process of spread from existing cultivated areas at the edge of natural lands e g rosa et al 2013 to represent spatial access to natural land at a local level the access nature capital is updated in each cell in each timestep during simulation runs based on the adjacency of nature and non nature land covers for any given cell if the moore neighbourhood is composed entirely of nature cells the capital takes a value of 1 0 if between 1 and 7 cells in the moore neighbourhood are nature cells the cell takes a value of 0 75 and finally a nature access capital value of 0 0 is taken if all neighbouring cells are non nature through time we represent regeneration of natural vegetation following land abandonment by modifying conservation capital cell values during a simulation based both on the time since last human disturbance but also the type of disturbance several recent studies support the hypothesis that forest regeneration rate is related to intensity of previous land use mesquita et al 2015 jakovac et al 2015 martines ramos et al 2016 and here we assume that the rate of regeneration is faster following extensive pasture land use than intensive crop soybean maize land uses hence for cells in a simulation run that have never been disturbed i e have always had a nature land cover the conservation capital will have value 1 0 the conservation capital value is reduced to 0 4 if converted from nature to pasture and to 0 0 if converted to one of the other non nature land covers following abandonment of a non nature use conservation capital is increased by a value of 0 01 each timestep the final endogenous process we represent is the accrual and repayment of debt by those producers changing land uses new producers often need to take out loans to pay for land new machinery seed and other start up costs producers can be trapped into activities needed to earn profits to make repayments e g silva et al 2020 and changes in land use are unlikely during the repayment period to represent this inertia following conversion we prevent new agricultural agents from changing land use until the debt is repaid debt is measured in years the number of years to pay off the debt and is specified for transitions as shown in appendix b 2 3 calibration testing and scenarios previous implementations of the crafty framework for modelling real world regions have calibrated model parameters using methods that ensure internal consistency and produce expected system trajectories but without comparing model outputs to empirical observations e g blanco et al 2017 brown et al 2019 in contrast here we use empirical data for land cover use and agricultural commodity production table 1 to parameterize aft production functions and capital conversions e g from climate dryness index to moisture capitals identifying values that reproduce trends and patterns observed over the period 2001 2018 this approach aims to both ensure internal consistency but also identify key areas of uncertainty in the model and is one that has not been employed in previous crafty modelling applications understanding this uncertainty is important and useful for assessing model outputs for scenarios that project future land cover use and agricultural production here we compare simulated land cover use and agricultural production to observed data for the same variables aggregated for the entire study area we also compare observed and simulated municipality level proportions of land cover use for snap shots in time 5 year intervals final calibrated production function values are shown in appendix c and capital conversions are shown in millington 2020a once calibrated we test the model to examine how exogenous processes influence simulated land use cover and agricultural production specifically we test for changes in commodity demand and capitals associated with agricultural yields and climate change table 4 to understand the relative importance of these inputs on land use and production we vary each by the same proportion 20 of 2018 values over 2019 2035 holding all other values constant at 2018 values for demand we also examine tests in which demand for all services except for the nature service are varied to examine the impacts of changes in non nature demand comparing outputs from all of these tests including to a simulation that holds all values constant helps us to better understand the drivers and dynamics in the model and identify most important drivers of future change finally we examine future change in land use and production under alternative scenarios of demand yield climate change and protected land table 4 scenario input values for commodity demand and yields are derived from projections by the brazilian ministério da agricultura pecuária e abastecimento mapa 2020 as these projections are only to 2030 we use proportional change as indicated in the projections for 2019 2030 then mean proportional change over that period for 2031 2035 see appendix d for climate change scenarios we use regionally downscaled projections of temperature and precipitation from the wcrp coordinated regional downscaling experiment cordex our chosen driving model was had gem2 as this model has been shown to have better agreement with observed rainfall in atlantic forest caatinga and cerrado biomes than other models although biases do remain rosolem et al 2018 we use monthly data for daily minimum near surface air temperature ta s min daily maximum near surface air temperature tasmax and precipitation pr from ensemble r1i1p1 and cordex region sam44 downscaling realisation v3 projections were for representative concentration pathways rcp4 5 and rcp 8 5 and were accessed via the esgf ceda project ceda 2019 scripts that convert these data to moisture capitals values are available in millington 2020a finally we also examine a scenario that includes removal of protected area designations in addition to other changes a possibility given the environmental policy direction the current brazilian federal government has taken recently e g abessa et al 2019 3 results 3 1 calibration results from model calibration indicate that crafty brazil is able to reproduce observed time series of total area and production for the entire study area but performs less well in terms of spatial allocation across the study area observed trends of decreasing pasture and nature area combined with increases in cropland area are reproduced well with small year to year variation fig 2 a the general trends of increases in production of soybean and maize are reproduced although much of the large inter annual variability in production is not captured fig 2b interestingly there is also a slight lag in the rate of increase in maize production 2011 2014 and dramatic decreases in recent years are not captured correspondence between the two sets of time series can also be noted for example with the under estimation of cropland area 2003 2005 linked to under estimation of soybean production in these years although time series of observed trends are reasonably well reproduced there are some disparities between observed and modelled locations of land use cover fig 3 for example the model tends to locate more cropland in the north east of the study area bahia state than has been observed with correspondingly less pasture than observed in this area conversely in the central part of the study area são paulo state the model produces more pasture than observed at the expense of cropland nature is reasonably well modelled across the study area although with some over estimation at the expense of cropland in the north west of the study area mato grosso state while we see generally consistent variation from observations in the simulated time series accuracy in spatial allocation of land use seems to deteriorate through time for example while the modal land use cover was incorrectly modelled for 9 8 of municipalities in 2009 this had risen to 16 4 by 2018 3 2 testing assuming constant 2018 conditions into the future const scenario fig 4 all land covers remain in a steady state with the exception of double cropping and maize dc replacing maize as former is more competitive hence soybean production continues to rise while maize production declines slightly results for tests examining change in commodity demand fig 4a and b show that decreases in demand whether for all services or only for non nature services result in decreases in production of services and decreases in agricultural area with commensurate increases in nature area all change is either 20 or 20 relative to starting conditions these changes are generally larger than changes observed in outputs from tests that examine increases in demand this can be seen spatially in maps for tests with decreased demand fig 5 which indicate much less change than increased demand tests compared to the constant test and with greatest differences in the north east and south west of the study area differing trends exist between increased demand scenarios that consider all services vs non nature services when demand across all services increases by 2035 nature production and land cover is greater than the initial 2018 state and beef pasture are lower however when demand increases only for non nature services the reverse situation arises by 2035 nature production and land cover is less than the initial 2018 state and beef pasture production land cover are greater although there is a difference in trend the final change is less than the change in input i e 20 regarding the shape of production timelines in demand scenarios initial production is below demandand so rises to meet that demand in year 2022 demand decreases to a value similar to that actually being produced thereafter demand continues to decrease and is at a value that can be met by existing land use consequently agricultural land is abandoned and nature land area increases more rapidly yield scenarios fig 4c have similar sensitivity to demand except for nature and other crops which don t have changes in yield inputs however maize and soy have greater changes in yield tests than demand tests for constant demans production is greater with greater yields and lower with lower yields there is minimal change in nature area for decreases in yield as relative pressure on land for all uses is high but large increase in nature area for increased yields as less land is needed to meet demands production of soy and maize in the yield increase test rise until 2025 when greater yields mean demand can be met high yields relative to demand from 2023 mean that this is the point at which the rate of abandonment and growth of nature land increases outputs are least sensitive to changes in climate inputs particularly for land area for climate scenarios all change in land area is 20 except for maize which as for all other scenarios decreases due to replacement by soy maize double cropping limited change in land area means that there is very little difference between spatial distribution of luc in climate tests compared to the constant state agricultural service production increases decreases for increases decreases in the moisture capital respectively as would be expected by the relationships encoded in the model as would be expected maps of spatial change fig 5 indicate greatest change for tests in which timeseries fig 4 indicate greatest aggregate change i e nature land area increases commensurate with decreases in agricultural land area maps show the location of these changes are focused in the north east and south west of the study area with shifts from pasture to nature in the former and from agriculture to nature in the latter 3 3 scenarios the results for scenarios figs 4e and 6 exhibit combinations of the trends and patterns seen in the other scenarios varying individual driving factors all three scenarios result in increased overall production through time in response to improving yields and increased demand greatest increases in production are found in the ext np scenario although corresponding increases in cropland and pasture land are not spatially confined to formerly protected areas e g blue red shades for cropland nature respectively in fig 6 are found across the entire study area however although inter annual variation in land area outputs for scenarios is similar to that for tests i e relatively low with smooth transitions it is much greater for production outputs greater inter annual variation in production for scenario simulations is a result of variation in precipitation and temperature from gcm outputs which directly influence moisture capitals and therefore agricultural service provision of particular note is the large drop in production for ext and ext np scenarios based on gcm output for rcp85 in the final year of the simulation the result of a deep and widespread decrease in annual precipitation in gcm outputs this drop in production does not produce a commensurate change in land cover in the same year as there is a lag in agent decision making e g to abandon land the lag in decision making can be seen in the noticeable decrease in pasture and increase in nature land covers indicating pasture abandonment simulated in 2034 for the bau scenario this abandonment of pasture is the result of consecutive years of low precipitation which also caused a period of relatively low soybean and maize production for 2030 2033 compared to 2025 2030 although with no effect on cropland cover 4 discussion and conclusions 4 1 calibrating crafty this paper represents one of the first attempts to calibrate the crafty land use cover modelling framework against observed data such an approach has not been used in the past often because comprehensive data describing capitals demand and spatial distribution of land use are unavailable at the broad national to continental scales crafty is designed for brown et al 2019 other approaches to calibrate crafty have used stability checks with baseline inputs to ensure outputs do not deviate or oscillate wildly from expected behaviour e g blanco et al 2017 similar to our constant conditions scenario or to ensure sensible outputs are produced when starting from a blank map of null land use e g brown et al 2019 as described in section 2 1 we have utilised multiple empirical data sets to enable our approach but have also needed to make assumptions about how data represent different processes for example commodity demands in any given year are difficult to assess and for our calibration here we have assumed continuous market clearing such that observed 2001 2018 production perfectly met demand in each year the value of such a strong but simple assumption during calibration is that it enables clear understanding of the meaning of commodity demand in future scenarios and matches data that can be readily produced by the system dynamics model of global trade based on warner et al 2013 that we plan to link with crafty brazil in the hybrid model produced demand will be modelled endogenously by the system dynamics model adding further variation to crafty brazil inputs that will need to be appropriately assessed e g via sensitivity analyses our calibration of crafty brazil was more successful in reproducing observed aggregate land cover and production values than for the spatial distribution of variables figs 2 and 3 a situation similar to previous applications of crafty blanco et al 2017 brown et al 2019 in particular results from our calibration show pasture outcompeting cropland in the centre of the study area and vice versa in the north east neither of which were observed historically this is surprising as the north east of the study area is relatively marginal for cropland uses while the conditions further south and centrally are better it seems that our calibration allows the marginal utility of the beef service to increase at a rate faster than cropland services pushing the latter to less productive land such issues are likely further exacerbated by uncertainty in the land use cover maps against which we calibrate our model see section 2 1 although the mapbiomas data are the best available the uncertainty in classifying grassland and mosaics of agriculture and pasture into the classes required for crafty brazil may also contribute some level of error in our calibration fig 3 4 2 tests and scenarios tests of the model using standard differences in model inputs 20 of 2018 values shows that production is insensitive to inputs outputs vary by 20 or 20 but that land cover change is more sensitive some change is 20 all tests result in large 20 decreases in maize land area with smaller but also often large changes in soy due to large increases in double cropping area this shift away from maize only land use is to be expected both due to intended model logic and observed and expected empirical shifts to double cropping systems more obvious in the time series of land cover fig 4 are the large simulated shifts in nature in tests that represent decreased demand dem all decr and dem nnat decr in fig 4a and b or increasing yield yield incr in fig 4c these shifts are due to abandonment of agricultural land crops and pasture which in our model logic then becomes nature abandonment in these tests is driven by decreased demand in agricultural services in the case of the demand tests or a decreased pressure on land for agriculture due to increasing yields which in turn means less land needed to meet the same demand in the case of the yield incr test land cover in other tests is relatively insensitive because of competition for land between services in the dem nnat incr test for example constant 2018 yields means that commodity production never reaches the required demand in these scenarios hence production and land cover time series differ little from those seen in the constant test as production is already at its limit at the start of simulations given the calibrated yield values in contrast when yields increase through time yield incr test less land is needed for pasture land to meet beef demand and much is abandoned reverting to nature land cover yield increases in pasture and crops produce what previous studies defined as land sparing where the increased volume of production per land unit i e agricultural intensification leads to a decrease in cropland or at least alleviating the pressure for cropland expansion angelsen and kaimowitz 2001 hertel et al 2014 in this test production of all products is able to meet the constant 2018 demand because of the high yields resulting in production timeseries that flatline scenarios were designed to enable examination of both the effects of variations in multiple input factors and potential alternative futures for example for all three scenarios we see inter annual variability in soybean and maize production due to climate but with differing overall trend due to variation between scenarios in yield and demand a combined pattern we do not see in the tests the ext np scenario results in greatest agricultural production and lowest nature land area as the ability to farm formerly protected areas plus decreased demand for nature land allow greatest shifts in land from nature to cropland and pasture however increases in cropland and pasture land in the ext np scenario are not spatially confined to formerly protected areas as might have been expected this is likely because these protected areas have relatively limited infrastructure which is considered invariant through time in the ext np scenario with many of the indigenous and park lands some distance from the core agricultural production areas and much pasture and other non protected land available for conversion processes also represented in the model even under the scenarios we examine there is little pressure on the current protected areas however this is a general trend for the entire study area which may be different in particular local realities i e conservation areas that already suffer land use pressure furthermore if restrictions on land use in protected areas really are relaxed e g abessa et al 2019 we might expect improvements in infrastructure e g road building which may in turn lead to a positive feedback and greater exploitation of these areas over the longer term e g weinhold and reis 2008 furthermore in our scenarios demand for nature is specified as an overall percentage change to reflect possible trajectories of policy or socio economic change that value ecosystem demand for ecosystem services as implicitly provided by our nature service is difficult to estimate carpenter et al 2009 hayha et al 2015 brown et al 2017 this is reflected in the fact that while projections of future demand for agricultural commodities are regularly generated by formal government institutions e g mapa 2020 aligned projections of demand for ecosystem services are not common aligned projections of agricultural e g soybeans and non agricultural e g carbon sequestration land benefits would improve our ability to model future scenarios particularly with respect to demand for the nature service comparing results for scenarios with those for tests highlights qualitative differences in spatio temporal variation tests used simple temporally uniform and spatially invariant rates of change based on observed values whereas scenarios used precipitation and temperature outputs from gcms to provide moisture capitals values which have much greater inter annual variability fig 4e the qualitative differences in input time series demonstrates a strong influence of climate inputs on production outputs but not on land cover change the inter annual variability in production in our modelling is not sufficient for land cover change to occur through abandonment as discussed by silva et al 2020 but many fine details of farm level financing that may be vital for individual farm viability are not represented in this model and so we cannot conclusively argue that land cover change would not occur under the climate projections we have examined spatially land change is generally diffuse but with some focused regions of change tests that produce large increases in nature area indicate greatest decreases in pasture and cropland in north east and south west regions respectively where these are initially in 2018 widespread these are prone to greatest decreases as pasture land in the north east areas are the most marginal with historically low stocking rates e g dias filho 2014 whereas the south west is initially dominated by dense cropland and so has most to lose furthermore this is also the region that was most poorly modelled during calibration and as above section 4 1 we suspect land classification challenges confusion between pasture grassland and pasture agriculture mosaic in the mapbiomas input data mapbiomas 2019b also play a role here for similar reasons spatial change is quite dispersed across the study area but with intense change in a focused region in the north east of the study area in bahia state again with switches from pasture to nature 4 3 agency based modelling the tradeoffs necessary in spatial agent based modelling of land use systems have been well identified in the literature in particular with respect to a perceived spectrum from empirically grounded and complicated models to theoretically focused and simple models e g o sullivan et al 2015 sun et al 2016 here our approach has been to build on the theoretical structure provided by the crafty framework and remain relatively conceptually simple but also incorporating empirical data where possible to ground our application for brazil in doing so this version of crafty brazil limited the number of agent functional types and services represented eight services aggregated to four land use cover types and yet this still required the representation of a greater number of capitals 15 and processes multiple including the accrual of debt than we had initially expected a limited number of afts may seem to produce what sayer 1992 p 138 termed a chaotic conception a group of agent types that artificially lump together the unrelated and the inessential and inadequately represent differences between real world actors that are needed to reproduce empirical events however given that our model is implemented at a spatial resolution of 5 km crafty brazil does not represent individual actors as individuated agents but instead aggregates actors across space into grid cells within which human agency is represented by an aft this representation means crafty brazil should be thought of as an agency based model representing the behaviour of aggregate human actions rather than an agent based model representing individual actors activities e g such as that developed by dou et al 2019 furthermore working with relatively coarse aft representations aligns with the relatively coarse spatial representation that inherently lumps multiple real world actors together the aggregation of 27 land types defined at 30 m spatial resolution to four types at 5 km appendix a is robust given that the original classification was hierarchical that we have aligned our reclassification on that hierarchy and that we made further analyses of variability see section 2 1 and millington 2019 although appropriate for the scales we are working at the combination of this agency approach with constraints of the crafty framework presents challenges to representing some processes that influence land decision making of individual actors in our study area for example the design of the crafty framework to ensure computational efficiency means that the history of simulated agents is not retained and that agents cannot anticipate change beyond the next time step we modified the crafty source code to enable some coarse representation of temporally contingent processes i e the debt that farmers incur when setting up a new farm section 2 2 on the agency of multiple aggregated actors however the agency approach combined with the difficulty of tracking history and representing planning strategies presents a challenge for representing the processes that trap producers in cycles of debt and investment silva et al 2020 this combination also limits our ability to understand possible vulnerabilities and responses of producers to temporal e g inter annual variability in climate or other exogenous factors as highlighted above such questions cannot be examined without incorporating representation of history and planning and or working at finer aggregations and scales such that individual actors are represented by individuated agents for example readers considering their own agent based modelling projects whether focused on land use or other environmental issues might learn from this example about aligning scale and detail of conceptualisation in particular we suggest readers compare our broad scale agency based approach for modelling soy and maize to the finer scale and explicitly agent based approached taken by dou et al 2019 to consider for themselves the advantages and disadvantages of the different approaches at the different scales our modelling will continue to focus on broader scale issues as we dynamically couple crafty brazil to a system dynamics model to create a hybrid simulation model for examining the land use impacts of telecoupled global trade millington et al 2017 liu et al 2018 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements we are grateful to numerous colleagues for ideas and understanding gained from discussion on the subjects presented here these include but are not limited to steve peterson jem woods hang xiong yue dou and jack liu plus several local stakeholders in brazil we are grateful to paul mccord for his early work with mapbiomas data thanks also to several anonymous reviewers for their comments which helped improve the manuscript appendix a mapbiomas v4 0 reclassification code and description are from mapbiomas reclassification is used here note that cropland are further disaggregated into soy maize and other crops using planted area data scripts used to resample reclassify and disaggregated the mapbiomas data are available in millington 2019 code description reclassification 1 forest formations nature 1 1 natural forest formations nature 1 1 1 dense forest nature 1 1 2 open forest nature 1 1 3 mangrove nature 1 2 forest plantations nature 2 non forest natural formations nature 2 1 non forest formations in wetlands nature 2 2 grassland pasture nature in protected areas 2 3 salt flat nature 2 4 rocky outcrop other 2 5 other non forest natural formations other 3 farming cropland 3 1 pasture pasture 3 2 agriculture cropland 3 2 1 annual and perennial crop cropland 3 2 2 semi perennial crop cropland 3 3 mosaic of agriculture and pasture cropland 4 non vegetated areas other 4 1 beaches and dunes other 4 2 urban infrastructure other 4 3 mining other 4 4 other non vegetated area other 5 water bodies other 5 1 river lake and ocean other 5 2 aquaculture other 6 not observed other appendix b debt debt incurred by agents following land use change units are years previous land use cropland agent pasture agent nature or other 5 3 soybean maize or other crops 3 3 double cropping 0 3 pasture 4 na appendix c production functions capitals agent functional types and their production weighting factors for each service a soybean aft soybean maize nature ocrops other beef moisture main 0 8 0 0 0 0 0 moisture second 0 0 0 0 0 0 transport 0 5 0 0 0 0 0 land value 0 0 0 0 0 0 conservation 0 0 0 0 0 0 tech soy maize 0 8 0 0 0 0 0 tech pasture 0 0 0 0 0 0 other 0 0 0 0 0 0 protection soy 1 0 0 0 0 0 protection maize 0 0 0 0 0 0 protection beef 0 0 0 0 0 0 protection ocrop 0 0 0 0 0 0 access nature 1 0 0 0 0 0 access soy maize 0 4 0 0 0 0 0 access ocrop 0 0 0 0 0 0 production 1 0 0 0 0 0 b maize aft soybean maize nature ocrops other beef moisture main 0 0 8 0 0 0 0 moisture second 0 0 0 0 0 0 transport 0 0 5 0 0 0 0 land value 0 0 0 0 0 0 conservation 0 0 0 0 0 0 tech soy maize 0 0 8 0 0 0 0 tech pasture 0 0 0 0 0 0 other 0 0 0 0 0 0 protection soy 0 0 0 0 0 0 protection maize 0 1 0 0 0 0 protection beef 0 0 0 0 0 0 protection ocrop 0 0 0 0 0 0 access nature 0 1 0 0 0 0 access soy maize 0 0 4 0 0 0 0 access ocrop 0 0 0 0 0 0 production 0 1 0 0 0 0 c double crop aft soybean maize nature ocrops other beef moisture main 0 8 0 0 0 0 0 moisture second 0 0 5 0 0 0 0 transport 0 5 0 5 0 0 0 0 land value 0 0 0 0 0 0 conservation 0 0 0 0 0 0 tech soy maize 0 8 0 8 0 0 0 0 tech pasture 0 0 0 0 0 0 other 0 0 0 0 0 0 protection soy 1 0 0 0 0 0 protection maize 0 1 0 0 0 0 protection beef 0 0 0 0 0 0 protection ocrop 0 0 0 0 0 0 access nature 1 1 0 0 0 0 access soy maize 0 4 0 4 0 0 0 0 access ocrop 0 0 0 0 0 0 production 0 8 0 75 0 0 0 0 d nature aft soybean maize nature ocrops other beef moisture main 0 0 0 0 0 0 moisture second 0 0 0 0 0 0 transport 0 0 0 0 0 0 land value 0 0 1 0 0 0 conservation 0 0 1 0 0 0 tech soy maize 0 0 0 0 0 0 tech pasture 0 0 0 0 0 0 other 0 0 0 0 0 0 protection soy 0 0 0 0 0 0 protection maize 0 0 0 0 0 0 protection beef 0 0 0 0 0 0 protection ocrop 0 0 0 0 0 0 access nature 0 0 0 0 0 0 access soy maize 0 0 0 0 0 0 access ocrop 0 0 0 0 0 0 production 0 0 1 0 0 0 e other crops aft soybean maize nature ocrops other beef moisture main 0 0 0 0 8 0 0 moisture second 0 0 0 0 0 0 transport 0 0 0 0 5 0 0 land value 0 0 0 0 0 0 conservation 0 0 0 0 0 0 tech soy maize 0 0 0 0 0 0 tech pasture 0 0 0 0 0 0 other 0 0 0 0 0 0 protection soy 0 0 0 0 0 0 protection maize 0 0 0 0 0 0 protection beef 0 0 0 0 0 0 protection ocrop 0 0 0 1 0 0 access nature 0 0 0 0 0 0 access soy maize 0 0 0 0 0 0 access ocrop 0 0 0 1 0 0 production 0 0 0 1 0 0 f other aft soybean maize nature ocrops other beef moisture main 0 0 0 0 0 0 moisture second 0 0 0 0 0 0 transport 0 0 0 0 0 0 land value 0 0 0 0 0 0 conservation 0 0 0 0 0 0 tech soy maize 0 0 0 0 0 0 tech pasture 0 0 0 0 0 0 other 0 0 0 0 1 0 protection soy 0 0 0 0 1 0 protection maize 0 0 0 0 1 0 protection beef 0 0 0 0 1 0 protection ocrop 0 0 0 0 1 0 access nature 0 0 0 0 0 0 access soy maize 0 0 0 0 0 0 access ocrop 0 0 0 0 0 0 production 0 0 0 0 1 0 g pasture aft soybean maize nature ocrops other beef moisture main 0 0 0 0 0 0 2 moisture second 0 0 0 0 0 0 transport 0 0 0 0 0 0 5 land value 0 0 0 0 0 0 conservation 0 0 0 0 0 0 tech soy maize 0 0 0 0 0 0 tech pasture 0 0 0 0 0 1 other 0 0 0 0 0 0 protection soy 0 0 0 0 0 0 protection maize 0 0 0 0 0 0 protection beef 0 0 0 0 0 1 protection ocrop 0 0 0 0 0 0 access nature 0 0 0 0 0 0 2 access soy maize 0 0 0 0 0 0 access ocrop 0 0 0 0 0 0 production 0 0 0 0 0 0 85 appendix d demand and yield projections values are annual change derived from mapa 2020 used in scenarios specified in table 4 a demand soy and maize beef year standard upper standard upper 2019 2 19 3 67 1 53 3 14 2020 2 19 3 67 1 53 3 14 2021 1 34 3 67 2 89 7 81 2022 3 15 5 41 0 30 2 46 2023 2 86 4 43 1 52 3 58 2024 2 68 4 09 5 26 6 50 2025 2 67 3 82 2 14 0 69 2026 2 51 3 41 0 08 0 87 2027 2 44 3 22 3 85 4 26 2028 2 38 3 06 2 07 2 76 2029 2 32 2 89 1 09 0 01 2030 2 25 2 75 3 37 3 84 2031 2 20 2 65 1 50 3 14 2032 2 15 2 55 1 50 3 14 2033 2 10 2 45 1 50 3 14 2034 2 05 2 35 1 50 3 14 2035 2 00 2 25 1 50 3 14 b yield year soy and maize beef standard upper standard upper 2019 0 78 1 66 1 52 2 67 2020 0 78 1 66 1 52 2 67 2021 1 51 1 66 1 66 5 50 2022 1 23 2 26 1 52 4 25 2023 1 10 2 14 1 08 3 26 2024 1 10 1 88 3 75 5 12 2025 1 05 1 72 1 97 1 22 2026 1 02 1 58 1 68 1 76 2027 0 99 1 47 1 71 1 77 2028 0 96 1 37 1 90 1 90 2029 0 93 1 28 0 53 0 79 2030 0 91 1 21 3 39 3 59 2031 0 89 1 16 1 50 2 67 2032 0 87 1 11 1 50 2 67 2033 0 85 1 06 1 50 2 67 2034 0 83 1 01 1 50 2 67 2035 0 81 0 96 1 50 2 67 software and data availability code for both the simulation model and our data analysis is freely available online we refer to the relevant github repositories in the text at the appropriate points the model can be deployed via docker using lane and millington 2021 also see victoria et al 2021 author contributions conceptualization all authors software jm and vk formal analysis jm rbs and dcv data curation jm rbs and dcv writing original draft preparation jm writing review editing jm dcv rbs and mb funding this work was supported by the uk natural environment research council under grant reference ne m021335 1 we gratefully acknowledge the funding support by the são paulo research foundation fapesp processes 14 50628 9 15 25892 7 and 18 08200 2 
25859,the main objective of this study is to develop a new modeling platform which integrates all necessary modeling components for watershed simulation including a pre processing tool a coupling of hydrologic and river transport models and a calibration and uncertainty analysis tool all components are connected and immersed into an open source non proprietary gis software the modeling platform is programmed in microsoft visual studio vb net and equipped with a built in gui facility a minimum data input requirement full capabilities and potential applicability are most compelling of this modeling platform the modeling platform was applied to different watersheds under distinguished weather conditions the simulation results show not only a good agreement with the observation data but also some improvements in comparison with the results obtained from other prevalent hydrologic models keywords modeling platform watershed modeling hydrological process point and non point source uncertainty analysis gis 1 introduction pollutants discharged from point and nonpoint sources into watercourses are major causes of water quality impairment of waterbodies across the globe recognition of vital impacts of point and non point sources of pollution has led to increase efforts over the past decades to evaluate the intensity and quantity of pollutant loads typical techniques for determining an extent and magnitudes of point and non point source pollution include computer based modeling and long term surface water monitoring due to time and expense associated with surface water monitoring a modeling tool is conceived as an effective system to support an estimate of total maximum daily loads tmdls to the waterbodies therefore watershed scale modeling has emerged as an important water resources management tool because of various characteristics and wide ranging applications a number of watershed modeling tools have been developed it creates a difficult situation to select an appropriate model for specific applications various efforts have been initiated to evaluate and compare watershed models each other based on their capabilities and suitability a review of watershed models and model recommendations are carried out by deliman et al 1999 wherein the hspf and swat were ranked the most comprehensive models later on other intensive reviews on prevalent watershed models were implemented by borah and bera 2003 and borah et al 2019 and their recommendations can be useful in helping a potential model user to select a suitable model table 1 shows the list of most 11 prevalent continuous type models selected by borah et al 2019 as shown in this table there is a few watershed models which can provide full facilities of modeling components such as an integration of hydrologic and river transport models interfacing with gis a built in graphic user interface gui and a calibration and uncertainty analysis tool among 11 prevalent models shown in table 1 mike she was ranked highest on the most accurate for representation of the physical processes and also the most numerically complex while the swat was ranked as a promising model for long term continuous simulation in predominantly agricultural watersheds however mike she requires extremely intensive input data see at https manuals mikepoweredbydhi help 2020 water resources mike she print pdf the swat also requires significant amount of data and empirical parameters about 15 required and 16 optional input files for running and calibration arnold et al 2012 in general most of available watershed models require significant input data this consequently causes more difficult to apply a model to the regions where the data are scarce and sparse particularly for watershed modeling in developing countries where the serial observation data are mostly insufficient in both temporal and spatial scales in such situations either the model cannot run due to the missing input data or the missing input data were filled in by model default values which are usually not suitable for a specific condition or region additionally most of watershed models are focused only on the simulation of runoff processes however in order to track the water paths after the runoff a watershed model should be included a channel river routing component to carry on the transport processes into channel and river systems a few watershed models are included the routing component such as swat hec hms annagnps hspf gssha casc2d and mike she nonetheless most of them are simplified the cross section of natural channels and rivers to be a trapezoid or rectangular except two commercial watershed models gssha casc2d and mike she can handle real river morphology geometry cross section furthermore a hydrological model has inherent uncertainties from various sources such as from input data model parameters model algorithm etc abbaspour et al 2004 blasone et al 2008 engeland et al 2016 which causes substantial difficulties for evaluation of these sources of uncertainties and calibration of model parameters due to the complexity of the uncertainty analysis most of hydrologic models are missing this tool as shown in table 1 only 4 models hec hms hspf mike she and swat among 11 prevalent hydrologic models are provided an uncertainty analysis and calibration tool particularly 3 models hspf mike she and swat from these four models have used an external calibration and uncertainty analysis tool eventually taking the most advantages of gis facility such as watershed delineation visualization presentation and extraction of the data soils land use land cover slope length source area selected simulation area etc from gis layers a watershed simulation tool included this capability allows users to quickly examine the impacts of pollutant loadings from point and nonpoint sources organize and analyze input and output information and visualize results through tables graphs and maps only a few watershed models swat hec hms and mike she were customized by gis interface in fact most of them were utilized by the commercial gis software arcgis recently some non proprietary open source gis software were developed and released to public domain such as mapwindow ames et al 2007 https www mapwindow org and qgis https qgis org en site which can provide a free gis platform to the developers to integrate their framework with gis facility particularly swat was interfaced with mapwindow and qgis namely mwswat and qswat respectively however the mwswat is no longer supported and has been replaced by qswat https swat tamu edu software additionally the gwlf also has been customized by mapwindow namely mapshed in reality the mapshed model still has substantial limitations in model algorithm inhered from the original gwlf see at section 2 2 below and missing channel river routing and calibration and uncertainty analysis tool indeed there are still very few models which can be fully facilitated with an open sources gis software with regard to the aspects mentioned above this study aims to develop a new modeling platform which is designed to facilitate and incorporates all necessary tools for the simulation of various watershed scales the novelty of this modeling platform is to provide the users comprehensive watershed simulation tool which integrates all modeling facilities pre processing tool hydrologic and a channel river transport models and a calibration and uncertainty analysis tool particularly all components are connected and immersed under an open source gis roof and equipped with a built in gui system most importantly this modeling platform requires a minimum input data following is detailed description of the modeling platform platform structure and methodology are focused in section 2 the verification and applications of the platform are presented in section 3 finally a conclusion and further development are expressed in section 4 2 platform structure and methodology fig 1 shows a flowchart of the structure of the integrated modeling platform namely seoul national university watershed simulation snu ws platform the modeling platform is programmed in vb net and contains three main components pre processing tool a coupling of hydrologic and a channel river transport models and a calibration and uncertainty analysis tool all three components are connected and immersed into an open source gis mapwindow software www mapwindow org as plug ins and equipped with gui system a detailed description of each component of the modeling platform is presented in the following sections 2 1 gis pre processing method the pre processing procedure is conducted by gis functions of mapwindow the input data are in grid grd files such as dem land use soil total phosphorous and groundwater nitrate and in shape shp files such as basins streams soils weather physiographic provinces counties point sources tile drain roads etc fig 2 shows a gui interactive window for inputting data which are classified into two groups the required layers and optional layers the layers located in grey box are optional while the layers located in green box are required herein six gis layers are required to input in order to run the modeling platform in fact only four among these six gis layers are required because the basins and streams layers can be obtained from dem layer by using watershed delineation tool provided by gis mapwindow less input data requirement is the most compelling of this platform undoubtedly the more reliable input data are available the more accurate simulation can be resulted however serial observed data set associated with temporal and spatial resolutions are always a big issue for many regions particular for developing countries where the budget and facility are still limited to overcome this limitation fig 3 shows a display window once all gis layers of spring creek watershed are loaded into gis mapwindow once the simulation area is selected the gis tool based on attributes of gis layers performing the clipping function to extract all necessary data for the selected area from the input data the outputs from this step are divided into 3 files namely transport dat nutrient dat and weather dat as shown in table 2 which will be inputted into the runoff calculation component the transport dat file contains necessary parameters related to water balance calculation e g area size curve number evaporation initial saturated unsaturated storages recession and seepage coefficients etc and sediment loads sediment delivery ratio erosivity coefficient etc for each considered source area the nutrient dat file contains various parameters related to nutrient loads n and p from different source areas and point sources the weather dat file is included daily maximum minimum of temperatures and average precipitation corresponding to weather station locations 2 2 runoff calculation methods the runoff processes are calculated based on the modification of the formulas using in the generalized watershed loading functions gwlf haith et al 1992 because it requires less data input than other available watershed models deliman et al 1999 in addition the u s environmental protection agency usepa characterizes gwlf as a mid range model in its compendium of tools for watershed assessment and tmdl development us epa 1997 it was also applied to many watersheds such as the hudson river basin in new york howarth et al 1996 the choptank river basin maryland lee et al 2001 etc however the original gwlf version is still inherent a number of limitations in model algorithm such as sediment yield calculation monthly time scale simulation only the sediment yield in gwlf is not consecutively carried over from one year to next year this causes an artificial discontinuity of sediment yield at the sediment year boundary therefore an important development in our modeling platform is to improve the erosion and sediment yield calculations the sediment transport equations are revised to remove the sediment year boundary by implementing the new improved formulae developed by schneiderman et al 2002 moreover instead of monthly time scale calculation the calculation of runoff and its associates have been implemented in both monthly and daily time scales in the new platform we should notice that the runoff calculation version of our platform is independently programming in vb net and definitely different with the original gwlf version which was written in quickbasic 4 5 under dos operation system haith et al 1992 2 2 1 water balance calculation the theoretical background of the runoff process calculation using in the modeling platform is presented in the following sections daily water balance is computed for unsaturated and saturated sub surface zones based on the mass conservation law as follows 1 u t 1 u t r t m t q t e t p c t 2 s t 1 s t p c t g t d t where u t and s t are the water in unsaturated and saturated storage zone at the beginning of day t cm and q t r t m t e t pc t g t and d t are watershed runoff rainfall snowmelt evapotranspiration percolation from unsaturated into the shallow saturated zone groundwater discharge into the stream and seepage from the shallow saturated zone into the deep saturated zone respectively as shown in fig 4 each term in the right hand side of above equations is calculated by following manners the daily runoff q t is computed from weather input data using the u s soil conservation service s curve number equation ogrosky and mockus 1964 3 q t r t m t 0 25 s t 2 r t m t 0 8 s t precipitation is assumed to be rain when daily mean air temperature tt 0c is above 0 the snowmelt m t is computed using temperature index approach that equates the total daily snowmelt to a coefficient times the temperature difference between the mean daily temperature t t and a base temperature t b generally 0 c hydrology national engineering handbook usda 2004 4 m t α t t t b the value of α is the degree day coefficient cm degree day c which varies seasonally and by location its typical values are from 0 16 to 0 6 cm degree day c and an appropriate value is obtained from calibration procedure the detention parameter s t is determined from a curve number cn t as 5 s t 2540 c n t 25 4 curve numbers are selected as functions of antecedent moisture as described in haith et al 1981 whereby the curve number is a function of 5 day antecedent precipitation a t and its exact form of the cn value according to a t is shown in fig 5 below subscription k indicates cn value corresponding to land use k recommended values for am1 and am2 are 1 3 cm and 2 8 cm for dormant seasons and 3 6 cm and 5 3 cm for growing seasons cn1 cn2 and cn3 denote the curve number in the state of driest average wettest moisture conditions cn2 should be provided by the model user and cn1 cn3 will be computed through following equations c n 1 c n 2 2 334 0 01334 c n 2 c n 3 c n 2 0 4036 0 0059 c n 2 because cn value is most renown influence parameter in hydrological modeling its evaluation will be assessed by calibration tool the percolation occurs when unsaturated zone water exceeds water holding capacity u of this zone as follows 6 p c t max 0 u t r t m t q t e t u in principle u is equivalent to a mean watershed maximum rooting depth multiplied by a mean volumetric soil available water capacity the latter also requires determination of a mean unsaturated zone depth and this is probably impractical for most watershed studies a default value of 10 cm can be assumed for pervious areas corresponding to a 100 cm rooting depth and a 0 1 cm cm volumetric available water capacity these values appear typical for a wide range of plants jensen et al 1989 u s forest service 1980 and soils rawls et al 1982 the evapotranspiration e t is initially evaluated as separate dormant and growing season coefficients for each land use 7 e t m i n c v t p e t u t r t m t q t where cv t is a land cover coefficient and its value is selected using the information provided by land use gis layer and from table 12 chapter 12 of crop evapotranspiration allen et al fao 1998 and table 5 in sharmar and thakur 2007 for each crop cover and land use pe t is potential evapotranspiration as given by hamon 1961 8 p e t 0 024 h t 2 e t t t 273 where h t is the number of daylight hours per day during the month containing day t e t is the saturated water vapor pressure approximated as in bosen 1960 9 e t 33 8639 0 00738 t t 0 8072 8 0 000019 1 8 t t 48 0 001316 t t 0 groundwater discharge g t and deep seepage d t are calculated as in haan 1972 10 g t r s t where r is a groundwater recession day 1 the groundwater recession coefficient r can be estimated from two stream flows f t 1 f t 2 measured on days t 1 and t 2 t 2 t 1 during the hydrograph recession 11 r ln f t 1 f t 2 t 2 t 1 the recession coefficient r is measured for a number of hydrographs and an average value is used for the simulation a typical value is in the range from 0 01 to 0 2 an appropriate value is determined by calibration procedure 12 d t s s t where s is a seepage coefficient day 1 no standard techniques are available for estimating the value of the seepage constant s this constant is determined by calibration procedure 2 2 2 nutrient and sediment load calculations nutrient flux contained in streamflow is dissolved and solid phases dissolved nutrients are associated with runoff point sources and groundwater discharges to the stream solid phase nutrients are due to point sources rural soil erosion or wash off of material from urban surfaces the snu ws model describes nonpoint sources with a distributed model for runoff erosion and urban wash off and a lumped parameter linear reservoir groundwater model point sources are added as constant mass loads which are assumed known daily loads of nitrogen or phosphorous in stream flow in any day t are 13 l d t d p t d r t d g t d s t 14 l s t s p t s r t s u t where ld t is dissolved nutrient load ls t is solid phase nutrient load dp t dr t dg t and ds t are point sources rural runoff groundwater nutrient loads septic system nutrient loads respectively and sp t sr t and su t are solid phase point sources rural runoff urban runoff nutrient loads kg respectively in above equations assuming that point source groundwater and septic system loads are entirely dissolved and urban nutrient loads are entirely solid dissolved loads from each source area are obtained by multiplying runoff by dissolved concentration 15 d r t 0 1 k c d k q k t a r k where cd k mg l is nutrient concentration in runoff from source area land use k q kt cm is runoff from source area k on day t ar k is the area of source area k ha solid phase loads are given by the product of daily watershed sediment yield y t m g and average sediment nutrient concentrations c s m g k g 16 s r t 0 001 c s y t in the original gwlf version y t is determined from the model developed by haith 1985 which is produced from soil eroded in the current year no carryover of sediment supply from one year to next year this assumption is significantly limited to sediment yield calculation therefore in the snu ws platform the daily sediment yield y t is determined as in schneiderman et al 2002 the new formulation is based on two well established empirical relationships the first basic empirical relationship is the expression of long term average annual sediment yield from a watershed as a fraction of long term average annual erosion e a n n in the watershed wishchmeirer and smith 1978 and the second basic empirical relationship is the expression of daily sediment yield y t as a power function of streamflow shen and julien 1993 17 y t k t c t e a n n s d r t c t t c a n n where the daily transport capacity tc t of the stream is calculated as t c t q t 1 67 and e a n n is an average annual erosion summed over all sources k 18 e a n n k t 1 n x k t n 365 25 days year n is number of days over which the calculation is made t c a n n is an average annual transport capacity over a long term multi year period and calculated as 19 t c a n n t 1 n t c t n 365 25 d a y y e a r the sediment delivery ratio sdr is calculated as vanoni 1975 20 s d r 0 451 b 0 298 or followed usda 1978 21 s d r 0 42 b 0 125 where b ha is area of the catchment in equation 18 x kt mg is the erosion from source k on day t based on the universal soil loss equation usle wischmeier and smith 1978 22 x k t 0 132 r e t k k l s k c k p k a r k in which re t k k ls k c k and p k are the standard values for the rainfall and runoff erosivity soil erodibility topographic cover and management and supporting practice factors respectively ar k is the area of source area k ha the rainfall and runoff erosivity re t can be estimated by the empirical equation developed by richardson et al 1983 and subsequently tested by haith and merrill 1987 23 r e t 64 6 a t r t 1 81 where the coefficient a t varies with season and geographical location r t is rainfall on day t the value of soil erodibility factor k k is estimated based primarily on soil texture usda 1975 and can be obtained from table a1 in the appendix the value of cover and management factor c k is varied with rainfall distribution and planting dates and ranged from 0 001 for well managed woodland to 1 0 for tilled and continuous fallow the values of the cover and management and supporting practice factors can be selected from table 4 and table 5 of usda 1975 respectively taking the advantage from gis functions which can calculate the topographic factor based on the dem layer the slope length factor ls k for the source area k is calculated as stewart et al 1975 24 l s k l k 22 13 m 0 065 0 043 s 0 0065 s 2 where l k is the average length of slope for the basin m and estimated by l k 0 5 a t s l k where a is area of the basin tsl k is total length of streams within a source area k m is a constant varied with slope gradient and s is the mean slope gradient for the source area the values of m and s are estimated as shown in table 3 the value of the slope length factor ls k obtained from equation 24 is more robust and precise than the method used in the original gwlf the daily groundwater nutrient load to the stream 25 d g t 0 1 c g a g t where c g is concentration of nutrient in groundwater mg l a is total watershed area ha g t is groundwater discharge into stream on day t cm the urban runoff has been modeled to more closely approximate procedures used in the soil conservation service s technical release 55 soil conservation service 1986 and other models such as swmm huber and dickinson 1988 and storm hydrologic engineering center 1977 the urban runoff model is based on general accumulation and wash off relationships proposed by amy et al 1974 and sartor and boyd 1972 daily urban runoff loads are given by 26 s u t 0 1 k w k t a r k where 27 w k t w k t n k t e 0 12 n k 0 12 1 e 0 12 in which w kt is runoff nutrient load from land use k on day t n kt is the nutrient accumulation at the beginning of day t kg ha n k is a constant accumulation rate kg ha day and w kt is the first order wash off function suggested by amy et al 1974 28 w k t 1 e 1 81 q k t where q kt is the daily runoff from the land use k on day t nutrient loads from septic systems are included and calculated by estimating the per capita daily load from each type of system and the number of people in the watershed served by each type septic systems are classified according to four types normal systems ponding systems short circuiting systems and direct discharge systems the daily septic system loads are given by 29 d s t d s 1 t d s 2 t d s 3 t d s 4 t ds it i 1 4 are the dissolved nutrient load to streamflow from normal short circuited ponded and direct discharge systems respectively a normal septic system is a system which is designed constructed and operated by regulations epa design manual normal systems are generally located distance from stream and the phosphates in their effluents are adsorbed and retained in the soil therefore the normal systems will not provide phosphorous loads to streamflow the nitrogen loaded to groundwater on day is calculated 30 d s 1 t 0 001 a 1 t e u t where e is daily nutrient load in septic tank effluent per capita g day u t is daily nutrient uptake by plant per capita g day the short circuited systems are located close enough to surface waters roughly about 15 m the daily nutrient load is estimated by 31 d s 2 t 0 001 a 2 t e u t the ponded systems expose hydraulic failure of septic tank s absorption and resulting in nutrient loads to surface waters the daily nutrient load is estimated by 32 d s 3 t 0 001 p n t where pn t is daily watershed nutrient load in runoff from ponded systems on day t g 33 p n t f n t u t a 3 t e s n t 0 a n d t t 0 0 o t h e r w i s e in which fn t is frozen nutrient accumulation in ponded systems and under freezing condition it is iterated by 34 f n t 1 f n t a 3 t e s n t 0 a n d t t 0 0 o t h e r w i s e sn t g is the amount of snowpack at the beginning of day t t t is mean temperature on day t the direct discharged systems are illegal systems hence the septic tank discharges directly to the surface waters the daily nutrient load is calculated 35 d s 4 t 0 001 a 4 t e where a it i 1 4 in equations 25 26 28 and 30 are loads computed from per capita daily effluent loads for each system 2 3 channel and river routing calculation the channel and river routing component is based on hec ras model https www hec usace army mil software hec ras and included into the modeling platform as a routing plugin see fig 23 in section 3 1 2 the continuity and momentum equations for one dimensional model read as 36 a s t s t q x q l 0 37 q t v q x g a s z x s f 0 where q is total discharge v is mean velocity a s is flow area at a cross section s is storage from non conveying portions of cross section q l is lateral inflow per unit distance s f is friction slope defined by manning equation 38 s f q q n 2 2 208 r 4 3 a s 2 where r is hydraulic radius and n is manning coefficient z x is the water surface slope the transport process in channels rivers is modeled by the advection dispersion for each water quality constituent as shown below 39 a s c t a s v c x x a s d l c x s l where c is the solute concentration of a constituent gm 3 d l is longitudinal dispersion coefficient m 2 s 1 obtained from empirical formulas v is the mean flow velocity ms 1 s l is source sink rate of a constituent gm 3 s 1 hec ras can simulate various water quality components such as nitrogen phosphorous algae carbonaceous biochemical oxygen demand cbod and dissolved oxygen do sediment routing calculation is based on exner s equation known as the sediment continuity equation which translates the difference between inflowing and outflowing loads into bed change eroding and depositing sediment 40 1 λ p b η t q s x where b is channel width η is channel elevation λ p is active layer porosity and q s is sediment load sediment calculation is including three processes bed suspended and wash loads for detail information on the simulation of hydrodynamics sediment transport and water quality parameters the readers can refer to the hec ras reference manual https www hec usace army mil software hec ras documentation aspx the results output from runoff calculation such as daily streamflow hydrograph sediment and nutrient loads total n and total p are input into hec ras at upstream boundary and tributary inlet conditions fig 6 shows the main gui interaction window to run the modeling tool as mentioned above after clipping function done input files transport dat nutrient dat and weather dat are generated to input to the runoff processes calculation these files can be edited by clicking on the buttons edit transport file edit nutrient file and edit weather file respectively after preparing and checking all input data click on the button run snu ws the runoff calculation will be proceeded on the other hand the button exit snu ws is to terminate the simulation run daily and monthly output can press the buttons daily monthly output 2 4 calibration and uncertainty analysis methods in most hydrological models the errors and approximations in input data model parameters model structure and model algorithms are variety sources of uncertainties which cause substantial difficulties for calibration process and uncertainty estimation of a hydrological model therefore we have applied the generalized likelihood uncertainty estimation glue method to build a calibration and uncertainty analysis tool into the simulation tool the glue method was suggested by beven and binley 1992 it generally consists 4 steps including sampling parameters based on an appropriate definition of a prior distribution defining likelihood measure estimating of uncertainty interval using calculated likelihood and lastly calibrating and validating the model within the range of the uncertainty following is the description of these 4 steps 2 4 1 sampling parameter due to numerous uncertainty parameters in hydrologic models an appropriate parameter range has to be determined and a prior distribution of parameter set will be assumed based on model structure after setting prior distribution parameter values will be randomly sampled by running a number of simulations with different parameters within their appropriate parameter ranges the drawback of typical glue application is considerable consuming computational time due to random sampling technique to improve this limitation the latin hypercube sampling strategy lhs is applied because lhs can reduce significantly sampling times in comparison to random sampling based on the set of parameters obtained from sampling procedure the likelihood measure will be defined by different methods is shown in the following section 2 4 2 defining likelihood measure as mentioned by beven and binley 1992 the glue methodology requires the definition of the likelihood measure various goodness of fit indices suggested by beven and binley 1992 have been implemented in this simulation tool as shown following the likelihood measure is defined as a coefficient of determination 41 l 1 σ e 2 σ o 2 σ e 2 σ o 2 where σ e 2 is the variance of the residual and σ o 2 is the variance of observations l will take the value between 0 and 1 l 0 when σ e 2 σ o 2 and l 1 when σ e 2 0 at everywhere this is actually nash sutcliffe efficiency coefficient nse which is frequently used in hydrologic model evaluation most commonly in hydrologic time series as time scale goes down sudden enormous peak values appear frequently if model cannot capture the physics behind it the variance of residuals will be influenced enormously to mitigate this effect another likelihood definition based on l moment can be adopted as follows 42 l 1 λ 2 ε λ 2 o where λ 2 ε is the l scale of residual and λ 2 o is the l scale of observed values another likelihood measure is the inverse variance of the residual given by beven and binley 2001 43 l σ e 2 n where n is the shape factor determined by the user when n 0 every simulation will have the equal likelihood when n the single best simulation will have a rescaled likelihood of 1 while all other simulations will have a likelihood of 0 in most hydrologic model there are numerous types of outputs including discharge sediment yield sediment and nutrient loads etc whose data can be observed from multiple sites in this case we should combine the individual likelihood measures to make overall contributing likelihood weight for each simulation thus a combination of individual likelihood measures for the observations is suggested by dubois and prade 1980 and zimmermann 1991 where the global likelihood measures is based on fuzzy rules which can be a product geometric and arithmetic means and minimum or maximum likelihood measures as follows product likelihood measure 44 l j 1 m l j geometric mean likelihood measure 45 l j 1 m l j m arithmetic mean likelihood measure 46 l 1 m j 1 m ω j l j minimum likelihood measure set intersection 47 l min j 1 m l j maximum likelihood measure set union 48 l max j 1 m l j where l 1 l 2 l m are m individual likelihood measures after defining likelihood measure the next step of the glue methodology is going to find posterior parameter distributions whose range should be broad enough to ensure that the simulation results will be belonged to the range of observations this requires that the users have to choose an appropriate prior range of the parameters to be considered together with the form of the distribution function within that range the suitability of the chosen range may be evaluated by comparison of the predicted responses within that range if the prior knowledge about the appropriate values parameters is insufficient a uniform distribution function is maybe chosen the assumption of a uniform reference prior is unlikely critical because as long as information is added in terms of comparisons between observed and simulation responses for instance beven and binley 1992 has successfully applied uniform distributions for the gwy catchment plynlimon wales 2 4 3 estimating uncertainty interval as mentioned above there are many error sources in any simulation tools caused by poorly defined initial and boundary conditions input data observation data for calibration and validation model structure etc the likelihood weights of the glue procedure will reflect all sources of errors in the modeling process and estimate the uncertainties associated with those errors to be carried forward into the predictions in general the residuals between the observations and simulations are biased and auto correlated with heteroscedastic variance the result of applying the glue procedure is to provide a range of likelihood weight associated with the parameter set producing that residual sequence the aim of this step is to provide an estimate of uncertainty to be consistent with the model limitations caused by the error sources that allows a direct quantitative comparison between different model parameters in comparison the simulation results obtained in time series with the observations the uncertainty interval corresponding to a certain level could be calculated through estimating quantiles of a certain significance level for each time step eventually we can investigate if observation values reside inside or outside of this interval if the uncertainty limits are drawn too narrowly then a comparison with observations will suggest that the model structure is invalid on the other hand if the uncertainty limits are drawn too widely then it might be concluded that the model has little predictive capability in a well posed model with accurate observations the posterior likelihoods should become increasingly constrained as additional observations are considered thus the posterior likelihood distribution may be used directly to evaluate the uncertainty limits for future periods when the observation data may be not available or scarce in order to validate a model 2 4 4 model validation and prediction uncertainty interval obtained from previous step was applied only for calibration period because the likelihood measure was obtained from the period when the observation data are available a maximum likelihood determines the best fit between the simulation result and observation hence the calibration parameters are obtained from this simulation furthermore we can use likelihood measure calculated from the calibration period for the validation period after importing real observed values in this period we can check if this interval covers observation value or not if the validation succeeds we can repeat previous procedure to get another uncertainty interval for prediction in future periods when the observation data are not available fig 7 shows an interactive window to input data for calibration and uncertainty analysis processes based on suggestions of haith et al 1992 and li et al 2010 the parameters and their ranges for the calibration and analysis tool are set up the left column shows the parameters contributed to water balance processes such as cn values associated with different types of landuse landcover groundwater recession and seepage coefficients available water holding capacity of unsaturated zone the right column shows the parameters contributed to water quality components such as sediment nitrogen and phosphorous the button include from drop down menu is selected to input the value of parameter once the value of parameter is not available the button exclude can be selected then a model default value is set for the calibration and uncertainty analysis of water balance the parameters from transport file will be loaded while the parameters from nutrient file will be loaded when the calibration and uncertainty analysis process is carried out for water quality components as shown in fig 8 a performance of uncertainty analysis can be proceeded by selecting the button daily obs data or monthly obs data depending on selected timescales the observation files of streamflow and nutrient nitrogen phosphorus are imported to the system in order to prepare for the calibration and validation periods whereby the box prediction period is selected after finishing the calibration and validation processes this step is applied for the period when the observation data are not available such as future or climate change evaluations fig 9 shows different options for sampling method likelihood definition and quantile boundary finally simulation number should be given and significance level for uncertainty interval should be defined the default values of simulation number and prediction interval are 1000 and 95 2 5 97 5 respectively after defining these required information the uncertainty analysis will be proceeded 3 verifications and applications the snu ws platform is applied to different watersheds under distinguished weather conditions the spring creek watershed located in the center region of pennsylvania usa and the la buong river basins located on downstream of dong nai river in vietnam simulation results are not only verified against available hydrological models but also calibrated and validated against observation data 3 1 application to spring creek watershed pennsylvania the spring creek watershed fig 10 is located along the eastern edge of the appalachian mountains which extend from maine to georgia and is bordered by long high ridges on the north and south with a broad valley in between its area is about 93400 acres 378 km2 including about 27519 acres 29 of agriculture 24421 acres 26 of urban 35689 acres 38 of forestland and 5463 acres 6 of vacant reclaimed or mined lands and it is a home of 140 000 people annual average precipitation is close to 110 6 cm 40 inches data obtained for this simulation are from different sources as shown in table 4 spring creek main stream has 5 tributaries big hollow buffalo run slab cabin run cedar and run logan branch fig 10 right despite rapid population growth and enhanced wastewater flows nitrate levels in spring creek basin springs and streams are generally declining and remain well below drinking water standards of 10 ppm of nitrate nitrogen figs 2 and 3 in section 2 1 show all gis layers of the spring creek watershed uploaded into the gis mapwindow interface of snu ws tool the appearance on fig 3 shows the sub basins and stream system in this watershed area fig 11 and table 5 show the locations and magnitudes of point sources in the spring creek watershed 3 1 1 verification against the mapshed model as mentioned in section 1 introduction mapshed is customized by original gwlf evans et al 2002 https wikiwatershed org help model help mapshed our runoff calculation was also developed based on the modification of the formulas used in original gwlf see section 2 2 therefore we first have verified our runoff simulation results against the mapshed model s results because the mapshed model does not include routing calculation in order to do that we applied two models to the same spring creek watershed the runoff simulation was carried out for 13 years from 1988 to 2000 using the coefficient of determination r2 to compare the results between our runoff calculation and mapshed model it shows very good agreement for the most of components in water balance equation evapotranspiration streamflow groundwater flow runoff etc and nutrient loads total n and total p with r2 in the range of 0 908 0 997 as shown in table 6 however the difference of erosion and sediment between the runoff simulation results of snu ws platform and the mapshed model are occurred because we have revised the formulae for sediment yields suggested by schneider et al in our model as shown in equations 17 19 and 24 above it should be noticed that the streamflow at houserville station obtained from runoff calculation of snu ws platform without routing calculation and mapshed model shown in fig 12 is still away from the observation data the runoff processes from land surface were continuously carried on by the routing hec ras model in the application to the spring creek the main stretch from downstream of houserville station to the outlet of spring creek is shown in fig 13 whereby the main stretch of spring creek on upstream of houserville station the slap cabin and cedar runs are aggregated flowing to houserville station where the inlet boundary condition was setup three other tributaries such as the big hollow buffalo and logan branches are simplified as 3 other inlet boundaries to the main stretch at the confluences whereby the rate of streamflow hydrograph sediment and nutrient loads are set at the upstream of houserville station and at three inlet boundaries from big hollow buffalo and logan tributaries 3 1 2 verification against observation data the following calibration and validation are focused on the streamflow and nutrients at two stations the houserville upstream and axemann downstream stations see red points in fig 13 the simulation has been performed for daily and monthly time scales within the period from 1988 to 2010 including 2 warm up years 1988 1989 wherein the calibration and validation for streamflow are carried out for the periods of 1990 2000 and 2001 2010 respectively for uncertainty analysis we ran 1000 simulation runs and using lhs for sampling parameter process and the uncertainty bound between 5 and 95 90 prediction interval is selected 3 1 2 1 daily calibration and validation figs 14 and 15 show the calibration for the period from 1990 to 2000 and validation for the period from 2001 to 2010 of simulation results against the observations of the daily streamflow at houserville and axemann stations respectively the uncertainty analysis of daily streamflow between simulation and observation at houserville and axemann stations with the prediction for quantile of 5 and 95 are shown in figs 16 and 17 respectively 3 1 2 2 monthly calibration and validation similar to processes carried out for daily streamflow the calibration validation and uncertainty analysis for monthly streamflow at houserville and axemann stations are shown in figs 18 21 it shows very good agreement between the simulation results and observations the observation data of monthly nutrient loads total n and total p are sparse and available only at axemann station from 2002 to 2010 therefore the calibration and validation of monthly nutrient loads are implemented at axemann station within this period shown in figs 22 and 23 because total nitrogen tot n is the sum of nitrate no3 nitrite no2 organic nitrogen and ammonia fig 22 show tot n at axemann station located downstream of spring creek is below 6 mg l 6 ppm it is again to confirm that the concentration of nitrate nitrogen remain well below drinking water standards of 10 ppm as mentioned above table 7 shows the statistical criteria classified by moriasi et al 2015 see table a2 in appendix it shows that the simulation performance for the daily streamflow 0 7 r 2 0 85 a n d 0 7 n s e 0 8 is in the good range while the simulation performance for the monthly streamflow r 2 0 85 a n d n s e 0 8 and nutrient loads t n r 2 0 7 a n d n s e 0 7 and t p r 2 0 8 a n d n s e 0 65 are in a very good range fig 24 shows the bed form was changed within 1 month at two cross sections one section 19025 located just downstream of houserville station and another one section 2671 located just downstream of axemann station see fig 13 at the cross section 19025 a sedimentation process was occurred while the erosion process was happened at cross section 2671 since we do not have the data of sediment transport along the river stretch a comparison between the simulation results and observation cannot perform herein 3 2 application to la buong river basin vietnam la buong river which has a length of 56 km and an annual discharge of 412 106 m3 is a tributary of dong nai river and located in downstream dong nai river the la buong river basin fig 25 has a total drainage area of 478 5 km2 with the stream density of 0 67 km km2 and an annual average precipitation and temperature of 1800 mm and 25 26 c respectively due to the relatively flat terrain and influenced by the tropical monsoon climate this area is divided into two main seasons the dry season november april and rainy season may october moreover the morphology of la buong river basin is leaf shaped the hydrology is heavy rainfall area causing flash flooding problems in the center and upper regions while the lower part is effected by inundation and tide this area has relatively fertilized land in which about 75 of the catchment area are covered by rhodic ferralsols and ferric acrisols and consistent with agricultural development the cultivated land occupies more than 80 of the total area in comparison to the spring watershed above the available input data for the la buong river basin are limited only four gis layers including topography dem land use soil and weather sole station a digital elevation model dem with a resolution of 12 5 m was downloaded from the alaska satellite facility asf distributed active archive center daac to delineate the whole basin and its sub basins land use data with a spatial resolution of 1 km were collected from the sub national institute of agricultural planning and projection of vietnam sub niapp soil data with a spatial resolution of 10 km were obtained from the food and agricultural organization fao of the united nations meteorological data from 1981 to 2015 at three stations were collected from the hydro meteorological data center of vietnam hmdc furthermore only monthly observed streamflow data are existed from 1983 to 1993 at the la buong station and the sparse nutrient tot n and tot p and sediment data at the station sw sbu 01 from 2010 to 2015 and at the station sw sbu 06 from 2012 to 2015 were collected by the department of natural resources and environment of dong nai province for model calibration and validation processes shown in fig 25 in this application the snu ws platform and swat are used in order to verify the performance of our platform against swat model the objective of this comparison is not to demonstrate that our modeling platform is better than swat because the swat is very well known and widely applicable for more than 20 years however this comparison is aimed to testify how two models can perform once the input data are limited as in this case study the swat applied an external calibration and uncertainty tool the swat calibration and uncertainty program swat cup arnold et al 2012 more information about the calibration process of swat cup for the la buong river basin can find in our previous publication khoi et al 2019 figs 26 and 27 show daily and monthly calibration 1983 1988 and validation 1989 1993 of the streamflow at la buong station see station location in fig 25 herewith the comparisons of daily and monthly streamflow between the observation data with the simulation results obtained from snu ws and swat are presented as well figs 28 and 29 show the prediction for quantiles of 5 95 of daily and monthly streamflow between the simulation results obtained from snu ws platform and the observation figs 30 32 show the calibration 2010 2012 and validation 2013 2015 of simulation results obtained from snu ws platform and swat model against the observation data for tot n tot p and sediment loads at station sw sbu 01 figs 33 35 show the calibration 2012 2013 and validation 2014 2015 of simulation results obtained from snu ws platform and swat model against the observation data for tot n tot p and sediment loads at station sw sbu 06 a comparison of model evaluation criteria between the simulation results obtained from our snu ws platform and swat model with the observation data is shown in table 8 based on the values of the statistical criteria r2 and nse see table a2 in appendix the model performance is marked in red in table 8 it shows that the snu ws performed better than swat for both daily and monthly streamflow whereby the snu ws is classified into very good performance r2 0 85 nse 0 85 while the swat model is classified into satisfactory performance 0 5 r2 0 7 0 5 nse 0 7 for daily streamflow and into satisfactory performance 0 7 r 2 0 8 0 7 n s e 0 85 for monthly streamflow within calibration period and into good performance 0 85 r 2 0 7 n s e 0 85 within validation period similar trends for the simulation of nutrient and sediment loads it again shows that the results obtained from snu ws are overall better than those results obtained from swat in comparison with the observation 4 conclusions and further development as shown above we have developed a new integrated modeling platform for watershed modeling including pre processing tool a coupling of hydrologic and a channel river transport models and a calibration and uncertainty analysis tool all components are connected and immersed into an open source gis mapwindow and equipped with gui system contrastively the modeling platform was applied to two distinguished watersheds under different weather conditions and different levels of available data for input calibration and validation the spring creek watershed pennsylvania usa and the la buong watershed vietnam while the spring creek watershed located in the temperate zone has more readily available data the la buong watershed located in tropical wet dry zone has less available data in the application to the spring creek watershed the verification of our modeling platform against the mapshed has showed significant improvements in our snu ws platform additionally as shown in the la buong watershed s application with a minimum input data the comparison of the simulation outcomes from our modeling platform and the swat model with the observation data indicates that the performance of our modeling platform is overall better furthermore the simulation results were calibrated and validated against the observation data a good agreement between model results and observation demonstrates the capability of our simulation tool therefore it can be used as a simulation tool for watershed management however the modeling platform should be further applied to different watersheds under various conditions and intensively calibrated and validated against observation data moreover it should state that our modeling platform still has some limitations such as the snowmelt calculation based on temperature index only the groundwater discharge and deep seepage calculations based on lumped parameter linear reservoir model of which they need to be enhanced in the future version we also plan to release the snu ws platform to the public domain by which we can receive the feedback from the users in order to further improve the simulation platform declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement the authors would like to thank to the support from the national research foundation of korea grant number nrf 2018r1d1a1a09083747 the authors also would like to thank the anonymous reviewers for their valuable and constructive comments to improve our manuscript appendix a table a1 values of soil erodibility factor k k control of water pollution from cropland usda 1975 table a1 texture class organic matter content 0 5 2 4 sand 0 05 0 03 0 02 fine sand 0 16 0 14 0 10 very fine sand 0 42 0 36 0 28 loamy sand 0 12 0 10 0 08 loamy fine sand 0 24 0 20 0 16 loamy very fine sand 0 44 0 38 0 30 sandy loam 0 27 0 24 0 19 fine sandy loam 0 35 0 30 0 24 very fine sandy loam 0 47 0 41 0 33 loam 0 38 0 34 0 29 silt loam 0 48 0 42 0 33 silt 0 60 0 52 0 42 sandy clay loam 0 27 0 25 0 21 clay loam 0 28 0 25 0 21 silty clay loam 0 37 0 32 0 26 sandy clay 0 14 0 13 0 12 silty clay 0 25 0 23 0 19 clay 0 13 0 29 table a2 the statistical criteria r 2 and n s e measured the performance evaluation of the watershed models moriasi et al 2015 table a2 measure component temporal scale very good good satisfactory not satisfactory r 2 flow annual 0 75 0 70 r2 0 75 0 60 r2 0 70 0 60 monthly 0 85 0 80 r2 0 85 0 70 r2 0 80 0 70 daily 0 85 0 70 r2 0 85 0 50 r2 0 70 0 50 sediment annual monthly 0 8 0 65 r2 0 80 0 40 r2 0 65 0 40 daily n annual monthly 0 7 0 60 r2 0 70 0 30 r2 0 60 0 30 daily p annual monthly 0 8 0 65 r2 0 80 0 40 r2 0 65 0 40 daily general 0 8 0 70 r2 0 80 0 50 r2 0 70 0 50 nse flow annual 0 75 0 60 nse 0 75 0 50 nse 0 60 0 50 monthly 0 85 0 70 nse 0 85 0 55 nse 0 70 0 55 daily 0 80 0 70 nse 0 80 0 50 nse 0 70 0 50 sediment annual monthly 0 80 0 70 nse 0 80 0 45 nse 0 70 0 45 daily n annual monthly 0 70 0 60 nse 0 70 0 35 nse 0 60 0 35 daily 0 55 0 40 nse 0 55 0 25 nse 0 40 0 25 p annual 0 65 0 60 nse 0 65 0 50 nse 0 60 0 50 monthly 0 65 0 50 nse 0 65 0 40 nse 0 50 0 40 daily the coefficient of determination r 2 and nash sutcliffe efficiency coefficient nse are calculated by the following equations a2 1 r 2 i 1 n o i o p i p i 1 n o i o 2 0 5 i 1 n p i p 2 0 5 2 a2 2 n s e 1 i 1 n o i p i 2 i 1 n o i o 2 where p i and o i are the simulated and observed values p and o are the mean of the simulated and observed values and n is the total number of observations 
25859,the main objective of this study is to develop a new modeling platform which integrates all necessary modeling components for watershed simulation including a pre processing tool a coupling of hydrologic and river transport models and a calibration and uncertainty analysis tool all components are connected and immersed into an open source non proprietary gis software the modeling platform is programmed in microsoft visual studio vb net and equipped with a built in gui facility a minimum data input requirement full capabilities and potential applicability are most compelling of this modeling platform the modeling platform was applied to different watersheds under distinguished weather conditions the simulation results show not only a good agreement with the observation data but also some improvements in comparison with the results obtained from other prevalent hydrologic models keywords modeling platform watershed modeling hydrological process point and non point source uncertainty analysis gis 1 introduction pollutants discharged from point and nonpoint sources into watercourses are major causes of water quality impairment of waterbodies across the globe recognition of vital impacts of point and non point sources of pollution has led to increase efforts over the past decades to evaluate the intensity and quantity of pollutant loads typical techniques for determining an extent and magnitudes of point and non point source pollution include computer based modeling and long term surface water monitoring due to time and expense associated with surface water monitoring a modeling tool is conceived as an effective system to support an estimate of total maximum daily loads tmdls to the waterbodies therefore watershed scale modeling has emerged as an important water resources management tool because of various characteristics and wide ranging applications a number of watershed modeling tools have been developed it creates a difficult situation to select an appropriate model for specific applications various efforts have been initiated to evaluate and compare watershed models each other based on their capabilities and suitability a review of watershed models and model recommendations are carried out by deliman et al 1999 wherein the hspf and swat were ranked the most comprehensive models later on other intensive reviews on prevalent watershed models were implemented by borah and bera 2003 and borah et al 2019 and their recommendations can be useful in helping a potential model user to select a suitable model table 1 shows the list of most 11 prevalent continuous type models selected by borah et al 2019 as shown in this table there is a few watershed models which can provide full facilities of modeling components such as an integration of hydrologic and river transport models interfacing with gis a built in graphic user interface gui and a calibration and uncertainty analysis tool among 11 prevalent models shown in table 1 mike she was ranked highest on the most accurate for representation of the physical processes and also the most numerically complex while the swat was ranked as a promising model for long term continuous simulation in predominantly agricultural watersheds however mike she requires extremely intensive input data see at https manuals mikepoweredbydhi help 2020 water resources mike she print pdf the swat also requires significant amount of data and empirical parameters about 15 required and 16 optional input files for running and calibration arnold et al 2012 in general most of available watershed models require significant input data this consequently causes more difficult to apply a model to the regions where the data are scarce and sparse particularly for watershed modeling in developing countries where the serial observation data are mostly insufficient in both temporal and spatial scales in such situations either the model cannot run due to the missing input data or the missing input data were filled in by model default values which are usually not suitable for a specific condition or region additionally most of watershed models are focused only on the simulation of runoff processes however in order to track the water paths after the runoff a watershed model should be included a channel river routing component to carry on the transport processes into channel and river systems a few watershed models are included the routing component such as swat hec hms annagnps hspf gssha casc2d and mike she nonetheless most of them are simplified the cross section of natural channels and rivers to be a trapezoid or rectangular except two commercial watershed models gssha casc2d and mike she can handle real river morphology geometry cross section furthermore a hydrological model has inherent uncertainties from various sources such as from input data model parameters model algorithm etc abbaspour et al 2004 blasone et al 2008 engeland et al 2016 which causes substantial difficulties for evaluation of these sources of uncertainties and calibration of model parameters due to the complexity of the uncertainty analysis most of hydrologic models are missing this tool as shown in table 1 only 4 models hec hms hspf mike she and swat among 11 prevalent hydrologic models are provided an uncertainty analysis and calibration tool particularly 3 models hspf mike she and swat from these four models have used an external calibration and uncertainty analysis tool eventually taking the most advantages of gis facility such as watershed delineation visualization presentation and extraction of the data soils land use land cover slope length source area selected simulation area etc from gis layers a watershed simulation tool included this capability allows users to quickly examine the impacts of pollutant loadings from point and nonpoint sources organize and analyze input and output information and visualize results through tables graphs and maps only a few watershed models swat hec hms and mike she were customized by gis interface in fact most of them were utilized by the commercial gis software arcgis recently some non proprietary open source gis software were developed and released to public domain such as mapwindow ames et al 2007 https www mapwindow org and qgis https qgis org en site which can provide a free gis platform to the developers to integrate their framework with gis facility particularly swat was interfaced with mapwindow and qgis namely mwswat and qswat respectively however the mwswat is no longer supported and has been replaced by qswat https swat tamu edu software additionally the gwlf also has been customized by mapwindow namely mapshed in reality the mapshed model still has substantial limitations in model algorithm inhered from the original gwlf see at section 2 2 below and missing channel river routing and calibration and uncertainty analysis tool indeed there are still very few models which can be fully facilitated with an open sources gis software with regard to the aspects mentioned above this study aims to develop a new modeling platform which is designed to facilitate and incorporates all necessary tools for the simulation of various watershed scales the novelty of this modeling platform is to provide the users comprehensive watershed simulation tool which integrates all modeling facilities pre processing tool hydrologic and a channel river transport models and a calibration and uncertainty analysis tool particularly all components are connected and immersed under an open source gis roof and equipped with a built in gui system most importantly this modeling platform requires a minimum input data following is detailed description of the modeling platform platform structure and methodology are focused in section 2 the verification and applications of the platform are presented in section 3 finally a conclusion and further development are expressed in section 4 2 platform structure and methodology fig 1 shows a flowchart of the structure of the integrated modeling platform namely seoul national university watershed simulation snu ws platform the modeling platform is programmed in vb net and contains three main components pre processing tool a coupling of hydrologic and a channel river transport models and a calibration and uncertainty analysis tool all three components are connected and immersed into an open source gis mapwindow software www mapwindow org as plug ins and equipped with gui system a detailed description of each component of the modeling platform is presented in the following sections 2 1 gis pre processing method the pre processing procedure is conducted by gis functions of mapwindow the input data are in grid grd files such as dem land use soil total phosphorous and groundwater nitrate and in shape shp files such as basins streams soils weather physiographic provinces counties point sources tile drain roads etc fig 2 shows a gui interactive window for inputting data which are classified into two groups the required layers and optional layers the layers located in grey box are optional while the layers located in green box are required herein six gis layers are required to input in order to run the modeling platform in fact only four among these six gis layers are required because the basins and streams layers can be obtained from dem layer by using watershed delineation tool provided by gis mapwindow less input data requirement is the most compelling of this platform undoubtedly the more reliable input data are available the more accurate simulation can be resulted however serial observed data set associated with temporal and spatial resolutions are always a big issue for many regions particular for developing countries where the budget and facility are still limited to overcome this limitation fig 3 shows a display window once all gis layers of spring creek watershed are loaded into gis mapwindow once the simulation area is selected the gis tool based on attributes of gis layers performing the clipping function to extract all necessary data for the selected area from the input data the outputs from this step are divided into 3 files namely transport dat nutrient dat and weather dat as shown in table 2 which will be inputted into the runoff calculation component the transport dat file contains necessary parameters related to water balance calculation e g area size curve number evaporation initial saturated unsaturated storages recession and seepage coefficients etc and sediment loads sediment delivery ratio erosivity coefficient etc for each considered source area the nutrient dat file contains various parameters related to nutrient loads n and p from different source areas and point sources the weather dat file is included daily maximum minimum of temperatures and average precipitation corresponding to weather station locations 2 2 runoff calculation methods the runoff processes are calculated based on the modification of the formulas using in the generalized watershed loading functions gwlf haith et al 1992 because it requires less data input than other available watershed models deliman et al 1999 in addition the u s environmental protection agency usepa characterizes gwlf as a mid range model in its compendium of tools for watershed assessment and tmdl development us epa 1997 it was also applied to many watersheds such as the hudson river basin in new york howarth et al 1996 the choptank river basin maryland lee et al 2001 etc however the original gwlf version is still inherent a number of limitations in model algorithm such as sediment yield calculation monthly time scale simulation only the sediment yield in gwlf is not consecutively carried over from one year to next year this causes an artificial discontinuity of sediment yield at the sediment year boundary therefore an important development in our modeling platform is to improve the erosion and sediment yield calculations the sediment transport equations are revised to remove the sediment year boundary by implementing the new improved formulae developed by schneiderman et al 2002 moreover instead of monthly time scale calculation the calculation of runoff and its associates have been implemented in both monthly and daily time scales in the new platform we should notice that the runoff calculation version of our platform is independently programming in vb net and definitely different with the original gwlf version which was written in quickbasic 4 5 under dos operation system haith et al 1992 2 2 1 water balance calculation the theoretical background of the runoff process calculation using in the modeling platform is presented in the following sections daily water balance is computed for unsaturated and saturated sub surface zones based on the mass conservation law as follows 1 u t 1 u t r t m t q t e t p c t 2 s t 1 s t p c t g t d t where u t and s t are the water in unsaturated and saturated storage zone at the beginning of day t cm and q t r t m t e t pc t g t and d t are watershed runoff rainfall snowmelt evapotranspiration percolation from unsaturated into the shallow saturated zone groundwater discharge into the stream and seepage from the shallow saturated zone into the deep saturated zone respectively as shown in fig 4 each term in the right hand side of above equations is calculated by following manners the daily runoff q t is computed from weather input data using the u s soil conservation service s curve number equation ogrosky and mockus 1964 3 q t r t m t 0 25 s t 2 r t m t 0 8 s t precipitation is assumed to be rain when daily mean air temperature tt 0c is above 0 the snowmelt m t is computed using temperature index approach that equates the total daily snowmelt to a coefficient times the temperature difference between the mean daily temperature t t and a base temperature t b generally 0 c hydrology national engineering handbook usda 2004 4 m t α t t t b the value of α is the degree day coefficient cm degree day c which varies seasonally and by location its typical values are from 0 16 to 0 6 cm degree day c and an appropriate value is obtained from calibration procedure the detention parameter s t is determined from a curve number cn t as 5 s t 2540 c n t 25 4 curve numbers are selected as functions of antecedent moisture as described in haith et al 1981 whereby the curve number is a function of 5 day antecedent precipitation a t and its exact form of the cn value according to a t is shown in fig 5 below subscription k indicates cn value corresponding to land use k recommended values for am1 and am2 are 1 3 cm and 2 8 cm for dormant seasons and 3 6 cm and 5 3 cm for growing seasons cn1 cn2 and cn3 denote the curve number in the state of driest average wettest moisture conditions cn2 should be provided by the model user and cn1 cn3 will be computed through following equations c n 1 c n 2 2 334 0 01334 c n 2 c n 3 c n 2 0 4036 0 0059 c n 2 because cn value is most renown influence parameter in hydrological modeling its evaluation will be assessed by calibration tool the percolation occurs when unsaturated zone water exceeds water holding capacity u of this zone as follows 6 p c t max 0 u t r t m t q t e t u in principle u is equivalent to a mean watershed maximum rooting depth multiplied by a mean volumetric soil available water capacity the latter also requires determination of a mean unsaturated zone depth and this is probably impractical for most watershed studies a default value of 10 cm can be assumed for pervious areas corresponding to a 100 cm rooting depth and a 0 1 cm cm volumetric available water capacity these values appear typical for a wide range of plants jensen et al 1989 u s forest service 1980 and soils rawls et al 1982 the evapotranspiration e t is initially evaluated as separate dormant and growing season coefficients for each land use 7 e t m i n c v t p e t u t r t m t q t where cv t is a land cover coefficient and its value is selected using the information provided by land use gis layer and from table 12 chapter 12 of crop evapotranspiration allen et al fao 1998 and table 5 in sharmar and thakur 2007 for each crop cover and land use pe t is potential evapotranspiration as given by hamon 1961 8 p e t 0 024 h t 2 e t t t 273 where h t is the number of daylight hours per day during the month containing day t e t is the saturated water vapor pressure approximated as in bosen 1960 9 e t 33 8639 0 00738 t t 0 8072 8 0 000019 1 8 t t 48 0 001316 t t 0 groundwater discharge g t and deep seepage d t are calculated as in haan 1972 10 g t r s t where r is a groundwater recession day 1 the groundwater recession coefficient r can be estimated from two stream flows f t 1 f t 2 measured on days t 1 and t 2 t 2 t 1 during the hydrograph recession 11 r ln f t 1 f t 2 t 2 t 1 the recession coefficient r is measured for a number of hydrographs and an average value is used for the simulation a typical value is in the range from 0 01 to 0 2 an appropriate value is determined by calibration procedure 12 d t s s t where s is a seepage coefficient day 1 no standard techniques are available for estimating the value of the seepage constant s this constant is determined by calibration procedure 2 2 2 nutrient and sediment load calculations nutrient flux contained in streamflow is dissolved and solid phases dissolved nutrients are associated with runoff point sources and groundwater discharges to the stream solid phase nutrients are due to point sources rural soil erosion or wash off of material from urban surfaces the snu ws model describes nonpoint sources with a distributed model for runoff erosion and urban wash off and a lumped parameter linear reservoir groundwater model point sources are added as constant mass loads which are assumed known daily loads of nitrogen or phosphorous in stream flow in any day t are 13 l d t d p t d r t d g t d s t 14 l s t s p t s r t s u t where ld t is dissolved nutrient load ls t is solid phase nutrient load dp t dr t dg t and ds t are point sources rural runoff groundwater nutrient loads septic system nutrient loads respectively and sp t sr t and su t are solid phase point sources rural runoff urban runoff nutrient loads kg respectively in above equations assuming that point source groundwater and septic system loads are entirely dissolved and urban nutrient loads are entirely solid dissolved loads from each source area are obtained by multiplying runoff by dissolved concentration 15 d r t 0 1 k c d k q k t a r k where cd k mg l is nutrient concentration in runoff from source area land use k q kt cm is runoff from source area k on day t ar k is the area of source area k ha solid phase loads are given by the product of daily watershed sediment yield y t m g and average sediment nutrient concentrations c s m g k g 16 s r t 0 001 c s y t in the original gwlf version y t is determined from the model developed by haith 1985 which is produced from soil eroded in the current year no carryover of sediment supply from one year to next year this assumption is significantly limited to sediment yield calculation therefore in the snu ws platform the daily sediment yield y t is determined as in schneiderman et al 2002 the new formulation is based on two well established empirical relationships the first basic empirical relationship is the expression of long term average annual sediment yield from a watershed as a fraction of long term average annual erosion e a n n in the watershed wishchmeirer and smith 1978 and the second basic empirical relationship is the expression of daily sediment yield y t as a power function of streamflow shen and julien 1993 17 y t k t c t e a n n s d r t c t t c a n n where the daily transport capacity tc t of the stream is calculated as t c t q t 1 67 and e a n n is an average annual erosion summed over all sources k 18 e a n n k t 1 n x k t n 365 25 days year n is number of days over which the calculation is made t c a n n is an average annual transport capacity over a long term multi year period and calculated as 19 t c a n n t 1 n t c t n 365 25 d a y y e a r the sediment delivery ratio sdr is calculated as vanoni 1975 20 s d r 0 451 b 0 298 or followed usda 1978 21 s d r 0 42 b 0 125 where b ha is area of the catchment in equation 18 x kt mg is the erosion from source k on day t based on the universal soil loss equation usle wischmeier and smith 1978 22 x k t 0 132 r e t k k l s k c k p k a r k in which re t k k ls k c k and p k are the standard values for the rainfall and runoff erosivity soil erodibility topographic cover and management and supporting practice factors respectively ar k is the area of source area k ha the rainfall and runoff erosivity re t can be estimated by the empirical equation developed by richardson et al 1983 and subsequently tested by haith and merrill 1987 23 r e t 64 6 a t r t 1 81 where the coefficient a t varies with season and geographical location r t is rainfall on day t the value of soil erodibility factor k k is estimated based primarily on soil texture usda 1975 and can be obtained from table a1 in the appendix the value of cover and management factor c k is varied with rainfall distribution and planting dates and ranged from 0 001 for well managed woodland to 1 0 for tilled and continuous fallow the values of the cover and management and supporting practice factors can be selected from table 4 and table 5 of usda 1975 respectively taking the advantage from gis functions which can calculate the topographic factor based on the dem layer the slope length factor ls k for the source area k is calculated as stewart et al 1975 24 l s k l k 22 13 m 0 065 0 043 s 0 0065 s 2 where l k is the average length of slope for the basin m and estimated by l k 0 5 a t s l k where a is area of the basin tsl k is total length of streams within a source area k m is a constant varied with slope gradient and s is the mean slope gradient for the source area the values of m and s are estimated as shown in table 3 the value of the slope length factor ls k obtained from equation 24 is more robust and precise than the method used in the original gwlf the daily groundwater nutrient load to the stream 25 d g t 0 1 c g a g t where c g is concentration of nutrient in groundwater mg l a is total watershed area ha g t is groundwater discharge into stream on day t cm the urban runoff has been modeled to more closely approximate procedures used in the soil conservation service s technical release 55 soil conservation service 1986 and other models such as swmm huber and dickinson 1988 and storm hydrologic engineering center 1977 the urban runoff model is based on general accumulation and wash off relationships proposed by amy et al 1974 and sartor and boyd 1972 daily urban runoff loads are given by 26 s u t 0 1 k w k t a r k where 27 w k t w k t n k t e 0 12 n k 0 12 1 e 0 12 in which w kt is runoff nutrient load from land use k on day t n kt is the nutrient accumulation at the beginning of day t kg ha n k is a constant accumulation rate kg ha day and w kt is the first order wash off function suggested by amy et al 1974 28 w k t 1 e 1 81 q k t where q kt is the daily runoff from the land use k on day t nutrient loads from septic systems are included and calculated by estimating the per capita daily load from each type of system and the number of people in the watershed served by each type septic systems are classified according to four types normal systems ponding systems short circuiting systems and direct discharge systems the daily septic system loads are given by 29 d s t d s 1 t d s 2 t d s 3 t d s 4 t ds it i 1 4 are the dissolved nutrient load to streamflow from normal short circuited ponded and direct discharge systems respectively a normal septic system is a system which is designed constructed and operated by regulations epa design manual normal systems are generally located distance from stream and the phosphates in their effluents are adsorbed and retained in the soil therefore the normal systems will not provide phosphorous loads to streamflow the nitrogen loaded to groundwater on day is calculated 30 d s 1 t 0 001 a 1 t e u t where e is daily nutrient load in septic tank effluent per capita g day u t is daily nutrient uptake by plant per capita g day the short circuited systems are located close enough to surface waters roughly about 15 m the daily nutrient load is estimated by 31 d s 2 t 0 001 a 2 t e u t the ponded systems expose hydraulic failure of septic tank s absorption and resulting in nutrient loads to surface waters the daily nutrient load is estimated by 32 d s 3 t 0 001 p n t where pn t is daily watershed nutrient load in runoff from ponded systems on day t g 33 p n t f n t u t a 3 t e s n t 0 a n d t t 0 0 o t h e r w i s e in which fn t is frozen nutrient accumulation in ponded systems and under freezing condition it is iterated by 34 f n t 1 f n t a 3 t e s n t 0 a n d t t 0 0 o t h e r w i s e sn t g is the amount of snowpack at the beginning of day t t t is mean temperature on day t the direct discharged systems are illegal systems hence the septic tank discharges directly to the surface waters the daily nutrient load is calculated 35 d s 4 t 0 001 a 4 t e where a it i 1 4 in equations 25 26 28 and 30 are loads computed from per capita daily effluent loads for each system 2 3 channel and river routing calculation the channel and river routing component is based on hec ras model https www hec usace army mil software hec ras and included into the modeling platform as a routing plugin see fig 23 in section 3 1 2 the continuity and momentum equations for one dimensional model read as 36 a s t s t q x q l 0 37 q t v q x g a s z x s f 0 where q is total discharge v is mean velocity a s is flow area at a cross section s is storage from non conveying portions of cross section q l is lateral inflow per unit distance s f is friction slope defined by manning equation 38 s f q q n 2 2 208 r 4 3 a s 2 where r is hydraulic radius and n is manning coefficient z x is the water surface slope the transport process in channels rivers is modeled by the advection dispersion for each water quality constituent as shown below 39 a s c t a s v c x x a s d l c x s l where c is the solute concentration of a constituent gm 3 d l is longitudinal dispersion coefficient m 2 s 1 obtained from empirical formulas v is the mean flow velocity ms 1 s l is source sink rate of a constituent gm 3 s 1 hec ras can simulate various water quality components such as nitrogen phosphorous algae carbonaceous biochemical oxygen demand cbod and dissolved oxygen do sediment routing calculation is based on exner s equation known as the sediment continuity equation which translates the difference between inflowing and outflowing loads into bed change eroding and depositing sediment 40 1 λ p b η t q s x where b is channel width η is channel elevation λ p is active layer porosity and q s is sediment load sediment calculation is including three processes bed suspended and wash loads for detail information on the simulation of hydrodynamics sediment transport and water quality parameters the readers can refer to the hec ras reference manual https www hec usace army mil software hec ras documentation aspx the results output from runoff calculation such as daily streamflow hydrograph sediment and nutrient loads total n and total p are input into hec ras at upstream boundary and tributary inlet conditions fig 6 shows the main gui interaction window to run the modeling tool as mentioned above after clipping function done input files transport dat nutrient dat and weather dat are generated to input to the runoff processes calculation these files can be edited by clicking on the buttons edit transport file edit nutrient file and edit weather file respectively after preparing and checking all input data click on the button run snu ws the runoff calculation will be proceeded on the other hand the button exit snu ws is to terminate the simulation run daily and monthly output can press the buttons daily monthly output 2 4 calibration and uncertainty analysis methods in most hydrological models the errors and approximations in input data model parameters model structure and model algorithms are variety sources of uncertainties which cause substantial difficulties for calibration process and uncertainty estimation of a hydrological model therefore we have applied the generalized likelihood uncertainty estimation glue method to build a calibration and uncertainty analysis tool into the simulation tool the glue method was suggested by beven and binley 1992 it generally consists 4 steps including sampling parameters based on an appropriate definition of a prior distribution defining likelihood measure estimating of uncertainty interval using calculated likelihood and lastly calibrating and validating the model within the range of the uncertainty following is the description of these 4 steps 2 4 1 sampling parameter due to numerous uncertainty parameters in hydrologic models an appropriate parameter range has to be determined and a prior distribution of parameter set will be assumed based on model structure after setting prior distribution parameter values will be randomly sampled by running a number of simulations with different parameters within their appropriate parameter ranges the drawback of typical glue application is considerable consuming computational time due to random sampling technique to improve this limitation the latin hypercube sampling strategy lhs is applied because lhs can reduce significantly sampling times in comparison to random sampling based on the set of parameters obtained from sampling procedure the likelihood measure will be defined by different methods is shown in the following section 2 4 2 defining likelihood measure as mentioned by beven and binley 1992 the glue methodology requires the definition of the likelihood measure various goodness of fit indices suggested by beven and binley 1992 have been implemented in this simulation tool as shown following the likelihood measure is defined as a coefficient of determination 41 l 1 σ e 2 σ o 2 σ e 2 σ o 2 where σ e 2 is the variance of the residual and σ o 2 is the variance of observations l will take the value between 0 and 1 l 0 when σ e 2 σ o 2 and l 1 when σ e 2 0 at everywhere this is actually nash sutcliffe efficiency coefficient nse which is frequently used in hydrologic model evaluation most commonly in hydrologic time series as time scale goes down sudden enormous peak values appear frequently if model cannot capture the physics behind it the variance of residuals will be influenced enormously to mitigate this effect another likelihood definition based on l moment can be adopted as follows 42 l 1 λ 2 ε λ 2 o where λ 2 ε is the l scale of residual and λ 2 o is the l scale of observed values another likelihood measure is the inverse variance of the residual given by beven and binley 2001 43 l σ e 2 n where n is the shape factor determined by the user when n 0 every simulation will have the equal likelihood when n the single best simulation will have a rescaled likelihood of 1 while all other simulations will have a likelihood of 0 in most hydrologic model there are numerous types of outputs including discharge sediment yield sediment and nutrient loads etc whose data can be observed from multiple sites in this case we should combine the individual likelihood measures to make overall contributing likelihood weight for each simulation thus a combination of individual likelihood measures for the observations is suggested by dubois and prade 1980 and zimmermann 1991 where the global likelihood measures is based on fuzzy rules which can be a product geometric and arithmetic means and minimum or maximum likelihood measures as follows product likelihood measure 44 l j 1 m l j geometric mean likelihood measure 45 l j 1 m l j m arithmetic mean likelihood measure 46 l 1 m j 1 m ω j l j minimum likelihood measure set intersection 47 l min j 1 m l j maximum likelihood measure set union 48 l max j 1 m l j where l 1 l 2 l m are m individual likelihood measures after defining likelihood measure the next step of the glue methodology is going to find posterior parameter distributions whose range should be broad enough to ensure that the simulation results will be belonged to the range of observations this requires that the users have to choose an appropriate prior range of the parameters to be considered together with the form of the distribution function within that range the suitability of the chosen range may be evaluated by comparison of the predicted responses within that range if the prior knowledge about the appropriate values parameters is insufficient a uniform distribution function is maybe chosen the assumption of a uniform reference prior is unlikely critical because as long as information is added in terms of comparisons between observed and simulation responses for instance beven and binley 1992 has successfully applied uniform distributions for the gwy catchment plynlimon wales 2 4 3 estimating uncertainty interval as mentioned above there are many error sources in any simulation tools caused by poorly defined initial and boundary conditions input data observation data for calibration and validation model structure etc the likelihood weights of the glue procedure will reflect all sources of errors in the modeling process and estimate the uncertainties associated with those errors to be carried forward into the predictions in general the residuals between the observations and simulations are biased and auto correlated with heteroscedastic variance the result of applying the glue procedure is to provide a range of likelihood weight associated with the parameter set producing that residual sequence the aim of this step is to provide an estimate of uncertainty to be consistent with the model limitations caused by the error sources that allows a direct quantitative comparison between different model parameters in comparison the simulation results obtained in time series with the observations the uncertainty interval corresponding to a certain level could be calculated through estimating quantiles of a certain significance level for each time step eventually we can investigate if observation values reside inside or outside of this interval if the uncertainty limits are drawn too narrowly then a comparison with observations will suggest that the model structure is invalid on the other hand if the uncertainty limits are drawn too widely then it might be concluded that the model has little predictive capability in a well posed model with accurate observations the posterior likelihoods should become increasingly constrained as additional observations are considered thus the posterior likelihood distribution may be used directly to evaluate the uncertainty limits for future periods when the observation data may be not available or scarce in order to validate a model 2 4 4 model validation and prediction uncertainty interval obtained from previous step was applied only for calibration period because the likelihood measure was obtained from the period when the observation data are available a maximum likelihood determines the best fit between the simulation result and observation hence the calibration parameters are obtained from this simulation furthermore we can use likelihood measure calculated from the calibration period for the validation period after importing real observed values in this period we can check if this interval covers observation value or not if the validation succeeds we can repeat previous procedure to get another uncertainty interval for prediction in future periods when the observation data are not available fig 7 shows an interactive window to input data for calibration and uncertainty analysis processes based on suggestions of haith et al 1992 and li et al 2010 the parameters and their ranges for the calibration and analysis tool are set up the left column shows the parameters contributed to water balance processes such as cn values associated with different types of landuse landcover groundwater recession and seepage coefficients available water holding capacity of unsaturated zone the right column shows the parameters contributed to water quality components such as sediment nitrogen and phosphorous the button include from drop down menu is selected to input the value of parameter once the value of parameter is not available the button exclude can be selected then a model default value is set for the calibration and uncertainty analysis of water balance the parameters from transport file will be loaded while the parameters from nutrient file will be loaded when the calibration and uncertainty analysis process is carried out for water quality components as shown in fig 8 a performance of uncertainty analysis can be proceeded by selecting the button daily obs data or monthly obs data depending on selected timescales the observation files of streamflow and nutrient nitrogen phosphorus are imported to the system in order to prepare for the calibration and validation periods whereby the box prediction period is selected after finishing the calibration and validation processes this step is applied for the period when the observation data are not available such as future or climate change evaluations fig 9 shows different options for sampling method likelihood definition and quantile boundary finally simulation number should be given and significance level for uncertainty interval should be defined the default values of simulation number and prediction interval are 1000 and 95 2 5 97 5 respectively after defining these required information the uncertainty analysis will be proceeded 3 verifications and applications the snu ws platform is applied to different watersheds under distinguished weather conditions the spring creek watershed located in the center region of pennsylvania usa and the la buong river basins located on downstream of dong nai river in vietnam simulation results are not only verified against available hydrological models but also calibrated and validated against observation data 3 1 application to spring creek watershed pennsylvania the spring creek watershed fig 10 is located along the eastern edge of the appalachian mountains which extend from maine to georgia and is bordered by long high ridges on the north and south with a broad valley in between its area is about 93400 acres 378 km2 including about 27519 acres 29 of agriculture 24421 acres 26 of urban 35689 acres 38 of forestland and 5463 acres 6 of vacant reclaimed or mined lands and it is a home of 140 000 people annual average precipitation is close to 110 6 cm 40 inches data obtained for this simulation are from different sources as shown in table 4 spring creek main stream has 5 tributaries big hollow buffalo run slab cabin run cedar and run logan branch fig 10 right despite rapid population growth and enhanced wastewater flows nitrate levels in spring creek basin springs and streams are generally declining and remain well below drinking water standards of 10 ppm of nitrate nitrogen figs 2 and 3 in section 2 1 show all gis layers of the spring creek watershed uploaded into the gis mapwindow interface of snu ws tool the appearance on fig 3 shows the sub basins and stream system in this watershed area fig 11 and table 5 show the locations and magnitudes of point sources in the spring creek watershed 3 1 1 verification against the mapshed model as mentioned in section 1 introduction mapshed is customized by original gwlf evans et al 2002 https wikiwatershed org help model help mapshed our runoff calculation was also developed based on the modification of the formulas used in original gwlf see section 2 2 therefore we first have verified our runoff simulation results against the mapshed model s results because the mapshed model does not include routing calculation in order to do that we applied two models to the same spring creek watershed the runoff simulation was carried out for 13 years from 1988 to 2000 using the coefficient of determination r2 to compare the results between our runoff calculation and mapshed model it shows very good agreement for the most of components in water balance equation evapotranspiration streamflow groundwater flow runoff etc and nutrient loads total n and total p with r2 in the range of 0 908 0 997 as shown in table 6 however the difference of erosion and sediment between the runoff simulation results of snu ws platform and the mapshed model are occurred because we have revised the formulae for sediment yields suggested by schneider et al in our model as shown in equations 17 19 and 24 above it should be noticed that the streamflow at houserville station obtained from runoff calculation of snu ws platform without routing calculation and mapshed model shown in fig 12 is still away from the observation data the runoff processes from land surface were continuously carried on by the routing hec ras model in the application to the spring creek the main stretch from downstream of houserville station to the outlet of spring creek is shown in fig 13 whereby the main stretch of spring creek on upstream of houserville station the slap cabin and cedar runs are aggregated flowing to houserville station where the inlet boundary condition was setup three other tributaries such as the big hollow buffalo and logan branches are simplified as 3 other inlet boundaries to the main stretch at the confluences whereby the rate of streamflow hydrograph sediment and nutrient loads are set at the upstream of houserville station and at three inlet boundaries from big hollow buffalo and logan tributaries 3 1 2 verification against observation data the following calibration and validation are focused on the streamflow and nutrients at two stations the houserville upstream and axemann downstream stations see red points in fig 13 the simulation has been performed for daily and monthly time scales within the period from 1988 to 2010 including 2 warm up years 1988 1989 wherein the calibration and validation for streamflow are carried out for the periods of 1990 2000 and 2001 2010 respectively for uncertainty analysis we ran 1000 simulation runs and using lhs for sampling parameter process and the uncertainty bound between 5 and 95 90 prediction interval is selected 3 1 2 1 daily calibration and validation figs 14 and 15 show the calibration for the period from 1990 to 2000 and validation for the period from 2001 to 2010 of simulation results against the observations of the daily streamflow at houserville and axemann stations respectively the uncertainty analysis of daily streamflow between simulation and observation at houserville and axemann stations with the prediction for quantile of 5 and 95 are shown in figs 16 and 17 respectively 3 1 2 2 monthly calibration and validation similar to processes carried out for daily streamflow the calibration validation and uncertainty analysis for monthly streamflow at houserville and axemann stations are shown in figs 18 21 it shows very good agreement between the simulation results and observations the observation data of monthly nutrient loads total n and total p are sparse and available only at axemann station from 2002 to 2010 therefore the calibration and validation of monthly nutrient loads are implemented at axemann station within this period shown in figs 22 and 23 because total nitrogen tot n is the sum of nitrate no3 nitrite no2 organic nitrogen and ammonia fig 22 show tot n at axemann station located downstream of spring creek is below 6 mg l 6 ppm it is again to confirm that the concentration of nitrate nitrogen remain well below drinking water standards of 10 ppm as mentioned above table 7 shows the statistical criteria classified by moriasi et al 2015 see table a2 in appendix it shows that the simulation performance for the daily streamflow 0 7 r 2 0 85 a n d 0 7 n s e 0 8 is in the good range while the simulation performance for the monthly streamflow r 2 0 85 a n d n s e 0 8 and nutrient loads t n r 2 0 7 a n d n s e 0 7 and t p r 2 0 8 a n d n s e 0 65 are in a very good range fig 24 shows the bed form was changed within 1 month at two cross sections one section 19025 located just downstream of houserville station and another one section 2671 located just downstream of axemann station see fig 13 at the cross section 19025 a sedimentation process was occurred while the erosion process was happened at cross section 2671 since we do not have the data of sediment transport along the river stretch a comparison between the simulation results and observation cannot perform herein 3 2 application to la buong river basin vietnam la buong river which has a length of 56 km and an annual discharge of 412 106 m3 is a tributary of dong nai river and located in downstream dong nai river the la buong river basin fig 25 has a total drainage area of 478 5 km2 with the stream density of 0 67 km km2 and an annual average precipitation and temperature of 1800 mm and 25 26 c respectively due to the relatively flat terrain and influenced by the tropical monsoon climate this area is divided into two main seasons the dry season november april and rainy season may october moreover the morphology of la buong river basin is leaf shaped the hydrology is heavy rainfall area causing flash flooding problems in the center and upper regions while the lower part is effected by inundation and tide this area has relatively fertilized land in which about 75 of the catchment area are covered by rhodic ferralsols and ferric acrisols and consistent with agricultural development the cultivated land occupies more than 80 of the total area in comparison to the spring watershed above the available input data for the la buong river basin are limited only four gis layers including topography dem land use soil and weather sole station a digital elevation model dem with a resolution of 12 5 m was downloaded from the alaska satellite facility asf distributed active archive center daac to delineate the whole basin and its sub basins land use data with a spatial resolution of 1 km were collected from the sub national institute of agricultural planning and projection of vietnam sub niapp soil data with a spatial resolution of 10 km were obtained from the food and agricultural organization fao of the united nations meteorological data from 1981 to 2015 at three stations were collected from the hydro meteorological data center of vietnam hmdc furthermore only monthly observed streamflow data are existed from 1983 to 1993 at the la buong station and the sparse nutrient tot n and tot p and sediment data at the station sw sbu 01 from 2010 to 2015 and at the station sw sbu 06 from 2012 to 2015 were collected by the department of natural resources and environment of dong nai province for model calibration and validation processes shown in fig 25 in this application the snu ws platform and swat are used in order to verify the performance of our platform against swat model the objective of this comparison is not to demonstrate that our modeling platform is better than swat because the swat is very well known and widely applicable for more than 20 years however this comparison is aimed to testify how two models can perform once the input data are limited as in this case study the swat applied an external calibration and uncertainty tool the swat calibration and uncertainty program swat cup arnold et al 2012 more information about the calibration process of swat cup for the la buong river basin can find in our previous publication khoi et al 2019 figs 26 and 27 show daily and monthly calibration 1983 1988 and validation 1989 1993 of the streamflow at la buong station see station location in fig 25 herewith the comparisons of daily and monthly streamflow between the observation data with the simulation results obtained from snu ws and swat are presented as well figs 28 and 29 show the prediction for quantiles of 5 95 of daily and monthly streamflow between the simulation results obtained from snu ws platform and the observation figs 30 32 show the calibration 2010 2012 and validation 2013 2015 of simulation results obtained from snu ws platform and swat model against the observation data for tot n tot p and sediment loads at station sw sbu 01 figs 33 35 show the calibration 2012 2013 and validation 2014 2015 of simulation results obtained from snu ws platform and swat model against the observation data for tot n tot p and sediment loads at station sw sbu 06 a comparison of model evaluation criteria between the simulation results obtained from our snu ws platform and swat model with the observation data is shown in table 8 based on the values of the statistical criteria r2 and nse see table a2 in appendix the model performance is marked in red in table 8 it shows that the snu ws performed better than swat for both daily and monthly streamflow whereby the snu ws is classified into very good performance r2 0 85 nse 0 85 while the swat model is classified into satisfactory performance 0 5 r2 0 7 0 5 nse 0 7 for daily streamflow and into satisfactory performance 0 7 r 2 0 8 0 7 n s e 0 85 for monthly streamflow within calibration period and into good performance 0 85 r 2 0 7 n s e 0 85 within validation period similar trends for the simulation of nutrient and sediment loads it again shows that the results obtained from snu ws are overall better than those results obtained from swat in comparison with the observation 4 conclusions and further development as shown above we have developed a new integrated modeling platform for watershed modeling including pre processing tool a coupling of hydrologic and a channel river transport models and a calibration and uncertainty analysis tool all components are connected and immersed into an open source gis mapwindow and equipped with gui system contrastively the modeling platform was applied to two distinguished watersheds under different weather conditions and different levels of available data for input calibration and validation the spring creek watershed pennsylvania usa and the la buong watershed vietnam while the spring creek watershed located in the temperate zone has more readily available data the la buong watershed located in tropical wet dry zone has less available data in the application to the spring creek watershed the verification of our modeling platform against the mapshed has showed significant improvements in our snu ws platform additionally as shown in the la buong watershed s application with a minimum input data the comparison of the simulation outcomes from our modeling platform and the swat model with the observation data indicates that the performance of our modeling platform is overall better furthermore the simulation results were calibrated and validated against the observation data a good agreement between model results and observation demonstrates the capability of our simulation tool therefore it can be used as a simulation tool for watershed management however the modeling platform should be further applied to different watersheds under various conditions and intensively calibrated and validated against observation data moreover it should state that our modeling platform still has some limitations such as the snowmelt calculation based on temperature index only the groundwater discharge and deep seepage calculations based on lumped parameter linear reservoir model of which they need to be enhanced in the future version we also plan to release the snu ws platform to the public domain by which we can receive the feedback from the users in order to further improve the simulation platform declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement the authors would like to thank to the support from the national research foundation of korea grant number nrf 2018r1d1a1a09083747 the authors also would like to thank the anonymous reviewers for their valuable and constructive comments to improve our manuscript appendix a table a1 values of soil erodibility factor k k control of water pollution from cropland usda 1975 table a1 texture class organic matter content 0 5 2 4 sand 0 05 0 03 0 02 fine sand 0 16 0 14 0 10 very fine sand 0 42 0 36 0 28 loamy sand 0 12 0 10 0 08 loamy fine sand 0 24 0 20 0 16 loamy very fine sand 0 44 0 38 0 30 sandy loam 0 27 0 24 0 19 fine sandy loam 0 35 0 30 0 24 very fine sandy loam 0 47 0 41 0 33 loam 0 38 0 34 0 29 silt loam 0 48 0 42 0 33 silt 0 60 0 52 0 42 sandy clay loam 0 27 0 25 0 21 clay loam 0 28 0 25 0 21 silty clay loam 0 37 0 32 0 26 sandy clay 0 14 0 13 0 12 silty clay 0 25 0 23 0 19 clay 0 13 0 29 table a2 the statistical criteria r 2 and n s e measured the performance evaluation of the watershed models moriasi et al 2015 table a2 measure component temporal scale very good good satisfactory not satisfactory r 2 flow annual 0 75 0 70 r2 0 75 0 60 r2 0 70 0 60 monthly 0 85 0 80 r2 0 85 0 70 r2 0 80 0 70 daily 0 85 0 70 r2 0 85 0 50 r2 0 70 0 50 sediment annual monthly 0 8 0 65 r2 0 80 0 40 r2 0 65 0 40 daily n annual monthly 0 7 0 60 r2 0 70 0 30 r2 0 60 0 30 daily p annual monthly 0 8 0 65 r2 0 80 0 40 r2 0 65 0 40 daily general 0 8 0 70 r2 0 80 0 50 r2 0 70 0 50 nse flow annual 0 75 0 60 nse 0 75 0 50 nse 0 60 0 50 monthly 0 85 0 70 nse 0 85 0 55 nse 0 70 0 55 daily 0 80 0 70 nse 0 80 0 50 nse 0 70 0 50 sediment annual monthly 0 80 0 70 nse 0 80 0 45 nse 0 70 0 45 daily n annual monthly 0 70 0 60 nse 0 70 0 35 nse 0 60 0 35 daily 0 55 0 40 nse 0 55 0 25 nse 0 40 0 25 p annual 0 65 0 60 nse 0 65 0 50 nse 0 60 0 50 monthly 0 65 0 50 nse 0 65 0 40 nse 0 50 0 40 daily the coefficient of determination r 2 and nash sutcliffe efficiency coefficient nse are calculated by the following equations a2 1 r 2 i 1 n o i o p i p i 1 n o i o 2 0 5 i 1 n p i p 2 0 5 2 a2 2 n s e 1 i 1 n o i p i 2 i 1 n o i o 2 where p i and o i are the simulated and observed values p and o are the mean of the simulated and observed values and n is the total number of observations 
