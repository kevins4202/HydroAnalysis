index,text
25655,recent advances in sensor and iot technologies allow for denser and mobile air quality measurements these measurements are still spatiotemporally sparse at city level but can be interpolated using data driven techniques this work presents validation results of two machine learning models to infer air quality sensor data in both space and time temporal validation exercises are performed at available regulatory monitoring stations following the fairmode protocol both models show scalable to different mobile datasets with comparable prediction performance for pm2 5 r2 0 68 0 75 mae 2 99 2 82 μg m 3 and no2 r2 0 8 0 82 mae 8 81 9 83 μg m 3 in utrecht and antwerp in oakland atlanta we observed a lower performance for no2 r2 0 46 0 41 mae 4 06 5 07 and bc r2 0 31 0 28 mae 0 48 0 27 likely caused by the less representative monitoring coverage although comparable in terms of prediction performance the geographical random forest grf model seems to achieve slightly better accuracies while the correlations are typically higher for the air variational graph autoencoder avgae model this work demonstrates the potential of data driven techniques for spatiotemporal air quality inference of complementary sensor data the observed performance metrics approach current state of the art chemical transport models in terms of performance while needing much lower resources computational power infrastructure and processing time keywords iot urban air quality mobile sensors machine learning 1 introduction air pollution continues to have significant health impacts especially in urban areas in terms of human health impact particulate matter pm no2 and ground level ozone o3 are considered as the most important pollutants eea 2020 who 2020 according to the european environmental agency eea 77 of the european urban population is exposed to fine particulate matter pm2 5 concentrations exceeding the old world health organisation who guideline values now much more stringent from 15 to 5 μg m³ annually while only 1 of the estimated urban population lives in areas exceeding the eu limit value eea 2020 who 2021 the same holds true for no2 for which we almost met the 40 μg m 3 annual limit value in 2019 estimate 3 of the population in exceedance eea 2020 but who recently recommends a much more stringent guideline value of 10 μg m 3 who 2021 pm is associated with airway irritation cardiovascular and respiratory diseases e g lung cancer and chronic obstructive pulmonary disease copd and impacts on central nervous and reproductive systems no2 is linked with secondary pm2 5 formation airway irritation breathing problems e g bronchitis lung function and impacts on liver spleen and blood black carbon bc is associated with cardiovascular mortality chronic risk of developing cancer janssen et al 2011 2012 or acute cardiovascular effects like lung function laeremans et al 2018a 2018b or carotid arterial stiffening provost et al 2016 as emerging pollutant pm constituent bc is recently introduced in the who guidelines as a set of good practices on systematic measurements emission inventories exposure assessments source apportionment and emission reductions who 2021 especially in urban environments where both pollution sources and people affected by pollution are concentrated air pollution tends to peak as pollution levels can vary dramatically over short distances or time instances hofman et al 2018 int panis et al 2010 kumar et al 2018 pattinson et al 2014 peters et al 2014 pirjola et al 2012 a high monitoring resolution in both space and time should be pursued to accurately estimate population exposure since traditional air quality monitoring stations are rather costly and cumbersome cities typically only deploy few at representative locations e g roadside urban background in order to better assess people s exposure to air pollution there is an urgent need for higher monitoring granularity thanks to rapid advances in sensor and internet of things iot technologies cities can now collect data from a wide range of static and mobile sensors in response to the need for higher monitoring granularity recent mobile sensing networks have been collecting data on routine service fleets examples include postal vans in antwerp be qin et al 2021 garbage trucks in cambridge massachusetts us desouza et al 2020 and trams and buses in lausanne and zurich ch mueller et al 2016 other examples also include personal monitors on bicycles franco et al 2016 hofman et al 2018 n genikomsakis et al 2018 peters et al 2014 qiu et al 2019 and city wardens van den bossche et al 2016 these networks provide valuable in situ data on experienced exposure levels throughout the city nevertheless the collected data points are still sparse in time and space and need interpolation in order to be useable for air quality mapping and fine grained exposure assessments inferring spatiotemporal point measurements to derive highly granular air quality based on real time sensor measurements is vital for policy makers and researchers to detect pollution events evaluate local vs regional source contributions evaluate policy measures in situ and improve existing chemical transport models ctm with experimental data this works anticipated to demonstrate the potential of data driven techniques for air quality data inference in space and time by feeding two machine learning models with mobile data collected by mobile sensor networks and temporally validate predicted pollutant levels at available regulatory monitoring locations 2 material methods 2 1 sensor testbeds urban environments rely on in situ measurements as experimental evidence and starting point for any representative air quality assessment in this regard mobile sensor networks complement existing regulatory networks in providing unprecedented spatial granularity to evaluate the scalability of the proposed machine learning models we considered multiple sensor testbeds on different mobile platforms in the netherlands belgium and the united states fig 1 2 1 1 utrecht the netherlands the snuffelfiets project aims at gathering mobile air quality data with bicycles in utrecht the netherlands https snuffelfiets nl utrecht is a medium sized 99 km2 3 812 inhabitants km2 city with a temperate climate located along the rhine river in the center of the netherlands mobile data includes ambient particulate matter pm geographical coordinates road surface quality temperature relative humidity and volatile organic compounds vocs and is collected opportunistically at a 10 s resolution in utrecht the netherlands using mobile sensors developed by sodaq http sodaq com sodaq snifferbike data are transmitted using lte m and collected in a cloud based data platform developed by civity https civity nl en our study focused on the particulate matter measurements pm1 pm2 5 and pm10 quantified by an onboard sps30 sensirion sensor the national institute for public health and the environment rivm compared raw snuffelfiets data to regulatory data resulting in standard deviations of the differences between 4 and 6 μg m 3 wesseling 2020 a mean bias of 1 2 μg m 3 was obtained while 95 of the sensor data lies within 4 μg m 3 of the official data 95 ci of 8 μg m 3 wesseling et al 2021 rivm cleaned pm2 5 outliers with unrealistic cycling speeds 45 km h 1 and calibrated the pm2 5 data based on a mass factor derived from the relationship between sensor and reference data average reference average sensor at three regulatory stations after calibration sensor data are within 3 μg m 3 of the official data 95 ci of 6 μg m 3 and show a mean bias of 0 3 μg m 3 wesseling et al 2021 this calibrated pm2 5 data was subsequently used to feed our air quality inference models see section 2 2 snuffelfiets data for june 2020 was downloaded from https dataplatform nl data 3660d2e1 84ee 46bf a7b6 7e9ac1fcaf3a while the regulatory data of monitoring stations nl10636 and nl10643 was collected via an open api https api docs luchtmeetnet nl model performances are evaluated on one month june 2020 of mobile pm2 5 data 2 1 2 antwerp belgium within the bel air project the belgian postal company bpost collaborated with the interuniversity microelectronics center imec deploying mobile air quality sensors kunak air mobile on 20 postal vans for opportunistic air quality monitoring in antwerp https www imeccityofthings be en projecten bel air kunak air mobile sensors consist of alphasense sensors for pm1 pm2 5 and pm10 opc n3 nitrogen dioxide no2 no2 b43f and ozone o3 ox b431 temperature relative humidity and gps are powered via the car battery 12 v 100 ma and are equipped with lte m connectivity moreover they include a dedicated mobile housing developed by labaqua to protect the sensors against turbulent air flows when mobile the sensors contain a property algorithm to correct for temp rh influence and gas cross interference and had an initial factory calibration at cuenca spain before deployment imec calibrated the sensors locally no2 baseline span pm linear slope calibration based on 2 weeks of co location data 1 week calibration 1 week validation reaching overall good sensor performance with r2 of 0 82 0 89 mae of 2 2 8 24 and expanded uncertainty wcm of 25 64 following the eu equivalence tool v3 1 jrc 2010 based on consecutive co location campaigns of 4 batches of 5 mobile sensors 20 in total on top of a local regulatory monitoring station 42r817 antwerp be calibration resulted in mean sensor uncertainties of 7 7 87 12 94 for batch 1 19 14 11 28 45 for batch 2 11 15 4 to 3 6 for batch 3 and 17 3 55 33 97 for batch 4 not published the postal vans mainly operated during daytime hours on weekdays and saturday see appendix 1 the monitoring resolution was configured to 10 s during the day when mobile and 10 min overnight when static parked to avoid car battery drainage as urban environments typically exhibit high spatial variability for traffic related pollutants like no2 degraeuwe et al 2015 lewné et al 2004 munir et al 2021 nicholas hewitt 1991 we applied the proposed models see section 2 2 on the no2 sensor data temporal validation was performed for one month march 2021 at 5 regulatory monitoring locations in antwerp located within different microenvironments namely urban background r801 park r803 roadside r802 street canyon r805 and highway r804 we refer to appendix 2 for further details 2 1 3 oakland united states within oakland two google street view vehicles equipped with aclima instrumentation apte et al 2017 solomon et al 2020 collected mobile air quality measurements from may 28 2015 to june 7 2019 atmospheric concentrations of o3 no2 nitrogen oxide no methane ch4 carbon dioxide co2 black carbon bc and ultrafine particles ufp were collected on weekdays during office hours 9 17 h at a 1 hz resolution this mobile data was made available via the web application for google maps air quality data access 2021 in the considered study area two regulatory monitoring stations california air monitoring network https www arb ca gov aqmis2 aqdselect php were available oakland west and oakland laney providing air quality data on o3 no2 pm2 5 and bc again we opted for pollutants with exhibiting highest spatial variability in this case no2 and bc no2 was measured using 450 nm cavity attenuation phase shift spectroscopy model t500u teledyne inc san diego ca while black carbon bc particles were measured using a photoacoustic extinction meter droplet measurement technologies boulder co the inlet system was designed to minimize self sampling and particle sampling losses weekly zero span calibrations resulted in 3 and 6 for the two cars no2 instruments and 25 for bc apte et al 2017 our inference models were applied on one month of mobile data may 2019 2 2 data inference in space and time mounting air quality sensors on mobile platforms enables in situ opportunistic collection of measurements with a high spatial coverage the collected data is valuable as it is collected dynamically at the roadside reflecting the actual cyclist exposure mobile monitoring studies have revealed large spatiotemporal variability in pollutant exposure before impacted by various factors i e road traffic meteorology street topology and background emission dynamics not represented by measurements derived from nearest regulatory monitoring stations gelb and apparicio 2020 hofman et al 2018 peters et al 2014 qiu et al 2019 nevertheless these collected data points are still sparse fig 2 left and require spatiotemporal interpolation fig 2 right in order to generalize air quality assessments i e to disentangle space and time variant features impacting your mobile measurements when calculating a yearly averaged air quality map of a city for example completed time series are needed at every location to derive representative averaged pollutant concentrations focusing on regulatory air quality data from antwerp belgium it can be observed that pollutant concentrations temporally correlate at different monitoring locations fig 3 due to the observed correlation over both space and time data matrices of air quality data can be considered low rank and thus explainable by statistical numerical techniques asif et al 2016 udell and townsend 2019 the underlying low rank and slowly time varying structure of the air quality data can be leveraged to create numerical models that facilitate an effective spatiotemporal extrapolation paliwal et al 2020 machine learning ml approaches allow for training of underlying dependencies based on large air quality datasets and supplied context information traffic meteorology street type speed limit hereby enabling data inference or matrix completion in both space and time fig 2 to this end we recently developed two machine learning models do et al 2020 qin et al 2021 trained on mobile air quality sensors mounted on postal vans in antwerp belgium compared against existing spatial interpolation techniques do et al 2020 qin et al 2021 and validated against regulatory data at four different monitoring stations hofman et al 2020 2 2 1 avgae model the avgae model is a deep learning model based on variational graph autoencoders vgae incorporating spatial correlation of measurements by considering the road network graph while training time variant dependencies based on additional context information points of interest poi road type meteorology street canyon index do et al 2019 2020 for the utrecht data considered context data included road network road type distance to road segment meteorological data relative humidity temperature and wind speed hourly background pollution derived from nl10644 cabauw wielsekade station and poi crossing gas station traffic light bus station taxi rank as the measurement time and location are continuous the measurements are first aggregated at hourly intervals median along road segment locations hence the aggregation across space is nonuniform and is adapted to the considered locations on the road network the aggregated discretized measurements result in an incomplete measurement matrix needing completion fig 2 the graph is subsequently constructed according to the geodesic distance between the considered locations along the road network connecting nodes when the geodesic distance is smaller than a predefined threshold δ or if they belong to the same road segment the weight of a connection is set equal to the inverse of the geodesic distance do et al 2020 matrix completion is solved via a neural network model accepting the concatenated measurement matrix x and geocoordinate matrix s as input learning the underlying graph dependencies through stacked graph convolutional gcn layers three stacked gcn layers with the same dimensionality i e d 512 are used for the encoder and one gcn layer for the decoder for model training the kl divergence coefficient was set to β 0 1 the temporal smoothness coefficient to γ 0 8 and the temporal neighborhood width to wt 3 moreover we used a learning rate of α 0 005 and dropout rate of 0 4 the loss function was evaluated on the model estimation of known entries from mobile and reference station measurements mae space variant features are horizontally added to the measurement matrix on the other hand the time variant features are arranged in a matrix multiplied by a weight vector after applying a tiling operation we obtain an adjustment matrix from the time variant features this adjustment matrix is added to output of the avgae model the measurement aspect refers to the spatiotemporal correlation that exists internally in the measurements the internal spatial correlation in this case is incorporated via gcn layers leveraging the road network the graph the internal temporal correlation is considered by adding an extra term in the loss function do et al 2019 the accuracy based on rmse mae of the avgae model has shown to outperform other state of the art inference models i e linear and exponential kriging knn based collaborative filtering svd nmf nmc and rgcn as shown in do et al 2020 more information and mathematical description of the avgae model can be found in do et al 2020 while the model architecture is visualized in fig 4 2 2 2 grf model the geographical random forest grf model captures the interaction between air pollution and various context features by a series of flexible random forest models as traditional random forest does not consider spatial heterogeneity between measurements only indirectly through context features we build a local rf model by considering k neighbors qin et al 2021 it adopts the idea of geographically weighted regression gwr moving a spatially weighted window over the observations while being coupled with a flexible non linear model which is very hard to overfit due to its bootstrapping nature first a map matching step is performed to map mobile measurements to nearest street segments followed by hourly aggregation of mobile measurements at centroid points of the street segments next the grf model makes use of all training measurements and context features to build a global model while a local rf model is fitted for each location only considering k nearest measurements here we consider not only neighbors that are close in geographical distance but also neighbors with similar context features finally a weighting mechanism is utilized to combine global estimations weight 0 8 with local estimations weight 0 2 200 trees to obtain the final predictions the grf model included context information namely road network road type distance to road segment meteorological data relative humidity temperature and wind speed and hourly background pollution derived from a regulatory regional background station the grf model is explained in more detail in qin et al 2020 2021 while the architecture is visualized in fig 4 the predictive power of both models was tested with different types of context information on antwerp and utrecht data with the idea of scalability in mind we finally selected context features that improve the predictive power of the analysis performance as defined in section 2 3 but are also openly available table 1 2 3 model performance following an earlier model validation exercise on the utrecht pm2 5 data hofman et al 2021a this work explores the generalizability of the avgae and grf approach on new mobile datasets in antwerp be and oakland us complementing hofman et al 2021 we now focus on different pollutants no2 bc and on different mobile collection platforms multiple postal vans or two google street view vehicles instead of bicycles 2 3 1 model training both ml models avgae and grf are trained on the different mobile datasets march 2021 in antwerp be june 2020 in utrecht nl and may 2019 in oakland us together with available context information on road type meteorology and background pollution and subsequently applied to infer air quality measurements in space and time fig 5 it is worth noting that the hyper parameters of the avgae and grf models are tuned for individual datasets and pollutants since the distributions of measurements vary across the considered pollutants and datasets the training procedure for the avgae model is performed using stochastic gradient descent with the adam optimization algorithm kingma and ba 2014 as the updating rule in this training procedure aggregate subsets of both mobile and fixed reference station measurements were used to address over fitting the avgae model uses both dropout srivastava et al 2021 and weight decay the avgae model is implemented using tensorflow abadi et al 2016 the grf model performs training process using random forest regression geurts et al 2006 with the kmeans algorithm pelleg and moore 1999 as clustering method 2 3 2 model validation temporal model validation of both considered models avgae grf was performed through an extensive set of performance statistics mae mbe rmse ia corr nmb nmsd and by following the fairmode guidance document on modelling quality objectives and benchmarking to derive the model quality indicator mqi janssen et al kushta et al 2019 for each reference station location hourly pollutant concentrations for the considered month were predicted by the considered models avgae and grf by inferring the available mobile sensor data and regulatory measurements of other reference stations the hourly predicted concentrations were subsequently compared to the regulatory measurements to calculate the performance metrics and time series graphs this temporal model validation exercise was repeated for every available reference station in the mobile monitoring domain of the considered cities utrecht antwerp and atlanta oakland in order to explore the spatial prediction performance for each validation exercise aggregate subsets of both mobile and available reference station data have been used for model training and prediction besides the reference station location used for validation this procedure was subsequently iterated for each available reference station 2 3 3 performance metrics the performance metrics are calculated based on hourly model predictions and associated reference data for each of the considered regulatory monitoring locations see section 2 2 the performance metrics included mean absolute error mae root mean squared error rmse index of agreement ia accuracy pearson correlation normalized mean bias nmb normalized mean squared deviation nmsd and the model quality indicator mqi and model performance criteria for bias mpcbias correlation mpccorr and standard deviation mpcstdev as defined by the fairmode guidance document janssen et al kushta et al 2019 1 r m s e 1 n i 1 n o i m i 2 2 m a e 1 n i 1 n o i m i 2 3 i a 1 i 1 n o i m i 2 i 1 n m i o o i o 2 4 r i 1 n m i m o i o i 1 n m i m 2 i 1 n o i o 2 5 a c c u r a c y 1 i 1 n m i o i i 1 n o i 6 n m b m o o 7 n m s d σ m σ o σ o w h e r e σ m 1 n i 1 n e i e 2 where m i represents the predicted pollutant concentration o i represents the observed concentration at time instant i and m and o are the mean concentrations rmse eq 1 and mae eq 2 are accuracy metrics representing the difference between predicted and observed concentrations where rmse is more sensitive to outliers the smaller the better ia eq 3 and accuracy eq 5 represent the similitude between predictions and observations and range between 0 and 1 where 1 represents perfect agreement r eq 4 represents the association correlation between model predictions and observations and ranges between 1 and 1 0 means no correlation where 1 or 1 represents perfect linear correlation between predicted and observed pollutant concentrations the ia and accuracy are preferred over r as they are independent of the relationships between the variables whereas r only refers to linear relationships qin et al 2021 but reported here as well for comparison purposes with other model validation studies nmb eq 6 represents a normalized bias metric showing the general model under or overestimation of the observations finally the nmsd score eq 7 shows how the standard deviations of the model predictions compare with the observed standard deviations a python script to calculate the performance metrics is provided as supplementary material 3 results discussion 3 1 utrecht nl the snuffelfiets dataset of pm2 5 for june 2020 consisted of 928047 measurements was clipped to the utrecht area 5 0596 52 0568 5 1636 52 1242 temporally aggregated to hourly values at 8262 locations road network and resulted in a known entry rate of 0 95 of the spatiotemporal matrix for june 2020 the measured pm2 5 concentration range was 1 0 4 07 3 0 11 0 μg m ³ min mean median max from the avgae and grf hourly predicted pm2 5 concentrations at each of the regulatory monitoring stations and the associated regulatory data we calculated the resulting performance metrics shown in table 2 both models perform well in terms of uncertainty mae 2 83 3 14 avgae and 2 74 2 88 grf and correlation r 0 63 0 72 avgae and 0 73 0 77 grf with grf slightly outperforming avgae the observed mae table 2 is in line with the reported sensor uncertainty of 3 μg m ³ 95 ci of 6 μg m ³ while the mean bias mbe is slightly higher 1 05 1 99 μg m ³ when compared to the mean calibrated sensor bias 0 3 μg m ³ in wesseling et al 2021 associated model performance criteria mpc for bias correlation and standard deviation are all within the bounds set by the fairmode protocol not shown residuals of hourly modelled concentrations showed normal distributions moreover as the model quality indicator mqi is 1 table 2 fairmode model quality guidelines state that the model can be considered suitable for policy purposes janssen et al we should however note that current applications of inference models are confined to spatial air quality assessments while forecasting scenario modelling are additional features of chemical transport models ctm fig 6 shows time series graphs of the avgae and grf predicted pm2 5 concentrations it illustrates that predicted pm2 5 concentrations mi agree reasonably well with the reported reference data oi for each of the considered models at each location mostly falling within the uncertainty bounds of the reference equipment rmsu provided by fairmode and the mqi limits defined by fairmode as twice k 2 the reference measurement uncertainty janssen et al as shown in fig 6 and table 2 from the inferred monthly averaged street segment maps of utrecht fig 7 spatial variation can be observed with common locations with elevated pm concentrations not only along major traffic axes but also in certain inner city areas e g nw of map nonetheless the observed spatial concentration variability is much lower for pm than for no2 3 2 or bc 3 3 which can be explained by the more various and widespread sources of pm and regional and cross boundary source attributions hofman et al 2016 kelly and fussell 2012 keuken et al 2013 kumar et al 2018 rivas et al 2020 3 2 antwerp be the antwerp no2 dataset for march 2021 consisted of 323691 10 s and 10 min no2 measurements was clipped to the antwerp area 4 2170 51 1430 4 4980 51 3780 and temporally aggregated to hourly values at 23569 street segment locations for march 2021 fig 8 the measured no2 concentration range was 0 29 15 24 48 642 47 μg m ³ min mean median max when comparing the avgae and grf hourly predicted no2 concentrations at each regulatory monitoring station location against the associated regulatory data we calculated the resulting performance metrics shown in table 3 in general both models seem to perform well on the new dataset table 3 with avgae showing slightly better prediction accuracy mae mbe rmse and acc while grf shows better prediction association correlation ia r again the observed model bias 31 to 31 is in line with the observed sensor bias during the co location campaign 15 to 34 the model quality indicator mqi falls below one indicating that both models perform as required for policy purposes comparing the performance metrics at the different microenvironments reference stations higher errors and lower correlations higher model quality indictors are observed at the r803 park and r804 highway ring road regulatory stations this is interesting as these environments can be considered as less representative for the inner city of antwerp and therefore less frequently sampled by the mobile sensors the highest performance is observed at the r801 station which can be considered as an urban background location at 30 m from a traffic intensive road in the inner city at this r801 location located next to street segments with a high monitoring coverage 148 passages during june 2021 as visualized in appendix 3 we observe best prediction performance table 3 for both models r 0 9 and mae 4 52 for avgae r 0 94 and mae 6 03 fig 9 shows time series graphs of the avgae and grf predicted no2 concentrations mi against the reported regulatory concentrations oi predicted no2 concentrations mi agree reasonably well with the reported reference data oi for each of the considered models at each location mostly falling within the uncertainty bounds of the reference equipment rmsu provided by fairmode and the mqi limits defined by fairmode as twice k 2 the reference measurement uncertainty janssen et al the models tend to underestimate the actual no2 concentration at the least performing highway location i e high concentration station r804 nmb 0 31 for avgae and 0 13 for grf and overestimate at an urban background location i e the low concentration station r803 nmb 0 02 for avgae and 0 31 for grf the best and least performing station are provided in fig 9 while time series graphs of remaining stations r802 r803 r805 are provided in appendix 4 a model validation exercise of the current physical air quality model applied for policy making in flanders belgium atmostreet was performed at similar antwerp monitoring stations r803 r804 r805 r817 and yielded following performance results for no2 r 0 73 0 9 rmse 8 29 18 93 and mbe 5 61 0 94 irceline 2017 lefebvre et al 2013 comparing the observed performance of our models table 3 against the atmostreet model the considered data driven techniques seem to approach the state of the art in terms of performance the physical atmostreet model which is currently used for policy purposes in belgium integrates multiple spatial scales regional local using a rio ifdm ospm modelling chain with the rio module spatially interpolating regulatory measurements ifdm bi gaussian dispersion model simulating emission plumes from known point and line emission sources and ospm being a street canyon module implementing the effect of urban topology street canyon effect on the resulting pollutant dispersion irceline 2017 lefebvre et al 2013 3 3 oakland us the may 2019 data which consists of 351018 no2 and 309216 bc 1 s measurements collected during working hours 9 17 h was clipped to the oakland area 37 7482 122 3791 37 8768 122 1860 and temporally aggregated to hourly values at 3207 no2 and 2838 bc unique street segment locations the measured no2 concentration range was 0 45 12 50 10 39 117 02 μg m 3 min mean median max while bc ranged between 0 0 99 1 43 55 78 μg m 3 applied context factors were road network road type speed limit meteorological data relative humidity temperature atmospheric pressure wind speed wind direction and background pollution derived from regulatory station 07436 aquatic park when comparing avgae to grf in oakland the performance metrics listed in table 4 again show better accuracy mae mbe rmse and acc for avgae and better association correlation ia r for grf a slightly better performance is observed for no2 compared to bc probably due to the typical high spatial variability of bc when compared to no2 hofman et al 2016 2018 this time a higher mean bias was observed 41 72 for no2 and 47 156 for bc when compared to the initial sensor bias 3 6 for no2 and 25 for bc in apte et al 2017 fig 10 plots the avgae and grf predicted no2 and bc concentrations mi against the reported concentrations by the monitoring stations it shows that the models perform much worse compared to the predictions in utrecht nl and antwerp be predictions sometimes fall outside of the uncertainty bounds of the reference equipment rmsu but still within the mqi limits 2 x rmsu defined by fairmode janssen et al although the performance metrics in table 4 show a better prediction accuracy for no2 mae 2 9 5 22 μg m 3 when compared to the antwerp no2 predictions mae 4 52 16 79 μg m 3 normalized error metrics nmb are much worse and significantly lower correlation between sensor and reference data is observed in oakland r2 0 34 0 57 when compared to antwerp r2 0 59 0 94 similarly for bc low absolute error mae 0 2 0 48 but high nmb 47 156 and low correlations r2 0 26 0 32 are observed in the predicted concentrations at the reference station locations table 4 we hypothesize that the mean prediction accuracy is likely impacted by the lower sensor uncertainty of the oakland no2 instruments compared to antwerp sensors while the observed correlations dynamics will be impacted by the spatial representativity of the collected mobile sensor data antwerp oakland fig 11 left panel shows all collected and hourly aggregated mobile no2 measurements n 7516 and grf inferred data within the considered time period may 2019 it can be observed that the sampling points are not really representative for the entire oakland domain but rather concentrated along main access roads fig 11 left panel the resulting inferred predictions fig 11 right panel show rather low variability in areas that are sparsely or not at all covered by the google cars illustrating the need for a representative spatial monitoring coverage in this regard fleet measurements with multiple moving sensors e g 17 service fleet vehicles in antwerp and 500 bicycles in utrecht provide a higher spatiotemporal monitoring coverage and seem to result in better prediction performance than only two google cars the importance of monitoring coverage was demonstrated before by evaluating the correlation and bias of repeated mobile subsamples along mobile trajectories subsamples against the average concentration of the full dataset resulting in 24 94 repeated runs 10th and 90th percentile median of 41 on 50 m segments to reach a deviation of 25 or 5 11 repeats to reach a deviation of 50 van den bossche 2016 and about 25 repeated runs to reach an r2 0 9 and cv rmse 0 5 apte et al 2017 we therefore hypothesize that the lower prediction performance in oakland is due to the lower monitoring coverage in both time and space resulting in less representative data for model training ultimately the prediction performance of inference models will not only depend on the quality of the models but also on the quality of the provided input data air quality sensor performance and spatiotemporal monitoring coverage representativity in space and time 4 conclusions inferring mobile measurements complementary to existing reference station networks to derive highly granular air quality maps based on real time sensor data is vital for citizens policy makers and researchers although not directly suitable for scenario modelling it opens up a range of potential applications including real time hotspot detection evaluation of local vs regional source contributions in situ policy measure evaluation improved air quality modelling e g emission factors based on experimental data and ultimately more accurate air quality assessments in heterogeneous urban areas moreover in an operational context their lower resources and computational power offer the potential of near real time pollution maps allowing for real time policy interventions street closure redirection of traffic speed reduction novel source identification wildfire detection this work demonstrates the potential of data driven techniques for spatiotemporal air quality inference of sensor data complementing an earlier model validation exercise on bicycle measurements in utrecht nl hofman et al 2021b this work now focused on the scalability of the developed models on different mobile carriers and pollutants we have demonstrated the scalability of the approach towards other datasets in antwerp belgium and oakland us different pollutants pm2 5 no2 bc and mobile carriers car and service fleet vehicles besides outperforming existing interpolation techniques kriging inverse distance weighing random forest this work shows that our data driven models approach state of the art physicochemical dispersion models in terms of performance while needing much lower resources computational power infrastructure and processing time although comparable in terms of prediction performance grf seems to achieve slightly better accuracies while the correlations are typically higher for avgae at the same time our results show that model performance still relies on the spatial representativity spatial monitoring coverage of the mobile measurements accurate and representative data in both space and time is therefore needed to properly train the models and provide reliable results ultimately the inference predictions will depend on the applied sensor performance the spatiotemporal monitoring coverage representativity in space and time of both mobile and regulatory measurements and the amount of available training data future work will focus on the explainability of the considered models sensitivity analysis of required quantitative training data and required spatial monitoring coverage known entry rate to obtain reliable model predictions declaration of competing interest the authors declare the following financial interests personal relationships which may be considered as potential competing interests esther rodrigo bonet reports financial support was provided by research foundation flanders nikos deligiannis reports financial support was provided by government of flanders acknowledgement this research was supported in part by the fonds wetenschappelijk onderzoek vlaanderen fwo under phd fellowship strategic basic research 1sc4521n and in part by the flemish government under the onderzoeksprogramma artificiële intelligentie ai vlaanderen programme appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105306 
25655,recent advances in sensor and iot technologies allow for denser and mobile air quality measurements these measurements are still spatiotemporally sparse at city level but can be interpolated using data driven techniques this work presents validation results of two machine learning models to infer air quality sensor data in both space and time temporal validation exercises are performed at available regulatory monitoring stations following the fairmode protocol both models show scalable to different mobile datasets with comparable prediction performance for pm2 5 r2 0 68 0 75 mae 2 99 2 82 μg m 3 and no2 r2 0 8 0 82 mae 8 81 9 83 μg m 3 in utrecht and antwerp in oakland atlanta we observed a lower performance for no2 r2 0 46 0 41 mae 4 06 5 07 and bc r2 0 31 0 28 mae 0 48 0 27 likely caused by the less representative monitoring coverage although comparable in terms of prediction performance the geographical random forest grf model seems to achieve slightly better accuracies while the correlations are typically higher for the air variational graph autoencoder avgae model this work demonstrates the potential of data driven techniques for spatiotemporal air quality inference of complementary sensor data the observed performance metrics approach current state of the art chemical transport models in terms of performance while needing much lower resources computational power infrastructure and processing time keywords iot urban air quality mobile sensors machine learning 1 introduction air pollution continues to have significant health impacts especially in urban areas in terms of human health impact particulate matter pm no2 and ground level ozone o3 are considered as the most important pollutants eea 2020 who 2020 according to the european environmental agency eea 77 of the european urban population is exposed to fine particulate matter pm2 5 concentrations exceeding the old world health organisation who guideline values now much more stringent from 15 to 5 μg m³ annually while only 1 of the estimated urban population lives in areas exceeding the eu limit value eea 2020 who 2021 the same holds true for no2 for which we almost met the 40 μg m 3 annual limit value in 2019 estimate 3 of the population in exceedance eea 2020 but who recently recommends a much more stringent guideline value of 10 μg m 3 who 2021 pm is associated with airway irritation cardiovascular and respiratory diseases e g lung cancer and chronic obstructive pulmonary disease copd and impacts on central nervous and reproductive systems no2 is linked with secondary pm2 5 formation airway irritation breathing problems e g bronchitis lung function and impacts on liver spleen and blood black carbon bc is associated with cardiovascular mortality chronic risk of developing cancer janssen et al 2011 2012 or acute cardiovascular effects like lung function laeremans et al 2018a 2018b or carotid arterial stiffening provost et al 2016 as emerging pollutant pm constituent bc is recently introduced in the who guidelines as a set of good practices on systematic measurements emission inventories exposure assessments source apportionment and emission reductions who 2021 especially in urban environments where both pollution sources and people affected by pollution are concentrated air pollution tends to peak as pollution levels can vary dramatically over short distances or time instances hofman et al 2018 int panis et al 2010 kumar et al 2018 pattinson et al 2014 peters et al 2014 pirjola et al 2012 a high monitoring resolution in both space and time should be pursued to accurately estimate population exposure since traditional air quality monitoring stations are rather costly and cumbersome cities typically only deploy few at representative locations e g roadside urban background in order to better assess people s exposure to air pollution there is an urgent need for higher monitoring granularity thanks to rapid advances in sensor and internet of things iot technologies cities can now collect data from a wide range of static and mobile sensors in response to the need for higher monitoring granularity recent mobile sensing networks have been collecting data on routine service fleets examples include postal vans in antwerp be qin et al 2021 garbage trucks in cambridge massachusetts us desouza et al 2020 and trams and buses in lausanne and zurich ch mueller et al 2016 other examples also include personal monitors on bicycles franco et al 2016 hofman et al 2018 n genikomsakis et al 2018 peters et al 2014 qiu et al 2019 and city wardens van den bossche et al 2016 these networks provide valuable in situ data on experienced exposure levels throughout the city nevertheless the collected data points are still sparse in time and space and need interpolation in order to be useable for air quality mapping and fine grained exposure assessments inferring spatiotemporal point measurements to derive highly granular air quality based on real time sensor measurements is vital for policy makers and researchers to detect pollution events evaluate local vs regional source contributions evaluate policy measures in situ and improve existing chemical transport models ctm with experimental data this works anticipated to demonstrate the potential of data driven techniques for air quality data inference in space and time by feeding two machine learning models with mobile data collected by mobile sensor networks and temporally validate predicted pollutant levels at available regulatory monitoring locations 2 material methods 2 1 sensor testbeds urban environments rely on in situ measurements as experimental evidence and starting point for any representative air quality assessment in this regard mobile sensor networks complement existing regulatory networks in providing unprecedented spatial granularity to evaluate the scalability of the proposed machine learning models we considered multiple sensor testbeds on different mobile platforms in the netherlands belgium and the united states fig 1 2 1 1 utrecht the netherlands the snuffelfiets project aims at gathering mobile air quality data with bicycles in utrecht the netherlands https snuffelfiets nl utrecht is a medium sized 99 km2 3 812 inhabitants km2 city with a temperate climate located along the rhine river in the center of the netherlands mobile data includes ambient particulate matter pm geographical coordinates road surface quality temperature relative humidity and volatile organic compounds vocs and is collected opportunistically at a 10 s resolution in utrecht the netherlands using mobile sensors developed by sodaq http sodaq com sodaq snifferbike data are transmitted using lte m and collected in a cloud based data platform developed by civity https civity nl en our study focused on the particulate matter measurements pm1 pm2 5 and pm10 quantified by an onboard sps30 sensirion sensor the national institute for public health and the environment rivm compared raw snuffelfiets data to regulatory data resulting in standard deviations of the differences between 4 and 6 μg m 3 wesseling 2020 a mean bias of 1 2 μg m 3 was obtained while 95 of the sensor data lies within 4 μg m 3 of the official data 95 ci of 8 μg m 3 wesseling et al 2021 rivm cleaned pm2 5 outliers with unrealistic cycling speeds 45 km h 1 and calibrated the pm2 5 data based on a mass factor derived from the relationship between sensor and reference data average reference average sensor at three regulatory stations after calibration sensor data are within 3 μg m 3 of the official data 95 ci of 6 μg m 3 and show a mean bias of 0 3 μg m 3 wesseling et al 2021 this calibrated pm2 5 data was subsequently used to feed our air quality inference models see section 2 2 snuffelfiets data for june 2020 was downloaded from https dataplatform nl data 3660d2e1 84ee 46bf a7b6 7e9ac1fcaf3a while the regulatory data of monitoring stations nl10636 and nl10643 was collected via an open api https api docs luchtmeetnet nl model performances are evaluated on one month june 2020 of mobile pm2 5 data 2 1 2 antwerp belgium within the bel air project the belgian postal company bpost collaborated with the interuniversity microelectronics center imec deploying mobile air quality sensors kunak air mobile on 20 postal vans for opportunistic air quality monitoring in antwerp https www imeccityofthings be en projecten bel air kunak air mobile sensors consist of alphasense sensors for pm1 pm2 5 and pm10 opc n3 nitrogen dioxide no2 no2 b43f and ozone o3 ox b431 temperature relative humidity and gps are powered via the car battery 12 v 100 ma and are equipped with lte m connectivity moreover they include a dedicated mobile housing developed by labaqua to protect the sensors against turbulent air flows when mobile the sensors contain a property algorithm to correct for temp rh influence and gas cross interference and had an initial factory calibration at cuenca spain before deployment imec calibrated the sensors locally no2 baseline span pm linear slope calibration based on 2 weeks of co location data 1 week calibration 1 week validation reaching overall good sensor performance with r2 of 0 82 0 89 mae of 2 2 8 24 and expanded uncertainty wcm of 25 64 following the eu equivalence tool v3 1 jrc 2010 based on consecutive co location campaigns of 4 batches of 5 mobile sensors 20 in total on top of a local regulatory monitoring station 42r817 antwerp be calibration resulted in mean sensor uncertainties of 7 7 87 12 94 for batch 1 19 14 11 28 45 for batch 2 11 15 4 to 3 6 for batch 3 and 17 3 55 33 97 for batch 4 not published the postal vans mainly operated during daytime hours on weekdays and saturday see appendix 1 the monitoring resolution was configured to 10 s during the day when mobile and 10 min overnight when static parked to avoid car battery drainage as urban environments typically exhibit high spatial variability for traffic related pollutants like no2 degraeuwe et al 2015 lewné et al 2004 munir et al 2021 nicholas hewitt 1991 we applied the proposed models see section 2 2 on the no2 sensor data temporal validation was performed for one month march 2021 at 5 regulatory monitoring locations in antwerp located within different microenvironments namely urban background r801 park r803 roadside r802 street canyon r805 and highway r804 we refer to appendix 2 for further details 2 1 3 oakland united states within oakland two google street view vehicles equipped with aclima instrumentation apte et al 2017 solomon et al 2020 collected mobile air quality measurements from may 28 2015 to june 7 2019 atmospheric concentrations of o3 no2 nitrogen oxide no methane ch4 carbon dioxide co2 black carbon bc and ultrafine particles ufp were collected on weekdays during office hours 9 17 h at a 1 hz resolution this mobile data was made available via the web application for google maps air quality data access 2021 in the considered study area two regulatory monitoring stations california air monitoring network https www arb ca gov aqmis2 aqdselect php were available oakland west and oakland laney providing air quality data on o3 no2 pm2 5 and bc again we opted for pollutants with exhibiting highest spatial variability in this case no2 and bc no2 was measured using 450 nm cavity attenuation phase shift spectroscopy model t500u teledyne inc san diego ca while black carbon bc particles were measured using a photoacoustic extinction meter droplet measurement technologies boulder co the inlet system was designed to minimize self sampling and particle sampling losses weekly zero span calibrations resulted in 3 and 6 for the two cars no2 instruments and 25 for bc apte et al 2017 our inference models were applied on one month of mobile data may 2019 2 2 data inference in space and time mounting air quality sensors on mobile platforms enables in situ opportunistic collection of measurements with a high spatial coverage the collected data is valuable as it is collected dynamically at the roadside reflecting the actual cyclist exposure mobile monitoring studies have revealed large spatiotemporal variability in pollutant exposure before impacted by various factors i e road traffic meteorology street topology and background emission dynamics not represented by measurements derived from nearest regulatory monitoring stations gelb and apparicio 2020 hofman et al 2018 peters et al 2014 qiu et al 2019 nevertheless these collected data points are still sparse fig 2 left and require spatiotemporal interpolation fig 2 right in order to generalize air quality assessments i e to disentangle space and time variant features impacting your mobile measurements when calculating a yearly averaged air quality map of a city for example completed time series are needed at every location to derive representative averaged pollutant concentrations focusing on regulatory air quality data from antwerp belgium it can be observed that pollutant concentrations temporally correlate at different monitoring locations fig 3 due to the observed correlation over both space and time data matrices of air quality data can be considered low rank and thus explainable by statistical numerical techniques asif et al 2016 udell and townsend 2019 the underlying low rank and slowly time varying structure of the air quality data can be leveraged to create numerical models that facilitate an effective spatiotemporal extrapolation paliwal et al 2020 machine learning ml approaches allow for training of underlying dependencies based on large air quality datasets and supplied context information traffic meteorology street type speed limit hereby enabling data inference or matrix completion in both space and time fig 2 to this end we recently developed two machine learning models do et al 2020 qin et al 2021 trained on mobile air quality sensors mounted on postal vans in antwerp belgium compared against existing spatial interpolation techniques do et al 2020 qin et al 2021 and validated against regulatory data at four different monitoring stations hofman et al 2020 2 2 1 avgae model the avgae model is a deep learning model based on variational graph autoencoders vgae incorporating spatial correlation of measurements by considering the road network graph while training time variant dependencies based on additional context information points of interest poi road type meteorology street canyon index do et al 2019 2020 for the utrecht data considered context data included road network road type distance to road segment meteorological data relative humidity temperature and wind speed hourly background pollution derived from nl10644 cabauw wielsekade station and poi crossing gas station traffic light bus station taxi rank as the measurement time and location are continuous the measurements are first aggregated at hourly intervals median along road segment locations hence the aggregation across space is nonuniform and is adapted to the considered locations on the road network the aggregated discretized measurements result in an incomplete measurement matrix needing completion fig 2 the graph is subsequently constructed according to the geodesic distance between the considered locations along the road network connecting nodes when the geodesic distance is smaller than a predefined threshold δ or if they belong to the same road segment the weight of a connection is set equal to the inverse of the geodesic distance do et al 2020 matrix completion is solved via a neural network model accepting the concatenated measurement matrix x and geocoordinate matrix s as input learning the underlying graph dependencies through stacked graph convolutional gcn layers three stacked gcn layers with the same dimensionality i e d 512 are used for the encoder and one gcn layer for the decoder for model training the kl divergence coefficient was set to β 0 1 the temporal smoothness coefficient to γ 0 8 and the temporal neighborhood width to wt 3 moreover we used a learning rate of α 0 005 and dropout rate of 0 4 the loss function was evaluated on the model estimation of known entries from mobile and reference station measurements mae space variant features are horizontally added to the measurement matrix on the other hand the time variant features are arranged in a matrix multiplied by a weight vector after applying a tiling operation we obtain an adjustment matrix from the time variant features this adjustment matrix is added to output of the avgae model the measurement aspect refers to the spatiotemporal correlation that exists internally in the measurements the internal spatial correlation in this case is incorporated via gcn layers leveraging the road network the graph the internal temporal correlation is considered by adding an extra term in the loss function do et al 2019 the accuracy based on rmse mae of the avgae model has shown to outperform other state of the art inference models i e linear and exponential kriging knn based collaborative filtering svd nmf nmc and rgcn as shown in do et al 2020 more information and mathematical description of the avgae model can be found in do et al 2020 while the model architecture is visualized in fig 4 2 2 2 grf model the geographical random forest grf model captures the interaction between air pollution and various context features by a series of flexible random forest models as traditional random forest does not consider spatial heterogeneity between measurements only indirectly through context features we build a local rf model by considering k neighbors qin et al 2021 it adopts the idea of geographically weighted regression gwr moving a spatially weighted window over the observations while being coupled with a flexible non linear model which is very hard to overfit due to its bootstrapping nature first a map matching step is performed to map mobile measurements to nearest street segments followed by hourly aggregation of mobile measurements at centroid points of the street segments next the grf model makes use of all training measurements and context features to build a global model while a local rf model is fitted for each location only considering k nearest measurements here we consider not only neighbors that are close in geographical distance but also neighbors with similar context features finally a weighting mechanism is utilized to combine global estimations weight 0 8 with local estimations weight 0 2 200 trees to obtain the final predictions the grf model included context information namely road network road type distance to road segment meteorological data relative humidity temperature and wind speed and hourly background pollution derived from a regulatory regional background station the grf model is explained in more detail in qin et al 2020 2021 while the architecture is visualized in fig 4 the predictive power of both models was tested with different types of context information on antwerp and utrecht data with the idea of scalability in mind we finally selected context features that improve the predictive power of the analysis performance as defined in section 2 3 but are also openly available table 1 2 3 model performance following an earlier model validation exercise on the utrecht pm2 5 data hofman et al 2021a this work explores the generalizability of the avgae and grf approach on new mobile datasets in antwerp be and oakland us complementing hofman et al 2021 we now focus on different pollutants no2 bc and on different mobile collection platforms multiple postal vans or two google street view vehicles instead of bicycles 2 3 1 model training both ml models avgae and grf are trained on the different mobile datasets march 2021 in antwerp be june 2020 in utrecht nl and may 2019 in oakland us together with available context information on road type meteorology and background pollution and subsequently applied to infer air quality measurements in space and time fig 5 it is worth noting that the hyper parameters of the avgae and grf models are tuned for individual datasets and pollutants since the distributions of measurements vary across the considered pollutants and datasets the training procedure for the avgae model is performed using stochastic gradient descent with the adam optimization algorithm kingma and ba 2014 as the updating rule in this training procedure aggregate subsets of both mobile and fixed reference station measurements were used to address over fitting the avgae model uses both dropout srivastava et al 2021 and weight decay the avgae model is implemented using tensorflow abadi et al 2016 the grf model performs training process using random forest regression geurts et al 2006 with the kmeans algorithm pelleg and moore 1999 as clustering method 2 3 2 model validation temporal model validation of both considered models avgae grf was performed through an extensive set of performance statistics mae mbe rmse ia corr nmb nmsd and by following the fairmode guidance document on modelling quality objectives and benchmarking to derive the model quality indicator mqi janssen et al kushta et al 2019 for each reference station location hourly pollutant concentrations for the considered month were predicted by the considered models avgae and grf by inferring the available mobile sensor data and regulatory measurements of other reference stations the hourly predicted concentrations were subsequently compared to the regulatory measurements to calculate the performance metrics and time series graphs this temporal model validation exercise was repeated for every available reference station in the mobile monitoring domain of the considered cities utrecht antwerp and atlanta oakland in order to explore the spatial prediction performance for each validation exercise aggregate subsets of both mobile and available reference station data have been used for model training and prediction besides the reference station location used for validation this procedure was subsequently iterated for each available reference station 2 3 3 performance metrics the performance metrics are calculated based on hourly model predictions and associated reference data for each of the considered regulatory monitoring locations see section 2 2 the performance metrics included mean absolute error mae root mean squared error rmse index of agreement ia accuracy pearson correlation normalized mean bias nmb normalized mean squared deviation nmsd and the model quality indicator mqi and model performance criteria for bias mpcbias correlation mpccorr and standard deviation mpcstdev as defined by the fairmode guidance document janssen et al kushta et al 2019 1 r m s e 1 n i 1 n o i m i 2 2 m a e 1 n i 1 n o i m i 2 3 i a 1 i 1 n o i m i 2 i 1 n m i o o i o 2 4 r i 1 n m i m o i o i 1 n m i m 2 i 1 n o i o 2 5 a c c u r a c y 1 i 1 n m i o i i 1 n o i 6 n m b m o o 7 n m s d σ m σ o σ o w h e r e σ m 1 n i 1 n e i e 2 where m i represents the predicted pollutant concentration o i represents the observed concentration at time instant i and m and o are the mean concentrations rmse eq 1 and mae eq 2 are accuracy metrics representing the difference between predicted and observed concentrations where rmse is more sensitive to outliers the smaller the better ia eq 3 and accuracy eq 5 represent the similitude between predictions and observations and range between 0 and 1 where 1 represents perfect agreement r eq 4 represents the association correlation between model predictions and observations and ranges between 1 and 1 0 means no correlation where 1 or 1 represents perfect linear correlation between predicted and observed pollutant concentrations the ia and accuracy are preferred over r as they are independent of the relationships between the variables whereas r only refers to linear relationships qin et al 2021 but reported here as well for comparison purposes with other model validation studies nmb eq 6 represents a normalized bias metric showing the general model under or overestimation of the observations finally the nmsd score eq 7 shows how the standard deviations of the model predictions compare with the observed standard deviations a python script to calculate the performance metrics is provided as supplementary material 3 results discussion 3 1 utrecht nl the snuffelfiets dataset of pm2 5 for june 2020 consisted of 928047 measurements was clipped to the utrecht area 5 0596 52 0568 5 1636 52 1242 temporally aggregated to hourly values at 8262 locations road network and resulted in a known entry rate of 0 95 of the spatiotemporal matrix for june 2020 the measured pm2 5 concentration range was 1 0 4 07 3 0 11 0 μg m ³ min mean median max from the avgae and grf hourly predicted pm2 5 concentrations at each of the regulatory monitoring stations and the associated regulatory data we calculated the resulting performance metrics shown in table 2 both models perform well in terms of uncertainty mae 2 83 3 14 avgae and 2 74 2 88 grf and correlation r 0 63 0 72 avgae and 0 73 0 77 grf with grf slightly outperforming avgae the observed mae table 2 is in line with the reported sensor uncertainty of 3 μg m ³ 95 ci of 6 μg m ³ while the mean bias mbe is slightly higher 1 05 1 99 μg m ³ when compared to the mean calibrated sensor bias 0 3 μg m ³ in wesseling et al 2021 associated model performance criteria mpc for bias correlation and standard deviation are all within the bounds set by the fairmode protocol not shown residuals of hourly modelled concentrations showed normal distributions moreover as the model quality indicator mqi is 1 table 2 fairmode model quality guidelines state that the model can be considered suitable for policy purposes janssen et al we should however note that current applications of inference models are confined to spatial air quality assessments while forecasting scenario modelling are additional features of chemical transport models ctm fig 6 shows time series graphs of the avgae and grf predicted pm2 5 concentrations it illustrates that predicted pm2 5 concentrations mi agree reasonably well with the reported reference data oi for each of the considered models at each location mostly falling within the uncertainty bounds of the reference equipment rmsu provided by fairmode and the mqi limits defined by fairmode as twice k 2 the reference measurement uncertainty janssen et al as shown in fig 6 and table 2 from the inferred monthly averaged street segment maps of utrecht fig 7 spatial variation can be observed with common locations with elevated pm concentrations not only along major traffic axes but also in certain inner city areas e g nw of map nonetheless the observed spatial concentration variability is much lower for pm than for no2 3 2 or bc 3 3 which can be explained by the more various and widespread sources of pm and regional and cross boundary source attributions hofman et al 2016 kelly and fussell 2012 keuken et al 2013 kumar et al 2018 rivas et al 2020 3 2 antwerp be the antwerp no2 dataset for march 2021 consisted of 323691 10 s and 10 min no2 measurements was clipped to the antwerp area 4 2170 51 1430 4 4980 51 3780 and temporally aggregated to hourly values at 23569 street segment locations for march 2021 fig 8 the measured no2 concentration range was 0 29 15 24 48 642 47 μg m ³ min mean median max when comparing the avgae and grf hourly predicted no2 concentrations at each regulatory monitoring station location against the associated regulatory data we calculated the resulting performance metrics shown in table 3 in general both models seem to perform well on the new dataset table 3 with avgae showing slightly better prediction accuracy mae mbe rmse and acc while grf shows better prediction association correlation ia r again the observed model bias 31 to 31 is in line with the observed sensor bias during the co location campaign 15 to 34 the model quality indicator mqi falls below one indicating that both models perform as required for policy purposes comparing the performance metrics at the different microenvironments reference stations higher errors and lower correlations higher model quality indictors are observed at the r803 park and r804 highway ring road regulatory stations this is interesting as these environments can be considered as less representative for the inner city of antwerp and therefore less frequently sampled by the mobile sensors the highest performance is observed at the r801 station which can be considered as an urban background location at 30 m from a traffic intensive road in the inner city at this r801 location located next to street segments with a high monitoring coverage 148 passages during june 2021 as visualized in appendix 3 we observe best prediction performance table 3 for both models r 0 9 and mae 4 52 for avgae r 0 94 and mae 6 03 fig 9 shows time series graphs of the avgae and grf predicted no2 concentrations mi against the reported regulatory concentrations oi predicted no2 concentrations mi agree reasonably well with the reported reference data oi for each of the considered models at each location mostly falling within the uncertainty bounds of the reference equipment rmsu provided by fairmode and the mqi limits defined by fairmode as twice k 2 the reference measurement uncertainty janssen et al the models tend to underestimate the actual no2 concentration at the least performing highway location i e high concentration station r804 nmb 0 31 for avgae and 0 13 for grf and overestimate at an urban background location i e the low concentration station r803 nmb 0 02 for avgae and 0 31 for grf the best and least performing station are provided in fig 9 while time series graphs of remaining stations r802 r803 r805 are provided in appendix 4 a model validation exercise of the current physical air quality model applied for policy making in flanders belgium atmostreet was performed at similar antwerp monitoring stations r803 r804 r805 r817 and yielded following performance results for no2 r 0 73 0 9 rmse 8 29 18 93 and mbe 5 61 0 94 irceline 2017 lefebvre et al 2013 comparing the observed performance of our models table 3 against the atmostreet model the considered data driven techniques seem to approach the state of the art in terms of performance the physical atmostreet model which is currently used for policy purposes in belgium integrates multiple spatial scales regional local using a rio ifdm ospm modelling chain with the rio module spatially interpolating regulatory measurements ifdm bi gaussian dispersion model simulating emission plumes from known point and line emission sources and ospm being a street canyon module implementing the effect of urban topology street canyon effect on the resulting pollutant dispersion irceline 2017 lefebvre et al 2013 3 3 oakland us the may 2019 data which consists of 351018 no2 and 309216 bc 1 s measurements collected during working hours 9 17 h was clipped to the oakland area 37 7482 122 3791 37 8768 122 1860 and temporally aggregated to hourly values at 3207 no2 and 2838 bc unique street segment locations the measured no2 concentration range was 0 45 12 50 10 39 117 02 μg m 3 min mean median max while bc ranged between 0 0 99 1 43 55 78 μg m 3 applied context factors were road network road type speed limit meteorological data relative humidity temperature atmospheric pressure wind speed wind direction and background pollution derived from regulatory station 07436 aquatic park when comparing avgae to grf in oakland the performance metrics listed in table 4 again show better accuracy mae mbe rmse and acc for avgae and better association correlation ia r for grf a slightly better performance is observed for no2 compared to bc probably due to the typical high spatial variability of bc when compared to no2 hofman et al 2016 2018 this time a higher mean bias was observed 41 72 for no2 and 47 156 for bc when compared to the initial sensor bias 3 6 for no2 and 25 for bc in apte et al 2017 fig 10 plots the avgae and grf predicted no2 and bc concentrations mi against the reported concentrations by the monitoring stations it shows that the models perform much worse compared to the predictions in utrecht nl and antwerp be predictions sometimes fall outside of the uncertainty bounds of the reference equipment rmsu but still within the mqi limits 2 x rmsu defined by fairmode janssen et al although the performance metrics in table 4 show a better prediction accuracy for no2 mae 2 9 5 22 μg m 3 when compared to the antwerp no2 predictions mae 4 52 16 79 μg m 3 normalized error metrics nmb are much worse and significantly lower correlation between sensor and reference data is observed in oakland r2 0 34 0 57 when compared to antwerp r2 0 59 0 94 similarly for bc low absolute error mae 0 2 0 48 but high nmb 47 156 and low correlations r2 0 26 0 32 are observed in the predicted concentrations at the reference station locations table 4 we hypothesize that the mean prediction accuracy is likely impacted by the lower sensor uncertainty of the oakland no2 instruments compared to antwerp sensors while the observed correlations dynamics will be impacted by the spatial representativity of the collected mobile sensor data antwerp oakland fig 11 left panel shows all collected and hourly aggregated mobile no2 measurements n 7516 and grf inferred data within the considered time period may 2019 it can be observed that the sampling points are not really representative for the entire oakland domain but rather concentrated along main access roads fig 11 left panel the resulting inferred predictions fig 11 right panel show rather low variability in areas that are sparsely or not at all covered by the google cars illustrating the need for a representative spatial monitoring coverage in this regard fleet measurements with multiple moving sensors e g 17 service fleet vehicles in antwerp and 500 bicycles in utrecht provide a higher spatiotemporal monitoring coverage and seem to result in better prediction performance than only two google cars the importance of monitoring coverage was demonstrated before by evaluating the correlation and bias of repeated mobile subsamples along mobile trajectories subsamples against the average concentration of the full dataset resulting in 24 94 repeated runs 10th and 90th percentile median of 41 on 50 m segments to reach a deviation of 25 or 5 11 repeats to reach a deviation of 50 van den bossche 2016 and about 25 repeated runs to reach an r2 0 9 and cv rmse 0 5 apte et al 2017 we therefore hypothesize that the lower prediction performance in oakland is due to the lower monitoring coverage in both time and space resulting in less representative data for model training ultimately the prediction performance of inference models will not only depend on the quality of the models but also on the quality of the provided input data air quality sensor performance and spatiotemporal monitoring coverage representativity in space and time 4 conclusions inferring mobile measurements complementary to existing reference station networks to derive highly granular air quality maps based on real time sensor data is vital for citizens policy makers and researchers although not directly suitable for scenario modelling it opens up a range of potential applications including real time hotspot detection evaluation of local vs regional source contributions in situ policy measure evaluation improved air quality modelling e g emission factors based on experimental data and ultimately more accurate air quality assessments in heterogeneous urban areas moreover in an operational context their lower resources and computational power offer the potential of near real time pollution maps allowing for real time policy interventions street closure redirection of traffic speed reduction novel source identification wildfire detection this work demonstrates the potential of data driven techniques for spatiotemporal air quality inference of sensor data complementing an earlier model validation exercise on bicycle measurements in utrecht nl hofman et al 2021b this work now focused on the scalability of the developed models on different mobile carriers and pollutants we have demonstrated the scalability of the approach towards other datasets in antwerp belgium and oakland us different pollutants pm2 5 no2 bc and mobile carriers car and service fleet vehicles besides outperforming existing interpolation techniques kriging inverse distance weighing random forest this work shows that our data driven models approach state of the art physicochemical dispersion models in terms of performance while needing much lower resources computational power infrastructure and processing time although comparable in terms of prediction performance grf seems to achieve slightly better accuracies while the correlations are typically higher for avgae at the same time our results show that model performance still relies on the spatial representativity spatial monitoring coverage of the mobile measurements accurate and representative data in both space and time is therefore needed to properly train the models and provide reliable results ultimately the inference predictions will depend on the applied sensor performance the spatiotemporal monitoring coverage representativity in space and time of both mobile and regulatory measurements and the amount of available training data future work will focus on the explainability of the considered models sensitivity analysis of required quantitative training data and required spatial monitoring coverage known entry rate to obtain reliable model predictions declaration of competing interest the authors declare the following financial interests personal relationships which may be considered as potential competing interests esther rodrigo bonet reports financial support was provided by research foundation flanders nikos deligiannis reports financial support was provided by government of flanders acknowledgement this research was supported in part by the fonds wetenschappelijk onderzoek vlaanderen fwo under phd fellowship strategic basic research 1sc4521n and in part by the flemish government under the onderzoeksprogramma artificiële intelligentie ai vlaanderen programme appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105306 
25656,coastal flooding is among the most significant disasters that threaten littoral populations in today s world climate change and global warming increase this risk and require implementing efficient strategies to anticipate the hazard and reduce human and material damage this paper describes a generic platform called littosim gen a coupling of agent based and hydrodynamic models which aims to involve stakeholders in a participatory simulation of land use management to minimize the risks of coastal submersion an application to the southwest coast of france rochefort demonstrates the features of this platform we propose several recommendations to improve the generic conception and implementation of models dedicated to participatory simulation keywords participatory simulation agent based model genericity littosim gen coastal flooding risk management 1 software and data availability the littosim gen model source code and data preparation scripts are available on github https github com littosim littosim model tree littodev littosim gen is developed by the littosim consortium a group of multidisciplinary researchers https littosim hypotheses org littosim gen is coded in gaml language under the gama platform https gama platform org software and hardware requirements to run littosim gen are detailed in the implementation details section of this paper 2 introduction since the 1960s the gaming simulation approach that combines system modeling and gaming interactions duke 2014 dukes et al 2011 stadsklev 1975 was used mainly for two purposes to foster learning and improve awareness learning games sorace et al 2018 or to support collective and deliberated decision making processes policy games mayer et al 2014 with the evolution of research and practices complex systems modelers have developed the participatory modeling approach gray et al 2016 voinov and bousquet 2010 in parallel video games have extended their application domain to the rapidly growing serious games since the 2000s michael and chen 2005 sawyer and rejeski 2002 participatory simulation ps is a gaming simulation approach that takes root more on the side of participatory modeling 1 1 some authors use the terms participatory simulation and serious game interchangeably assuming that the serious game is multiplayer and serves a participatory purpose in this paper we use the first terminology it uses gaming and models to develop frameworks for groups of participants that share control over decisions bakhanova et al 2020 becu 2020 these participants collectively build ideas abstractions concepts strategies and plans that are useful for the game ps is closely tied to the companion modeling approach étienne 2013 as they share the same participatory and empowerment objectives becu and crookall 2020 multiple research works have investigated the use of ps among different environmental disciplines hedelin et al 2021 van der wal et al 2016 implemented a model of water management where the game participants receive feedback depending on their chosen measures embankment changing land use evacuation etc ornetsmüller et al 2018 used an approach based on board games and cards to encourage farmers to change their cultivation strategies adam et al 2018 propose a ps to examine the role of communication in reducing casualties during wildfires briot et al 2017 developed a game for park managers to explore the impact of democratic practices and local governance on the biodiversity in protected areas in flooding management several applications are dedicated to various aspects of submersion risk sprite taillandier and adam 2018 is a game designed for students to explore diverse policies of coastal flooding management vigiflood adam and andonoff 2019 is a game for crisis communication during floods anycare terti et al 2019 is a role playing simulation that investigates how decision makers anticipate and respond to weather related events khoury et al 2018 propose a ps to understand flood prevention and improve resilience in mixed urban rural environments tanwattana and toyoda 2018 used gaming simulation as a learning tool to build community based disaster risk management some online games are also available on the internet such as stop disasters 2 2 https www stopdisastersgame org developed by the un office for disaster risk reduction participatory modeling and simulation which involve both models and stakeholders are becoming popular in environmental studies sterling et al 2019 van bruggen et al 2019 gray et al 2018 le page et al 2013 mayer 2009 that use agent based modeling to approach social economic and natural problems this increasing use of participatory modeling and simulation has revealed many issues concerning their methods and results some gaps highlighted by researchers and related to complex models are lack of comprehension dissemination and replication laatabi et al 2018 these limitations affect the genericity of ps models as stressed in the concluding chapter of the companion modeling book étienne 2013 therefore existing models and games are not usually reused or replicated to address similar questions in different contexts to investigate the issue of genericity in participatory modeling and simulation we use the littosim becu et al 2017 kit 3 3 the kit is composed of i an agent based model coupled with a hydrodynamic model ii a set of operating instructions to organize conduct and facilitate a workshop and iii a list of equipment to run the workshop a flooding risk management ps involving decision makers this toolkit is an agent based model initially developed for the territory of oléron island in france thus it faces considerable limitations since it is highly contextualized and hard to reuse in another context or even in the same context when the risk process or the field data are updated therefore we edited littosim gen generic littosim an extended version to make the model generic reusable and adaptable to other territories this work describes the architecture of the littosim gen platform as an attempt to address the lack of genericity in ps models the paper is structured as follows the following section presents the context of flooding risk management and its process in the french case and introduces the original version of the littosim model after that we describe the new generic version of the model following the structure of a standard protocol in the fourth section we present an example of an application of littosim gen to the estuary of rochefort france then we discuss the changes made to generalize littosim and we draw up a set of recommendations to develop generic and reusable participatory simulations the paper ends with a conclusion and some perspectives of this work 3 context and background flooding risk management in france global warming and climate change increase the risk of natural disasters such as coastal flooding wong et al 2014 large waves and storms can lead to catastrophic losses in coastal areas paprotny et al 2018 where populations are growing worldwide neumann et al 2015 low coastal zones become extremely dangerous for human activities especially when the sea level rise in concomitance with continental and pluvial flooding such scenarios amplify the risk and cause severe damage to exposed zones hence decision makers and urban planners must deal with this new socioeconomic challenge requiring them to mitigate the vulnerability and reinforce the resilience of their coastal cities in france efforts to address the problem of coastal flooding risk are getting more attention particularly after the xynthia storm struck the atlantic coast in 2010 bertin et al 2014 the management of this risk is the responsibility of local structures formerly municipalities district councils since 2018 that collaborate with the state services to elaborate plans and strategies to reduce flooding damage the risk prevention plan ppr is one of the tools used by municipalities to make decisions about land use and urbanization in areas exposed to natural hazards such as flooding the ppr zoning is associated with the local urbanization plan plu to provide decision makers with urban planning and management guidelines however depending on the territorial configuration risk prevention may not be managed at the micro scale and may require strategies that exceed the territory and the capacity of municipalities the inundation prevention action program papi is defined to allow an integrated risk management program at the macro scale inter district this program gathers several municipalities to develop collective and more efficient plans that are difficult to elaborate on the micro scale district flood damage may highly depend on individual choices and awareness of flood management dawson et al 2011 filatova et al 2011 this culture concerns local authorities and decision makers who manage strategic policies urban planners for example determine habitable and risk areas build coastal defenses and encourage or discourage residential choices and preferences of coastal populations hence developing a risk culture requires the implication of stakeholders in coastal flooding management training teaching and producing manuals are popular ways of learning and consolidating personal experience however modern technical developments allow more innovative and virtual methodologies to spread culture and improve awareness participatory simulations offer a simulated environment to put players in real world situations bakhanova et al 2020 in flooding risk management these tools foster social learning about coastal flooding by experiencing different scenarios to reduce the potential damage of submersion events littosim model becu et al 2017 is one of these computer simulation tools dedicated to local authorities to learn new and alternative strategies by testing different scenarios of preventive measures before the submersion event this toolkit focuses on participation and collaborative management of risk to rethink actual policies and overcome obstacles preventing the efficient handling of flooding problems littosim was developed for the territory of oléron island in southwest france and experienced in several workshops between 2015 and 2018 with the community of oléron districts amalric et al 2017 the simulation environment was designed in collaboration with local stakeholders and technicians according to their insights about the flooding problem therefore littosim includes a set of concepts rules and constraints that are specific to the original territory these specifications do not apply to other contexts e g the restrictions on building outside a defined urban ring the prohibition of changing urban areas to agricultural areas etc besides the model design includes some behaviors and data types that may be relevant only in oléron e g types of land use units types of coastal defenses installing sand fences process costs and delays of player actions etc these concepts are implemented through complex modeling aspects and data structures that are not generic thus understanding the model and adapting its parameters is challenging and makes littosim extremely complicated to reuse in a different territory hence generalizing littosim requires revising its structure to externalize independent components data inputs and simulation parameters 4 littosim gen a generic version of the littosim model in this section we follow the structure of the odd 2d protocol laatabi et al 2018 to describe all relevant concepts and necessary components to understand the model architecture and reproduce its results in different contexts for the sake of understanding we introduce the theoretical and empirical background element as a part of the overview block since describing general concepts theories and empirical basis underlying the model is necessary to understand the following blocks of the description 4 1 overview 4 1 1 purpose littosim gen is a coupling of an agent based model and a hydrodynamic model and implemented as a gaming simulation of submersion risk management participants of a littosim gen workshop whom we call players manage their territories and test alternative strategies on land use planning and coastal defenses to reduce flooding damage the game offers the possibility to experience different realistic management situations e g deciding to invest in habitat densification or protecting populations anticipating work delays due to regulatory constraints etc multiple actions e g building dikes withdrawing people adapting buildings etc are experienced and combined to assess their efficiency for protecting coastal populations activities and assets hence the principal aim of littosim gen is to help stakeholders and urban planners decision makers improve their risk culture and awareness by discovering new ways to optimize their decisions and collaborate with other stakeholders implied in the flooding risk question the current version of littosim gen proposes three territorial archetypes overflow coast estuary coast and cliff coast a territorial archetype is the typical example of a situation of inhabited space defined according to physical socio economic historical and governance characteristics rocle et al 2020 new archetypes can be implemented in littosim gen by including relevant data and updating parameter files 4 1 2 theoretical and empirical background littosim gen is composed of three agent based models representing the three roles of the game fig 1 manager is the model representing the study area and manages game rounds launches submersion events and communicates with the two other roles player represents decision makers of the districts of the study area that administer their territories through actions on land use units and coastal defenses players do not interact directly between them and their number is limited to four in each game for fluency reasons leader represents the role of a hypothetical risk agency 4 4 the risk agency collaborates with districts and local authorities to encourage collaborative management and initiate collective strategies the agency funds a part of the proposed projects and supervises their implementation to allow their continuity under governance practices and alternative approaches despite financial and political conflicts between various stakeholders that provides policy advice to players and manages meetings between them to set out collective strategies the leader participates in the game through financial giving and taking money and information transactions orientation expertise these interventions can alter the expected process of the game by changing conditions and constraints governing the application of player actions littosim gen uses empirical data from the case study these data are loaded into the model through several agents to implement different theoretical concepts that we introduce in the following paragraphs districts the study area may be composed of multiple districts fig 2 b each district determines the territory that a player team can manage a game is limited to four players risk areas the high risk zones are more subject to submersion during flooding events fig 2h the boundaries of these areas are determined by competent authorities and do not evolve during the game protected areas natural and special protected areas spa are forbidden from urban activities fig 2i non urban projects agriculture managing coastal defenses are subject to temporal and financial constraints due to the applied laws the boundaries of these areas are determined by authorities and do not evolve during the game soil cells soil cells are the micro level of the spatial scale usually 20 20 m2 or 10 10 m2 for high resolution grids in littosim gen fig 3 the soil cells grid plays three distinct roles the dem digital elevation model grid storing the soil altitude at each cell fig 2a the rugosity water resistance grid saving the corresponding manning value to the land cover of each cell fig 2j and the flooding grid displaying within each cell the water height during submersion events the altitude and rugosity of soil cells are not changed directly by players but updated indirectly through actions on land use units and coastal defenses fig 3 land use units a land use unit is a principal element of planning management in littosim gen fig 3 these macro cells are represented by regular squares or by irregular polygons when located at the boundaries of districts fig 2g according to the applied rules players may change the type of a land use unit fig 4 six types are currently implemented natural n this type corresponds to green zones like forests and meadows urbanizing in these areas may be allowed or prohibited when in protected areas non protected natural units can become agricultural or authorized for urbanization agricultural a these zones are open for human activities related to agriculture but not urbanizing to build in these areas they first need to become authorized for urbanization players can also transform them into natural areas authorized for urbanization au this type corresponds to previously natural or agricultural areas open to urbanization they can become urbanized u or adapted special au aus authorized for adapted urbanization aus authorities enhance infrastructures and subsidize flood adapted buildings this type is turned afterward to adapted urban us urban u this type contains residential buildings and can receive new populations it may become natural through expropriation n densified to attract more population ui or transformed to an adapted urban us area through the adaptation process adapted urban us it represents adapted urban areas with buildings that are more resistant to flooding risks urban in densification ui this transition state occurs when authorities take measures to encourage populations to settle in these zones densification continues until the unit reaches the next level of population density coastal defenses coastal defenses are spatial lines created to protect littoral areas from flooding when the sea level exceeds the land elevation after storms or large waves fig 2c building a new coastal defense increases the altitude of the soil by adding the height of the created object modifying this height raising affects the corresponding soil cells dismantling an object resets the soil altitude a coastal defense has a state good medium or degraded that degrades in time but can be upgraded with a repair action installing sand fences or loading pebbles during a flood event a coastal defense has a probability of rupture that depends on its state fig 5 depicts the state diagram of coastal defenses in littosim gen state dynamics depend on whether the object is a dike a dune or a pebble dike dike a dike is a human engineered barrier built with stones or other building materials dikes are littoral when installed directly on the coast or inland when constructed away from the coastline with a specified distance inside the area of fig 2d dikes are detrimental and non natural structures but inland dikes are less harmful to the environment dune a dune is a hill of sand constructed naturally or built by humans through for example installing sand fences a sand fence is a wooden barrier made of slats that regenerates a dune around it dunes are natural barriers and preferred to dikes for ecological reasons pebble dike a pebble dike is a natural or human engineered but without any other building materials dike of pebbles small or large pieces of rocks this barrier is also eco friendly as dunes and is often used to reinforce existing solid dykes player actions players can take several measures to reduce the submersion risk these actions are of two categories depending on their target object a land use lu or a coastal defense cd action each of these actions has a cost and a delay number of years rounds before being applied and may belong to different strategies depending on its application conditions see table 1 strategic profiles the principal aim of littosim gen is to allow decision makers to experience alternative strategies of flooding risk prevention the experience with earlier workshops of littosim becu et al 2017 has revealed three different player profiles based on their actions builder builders build coastal dikes and initiate urbanization and densification projects even in risk areas the builder profile opts for solid coastal defenses to protect coastal populations and assets soft defense players opt for more natural solutions to protect their coasts such as building and maintaining dunes and creating inland dikes for urbanization and densification projects this profile favors risk adapted areas strategic withdrawal this profile regroups retreat actions such as dismantling coastal defenses expropriating risk urban areas and urbanizing and densifying out of littoral and risk areas these three strategies classify a player that can be for example 50 builder 40 soft defense and 10 withdrawal based on his actions associating a player to a strategy depends on a specified threshold e g a player has a builder profile when he is at least 40 builder the classification of some actions within a strategy depends on the player profile e g classifying urbanization out of littoral and risk areas as a strategic withdrawal requires that the player already has a withdrawal profile this dynamic profiling anticipates player strategies an action may be categorized as other if it does belong to none of the three profiles e g densification out of risk areas but without being neither withdrawal nor soft defense table 1 depicts the implemented actions in the current littosim gen with their classification among different strategies each of these player actions may trigger one or several levers depending on a set of conditions and parameters levers a lever is a bonus or penalty intervention that rewards or penalizes a player based on his actions during the game different levers can be triggered after a certain threshold and applied if not canceled on player actions fig 6 there are two types of levers cost levers add to penalize or subtract to reward an amount to an action cost for example constructing a new dike after exceeding a fixed threshold penalty the action cost is marked up or installing sand fences on a dune bonus the action cost is marked down delay levers add or deduce a duration from an action delay for example creating an inland dike bonus the action delay is accelerated or urbanizing a land use unit in a coastal area penalty the action delay is retarded levers are implemented in littosim gen to encourage or discourage player actions depending on their ecological and virtuous strategies these levers fit into the three previous strategic profiles builder levers b concern dike building and urbanization actions soft defense levers s are applied on dunes inland dikes and adapted urbanization and withdrawal levers w concern actions of strategic withdrawing of urban populations and dismantling of dykes builder levers penalize and soft defense and strategic withdrawal levers reward players table 2 lists implemented levers in the current littosim gen 4 1 3 entities state variables and scales littosim gen simulates the territory evolution over a set of rounds usually 10 each game round represents one year in the real world the model is composed of three separate agent based models representing the three roles of the game we describe their conceptual structures separately each uml entity describes an agent game manager fig 7 depicts the structure of the game manager model as a class diagram with only relevant attributes it is composed of three categories of classes six core entities representing principal agents allow managing coastal defenses and land use units by applying received actions from players and activated levers from the game leader two network entities to communicate send and receive data with players and the game leader six display entities add additional information to the simulation such as roads rivers natural protected areas and flooding risk areas game leader three entities of the manager model district player action and activated lever are re implemented here to represent information that the two models share an activated lever is a lever triggered by an action and accepted not canceled by the leader according to its type a lever may change the delay of an action delay lever or change its cost cost lever several entities extend these two abstract classes to represent all levers introduced in table 2 one network agent performs communications with both the game manager and players fig 8 depicts the uml class diagram of this model with relevant attributes of each entity players player action is extended to represent the two types of player actions coastal defense action and land use action communications with the game manager are handled by network player and with the game leader by network listener to leader additional classes from the game manager provide players with all the information required to play the game and optimize their actions three new classes help manage the game interface on the player side basket allows storing current player actions name cost delay with the possibility of canceling history lists already validated player actions with their information final cost after taking into consideration applied cost levers remaining delay after considering applied delay levers message displays received information concerning the game rounds population budget messages from the leader validated actions and activated levers 4 1 4 process overview and scheduling a littosim gen game takes place over three phases at first the workshop animator explains game rules to the four playing teams in the second phase the game begins and players start to manage their territories in about ten rounds during which three or four submersion events occur the third phase is devoted to debriefing and discussing the results during the game the game manager launches submersion events and controls different components of the game rounds player actions and leader commands players decision makers representing the districts execute their planning actions on coastal defenses and land use units and submit them to the game manager the game leader validates or cancels the triggered levers and exchanges information with players financial transactions and text messages the activity diagram in fig 10 shows the three actors of littosim gen with their different interactions and activities within the model 4 2 design concepts 4 2 1 individual decision making levers are activated automatically based on player actions and activation parameters fig 6 and table 2 other agents do not implement any automatic decision making all decisions are made by humans implied in the game game manager based on a predefined scenario the game manager decides when to switch rounds and launch submersion events flooding events are planned according to the workshop time and the number of game rounds this scheduling should not be told to the playing teams game leader the risk agency has a set of indicators to help make proper decisions depending on the adopted strategies builder soft defense strategic withdrawal or others the budget of districts and the shared financial plan that players may want to set up these decisions are critical and can alter the game process hence the game leader must take the following actions with vigilance validating canceling levers a player can take actions that are too penalized or rewarded by automatic levers the game leader may intervene to correct this situation and apply or cancel some levers giving taking money the game leader can give or take money from some players to balance their budgets reorient their strategies or reward or penalize them if the impact of levers is biased sending messages information and warnings are sent to players to inform them about decisions concerning their budgets populations and strategies players each playing team makes decisions individually or with the risk agency and the other players some decisions may concern many districts and must be subject to a collective discussion that leads to a shared financial plan recommendations of the risk agency may affect the decision making process of players 4 2 2 learning model agents do not implement any form of automatic learning to optimize their decisions human agents namely the game leader and the players learn from their past experiences and the feedback of their actions discussions and face to face role playing reinforce social and horizontal learning solinska nowak et al 2018 voinov et al 2018 4 2 3 individual sensing the game manager and the game leader have a global perception of the game and use several indicators to optimize their decisions each player team has a partial perception limited to its environment and has no access to the actions of the game manager submersion events and the game leader levers and transactions players may get more information through collaboration exchanges with the game leader and other players and at the shared space while displaying a submersion event 4 2 4 individual prediction the model does not implement any form of automatic prediction only the human intelligence of the game participants predicts future events and the consequences of current decisions 4 2 5 interaction interactions among model components follow the conceptual model and relationships between agents figs 7 9 network entities of each model allow to exchange data through remote communications fig 10 all player actions and leader commands financial transactions and levers pass through the game manager except textual messages from leader to players at each game round the game manager updates the environment and sends back the current state to the game leader and players players of different districts can collaborate and interact to develop collective strategies global interactions take place at the shared space when visualizing submersion results additional exchanges can also occur between players and the game leader these interactions aim at reorienting strategies through advice technical expertise and initiating collaborations 4 2 6 collectives agents land use units and coastal defenses of the same district form collectives and are affected by the same decisions budget limits and applied levers players of the same district form a human group that deliberates to make decisions different playing teams may also set an inter district collective while discussing collaborative projects and strategies 4 2 7 heterogeneity agents differ by their state variables each district has its geographical configuration budget and population that evolve differently game players are also heterogeneous and make decisions differently based on their objectives perceptions and personal experiences 4 2 8 stochasticity simulation parameters are read from predefined configuration files see initialization block under the details section only three parameters are randomly attributed each coastal defense object dike or dune has an initial number of game rounds before updating its state this counter parameter is randomly initialized between 0 and the maximum number of rounds to change the state steps degrade status dike or steps degrade status dune each coastal defense object has a probability of breach rupture that is assessed randomly and according to the predefined values for each state after each flooding event the game manager sends a random number between 1 and 5 of submerged land use units to each district as flood marks these landmarks give partial information about the extent of the submersion 4 2 9 observation the game manager has a set of real time indicators used at the end of the game for discussion and debriefing some of these indicators are also available for the game leader to optimize his decisions players have a limited view of the environment and cannot see the global information until they are in the shared space with the game manager 4 3 details 4 3 1 implementation details littosim gen includes several distributed tools combined to allow playing the game between different actors implied in coastal flooding management the agent based model is implemented under the gama platform taillandier et al 2019 and uses empirical data to display realistic views of the study area flooding simulation uses the lifslood fp neal et al 2011 model to compute actual flood extent based on real submersion events network connections use apache activemq wrapper 5 5 https activemq apache org gama platform gama taillandier et al 2019 is an eclipse based modeling platform for developing spatial agent based simulations it allows manipulating vector and raster files and provides many features and libraries to ease coding and implementing large spatial agent models all devices implied in the game must install gama fig 11 the current littosim gen requires gama1 8 with java version 1 8 or later activemq littosim gen is a distributed game that can be played over the network using apache activemq broker models have networking agents that use a shared mailbox to exchange data through the mqtt protocol the activemq server is usually installed on the game manager fig 11 but can be in any other accessible machine on the network lisflood fp lisflood neal et al 2011 is a 2d hydrodynamic model that simulates inundations by calculating water elevation on a grid space according to a predefined flood event it must be on the manager fig 11 under windows or os x systems on a machine with powerful calculation capabilities 6 6 lisflood is better under os x with a macos mojave 10 14 3 macbook pro 2018 2 6 ghz intel core i7 16 go 2400 mhz ddr4 it needs 4 min to calculate the submersion on a 631 906 grid of 20 20 m2 lisflood needs a set of parameters about the reference event and the study area to submerge a bdy file stores the event scenario as a time series of water elevations a second bci file specifies to lisflood a set of geographical coordinates where the submersion will start to discharge inflow points finally lisflood needs a territory representing the study area where the submersion occurs two raster grids represent the territory a dem file as a grid of soil altitudes and a rugosity roughness file as a grid of manning coefficients resistance to water flow littosim gen produces these two grids each time the manager launches a flood event generated grids incorporate all player actions that change the nature of the soil therefore changing coastal defenses by creating raising or destroying dikes and dunes affects soil altitudes dem while land use management changing the type of a unit affects the rugosity grid since the manning coefficients depend on the nature of the land cover as a result lisflood returns a set of grids 14 files with water elevation at each iteration the platform uses these grids to display the propagation of the submersion event in 14 timesteps fig 12 4 3 2 initialization littosim gen reads simulation data and parameters from five configuration files littosim conf this is the main configuration file and contains general parameters such as network server address default language lisflood path and paths towards the other configuration files study area conf it is the configuration file storing all specific parameters to each study area territorial archetype such as paths of shape and raster files simulation parameters actions and levers details etc applying littosim gen to a new case requires configuring a new study area configuration file actions conf this file is also specific to the study area and contains parameters cost delay description of available player actions build a dike change a land use unit from n to a etc each player can have a different configuration of actions player actions implemented in the current littosim gen are shown in table 1 levers conf it is a specific file for each study area and includes available levers with their parameters threshold cost delay that can be enabled or disabled for each district in the game table 2 lists the currently supported levers langs conf this file gives each displayed message the translation in different languages supported by littosim gen currently french english and vietnamese each model can have a distinct language 4 3 3 input data this paper describes the data of the case study of rochefort data preparation for littosim gen is detailed in laatabi et al 2020 data overview data represent the districts of rochefort estuary in southwest france administrative and topographical data correspond to the ign national institute for geographic and forest information bd topo database land cover data are from the european union corine land cover 2018 and local urban planning plu data are provided by local authorities protected natural areas and risk prevention plan ppr files are from the local french state coastal defenses data are from the local authorities and the artelia group engineering office and lisflood data scenarios are based on the xynthia storm bertin et al 2014 geospatial data are reprojected into the rgf93 lambert 93 coordinate reference system epsg 2154 data structure table 3 lists data files required by the littosim gen model these files are recorded in the study area conf configuration file and then loaded into the simulation as input data table 4 depicts the data structure of each file attributes and their corresponding values besides these attributes all files have a default auto generated numeric id to identify each spatial object in the shapefile data mapping the mapping scheme in fig 13 represents entities i in blue the original structure of rochefort data ii in yellow the target data structure mapping patterns gray ovals summarize data transformations to the littosim gen structure basic patterns are self explaining e g rename is a simple operation of copying and renaming the original data complex transformation patterns are identified with their output name file or attribute and explained in the next section data patterns more details about data mapping and transformation patterns in littosim gen are in laatabi et al 2020 data patterns districts represents the districts of the study area estuary of rochefort dist area generates the area of a district by calculating the area of its polygon buffer in 100 represents a bounding zone of 100 m inside the unified area of districts and is used to identify a coastal defense dike as littoral or inland retro dike convex hull generates a convex hull specifying the global study area for the simulation as a rectangular closure envelope around districts buildings represents the buildings of the study area bld type since only residential buildings are relevant this attribute contains two values residential other created by a pattern of replacement by replacing résidentiel with residential and all other values with other bld area calculates the area of the building based on its polygon topology the topological verification of buildings file consists in splitting polygons intersecting with several land use units and deleting buildings in natural areas urban plan represents the land use planning plu that determines the exploitation type of local areas unit code specifies the type of each area in urban planning based on the original file with a type conversion text to numeric and replacement of values as follows n nh na 1 u us 2 auc aub au aus 4 a az ax 5 land cover determines the land cover type of the area based on the european clc database cover type specifies the type of each zone in the study area this attribute can take 44 standardized values 111 242 523 representing various land covers forests airports rice fields etc land use is generated as a grid cell of a specified size 200 200 m2 for rochefort covering the area of the active districts this grid is the principal component allowing planning and land use management through editing cell units in littosim gen unit code is created through an intersection between the land use unit one cell of land use and the urban planning file urban plan it specifies the planning type of the cell and initially takes four values 1 natural n 2 urban u 4 authorized for urbanization au and 5 agricultural a this variable may change during the game and take other values 6 adapted urban us 7 authorized for adapted urbanization aus sub type this attribute is created through an intersection with the land cover file to specify the sub type of a land use unit for example in some cases agricultural areas may have various sub types dist code affects the district code to each land use unit through an intersection with the districts file the model splits boundary cells to fit within districts unit area calculates the area of each land use unit for ergonomic purposes players cannot select small scale cells a cell smaller than a specified minimum size 20 000 m2 for rochefort is merged with the neighbor that shares the longest border with it unit pop uses aggregate and intersect patterns to calculate the population of each land use unit it takes the sum of the area of residential buildings intersecting the cell bld area divides it by the sum of the areas of residential buildings of the district and multiplies the result by the total population of the district dist pop littosim gen considers that the population rate is the proportion of residential buildings within each cell equation 1 1 r e s i d b l d s b u i l d i n g s b l d t y p e residential u n i t p o p b l d a r e a r e s i d b l d s l a n d u s e b l d a r e a r e s i d b l d s d i s t r i c t s d i s t p o p expro cost the expropriation transforming to natural cost of an urban cell depends on its population empty cells unit pop 0 take a predefined parameter empty expro cost in the study area conf file populated cells are expropriated with a cost that follows the next function equation 2 this cost increases less as the cell population goes up the number 400 represents the expropriation cost of a cell with one inhabitant 2 e x p r o c o s t u n i t p o p 400 u n i t p o p topology the topological verification of the land use grid consists of fixing cell merging issues and avoiding that a natural cell contains buildings or that an agricultural cell has populations agricultural cells may contain non residential buildings coast al defenses represents the dunes and the dikes that protect the coastline of districts from flooding typ e specifies the type of a coastal defense among the two possible values this attribute is created with a replacement of original values naturel cordon become dune and all other values become dike stat us determines the quality of a coastal defense that affects its probability of rupture during flood events this attribute is renamed and replaces old values as follows bon good moyen medium degraded rugosity the rugosity grid has the same dimensions as the dem grid 732 472 of 20 20 m2 cells for rochefort it is created based on a predefined set of rugosity coefficients manning depending on the land cover of the area covering each cell the initial rugosity grid is generated with the detailed land cover clc data cells updated during the game take simplified coefficients corresponding to the land use cell type in the case of rochefort we use the following coefficients natural n 0 11 urban u 0 05 authorized for urbanization au 0 09 agricultural a 0 07 adapted urban us 0 09 authorized for adapted urbanization aus 0 09 therefore each updated standard land use unit 200 200 m2 contains 100 rugosity cells 20 20 m2 with the same coefficient spa the protected areas file is renamed and its polygons merged to form a unified polygon this merge prevents potential geometry errors intersections rpp the risk areas file is also renamed and merged for the same reason as the spa file 4 3 4 submodels we describe the principal dynamics implemented implicitly in the littosim gen model namely the evolution of populations budgets and coastal defenses explicit player actions trigger all other dynamics population dynamics at each round littosim gen calculates the number of individuals to dispatch among different districts the model supports population growth and decline the dispatched number of people may be positive or negative this dynamic is computed as depicted in equation 3 first it calculates the population growth based on the annual pop growth rate parameter then adds an attraction factor to attract people according to the number of land use units in the densification process ui state 3 p o p t o d i s p a t c h d i s t r i c t a c c u m u l a t e each current population annual pop growth rate l a n d u s e w h e r e each is in densification pop immigration if densif the number pop to dispatch is then distributed randomly to urban land use units of the study area until it reaches 0 each urban unit takes a number according to its area and type au that becomes u takes pop for new u people ui takes pop for u densification and u us take pop for u standard individuals budgets the budget is first calculated based on the initial population and predefined tax table rates impot unit table then it is increased with a fixed bonus rate initial budget bonus during the game and at each new round a tax is calculated based on district population and the previous tax rates then this tax is added to the total budget equation 4 shows this dynamic for each district 4 tax unit impot unit table d i s t r i c t n a m e a t i n i t i a l i z a t i o n b u d g e t current population tax unit 1 initial budget bonus t h e n b u d g e t b u d g e t current population tax unit coastal defenses at each game round a counter is increased by one and the model checks all coastal defenses and updates the status of objects that reached one of the defined thresholds at each submersion event a coastal defense object has a probability of breaking depending on its state the study area configuration file is used to initialize all parameters dikes each dike has a counter specifying the number of remaining rounds before it degrades to the down status algorithm 1 the degradation occurs when this counter reaches steps degrade status dike dunes each dune has a counter specifying the number of remaining rounds before changing its status algorithm 2 a dune degrades to down status after steps degrade status dune game rounds if no sand fences are installed or when its accretion is not enhanced otherwise it upgrades to the upper status after steps upgrade status dune rounds a maintained dune does not update its status for steps maintain status dune rounds pebble dikes each pebble dike has several slices and loses nb slices lost per round at each game round two parameters determine the thresholds for status degradation depending on the number of remaining slices algorithm 3 the player action loading pebbles triggers the upgrade of pebble dikes 5 an application of littosim gen in this section we apply littosim gen to the case study of rochefort estuary coast in southwest france where the xynthia storm bertin et al 2014 caused considerable damage in 2010 applying the littosim gen model to a territory is a simplified process that requires only two tasks the first one consists in collecting raw data from various sources and transforming them to the input data structure of littosim gen tables 3 and 4 laatabi et al 2020 explain the data preparation process performing the automatic transformation based on xml r scripts and the sources are in appendix a 1 processed data files are included in the new project folder containing the related configuration files the second task is to parameterize those configuration files to use the newly prepared data the study area conf configuration file must include all general parameters concerning the study area such as the names of districts the resolution of raster grids and the paths towards source data files the actions conf must contain only relevant actions and relevant levers are in levers conf after configuring these files with the right parameters littosim gen is ready to run the new project 5 1 littosim gen platform with rochefort data the littosim gen platform offers multiple graphical interfaces for the three agent models game manager game leader and player we demonstrate the tool s features through examples from the rochefort case study in the game manager model the game control interface offers a set of buttons to manage various aspects of the simulation switch rounds pause and resume the game launch submersion events etc the flooding display fig 14 a uses the dem grid to show the altitude map of the territory where the submersion extent and ruptures of coastal defenses are displayed fig 14b shows the area of flooded cells by land use type and water height the blue scale color represents the three levels of water heights additional displays show the evolution of populations and budgets player actions and strategies land use and coastal defenses and the flooded area at each submersion event in the game leader model the main interface fig 15 a allows the risk agency to follow the activated levers for each district send messages and make financial transactions an activated lever can be validated canceled or disabled to prevent its activation until it is enabled again the actions interface fig 15b details the strategic tendency of each playing team by providing the number of each player action by strategy builder soft defense strategic withdrawal other interfaces allow the game leader to follow the principal indicators representing the evolution of populations and budgets of each district and the profiling of player actions the game leader uses these graphs to decide about levers financial transactions and other decisions used to orient and interact with players in the player model two different displays fig 16 allow players to manage the land use and coastal defenses the top left part of each interface shows the area where players can select spatial objects buttons on the top right display additional information on the map such as risk and protected areas landmarks of submersion events and the history of previously executed actions buttons on the top left present the available player actions at the top right side the basket lists all selected player actions that may be validated or canceled the history component at the bottom right of the window displays validated player actions received messages at the bottom provide players with information about their territory and about the current state of the game 5 2 littosim gen workshops and results littosim gen is designed to organize participatory simulations during workshops involving local stakeholders and decision makers fig 17 a playing teams are usually composed of two or three individuals technicians and members of the district council larger groups may generate extended discussions and hamper the planning of the workshop each district team takes place on a separate table from the others and meetings with the risk agency are controlled by a timekeeper who also facilitates the workshop to avoid losing much time in discussions to the detriment of playing the game van hardeveld et al 2019 the platform collects data about the study area land use and coastal defenses flood events player actions and game leader activities in parallel an observer team watches the game and collects additional data about participants actions interactions and behaviors further analysis using collected data and pre post workshop surveys allow obtaining more results about learning and adaptation strategies conclusions are communicated to all participants in the game through reports or debriefing the platform is implemented to be user friendly without neglecting principal aspects that give realistic results amalric et al 2017 the hydrodynamic model used by littosim gen lisflood neal et al 2011 calculates the submersion extent according to the reference event and the configuration of the supplied territory fig 17b shows for example a comparison between the actual and the simulated extent of the xynthia storm bertin et al 2014 in the estuary of rochefort this realism enhances the simulation results voinov et al 2018 particularly the relationship between player actions and the flooding damage discussions and debriefing during these workshops foster the social learning of participants van hardeveld et al 2019 by simulating scenarios where collaborative projects and collective management of the risk may be the efficient way to reduce flood damage and protect coastal populations and activities 6 discussion genericity in participatory simulation models this section discusses the experience of developing the generic version of littosim generic models are transparent and easier to extend to other contexts and use cases and their development can be carried out by teams other than the initial developers we structure this experience as recommendations to practitioners of participatory modeling and simulation 6 1 participatory conception local stakeholders must be involved in the conception and development of participatory simulations voinov et al 2018 their internal view of the problem external to the solution helps develop contextualized models easily applicable to the research question involving decision makers and domain experts guarantees the link with empirical observations representing real phenomena laatabi et al 2018 and makes the model observable for the local stakeholders klabbers 2009 this engagement allows the model to include various potential needs of final users in terms of features and functionalities stakeholders consider themselves as participants in the model creation ownership which may push them to take more responsibilities in its implementation and application to real problems motivation in the case of littosim gen stakeholders have contributed to various aspects of the contextualized model player actions and their details applied levers and their parameters game rules decision making process the evolution of populations and territories submersion scenarios empirical data etc these contributions concern the context of the model and its components that also shape its conception the generic architecture separates the context as an independent element from the model itself 6 2 model architecture the context is the set of elements that compose a case study it includes empirical data model dynamics and simulation parameters the context is independent of the model that may contain various contexts with different configurations fig 18 therefore a context is loaded dynamically into the model through external tools such as configuration files the separation between the context and the model allows one to apply the same model to different case studies with a simple reconfiguration and to edit the rules and the data context without modifying the model itself container a generic model is a container reusable with several contexts and it is the context specifications that define the model specificity however if a model is too generic it becomes difficult to be reused as more adaptation and configuration are needed to load new content developers must maintain the model within the reusability domain fig 19 this domain is the equilibrium between the genericity specificity of the model and the simplicity complexity of the context specification in this zone the generic model is reusable with other contexts without being complicated to configure very generic models need complex context specifications data configuration that hinder their reusability hence the complexity of the context specification adaptation and setting is a good measure of reusability this trade off between genericity specificity is similar to the one proposed by sun et al 2016 to develop mid level models as an intermediate between theoretical and empirical modeling and the one proposed by klabbers 2009 to adopt a game design that balances between generic specific and observable qualities the principal components of the model must be separated to avoid monolithic models and build a resilient model composed of many independent and related parts such models guarantee a high level of flexibility and fault tolerance developers can modify or replace a single component or add a new one independently from the rest of the model flexible links couple various parts with each other but keep their independence those coupling links manage the compatibility between model components and allow reorganizing the model s architecture without much effort littosim gen uses configuration files to load the study area context the externalization of data and parameters allow playing the same model as a different game in different territories the configuration of player actions and levers can differ between case studies or between players of the same game the three agent models game manager game leader player are implemented separately and the cessation of one component does not affect the others the connection between these components through the exchange of text messages allows any new role to retrieve exchanged data by connecting to the shared mailbox the multi component architecture also offers the possibility of playing the game with any number of players the hydrodynamic model lisflood is coupled with the agent based model through a grid based link fig 12 that may be modified to use any other compatible model this architecture allows a fully modular design of the simulation jaxa rozen et al 2019 6 3 model and data description genericity and reusability require other users and developers to understand the model therefore a detailed description explaining the structure and the dynamics of the model is needed a standard protocol is preferred to organize and disseminate model descriptions the odd protocol grimm et al 2020 for example presents the necessary information to understand replicate and reuse agent based models by explaining the model structure such descriptions address the model genericity complexity issue and drive any new use or adaptation to other contexts but to fully bridge the gap between the model and the context descriptions should also describe how data are used and transformed to suit the model structure the odd 2d protocol laatabi et al 2018 adds this feature to the standard odd and extends the description to the data model relationship when transforming raw data to the model structure is complicated more tools may be needed to foster the reusability of models for example algorithms or automated scripts that perform data transformation in the case of littosim gen this paper provided an odd 2d description to help readers understand the structure of the model in terms of theoretical concepts agent dynamics process scheduling and empirical data an automated script transforms data to the littosim gen format based on a mapping configuration file appendix a 1 this data compiler saves the time and effort necessary to use littosim gen with data from other territories 6 4 simulation use and handling participatory simulations designed for stakeholders or other non expert public must guarantee a high simplicity of use bakhanova et al 2020 game participants often highlight the ergonomics of the gameplay as a weakness of computer based participatory simulations becu et al 2014 facilitation capabilities voinov et al 2018 should target intermediate users workshop animators and final users game players hence control panels and outputs should use simple and user friendly interfaces to display only relevant information user manuals for both intermediate and end users are also a useful tool to disseminate platforms and ease handling them these manuals should include information about required material configuration installation setting and a user guide to manipulate the tool and get full access to its inputs outputs in littosim gen user interfaces give access to the functionalities of different models the control panel of the game manager allows to switch game rounds and launch submersion events multiple interfaces display additional game details fig 14 the game leader uses several indicators to supervise the game fig 15 players have a user friendly interface to manage their territory and trace their actions to optimize their future decisions fig 16 game players are initiated to the game during a warm up lap at the beginning of the workshop an ergonomist participates in the project to improve the efficiency of graphical interfaces finally user manuals and training courses serve to train future workshop animators 6 5 knowledge extraction and learning developed tools must collect data during games and workshops to fulfill the objective of participatory modeling and simulation hedelin et al 2021 such data are essential for post analysis using statistical and machine learning methods to extract knowledge and hidden patterns that cannot be observed in real time this task is complementary to the learning process initiated at the game time van hardeveld et al 2019 van der wal et al 2016 and helps to understand how the participants have responded to different situations the analyzed data contain quantitative and qualitative records collected by the platform remarks and notes taken by observers and potential pre post surveys voinov et al 2018 such data give an idea of how players have evolved and learned during the game in the case of littosim gen all the previous features are combined saved data observer notes surveys and a set of r scripts analyze collected data and visualize their principal aspects an additional quantitative analysis estimates how littosim gen contributes to learning and raising awareness about coastal risk management the structure of collected data and the protocol of data analysis must be maintained to keep the generic aspect of the platform and to be able to compare the results of different games with diverse territorial archetypes 7 conclusions and perspectives in this paper we described the littosim gen agent based model and platform this tool allows conducting workshops to foster learning about coastal flooding risk management externalized configuration parameters make littosim gen a generic and reusable model with new case studies new users can edit only three configuration files to reuse the platform with new data the resulting environment is a set of graphical interfaces that display the evolution of the study area based on player actions and scenarios of flooding events several game outputs provide necessary information to improve game efficiency and engage discussions and exchanges between participants in parallel the platform saves all results and game states for further analysis analyzing outputs of littosim gen is essential to detect latent aspects and hidden patterns littosim gen is a realistic gaming simulation that uses empirical data involves stakeholders and simulates actual submersion events the flooding model lisflood calculates the true extent of submersion based on the state of the territory and the predefined event scenario integrating hydrodynamic models into risk management platforms is not straightforward and makes accessing results difficult for non experts jaxa rozen et al 2019 zerger and wealands 2004 the architecture of littosim gen simplifies this task by using a grid approach the model currently uses lisflood but due to the modularity of its architecture it may integrate any other flooding model that fits into the coupling scheme fig 12 from this experience to generalize the littosim model we draw a set of recommendations to develop generic and reusable participatory simulation models separating model and context specifications multi component architecture and modularity model and input data description ergonomics of user interfaces and simulation handling and output data collection and parsing these recommendations foster the reusability of models and save time and effort when developing similar tools and solutions the future aim of the littosim gen project is to offer a generic and documented toolkit easy to reuse for coastal flooding simulation and risk management the model is currently applied to four different french territories oléron and camargue overflow coasts normandie cliff coast and rochefort estuary coast implementing the model by other users for different case studies is essential for validating applying littosim gen with different data users and experiences is a validation criterion of its genericity the validity of littosim gen concerns multiple components agent based gama and flooding lisflood models operating instructions and equipment and the participatory process related to the organization of workshops extending the flooding model lisflood simulates specific types of submersion and may not be suitable for all case studies hence other contexts such as continental or overtopping flooding may need a different hydrodynamic model littosim gen can be used in these contexts since its architecture allows to couple the platform with any other grid based submersion model improving testing the model by new teams other than its developers may lead to improvements and changes in its conceptual model architecture building blocks and the implication of stakeholders in the game popularizing by diffusing the model among the scientific community interested in coastal flooding management littosim gen will be reused and therefore validated extended and improved by third parties future developments of littosim gen will focus on more dynamical aspects such as the extensibility of the model components these features will add new functionalities and game rules through external configuration files without modifying model sources declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this research was conducted with the financial support of fondation de france and the nouvelle aquitaine region a appendix a 1 littosim gen sources the littosim gen model source code and data preparation scripts are available on github https github com littosim littosim model tree littodev a 2 algorithms algorithm 1 dike status degradation at each game round image 1 algorithm 2 dune status evolution at each game round image 2 algorithm 3 pebble dike status degradation at each game round image 3 
25656,coastal flooding is among the most significant disasters that threaten littoral populations in today s world climate change and global warming increase this risk and require implementing efficient strategies to anticipate the hazard and reduce human and material damage this paper describes a generic platform called littosim gen a coupling of agent based and hydrodynamic models which aims to involve stakeholders in a participatory simulation of land use management to minimize the risks of coastal submersion an application to the southwest coast of france rochefort demonstrates the features of this platform we propose several recommendations to improve the generic conception and implementation of models dedicated to participatory simulation keywords participatory simulation agent based model genericity littosim gen coastal flooding risk management 1 software and data availability the littosim gen model source code and data preparation scripts are available on github https github com littosim littosim model tree littodev littosim gen is developed by the littosim consortium a group of multidisciplinary researchers https littosim hypotheses org littosim gen is coded in gaml language under the gama platform https gama platform org software and hardware requirements to run littosim gen are detailed in the implementation details section of this paper 2 introduction since the 1960s the gaming simulation approach that combines system modeling and gaming interactions duke 2014 dukes et al 2011 stadsklev 1975 was used mainly for two purposes to foster learning and improve awareness learning games sorace et al 2018 or to support collective and deliberated decision making processes policy games mayer et al 2014 with the evolution of research and practices complex systems modelers have developed the participatory modeling approach gray et al 2016 voinov and bousquet 2010 in parallel video games have extended their application domain to the rapidly growing serious games since the 2000s michael and chen 2005 sawyer and rejeski 2002 participatory simulation ps is a gaming simulation approach that takes root more on the side of participatory modeling 1 1 some authors use the terms participatory simulation and serious game interchangeably assuming that the serious game is multiplayer and serves a participatory purpose in this paper we use the first terminology it uses gaming and models to develop frameworks for groups of participants that share control over decisions bakhanova et al 2020 becu 2020 these participants collectively build ideas abstractions concepts strategies and plans that are useful for the game ps is closely tied to the companion modeling approach étienne 2013 as they share the same participatory and empowerment objectives becu and crookall 2020 multiple research works have investigated the use of ps among different environmental disciplines hedelin et al 2021 van der wal et al 2016 implemented a model of water management where the game participants receive feedback depending on their chosen measures embankment changing land use evacuation etc ornetsmüller et al 2018 used an approach based on board games and cards to encourage farmers to change their cultivation strategies adam et al 2018 propose a ps to examine the role of communication in reducing casualties during wildfires briot et al 2017 developed a game for park managers to explore the impact of democratic practices and local governance on the biodiversity in protected areas in flooding management several applications are dedicated to various aspects of submersion risk sprite taillandier and adam 2018 is a game designed for students to explore diverse policies of coastal flooding management vigiflood adam and andonoff 2019 is a game for crisis communication during floods anycare terti et al 2019 is a role playing simulation that investigates how decision makers anticipate and respond to weather related events khoury et al 2018 propose a ps to understand flood prevention and improve resilience in mixed urban rural environments tanwattana and toyoda 2018 used gaming simulation as a learning tool to build community based disaster risk management some online games are also available on the internet such as stop disasters 2 2 https www stopdisastersgame org developed by the un office for disaster risk reduction participatory modeling and simulation which involve both models and stakeholders are becoming popular in environmental studies sterling et al 2019 van bruggen et al 2019 gray et al 2018 le page et al 2013 mayer 2009 that use agent based modeling to approach social economic and natural problems this increasing use of participatory modeling and simulation has revealed many issues concerning their methods and results some gaps highlighted by researchers and related to complex models are lack of comprehension dissemination and replication laatabi et al 2018 these limitations affect the genericity of ps models as stressed in the concluding chapter of the companion modeling book étienne 2013 therefore existing models and games are not usually reused or replicated to address similar questions in different contexts to investigate the issue of genericity in participatory modeling and simulation we use the littosim becu et al 2017 kit 3 3 the kit is composed of i an agent based model coupled with a hydrodynamic model ii a set of operating instructions to organize conduct and facilitate a workshop and iii a list of equipment to run the workshop a flooding risk management ps involving decision makers this toolkit is an agent based model initially developed for the territory of oléron island in france thus it faces considerable limitations since it is highly contextualized and hard to reuse in another context or even in the same context when the risk process or the field data are updated therefore we edited littosim gen generic littosim an extended version to make the model generic reusable and adaptable to other territories this work describes the architecture of the littosim gen platform as an attempt to address the lack of genericity in ps models the paper is structured as follows the following section presents the context of flooding risk management and its process in the french case and introduces the original version of the littosim model after that we describe the new generic version of the model following the structure of a standard protocol in the fourth section we present an example of an application of littosim gen to the estuary of rochefort france then we discuss the changes made to generalize littosim and we draw up a set of recommendations to develop generic and reusable participatory simulations the paper ends with a conclusion and some perspectives of this work 3 context and background flooding risk management in france global warming and climate change increase the risk of natural disasters such as coastal flooding wong et al 2014 large waves and storms can lead to catastrophic losses in coastal areas paprotny et al 2018 where populations are growing worldwide neumann et al 2015 low coastal zones become extremely dangerous for human activities especially when the sea level rise in concomitance with continental and pluvial flooding such scenarios amplify the risk and cause severe damage to exposed zones hence decision makers and urban planners must deal with this new socioeconomic challenge requiring them to mitigate the vulnerability and reinforce the resilience of their coastal cities in france efforts to address the problem of coastal flooding risk are getting more attention particularly after the xynthia storm struck the atlantic coast in 2010 bertin et al 2014 the management of this risk is the responsibility of local structures formerly municipalities district councils since 2018 that collaborate with the state services to elaborate plans and strategies to reduce flooding damage the risk prevention plan ppr is one of the tools used by municipalities to make decisions about land use and urbanization in areas exposed to natural hazards such as flooding the ppr zoning is associated with the local urbanization plan plu to provide decision makers with urban planning and management guidelines however depending on the territorial configuration risk prevention may not be managed at the micro scale and may require strategies that exceed the territory and the capacity of municipalities the inundation prevention action program papi is defined to allow an integrated risk management program at the macro scale inter district this program gathers several municipalities to develop collective and more efficient plans that are difficult to elaborate on the micro scale district flood damage may highly depend on individual choices and awareness of flood management dawson et al 2011 filatova et al 2011 this culture concerns local authorities and decision makers who manage strategic policies urban planners for example determine habitable and risk areas build coastal defenses and encourage or discourage residential choices and preferences of coastal populations hence developing a risk culture requires the implication of stakeholders in coastal flooding management training teaching and producing manuals are popular ways of learning and consolidating personal experience however modern technical developments allow more innovative and virtual methodologies to spread culture and improve awareness participatory simulations offer a simulated environment to put players in real world situations bakhanova et al 2020 in flooding risk management these tools foster social learning about coastal flooding by experiencing different scenarios to reduce the potential damage of submersion events littosim model becu et al 2017 is one of these computer simulation tools dedicated to local authorities to learn new and alternative strategies by testing different scenarios of preventive measures before the submersion event this toolkit focuses on participation and collaborative management of risk to rethink actual policies and overcome obstacles preventing the efficient handling of flooding problems littosim was developed for the territory of oléron island in southwest france and experienced in several workshops between 2015 and 2018 with the community of oléron districts amalric et al 2017 the simulation environment was designed in collaboration with local stakeholders and technicians according to their insights about the flooding problem therefore littosim includes a set of concepts rules and constraints that are specific to the original territory these specifications do not apply to other contexts e g the restrictions on building outside a defined urban ring the prohibition of changing urban areas to agricultural areas etc besides the model design includes some behaviors and data types that may be relevant only in oléron e g types of land use units types of coastal defenses installing sand fences process costs and delays of player actions etc these concepts are implemented through complex modeling aspects and data structures that are not generic thus understanding the model and adapting its parameters is challenging and makes littosim extremely complicated to reuse in a different territory hence generalizing littosim requires revising its structure to externalize independent components data inputs and simulation parameters 4 littosim gen a generic version of the littosim model in this section we follow the structure of the odd 2d protocol laatabi et al 2018 to describe all relevant concepts and necessary components to understand the model architecture and reproduce its results in different contexts for the sake of understanding we introduce the theoretical and empirical background element as a part of the overview block since describing general concepts theories and empirical basis underlying the model is necessary to understand the following blocks of the description 4 1 overview 4 1 1 purpose littosim gen is a coupling of an agent based model and a hydrodynamic model and implemented as a gaming simulation of submersion risk management participants of a littosim gen workshop whom we call players manage their territories and test alternative strategies on land use planning and coastal defenses to reduce flooding damage the game offers the possibility to experience different realistic management situations e g deciding to invest in habitat densification or protecting populations anticipating work delays due to regulatory constraints etc multiple actions e g building dikes withdrawing people adapting buildings etc are experienced and combined to assess their efficiency for protecting coastal populations activities and assets hence the principal aim of littosim gen is to help stakeholders and urban planners decision makers improve their risk culture and awareness by discovering new ways to optimize their decisions and collaborate with other stakeholders implied in the flooding risk question the current version of littosim gen proposes three territorial archetypes overflow coast estuary coast and cliff coast a territorial archetype is the typical example of a situation of inhabited space defined according to physical socio economic historical and governance characteristics rocle et al 2020 new archetypes can be implemented in littosim gen by including relevant data and updating parameter files 4 1 2 theoretical and empirical background littosim gen is composed of three agent based models representing the three roles of the game fig 1 manager is the model representing the study area and manages game rounds launches submersion events and communicates with the two other roles player represents decision makers of the districts of the study area that administer their territories through actions on land use units and coastal defenses players do not interact directly between them and their number is limited to four in each game for fluency reasons leader represents the role of a hypothetical risk agency 4 4 the risk agency collaborates with districts and local authorities to encourage collaborative management and initiate collective strategies the agency funds a part of the proposed projects and supervises their implementation to allow their continuity under governance practices and alternative approaches despite financial and political conflicts between various stakeholders that provides policy advice to players and manages meetings between them to set out collective strategies the leader participates in the game through financial giving and taking money and information transactions orientation expertise these interventions can alter the expected process of the game by changing conditions and constraints governing the application of player actions littosim gen uses empirical data from the case study these data are loaded into the model through several agents to implement different theoretical concepts that we introduce in the following paragraphs districts the study area may be composed of multiple districts fig 2 b each district determines the territory that a player team can manage a game is limited to four players risk areas the high risk zones are more subject to submersion during flooding events fig 2h the boundaries of these areas are determined by competent authorities and do not evolve during the game protected areas natural and special protected areas spa are forbidden from urban activities fig 2i non urban projects agriculture managing coastal defenses are subject to temporal and financial constraints due to the applied laws the boundaries of these areas are determined by authorities and do not evolve during the game soil cells soil cells are the micro level of the spatial scale usually 20 20 m2 or 10 10 m2 for high resolution grids in littosim gen fig 3 the soil cells grid plays three distinct roles the dem digital elevation model grid storing the soil altitude at each cell fig 2a the rugosity water resistance grid saving the corresponding manning value to the land cover of each cell fig 2j and the flooding grid displaying within each cell the water height during submersion events the altitude and rugosity of soil cells are not changed directly by players but updated indirectly through actions on land use units and coastal defenses fig 3 land use units a land use unit is a principal element of planning management in littosim gen fig 3 these macro cells are represented by regular squares or by irregular polygons when located at the boundaries of districts fig 2g according to the applied rules players may change the type of a land use unit fig 4 six types are currently implemented natural n this type corresponds to green zones like forests and meadows urbanizing in these areas may be allowed or prohibited when in protected areas non protected natural units can become agricultural or authorized for urbanization agricultural a these zones are open for human activities related to agriculture but not urbanizing to build in these areas they first need to become authorized for urbanization players can also transform them into natural areas authorized for urbanization au this type corresponds to previously natural or agricultural areas open to urbanization they can become urbanized u or adapted special au aus authorized for adapted urbanization aus authorities enhance infrastructures and subsidize flood adapted buildings this type is turned afterward to adapted urban us urban u this type contains residential buildings and can receive new populations it may become natural through expropriation n densified to attract more population ui or transformed to an adapted urban us area through the adaptation process adapted urban us it represents adapted urban areas with buildings that are more resistant to flooding risks urban in densification ui this transition state occurs when authorities take measures to encourage populations to settle in these zones densification continues until the unit reaches the next level of population density coastal defenses coastal defenses are spatial lines created to protect littoral areas from flooding when the sea level exceeds the land elevation after storms or large waves fig 2c building a new coastal defense increases the altitude of the soil by adding the height of the created object modifying this height raising affects the corresponding soil cells dismantling an object resets the soil altitude a coastal defense has a state good medium or degraded that degrades in time but can be upgraded with a repair action installing sand fences or loading pebbles during a flood event a coastal defense has a probability of rupture that depends on its state fig 5 depicts the state diagram of coastal defenses in littosim gen state dynamics depend on whether the object is a dike a dune or a pebble dike dike a dike is a human engineered barrier built with stones or other building materials dikes are littoral when installed directly on the coast or inland when constructed away from the coastline with a specified distance inside the area of fig 2d dikes are detrimental and non natural structures but inland dikes are less harmful to the environment dune a dune is a hill of sand constructed naturally or built by humans through for example installing sand fences a sand fence is a wooden barrier made of slats that regenerates a dune around it dunes are natural barriers and preferred to dikes for ecological reasons pebble dike a pebble dike is a natural or human engineered but without any other building materials dike of pebbles small or large pieces of rocks this barrier is also eco friendly as dunes and is often used to reinforce existing solid dykes player actions players can take several measures to reduce the submersion risk these actions are of two categories depending on their target object a land use lu or a coastal defense cd action each of these actions has a cost and a delay number of years rounds before being applied and may belong to different strategies depending on its application conditions see table 1 strategic profiles the principal aim of littosim gen is to allow decision makers to experience alternative strategies of flooding risk prevention the experience with earlier workshops of littosim becu et al 2017 has revealed three different player profiles based on their actions builder builders build coastal dikes and initiate urbanization and densification projects even in risk areas the builder profile opts for solid coastal defenses to protect coastal populations and assets soft defense players opt for more natural solutions to protect their coasts such as building and maintaining dunes and creating inland dikes for urbanization and densification projects this profile favors risk adapted areas strategic withdrawal this profile regroups retreat actions such as dismantling coastal defenses expropriating risk urban areas and urbanizing and densifying out of littoral and risk areas these three strategies classify a player that can be for example 50 builder 40 soft defense and 10 withdrawal based on his actions associating a player to a strategy depends on a specified threshold e g a player has a builder profile when he is at least 40 builder the classification of some actions within a strategy depends on the player profile e g classifying urbanization out of littoral and risk areas as a strategic withdrawal requires that the player already has a withdrawal profile this dynamic profiling anticipates player strategies an action may be categorized as other if it does belong to none of the three profiles e g densification out of risk areas but without being neither withdrawal nor soft defense table 1 depicts the implemented actions in the current littosim gen with their classification among different strategies each of these player actions may trigger one or several levers depending on a set of conditions and parameters levers a lever is a bonus or penalty intervention that rewards or penalizes a player based on his actions during the game different levers can be triggered after a certain threshold and applied if not canceled on player actions fig 6 there are two types of levers cost levers add to penalize or subtract to reward an amount to an action cost for example constructing a new dike after exceeding a fixed threshold penalty the action cost is marked up or installing sand fences on a dune bonus the action cost is marked down delay levers add or deduce a duration from an action delay for example creating an inland dike bonus the action delay is accelerated or urbanizing a land use unit in a coastal area penalty the action delay is retarded levers are implemented in littosim gen to encourage or discourage player actions depending on their ecological and virtuous strategies these levers fit into the three previous strategic profiles builder levers b concern dike building and urbanization actions soft defense levers s are applied on dunes inland dikes and adapted urbanization and withdrawal levers w concern actions of strategic withdrawing of urban populations and dismantling of dykes builder levers penalize and soft defense and strategic withdrawal levers reward players table 2 lists implemented levers in the current littosim gen 4 1 3 entities state variables and scales littosim gen simulates the territory evolution over a set of rounds usually 10 each game round represents one year in the real world the model is composed of three separate agent based models representing the three roles of the game we describe their conceptual structures separately each uml entity describes an agent game manager fig 7 depicts the structure of the game manager model as a class diagram with only relevant attributes it is composed of three categories of classes six core entities representing principal agents allow managing coastal defenses and land use units by applying received actions from players and activated levers from the game leader two network entities to communicate send and receive data with players and the game leader six display entities add additional information to the simulation such as roads rivers natural protected areas and flooding risk areas game leader three entities of the manager model district player action and activated lever are re implemented here to represent information that the two models share an activated lever is a lever triggered by an action and accepted not canceled by the leader according to its type a lever may change the delay of an action delay lever or change its cost cost lever several entities extend these two abstract classes to represent all levers introduced in table 2 one network agent performs communications with both the game manager and players fig 8 depicts the uml class diagram of this model with relevant attributes of each entity players player action is extended to represent the two types of player actions coastal defense action and land use action communications with the game manager are handled by network player and with the game leader by network listener to leader additional classes from the game manager provide players with all the information required to play the game and optimize their actions three new classes help manage the game interface on the player side basket allows storing current player actions name cost delay with the possibility of canceling history lists already validated player actions with their information final cost after taking into consideration applied cost levers remaining delay after considering applied delay levers message displays received information concerning the game rounds population budget messages from the leader validated actions and activated levers 4 1 4 process overview and scheduling a littosim gen game takes place over three phases at first the workshop animator explains game rules to the four playing teams in the second phase the game begins and players start to manage their territories in about ten rounds during which three or four submersion events occur the third phase is devoted to debriefing and discussing the results during the game the game manager launches submersion events and controls different components of the game rounds player actions and leader commands players decision makers representing the districts execute their planning actions on coastal defenses and land use units and submit them to the game manager the game leader validates or cancels the triggered levers and exchanges information with players financial transactions and text messages the activity diagram in fig 10 shows the three actors of littosim gen with their different interactions and activities within the model 4 2 design concepts 4 2 1 individual decision making levers are activated automatically based on player actions and activation parameters fig 6 and table 2 other agents do not implement any automatic decision making all decisions are made by humans implied in the game game manager based on a predefined scenario the game manager decides when to switch rounds and launch submersion events flooding events are planned according to the workshop time and the number of game rounds this scheduling should not be told to the playing teams game leader the risk agency has a set of indicators to help make proper decisions depending on the adopted strategies builder soft defense strategic withdrawal or others the budget of districts and the shared financial plan that players may want to set up these decisions are critical and can alter the game process hence the game leader must take the following actions with vigilance validating canceling levers a player can take actions that are too penalized or rewarded by automatic levers the game leader may intervene to correct this situation and apply or cancel some levers giving taking money the game leader can give or take money from some players to balance their budgets reorient their strategies or reward or penalize them if the impact of levers is biased sending messages information and warnings are sent to players to inform them about decisions concerning their budgets populations and strategies players each playing team makes decisions individually or with the risk agency and the other players some decisions may concern many districts and must be subject to a collective discussion that leads to a shared financial plan recommendations of the risk agency may affect the decision making process of players 4 2 2 learning model agents do not implement any form of automatic learning to optimize their decisions human agents namely the game leader and the players learn from their past experiences and the feedback of their actions discussions and face to face role playing reinforce social and horizontal learning solinska nowak et al 2018 voinov et al 2018 4 2 3 individual sensing the game manager and the game leader have a global perception of the game and use several indicators to optimize their decisions each player team has a partial perception limited to its environment and has no access to the actions of the game manager submersion events and the game leader levers and transactions players may get more information through collaboration exchanges with the game leader and other players and at the shared space while displaying a submersion event 4 2 4 individual prediction the model does not implement any form of automatic prediction only the human intelligence of the game participants predicts future events and the consequences of current decisions 4 2 5 interaction interactions among model components follow the conceptual model and relationships between agents figs 7 9 network entities of each model allow to exchange data through remote communications fig 10 all player actions and leader commands financial transactions and levers pass through the game manager except textual messages from leader to players at each game round the game manager updates the environment and sends back the current state to the game leader and players players of different districts can collaborate and interact to develop collective strategies global interactions take place at the shared space when visualizing submersion results additional exchanges can also occur between players and the game leader these interactions aim at reorienting strategies through advice technical expertise and initiating collaborations 4 2 6 collectives agents land use units and coastal defenses of the same district form collectives and are affected by the same decisions budget limits and applied levers players of the same district form a human group that deliberates to make decisions different playing teams may also set an inter district collective while discussing collaborative projects and strategies 4 2 7 heterogeneity agents differ by their state variables each district has its geographical configuration budget and population that evolve differently game players are also heterogeneous and make decisions differently based on their objectives perceptions and personal experiences 4 2 8 stochasticity simulation parameters are read from predefined configuration files see initialization block under the details section only three parameters are randomly attributed each coastal defense object dike or dune has an initial number of game rounds before updating its state this counter parameter is randomly initialized between 0 and the maximum number of rounds to change the state steps degrade status dike or steps degrade status dune each coastal defense object has a probability of breach rupture that is assessed randomly and according to the predefined values for each state after each flooding event the game manager sends a random number between 1 and 5 of submerged land use units to each district as flood marks these landmarks give partial information about the extent of the submersion 4 2 9 observation the game manager has a set of real time indicators used at the end of the game for discussion and debriefing some of these indicators are also available for the game leader to optimize his decisions players have a limited view of the environment and cannot see the global information until they are in the shared space with the game manager 4 3 details 4 3 1 implementation details littosim gen includes several distributed tools combined to allow playing the game between different actors implied in coastal flooding management the agent based model is implemented under the gama platform taillandier et al 2019 and uses empirical data to display realistic views of the study area flooding simulation uses the lifslood fp neal et al 2011 model to compute actual flood extent based on real submersion events network connections use apache activemq wrapper 5 5 https activemq apache org gama platform gama taillandier et al 2019 is an eclipse based modeling platform for developing spatial agent based simulations it allows manipulating vector and raster files and provides many features and libraries to ease coding and implementing large spatial agent models all devices implied in the game must install gama fig 11 the current littosim gen requires gama1 8 with java version 1 8 or later activemq littosim gen is a distributed game that can be played over the network using apache activemq broker models have networking agents that use a shared mailbox to exchange data through the mqtt protocol the activemq server is usually installed on the game manager fig 11 but can be in any other accessible machine on the network lisflood fp lisflood neal et al 2011 is a 2d hydrodynamic model that simulates inundations by calculating water elevation on a grid space according to a predefined flood event it must be on the manager fig 11 under windows or os x systems on a machine with powerful calculation capabilities 6 6 lisflood is better under os x with a macos mojave 10 14 3 macbook pro 2018 2 6 ghz intel core i7 16 go 2400 mhz ddr4 it needs 4 min to calculate the submersion on a 631 906 grid of 20 20 m2 lisflood needs a set of parameters about the reference event and the study area to submerge a bdy file stores the event scenario as a time series of water elevations a second bci file specifies to lisflood a set of geographical coordinates where the submersion will start to discharge inflow points finally lisflood needs a territory representing the study area where the submersion occurs two raster grids represent the territory a dem file as a grid of soil altitudes and a rugosity roughness file as a grid of manning coefficients resistance to water flow littosim gen produces these two grids each time the manager launches a flood event generated grids incorporate all player actions that change the nature of the soil therefore changing coastal defenses by creating raising or destroying dikes and dunes affects soil altitudes dem while land use management changing the type of a unit affects the rugosity grid since the manning coefficients depend on the nature of the land cover as a result lisflood returns a set of grids 14 files with water elevation at each iteration the platform uses these grids to display the propagation of the submersion event in 14 timesteps fig 12 4 3 2 initialization littosim gen reads simulation data and parameters from five configuration files littosim conf this is the main configuration file and contains general parameters such as network server address default language lisflood path and paths towards the other configuration files study area conf it is the configuration file storing all specific parameters to each study area territorial archetype such as paths of shape and raster files simulation parameters actions and levers details etc applying littosim gen to a new case requires configuring a new study area configuration file actions conf this file is also specific to the study area and contains parameters cost delay description of available player actions build a dike change a land use unit from n to a etc each player can have a different configuration of actions player actions implemented in the current littosim gen are shown in table 1 levers conf it is a specific file for each study area and includes available levers with their parameters threshold cost delay that can be enabled or disabled for each district in the game table 2 lists the currently supported levers langs conf this file gives each displayed message the translation in different languages supported by littosim gen currently french english and vietnamese each model can have a distinct language 4 3 3 input data this paper describes the data of the case study of rochefort data preparation for littosim gen is detailed in laatabi et al 2020 data overview data represent the districts of rochefort estuary in southwest france administrative and topographical data correspond to the ign national institute for geographic and forest information bd topo database land cover data are from the european union corine land cover 2018 and local urban planning plu data are provided by local authorities protected natural areas and risk prevention plan ppr files are from the local french state coastal defenses data are from the local authorities and the artelia group engineering office and lisflood data scenarios are based on the xynthia storm bertin et al 2014 geospatial data are reprojected into the rgf93 lambert 93 coordinate reference system epsg 2154 data structure table 3 lists data files required by the littosim gen model these files are recorded in the study area conf configuration file and then loaded into the simulation as input data table 4 depicts the data structure of each file attributes and their corresponding values besides these attributes all files have a default auto generated numeric id to identify each spatial object in the shapefile data mapping the mapping scheme in fig 13 represents entities i in blue the original structure of rochefort data ii in yellow the target data structure mapping patterns gray ovals summarize data transformations to the littosim gen structure basic patterns are self explaining e g rename is a simple operation of copying and renaming the original data complex transformation patterns are identified with their output name file or attribute and explained in the next section data patterns more details about data mapping and transformation patterns in littosim gen are in laatabi et al 2020 data patterns districts represents the districts of the study area estuary of rochefort dist area generates the area of a district by calculating the area of its polygon buffer in 100 represents a bounding zone of 100 m inside the unified area of districts and is used to identify a coastal defense dike as littoral or inland retro dike convex hull generates a convex hull specifying the global study area for the simulation as a rectangular closure envelope around districts buildings represents the buildings of the study area bld type since only residential buildings are relevant this attribute contains two values residential other created by a pattern of replacement by replacing résidentiel with residential and all other values with other bld area calculates the area of the building based on its polygon topology the topological verification of buildings file consists in splitting polygons intersecting with several land use units and deleting buildings in natural areas urban plan represents the land use planning plu that determines the exploitation type of local areas unit code specifies the type of each area in urban planning based on the original file with a type conversion text to numeric and replacement of values as follows n nh na 1 u us 2 auc aub au aus 4 a az ax 5 land cover determines the land cover type of the area based on the european clc database cover type specifies the type of each zone in the study area this attribute can take 44 standardized values 111 242 523 representing various land covers forests airports rice fields etc land use is generated as a grid cell of a specified size 200 200 m2 for rochefort covering the area of the active districts this grid is the principal component allowing planning and land use management through editing cell units in littosim gen unit code is created through an intersection between the land use unit one cell of land use and the urban planning file urban plan it specifies the planning type of the cell and initially takes four values 1 natural n 2 urban u 4 authorized for urbanization au and 5 agricultural a this variable may change during the game and take other values 6 adapted urban us 7 authorized for adapted urbanization aus sub type this attribute is created through an intersection with the land cover file to specify the sub type of a land use unit for example in some cases agricultural areas may have various sub types dist code affects the district code to each land use unit through an intersection with the districts file the model splits boundary cells to fit within districts unit area calculates the area of each land use unit for ergonomic purposes players cannot select small scale cells a cell smaller than a specified minimum size 20 000 m2 for rochefort is merged with the neighbor that shares the longest border with it unit pop uses aggregate and intersect patterns to calculate the population of each land use unit it takes the sum of the area of residential buildings intersecting the cell bld area divides it by the sum of the areas of residential buildings of the district and multiplies the result by the total population of the district dist pop littosim gen considers that the population rate is the proportion of residential buildings within each cell equation 1 1 r e s i d b l d s b u i l d i n g s b l d t y p e residential u n i t p o p b l d a r e a r e s i d b l d s l a n d u s e b l d a r e a r e s i d b l d s d i s t r i c t s d i s t p o p expro cost the expropriation transforming to natural cost of an urban cell depends on its population empty cells unit pop 0 take a predefined parameter empty expro cost in the study area conf file populated cells are expropriated with a cost that follows the next function equation 2 this cost increases less as the cell population goes up the number 400 represents the expropriation cost of a cell with one inhabitant 2 e x p r o c o s t u n i t p o p 400 u n i t p o p topology the topological verification of the land use grid consists of fixing cell merging issues and avoiding that a natural cell contains buildings or that an agricultural cell has populations agricultural cells may contain non residential buildings coast al defenses represents the dunes and the dikes that protect the coastline of districts from flooding typ e specifies the type of a coastal defense among the two possible values this attribute is created with a replacement of original values naturel cordon become dune and all other values become dike stat us determines the quality of a coastal defense that affects its probability of rupture during flood events this attribute is renamed and replaces old values as follows bon good moyen medium degraded rugosity the rugosity grid has the same dimensions as the dem grid 732 472 of 20 20 m2 cells for rochefort it is created based on a predefined set of rugosity coefficients manning depending on the land cover of the area covering each cell the initial rugosity grid is generated with the detailed land cover clc data cells updated during the game take simplified coefficients corresponding to the land use cell type in the case of rochefort we use the following coefficients natural n 0 11 urban u 0 05 authorized for urbanization au 0 09 agricultural a 0 07 adapted urban us 0 09 authorized for adapted urbanization aus 0 09 therefore each updated standard land use unit 200 200 m2 contains 100 rugosity cells 20 20 m2 with the same coefficient spa the protected areas file is renamed and its polygons merged to form a unified polygon this merge prevents potential geometry errors intersections rpp the risk areas file is also renamed and merged for the same reason as the spa file 4 3 4 submodels we describe the principal dynamics implemented implicitly in the littosim gen model namely the evolution of populations budgets and coastal defenses explicit player actions trigger all other dynamics population dynamics at each round littosim gen calculates the number of individuals to dispatch among different districts the model supports population growth and decline the dispatched number of people may be positive or negative this dynamic is computed as depicted in equation 3 first it calculates the population growth based on the annual pop growth rate parameter then adds an attraction factor to attract people according to the number of land use units in the densification process ui state 3 p o p t o d i s p a t c h d i s t r i c t a c c u m u l a t e each current population annual pop growth rate l a n d u s e w h e r e each is in densification pop immigration if densif the number pop to dispatch is then distributed randomly to urban land use units of the study area until it reaches 0 each urban unit takes a number according to its area and type au that becomes u takes pop for new u people ui takes pop for u densification and u us take pop for u standard individuals budgets the budget is first calculated based on the initial population and predefined tax table rates impot unit table then it is increased with a fixed bonus rate initial budget bonus during the game and at each new round a tax is calculated based on district population and the previous tax rates then this tax is added to the total budget equation 4 shows this dynamic for each district 4 tax unit impot unit table d i s t r i c t n a m e a t i n i t i a l i z a t i o n b u d g e t current population tax unit 1 initial budget bonus t h e n b u d g e t b u d g e t current population tax unit coastal defenses at each game round a counter is increased by one and the model checks all coastal defenses and updates the status of objects that reached one of the defined thresholds at each submersion event a coastal defense object has a probability of breaking depending on its state the study area configuration file is used to initialize all parameters dikes each dike has a counter specifying the number of remaining rounds before it degrades to the down status algorithm 1 the degradation occurs when this counter reaches steps degrade status dike dunes each dune has a counter specifying the number of remaining rounds before changing its status algorithm 2 a dune degrades to down status after steps degrade status dune game rounds if no sand fences are installed or when its accretion is not enhanced otherwise it upgrades to the upper status after steps upgrade status dune rounds a maintained dune does not update its status for steps maintain status dune rounds pebble dikes each pebble dike has several slices and loses nb slices lost per round at each game round two parameters determine the thresholds for status degradation depending on the number of remaining slices algorithm 3 the player action loading pebbles triggers the upgrade of pebble dikes 5 an application of littosim gen in this section we apply littosim gen to the case study of rochefort estuary coast in southwest france where the xynthia storm bertin et al 2014 caused considerable damage in 2010 applying the littosim gen model to a territory is a simplified process that requires only two tasks the first one consists in collecting raw data from various sources and transforming them to the input data structure of littosim gen tables 3 and 4 laatabi et al 2020 explain the data preparation process performing the automatic transformation based on xml r scripts and the sources are in appendix a 1 processed data files are included in the new project folder containing the related configuration files the second task is to parameterize those configuration files to use the newly prepared data the study area conf configuration file must include all general parameters concerning the study area such as the names of districts the resolution of raster grids and the paths towards source data files the actions conf must contain only relevant actions and relevant levers are in levers conf after configuring these files with the right parameters littosim gen is ready to run the new project 5 1 littosim gen platform with rochefort data the littosim gen platform offers multiple graphical interfaces for the three agent models game manager game leader and player we demonstrate the tool s features through examples from the rochefort case study in the game manager model the game control interface offers a set of buttons to manage various aspects of the simulation switch rounds pause and resume the game launch submersion events etc the flooding display fig 14 a uses the dem grid to show the altitude map of the territory where the submersion extent and ruptures of coastal defenses are displayed fig 14b shows the area of flooded cells by land use type and water height the blue scale color represents the three levels of water heights additional displays show the evolution of populations and budgets player actions and strategies land use and coastal defenses and the flooded area at each submersion event in the game leader model the main interface fig 15 a allows the risk agency to follow the activated levers for each district send messages and make financial transactions an activated lever can be validated canceled or disabled to prevent its activation until it is enabled again the actions interface fig 15b details the strategic tendency of each playing team by providing the number of each player action by strategy builder soft defense strategic withdrawal other interfaces allow the game leader to follow the principal indicators representing the evolution of populations and budgets of each district and the profiling of player actions the game leader uses these graphs to decide about levers financial transactions and other decisions used to orient and interact with players in the player model two different displays fig 16 allow players to manage the land use and coastal defenses the top left part of each interface shows the area where players can select spatial objects buttons on the top right display additional information on the map such as risk and protected areas landmarks of submersion events and the history of previously executed actions buttons on the top left present the available player actions at the top right side the basket lists all selected player actions that may be validated or canceled the history component at the bottom right of the window displays validated player actions received messages at the bottom provide players with information about their territory and about the current state of the game 5 2 littosim gen workshops and results littosim gen is designed to organize participatory simulations during workshops involving local stakeholders and decision makers fig 17 a playing teams are usually composed of two or three individuals technicians and members of the district council larger groups may generate extended discussions and hamper the planning of the workshop each district team takes place on a separate table from the others and meetings with the risk agency are controlled by a timekeeper who also facilitates the workshop to avoid losing much time in discussions to the detriment of playing the game van hardeveld et al 2019 the platform collects data about the study area land use and coastal defenses flood events player actions and game leader activities in parallel an observer team watches the game and collects additional data about participants actions interactions and behaviors further analysis using collected data and pre post workshop surveys allow obtaining more results about learning and adaptation strategies conclusions are communicated to all participants in the game through reports or debriefing the platform is implemented to be user friendly without neglecting principal aspects that give realistic results amalric et al 2017 the hydrodynamic model used by littosim gen lisflood neal et al 2011 calculates the submersion extent according to the reference event and the configuration of the supplied territory fig 17b shows for example a comparison between the actual and the simulated extent of the xynthia storm bertin et al 2014 in the estuary of rochefort this realism enhances the simulation results voinov et al 2018 particularly the relationship between player actions and the flooding damage discussions and debriefing during these workshops foster the social learning of participants van hardeveld et al 2019 by simulating scenarios where collaborative projects and collective management of the risk may be the efficient way to reduce flood damage and protect coastal populations and activities 6 discussion genericity in participatory simulation models this section discusses the experience of developing the generic version of littosim generic models are transparent and easier to extend to other contexts and use cases and their development can be carried out by teams other than the initial developers we structure this experience as recommendations to practitioners of participatory modeling and simulation 6 1 participatory conception local stakeholders must be involved in the conception and development of participatory simulations voinov et al 2018 their internal view of the problem external to the solution helps develop contextualized models easily applicable to the research question involving decision makers and domain experts guarantees the link with empirical observations representing real phenomena laatabi et al 2018 and makes the model observable for the local stakeholders klabbers 2009 this engagement allows the model to include various potential needs of final users in terms of features and functionalities stakeholders consider themselves as participants in the model creation ownership which may push them to take more responsibilities in its implementation and application to real problems motivation in the case of littosim gen stakeholders have contributed to various aspects of the contextualized model player actions and their details applied levers and their parameters game rules decision making process the evolution of populations and territories submersion scenarios empirical data etc these contributions concern the context of the model and its components that also shape its conception the generic architecture separates the context as an independent element from the model itself 6 2 model architecture the context is the set of elements that compose a case study it includes empirical data model dynamics and simulation parameters the context is independent of the model that may contain various contexts with different configurations fig 18 therefore a context is loaded dynamically into the model through external tools such as configuration files the separation between the context and the model allows one to apply the same model to different case studies with a simple reconfiguration and to edit the rules and the data context without modifying the model itself container a generic model is a container reusable with several contexts and it is the context specifications that define the model specificity however if a model is too generic it becomes difficult to be reused as more adaptation and configuration are needed to load new content developers must maintain the model within the reusability domain fig 19 this domain is the equilibrium between the genericity specificity of the model and the simplicity complexity of the context specification in this zone the generic model is reusable with other contexts without being complicated to configure very generic models need complex context specifications data configuration that hinder their reusability hence the complexity of the context specification adaptation and setting is a good measure of reusability this trade off between genericity specificity is similar to the one proposed by sun et al 2016 to develop mid level models as an intermediate between theoretical and empirical modeling and the one proposed by klabbers 2009 to adopt a game design that balances between generic specific and observable qualities the principal components of the model must be separated to avoid monolithic models and build a resilient model composed of many independent and related parts such models guarantee a high level of flexibility and fault tolerance developers can modify or replace a single component or add a new one independently from the rest of the model flexible links couple various parts with each other but keep their independence those coupling links manage the compatibility between model components and allow reorganizing the model s architecture without much effort littosim gen uses configuration files to load the study area context the externalization of data and parameters allow playing the same model as a different game in different territories the configuration of player actions and levers can differ between case studies or between players of the same game the three agent models game manager game leader player are implemented separately and the cessation of one component does not affect the others the connection between these components through the exchange of text messages allows any new role to retrieve exchanged data by connecting to the shared mailbox the multi component architecture also offers the possibility of playing the game with any number of players the hydrodynamic model lisflood is coupled with the agent based model through a grid based link fig 12 that may be modified to use any other compatible model this architecture allows a fully modular design of the simulation jaxa rozen et al 2019 6 3 model and data description genericity and reusability require other users and developers to understand the model therefore a detailed description explaining the structure and the dynamics of the model is needed a standard protocol is preferred to organize and disseminate model descriptions the odd protocol grimm et al 2020 for example presents the necessary information to understand replicate and reuse agent based models by explaining the model structure such descriptions address the model genericity complexity issue and drive any new use or adaptation to other contexts but to fully bridge the gap between the model and the context descriptions should also describe how data are used and transformed to suit the model structure the odd 2d protocol laatabi et al 2018 adds this feature to the standard odd and extends the description to the data model relationship when transforming raw data to the model structure is complicated more tools may be needed to foster the reusability of models for example algorithms or automated scripts that perform data transformation in the case of littosim gen this paper provided an odd 2d description to help readers understand the structure of the model in terms of theoretical concepts agent dynamics process scheduling and empirical data an automated script transforms data to the littosim gen format based on a mapping configuration file appendix a 1 this data compiler saves the time and effort necessary to use littosim gen with data from other territories 6 4 simulation use and handling participatory simulations designed for stakeholders or other non expert public must guarantee a high simplicity of use bakhanova et al 2020 game participants often highlight the ergonomics of the gameplay as a weakness of computer based participatory simulations becu et al 2014 facilitation capabilities voinov et al 2018 should target intermediate users workshop animators and final users game players hence control panels and outputs should use simple and user friendly interfaces to display only relevant information user manuals for both intermediate and end users are also a useful tool to disseminate platforms and ease handling them these manuals should include information about required material configuration installation setting and a user guide to manipulate the tool and get full access to its inputs outputs in littosim gen user interfaces give access to the functionalities of different models the control panel of the game manager allows to switch game rounds and launch submersion events multiple interfaces display additional game details fig 14 the game leader uses several indicators to supervise the game fig 15 players have a user friendly interface to manage their territory and trace their actions to optimize their future decisions fig 16 game players are initiated to the game during a warm up lap at the beginning of the workshop an ergonomist participates in the project to improve the efficiency of graphical interfaces finally user manuals and training courses serve to train future workshop animators 6 5 knowledge extraction and learning developed tools must collect data during games and workshops to fulfill the objective of participatory modeling and simulation hedelin et al 2021 such data are essential for post analysis using statistical and machine learning methods to extract knowledge and hidden patterns that cannot be observed in real time this task is complementary to the learning process initiated at the game time van hardeveld et al 2019 van der wal et al 2016 and helps to understand how the participants have responded to different situations the analyzed data contain quantitative and qualitative records collected by the platform remarks and notes taken by observers and potential pre post surveys voinov et al 2018 such data give an idea of how players have evolved and learned during the game in the case of littosim gen all the previous features are combined saved data observer notes surveys and a set of r scripts analyze collected data and visualize their principal aspects an additional quantitative analysis estimates how littosim gen contributes to learning and raising awareness about coastal risk management the structure of collected data and the protocol of data analysis must be maintained to keep the generic aspect of the platform and to be able to compare the results of different games with diverse territorial archetypes 7 conclusions and perspectives in this paper we described the littosim gen agent based model and platform this tool allows conducting workshops to foster learning about coastal flooding risk management externalized configuration parameters make littosim gen a generic and reusable model with new case studies new users can edit only three configuration files to reuse the platform with new data the resulting environment is a set of graphical interfaces that display the evolution of the study area based on player actions and scenarios of flooding events several game outputs provide necessary information to improve game efficiency and engage discussions and exchanges between participants in parallel the platform saves all results and game states for further analysis analyzing outputs of littosim gen is essential to detect latent aspects and hidden patterns littosim gen is a realistic gaming simulation that uses empirical data involves stakeholders and simulates actual submersion events the flooding model lisflood calculates the true extent of submersion based on the state of the territory and the predefined event scenario integrating hydrodynamic models into risk management platforms is not straightforward and makes accessing results difficult for non experts jaxa rozen et al 2019 zerger and wealands 2004 the architecture of littosim gen simplifies this task by using a grid approach the model currently uses lisflood but due to the modularity of its architecture it may integrate any other flooding model that fits into the coupling scheme fig 12 from this experience to generalize the littosim model we draw a set of recommendations to develop generic and reusable participatory simulation models separating model and context specifications multi component architecture and modularity model and input data description ergonomics of user interfaces and simulation handling and output data collection and parsing these recommendations foster the reusability of models and save time and effort when developing similar tools and solutions the future aim of the littosim gen project is to offer a generic and documented toolkit easy to reuse for coastal flooding simulation and risk management the model is currently applied to four different french territories oléron and camargue overflow coasts normandie cliff coast and rochefort estuary coast implementing the model by other users for different case studies is essential for validating applying littosim gen with different data users and experiences is a validation criterion of its genericity the validity of littosim gen concerns multiple components agent based gama and flooding lisflood models operating instructions and equipment and the participatory process related to the organization of workshops extending the flooding model lisflood simulates specific types of submersion and may not be suitable for all case studies hence other contexts such as continental or overtopping flooding may need a different hydrodynamic model littosim gen can be used in these contexts since its architecture allows to couple the platform with any other grid based submersion model improving testing the model by new teams other than its developers may lead to improvements and changes in its conceptual model architecture building blocks and the implication of stakeholders in the game popularizing by diffusing the model among the scientific community interested in coastal flooding management littosim gen will be reused and therefore validated extended and improved by third parties future developments of littosim gen will focus on more dynamical aspects such as the extensibility of the model components these features will add new functionalities and game rules through external configuration files without modifying model sources declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this research was conducted with the financial support of fondation de france and the nouvelle aquitaine region a appendix a 1 littosim gen sources the littosim gen model source code and data preparation scripts are available on github https github com littosim littosim model tree littodev a 2 algorithms algorithm 1 dike status degradation at each game round image 1 algorithm 2 dune status evolution at each game round image 2 algorithm 3 pebble dike status degradation at each game round image 3 
25657,hyper resolution land surface models can explicitly represent landscape scale heterogeneity but the complexity of representing finer scale processes and meeting computational needs makes them inaccessible to the general scientific community and limits their adoption in this work we present the multi layer canopy and 3d soil mlcan3d model which is a high resolution high fidelity physical ecohydrologic model that aims to maintain accessibility mlcan3d implements detailed physical process representations is accessible through graphical user interface and is tested using comparisons with other models and field data this work demonstrates the feasibility of using mlcan3d to produce simulations close to real systems and the potential of the model to perform virtual experiments to explore model results our goal is for mlcan3d to serve as a virtual laboratory that enables virtual experiments from the broader scientific community and contribute to our understanding of ecohydrologic process heterogeneity dynamics and interactions across scales keywords ecohydrologic modeling 3d flow model spatial variability graphical user interface 1 introduction modeling of ecohydrologic processes is important in quantifying the water resource dynamics such as prediction of floods and droughts and among others in understanding vegetation processes and ecosystem carbon fluxes accurate quantification of land surface processes is also important for weather and climate predictions because of the strong interaction and feedback between land surface and atmospheric processes walker and rowntree 1977 shukla et al 1990 fine scale ecohydrologic heterogeneity such as those related to topography soil moisture and water flux has been shown to significantly influence larger scale ecosystem water and energy fluxes le and kumar 2017 riley and shen 2014 vivoni et al 2010 runoff and streamflow arrigo and salvucci 2005 barrios and francés 2012 and atmospheric feedback and circulation nykanen and foufoula georgiou 2001 current models tend to focus on their ability to simulate large domains and computation efficiency they make sacrifices such as simplification of process representations reduction of process complexity and the range of processes captured and are often limited by computing hardware constrains maxwell et al 2015 le et al 2015 in this work we present a high resolution and high fidelity ecohydrologic model that focuses on detailed physical process representation ease of use and broad applicability to serve as a virtual laboratory it is aimed to enable the general scientific community to use it to explore the interactions between topographic variability and ecohydrologic processes where complex feedback between these processes lead to non linear ecohydrologic dynamics with recent advances in environmental data availability and computational capability there is a call for hyper resolution land surface models over the regional to global extent that can more explicitly represent ecohydrologic heterogeneity to better understand and predict the non linear interactions between spatial variability and ecohydrologic processes bierkens et al 2015 wood et al 2011 hydrologic and land surface models are starting to become available at the 100 m resolution for continental extent and 1 km resolution for global extent maxwell et al 2015 sutanudjaja et al 2018 de graaf et al 2017 subgrid scale processes such as those associated with micro topographic variability are generally simplified with subgrid parameterizations or ignored clark et al 2015 however current understanding and model representation of many ecosystem processes are developed and validated with sensor measurements at the meter scale wood et al 2011 and understanding and quantifying how processes behave at the 100 m scale remains an issue clark et al 2015 some hypothesize that current model formulations relating to ecohydrologic processes are applicable and meaningful up to 100 m resolution as the limit wood et al 2011 riley and shen 2014 however other works find that meter scale micro topographic features influence stream flow dunne et al 1991 frei et al 2010 thompson et al 2010 infiltration frei and fleckenstein 2014 le and kumar 2017 and vegetation mcgrath et al 2012 therefore micro topographic variability is important for better understanding of heterogeneous ecohydrologic processes and how they behave at larger scales in order to better understand the complex interactions between topographic variability and ecohydrologic processes we need models that can capture the influence of micro topographic variability and represent physical processes that couples heterogeneity with other ecohydrologic dynamics while such models are starting to emerge the increases in model resolution fidelity of process representation and range of processes represented incur significant computational costs many models prioritize the ability to simulate large domains and efficient computation maxwell et al 2015 le and kumar 2017 and they are effective for their purpose and necessary for advancement of the science however to meet computation needs they may simplify process representations and reduce process representations and couplings they also extensively leverage advanced hardware and computation methods and thus tend to be less accessible to general scientific community due to the learning curve needed to set up the models complexity of the code and limiting knowledge to exploit advanced hardware such as hybrid computing involving both cpu central processing units and gpu graphical processing units le and kumar 2017 due to such complexity these models are also often only tested on individual or limited number of sites baatz et al 2018 further limiting their adoption by general users in order to stimulate scientific exploration of how fine scale landscape heterogeneity affects ecohydrologic process dynamics a model that overcomes the above issues and focuses on physical process representation greater accessibility and wider applicability is needed therefore in this work we present the multi layer canopy and 3d soil mlcan3d model a high resolution high fidelity but easy to use ecohydrologic model for simulating heterogeneities from micro topographic scale to larger scales and their interactions with coupled ecohydrologic processes our goal is to provide an accessible model for the general scientific community that can be used as a virtual laboratory for testing scientific hypotheses investigating knowledge gaps and understanding ecohydrologic process interactions and dynamics fatichi et al 2016 in mlcan3d we implement topography aware surface and subsurface moisture dynamics with 2d diffusive overland flow and 3d terrain following richards equation for subsurface flow we model vegetation energy and above ground moisture processes with an advanced ecohydrologic process model the multi layer canopy mlcan model that has been tested on a wide range of ecosystems drewry et al 2010a b quijano et al 2012 2013 quijano and kumar 2015 we tightly integrate our state of the art 3d flow model with mlcan through root soil moisture exchange and fully coupled surface flow and land surface water and energy processes to keep the model accessible we improve upon the graphical user interface gui developed for mlcan le et al 2012 to facilitate model setup and we use the advanced iterative alternating direction implicit aiadi method an et al 2011 douglas and rachford 1956 to solve the subsurface flow so that the model can be run without stringent computing hardware constraints we simulate two very different ecosystems to test and demonstrate the broad applicability of mlcan3d with the availability of extensive ecohydrologic observations from observational networks such as the critical zone observatories czos brantley et al 2006 the long term ecosystem research lter network callahan 1984 and the national ecological observatory network neon national research council 2004 we have the ability to develop simulations that capture real world behavior with high fidelity baatz et al 2018 based on these simulations we can then perform virtual experiments to gain understanding about processes in temporal and spatial extents and resolutions that cannot be measurement in the field fatichi et al 2016 or guide the design of new experiments by focusing on process representation accessibility and wide applicability we believe that mlcan3d can serve as a virtual laboratory that encourage such experiments by the scientific community in section 2 we first discuss the theory numerical implementation and benchmark verification of the coupled surface subsurface 3d flow model that captures topographic heterogeneity next we discuss the integration of the flow model with the mlcan model in section 3 then in section 4 we apply the model to two very different neon sites to test and demonstrate model capability we close with sections 5 and 6 with discussion of model sensitivity results and next steps 2 surface subsurface 3d flow model 2 1 3d subsurface flow in many diverse fields of study richards equation is seen as the standard for modeling water flow in the near surface soil layers hillel 2013 there are three forms of the equation used in existing studies the h based form solving for pressure head the θ based form solving for soil moisture and the mixed form using both pressure and moisture as dependent variables studies have shown that the mixed form richards equation is perfectly mass conserved using finite difference and gives better performance over the other forms at no additional computational cost celia et al 1990 an et al 2011 therefore in this work we use the mixed form richards equation that is generally written as follows celia et al 1990 1 θ t k ψ ψ k z here θ is the volumetric soil moisture content k ψ is the unsaturated hydraulic conductivity which depends on soil matric potential ψ the relationships between k θ and ψ are calculated using the well known soil moisture retention curve from van genutchen van genuchten 1980 and the unsaturated hydraulic conductivity function from mualem 1976 2 θ θ θ r θ s θ r 1 1 α ψ n v m v 3 k θ k s θ 1 2 1 1 θ 1 m v m v 2 θ r θ s are the residual and saturated soil moisture k s is the saturated hydraulic conductivity α is a parameter corresponding to the inverse air entry value n v is the pore size distribution and m v 1 1 n v all parameters can be found based on soil type or composition to generalize the richards equation to be applicable to model a natural environment we modify equation 1 to account for plant root uptake elastic storage and topography as follows 4 θ t θ φ s s h t k ψ h cos ϑ sin ϑ q t the inclusion of flux due to transpiration q t accounts for vegetation root uptake θ φ s s h t accounts for elastic storage bear et al 1979 where φ is the porosity and s s is elastic storage coefficient to account for changes in elevation within the domain we use h the total pressure head where h ψ z ψ is the soil matric head as previously defined and z is the gravity head the elevation above a given datum we further apply a terrain following transform to better account for topography in the domain maxwell 2013 childs 1971 sloan and moore 1984 brutsaert 1994 a representation of the 3d subsurface model with terrain following transform is shown at the top of fig 1 the terrain following transform h cos ϑ sin ϑ modifies horizontal fluxes to follow local topographic slope with angle ϑ assuming local slope is constant over depth angles of local slope in the horizontal directions are calculated as ϑ x tan 1 z x and ϑ y tan 1 z y since there is no need to account for slope for vertical fluxes ϑ z 0 compared to traditional orthogonal fluxes the terrain following implementation improves in accuracy and is better suited to modeling complex terrains or at coarse spatial resolutions maxwell 2013 we solve equation 4 using backward euler for the time discretization and finite difference for space discretization the bottom of fig 1 shows the finite difference stensil in the x z plane since the richards equation is non linear due to the relationship between θ k and h we use the modified picard method celia et al 1990 to linearize the equation and solve each time step iteratively please refer to appendix a 1 for more details on the discretization of equation 4 2 1 1 aiadi due to considerations for ease of use and computational effort an adi scheme is used to solve equation 4 one dimension at a time instead of the full implicit solution previous studies have compared adi with full implicit implementations for multi dimensional richards equation and found adi method to be faster with similar simulation results an et al 2011 however traditional adi methods peaceman and rachford 1955 rubin 1968 can encounter instabilities and difficulty in convergence when solving higher dimension problems clement et al 1994 to overcome these issues we use the advanced iterative alternating direction implicit aiadi method an et al 2011 douglas and rachford 1956 which was first developed for linear parabolic partial differential equations and is unconditionally stable in 2d and 3d douglas and rachford 1956 in non linear cases stability is not guaranteed but is improved over traditional adi methods an et al 2011 we adapt the aiadi method to solve the generalized richards equation 4 for natural environments our modified aiadi method uses three passes as follows 5 first pass in x direction θ n 1 3 m θ n δ t c n 1 3 m δ t i m k n 1 3 m h n 1 3 m 1 h n 1 3 m θ n 1 3 m φ s s δ t h n 1 3 m 1 h n x k n 1 3 m h x n 1 3 m 1 cos ϑ x sin ϑ x y k n 1 3 m h y n 1 3 m cos ϑ y sin ϑ y z k n 1 3 m h z n 1 3 m q t second pass in y direction c n 1 3 m δ t i m k n 1 3 m θ n 1 3 m φ s s δ t h n 1 3 m 2 h n 1 3 m 1 y k n 1 3 m h y n 1 3 m 2 cos ϑ y sin ϑ y y k n 1 3 m h y n 1 3 m cos ϑ y sin ϑ y y k n 1 3 m h y n 1 3 m 2 c o s ϑ y y k n 1 3 m h y n 1 3 m cos ϑ y and third pass in z direction c n 1 3 m δ t i m k n 1 3 m θ n 1 3 m φ s s δ t h n 1 3 m 3 h n 1 3 m 2 z k n 1 3 m h z n 1 3 m 3 z k n 1 3 m h z n 1 3 m where k i j k n k i 1 2 j k n k i 1 2 j k n k i j 1 2 k n k i j 1 2 k n k i j k 1 2 n k i j k 1 2 n and 6 i m 0 5 5 m here n is the timestep index and m is the picard iteration level index i m is an iteration parameter for the disturbance term in the aiadi scheme 0 55 is chosen in this work following previous studies an et al 2011 weeks et al 2004 each time step is iterated until h n 1 3m 3 h n 1 3m is less than the tolerance value given by the user 2 2 2d overland flow overland flow is simulated using the st venant equations which consists of the continuity equation and two momentum equations the continuity equation for 2d application is 7 w t x w u y w v q i q o 0 where w is the depth of water u and v are velocities in the x and y direction respectively and q i and q o are the inflow and outflow flux term contributing to the overland flow these terms in units of length time account for sources and sinks in the overland flow process and facilitate the exchange of water between overland and subsurface components of the model detailed physical processes included in each term is discussed in section 2 3 in this work we use diffusion flow to model the overland flow process while omitting the inertial terms in the st venant equations diffusion flow is still capable of accurately representing many natural flow situations lal 1998 fennema et al 1994 akan and yen 1981 hromadka and lai 1985 the momentum equations are reduced to 8 h x s f x a n d h y s f y where h w z is the depth of the water w in addition to surface elevation z above a given datum thus s fx s fy the friction slopes are the same as the slope of the water surface s w in 2d where s w s f x 2 s f y 2 combined with manning s equation flow velocity can be expressed in terms of h hromadka and lai 1985 9 u w 2 3 n m s w h x d h h x 10 v w 2 3 n m s w h y d h h y where n m is the manning s coefficient and d the diffusion coefficient is expressed as d w 5 3 n m s w for s w s m i n a n d w w m i n 0 otherwise parameters s min and h min are both user defined s min is used to keep d within a finite limit and w min facilitates wetting and drying using equations 9 and 10 the governing equation based on continuity equation 7 is written as 11 h t x d h x y d h y q i q o the overland flow equation can be linearized and solved with a wide variety of approaches lal 1998 in this work we use a linearized implicit method where equation 11 is linearized using explicit d and then solved using the implicit backward euler and finite difference in 2d please refer to appendix a 2 for the discretization of equation 11 2 3 surface subsurface coupling infiltration and evaporation determined by surface processes and subsurface moisture conditions couple the overland flow and subsurface components of the 3d flow model the amount of water exchanged between the two model components is calculated at each timestep it is included in the sink term in the overland flow model q o in equation 11 and it is a source to the subsurface model by serving as the top boundary condition we use a switching top boundary condition in the subsurface where depending on moisture conditions the top boundary of any cell is dirichlet or neumann paniconi and wood 1993 camporese et al 2010 2014 sulis et al 2010 le et al 2015 dirichlet boundary condition applies when infiltration and evaporation becomes limited by soil moisture conditions such as in the case of saturation excess otherwise neumann boundary condition applies precipitation contributes to the overland flow model through the term q i from equation 11 and excess water that does not infiltrate into the soil is included in q i at the next timestep of the simulation additional processes contributing to sources and sinks for the coupled 3d flow model are discussed in section 3 where the 3d flow model is integrated with a 1d ecohydrologic model 2 4 flow model benchmarks we use a set of benchmarks as preliminary tests for our coupled 3d flow model these benchmark simulations have been established through previous works and are designed to compare the physical responses of models kollet and maxwell 2006 sulis et al 2010 maxwell et al 2014 we use two established test cases infiltration excess and saturation excess to examine the most prevalent hydrologic responses and the interaction between overland flow and subsurface flow components of the model both test cases use a domain that slopes in one direction as shown in fig 2 the soil depth is 5 m with no water flowing through any boundaries of the domain except the outlet on the right side as depicted in fig 2 they both use the same van genuchten parameters except for saturated hydraulic conductivity k s based on values for sandy loam soil estimated by schaap and leij 1998 both tests consists of a 300 min simulation that starts with 200 min of rainfall followed by 100 min of recession parameters used are shown in table 1 and the two cases differ in their parameterization for the saturated hydraulic conductivity and initial water table depth as follows 1 the infiltration excess case tests for runoff before the soil column is saturated due to rainfall rate that is higher than the infiltration rate therefore this case tests two saturated hydraulic conductivity k s values that are smaller than the rainfall rate shown in table 1 2 the saturation excess case tests for runoff when the soil column is saturated this is simulated with a k s that is larger than the rainfall rate two values for initial water table depth are tested given in table 1 fig 3 compares the outflow rate from our model with that of five other models for the infiltration excess benchmark test case our model generally agrees well with other models especially for the magnitude of the peak outflow and the recession curve our model matches other model outputs very closely or is within the range of variability of the other models the largest discrepancy occurs in the rising limb of the outflow for the low k s case our model produces a slightly steeper rising limb compared to other models tested and thus plateaus faster where the outflow is equivalent to rainfall rate one possible cause of this is numerical differences in the overland flow model implementation another possible cause is the difference in mesh size and timestep for our model based on the comparison of a similar test case with different mesh sizes and the analytical solution from kollet and maxwell 2006 we see that the analytical solution and simulations with very fine mesh tend to also have a steeper rising limb and plateau faster than models with coarser grids fig 4 compares the outflow rate from our model with that of other models for the saturation excess benchmark test case our model also agrees well with other models for this case similar to the infiltration excess test case our model matches other model outputs very closely or is within the range of variability of the other models for the peak outflow and the recession curve the largest difference occurs at the rising limb for the case with initial water table at 1 0 m where our model produces outflow slightly slower than other models and have a steeper rising limb similar to the infiltration excess case cause for the discrepancy is also likely a combination of differences in numerical implementation and mesh size same as that of the infiltration excess case however in general results from our model match with other models in the benchmark tests reasonably closely and show expected model behavior which provides confidence in the validity of our model 3 a virtual laboratory 3 1 canopy and flow model process integration in order to simulate land surface processes we integrate our flow model into the existing multi layer canopy mlcan model drewry et al 2010a b quijano et al 2012 le et al 2012 quijano et al 2013 quijano and kumar 2015 mlcan is a high fidelity high complexity 1d model that simulates above ground canopy processes by 1 fully coupling leaf biophysical processes including photosynthesis leaf stomatal conductance leaf boundary layer conductance and leaf energy balance 2 scaling from leaf to canopy level with a multi layer approach using sunlit and shaded leaf fractions for each layer and 3 resolving the vertical profiles of radiation water storage energy balance and co 2 flux mlcan also describes surface and below ground processes such as water storage and energy balance in the litter and snow layers root and soil water interactions that couples to photosynthesis soil heat transport and 1d soil water movement we integrate our 3d flow model with mlcan by replacing the original 1d soil moisture model with our 3d subsurface model and adding the 2d overland flow model as shown in fig 5 we maintain all process interaction in the original mlcan model such as soil water interactions with plant roots the transpiration flux q t in equation 4 is modeled as a sink term in the subsurface model and it is generally uptake of water by plants determined by the canopy transpiration however due to the hydraulic redistribution incorporated in the model amenu and kumar 2008 quijano et al 2012 where water travels through plant roots from wet to dry parts of the soil upwards or downwards q t can be both sink or source in different parts of the model domain at any timestep we also implement additional process interactions with the addition of the 2d overland flow model sinks in the overland flow model q o from equation 11 include evaporation and infiltration as described in section 2 3 to couple overland flow with subsurface processes evaporation from the overland flow model is integrated into mlcan canopy processes to further integrate the 3d flow model into mlcan surface and canopy processes q o includes the contribution of overland flow to water stored in the litter layer and precipitation which contributes to q i from equation 11 is determined by throughfall from the canopy model and uptake by the litter and snow model drainage of excess water from the litter and snow layer as well as overland flow contributes to infiltration into the subsurface the resulting multi layer canopy and 3d soil mlcan3d model has the capability to represent vegetation dynamics with high fidelity including acclimation response of vegetation to changes in atmospheric co 2 drewry et al 2010a b and its consequent impact in water and energy partitioning capture high resolution heterogeneity in the topography and subsurface and maintains the tight process interactions between vegetation and soil moisture fig 5 shows the schematic of process interactions of the mlcan3d model with the new coupled 3d flow model and associated process interactions highlighted in blue 3 2 model capabilities a detailed physical process model such as mlcan3d inevitably need a lot of data to set up and is computationally demanding to encourage the use of mlcan3d as a virtual laboratory by the broader scientific community we develop mlcan3d with an emphasis on accessibility where the model is easy to set up and understand as well as run without stringent computing hardware constraints we use a graphical user interface gui to guide users step by step through the model setup process the gui was first developed for mlcan le et al 2012 and is now modified to include setup of the coupled surface and subsurface flow components in setting up the model users first specify the simulation location as latitude and longitude plant species composition and vegetation structure through leaf area index lai and vertical leaf area density lad profile for each species then users have the ability to specify which modules to include in the simulation such as using the 3d flow model 2d overland and 3d subsurface or the 1d soil moisture module in the original mlcan model mlcan3d then takes available eddy covariance flux tower data as model forcings and uses lidar derived digital elevation models dems to characterize topography for the 3d flow model for initial soil moisture conditions users have the option to use a vertical profile that is homogeneous over the domain or input 3d data as a grid of user specified values similarly users can specify 3d heterogeneous soil parameters for soil moisture retention and hydraulic conductivity as described in equations 2 and 3 appendix b contains details on how to use the gui to set up the 3d flow model mlcan3d outputs water co 2 and energy fluxes for the canopy and the soil as well as vegetation dynamics and microclimate conditions in addition mlcan3d outputs moisture conditions from the 3d flow model including ponded water depth on the land surface and subsurface soil moisture since we use the advanced iterative alternating direction implicit aiadi method an et al 2011 douglas and rachford 1956 to solve the subsurface flow the model is significantly less memory intensive than fully implicit methods and is easily parallelized therefore mlcan3d can be run on personal computers for small virtual experiments or in highly parallel computing environments for large experiments with the help of the gui and limited hardware constraints mlcan3d can be more accessible to users from broader range of backgrounds and encourage virtual experiments to answer more questions from diverse disciplines mlcan3d is open source and available for download at https github com hydrocomplexity mlcan3d 4 model application mlcan3d can be applied to a wide range of ecosystems to demonstrate its applicability we simulate without calibration two very different ecosystems to test mlcan3d in real world situations using data from neon national ecological observatory network national research council 2004 neon funded by nsf consists of a network of long term data collection facilities that provides comprehensive data for quantifying land surface ecological processes neon has sites across the us and covers a range of ecosystems and climates they collect long term open access data and provide more than 175 data products for ecologic and biological studies and their standardized data collection and processing protocols can provide comparable data across different sites at each neon site an eddy covariance flux tower collects weather data needed as forcings for mlcan3d such as radiation precipitation air temperature and wind speed the tower also collects ecosystem fluxes of water energy and carbon which can be used to compare with model output at each site there is also an array of soil plots near the tower fig 6 that have sensors at various depths in the soil to measure soil variables such as water content salinity and temperature neon also collects processes and provides high resolution airborne remote sensing data including hyperspectral lidar and digital photography for each site around the time of peak greenness each year hyperspectral and lidar remote sensing products are provided at 1 m resolution the hyperspectral data products include leaf area index lai and lidar data products include canopy height chm and surface elevation dem observational samples are also taken for a variety of ecosystem factors such as plants soil and organisms relevant data include plant species soil texture and litter layer information we apply mlcan3d to two sites in order to test model performance for different conditions of vegetation topography and soil textures neon datasets used to set up model simulations and verify model results are listed in table 3 in the appendix 4 1 ordway swisher biological station site the ordway swisher biological station osbs site is located in central florida lat long 29 689 27 81 993 43 mean annual precipitation is 1290 mm with more rain in the summer months it consists of fairly homogeneous evergreen forests dominated by longleaf pine pinus palustris within the eddy covariance flux tower airshed average canopy height based on airborne lidar data is 23 m with relatively open canopy and low leaf area index lai of 0 79 ornl daac 2018a myneni et al 2015 we use 1 m resolution dem from airborne lidar to characterize the topography of the site for the 3d flow model while the osbs site has limited elevation changes it consists of very fast draining deep sandy soils whose sharp changes in soil moisture conditions really test the numerical stability of the subsurface model according to soil sample measurements the osbs soil contains more than 95 sand and is therefore a sandy textured soil texture based soil parameters for equations 2 and 3 found in literature are used as model parameters as shown in table 2 dingman 2015 clapp and hornberger 1978 leij 1996 ghanbarian alavijeh et al 2010 in this work we model a 150 60 m 2 plot at 1 m 2 resolution covering all soil moisture sensor as shown in fig 6 the site has a slight topographic gradient of about 2 5 m going down slope from the north east to the south west we run the model for june 2018 at 30 min timesteps with a 10 day spin up period the 3d flow model s initial soil moisture condition is homogeneous with volumetric soil moisture of 0 1 and measured forcing data from may 2018 is used for the spin up period due to the fast draining nature of the soil initial soil moisture conditions do not significantly affect results after the spin up period the summary of model results for the simulation period is shown in fig 7 where we compare our model simulation with field measured data from neon in fig 7a we compare simulated plot average results of soil moisture and temperature with measured data from five soil plots at 6 cm depth corresponding to the first layer of 10 cm in our model we also display the vertical profile of plot average soil moisture over time we find that on the plot level average our model simulation is in good agreement with measured data though simulated soil temperature does not show diurnal variations as strong as that of the observed data in fig 7b we show topography of the plot and the heterogeneity of soil moisture over the plot at various depths here we observe that simulated soil moisture varies at fine scales that corresponds to heterogeneity due to micro topographic variability on the order of a few meters however soil moisture does not change significantly over the domain and the variance of the top layer soil moisture shown in 7b is less than what is observed by the five soil moisture sensors ploted in 7a this indicates that simulated soil moisture for the osbs site does not react to larger scale topographic features such as the gradual slope in the plot as much as what the soil moisture sensors measure in fig 7c we compare diurnally averaged simulated latent heat flux from the plot with that of the measured data from the flux tower we find that simulated latent heat flux is in good agreement with measured 4 2 oak ridge national laboratory site the oak ridge national laboratory ornl site is located in tennessee lat long 35 964 12 84 282 6 mean annual precipitation is 1222 mm with slightly more precipitation in the winter and spring it consists of mainly mixed deciduous forests dominated by oaks maples and hickories within the flux tower airshed with average canopy height of 28 m and lai of 5 2 ornl daac 2018b myneni et al 2015 as part of the ridge and valley appalachians the site is situated within five parallel ridges and valleys with dramatic variations in elevation therefore this site allows us to test the model s ability to capture the effects of topographic variations due to the large topographic variability at the ornl site soil texture also varies dramatically from sandy loam and silt loam to clay according to soil sample measurements from neon neon does not have soil sample measurements very close to the eddy covariance flux tower but we know from literature and measured data that the soil at the top of the ridges where the tower is tends to be more coarse and well drained compared to soil in valleys solomon et al 1992 therefore for this study we use silt loam soil with low porosity as described in solomon et al 1992 texture based soil parameters for equations 2 and 3 found in literature are shown in table 2 dingman 2015 clapp and hornberger 1978 leij 1996 ghanbarian alavijeh et al 2010 for the ornl site we model a 200 80 m 2 plot at 1 m 2 resolution covering all soil moisture sensor and the eddy covariance flux tower as shown in fig 6 the site has topographic changes of over 17 m over the plot much larger than that of the osbs site we run the model for june 2020 at 30 min timesteps with a 20 day spin up period initial soil moisture condition is homogeneous with volumetric soil moisture set to 0 2 to mitigate effects of the initial condition we use a longer spin up period to compensate for the slower draining soil at the site the summary of model results for the simulation period is shown in fig 8 where we compare our model simulation with field measured data from neon in fig 8a we compare simulated plot average results of soil moisture and temperature with measured data from five soil plots at 0 06 m depth corresponding to the first layer of 0 1 m in our model we find that on the plot level average our model simulation for the ornl site is also in good agreement with measured data though soil moisture does not seem to react as sharply to precipitation as the measured data also towards the end of june simulated soil temperature is slightly lower compared to measured and in general does not see as strong diurnal variations in fig 8b we show topography of the plot and the heterogeneity of soil moisture over the plot at various depths simulated soil moisture is able to capture soil moisture heterogeneity due to large scale topographic gradient as well as micro topographic variability however soil moisture simulation does not show as much spatial variability as is measured by the five soil moisture sensors in fig 8c we compare diurnally averaged simulated latent heat flux from the plot with that of the measured data from the flux tower we find that simulated latent heat flux is in good agreement with measured 5 discussion and model sensitivities the canopy component of mlcan3d is strongly affected by vegetation s physical properties and its photosynthetic parameterization important physical properties includes lai and canopy height which specify the structure of the vegetation important photosynthetic parameters for c3 plants as simulated in our study include maximum rubisco limited carboxylation rate v cmax and maximum electron transport rate j max a few parameters are important in facilitating the interaction of canopy land surface and soil soil moisture is sensitive to the amount of throughfall the precipitation that reaches the ground after canopy interception and the thickness of the surface litter layer since these components directly influence the amount of infiltration soil moisture is also affected by root water uptake as determined by vegetation properties discussed above and conductivity of roots manning s coefficient n m affects the overland flow and thus influence heterogeneity of infiltration into the subsurface soil moisture is most directly affected by the soil moisture retention curve parameters used in equations 2 and 3 simulation results tends to be the most sensitive to k s where water travels faster through soils with larger k s leading to sharper peaks during precipitation and faster drainage the residual and saturated soil moisture θ r θ s are important in bounding the range of soil moisture and larger θ s will lead to higher soil moisture and similarly for θ r when all other parameters remain constant n v and α also have small affects on soil moisture larger n v leads to drier conditions larger α produces slightly higher peaks during precipitation and less dry down leading to more water retention over time however the effect is not very significant when varying α within the reasonable range for the given soil texture we parameterize the application cases of our model using parameters found in existing literature when available and do not calibrate the model to fit measured data from the results we find that on average the model can closely estimate soil moisture levels compared to sensor measured values vertical movement of moisture in the soil column also seems reasonable from comparing with deeper layer results in the osbs site canopy level latent heat flux is also comparable with measured values based on current results the model performs well in general but have a few issues that warrants further discussion one noticeable difference between the simulated soil moisture and measured soil moisture curves is that sensor measurements register faster response to precipitation when compared with that of the model simulation we believe two factors contribute to the more rounded peaks in the simulated soil moisture one factor is that since the curve is an aggregation of the plot where the variable response from different locations in the plot are averaged the less sharp peaks reflects the variability within the plot another factor is that the modeled canopy and litter layer attenuation of the precipitation signal is not representative of individual sensor locations if a sensor is not located under vegetation then it does not experience attenuation of precipitation and will exhibit different behavior compared to locations under vegetation and the average behavior of the plot while simulated soil temperature is generally close to measured values and shows correct diurnal oscillations the magnitude of simulated oscillations are much less than that of the sensor measurements this difference may be due to the insulating effects of the litter layer or the limited parameterization of soil heat capacity based on soil texture in the model from figs 7b and 8b we see that while micro topographic effects on soil moisture are evident the variability of soil moisture across the study plots is not as significant as demonstrated by the measured data one possibility is that due to the homogeneous initial conditions we currently use for our simulations soil moisture gradient across each plot has not had time to form during the spin up period this can be tested with a longer simulation or heterogeneous initial conditions another is that there is notable heterogeneity in soil texture that our available data does not capture which influences soil moisture however there may also be special unknown conditions influencing individual soil moisture sensors for example soil moisture sensor 3 s3 in the ornl case measures soil moisture at around 0 1 barely above the residual soil moisture for the entire duration of the simulation and does not respond significantly to precipitation while simulated soil moisture at the location of sensor 3 is drier due to elevation fig 6 this extremely dry soil moisture measurements seems unlikely to be solely caused by topography similarly the low elevation of sensor 5 in the ornl case fig 6 does not corroborate its low soil moisture measurements and suggests additional factors that can contribute to the large variations we see in soil moisture measurements in general initial simulations using mlcan3d shows that models results are consistent with measured data on average but do not capture as much variability both in time and space this demonstrates there is still a need for further studies to understand and draw connections between in situ sensors measurements and models simulations using mlcan3d as a virtual laboratory we can conduct virtual experiments that explore this issue for example in this work we find that using topography as the only source of heterogeneity is not enough to capture the variability in the field we hypothesize that model parameterization may contribute to the discrepancy between measured and modeled soil moisture variability across the study plots therefore we can conduct a virtual experiment using user specified heterogeneous soil parameters to test this hypothesis however topography is readily available for large extents at high resolutions while soil parameters are not therefore we can also use mlcan3d to ask questions such as how topographic variability interacts with ecohydrologic processes to affect soil moisture and other ecohydrologic fluxes and to what extent can topography inform heterogeneity in ecohydrologic fluxes remotely sensed heterogeneity in vegetation which leads to variability in throughfall infiltration and water uptake can also be included in future works with the ability to model across scales from micro topographic to larger scales and across ecosystems mlcan3d can also be used to explore how model scale affects heterogeneous ecohydrologic process behaviors and their interactions at different scales by gaining better understanding and quantification of outstanding scale issues such as how fine scale dynamics amplify or attenuate as scale increases simulation experiments from mlcan3d can contribute to the hyper resolution land surface modeling effort in quantifying high resolution heterogeneity over large extents 6 conclusion in this work we presented mlcan3d an integrated ecohydrologic model that focuses on high fidelity physical process representation greater accessibility and wider applicability it couples existing advanced ecohydrologic process model mlcan with new state of the art topography aware 3d flow model mlcan models vegetation land surface energy fluxes and above ground moisture processes the 3d flow model simulates surface and subsurface moisture dynamics with 2d diffusive overland flow and 3d terrain following richards equation for subsurface flow the 3d flow model is in good agreement with other similar models when compared in standard benchmark tests and when coupled with mlcan the resulting mlcan3d model enables high resolution ecohydrologic modeling that accounts for micro topographic variability in the land surface to make mlcan3d easily accessible we provide a gui to facilitate the initialization and the full simulation of the model we also use the aiadi method for the 3d subsurface flow to reduces computation cost without sacrificing numerical stability the guidance from the gui and lack of stringent computing hardware constraints makes mlcan3d more accessible to the general scientific community thus encouraging wider adoption of the model we apply mlcan3d to two neon sites with very different characteristics to demonstrate the broad applicability of the model model results are comparable with field measured data at the plot scale and demonstrate the effect of micro topographic variations on soil moisture model simulations did not capture as much heterogeneity as demonstrated by in situ sensors a reflection of the fact that topography is not the only source of heterogeneity that contributes to variability of soil moisture in the field we aim to further explore this result using mlcan3d by investigating additional factors contributing to land surface heterogeneity as well as how topographic variability affects modeled ecohydrologic fluxes this work demonstrated the feasibility of using the mlcan3d model as a virtual laboratory that enables a range of virtual experiments by the broader scientific community and contribute to the advancement of our understanding of ecohydrologic process heterogeneity dynamics and interactions software and data availability mlcan3d is written in matlab and is tested for matlab version r2018b it is first made available in 2021 requires matlab to run and has no strict hardware requirements mlcan3d is open source and available for education and research uses at https github com hydrocomplexity mlcan3d developers contact information is provided in the mlcan3d readme on github declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgment this work was supported by the national science foundation grant numbers oac 1835834 ear 2012850 ear 1331906 and aci 1261582 the authors would like to acknowledge the national ecological observatory network as the source of field measured data used in this work we would also like to thank professor reed maxwell and his students for providing the data for the benchmark comparisons appendix a flow model discretization a 1 subsurface backward euler scheme is used for the time discretization and finite difference for space discretization since the richards equation is non linear due to the relationship between θ k and h the modified picard method celia et al 1990 is used to linearize the equation and solve each time step iteratively discretizing equation 4 with backward euler and picard linearization gives 12 θ n 1 m 1 θ n δ t θ n 1 m φ s s δ t h n 1 m 1 h n x k n 1 m h x n 1 m 1 cos ϑ sin ϑ y k n 1 m h y n 1 m 1 cos ϑ sin ϑ z k n 1 m h z n 1 m 1 q t where n is the time step and m is the iteration level of the picard iteration next as shown in celia et al 1990 dependent variable θ n 1 m 1 can be written in terms of ψ and h using its taylor series expansion about ψ n 1 m 13 θ n 1 m 1 θ n 1 m θ ψ n 1 m ψ n 1 m 1 ψ n 1 m o δ 2 θ n 1 m c n 1 m h n 1 m 1 h n 1 m notice that the temporal difference of ψ is equivalent to that of h because the gravitational potential component of h does not change in time and cancels out using c θ ψ and with higher order terms omitted we can plug equation 13 into equation 12 to replace θ n 1 m 1 14 θ n 1 m θ n δ t c n 1 m δ t h n 1 m 1 h n 1 m θ n 1 m φ s s δ t h n 1 m 1 h n x k n 1 m h x n 1 m 1 cos ϑ sin ϑ y k n 1 m h y n 1 m 1 cos ϑ sin ϑ z k n 1 m h z n 1 m 1 q t applying finite difference we can write equation 14 as 15 θ i j k n 1 m θ i j k n δ t c i j k n 1 m δ t h i j k n 1 m 1 h i j k n 1 m θ i j k n 1 m φ i j k s s δ t h i j k n 1 m 1 h i j k n 1 δ x 2 k i 1 2 j k n 1 m h i 1 j k n 1 m 1 h i j k n 1 m 1 cos ϑ i 1 2 j k i 1 2 j k n 1 m h i j k n 1 m 1 h i 1 j k n 1 m 1 cos ϑ i 1 2 j 1 δ x k i 1 2 j k n 1 m sin ϑ i 1 2 j k i 1 2 j k n 1 m sin ϑ i 1 2 j 1 δ y 2 k i j 1 2 k n 1 m h i j 1 k n 1 m 1 h i j k n 1 m 1 cos ϑ i j 1 2 k i j 1 2 k n 1 m h i j k n 1 m 1 h i j 1 k n 1 m 1 cos ϑ i j 1 2 1 δ y k i j 1 2 k n 1 m sin ϑ i j 1 2 k i j 1 2 k n 1 m sin ϑ i j 1 2 1 δ z 2 k i j k 1 2 n 1 m h i j k 1 n 1 m 1 h i j k n 1 m 1 k i j k 1 2 n 1 m h i j k n 1 m 1 h i j k 1 n 1 m 1 q t a 2 overland we linearize equation 11 by using explicit d and then solved using the implicit backward euler and finite difference in 2d discretized equation 11 is written as 16 h n 1 h n δ t 1 δ x 2 d i 1 2 j n h i 1 j n 1 h i j n 1 d i 1 2 j n h i j n 1 h i 1 j n 1 1 δ y 2 d i j 1 2 n h i j 1 n 1 h i j n 1 d i j 1 2 n h i j n 1 h i j 1 n 1 q i q o where n is the current timestep b gui for 3d flow model in this section we provide details on setting up the 3d flow model including 2d overland flow and 3d subsurface flow within mlcan3d main steps to set up the 3d flow model include enabling the model in the options window loading topography data and initial conditions in forcing and initial conditions window then specifying parameters for the model in parameters window more details on setting up the other model components are discussed in le et al 2012 fig 9 screenshot of the model options window users can select which model components to include in the simulation such the 3d flow model consisting of 2d overland flow and 3d subsurface flow fig 9 first to simulate 2d overland flow and 3d subsurface flow users must enable the 3d flow model option by selecting the model in the models panel within the options window as shown in fig 9 this step enables users to proceed to the following steps in setting up the 3d flow model other subsurface models including the nutrient model soil heat model and hydraulic redistribution are coupled with the 1d aggregated soil moisture profile for the domain and if the 3d flow model checkbox is not selected soil moisture is simulated in 1d amenu and kumar 2008 drewry et al 2010a quijano et al 2012 woo and kumar 2016 fig 10 screenshot of the model forcing and initial conditions window here users must specify topography data and have the option to use 3d heterogeneous initial conditions the gui also provides visualization for the uploaded data fig 10 once the 3d flow model is enabled to set up the model users must first use the forcing and initial conditions window to specify the topography of the simulated area fig 10 topography can be uploaded as a 2d matrix with variable name dem in a mat file or as a tiff or tif file the domain size nx ny of the simulation is determined based on the size of the uploaded topography data and the user must specify the size of each grid in meters the number of layers nz and the depth of each layer is specified by the root structure earlier in the model setup process le et al 2012 by default initial conditions for the overland flow assumes no ponded water and soil moisture is specified as a 1d vertical profile in the ics for canopy tab found at the top of the window the 1d profile is then extrapolated to the 3d domain however users have the option to specify more complex initial conditions including 2d ponded water depth and 3d soil moisture by checking each option and uploading the corresponding data in mat files details on the file structure requirements can be found by clicking the help buttons at each step the view initial conditions panel provides options to visualize the uploaded data for easy verification of the data fig 11 screenshot of the model parameters window under the 3d flow model tab users must specify parameters needed in the model fig 11 the finial step in setting up the 3d flow model is to specify the parameters used by the 3d flow model in the 3d flow model tab of the parameters window as shown in fig 11 there are two main sections for the 3d flow model parameters the first section includes parameters for the numerical model such as the maximum number of iterations for the subsurface model boundary conditions parallel computing and how often to save results choices for boundary conditions are no flow 0 or free flow 1 for the subsurface bottom boundary and no flow for side boundaries of the domain to simulate free flow side boundaries for both overland and subsurface users can use specify additional boundary cells at each side of the simulation domain the number of boundary cells can be specified as parameters with value 0 no boundary cells being the default no flow condition or any integer for the corresponding number of boundary cells from each edge of the domain when boundary cells are used values in those cells are initialized and parameterized similar to other cells in the domain their values are also saved in the 3d output so that boundary fluxes can be calculated as needed however boundary cells are not considered in the feedback of the 3d flow model with other mlcan3d processes such as root water uptake users can also set the number workers used to run the 3d flow model in parallel outputs from the 3d flow model including the depth of ponded water w from equation 7 the soil moisture θ the total head h and the hydraulic conductivity k ψ from equation 4 are saved at user defined intervals the second section of the parameters window contains detailed specification of parameters for soil moisture retention and hydraulic conductivity as described in equations 2 and 3 users have three options for specifying the soil parameters 1 use the texture based option where no additional data input is needed and the soil parameters are determined based on the soil texture specified by the user during the model setup 2 use the single value option where the entire simulation domain is set to one homogeneous value based on user input and 3 use the 2d 3d matrix option where users can specify 3d heterogeneous soil parameters when using the 2d 3d matrix option for certain parameters the user must load a mat file containing data for each parameter help windows and error messages are available to direct users during the setup process table 3 data sets from neon used for model applications at osbs and ornl as model forcings and model verification table 3 data product id name osbs date range ornl date range dp4 00 200 001 bundled data products eddy covariance 2018 05 01 06 30 2020 05 01 06 30 dp1 00 003 001 triple aspirated air temperature 2018 05 01 06 30 2020 05 01 06 30 dp1 00 004 001 barometric pressure 2018 05 01 06 30 2020 05 01 06 30 dp1 00 023 001 shortwave and longwave radiation net radiometer 2018 05 01 06 30 2020 05 01 06 30 dp1 00 006 001 precipitation 2018 05 01 06 30 2020 05 01 06 30 dp1 00 098 001 relative humidity 2018 05 01 06 30 2020 05 01 06 30 dp1 00 001 001 2d wind speed and direction 2018 05 01 06 30 2020 05 01 06 30 dp1 10 058 001 plant presence and percent cover 2016 05 01 06 30 2020 06 15 06 29 dp3 30 015 001 ecosystem structure 2018 09 2016 06 dp3 30 024 001 elevation lidar 2018 09 2016 06 dp1 10 047 001 soil physical properties distributed initial characterization 2016 03 2016 08 dp1 00 094 001 soil water content and water salinity 2018 05 01 06 30 2020 05 01 06 30 dp1 00 041 001 soil temperature 2018 05 01 06 30 2020 05 01 06 30 table 4 selection of model vegetation parameters table 4 parameter units osbs ornl canopy structure a canopy height h m 25 28 leaf area index lai 0 74 5 2 foliage clumping factor ω 1 0 0 9 flux tower observation height m 35 40 canopy roughness length z 0 m 2 5 2 8 maximum water storage capacity of a leaf s m mm lai 0 2 0 3 leaf photosynthesis b maximum rubisco limited carboxylation rate at 25 c v cmax 25 μmol m 2 s 60 80 maximum electron transport rate at 25 c j max 25 μmol m 2 s 110 140 leaf respiration rate at 25 c r d 25 μmolco 2 m 2 s 0 3 0 3 root structure c root depth r d m 4 5 4 7 50th percentile rooting depth z 50 m 0 6 0 5 95th percentile rooting depth z 95 m 2 5 2 a jensen 2002 brutsaert 2013 campbell and norman 2012 kitchings and mann 1976 and neon eddy covariance dp4 00 200 001 and ecosystem structure dp3 30 015 001 data shown in table 3 b wright et al 2013 sampson et al 2006 walker et al 2014 c heyward 1933 
25657,hyper resolution land surface models can explicitly represent landscape scale heterogeneity but the complexity of representing finer scale processes and meeting computational needs makes them inaccessible to the general scientific community and limits their adoption in this work we present the multi layer canopy and 3d soil mlcan3d model which is a high resolution high fidelity physical ecohydrologic model that aims to maintain accessibility mlcan3d implements detailed physical process representations is accessible through graphical user interface and is tested using comparisons with other models and field data this work demonstrates the feasibility of using mlcan3d to produce simulations close to real systems and the potential of the model to perform virtual experiments to explore model results our goal is for mlcan3d to serve as a virtual laboratory that enables virtual experiments from the broader scientific community and contribute to our understanding of ecohydrologic process heterogeneity dynamics and interactions across scales keywords ecohydrologic modeling 3d flow model spatial variability graphical user interface 1 introduction modeling of ecohydrologic processes is important in quantifying the water resource dynamics such as prediction of floods and droughts and among others in understanding vegetation processes and ecosystem carbon fluxes accurate quantification of land surface processes is also important for weather and climate predictions because of the strong interaction and feedback between land surface and atmospheric processes walker and rowntree 1977 shukla et al 1990 fine scale ecohydrologic heterogeneity such as those related to topography soil moisture and water flux has been shown to significantly influence larger scale ecosystem water and energy fluxes le and kumar 2017 riley and shen 2014 vivoni et al 2010 runoff and streamflow arrigo and salvucci 2005 barrios and francés 2012 and atmospheric feedback and circulation nykanen and foufoula georgiou 2001 current models tend to focus on their ability to simulate large domains and computation efficiency they make sacrifices such as simplification of process representations reduction of process complexity and the range of processes captured and are often limited by computing hardware constrains maxwell et al 2015 le et al 2015 in this work we present a high resolution and high fidelity ecohydrologic model that focuses on detailed physical process representation ease of use and broad applicability to serve as a virtual laboratory it is aimed to enable the general scientific community to use it to explore the interactions between topographic variability and ecohydrologic processes where complex feedback between these processes lead to non linear ecohydrologic dynamics with recent advances in environmental data availability and computational capability there is a call for hyper resolution land surface models over the regional to global extent that can more explicitly represent ecohydrologic heterogeneity to better understand and predict the non linear interactions between spatial variability and ecohydrologic processes bierkens et al 2015 wood et al 2011 hydrologic and land surface models are starting to become available at the 100 m resolution for continental extent and 1 km resolution for global extent maxwell et al 2015 sutanudjaja et al 2018 de graaf et al 2017 subgrid scale processes such as those associated with micro topographic variability are generally simplified with subgrid parameterizations or ignored clark et al 2015 however current understanding and model representation of many ecosystem processes are developed and validated with sensor measurements at the meter scale wood et al 2011 and understanding and quantifying how processes behave at the 100 m scale remains an issue clark et al 2015 some hypothesize that current model formulations relating to ecohydrologic processes are applicable and meaningful up to 100 m resolution as the limit wood et al 2011 riley and shen 2014 however other works find that meter scale micro topographic features influence stream flow dunne et al 1991 frei et al 2010 thompson et al 2010 infiltration frei and fleckenstein 2014 le and kumar 2017 and vegetation mcgrath et al 2012 therefore micro topographic variability is important for better understanding of heterogeneous ecohydrologic processes and how they behave at larger scales in order to better understand the complex interactions between topographic variability and ecohydrologic processes we need models that can capture the influence of micro topographic variability and represent physical processes that couples heterogeneity with other ecohydrologic dynamics while such models are starting to emerge the increases in model resolution fidelity of process representation and range of processes represented incur significant computational costs many models prioritize the ability to simulate large domains and efficient computation maxwell et al 2015 le and kumar 2017 and they are effective for their purpose and necessary for advancement of the science however to meet computation needs they may simplify process representations and reduce process representations and couplings they also extensively leverage advanced hardware and computation methods and thus tend to be less accessible to general scientific community due to the learning curve needed to set up the models complexity of the code and limiting knowledge to exploit advanced hardware such as hybrid computing involving both cpu central processing units and gpu graphical processing units le and kumar 2017 due to such complexity these models are also often only tested on individual or limited number of sites baatz et al 2018 further limiting their adoption by general users in order to stimulate scientific exploration of how fine scale landscape heterogeneity affects ecohydrologic process dynamics a model that overcomes the above issues and focuses on physical process representation greater accessibility and wider applicability is needed therefore in this work we present the multi layer canopy and 3d soil mlcan3d model a high resolution high fidelity but easy to use ecohydrologic model for simulating heterogeneities from micro topographic scale to larger scales and their interactions with coupled ecohydrologic processes our goal is to provide an accessible model for the general scientific community that can be used as a virtual laboratory for testing scientific hypotheses investigating knowledge gaps and understanding ecohydrologic process interactions and dynamics fatichi et al 2016 in mlcan3d we implement topography aware surface and subsurface moisture dynamics with 2d diffusive overland flow and 3d terrain following richards equation for subsurface flow we model vegetation energy and above ground moisture processes with an advanced ecohydrologic process model the multi layer canopy mlcan model that has been tested on a wide range of ecosystems drewry et al 2010a b quijano et al 2012 2013 quijano and kumar 2015 we tightly integrate our state of the art 3d flow model with mlcan through root soil moisture exchange and fully coupled surface flow and land surface water and energy processes to keep the model accessible we improve upon the graphical user interface gui developed for mlcan le et al 2012 to facilitate model setup and we use the advanced iterative alternating direction implicit aiadi method an et al 2011 douglas and rachford 1956 to solve the subsurface flow so that the model can be run without stringent computing hardware constraints we simulate two very different ecosystems to test and demonstrate the broad applicability of mlcan3d with the availability of extensive ecohydrologic observations from observational networks such as the critical zone observatories czos brantley et al 2006 the long term ecosystem research lter network callahan 1984 and the national ecological observatory network neon national research council 2004 we have the ability to develop simulations that capture real world behavior with high fidelity baatz et al 2018 based on these simulations we can then perform virtual experiments to gain understanding about processes in temporal and spatial extents and resolutions that cannot be measurement in the field fatichi et al 2016 or guide the design of new experiments by focusing on process representation accessibility and wide applicability we believe that mlcan3d can serve as a virtual laboratory that encourage such experiments by the scientific community in section 2 we first discuss the theory numerical implementation and benchmark verification of the coupled surface subsurface 3d flow model that captures topographic heterogeneity next we discuss the integration of the flow model with the mlcan model in section 3 then in section 4 we apply the model to two very different neon sites to test and demonstrate model capability we close with sections 5 and 6 with discussion of model sensitivity results and next steps 2 surface subsurface 3d flow model 2 1 3d subsurface flow in many diverse fields of study richards equation is seen as the standard for modeling water flow in the near surface soil layers hillel 2013 there are three forms of the equation used in existing studies the h based form solving for pressure head the θ based form solving for soil moisture and the mixed form using both pressure and moisture as dependent variables studies have shown that the mixed form richards equation is perfectly mass conserved using finite difference and gives better performance over the other forms at no additional computational cost celia et al 1990 an et al 2011 therefore in this work we use the mixed form richards equation that is generally written as follows celia et al 1990 1 θ t k ψ ψ k z here θ is the volumetric soil moisture content k ψ is the unsaturated hydraulic conductivity which depends on soil matric potential ψ the relationships between k θ and ψ are calculated using the well known soil moisture retention curve from van genutchen van genuchten 1980 and the unsaturated hydraulic conductivity function from mualem 1976 2 θ θ θ r θ s θ r 1 1 α ψ n v m v 3 k θ k s θ 1 2 1 1 θ 1 m v m v 2 θ r θ s are the residual and saturated soil moisture k s is the saturated hydraulic conductivity α is a parameter corresponding to the inverse air entry value n v is the pore size distribution and m v 1 1 n v all parameters can be found based on soil type or composition to generalize the richards equation to be applicable to model a natural environment we modify equation 1 to account for plant root uptake elastic storage and topography as follows 4 θ t θ φ s s h t k ψ h cos ϑ sin ϑ q t the inclusion of flux due to transpiration q t accounts for vegetation root uptake θ φ s s h t accounts for elastic storage bear et al 1979 where φ is the porosity and s s is elastic storage coefficient to account for changes in elevation within the domain we use h the total pressure head where h ψ z ψ is the soil matric head as previously defined and z is the gravity head the elevation above a given datum we further apply a terrain following transform to better account for topography in the domain maxwell 2013 childs 1971 sloan and moore 1984 brutsaert 1994 a representation of the 3d subsurface model with terrain following transform is shown at the top of fig 1 the terrain following transform h cos ϑ sin ϑ modifies horizontal fluxes to follow local topographic slope with angle ϑ assuming local slope is constant over depth angles of local slope in the horizontal directions are calculated as ϑ x tan 1 z x and ϑ y tan 1 z y since there is no need to account for slope for vertical fluxes ϑ z 0 compared to traditional orthogonal fluxes the terrain following implementation improves in accuracy and is better suited to modeling complex terrains or at coarse spatial resolutions maxwell 2013 we solve equation 4 using backward euler for the time discretization and finite difference for space discretization the bottom of fig 1 shows the finite difference stensil in the x z plane since the richards equation is non linear due to the relationship between θ k and h we use the modified picard method celia et al 1990 to linearize the equation and solve each time step iteratively please refer to appendix a 1 for more details on the discretization of equation 4 2 1 1 aiadi due to considerations for ease of use and computational effort an adi scheme is used to solve equation 4 one dimension at a time instead of the full implicit solution previous studies have compared adi with full implicit implementations for multi dimensional richards equation and found adi method to be faster with similar simulation results an et al 2011 however traditional adi methods peaceman and rachford 1955 rubin 1968 can encounter instabilities and difficulty in convergence when solving higher dimension problems clement et al 1994 to overcome these issues we use the advanced iterative alternating direction implicit aiadi method an et al 2011 douglas and rachford 1956 which was first developed for linear parabolic partial differential equations and is unconditionally stable in 2d and 3d douglas and rachford 1956 in non linear cases stability is not guaranteed but is improved over traditional adi methods an et al 2011 we adapt the aiadi method to solve the generalized richards equation 4 for natural environments our modified aiadi method uses three passes as follows 5 first pass in x direction θ n 1 3 m θ n δ t c n 1 3 m δ t i m k n 1 3 m h n 1 3 m 1 h n 1 3 m θ n 1 3 m φ s s δ t h n 1 3 m 1 h n x k n 1 3 m h x n 1 3 m 1 cos ϑ x sin ϑ x y k n 1 3 m h y n 1 3 m cos ϑ y sin ϑ y z k n 1 3 m h z n 1 3 m q t second pass in y direction c n 1 3 m δ t i m k n 1 3 m θ n 1 3 m φ s s δ t h n 1 3 m 2 h n 1 3 m 1 y k n 1 3 m h y n 1 3 m 2 cos ϑ y sin ϑ y y k n 1 3 m h y n 1 3 m cos ϑ y sin ϑ y y k n 1 3 m h y n 1 3 m 2 c o s ϑ y y k n 1 3 m h y n 1 3 m cos ϑ y and third pass in z direction c n 1 3 m δ t i m k n 1 3 m θ n 1 3 m φ s s δ t h n 1 3 m 3 h n 1 3 m 2 z k n 1 3 m h z n 1 3 m 3 z k n 1 3 m h z n 1 3 m where k i j k n k i 1 2 j k n k i 1 2 j k n k i j 1 2 k n k i j 1 2 k n k i j k 1 2 n k i j k 1 2 n and 6 i m 0 5 5 m here n is the timestep index and m is the picard iteration level index i m is an iteration parameter for the disturbance term in the aiadi scheme 0 55 is chosen in this work following previous studies an et al 2011 weeks et al 2004 each time step is iterated until h n 1 3m 3 h n 1 3m is less than the tolerance value given by the user 2 2 2d overland flow overland flow is simulated using the st venant equations which consists of the continuity equation and two momentum equations the continuity equation for 2d application is 7 w t x w u y w v q i q o 0 where w is the depth of water u and v are velocities in the x and y direction respectively and q i and q o are the inflow and outflow flux term contributing to the overland flow these terms in units of length time account for sources and sinks in the overland flow process and facilitate the exchange of water between overland and subsurface components of the model detailed physical processes included in each term is discussed in section 2 3 in this work we use diffusion flow to model the overland flow process while omitting the inertial terms in the st venant equations diffusion flow is still capable of accurately representing many natural flow situations lal 1998 fennema et al 1994 akan and yen 1981 hromadka and lai 1985 the momentum equations are reduced to 8 h x s f x a n d h y s f y where h w z is the depth of the water w in addition to surface elevation z above a given datum thus s fx s fy the friction slopes are the same as the slope of the water surface s w in 2d where s w s f x 2 s f y 2 combined with manning s equation flow velocity can be expressed in terms of h hromadka and lai 1985 9 u w 2 3 n m s w h x d h h x 10 v w 2 3 n m s w h y d h h y where n m is the manning s coefficient and d the diffusion coefficient is expressed as d w 5 3 n m s w for s w s m i n a n d w w m i n 0 otherwise parameters s min and h min are both user defined s min is used to keep d within a finite limit and w min facilitates wetting and drying using equations 9 and 10 the governing equation based on continuity equation 7 is written as 11 h t x d h x y d h y q i q o the overland flow equation can be linearized and solved with a wide variety of approaches lal 1998 in this work we use a linearized implicit method where equation 11 is linearized using explicit d and then solved using the implicit backward euler and finite difference in 2d please refer to appendix a 2 for the discretization of equation 11 2 3 surface subsurface coupling infiltration and evaporation determined by surface processes and subsurface moisture conditions couple the overland flow and subsurface components of the 3d flow model the amount of water exchanged between the two model components is calculated at each timestep it is included in the sink term in the overland flow model q o in equation 11 and it is a source to the subsurface model by serving as the top boundary condition we use a switching top boundary condition in the subsurface where depending on moisture conditions the top boundary of any cell is dirichlet or neumann paniconi and wood 1993 camporese et al 2010 2014 sulis et al 2010 le et al 2015 dirichlet boundary condition applies when infiltration and evaporation becomes limited by soil moisture conditions such as in the case of saturation excess otherwise neumann boundary condition applies precipitation contributes to the overland flow model through the term q i from equation 11 and excess water that does not infiltrate into the soil is included in q i at the next timestep of the simulation additional processes contributing to sources and sinks for the coupled 3d flow model are discussed in section 3 where the 3d flow model is integrated with a 1d ecohydrologic model 2 4 flow model benchmarks we use a set of benchmarks as preliminary tests for our coupled 3d flow model these benchmark simulations have been established through previous works and are designed to compare the physical responses of models kollet and maxwell 2006 sulis et al 2010 maxwell et al 2014 we use two established test cases infiltration excess and saturation excess to examine the most prevalent hydrologic responses and the interaction between overland flow and subsurface flow components of the model both test cases use a domain that slopes in one direction as shown in fig 2 the soil depth is 5 m with no water flowing through any boundaries of the domain except the outlet on the right side as depicted in fig 2 they both use the same van genuchten parameters except for saturated hydraulic conductivity k s based on values for sandy loam soil estimated by schaap and leij 1998 both tests consists of a 300 min simulation that starts with 200 min of rainfall followed by 100 min of recession parameters used are shown in table 1 and the two cases differ in their parameterization for the saturated hydraulic conductivity and initial water table depth as follows 1 the infiltration excess case tests for runoff before the soil column is saturated due to rainfall rate that is higher than the infiltration rate therefore this case tests two saturated hydraulic conductivity k s values that are smaller than the rainfall rate shown in table 1 2 the saturation excess case tests for runoff when the soil column is saturated this is simulated with a k s that is larger than the rainfall rate two values for initial water table depth are tested given in table 1 fig 3 compares the outflow rate from our model with that of five other models for the infiltration excess benchmark test case our model generally agrees well with other models especially for the magnitude of the peak outflow and the recession curve our model matches other model outputs very closely or is within the range of variability of the other models the largest discrepancy occurs in the rising limb of the outflow for the low k s case our model produces a slightly steeper rising limb compared to other models tested and thus plateaus faster where the outflow is equivalent to rainfall rate one possible cause of this is numerical differences in the overland flow model implementation another possible cause is the difference in mesh size and timestep for our model based on the comparison of a similar test case with different mesh sizes and the analytical solution from kollet and maxwell 2006 we see that the analytical solution and simulations with very fine mesh tend to also have a steeper rising limb and plateau faster than models with coarser grids fig 4 compares the outflow rate from our model with that of other models for the saturation excess benchmark test case our model also agrees well with other models for this case similar to the infiltration excess test case our model matches other model outputs very closely or is within the range of variability of the other models for the peak outflow and the recession curve the largest difference occurs at the rising limb for the case with initial water table at 1 0 m where our model produces outflow slightly slower than other models and have a steeper rising limb similar to the infiltration excess case cause for the discrepancy is also likely a combination of differences in numerical implementation and mesh size same as that of the infiltration excess case however in general results from our model match with other models in the benchmark tests reasonably closely and show expected model behavior which provides confidence in the validity of our model 3 a virtual laboratory 3 1 canopy and flow model process integration in order to simulate land surface processes we integrate our flow model into the existing multi layer canopy mlcan model drewry et al 2010a b quijano et al 2012 le et al 2012 quijano et al 2013 quijano and kumar 2015 mlcan is a high fidelity high complexity 1d model that simulates above ground canopy processes by 1 fully coupling leaf biophysical processes including photosynthesis leaf stomatal conductance leaf boundary layer conductance and leaf energy balance 2 scaling from leaf to canopy level with a multi layer approach using sunlit and shaded leaf fractions for each layer and 3 resolving the vertical profiles of radiation water storage energy balance and co 2 flux mlcan also describes surface and below ground processes such as water storage and energy balance in the litter and snow layers root and soil water interactions that couples to photosynthesis soil heat transport and 1d soil water movement we integrate our 3d flow model with mlcan by replacing the original 1d soil moisture model with our 3d subsurface model and adding the 2d overland flow model as shown in fig 5 we maintain all process interaction in the original mlcan model such as soil water interactions with plant roots the transpiration flux q t in equation 4 is modeled as a sink term in the subsurface model and it is generally uptake of water by plants determined by the canopy transpiration however due to the hydraulic redistribution incorporated in the model amenu and kumar 2008 quijano et al 2012 where water travels through plant roots from wet to dry parts of the soil upwards or downwards q t can be both sink or source in different parts of the model domain at any timestep we also implement additional process interactions with the addition of the 2d overland flow model sinks in the overland flow model q o from equation 11 include evaporation and infiltration as described in section 2 3 to couple overland flow with subsurface processes evaporation from the overland flow model is integrated into mlcan canopy processes to further integrate the 3d flow model into mlcan surface and canopy processes q o includes the contribution of overland flow to water stored in the litter layer and precipitation which contributes to q i from equation 11 is determined by throughfall from the canopy model and uptake by the litter and snow model drainage of excess water from the litter and snow layer as well as overland flow contributes to infiltration into the subsurface the resulting multi layer canopy and 3d soil mlcan3d model has the capability to represent vegetation dynamics with high fidelity including acclimation response of vegetation to changes in atmospheric co 2 drewry et al 2010a b and its consequent impact in water and energy partitioning capture high resolution heterogeneity in the topography and subsurface and maintains the tight process interactions between vegetation and soil moisture fig 5 shows the schematic of process interactions of the mlcan3d model with the new coupled 3d flow model and associated process interactions highlighted in blue 3 2 model capabilities a detailed physical process model such as mlcan3d inevitably need a lot of data to set up and is computationally demanding to encourage the use of mlcan3d as a virtual laboratory by the broader scientific community we develop mlcan3d with an emphasis on accessibility where the model is easy to set up and understand as well as run without stringent computing hardware constraints we use a graphical user interface gui to guide users step by step through the model setup process the gui was first developed for mlcan le et al 2012 and is now modified to include setup of the coupled surface and subsurface flow components in setting up the model users first specify the simulation location as latitude and longitude plant species composition and vegetation structure through leaf area index lai and vertical leaf area density lad profile for each species then users have the ability to specify which modules to include in the simulation such as using the 3d flow model 2d overland and 3d subsurface or the 1d soil moisture module in the original mlcan model mlcan3d then takes available eddy covariance flux tower data as model forcings and uses lidar derived digital elevation models dems to characterize topography for the 3d flow model for initial soil moisture conditions users have the option to use a vertical profile that is homogeneous over the domain or input 3d data as a grid of user specified values similarly users can specify 3d heterogeneous soil parameters for soil moisture retention and hydraulic conductivity as described in equations 2 and 3 appendix b contains details on how to use the gui to set up the 3d flow model mlcan3d outputs water co 2 and energy fluxes for the canopy and the soil as well as vegetation dynamics and microclimate conditions in addition mlcan3d outputs moisture conditions from the 3d flow model including ponded water depth on the land surface and subsurface soil moisture since we use the advanced iterative alternating direction implicit aiadi method an et al 2011 douglas and rachford 1956 to solve the subsurface flow the model is significantly less memory intensive than fully implicit methods and is easily parallelized therefore mlcan3d can be run on personal computers for small virtual experiments or in highly parallel computing environments for large experiments with the help of the gui and limited hardware constraints mlcan3d can be more accessible to users from broader range of backgrounds and encourage virtual experiments to answer more questions from diverse disciplines mlcan3d is open source and available for download at https github com hydrocomplexity mlcan3d 4 model application mlcan3d can be applied to a wide range of ecosystems to demonstrate its applicability we simulate without calibration two very different ecosystems to test mlcan3d in real world situations using data from neon national ecological observatory network national research council 2004 neon funded by nsf consists of a network of long term data collection facilities that provides comprehensive data for quantifying land surface ecological processes neon has sites across the us and covers a range of ecosystems and climates they collect long term open access data and provide more than 175 data products for ecologic and biological studies and their standardized data collection and processing protocols can provide comparable data across different sites at each neon site an eddy covariance flux tower collects weather data needed as forcings for mlcan3d such as radiation precipitation air temperature and wind speed the tower also collects ecosystem fluxes of water energy and carbon which can be used to compare with model output at each site there is also an array of soil plots near the tower fig 6 that have sensors at various depths in the soil to measure soil variables such as water content salinity and temperature neon also collects processes and provides high resolution airborne remote sensing data including hyperspectral lidar and digital photography for each site around the time of peak greenness each year hyperspectral and lidar remote sensing products are provided at 1 m resolution the hyperspectral data products include leaf area index lai and lidar data products include canopy height chm and surface elevation dem observational samples are also taken for a variety of ecosystem factors such as plants soil and organisms relevant data include plant species soil texture and litter layer information we apply mlcan3d to two sites in order to test model performance for different conditions of vegetation topography and soil textures neon datasets used to set up model simulations and verify model results are listed in table 3 in the appendix 4 1 ordway swisher biological station site the ordway swisher biological station osbs site is located in central florida lat long 29 689 27 81 993 43 mean annual precipitation is 1290 mm with more rain in the summer months it consists of fairly homogeneous evergreen forests dominated by longleaf pine pinus palustris within the eddy covariance flux tower airshed average canopy height based on airborne lidar data is 23 m with relatively open canopy and low leaf area index lai of 0 79 ornl daac 2018a myneni et al 2015 we use 1 m resolution dem from airborne lidar to characterize the topography of the site for the 3d flow model while the osbs site has limited elevation changes it consists of very fast draining deep sandy soils whose sharp changes in soil moisture conditions really test the numerical stability of the subsurface model according to soil sample measurements the osbs soil contains more than 95 sand and is therefore a sandy textured soil texture based soil parameters for equations 2 and 3 found in literature are used as model parameters as shown in table 2 dingman 2015 clapp and hornberger 1978 leij 1996 ghanbarian alavijeh et al 2010 in this work we model a 150 60 m 2 plot at 1 m 2 resolution covering all soil moisture sensor as shown in fig 6 the site has a slight topographic gradient of about 2 5 m going down slope from the north east to the south west we run the model for june 2018 at 30 min timesteps with a 10 day spin up period the 3d flow model s initial soil moisture condition is homogeneous with volumetric soil moisture of 0 1 and measured forcing data from may 2018 is used for the spin up period due to the fast draining nature of the soil initial soil moisture conditions do not significantly affect results after the spin up period the summary of model results for the simulation period is shown in fig 7 where we compare our model simulation with field measured data from neon in fig 7a we compare simulated plot average results of soil moisture and temperature with measured data from five soil plots at 6 cm depth corresponding to the first layer of 10 cm in our model we also display the vertical profile of plot average soil moisture over time we find that on the plot level average our model simulation is in good agreement with measured data though simulated soil temperature does not show diurnal variations as strong as that of the observed data in fig 7b we show topography of the plot and the heterogeneity of soil moisture over the plot at various depths here we observe that simulated soil moisture varies at fine scales that corresponds to heterogeneity due to micro topographic variability on the order of a few meters however soil moisture does not change significantly over the domain and the variance of the top layer soil moisture shown in 7b is less than what is observed by the five soil moisture sensors ploted in 7a this indicates that simulated soil moisture for the osbs site does not react to larger scale topographic features such as the gradual slope in the plot as much as what the soil moisture sensors measure in fig 7c we compare diurnally averaged simulated latent heat flux from the plot with that of the measured data from the flux tower we find that simulated latent heat flux is in good agreement with measured 4 2 oak ridge national laboratory site the oak ridge national laboratory ornl site is located in tennessee lat long 35 964 12 84 282 6 mean annual precipitation is 1222 mm with slightly more precipitation in the winter and spring it consists of mainly mixed deciduous forests dominated by oaks maples and hickories within the flux tower airshed with average canopy height of 28 m and lai of 5 2 ornl daac 2018b myneni et al 2015 as part of the ridge and valley appalachians the site is situated within five parallel ridges and valleys with dramatic variations in elevation therefore this site allows us to test the model s ability to capture the effects of topographic variations due to the large topographic variability at the ornl site soil texture also varies dramatically from sandy loam and silt loam to clay according to soil sample measurements from neon neon does not have soil sample measurements very close to the eddy covariance flux tower but we know from literature and measured data that the soil at the top of the ridges where the tower is tends to be more coarse and well drained compared to soil in valleys solomon et al 1992 therefore for this study we use silt loam soil with low porosity as described in solomon et al 1992 texture based soil parameters for equations 2 and 3 found in literature are shown in table 2 dingman 2015 clapp and hornberger 1978 leij 1996 ghanbarian alavijeh et al 2010 for the ornl site we model a 200 80 m 2 plot at 1 m 2 resolution covering all soil moisture sensor and the eddy covariance flux tower as shown in fig 6 the site has topographic changes of over 17 m over the plot much larger than that of the osbs site we run the model for june 2020 at 30 min timesteps with a 20 day spin up period initial soil moisture condition is homogeneous with volumetric soil moisture set to 0 2 to mitigate effects of the initial condition we use a longer spin up period to compensate for the slower draining soil at the site the summary of model results for the simulation period is shown in fig 8 where we compare our model simulation with field measured data from neon in fig 8a we compare simulated plot average results of soil moisture and temperature with measured data from five soil plots at 0 06 m depth corresponding to the first layer of 0 1 m in our model we find that on the plot level average our model simulation for the ornl site is also in good agreement with measured data though soil moisture does not seem to react as sharply to precipitation as the measured data also towards the end of june simulated soil temperature is slightly lower compared to measured and in general does not see as strong diurnal variations in fig 8b we show topography of the plot and the heterogeneity of soil moisture over the plot at various depths simulated soil moisture is able to capture soil moisture heterogeneity due to large scale topographic gradient as well as micro topographic variability however soil moisture simulation does not show as much spatial variability as is measured by the five soil moisture sensors in fig 8c we compare diurnally averaged simulated latent heat flux from the plot with that of the measured data from the flux tower we find that simulated latent heat flux is in good agreement with measured 5 discussion and model sensitivities the canopy component of mlcan3d is strongly affected by vegetation s physical properties and its photosynthetic parameterization important physical properties includes lai and canopy height which specify the structure of the vegetation important photosynthetic parameters for c3 plants as simulated in our study include maximum rubisco limited carboxylation rate v cmax and maximum electron transport rate j max a few parameters are important in facilitating the interaction of canopy land surface and soil soil moisture is sensitive to the amount of throughfall the precipitation that reaches the ground after canopy interception and the thickness of the surface litter layer since these components directly influence the amount of infiltration soil moisture is also affected by root water uptake as determined by vegetation properties discussed above and conductivity of roots manning s coefficient n m affects the overland flow and thus influence heterogeneity of infiltration into the subsurface soil moisture is most directly affected by the soil moisture retention curve parameters used in equations 2 and 3 simulation results tends to be the most sensitive to k s where water travels faster through soils with larger k s leading to sharper peaks during precipitation and faster drainage the residual and saturated soil moisture θ r θ s are important in bounding the range of soil moisture and larger θ s will lead to higher soil moisture and similarly for θ r when all other parameters remain constant n v and α also have small affects on soil moisture larger n v leads to drier conditions larger α produces slightly higher peaks during precipitation and less dry down leading to more water retention over time however the effect is not very significant when varying α within the reasonable range for the given soil texture we parameterize the application cases of our model using parameters found in existing literature when available and do not calibrate the model to fit measured data from the results we find that on average the model can closely estimate soil moisture levels compared to sensor measured values vertical movement of moisture in the soil column also seems reasonable from comparing with deeper layer results in the osbs site canopy level latent heat flux is also comparable with measured values based on current results the model performs well in general but have a few issues that warrants further discussion one noticeable difference between the simulated soil moisture and measured soil moisture curves is that sensor measurements register faster response to precipitation when compared with that of the model simulation we believe two factors contribute to the more rounded peaks in the simulated soil moisture one factor is that since the curve is an aggregation of the plot where the variable response from different locations in the plot are averaged the less sharp peaks reflects the variability within the plot another factor is that the modeled canopy and litter layer attenuation of the precipitation signal is not representative of individual sensor locations if a sensor is not located under vegetation then it does not experience attenuation of precipitation and will exhibit different behavior compared to locations under vegetation and the average behavior of the plot while simulated soil temperature is generally close to measured values and shows correct diurnal oscillations the magnitude of simulated oscillations are much less than that of the sensor measurements this difference may be due to the insulating effects of the litter layer or the limited parameterization of soil heat capacity based on soil texture in the model from figs 7b and 8b we see that while micro topographic effects on soil moisture are evident the variability of soil moisture across the study plots is not as significant as demonstrated by the measured data one possibility is that due to the homogeneous initial conditions we currently use for our simulations soil moisture gradient across each plot has not had time to form during the spin up period this can be tested with a longer simulation or heterogeneous initial conditions another is that there is notable heterogeneity in soil texture that our available data does not capture which influences soil moisture however there may also be special unknown conditions influencing individual soil moisture sensors for example soil moisture sensor 3 s3 in the ornl case measures soil moisture at around 0 1 barely above the residual soil moisture for the entire duration of the simulation and does not respond significantly to precipitation while simulated soil moisture at the location of sensor 3 is drier due to elevation fig 6 this extremely dry soil moisture measurements seems unlikely to be solely caused by topography similarly the low elevation of sensor 5 in the ornl case fig 6 does not corroborate its low soil moisture measurements and suggests additional factors that can contribute to the large variations we see in soil moisture measurements in general initial simulations using mlcan3d shows that models results are consistent with measured data on average but do not capture as much variability both in time and space this demonstrates there is still a need for further studies to understand and draw connections between in situ sensors measurements and models simulations using mlcan3d as a virtual laboratory we can conduct virtual experiments that explore this issue for example in this work we find that using topography as the only source of heterogeneity is not enough to capture the variability in the field we hypothesize that model parameterization may contribute to the discrepancy between measured and modeled soil moisture variability across the study plots therefore we can conduct a virtual experiment using user specified heterogeneous soil parameters to test this hypothesis however topography is readily available for large extents at high resolutions while soil parameters are not therefore we can also use mlcan3d to ask questions such as how topographic variability interacts with ecohydrologic processes to affect soil moisture and other ecohydrologic fluxes and to what extent can topography inform heterogeneity in ecohydrologic fluxes remotely sensed heterogeneity in vegetation which leads to variability in throughfall infiltration and water uptake can also be included in future works with the ability to model across scales from micro topographic to larger scales and across ecosystems mlcan3d can also be used to explore how model scale affects heterogeneous ecohydrologic process behaviors and their interactions at different scales by gaining better understanding and quantification of outstanding scale issues such as how fine scale dynamics amplify or attenuate as scale increases simulation experiments from mlcan3d can contribute to the hyper resolution land surface modeling effort in quantifying high resolution heterogeneity over large extents 6 conclusion in this work we presented mlcan3d an integrated ecohydrologic model that focuses on high fidelity physical process representation greater accessibility and wider applicability it couples existing advanced ecohydrologic process model mlcan with new state of the art topography aware 3d flow model mlcan models vegetation land surface energy fluxes and above ground moisture processes the 3d flow model simulates surface and subsurface moisture dynamics with 2d diffusive overland flow and 3d terrain following richards equation for subsurface flow the 3d flow model is in good agreement with other similar models when compared in standard benchmark tests and when coupled with mlcan the resulting mlcan3d model enables high resolution ecohydrologic modeling that accounts for micro topographic variability in the land surface to make mlcan3d easily accessible we provide a gui to facilitate the initialization and the full simulation of the model we also use the aiadi method for the 3d subsurface flow to reduces computation cost without sacrificing numerical stability the guidance from the gui and lack of stringent computing hardware constraints makes mlcan3d more accessible to the general scientific community thus encouraging wider adoption of the model we apply mlcan3d to two neon sites with very different characteristics to demonstrate the broad applicability of the model model results are comparable with field measured data at the plot scale and demonstrate the effect of micro topographic variations on soil moisture model simulations did not capture as much heterogeneity as demonstrated by in situ sensors a reflection of the fact that topography is not the only source of heterogeneity that contributes to variability of soil moisture in the field we aim to further explore this result using mlcan3d by investigating additional factors contributing to land surface heterogeneity as well as how topographic variability affects modeled ecohydrologic fluxes this work demonstrated the feasibility of using the mlcan3d model as a virtual laboratory that enables a range of virtual experiments by the broader scientific community and contribute to the advancement of our understanding of ecohydrologic process heterogeneity dynamics and interactions software and data availability mlcan3d is written in matlab and is tested for matlab version r2018b it is first made available in 2021 requires matlab to run and has no strict hardware requirements mlcan3d is open source and available for education and research uses at https github com hydrocomplexity mlcan3d developers contact information is provided in the mlcan3d readme on github declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgment this work was supported by the national science foundation grant numbers oac 1835834 ear 2012850 ear 1331906 and aci 1261582 the authors would like to acknowledge the national ecological observatory network as the source of field measured data used in this work we would also like to thank professor reed maxwell and his students for providing the data for the benchmark comparisons appendix a flow model discretization a 1 subsurface backward euler scheme is used for the time discretization and finite difference for space discretization since the richards equation is non linear due to the relationship between θ k and h the modified picard method celia et al 1990 is used to linearize the equation and solve each time step iteratively discretizing equation 4 with backward euler and picard linearization gives 12 θ n 1 m 1 θ n δ t θ n 1 m φ s s δ t h n 1 m 1 h n x k n 1 m h x n 1 m 1 cos ϑ sin ϑ y k n 1 m h y n 1 m 1 cos ϑ sin ϑ z k n 1 m h z n 1 m 1 q t where n is the time step and m is the iteration level of the picard iteration next as shown in celia et al 1990 dependent variable θ n 1 m 1 can be written in terms of ψ and h using its taylor series expansion about ψ n 1 m 13 θ n 1 m 1 θ n 1 m θ ψ n 1 m ψ n 1 m 1 ψ n 1 m o δ 2 θ n 1 m c n 1 m h n 1 m 1 h n 1 m notice that the temporal difference of ψ is equivalent to that of h because the gravitational potential component of h does not change in time and cancels out using c θ ψ and with higher order terms omitted we can plug equation 13 into equation 12 to replace θ n 1 m 1 14 θ n 1 m θ n δ t c n 1 m δ t h n 1 m 1 h n 1 m θ n 1 m φ s s δ t h n 1 m 1 h n x k n 1 m h x n 1 m 1 cos ϑ sin ϑ y k n 1 m h y n 1 m 1 cos ϑ sin ϑ z k n 1 m h z n 1 m 1 q t applying finite difference we can write equation 14 as 15 θ i j k n 1 m θ i j k n δ t c i j k n 1 m δ t h i j k n 1 m 1 h i j k n 1 m θ i j k n 1 m φ i j k s s δ t h i j k n 1 m 1 h i j k n 1 δ x 2 k i 1 2 j k n 1 m h i 1 j k n 1 m 1 h i j k n 1 m 1 cos ϑ i 1 2 j k i 1 2 j k n 1 m h i j k n 1 m 1 h i 1 j k n 1 m 1 cos ϑ i 1 2 j 1 δ x k i 1 2 j k n 1 m sin ϑ i 1 2 j k i 1 2 j k n 1 m sin ϑ i 1 2 j 1 δ y 2 k i j 1 2 k n 1 m h i j 1 k n 1 m 1 h i j k n 1 m 1 cos ϑ i j 1 2 k i j 1 2 k n 1 m h i j k n 1 m 1 h i j 1 k n 1 m 1 cos ϑ i j 1 2 1 δ y k i j 1 2 k n 1 m sin ϑ i j 1 2 k i j 1 2 k n 1 m sin ϑ i j 1 2 1 δ z 2 k i j k 1 2 n 1 m h i j k 1 n 1 m 1 h i j k n 1 m 1 k i j k 1 2 n 1 m h i j k n 1 m 1 h i j k 1 n 1 m 1 q t a 2 overland we linearize equation 11 by using explicit d and then solved using the implicit backward euler and finite difference in 2d discretized equation 11 is written as 16 h n 1 h n δ t 1 δ x 2 d i 1 2 j n h i 1 j n 1 h i j n 1 d i 1 2 j n h i j n 1 h i 1 j n 1 1 δ y 2 d i j 1 2 n h i j 1 n 1 h i j n 1 d i j 1 2 n h i j n 1 h i j 1 n 1 q i q o where n is the current timestep b gui for 3d flow model in this section we provide details on setting up the 3d flow model including 2d overland flow and 3d subsurface flow within mlcan3d main steps to set up the 3d flow model include enabling the model in the options window loading topography data and initial conditions in forcing and initial conditions window then specifying parameters for the model in parameters window more details on setting up the other model components are discussed in le et al 2012 fig 9 screenshot of the model options window users can select which model components to include in the simulation such the 3d flow model consisting of 2d overland flow and 3d subsurface flow fig 9 first to simulate 2d overland flow and 3d subsurface flow users must enable the 3d flow model option by selecting the model in the models panel within the options window as shown in fig 9 this step enables users to proceed to the following steps in setting up the 3d flow model other subsurface models including the nutrient model soil heat model and hydraulic redistribution are coupled with the 1d aggregated soil moisture profile for the domain and if the 3d flow model checkbox is not selected soil moisture is simulated in 1d amenu and kumar 2008 drewry et al 2010a quijano et al 2012 woo and kumar 2016 fig 10 screenshot of the model forcing and initial conditions window here users must specify topography data and have the option to use 3d heterogeneous initial conditions the gui also provides visualization for the uploaded data fig 10 once the 3d flow model is enabled to set up the model users must first use the forcing and initial conditions window to specify the topography of the simulated area fig 10 topography can be uploaded as a 2d matrix with variable name dem in a mat file or as a tiff or tif file the domain size nx ny of the simulation is determined based on the size of the uploaded topography data and the user must specify the size of each grid in meters the number of layers nz and the depth of each layer is specified by the root structure earlier in the model setup process le et al 2012 by default initial conditions for the overland flow assumes no ponded water and soil moisture is specified as a 1d vertical profile in the ics for canopy tab found at the top of the window the 1d profile is then extrapolated to the 3d domain however users have the option to specify more complex initial conditions including 2d ponded water depth and 3d soil moisture by checking each option and uploading the corresponding data in mat files details on the file structure requirements can be found by clicking the help buttons at each step the view initial conditions panel provides options to visualize the uploaded data for easy verification of the data fig 11 screenshot of the model parameters window under the 3d flow model tab users must specify parameters needed in the model fig 11 the finial step in setting up the 3d flow model is to specify the parameters used by the 3d flow model in the 3d flow model tab of the parameters window as shown in fig 11 there are two main sections for the 3d flow model parameters the first section includes parameters for the numerical model such as the maximum number of iterations for the subsurface model boundary conditions parallel computing and how often to save results choices for boundary conditions are no flow 0 or free flow 1 for the subsurface bottom boundary and no flow for side boundaries of the domain to simulate free flow side boundaries for both overland and subsurface users can use specify additional boundary cells at each side of the simulation domain the number of boundary cells can be specified as parameters with value 0 no boundary cells being the default no flow condition or any integer for the corresponding number of boundary cells from each edge of the domain when boundary cells are used values in those cells are initialized and parameterized similar to other cells in the domain their values are also saved in the 3d output so that boundary fluxes can be calculated as needed however boundary cells are not considered in the feedback of the 3d flow model with other mlcan3d processes such as root water uptake users can also set the number workers used to run the 3d flow model in parallel outputs from the 3d flow model including the depth of ponded water w from equation 7 the soil moisture θ the total head h and the hydraulic conductivity k ψ from equation 4 are saved at user defined intervals the second section of the parameters window contains detailed specification of parameters for soil moisture retention and hydraulic conductivity as described in equations 2 and 3 users have three options for specifying the soil parameters 1 use the texture based option where no additional data input is needed and the soil parameters are determined based on the soil texture specified by the user during the model setup 2 use the single value option where the entire simulation domain is set to one homogeneous value based on user input and 3 use the 2d 3d matrix option where users can specify 3d heterogeneous soil parameters when using the 2d 3d matrix option for certain parameters the user must load a mat file containing data for each parameter help windows and error messages are available to direct users during the setup process table 3 data sets from neon used for model applications at osbs and ornl as model forcings and model verification table 3 data product id name osbs date range ornl date range dp4 00 200 001 bundled data products eddy covariance 2018 05 01 06 30 2020 05 01 06 30 dp1 00 003 001 triple aspirated air temperature 2018 05 01 06 30 2020 05 01 06 30 dp1 00 004 001 barometric pressure 2018 05 01 06 30 2020 05 01 06 30 dp1 00 023 001 shortwave and longwave radiation net radiometer 2018 05 01 06 30 2020 05 01 06 30 dp1 00 006 001 precipitation 2018 05 01 06 30 2020 05 01 06 30 dp1 00 098 001 relative humidity 2018 05 01 06 30 2020 05 01 06 30 dp1 00 001 001 2d wind speed and direction 2018 05 01 06 30 2020 05 01 06 30 dp1 10 058 001 plant presence and percent cover 2016 05 01 06 30 2020 06 15 06 29 dp3 30 015 001 ecosystem structure 2018 09 2016 06 dp3 30 024 001 elevation lidar 2018 09 2016 06 dp1 10 047 001 soil physical properties distributed initial characterization 2016 03 2016 08 dp1 00 094 001 soil water content and water salinity 2018 05 01 06 30 2020 05 01 06 30 dp1 00 041 001 soil temperature 2018 05 01 06 30 2020 05 01 06 30 table 4 selection of model vegetation parameters table 4 parameter units osbs ornl canopy structure a canopy height h m 25 28 leaf area index lai 0 74 5 2 foliage clumping factor ω 1 0 0 9 flux tower observation height m 35 40 canopy roughness length z 0 m 2 5 2 8 maximum water storage capacity of a leaf s m mm lai 0 2 0 3 leaf photosynthesis b maximum rubisco limited carboxylation rate at 25 c v cmax 25 μmol m 2 s 60 80 maximum electron transport rate at 25 c j max 25 μmol m 2 s 110 140 leaf respiration rate at 25 c r d 25 μmolco 2 m 2 s 0 3 0 3 root structure c root depth r d m 4 5 4 7 50th percentile rooting depth z 50 m 0 6 0 5 95th percentile rooting depth z 95 m 2 5 2 a jensen 2002 brutsaert 2013 campbell and norman 2012 kitchings and mann 1976 and neon eddy covariance dp4 00 200 001 and ecosystem structure dp3 30 015 001 data shown in table 3 b wright et al 2013 sampson et al 2006 walker et al 2014 c heyward 1933 
25658,in a companion paper sikorska senoner and quilty 2021 introduced the ensemble based conceptual data driven approach cdda for improving hydrological simulations this approach consists of an ensemble of hydrological model hm simulations generated via different parameter sets whose residuals are corrected by a data driven model one per hm parameter set resulting in an improved ensemble simulation through a case study involving three swiss catchments it was demonstrated that cdda generates significantly improved ensemble streamflow simulations when compared to the ensemble hm in this follow up study a stochastic version of cdda scdda is developed that in addition to parameter uncertainty accounts for input data input variable selection and model output uncertainty using several deterministic and probabilistic performance metrics it is shown that scdda results in significantly more accurate and reliable ensemble based streamflow simulations than the cdda ensemble and stochastic hms and a quantile regression based approach improving the mean interval score by 26 79 keywords ensemble stochastic streamflow simulation data driven model hydrological model uncertainty 1 introduction although data driven models ddms have been used for hydrological simulation and forecasting since the 1970 s remesan and mathew 2015 troin et al 2021 sometimes placed in opposition to their process based counterparts e g kim et al 2021 nearing et al 2020 there is an increasing interest in coupling process based conceptual and physical hydrological models hms with ddms in order to improve hm simulations and forecasts see for example tongal and booij 2018 senent aparicio et al 2019 wu et al 2019 konapala et al 2020 yang et al 2020a althoff et al 2021 cui et al 2021 sikorska senoner and quilty 2021 wang et al 2021a the main idea behind this coupled approach is that ddms have the ability to capture additional information about the hydrological process that is missed by the hm while the hm is able to provide the ddm with process based information that the ddm lacks due to its statistical formulation konapala et al 2020 approaches that directly couple a hm with a ddm go by different names including hybrid models konapala et al 2020 hydrologically informed machine learning herath et al 2021 physics informed hybrid models lu et al 2021 these approaches are placed in contrast with other methods that seek to impose physical constraints on the calibration training of physics guided ddms i e without directly incorporating a hm such as in xie et al 2021 that sometimes are referred to as theory guided or process guided models adombi et al 2021 read et al 2019 before proceeding it is important to clarify that the terms simulation and forecasting as used here adopt the convention of beven and young 2013 however as noted by the authors since the term prediction is used loosely in the hydrology and water resources literature it is taken here to be synonymous with simulation ddms have also been used to support different aspects of hm simulations for example ddms have been traditionally used as surrogate models for more computationally intensive hms razavi et al 2012 where new deep learning based approaches have demonstrated promise for this task gu et al 2020 boucher et al 2020 used different types of artificial neural networks ann for data assimilation within a hm showing their potential to produce reliable ensemble streamflow simulations while teweldebrhan et al 2020 used different machine learning models to generate the error response surface of a distributed hm for improved streamflow simulation more recently tsai et al 2021 showed how differentiable deep learning based ddms can be used to learn the responses and parameters of hms achieving substantially improved performance or at least similar performance at a lower computational cost compared to evolutionary and regionalization techniques while there are numerous approaches for coupling hms with ddms this work focuses on using an ensemble of hm simulations as input to a ddm in order to improve streamflow simulation although the literature includes an increasing variety of combined hm and ddm approaches as shown above and also in xu and liang 2021 an important and under studied niche topic within this area is how to effectively include uncertainty assessment in these combined hm and ddm approaches some examples from the literature follow mekonnen et al 2015 used a hm to simulate streamflow from contributing areas in the canadian prairies and adopted ann to correct the hm residuals which simulated streamflow from non contributing areas the authors considered the impact of uncertainty in the ann parameters on the coupled hm and ddm simulations humphrey et al 2016 followed a slightly different approach where the soil moisture simulated by a hm was used as input alongside other meteorological variables to bayesian ann for streamflow forecasting the hm and bayesian ann both estimated parameter and model output uncertainty it was found that the bayesian ann led to substantially better forecasts in terms of accuracy resolution and reliability when compared to the hm a similar approach by ren et al 2018 was undertaken where the hm was used to simulate variables representing the contribution of rain snow and glaciers to streamflow these variables were used as input alongside other meteorological variables to bayesian ann and least squares support vector regression for streamflow simulation the bayesian ann produced the most accurate and reliable simulations considering the two models both tyralis et al 2019c and papacharalampous et al 2019 post processed hm simulations the former by stacking two different quantile regression variants through gradient boosting and the latter by applying equal weight averaging to six different quantile regression variants resulting in sharp and reliable streamflow simulations in these studies the hm simulated streamflow and its time lagged copies were used as input to the quantile regression models li et al 2021a b used probabilistic long short term memory lstm networks to correct the residuals of a hm outperforming benchmark methods for streamflow simulation sikorska senoner and quilty 2021 hereafter referred to as ssq2021 proposed an ensemble based conceptual data driven approach cdda where the residuals from an ensemble of hm simulations were corrected by ddms one ddm for each hm ensemble member resulting in improved ensemble streamflow simulations in ssq2021 it was shown that using non linear ddms within cdda to simulate hm residuals and afterwards adding the ddm output to the hm simulations can lead to more accurate streamflow simulations than using the standalone ensemble hm however the cdda only considered parameter uncertainty in the hm when generating the ensemble streamflow simulations which was represented by confidence intervals hence this limitation of the cdda motivated the current study to consider several sources of uncertainty input data input variable selection ivs parameters and model output to generate ensemble simulations through a stochastic approach from which prediction intervals can be estimated thus providing a more thorough assessment of uncertainty when compared to confidence intervals the approach proposed in this study is referred to as the stochastic cdda or scdda the scdda relies on offline estimates of probability distributions for the input data ivs parameters and model output and uses this information during online stochastic simulation to generate a probability distribution of the variable to be simulated see section 3 4 in the scdda hm simulations are used as input to a ddm along with explanatory variables e g meteorological variables and optionally previous measurements of the target variable e g streamflow the scdda is inspired by the blueprint for stochastic process based models proposed in montanari and koutsoyiannis 2012 which was explored further in sikorska et al 2015 and subsequently modified for use with ddms incorporating multiscale change in quilty et al 2019 and quilty and adamowski 2020 some other recent studies inspired by the blueprint for stochastic hydrological simulation are contrasted with the scdda below although there are a limited number of studies capturing both hm and ddm uncertainty for coupled approaches to hydrological simulation and forecasting there are some important studies from the literature related to the blueprint and scdda that should be mentioned papacharalampous et al 2020a b proposed a probabilistic framework that generates a number of hm simulations using a single hm structure but with different parameter sets referred to as sister predictions and builds an error model for each of the sister predictions using quantile regression representing model error uncertainty the probabilistic prediction is obtained by subtracting the quantile regression predictions made at several quantiles from the sister predictions and then averaging the resulting predictions across each quantile in their approach the authors use each sister prediction as the sole explanatory variable and each sister prediction s error as the response variable the scdda differs from this approach in several key ways in scdda an ensemble of hm simulations represented by a probability density function pdf at each time step are used to represent input data uncertainty and are considered as predictors alongside meteorological variables and optionally previous measurements of the target variable within ddms that are used to directly simulate the target variable e g streamflow ivs uncertainty which is often neglected but recently shown to be an important source of uncertainty when using ddms quilty and adamowski 2020 is considered since there are multiple explanatory variables both ivs and parameter uncertainty are jointly estimated using bayesian optimization snoek et al 2012 and decision tree models the estimation of model error uncertainty relies on k nearest neighbours resampling sikorska et al 2015 a very simple non parametric method and instead of using a set of ddms to simulate a limited range of quantiles to define the probabilistic simulation at each time step the pdf of the target variable is estimated through stochastic simulation incorporating the above mentioned sources of uncertainty allowing for any quantile of the simulations that estimate the target probability distribution to be returned at any time step in ssq2021 eight different ddms were explored multiple linear regression mlr k nearest neighbours regression knn a second order volterra series model ann and two variants of extreme gradient boosting xgb and random forests rf the latter two methods xgb and rf were recommended for further use due to their high performance thus following this recommendation both xgb and rf were used in the scdda to account for additional sources of uncertainty not explored in cdda however similar to the cdda the scdda also generates ensemble based simulations therefore the main goal of this work is to explore the added value of the scdda when benchmarked with the cdda the stochastic version of the hm the ensemble version of the hm and the quantile regression based approach proposed by papacharalampous et al 2020a b the main novelty of this work lies in the development of a stochastic approach that couples multiple realizations of a hm i e a single hm structure but with simulations derived from different parameter sets with ddms the approach is general and can be used with any hm and any ddm as well as accommodate multiple hms and ddms although this is outside the scope of this study the scdda is applied to a daily streamflow simulation case study that includes three swiss catchments the same catchments and data adopted in ssq2021 the remainder of this paper is organized as follows section 2 introduces the methods adopted in the development of scdda section 3 provides details on the experimental settings used for testing the scdda section 4 describes the main results of the experiments section 5 discusses the major findings and section 6 provides concluding remarks 2 methods 2 1 overview of the ensemble based conceptual data driven approach cdda the ensemble based conceptual data driven approach recently introduced in ssq2021 consists of a hm to simulate a hydrological process e g streamflow that is coupled with a data driven model which is used to simulate the hm residuals in ssq2021 the target variable of the hm was streamflow while the target variable of the ddm was the hm residuals i e the difference between the hm simulated streamflow and observed streamflow the rest of this paper will refer to the specific case of streamflow simulation when describing the cdda and scdda although both approaches are generalizable to any hydrological variable for which an hm can produce simulations a single hm i e model structure is used within the cdda to produce an ensemble of streamflow simulations by running the model with different parameter sets attached to each hm parameter set is a ddm enabling the cdda to issue an ensemble of streamflow simulations while a single hm is used within cdda and scdda it is very straightforward to accommodate multiple model structures e g by applying an equal weight to each model structure and its ensemble simulations in detail each hm ensemble member simulated streamflow is associated with an individual realization of the ddm simulated error to provide an improved ensemble streamflow simulation as shown in equation 1 1 y i p t t d t t t d q t 1 t d θ c d d a i y i p t t t θ h m i r i p t t d t t t d q t 1 t d θ d d m i where y i y i and r i are the cdda hm and ddm simulations for ensemble member i respectively p t t d t t t d and q t 1 t d are observed precipitation and air temperature at lag times t t d and observed streamflow at times t 1 t d where d is the maximum time lag respectively θ c d d a i θ h m i and θ d d m i are the parameter sets of the cdda hm and ddm for ensemble member i respectively with θ c d d a i θ h m i θ d d m i the main limitation of the cdda is that it only considers parameter uncertainty associated with the hm thus when quantifying the uncertainty in the ensemble simulations produced by cdda at a particular confidence level only confidence intervals can be estimated since the model error uncertainty is not considered in addition to other important uncertainty sources such as input data and ivs 2 2 stochastic conceptual data driven approach scdda to overcome the above mentioned limitation of the cdda the stochastic conceptual data driven approach or scdda is introduced to account for input data ivs parameter and model output uncertainty allowing for prediction intervals to be estimated from an ensemble of streamflow simulations scdda which stems from the combined work of montanari and koutsoyiannis 2012 and quilty et al 2019 allows for coupling of both hms and ddms within a stochastic framework scdda can be implemented by adopting the same equation as the stochastic data driven forecasting framework sddff quilty et al 2019 2 f q q θ x ω ω f e q s θ x ω θ x ω f θ θ x ω f ω ω x f x x d θ d x where x is the input data for a given model s in this case a ddm θ represents the parameters supplied to the model ω ω is a binary vector identifying which input variables were selected during ivs f q represents the pdf of the target variable i e streamflow f e represents the conditional pdf of the model error f θ represents the conditional pdf of the parameters f ω represents the conditional pdf of ivs and f x represents the pdf of the input data to derive scdda in equation 2 x includes y i p t t t θ h m i t t d p t t d t t t d q t 1 t d where y i p t t t θ h m i t t d represents time lagged versions of a randomly selected hm simulation at times t t d since a randomly selected hm simulation and its time lagged versions are used as input to a ddm see section 3 3 1 for further details the parameters in equation 2 which are solely related to the ddm can be estimated independently of the hm parameters further if a ddm is adopted that inherently performs ivs as part of the parameter calibration stage such as in decision tree methods e g random forests and input data uncertainty is considered independent of the parameter uncertainty see montanari and koutsoyiannis 2012 for discussion on the reasonableness of this assumption then equation 2 can be simplified as montanari and koutsoyiannis 2012 3 f q q θ x f e q s θ x θ x f θ θ f x x d θ d x where it is assumed that estimating the model parameters will automatically determine the selected input variables hence by using the hm simulations as input to the framework uncertainties in both the hm and ddm can be accounted for in scdda in the experiments adopted in this study scdda only considers input data uncertainty with respect to the ensemble streamflow simulations from the hm in other words the pdf of the input data f x is represented by the ensemble of hm simulations since the remaining inputs are considered to carry no uncertainty and are thus fixed at their observed values however if one has access to reasonable estimates of uncertainty for these variables it can be easily incorporated within scdda see montanari and koutsoyiannis 2012 for a related example the pdf of the parameters f θ is estimated using bayesian optimization snoek et al 2012 as described in section 3 4 1 the conditional pdf of the model error f e is estimated using the validation set errors associated with the parameters defined by f θ please refer to section 3 4 1 finally the double integral in the scdda is estimated using monte carlo sampling and is described in further detail in section 3 4 2 to better understand the details necessary to implement the scdda readers are encouraged to review the mathematical background in montanari and koutsoyiannis 2012 as well as the sddff implementation details and workflow described in quilty et al 2019 and quilty and adamowski 2020 2 3 data driven models although any ddm can be used within scdda this study considers only two ddms random forests and extreme gradient boosting based on the recommendations of its companion paper sikorska senoner and quilty 2021 which adopted the same case study thus only rf and xgb will be described in this section 2 3 1 random forests random forests rf are a machine learning algorithm based on decision trees first introduced by breiman 2001 which have become popular in diverse disciplines due to the algorithm s capability to produce accurate and well generalized models even in cases with many input variables belgiu and drăgu 2016 biau 2012 díaz uriarte and alvarez de andrés 2006 iwendi et al 2020 svetnik et al 2003 zhou et al 2020 the rf regression algorithm involves bootstrapping both the input variables and training examples samples creating an ensemble of decision trees i e one for each bootstrap issuing a prediction for each decision tree and aggregating the result of each decision tree by applying an equal weight to each models predictions to produce the final prediction since rf utilize bagging i e bootstrapping the data and aggregating predictions associated with each bootstrap the rf can reduce predictive variance without increasing the bias of the model breiman 1996 2001 another significant benefit of rf is that it implicitly estimates input variable importance which can be used for ivs speiser et al 2019 in the hydrology and water resources domains rf are becoming more popular for example with recent exploration in regional flood quantile estimation at ungauged sites desai and ouarda 2021 estimating changes in streamflow response due to forest cover li et al 2020 and estimating root zone soil moisture carranza et al 2021 the interested reader may refer to tyralis et al 2019b for a review on the use of rf in the water resources domain as ivs is an important feature of rf this study utilizes guided regularized random forests grrf since it has been shown to select input variables that result in improved prediction models when compared to those selected by rf deng and runger 2013 interestingly grrf has yet to be used in the hydrology or water resources domains although it has been used with success for different applications including genetic population assignment sylvester et al 2018 classification of invasive plant species omeer and deshmukh 2021 and remote sensing izquierdo verdiguier and zurita milla 2020 the grrf input variable selection process selects a subset of relevant and non redundant inputs by assigning penalties through regularization to information retrieved from previous nodes in the decision tree rf and grrf were implemented using the rrf r package deng 2013 2 3 2 extreme gradient boosting the extreme gradient boosting method a variant of the gradient tree boosting algorithm provides computationally efficient accurate and well generalized models useful for a host of diverse real world problems chen and guestrin 2016 unlike rf which utilizes bagging for ensemble model predictions the xgb algorithm produces an ensemble of models where each tree is combined in sequence and scaled by a learning rate resembling the gradient boosting algorithm in addition to maintaining a generalized model xgb uses l2 norm regularization to reduce the likelihood of overfitting chen and guestrin 2016 finally similar to rf input variable importance is estimated within xgb which can be used for ivs alsahaf et al 2022 adoption of xgb in the hydrology water resources and complimentary fields include applications to daily streamflow forecasting tyralis et al 2019a wetland classification delancey et al 2019 predicting groundwater levels ibrahem ahmed osman et al 2021 correcting precipitation forecasts ko et al 2020 predicting daily global solar radiation fan et al 2019 and predicting decontamination effects on bioretention cells wang et al 2021b xgb was implemented using the xgboost r package chen et al 2021 2 4 input variable selection input variable selection is considered an essential step when designing a ddm in ivs the goal is to select only relevant input variables and to remove irrelevant and redundant variables which can deteriorate model performance galelli et al 2014 despite certain ddms requiring an external ivs method e g ann support vector machines the two ddms applied here i e rf and xgb inherently perform ivs when estimating model parameters using the same scenario as in ssq2021 air temperature precipitation and simulated streamflow for the current and previous nine days were considered along with the previous nine days of observed streamflow as potential additional inputs to the ddms case 1 a second case case 2 was also considered where observed streamflow was not included in the set of potential inputs e g for situations where a site is no longer gauged but streamflow simulations may still be desired thus in total 39 and 30 input variables were considered for each of the ddms in case 1 and case 2 respectively variable importance scores were used to identify the different input variables selected by rf and xgb reflecting the relative importance of each input with respect to the final model prediction where higher variable importance scores indicate inputs with higher importance rf and xgb operate in a similar way in terms of estimating variable importance where a particular metric is calculated for and averaged across all decision trees to estimate input variable importance during node splitting an impurity score is calculated by rf while a gain score is calculated by xgb see li et al 2019 for additional details the grrf method adopted here follows a three step ivs process in the first step rf is used to estimate the variable importance score v i k for each input variable k 1 k where k is the number of inputs the scores are then normalized in the second step a penalty weight λ k 0 1 is assigned to each normalized variable importance score n v i k which can be calculated by deng and runger 2013 4 λ k 1 γ λ 0 γ n v i k where γ 0 1 is an importance weight and λ 0 0 1 is a regularization coefficient note that the lower the penalty weight λ k the less weight a given input variable receives in other words input variables with a high v i are assigned a high λ k and thus have a higher impact on the resulting model predictions in the third step grrf is run using the vector of penalty weights λ 1 λ k to guide the ivs process the grrf model thereafter provides a set of new variable importance scores that can be used to identify selected input variables and their relative influence on the grrf model predictions additional information on grrf can be found in deng and runger 2013 in the next sub section the various uncertainty sources considered in the scdda and its benchmarks will be discussed 2 5 uncertainty sources associated with the different approaches the scdda which enables the estimation of the pdf of the target variable f q in this case streamflow considers and requires estimation of the following sources of uncertainty input data uncertainty f x represented by the ensemble of hm simulations generated by a single hm structure but using multiple parameter sets additional details can be found in section 3 3 1 and 3 4 1 parameter and ivs uncertainty f θ represented by the collection of ddm parameters estimated by bayesian optimization during calibration of the ddms i e whose simulation quality was compared against observed streamflow q using the calibration dataset since the adopted ddms rf and xgb inherently perform ivs when calibrating model parameters ivs uncertainty is implicitly considered in the parameter uncertainty given that the grrf ivs method requires the specification of two hyper parameters λ 0 and γ these hyper parameters were optimized along with rf hyper parameters during model calibration note xgb does not require the specification of any hyper parameters specific to ivs additional details can be found in section 3 3 2 and 3 3 3 model error uncertainty f e represented by the collection of validation set errors attached to each ddm i e the collection of ddms resulting from bayesian optimization of model parameters it is important to note that all other sources of uncertainty not explicitly accounted for in the input data and parameters as well as ivs such as initial conditions of the hm are captured within the model error uncertainty additional details can be found in section 3 4 1 the stochastic hm incorporates and requires estimation of parameter uncertainty associated with different parameter sets attached to a single hm structure and model error uncertainty validation set errors attached to each set of hm parameters i e resulting from parameter optimization the quantile regression based approach of papacharalampous et al 2020a b hereafter referred to as qr takes into account and requires estimation of parameter uncertainty associated with the hms and model error uncertainty the ensemble hm incorporates and requires estimation of parameter uncertainty while the cdda incorporates and requires estimation of the same source of uncertainty as the ensemble hm it does not consider uncertainty in the ddms used to correct the ensemble hm residuals thus the stochastic hm qr ensemble hm and cdda each provide an estimate of the pdf of the target variable f q each considering fewer sources of uncertainty than scdda since scdda the stochastic hm and qr each consider at least model error uncertainty they can be used to generate prediction intervals using for example the 0 025 and 0 975 quantiles of the simulations used to estimate f q representing prediction intervals at the 95 confidence level since the ensemble hm and cdda only consider parameter uncertainty the quantiles of the simulations used to estimate f q at a prescribed confidence level are similar to confidence intervals 3 experimental settings 3 1 study catchments data and dataset partitioning the proposed scdda was tested on the same three medium sized swiss mountainous catchments from the companion study sikorska senoner and quilty 2021 dünnern at olten 234 km2 kleine emme at littau 478 km2 and muota at ingenbohl 317 km2 these catchments are considered free from glaciers i e with an areal glacier percentage 5 and without any significant human impact during the observation period 1981 2015 the hydrological processes within these catchments are driven by rainfall dynamics dünnern or mixed rainfall and snowmelt processes kleine emme and muota see sikorska senoner and seibert 2020 for additional details the data for the three catchments include daily observations of mean air temperature c precipitation mm day evaporation rates mm day and streamflow mm day at the respective catchment outlet the data for each catchment was divided into four distinct partitions warm up 1981 1984 calibration 1985 2004 validation 2005 2009 and testing 2010 2014 the use of these distinct partitions for developing and applying the various models is explained in the sub sections that follow 3 2 hydrological model set up the hbv light seibert and vis 2012 conceptual hm was adopted in this study following the same set up as in the companion study sikorska senoner and quilty 2021 hbv light is suitable for the study catchments since it simulates rainfall and snowmelt processes the model output is a single realization of a continuous streamflow time series at the catchment outlet at a daily time step hbv light was calibrated for each catchment using the metaheuristic approach outlined in sikorska senoner et al 2020 in ssq2021 hbv light was calibrated separately for each catchment using daily data for 1985 2004 with a 4 year warm up period of 1981 1984 the calibration exercise resulted in 1000 optimized model parameter sets that were afterwards used to generate an ensemble of streamflow simulations at the outlet of each catchment during the period 1985 2014 the ensemble streamflow simulations from ssq2021 along with daily measurements of observed streamflow mean air temperature and precipitation are considered in this study when developing the scdda stochastic hm and qr models the ensemble hm and cdda simulations from ssq2021 are also used as additional benchmarks for scdda for a more detailed description of hbv light and its calibration in the study catchments the reader is referred to sikorska senoner and quilty 2021 3 3 ddm set up 3 3 1 overview for the ddms the calibration dataset 1985 2004 included observed streamflow at time t i e q t as the target variable and 1 m i 1 m y i p t t t θ h m i t t d p t t d t t t d q t 1 t d as the input variables where m is the number of ensemble members see also section 2 2 the mean of the ensemble hm simulations and its time lagged versions were used during calibration since it was more convenient and time efficient than calibrating multiple ddms for each of the hm ensemble members separately this approach is reasonable as the input data and parameter uncertainties were considered independent of one another see section 2 2 however if one wanted to take full advantage of the ensemble hm simulations instead of using only the mean of the ensemble hm simulations when calibrating the ddms then a single ddm could be trained on a dataset that includes the concatenation of each member of the ensemble hm simulations and their time lagged versions with the remaining inputs in a single matrix i e y i p t t t θ h m i t t d p t t d t t t d q t 1 t d i likewise the target variable would be concatenated m times in order to match the corresponding input variables note that this approach would result in a calibration set that is m times larger than the approach adopted here leading to longer model training times that may be prohibitive depending on available computational resources once prepared the calibration dataset was used for optimizing the ddms parameters via bayesian optimization resulting in multiple parameter sets defining the pdf of the model parameters f θ each set of parameters for each ddm rf and xgb was then used to generate a set of simulations for the validation dataset resulting in an estimate of the model error pdf f e the ddms were then run in simulation mode on the test set as described in section 3 4 in order to estimate f q next specific settings related to rf grrf and xgb are discussed 3 3 2 random forests and guided regularized random forests the rf models considered two hyper parameters the number of trees in the forest b and the number of variables selected randomly during node splitting m additionally grrf considered two hyper parameters the regularization coefficient λ 0 and importance weight γ by exploring different values of the four hyper parameters in this case through bayesian optimization the pdf of the rf parameters variables included at each split and split points represented by f θ can be estimated the following range of values were explored for each hyper parameter where integer values are appended with an l b 50l 300l m 1l 15l λ 0 0 01 1 γ 0 1 all four hyper parameters were optimized simultaneously at first an initial rf model was run for a pair of b and m values afterwards the normalized variable importance scores calculated from the resulting rf model were used alongside a pair of λ 0 and γ values to calculate λ k for each input variable according to equation 4 finally after applying the set of λ k values to the input variables the grrf algorithm was run the grrf outputs included the updated variable importance scores and any input variables that had non zero variable importance scores were then used to develop the final rf model based on the same b and m values additional information on the optimization procedure is provided in section 3 4 1 3 3 3 extreme gradient boosting the xgb models had a total of nine hyper parameters that required optimization which influenced the model parameters splitting variables and split points the range of values assigned to each hyper parameter is provided in ssq2021 while a description of each hyper parameter is included in the references mentioned therein and is not repeated here for brevity bayesian optimization was used to optimize xgb hyper parameters in the same way as with rf and grrf out of the nine hyper parameters only the nrounds hyper parameter representing the number of trees included in the model was optimized via five fold cross validation which also included an early stopping criterion set at five rounds the cross validation procedure was carried out within the bayesian optimization routine allowing for the eight other hyper parameters to remain fixed as the nrounds hyper parameter was optimized 3 4 stochastic conceptual data driven approach scdda the scdda is comprised of two separate components an offline mode where the various pdfs f x f θ and f e are estimated and an online mode where for a set of new inputs x the stochastic simulation procedure is run to obtain an estimate of f q more information on the offline and online modes of scdda is provided below scdda was developed using custom r scripts 3 4 1 estimation of probability density functions offline mode as noted in section 2 2 the pdf of the input data f x was represented by the ensemble of hm simulations 1000 members from ssq2021 as the remaining input variables p t t d t t t d and q t 1 t d remain fixed the pdf of the parameters f θ was obtained by running bayesian optimization for 100 iterations resulting in 100 ddm parameter sets as determined by the hyper parameters explored during optimization bayesian optimization was used for minimizing the root mean square error on the calibration dataset since the bayesian optimization algorithm adopted here is based on gaussian process regression an initial set of points were needed to estimate the surrogate model which was selected as 10 the parbaeysianoptimization wilson 2019 and doparallel calaway et al 2019 r packages were used for training the ddms in parallel additional information on the bayesian optimization method can be found in ssq2021 as well as trierweiler ribeiro et al 2020 and zuo et al 2020 the 100 parameters sets for each ddm rf and xgb were then used to generate a set of simulations for the validation dataset and afterwards subtracted from the observed streamflow also for the validation set resulting in an estimate of the model error pdf f e thus each parameter set was associated with a set of validation errors in earlier studies montanari and koutsoyiannis 2012 sikorska et al 2015 quilty et al 2019 quilty and adamowski 2020 the model error uncertainty was estimated using validation set errors associated with model realizations for a single parameter set the approach adopted here has higher potential to better represent the model error uncertainty as it incorporates errors from multiple ddms rather than a single ddm 3 4 2 stochastic simulation online mode stochastic simulation within the scdda can be accomplished through the following steps see also quilty and adamowski 2020 1 for a new set of air temperature and precipitation observations p t t t a hm parameter vector θ h m i is picked at random from θ h m 1 θ h m m and a hm simulation generated y i p t t t θ h m i 2 the hm simulation and its time lagged versions are appended to time lagged versions of the new set of air temperature and precipitation observations and time lagged versions of the observed streamflow resulting in x y i p t t t θ h m i t t d p t t d t t t d q t 1 t d which is equivalent to drawing a random sample from the probability density function f x x note if one does not have access to or wish to use observed streamflow then x y i p t t t θ h m i t t d p t t d t t t d 3 a ddm parameter vector is picked at random from the pdf f θ θ 4 using the sampled information θ x an output is generated via the ddm represented by s θ x 5 a random error is picked up from the conditional pdf f e q s θ x θ x and added to s θ x 6 steps 1 to 5 are repeated a sufficient number of times n leading to n different simulations of q 7 the n different simulations of q provide an estimate of f q q resampling from the conditional pdf for the model error i e step 5 is carried out through k nearest neighbours resampling using the validation set attached to the randomly sampled parameter vector θ for further details please refer to sikorska et al 2015 similar to earlier studies quilty and adamowski 2020 quilty et al 2019 10 nearest neighbours were considered when resampling from the conditional pdf of the model error note that the result of running step 5 of the stochastic simulation procedure n times provides an assessment of model output uncertainty thus even if steps 1 to 4 were ignored i e input data and parameter uncertainties were not considered one would still have access to the model output uncertainty similarly other steps of the stochastic simulation procedure may be ignored in order to focus on specific aspects of the scdda such as uncertainty solely due to the input data it is important to note that only data in the test set should be used in the stochastic simulation procedure since the calibration data was used to estimate f x and f θ while validation data was used to estimate f e in this way a more realistic assessment of out of sample performance can be carried out by comparing observed q to f q q and its related statistics such as the mean or median on the test data 3 5 benchmarks the scdda was benchmarked against four other approaches in order to assess any relative gains in simulation accuracy reliability and sharpness see section 3 6 as noted earlier since xgb and rf performed best with respect to six other variants tested in ssq2021 these two ddms were incorporated within scdda further since observed streamflow is not always guaranteed to be measured at regular intervals or since it may only be recorded for a previous historical period two variants of scdda were considered for each ddm with xgb wqobs and rf wqobs and without xgb and rf time lagged versions of observed streamflow as model inputs scdda considered input data input variable selection parameter and model output uncertainty two of the benchmarks the ensemble hm and the cdda are from ssq2021 representing approaches that only consider parametric uncertainty the ddm that performed best in each catchment xgb was used as the cdda benchmark the third benchmark was a stochastic hm that considered only steps 1 5 6 and 7 from the procedure outlined in section 3 4 2 at step 5 s θ x was swapped for y i p t t t θ h m i the stochastic hm considered both parametric and model output uncertainty the final benchmark was the quantile regression based approach from papacharalampous et al 2020a b qr described in section 1 papacharalampous et al 2020a b proposed six different schemes of their qr approach the qr approach adopted here is referred to as ensemble scheme 5 since the details of this approach are quite involved the reader is directed to papacharalampous et al 2020b for a thorough explanation of the approach the qr approach makes use of the quantreg r package koenker 2019 and adopts the frisch newton approach koenker and portnoy 1997 for estimating the slope and bias coefficients of the qr models the stochastic simulation procedure was run for 1000 iterations n 1000 for the stochastic methods scdda and its hm counterpart resulting in 1000 simulations representing f q q also referred to as ensemble members the ensemble methods also included 1000 ensemble members representing f q q the qr approach cannot estimate f q q directly and instead estimates selected quantiles of the streamflow as described in the sub section below 3 6 performance evaluation several deterministic and probabilistic performance metrics were adopted for comparing scdda against the various benchmarks since the adopted metrics are well known in the hydrology and water resources communities the interested reader can refer to the cited studies for detailed explanations and formulae for these metrics the deterministic metrics include the kling gupta efficiency index kge nash sutcliffe efficiency index nse root mean square error rmse and mean absolute error mae althoff and rodrigues 2021 which were calculated using the hydrogof r package zambrano bigiarini 2017 the probabilistic metrics include the mean interval score mis prediction interval coverage probability cp prediction interval average width aw papacharalampous et al 2020a mean continuous ranked probability score crps gneiting and raftery 2007 and the alpha index α r renard et al 2010 the crps was calculated using the scoringrules r package jordan et al 2019 while the remaining probabilistic metrics were estimated using custom r functions of the adopted probabilistic metrics both the cp and α r assess the reliability of a series of probabilistic simulations while the aw measures the simulations sharpness the mis and crps summarize both sharpness and reliability in a single score of note a key goal when issuing probabilistic simulations is to obtain simulations that are both reliable and sharp in general if a probabilistic simulation is not reliable then it is of little value in practical settings however if two competing methods provide equally reliable simulations the method that provides sharper simulations should be preferred the above mentioned metrics which summarize different aspects of deterministic and probabilistic performance are supported by a graphical assessment of simulation quality that includes time series plots plots showing the pdfs of simulations at high flow events coverage probability plots cpps also referred to as predictive probability probability plots probability plots or q q plots laio and tamea 2007 koutsoyiannis and montanari 2022 and plots evaluating performance as a function of ensemble member size it should be noted that the α r metric is directly linked to the cpp and represents the area between a simulation models coverage probabilities with respect to the theoretical coverage probabilities which should be uniform on 0 1 and the cpp bisector a perfectly reliable model will have α r 1 the deterministic metrics were evaluated using the median of f q q for scdda cdda ensemble and stochastic hms and qr to evaluate both the crps and α r the full distribution of f q q is needed since the qr approach does not generate the full distribution of f q q but instead a selected range of quantiles here 0 025 0 5 and 0 975 both crps and α r are not computed for this approach however to evaluate the mis cp and aw metrics upper and lower prediction intervals for a particular confidence level here 95 are required thus to compute 95 prediction intervals 0 025 and 0 975 quantiles were estimated by the qr approach and the same quantiles were calculated using the simulations that estimate f q q for scdda cdda and both ensemble and stochastic hms unless specified otherwise all deterministic and probabilistic metrics were calculated using 1000 ensemble members with qr being an exception for all metrics in the next section the scdda and its benchmarks are compared against one another using the metrics and graphical tools outlined above 4 results this section highlights the main results including a quantitative assessment of deterministic and probabilistic model performance a comparison between the approaches prediction intervals pdfs at select high flow events and coverage probability plots as well as the effect of ensemble size on model performance appendix a supplementary data includes additional results that support the main findings while appendix b provides references to the software used to generate the various plots in this section 4 1 quantitative assessment of deterministic and probabilistic model performance in table 1 the deterministic performance of the scdda variants is compared against each of its benchmarks cdda ensemble and stochastic hms and qr using the median of f q q the probabilistic performance of each approach is reported in table 2 although results for the crps and α r are not available for qr as noted in section 3 6 through tables 1 and 2 it is clear that at least one of the scdda variants provides superior or at least the same performance as the benchmarks considering all metrics for example the best scdda variant when compared against the stochastic hm in terms of mae provides relative improvements of 33 48 across the three catchments when considering the median of f q q comparing the best scdda variant against cdda reveals improvements in the mae between 13 and 35 across the three catchments it can also be seen that including observed streamflow at previous time lags as inputs to the ddm significantly improves deterministic performance with a decrease in mae of 18 45 across all three catchments improvements in performance being nearly identical for both ddms i e when comparing xgb wqobs vs xgb and rf wqobs vs rf it is also worth noting that the cdda included previous time lags of observed streamflow as input to the ddms thus a comparison with the xgb and rf scdda variants is not as straightforward it is important to note that both the stochastic hm and qr have very similar performance this is not surprising as both methods are quite similar the stochastic hm uses k nearest neighbours resampling to estimate the pdf of the model error while qr uses quantile regression to estimate quantiles of this pdf both methods relying on the simulations generated by the ensemble hm the stochastic hm and qr appear to provide equal estimates of the median of f q q while qr is able to estimate the median of f q q more efficiently than the stochastic hm the latter is able to provide an estimate of the entire distribution of f q q which is especially useful for simultaneously estimating simulation reliability across all quantiles afforded by the sample size table 2 reveals that the best scdda variants provide significant improvements in probabilistic performance for most metrics when compared against the benchmark methods for example across all three catchments the best scdda variant improves the mis by 74 79 compared to the ensemble hm 56 67 compared to the cdda 29 38 compared to the stochastic hm and 26 33 compared to qr since the mis is based on a particular confidence level here 95 using a probabilistic metric that considers the full probability distribution such as the crps may be more useful for comparing probabilistic performance a similar conclusion is reached when considering the crps the best scdda variant is able to provide improved performance compared to its benchmarks with decreases in crps across all three catchments of 43 56 compared to the ensemble hm 20 41 compared to the cdda and 32 46 compared to the stochastic hm using previous time lags of observed streamflow as input to the ddms crps was decreased by 17 41 for xgb and 17 44 for rf across all three catchments i e by comparing the crps of xgb wqobs vs xgb and rf wqobs vs rf by examining the cp and α r it is clear that the ensemble hm and cdda provide a relatively lower level of reliability compared to the stochastic approaches in the next sub section the prediction intervals related to the cp results in table 2 are examined via time series plots in section 4 3 pdfs related to probabilistic simulations at high flow events are studied and in section 4 4 cpps are used to assess reliability across the entire range of quantiles afforded by the sample size which is summarized by the α r reported in table 2 4 2 prediction intervals the 95 prediction intervals 95 pis for the scdda variants and the benchmarks are presented in figs 1 3 for all three catchments using a 30 day period containing the highest flow event in the test set similar plots considering the entire test set are included in the supplementary data appendix a the pis for the scdda variants the stochastic hm and qr include the observations throughout most of the period examined in figs 1 3 while the ensemble hm and cdda do not capture the high flow events at dünnern and muota catchments thus it is clear that the scdda provides a substantial improvement in reliability when compared to cdda for the study catchments in addition to these findings there are three interesting features to take away from figs 1 3 1 the scdda variants have nearly equal reliability as the stochastic hm but result in sharper pis 2 compared to the stochastic hm the pis for the scdda variants have a higher spread at high flows and a lower spread at low flows which is a desirable feature of probabilistic streamflow simulations 3 the xgb wqobs and xgb scdda variants tend to provide more conservative high flow simulations compared to their rf counterparts which allows xgb wqobs and xgb to capture two high flow events occurring on two consecutive days where the larger flood on the second day is missed by the rf variants and the stochastic hm in the next sub section pdfs of the simulations produced by the scdda variants cdda and the ensemble and stochastic hms are compared against one another for the highest flow event that occurred at each catchment in the test set as depicted in figs 1 3 4 3 probability density functions of simulations at high flow events while pis can be useful for evaluating the quality of probabilistic simulations at a prescribed confidence level the pdf of a probabilistic simulation can provide additional insights about the simulation s characteristics and quality modality spread etc that is not apparent from a single set of pis in figs 4 6 the pdfs of the scdda variants cdda and the ensemble and stochastic hms are provided for the highest flow event in the test period at each catchment additionally each sub plot in figs 4 6 includes the observed value for the high flow event along with the mean of the pdf additional details about the pdfs are provided in appendix b from figs 4 6 it can be seen that the ensemble hm and cdda produce very narrow pdfs that are approximately symmetric about their mean value with the exception of kleine emme the pdfs of the ensemble hm and cdda do not contain the observed value in contrast the stochastic approaches generate asymmetric pdfs with multiple modes at the depicted high flow events with the exception of rf wqobs and rf at muota catchment and in most cases the pdf of the simulation associated with each stochastic approach includes the observed value of further significance the mean of the stochastic approaches pdfs is very close to the observed value with the exception of muota catchment however at muota catchment the xgb based scdda variants pdfs have non negligible density for the range of flows in close proximity to the observed value which as can be seen in fig 3 represents a higher magnitude flood following a similar flood event the day before the pdfs of each model for the flooding event preceding the highest flow event in the test set at muota catchment are provided in the supplementary data appendix a in summary figs 4 6 aptly show the value in adopting the stochastic hm and scdda variants for simulating flood events in the study catchments in the next sub section coverage probability plots are used to assess the reliability of the various models 4 4 coverage probability plots the cpps for the scdda variants cdda and ensemble and stochastic hms associated with all three study catchments are included in fig 7 analysis of the cpps demonstrates that first the scdda variants provide much more reliable simulations in comparison to the cdda approach which significantly overestimates low quantiles and underestimate high quantiles referred to as narrow simulations laio and tamea 2007 second it appears that all scdda variants generate simulation quantiles that closely match the theoretical quantiles i e lie close to the bisector line as indicated by the high α r values with slight underestimation of low quantiles and overestimation of high quantiles indicative of large simulations laio and tamea 2007 similar patterns are observed for all three study catchments although the inflection point from under prediction to over prediction occurs at different quantiles in different catchments third at least one of the scdda variants had equal or better reliability than the stochastic hm the cpps for the ensemble hm and cdda clearly demonstrate that parametric uncertainty alone with respect to the specific formulation of these models is insufficient for generating reliable streamflow simulations in all three catchments although using different hms and or ddms in these approaches may lead to different results in contrast the cpps for the stochastic hm show that by accounting for model output uncertainty in addition to parametric uncertainty reliability can be significantly improved while the cpps along with tables 1 and 2 for the scdda variants show that the inclusion of input data input variable selection parameter and model output uncertainty within a coupled hm ddm approach can lead to further improvements in reliability along with accuracy and sharpness the next sub section explores the effect of ensemble size or the number of simulations considered in the stochastic models on probabilistic performance 4 5 effect of ensemble size on model performance the results presented in tables 1 and 2 as well as figs 1 7 are all based upon 1000 ensemble members with the exception of qr see section 3 6 consistent with the results reported in ssq2021 however it is often a valuable exercise to evaluate the impact of ensemble size on model performance as using fewer ensemble members may not only lead to similar or even improved performance but also lower computational expense as in ssq2021 the effect of the ensemble size on model performance was evaluated using the crps the results are reported in fig 8 similar to ssq2021 it appears that approximately 100 ensemble members provide nearly the same model performance as the 1000 member ensemble it can be seen that the crps significantly improves from 1 to 100 ensemble members while increasing the number of ensemble members beyond 100 leads only to marginal improvement interestingly the same effect was observed for the ensemble hm and the various cdda variants based on eight different ddms in ssq2021 this observation provides further justification for using 100 different parameter sets for defining parametric uncertainty i e f θ θ for the ensemble and stochastic models explored herein 5 discussion in section 4 it was demonstrated that at least one of the scdda variants xgb wqobs xgb rf wqobs rf provided superior deterministic and probabilistic performance when compared against its benchmarks cdda ensemble and stochastic hms and qr in all three study catchments this section begins by commenting on the positive characteristics of the scdda s probabilistic simulations at first the pdfs of the scdda simulations with respect to the approach s overall reliability will be discussed followed by remarks on the importance of including previous time lags of observed streamflow as model inputs the section ends by focusing on some future improvements that may be realized by modifying the scdda e g to account for multiscale change along with recommendations for testing its generality through large sample studies when exploring the pdfs of the scdda variants cdda and ensemble and stochastic hms for the highest flow events in the test set it was found that the stochastic methods generally produced multi modal asymmetrical distributions with mean values close to the observed value with the xgb wqobs and xgb scdda variants being the only models containing the highest flow events within the support of their pdfs across all three catchments interestingly although the xgb wqobs and xgb scdda variants can be considered highly reliable see table 2 and figs 1 7 their overall reliability as measured by the alpha index α r did not surpass their respective rf counterparts across all three catchments nor the stochastic hm for dünnern and kleine emme catchments although some scdda variants had lower α r values than the stochastic hm this should not necessarily be understood as a negative characteristic of the approach s simulation quality since in all cases the scdda variants demonstrated near perfect reliability or had large more conservative simulations reasonably conservative simulations can be particularly helpful for mitigating potential effects due to droughts or floods for example in the muota catchment the xgb wqobs and xgb scdda variants which generated slightly more conservative simulations than rf wqobs were able to assign a non negligible probability to higher flows see fig 6 capturing the largest flood in the test set that was missed by the cdda the ensemble hm and the other stochastic models the xgb wqobs and xgb scdda variants were also able to capture a nearly equal in magnitude flood on the preceding day producing very similar pdfs for both flood events compare fig 6 with figure sa4 in the supplementary data appendix a thus it is likely that the xgb wqobs and xgb scdda variants could be very useful tools for flood forecasting in the study catchments and are recommended for this purpose instead of the other stochastic alternatives cdda and the ensemble hm for muota catchment it was also shown that including previous time lags of observed streamflow as input to the ddms can significantly enhance simulation accuracy and reliability tables 1 and 2 in cases where streamflow observations may not be available at a regular daily interval the xgb and rf scdda variants may be used instead of the stochastic hm as they provide higher accuracy and in most cases reliability however if regular streamflow observations are available it is likely that the stochastic hm and therefore the scdda variants could be further improved by accounting for uncertainty in the state variables likewise uncertainty estimates in other model inputs such as rainfall could be considered as in wu et al 2021 furthermore given that the stochastic hm and the scdda variants can produce probabilistic simulations whose pdfs have different shapes see fig 6 it may also be fruitful to combine probabilistic simulations from the various models through a weighted average for example see equation 10 in montanari and koutsoyiannis 2012 it is important to note that the scdda is not restricted to the studied hm and ddms but applicable to any hm and ddm such as those generated via flexible hydrological modelling frameworks e g raven craig et al 2020 and new deep learning methods e g bayesian long short term memory networks lu et al 2021 respectively additionally the scdda could be coupled with wavelet decomposition to increase the approach s ability to handle multiscale change an inherent characteristic of many hydrological processes including streamflow ho et al 2017 tang and cao 2021 each of these potential modifications to the scdda represents further research avenues that may be worthwhile to explore while each of the abovementioned ideas have potential to improve the quality of scdda s simulations they are outside the scope of this study and are left for future work given the high performance of scdda in the study catchments future studies could also test scdda in contrasting climates semi arid tropical etc as well as exploring scdda in large scale experiments papacharalampous et al 2020a b for regional streamflow simulation kratzert et al 2019 lees et al 2021 and simulating water quantity or quality in ungauged basins bourgin et al 2015 qi et al 2020 yin et al 2021 while there will likely remain a high demand and interest in the use of process based hms despite the promising future of ddms in hydrology nearing et al 2020 shen et al 2021 it is anticipated that process based modellers will be interested in seeing how far their hms can be improved by augmenting their models with ddms given that hms and especially ensemble hms are often post processed using statistical techniques biondi et al 2021 siqueira et al 2021 together the stochastic hm and the scdda allow the hydrologist engineer etc to quantitatively and qualitatively assess the added value in adopting ddms in the hydrological simulation and or forecasting chain using the various plots and metrics adopted in this study overall it is clear that the scdda provides enhanced deterministic and probabilistic performance when compared to the stochastic hm and other benchmarks i e ensemble hm and qr and can generate reliable multi modal simulation distributions that may be especially useful for decision making subject to uncertainty 6 conclusions there is increasing interest in the hydrology and water resources communities to couple process based conceptual and physics based hydrological models hms with data driven models ddms since ddms can be used to estimate complex relationships between target and explanatory variables missed by the hm and at the same time take advantage of the physics based information contained in the hm output however it is rare for such coupled hm ddm approaches to account for uncertainty thus in a companion study sikorska senoner and quilty 2021 the ensemble conceptual data driven approach cdda was proposed to address this gap however the cdda only accounted for parameter uncertainty this follow up study introduced the stochastic cdda scdda to account for multiple uncertainty sources such as input data input variable selection parameter and model output with the goal of assessing the added value in adopting scdda instead of benchmark approaches including cdda ensemble and stochastic hms and a quantile regression based approach qr to achieve this goal scdda variants based on extreme gradient boosting and random forests with xgb wqobs and rf wqobs and without xgb and rf previously observed streamflow considered as model inputs along with precipitation and mean air temperature were compared against the benchmarks for daily streamflow simulation in three swiss catchments using popular deterministic and probabilistic performance metrics e g mean absolute error mae mean interval score mis and various graphical tools e g time series plots coverage probability plots the main conclusion is that at least one of the scdda variants provides significantly better deterministic and probabilistic performance when compared against benchmark methods across all catchments e g with improvements in the mis ranging between 26 and 79 by including previous observations of streamflow as input to scdda improvements in mae of 18 45 were achieved of interest to traditional process based modellers the stochastic hm can be directly compared against scdda to assess the added value of augmenting hms with ddms within the stochastic framework overall scdda represents a promising approach for coupling hms and ddms within a stochastic framework that can be applied for hydrological simulation and forecasting in the face of uncertainty data availability the observed streamflow data can be ordered from the foen https www bafu admin ch last access december 11 2020 while the observed meteorological data be purchased from meteoswiss http www meteoswiss ch last access december 11 2020 the most recent version of the hbv light model can be downloaded from https www geo uzh ch en units h2k services hbv model html funding this research received financial support through the following sources natural sciences and engineering research council discovery grant launch supplement jq university of waterloo department of civil and environmental engineering graduate student support allowance jq queen elizabeth ii graduate scholarship in science and technology dh and university of waterloo president s graduate scholarship dh the agencies institutions providing the above mentioned financial support did not influence the research contained herein or the decision to publish findings related to this research declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the authors gratefully acknowledge meteoswiss for providing the meteorological data used in this study and the swiss federal office for the environment foen for providing the streamflow data used in this research in accordance with project no 15 0054 pj o503 1381 the hbv models adopted in this paper were calibrated using the sciencecloud provided by s3it at the university of zurich appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105326 appendix b software packages the figures included in section 4 were generated using one or more of the following r packages ggplot2 wickham et al 2020a ggnewscale campitelli 2021 ggsci xiao 2018 reshape2 wickham 2020 dplyr wickham et al 2020b and foreach microsoft and weston 2020 the pdfs presented in figs 4 6 were generated using the geom density function included in the ggplot2 r package wickham et al 2020a using default settings aside from applying a multiplicative kernel bandwidth adjustment of 1 5 higher values for this parameter led to smoother pdfs while lower values led to very choppy pdfs the adopted value was used in order to strike a balance between the smoothness of the pdf and the ability to show the unique modality of the stochastic approaches pdfs 
25658,in a companion paper sikorska senoner and quilty 2021 introduced the ensemble based conceptual data driven approach cdda for improving hydrological simulations this approach consists of an ensemble of hydrological model hm simulations generated via different parameter sets whose residuals are corrected by a data driven model one per hm parameter set resulting in an improved ensemble simulation through a case study involving three swiss catchments it was demonstrated that cdda generates significantly improved ensemble streamflow simulations when compared to the ensemble hm in this follow up study a stochastic version of cdda scdda is developed that in addition to parameter uncertainty accounts for input data input variable selection and model output uncertainty using several deterministic and probabilistic performance metrics it is shown that scdda results in significantly more accurate and reliable ensemble based streamflow simulations than the cdda ensemble and stochastic hms and a quantile regression based approach improving the mean interval score by 26 79 keywords ensemble stochastic streamflow simulation data driven model hydrological model uncertainty 1 introduction although data driven models ddms have been used for hydrological simulation and forecasting since the 1970 s remesan and mathew 2015 troin et al 2021 sometimes placed in opposition to their process based counterparts e g kim et al 2021 nearing et al 2020 there is an increasing interest in coupling process based conceptual and physical hydrological models hms with ddms in order to improve hm simulations and forecasts see for example tongal and booij 2018 senent aparicio et al 2019 wu et al 2019 konapala et al 2020 yang et al 2020a althoff et al 2021 cui et al 2021 sikorska senoner and quilty 2021 wang et al 2021a the main idea behind this coupled approach is that ddms have the ability to capture additional information about the hydrological process that is missed by the hm while the hm is able to provide the ddm with process based information that the ddm lacks due to its statistical formulation konapala et al 2020 approaches that directly couple a hm with a ddm go by different names including hybrid models konapala et al 2020 hydrologically informed machine learning herath et al 2021 physics informed hybrid models lu et al 2021 these approaches are placed in contrast with other methods that seek to impose physical constraints on the calibration training of physics guided ddms i e without directly incorporating a hm such as in xie et al 2021 that sometimes are referred to as theory guided or process guided models adombi et al 2021 read et al 2019 before proceeding it is important to clarify that the terms simulation and forecasting as used here adopt the convention of beven and young 2013 however as noted by the authors since the term prediction is used loosely in the hydrology and water resources literature it is taken here to be synonymous with simulation ddms have also been used to support different aspects of hm simulations for example ddms have been traditionally used as surrogate models for more computationally intensive hms razavi et al 2012 where new deep learning based approaches have demonstrated promise for this task gu et al 2020 boucher et al 2020 used different types of artificial neural networks ann for data assimilation within a hm showing their potential to produce reliable ensemble streamflow simulations while teweldebrhan et al 2020 used different machine learning models to generate the error response surface of a distributed hm for improved streamflow simulation more recently tsai et al 2021 showed how differentiable deep learning based ddms can be used to learn the responses and parameters of hms achieving substantially improved performance or at least similar performance at a lower computational cost compared to evolutionary and regionalization techniques while there are numerous approaches for coupling hms with ddms this work focuses on using an ensemble of hm simulations as input to a ddm in order to improve streamflow simulation although the literature includes an increasing variety of combined hm and ddm approaches as shown above and also in xu and liang 2021 an important and under studied niche topic within this area is how to effectively include uncertainty assessment in these combined hm and ddm approaches some examples from the literature follow mekonnen et al 2015 used a hm to simulate streamflow from contributing areas in the canadian prairies and adopted ann to correct the hm residuals which simulated streamflow from non contributing areas the authors considered the impact of uncertainty in the ann parameters on the coupled hm and ddm simulations humphrey et al 2016 followed a slightly different approach where the soil moisture simulated by a hm was used as input alongside other meteorological variables to bayesian ann for streamflow forecasting the hm and bayesian ann both estimated parameter and model output uncertainty it was found that the bayesian ann led to substantially better forecasts in terms of accuracy resolution and reliability when compared to the hm a similar approach by ren et al 2018 was undertaken where the hm was used to simulate variables representing the contribution of rain snow and glaciers to streamflow these variables were used as input alongside other meteorological variables to bayesian ann and least squares support vector regression for streamflow simulation the bayesian ann produced the most accurate and reliable simulations considering the two models both tyralis et al 2019c and papacharalampous et al 2019 post processed hm simulations the former by stacking two different quantile regression variants through gradient boosting and the latter by applying equal weight averaging to six different quantile regression variants resulting in sharp and reliable streamflow simulations in these studies the hm simulated streamflow and its time lagged copies were used as input to the quantile regression models li et al 2021a b used probabilistic long short term memory lstm networks to correct the residuals of a hm outperforming benchmark methods for streamflow simulation sikorska senoner and quilty 2021 hereafter referred to as ssq2021 proposed an ensemble based conceptual data driven approach cdda where the residuals from an ensemble of hm simulations were corrected by ddms one ddm for each hm ensemble member resulting in improved ensemble streamflow simulations in ssq2021 it was shown that using non linear ddms within cdda to simulate hm residuals and afterwards adding the ddm output to the hm simulations can lead to more accurate streamflow simulations than using the standalone ensemble hm however the cdda only considered parameter uncertainty in the hm when generating the ensemble streamflow simulations which was represented by confidence intervals hence this limitation of the cdda motivated the current study to consider several sources of uncertainty input data input variable selection ivs parameters and model output to generate ensemble simulations through a stochastic approach from which prediction intervals can be estimated thus providing a more thorough assessment of uncertainty when compared to confidence intervals the approach proposed in this study is referred to as the stochastic cdda or scdda the scdda relies on offline estimates of probability distributions for the input data ivs parameters and model output and uses this information during online stochastic simulation to generate a probability distribution of the variable to be simulated see section 3 4 in the scdda hm simulations are used as input to a ddm along with explanatory variables e g meteorological variables and optionally previous measurements of the target variable e g streamflow the scdda is inspired by the blueprint for stochastic process based models proposed in montanari and koutsoyiannis 2012 which was explored further in sikorska et al 2015 and subsequently modified for use with ddms incorporating multiscale change in quilty et al 2019 and quilty and adamowski 2020 some other recent studies inspired by the blueprint for stochastic hydrological simulation are contrasted with the scdda below although there are a limited number of studies capturing both hm and ddm uncertainty for coupled approaches to hydrological simulation and forecasting there are some important studies from the literature related to the blueprint and scdda that should be mentioned papacharalampous et al 2020a b proposed a probabilistic framework that generates a number of hm simulations using a single hm structure but with different parameter sets referred to as sister predictions and builds an error model for each of the sister predictions using quantile regression representing model error uncertainty the probabilistic prediction is obtained by subtracting the quantile regression predictions made at several quantiles from the sister predictions and then averaging the resulting predictions across each quantile in their approach the authors use each sister prediction as the sole explanatory variable and each sister prediction s error as the response variable the scdda differs from this approach in several key ways in scdda an ensemble of hm simulations represented by a probability density function pdf at each time step are used to represent input data uncertainty and are considered as predictors alongside meteorological variables and optionally previous measurements of the target variable within ddms that are used to directly simulate the target variable e g streamflow ivs uncertainty which is often neglected but recently shown to be an important source of uncertainty when using ddms quilty and adamowski 2020 is considered since there are multiple explanatory variables both ivs and parameter uncertainty are jointly estimated using bayesian optimization snoek et al 2012 and decision tree models the estimation of model error uncertainty relies on k nearest neighbours resampling sikorska et al 2015 a very simple non parametric method and instead of using a set of ddms to simulate a limited range of quantiles to define the probabilistic simulation at each time step the pdf of the target variable is estimated through stochastic simulation incorporating the above mentioned sources of uncertainty allowing for any quantile of the simulations that estimate the target probability distribution to be returned at any time step in ssq2021 eight different ddms were explored multiple linear regression mlr k nearest neighbours regression knn a second order volterra series model ann and two variants of extreme gradient boosting xgb and random forests rf the latter two methods xgb and rf were recommended for further use due to their high performance thus following this recommendation both xgb and rf were used in the scdda to account for additional sources of uncertainty not explored in cdda however similar to the cdda the scdda also generates ensemble based simulations therefore the main goal of this work is to explore the added value of the scdda when benchmarked with the cdda the stochastic version of the hm the ensemble version of the hm and the quantile regression based approach proposed by papacharalampous et al 2020a b the main novelty of this work lies in the development of a stochastic approach that couples multiple realizations of a hm i e a single hm structure but with simulations derived from different parameter sets with ddms the approach is general and can be used with any hm and any ddm as well as accommodate multiple hms and ddms although this is outside the scope of this study the scdda is applied to a daily streamflow simulation case study that includes three swiss catchments the same catchments and data adopted in ssq2021 the remainder of this paper is organized as follows section 2 introduces the methods adopted in the development of scdda section 3 provides details on the experimental settings used for testing the scdda section 4 describes the main results of the experiments section 5 discusses the major findings and section 6 provides concluding remarks 2 methods 2 1 overview of the ensemble based conceptual data driven approach cdda the ensemble based conceptual data driven approach recently introduced in ssq2021 consists of a hm to simulate a hydrological process e g streamflow that is coupled with a data driven model which is used to simulate the hm residuals in ssq2021 the target variable of the hm was streamflow while the target variable of the ddm was the hm residuals i e the difference between the hm simulated streamflow and observed streamflow the rest of this paper will refer to the specific case of streamflow simulation when describing the cdda and scdda although both approaches are generalizable to any hydrological variable for which an hm can produce simulations a single hm i e model structure is used within the cdda to produce an ensemble of streamflow simulations by running the model with different parameter sets attached to each hm parameter set is a ddm enabling the cdda to issue an ensemble of streamflow simulations while a single hm is used within cdda and scdda it is very straightforward to accommodate multiple model structures e g by applying an equal weight to each model structure and its ensemble simulations in detail each hm ensemble member simulated streamflow is associated with an individual realization of the ddm simulated error to provide an improved ensemble streamflow simulation as shown in equation 1 1 y i p t t d t t t d q t 1 t d θ c d d a i y i p t t t θ h m i r i p t t d t t t d q t 1 t d θ d d m i where y i y i and r i are the cdda hm and ddm simulations for ensemble member i respectively p t t d t t t d and q t 1 t d are observed precipitation and air temperature at lag times t t d and observed streamflow at times t 1 t d where d is the maximum time lag respectively θ c d d a i θ h m i and θ d d m i are the parameter sets of the cdda hm and ddm for ensemble member i respectively with θ c d d a i θ h m i θ d d m i the main limitation of the cdda is that it only considers parameter uncertainty associated with the hm thus when quantifying the uncertainty in the ensemble simulations produced by cdda at a particular confidence level only confidence intervals can be estimated since the model error uncertainty is not considered in addition to other important uncertainty sources such as input data and ivs 2 2 stochastic conceptual data driven approach scdda to overcome the above mentioned limitation of the cdda the stochastic conceptual data driven approach or scdda is introduced to account for input data ivs parameter and model output uncertainty allowing for prediction intervals to be estimated from an ensemble of streamflow simulations scdda which stems from the combined work of montanari and koutsoyiannis 2012 and quilty et al 2019 allows for coupling of both hms and ddms within a stochastic framework scdda can be implemented by adopting the same equation as the stochastic data driven forecasting framework sddff quilty et al 2019 2 f q q θ x ω ω f e q s θ x ω θ x ω f θ θ x ω f ω ω x f x x d θ d x where x is the input data for a given model s in this case a ddm θ represents the parameters supplied to the model ω ω is a binary vector identifying which input variables were selected during ivs f q represents the pdf of the target variable i e streamflow f e represents the conditional pdf of the model error f θ represents the conditional pdf of the parameters f ω represents the conditional pdf of ivs and f x represents the pdf of the input data to derive scdda in equation 2 x includes y i p t t t θ h m i t t d p t t d t t t d q t 1 t d where y i p t t t θ h m i t t d represents time lagged versions of a randomly selected hm simulation at times t t d since a randomly selected hm simulation and its time lagged versions are used as input to a ddm see section 3 3 1 for further details the parameters in equation 2 which are solely related to the ddm can be estimated independently of the hm parameters further if a ddm is adopted that inherently performs ivs as part of the parameter calibration stage such as in decision tree methods e g random forests and input data uncertainty is considered independent of the parameter uncertainty see montanari and koutsoyiannis 2012 for discussion on the reasonableness of this assumption then equation 2 can be simplified as montanari and koutsoyiannis 2012 3 f q q θ x f e q s θ x θ x f θ θ f x x d θ d x where it is assumed that estimating the model parameters will automatically determine the selected input variables hence by using the hm simulations as input to the framework uncertainties in both the hm and ddm can be accounted for in scdda in the experiments adopted in this study scdda only considers input data uncertainty with respect to the ensemble streamflow simulations from the hm in other words the pdf of the input data f x is represented by the ensemble of hm simulations since the remaining inputs are considered to carry no uncertainty and are thus fixed at their observed values however if one has access to reasonable estimates of uncertainty for these variables it can be easily incorporated within scdda see montanari and koutsoyiannis 2012 for a related example the pdf of the parameters f θ is estimated using bayesian optimization snoek et al 2012 as described in section 3 4 1 the conditional pdf of the model error f e is estimated using the validation set errors associated with the parameters defined by f θ please refer to section 3 4 1 finally the double integral in the scdda is estimated using monte carlo sampling and is described in further detail in section 3 4 2 to better understand the details necessary to implement the scdda readers are encouraged to review the mathematical background in montanari and koutsoyiannis 2012 as well as the sddff implementation details and workflow described in quilty et al 2019 and quilty and adamowski 2020 2 3 data driven models although any ddm can be used within scdda this study considers only two ddms random forests and extreme gradient boosting based on the recommendations of its companion paper sikorska senoner and quilty 2021 which adopted the same case study thus only rf and xgb will be described in this section 2 3 1 random forests random forests rf are a machine learning algorithm based on decision trees first introduced by breiman 2001 which have become popular in diverse disciplines due to the algorithm s capability to produce accurate and well generalized models even in cases with many input variables belgiu and drăgu 2016 biau 2012 díaz uriarte and alvarez de andrés 2006 iwendi et al 2020 svetnik et al 2003 zhou et al 2020 the rf regression algorithm involves bootstrapping both the input variables and training examples samples creating an ensemble of decision trees i e one for each bootstrap issuing a prediction for each decision tree and aggregating the result of each decision tree by applying an equal weight to each models predictions to produce the final prediction since rf utilize bagging i e bootstrapping the data and aggregating predictions associated with each bootstrap the rf can reduce predictive variance without increasing the bias of the model breiman 1996 2001 another significant benefit of rf is that it implicitly estimates input variable importance which can be used for ivs speiser et al 2019 in the hydrology and water resources domains rf are becoming more popular for example with recent exploration in regional flood quantile estimation at ungauged sites desai and ouarda 2021 estimating changes in streamflow response due to forest cover li et al 2020 and estimating root zone soil moisture carranza et al 2021 the interested reader may refer to tyralis et al 2019b for a review on the use of rf in the water resources domain as ivs is an important feature of rf this study utilizes guided regularized random forests grrf since it has been shown to select input variables that result in improved prediction models when compared to those selected by rf deng and runger 2013 interestingly grrf has yet to be used in the hydrology or water resources domains although it has been used with success for different applications including genetic population assignment sylvester et al 2018 classification of invasive plant species omeer and deshmukh 2021 and remote sensing izquierdo verdiguier and zurita milla 2020 the grrf input variable selection process selects a subset of relevant and non redundant inputs by assigning penalties through regularization to information retrieved from previous nodes in the decision tree rf and grrf were implemented using the rrf r package deng 2013 2 3 2 extreme gradient boosting the extreme gradient boosting method a variant of the gradient tree boosting algorithm provides computationally efficient accurate and well generalized models useful for a host of diverse real world problems chen and guestrin 2016 unlike rf which utilizes bagging for ensemble model predictions the xgb algorithm produces an ensemble of models where each tree is combined in sequence and scaled by a learning rate resembling the gradient boosting algorithm in addition to maintaining a generalized model xgb uses l2 norm regularization to reduce the likelihood of overfitting chen and guestrin 2016 finally similar to rf input variable importance is estimated within xgb which can be used for ivs alsahaf et al 2022 adoption of xgb in the hydrology water resources and complimentary fields include applications to daily streamflow forecasting tyralis et al 2019a wetland classification delancey et al 2019 predicting groundwater levels ibrahem ahmed osman et al 2021 correcting precipitation forecasts ko et al 2020 predicting daily global solar radiation fan et al 2019 and predicting decontamination effects on bioretention cells wang et al 2021b xgb was implemented using the xgboost r package chen et al 2021 2 4 input variable selection input variable selection is considered an essential step when designing a ddm in ivs the goal is to select only relevant input variables and to remove irrelevant and redundant variables which can deteriorate model performance galelli et al 2014 despite certain ddms requiring an external ivs method e g ann support vector machines the two ddms applied here i e rf and xgb inherently perform ivs when estimating model parameters using the same scenario as in ssq2021 air temperature precipitation and simulated streamflow for the current and previous nine days were considered along with the previous nine days of observed streamflow as potential additional inputs to the ddms case 1 a second case case 2 was also considered where observed streamflow was not included in the set of potential inputs e g for situations where a site is no longer gauged but streamflow simulations may still be desired thus in total 39 and 30 input variables were considered for each of the ddms in case 1 and case 2 respectively variable importance scores were used to identify the different input variables selected by rf and xgb reflecting the relative importance of each input with respect to the final model prediction where higher variable importance scores indicate inputs with higher importance rf and xgb operate in a similar way in terms of estimating variable importance where a particular metric is calculated for and averaged across all decision trees to estimate input variable importance during node splitting an impurity score is calculated by rf while a gain score is calculated by xgb see li et al 2019 for additional details the grrf method adopted here follows a three step ivs process in the first step rf is used to estimate the variable importance score v i k for each input variable k 1 k where k is the number of inputs the scores are then normalized in the second step a penalty weight λ k 0 1 is assigned to each normalized variable importance score n v i k which can be calculated by deng and runger 2013 4 λ k 1 γ λ 0 γ n v i k where γ 0 1 is an importance weight and λ 0 0 1 is a regularization coefficient note that the lower the penalty weight λ k the less weight a given input variable receives in other words input variables with a high v i are assigned a high λ k and thus have a higher impact on the resulting model predictions in the third step grrf is run using the vector of penalty weights λ 1 λ k to guide the ivs process the grrf model thereafter provides a set of new variable importance scores that can be used to identify selected input variables and their relative influence on the grrf model predictions additional information on grrf can be found in deng and runger 2013 in the next sub section the various uncertainty sources considered in the scdda and its benchmarks will be discussed 2 5 uncertainty sources associated with the different approaches the scdda which enables the estimation of the pdf of the target variable f q in this case streamflow considers and requires estimation of the following sources of uncertainty input data uncertainty f x represented by the ensemble of hm simulations generated by a single hm structure but using multiple parameter sets additional details can be found in section 3 3 1 and 3 4 1 parameter and ivs uncertainty f θ represented by the collection of ddm parameters estimated by bayesian optimization during calibration of the ddms i e whose simulation quality was compared against observed streamflow q using the calibration dataset since the adopted ddms rf and xgb inherently perform ivs when calibrating model parameters ivs uncertainty is implicitly considered in the parameter uncertainty given that the grrf ivs method requires the specification of two hyper parameters λ 0 and γ these hyper parameters were optimized along with rf hyper parameters during model calibration note xgb does not require the specification of any hyper parameters specific to ivs additional details can be found in section 3 3 2 and 3 3 3 model error uncertainty f e represented by the collection of validation set errors attached to each ddm i e the collection of ddms resulting from bayesian optimization of model parameters it is important to note that all other sources of uncertainty not explicitly accounted for in the input data and parameters as well as ivs such as initial conditions of the hm are captured within the model error uncertainty additional details can be found in section 3 4 1 the stochastic hm incorporates and requires estimation of parameter uncertainty associated with different parameter sets attached to a single hm structure and model error uncertainty validation set errors attached to each set of hm parameters i e resulting from parameter optimization the quantile regression based approach of papacharalampous et al 2020a b hereafter referred to as qr takes into account and requires estimation of parameter uncertainty associated with the hms and model error uncertainty the ensemble hm incorporates and requires estimation of parameter uncertainty while the cdda incorporates and requires estimation of the same source of uncertainty as the ensemble hm it does not consider uncertainty in the ddms used to correct the ensemble hm residuals thus the stochastic hm qr ensemble hm and cdda each provide an estimate of the pdf of the target variable f q each considering fewer sources of uncertainty than scdda since scdda the stochastic hm and qr each consider at least model error uncertainty they can be used to generate prediction intervals using for example the 0 025 and 0 975 quantiles of the simulations used to estimate f q representing prediction intervals at the 95 confidence level since the ensemble hm and cdda only consider parameter uncertainty the quantiles of the simulations used to estimate f q at a prescribed confidence level are similar to confidence intervals 3 experimental settings 3 1 study catchments data and dataset partitioning the proposed scdda was tested on the same three medium sized swiss mountainous catchments from the companion study sikorska senoner and quilty 2021 dünnern at olten 234 km2 kleine emme at littau 478 km2 and muota at ingenbohl 317 km2 these catchments are considered free from glaciers i e with an areal glacier percentage 5 and without any significant human impact during the observation period 1981 2015 the hydrological processes within these catchments are driven by rainfall dynamics dünnern or mixed rainfall and snowmelt processes kleine emme and muota see sikorska senoner and seibert 2020 for additional details the data for the three catchments include daily observations of mean air temperature c precipitation mm day evaporation rates mm day and streamflow mm day at the respective catchment outlet the data for each catchment was divided into four distinct partitions warm up 1981 1984 calibration 1985 2004 validation 2005 2009 and testing 2010 2014 the use of these distinct partitions for developing and applying the various models is explained in the sub sections that follow 3 2 hydrological model set up the hbv light seibert and vis 2012 conceptual hm was adopted in this study following the same set up as in the companion study sikorska senoner and quilty 2021 hbv light is suitable for the study catchments since it simulates rainfall and snowmelt processes the model output is a single realization of a continuous streamflow time series at the catchment outlet at a daily time step hbv light was calibrated for each catchment using the metaheuristic approach outlined in sikorska senoner et al 2020 in ssq2021 hbv light was calibrated separately for each catchment using daily data for 1985 2004 with a 4 year warm up period of 1981 1984 the calibration exercise resulted in 1000 optimized model parameter sets that were afterwards used to generate an ensemble of streamflow simulations at the outlet of each catchment during the period 1985 2014 the ensemble streamflow simulations from ssq2021 along with daily measurements of observed streamflow mean air temperature and precipitation are considered in this study when developing the scdda stochastic hm and qr models the ensemble hm and cdda simulations from ssq2021 are also used as additional benchmarks for scdda for a more detailed description of hbv light and its calibration in the study catchments the reader is referred to sikorska senoner and quilty 2021 3 3 ddm set up 3 3 1 overview for the ddms the calibration dataset 1985 2004 included observed streamflow at time t i e q t as the target variable and 1 m i 1 m y i p t t t θ h m i t t d p t t d t t t d q t 1 t d as the input variables where m is the number of ensemble members see also section 2 2 the mean of the ensemble hm simulations and its time lagged versions were used during calibration since it was more convenient and time efficient than calibrating multiple ddms for each of the hm ensemble members separately this approach is reasonable as the input data and parameter uncertainties were considered independent of one another see section 2 2 however if one wanted to take full advantage of the ensemble hm simulations instead of using only the mean of the ensemble hm simulations when calibrating the ddms then a single ddm could be trained on a dataset that includes the concatenation of each member of the ensemble hm simulations and their time lagged versions with the remaining inputs in a single matrix i e y i p t t t θ h m i t t d p t t d t t t d q t 1 t d i likewise the target variable would be concatenated m times in order to match the corresponding input variables note that this approach would result in a calibration set that is m times larger than the approach adopted here leading to longer model training times that may be prohibitive depending on available computational resources once prepared the calibration dataset was used for optimizing the ddms parameters via bayesian optimization resulting in multiple parameter sets defining the pdf of the model parameters f θ each set of parameters for each ddm rf and xgb was then used to generate a set of simulations for the validation dataset resulting in an estimate of the model error pdf f e the ddms were then run in simulation mode on the test set as described in section 3 4 in order to estimate f q next specific settings related to rf grrf and xgb are discussed 3 3 2 random forests and guided regularized random forests the rf models considered two hyper parameters the number of trees in the forest b and the number of variables selected randomly during node splitting m additionally grrf considered two hyper parameters the regularization coefficient λ 0 and importance weight γ by exploring different values of the four hyper parameters in this case through bayesian optimization the pdf of the rf parameters variables included at each split and split points represented by f θ can be estimated the following range of values were explored for each hyper parameter where integer values are appended with an l b 50l 300l m 1l 15l λ 0 0 01 1 γ 0 1 all four hyper parameters were optimized simultaneously at first an initial rf model was run for a pair of b and m values afterwards the normalized variable importance scores calculated from the resulting rf model were used alongside a pair of λ 0 and γ values to calculate λ k for each input variable according to equation 4 finally after applying the set of λ k values to the input variables the grrf algorithm was run the grrf outputs included the updated variable importance scores and any input variables that had non zero variable importance scores were then used to develop the final rf model based on the same b and m values additional information on the optimization procedure is provided in section 3 4 1 3 3 3 extreme gradient boosting the xgb models had a total of nine hyper parameters that required optimization which influenced the model parameters splitting variables and split points the range of values assigned to each hyper parameter is provided in ssq2021 while a description of each hyper parameter is included in the references mentioned therein and is not repeated here for brevity bayesian optimization was used to optimize xgb hyper parameters in the same way as with rf and grrf out of the nine hyper parameters only the nrounds hyper parameter representing the number of trees included in the model was optimized via five fold cross validation which also included an early stopping criterion set at five rounds the cross validation procedure was carried out within the bayesian optimization routine allowing for the eight other hyper parameters to remain fixed as the nrounds hyper parameter was optimized 3 4 stochastic conceptual data driven approach scdda the scdda is comprised of two separate components an offline mode where the various pdfs f x f θ and f e are estimated and an online mode where for a set of new inputs x the stochastic simulation procedure is run to obtain an estimate of f q more information on the offline and online modes of scdda is provided below scdda was developed using custom r scripts 3 4 1 estimation of probability density functions offline mode as noted in section 2 2 the pdf of the input data f x was represented by the ensemble of hm simulations 1000 members from ssq2021 as the remaining input variables p t t d t t t d and q t 1 t d remain fixed the pdf of the parameters f θ was obtained by running bayesian optimization for 100 iterations resulting in 100 ddm parameter sets as determined by the hyper parameters explored during optimization bayesian optimization was used for minimizing the root mean square error on the calibration dataset since the bayesian optimization algorithm adopted here is based on gaussian process regression an initial set of points were needed to estimate the surrogate model which was selected as 10 the parbaeysianoptimization wilson 2019 and doparallel calaway et al 2019 r packages were used for training the ddms in parallel additional information on the bayesian optimization method can be found in ssq2021 as well as trierweiler ribeiro et al 2020 and zuo et al 2020 the 100 parameters sets for each ddm rf and xgb were then used to generate a set of simulations for the validation dataset and afterwards subtracted from the observed streamflow also for the validation set resulting in an estimate of the model error pdf f e thus each parameter set was associated with a set of validation errors in earlier studies montanari and koutsoyiannis 2012 sikorska et al 2015 quilty et al 2019 quilty and adamowski 2020 the model error uncertainty was estimated using validation set errors associated with model realizations for a single parameter set the approach adopted here has higher potential to better represent the model error uncertainty as it incorporates errors from multiple ddms rather than a single ddm 3 4 2 stochastic simulation online mode stochastic simulation within the scdda can be accomplished through the following steps see also quilty and adamowski 2020 1 for a new set of air temperature and precipitation observations p t t t a hm parameter vector θ h m i is picked at random from θ h m 1 θ h m m and a hm simulation generated y i p t t t θ h m i 2 the hm simulation and its time lagged versions are appended to time lagged versions of the new set of air temperature and precipitation observations and time lagged versions of the observed streamflow resulting in x y i p t t t θ h m i t t d p t t d t t t d q t 1 t d which is equivalent to drawing a random sample from the probability density function f x x note if one does not have access to or wish to use observed streamflow then x y i p t t t θ h m i t t d p t t d t t t d 3 a ddm parameter vector is picked at random from the pdf f θ θ 4 using the sampled information θ x an output is generated via the ddm represented by s θ x 5 a random error is picked up from the conditional pdf f e q s θ x θ x and added to s θ x 6 steps 1 to 5 are repeated a sufficient number of times n leading to n different simulations of q 7 the n different simulations of q provide an estimate of f q q resampling from the conditional pdf for the model error i e step 5 is carried out through k nearest neighbours resampling using the validation set attached to the randomly sampled parameter vector θ for further details please refer to sikorska et al 2015 similar to earlier studies quilty and adamowski 2020 quilty et al 2019 10 nearest neighbours were considered when resampling from the conditional pdf of the model error note that the result of running step 5 of the stochastic simulation procedure n times provides an assessment of model output uncertainty thus even if steps 1 to 4 were ignored i e input data and parameter uncertainties were not considered one would still have access to the model output uncertainty similarly other steps of the stochastic simulation procedure may be ignored in order to focus on specific aspects of the scdda such as uncertainty solely due to the input data it is important to note that only data in the test set should be used in the stochastic simulation procedure since the calibration data was used to estimate f x and f θ while validation data was used to estimate f e in this way a more realistic assessment of out of sample performance can be carried out by comparing observed q to f q q and its related statistics such as the mean or median on the test data 3 5 benchmarks the scdda was benchmarked against four other approaches in order to assess any relative gains in simulation accuracy reliability and sharpness see section 3 6 as noted earlier since xgb and rf performed best with respect to six other variants tested in ssq2021 these two ddms were incorporated within scdda further since observed streamflow is not always guaranteed to be measured at regular intervals or since it may only be recorded for a previous historical period two variants of scdda were considered for each ddm with xgb wqobs and rf wqobs and without xgb and rf time lagged versions of observed streamflow as model inputs scdda considered input data input variable selection parameter and model output uncertainty two of the benchmarks the ensemble hm and the cdda are from ssq2021 representing approaches that only consider parametric uncertainty the ddm that performed best in each catchment xgb was used as the cdda benchmark the third benchmark was a stochastic hm that considered only steps 1 5 6 and 7 from the procedure outlined in section 3 4 2 at step 5 s θ x was swapped for y i p t t t θ h m i the stochastic hm considered both parametric and model output uncertainty the final benchmark was the quantile regression based approach from papacharalampous et al 2020a b qr described in section 1 papacharalampous et al 2020a b proposed six different schemes of their qr approach the qr approach adopted here is referred to as ensemble scheme 5 since the details of this approach are quite involved the reader is directed to papacharalampous et al 2020b for a thorough explanation of the approach the qr approach makes use of the quantreg r package koenker 2019 and adopts the frisch newton approach koenker and portnoy 1997 for estimating the slope and bias coefficients of the qr models the stochastic simulation procedure was run for 1000 iterations n 1000 for the stochastic methods scdda and its hm counterpart resulting in 1000 simulations representing f q q also referred to as ensemble members the ensemble methods also included 1000 ensemble members representing f q q the qr approach cannot estimate f q q directly and instead estimates selected quantiles of the streamflow as described in the sub section below 3 6 performance evaluation several deterministic and probabilistic performance metrics were adopted for comparing scdda against the various benchmarks since the adopted metrics are well known in the hydrology and water resources communities the interested reader can refer to the cited studies for detailed explanations and formulae for these metrics the deterministic metrics include the kling gupta efficiency index kge nash sutcliffe efficiency index nse root mean square error rmse and mean absolute error mae althoff and rodrigues 2021 which were calculated using the hydrogof r package zambrano bigiarini 2017 the probabilistic metrics include the mean interval score mis prediction interval coverage probability cp prediction interval average width aw papacharalampous et al 2020a mean continuous ranked probability score crps gneiting and raftery 2007 and the alpha index α r renard et al 2010 the crps was calculated using the scoringrules r package jordan et al 2019 while the remaining probabilistic metrics were estimated using custom r functions of the adopted probabilistic metrics both the cp and α r assess the reliability of a series of probabilistic simulations while the aw measures the simulations sharpness the mis and crps summarize both sharpness and reliability in a single score of note a key goal when issuing probabilistic simulations is to obtain simulations that are both reliable and sharp in general if a probabilistic simulation is not reliable then it is of little value in practical settings however if two competing methods provide equally reliable simulations the method that provides sharper simulations should be preferred the above mentioned metrics which summarize different aspects of deterministic and probabilistic performance are supported by a graphical assessment of simulation quality that includes time series plots plots showing the pdfs of simulations at high flow events coverage probability plots cpps also referred to as predictive probability probability plots probability plots or q q plots laio and tamea 2007 koutsoyiannis and montanari 2022 and plots evaluating performance as a function of ensemble member size it should be noted that the α r metric is directly linked to the cpp and represents the area between a simulation models coverage probabilities with respect to the theoretical coverage probabilities which should be uniform on 0 1 and the cpp bisector a perfectly reliable model will have α r 1 the deterministic metrics were evaluated using the median of f q q for scdda cdda ensemble and stochastic hms and qr to evaluate both the crps and α r the full distribution of f q q is needed since the qr approach does not generate the full distribution of f q q but instead a selected range of quantiles here 0 025 0 5 and 0 975 both crps and α r are not computed for this approach however to evaluate the mis cp and aw metrics upper and lower prediction intervals for a particular confidence level here 95 are required thus to compute 95 prediction intervals 0 025 and 0 975 quantiles were estimated by the qr approach and the same quantiles were calculated using the simulations that estimate f q q for scdda cdda and both ensemble and stochastic hms unless specified otherwise all deterministic and probabilistic metrics were calculated using 1000 ensemble members with qr being an exception for all metrics in the next section the scdda and its benchmarks are compared against one another using the metrics and graphical tools outlined above 4 results this section highlights the main results including a quantitative assessment of deterministic and probabilistic model performance a comparison between the approaches prediction intervals pdfs at select high flow events and coverage probability plots as well as the effect of ensemble size on model performance appendix a supplementary data includes additional results that support the main findings while appendix b provides references to the software used to generate the various plots in this section 4 1 quantitative assessment of deterministic and probabilistic model performance in table 1 the deterministic performance of the scdda variants is compared against each of its benchmarks cdda ensemble and stochastic hms and qr using the median of f q q the probabilistic performance of each approach is reported in table 2 although results for the crps and α r are not available for qr as noted in section 3 6 through tables 1 and 2 it is clear that at least one of the scdda variants provides superior or at least the same performance as the benchmarks considering all metrics for example the best scdda variant when compared against the stochastic hm in terms of mae provides relative improvements of 33 48 across the three catchments when considering the median of f q q comparing the best scdda variant against cdda reveals improvements in the mae between 13 and 35 across the three catchments it can also be seen that including observed streamflow at previous time lags as inputs to the ddm significantly improves deterministic performance with a decrease in mae of 18 45 across all three catchments improvements in performance being nearly identical for both ddms i e when comparing xgb wqobs vs xgb and rf wqobs vs rf it is also worth noting that the cdda included previous time lags of observed streamflow as input to the ddms thus a comparison with the xgb and rf scdda variants is not as straightforward it is important to note that both the stochastic hm and qr have very similar performance this is not surprising as both methods are quite similar the stochastic hm uses k nearest neighbours resampling to estimate the pdf of the model error while qr uses quantile regression to estimate quantiles of this pdf both methods relying on the simulations generated by the ensemble hm the stochastic hm and qr appear to provide equal estimates of the median of f q q while qr is able to estimate the median of f q q more efficiently than the stochastic hm the latter is able to provide an estimate of the entire distribution of f q q which is especially useful for simultaneously estimating simulation reliability across all quantiles afforded by the sample size table 2 reveals that the best scdda variants provide significant improvements in probabilistic performance for most metrics when compared against the benchmark methods for example across all three catchments the best scdda variant improves the mis by 74 79 compared to the ensemble hm 56 67 compared to the cdda 29 38 compared to the stochastic hm and 26 33 compared to qr since the mis is based on a particular confidence level here 95 using a probabilistic metric that considers the full probability distribution such as the crps may be more useful for comparing probabilistic performance a similar conclusion is reached when considering the crps the best scdda variant is able to provide improved performance compared to its benchmarks with decreases in crps across all three catchments of 43 56 compared to the ensemble hm 20 41 compared to the cdda and 32 46 compared to the stochastic hm using previous time lags of observed streamflow as input to the ddms crps was decreased by 17 41 for xgb and 17 44 for rf across all three catchments i e by comparing the crps of xgb wqobs vs xgb and rf wqobs vs rf by examining the cp and α r it is clear that the ensemble hm and cdda provide a relatively lower level of reliability compared to the stochastic approaches in the next sub section the prediction intervals related to the cp results in table 2 are examined via time series plots in section 4 3 pdfs related to probabilistic simulations at high flow events are studied and in section 4 4 cpps are used to assess reliability across the entire range of quantiles afforded by the sample size which is summarized by the α r reported in table 2 4 2 prediction intervals the 95 prediction intervals 95 pis for the scdda variants and the benchmarks are presented in figs 1 3 for all three catchments using a 30 day period containing the highest flow event in the test set similar plots considering the entire test set are included in the supplementary data appendix a the pis for the scdda variants the stochastic hm and qr include the observations throughout most of the period examined in figs 1 3 while the ensemble hm and cdda do not capture the high flow events at dünnern and muota catchments thus it is clear that the scdda provides a substantial improvement in reliability when compared to cdda for the study catchments in addition to these findings there are three interesting features to take away from figs 1 3 1 the scdda variants have nearly equal reliability as the stochastic hm but result in sharper pis 2 compared to the stochastic hm the pis for the scdda variants have a higher spread at high flows and a lower spread at low flows which is a desirable feature of probabilistic streamflow simulations 3 the xgb wqobs and xgb scdda variants tend to provide more conservative high flow simulations compared to their rf counterparts which allows xgb wqobs and xgb to capture two high flow events occurring on two consecutive days where the larger flood on the second day is missed by the rf variants and the stochastic hm in the next sub section pdfs of the simulations produced by the scdda variants cdda and the ensemble and stochastic hms are compared against one another for the highest flow event that occurred at each catchment in the test set as depicted in figs 1 3 4 3 probability density functions of simulations at high flow events while pis can be useful for evaluating the quality of probabilistic simulations at a prescribed confidence level the pdf of a probabilistic simulation can provide additional insights about the simulation s characteristics and quality modality spread etc that is not apparent from a single set of pis in figs 4 6 the pdfs of the scdda variants cdda and the ensemble and stochastic hms are provided for the highest flow event in the test period at each catchment additionally each sub plot in figs 4 6 includes the observed value for the high flow event along with the mean of the pdf additional details about the pdfs are provided in appendix b from figs 4 6 it can be seen that the ensemble hm and cdda produce very narrow pdfs that are approximately symmetric about their mean value with the exception of kleine emme the pdfs of the ensemble hm and cdda do not contain the observed value in contrast the stochastic approaches generate asymmetric pdfs with multiple modes at the depicted high flow events with the exception of rf wqobs and rf at muota catchment and in most cases the pdf of the simulation associated with each stochastic approach includes the observed value of further significance the mean of the stochastic approaches pdfs is very close to the observed value with the exception of muota catchment however at muota catchment the xgb based scdda variants pdfs have non negligible density for the range of flows in close proximity to the observed value which as can be seen in fig 3 represents a higher magnitude flood following a similar flood event the day before the pdfs of each model for the flooding event preceding the highest flow event in the test set at muota catchment are provided in the supplementary data appendix a in summary figs 4 6 aptly show the value in adopting the stochastic hm and scdda variants for simulating flood events in the study catchments in the next sub section coverage probability plots are used to assess the reliability of the various models 4 4 coverage probability plots the cpps for the scdda variants cdda and ensemble and stochastic hms associated with all three study catchments are included in fig 7 analysis of the cpps demonstrates that first the scdda variants provide much more reliable simulations in comparison to the cdda approach which significantly overestimates low quantiles and underestimate high quantiles referred to as narrow simulations laio and tamea 2007 second it appears that all scdda variants generate simulation quantiles that closely match the theoretical quantiles i e lie close to the bisector line as indicated by the high α r values with slight underestimation of low quantiles and overestimation of high quantiles indicative of large simulations laio and tamea 2007 similar patterns are observed for all three study catchments although the inflection point from under prediction to over prediction occurs at different quantiles in different catchments third at least one of the scdda variants had equal or better reliability than the stochastic hm the cpps for the ensemble hm and cdda clearly demonstrate that parametric uncertainty alone with respect to the specific formulation of these models is insufficient for generating reliable streamflow simulations in all three catchments although using different hms and or ddms in these approaches may lead to different results in contrast the cpps for the stochastic hm show that by accounting for model output uncertainty in addition to parametric uncertainty reliability can be significantly improved while the cpps along with tables 1 and 2 for the scdda variants show that the inclusion of input data input variable selection parameter and model output uncertainty within a coupled hm ddm approach can lead to further improvements in reliability along with accuracy and sharpness the next sub section explores the effect of ensemble size or the number of simulations considered in the stochastic models on probabilistic performance 4 5 effect of ensemble size on model performance the results presented in tables 1 and 2 as well as figs 1 7 are all based upon 1000 ensemble members with the exception of qr see section 3 6 consistent with the results reported in ssq2021 however it is often a valuable exercise to evaluate the impact of ensemble size on model performance as using fewer ensemble members may not only lead to similar or even improved performance but also lower computational expense as in ssq2021 the effect of the ensemble size on model performance was evaluated using the crps the results are reported in fig 8 similar to ssq2021 it appears that approximately 100 ensemble members provide nearly the same model performance as the 1000 member ensemble it can be seen that the crps significantly improves from 1 to 100 ensemble members while increasing the number of ensemble members beyond 100 leads only to marginal improvement interestingly the same effect was observed for the ensemble hm and the various cdda variants based on eight different ddms in ssq2021 this observation provides further justification for using 100 different parameter sets for defining parametric uncertainty i e f θ θ for the ensemble and stochastic models explored herein 5 discussion in section 4 it was demonstrated that at least one of the scdda variants xgb wqobs xgb rf wqobs rf provided superior deterministic and probabilistic performance when compared against its benchmarks cdda ensemble and stochastic hms and qr in all three study catchments this section begins by commenting on the positive characteristics of the scdda s probabilistic simulations at first the pdfs of the scdda simulations with respect to the approach s overall reliability will be discussed followed by remarks on the importance of including previous time lags of observed streamflow as model inputs the section ends by focusing on some future improvements that may be realized by modifying the scdda e g to account for multiscale change along with recommendations for testing its generality through large sample studies when exploring the pdfs of the scdda variants cdda and ensemble and stochastic hms for the highest flow events in the test set it was found that the stochastic methods generally produced multi modal asymmetrical distributions with mean values close to the observed value with the xgb wqobs and xgb scdda variants being the only models containing the highest flow events within the support of their pdfs across all three catchments interestingly although the xgb wqobs and xgb scdda variants can be considered highly reliable see table 2 and figs 1 7 their overall reliability as measured by the alpha index α r did not surpass their respective rf counterparts across all three catchments nor the stochastic hm for dünnern and kleine emme catchments although some scdda variants had lower α r values than the stochastic hm this should not necessarily be understood as a negative characteristic of the approach s simulation quality since in all cases the scdda variants demonstrated near perfect reliability or had large more conservative simulations reasonably conservative simulations can be particularly helpful for mitigating potential effects due to droughts or floods for example in the muota catchment the xgb wqobs and xgb scdda variants which generated slightly more conservative simulations than rf wqobs were able to assign a non negligible probability to higher flows see fig 6 capturing the largest flood in the test set that was missed by the cdda the ensemble hm and the other stochastic models the xgb wqobs and xgb scdda variants were also able to capture a nearly equal in magnitude flood on the preceding day producing very similar pdfs for both flood events compare fig 6 with figure sa4 in the supplementary data appendix a thus it is likely that the xgb wqobs and xgb scdda variants could be very useful tools for flood forecasting in the study catchments and are recommended for this purpose instead of the other stochastic alternatives cdda and the ensemble hm for muota catchment it was also shown that including previous time lags of observed streamflow as input to the ddms can significantly enhance simulation accuracy and reliability tables 1 and 2 in cases where streamflow observations may not be available at a regular daily interval the xgb and rf scdda variants may be used instead of the stochastic hm as they provide higher accuracy and in most cases reliability however if regular streamflow observations are available it is likely that the stochastic hm and therefore the scdda variants could be further improved by accounting for uncertainty in the state variables likewise uncertainty estimates in other model inputs such as rainfall could be considered as in wu et al 2021 furthermore given that the stochastic hm and the scdda variants can produce probabilistic simulations whose pdfs have different shapes see fig 6 it may also be fruitful to combine probabilistic simulations from the various models through a weighted average for example see equation 10 in montanari and koutsoyiannis 2012 it is important to note that the scdda is not restricted to the studied hm and ddms but applicable to any hm and ddm such as those generated via flexible hydrological modelling frameworks e g raven craig et al 2020 and new deep learning methods e g bayesian long short term memory networks lu et al 2021 respectively additionally the scdda could be coupled with wavelet decomposition to increase the approach s ability to handle multiscale change an inherent characteristic of many hydrological processes including streamflow ho et al 2017 tang and cao 2021 each of these potential modifications to the scdda represents further research avenues that may be worthwhile to explore while each of the abovementioned ideas have potential to improve the quality of scdda s simulations they are outside the scope of this study and are left for future work given the high performance of scdda in the study catchments future studies could also test scdda in contrasting climates semi arid tropical etc as well as exploring scdda in large scale experiments papacharalampous et al 2020a b for regional streamflow simulation kratzert et al 2019 lees et al 2021 and simulating water quantity or quality in ungauged basins bourgin et al 2015 qi et al 2020 yin et al 2021 while there will likely remain a high demand and interest in the use of process based hms despite the promising future of ddms in hydrology nearing et al 2020 shen et al 2021 it is anticipated that process based modellers will be interested in seeing how far their hms can be improved by augmenting their models with ddms given that hms and especially ensemble hms are often post processed using statistical techniques biondi et al 2021 siqueira et al 2021 together the stochastic hm and the scdda allow the hydrologist engineer etc to quantitatively and qualitatively assess the added value in adopting ddms in the hydrological simulation and or forecasting chain using the various plots and metrics adopted in this study overall it is clear that the scdda provides enhanced deterministic and probabilistic performance when compared to the stochastic hm and other benchmarks i e ensemble hm and qr and can generate reliable multi modal simulation distributions that may be especially useful for decision making subject to uncertainty 6 conclusions there is increasing interest in the hydrology and water resources communities to couple process based conceptual and physics based hydrological models hms with data driven models ddms since ddms can be used to estimate complex relationships between target and explanatory variables missed by the hm and at the same time take advantage of the physics based information contained in the hm output however it is rare for such coupled hm ddm approaches to account for uncertainty thus in a companion study sikorska senoner and quilty 2021 the ensemble conceptual data driven approach cdda was proposed to address this gap however the cdda only accounted for parameter uncertainty this follow up study introduced the stochastic cdda scdda to account for multiple uncertainty sources such as input data input variable selection parameter and model output with the goal of assessing the added value in adopting scdda instead of benchmark approaches including cdda ensemble and stochastic hms and a quantile regression based approach qr to achieve this goal scdda variants based on extreme gradient boosting and random forests with xgb wqobs and rf wqobs and without xgb and rf previously observed streamflow considered as model inputs along with precipitation and mean air temperature were compared against the benchmarks for daily streamflow simulation in three swiss catchments using popular deterministic and probabilistic performance metrics e g mean absolute error mae mean interval score mis and various graphical tools e g time series plots coverage probability plots the main conclusion is that at least one of the scdda variants provides significantly better deterministic and probabilistic performance when compared against benchmark methods across all catchments e g with improvements in the mis ranging between 26 and 79 by including previous observations of streamflow as input to scdda improvements in mae of 18 45 were achieved of interest to traditional process based modellers the stochastic hm can be directly compared against scdda to assess the added value of augmenting hms with ddms within the stochastic framework overall scdda represents a promising approach for coupling hms and ddms within a stochastic framework that can be applied for hydrological simulation and forecasting in the face of uncertainty data availability the observed streamflow data can be ordered from the foen https www bafu admin ch last access december 11 2020 while the observed meteorological data be purchased from meteoswiss http www meteoswiss ch last access december 11 2020 the most recent version of the hbv light model can be downloaded from https www geo uzh ch en units h2k services hbv model html funding this research received financial support through the following sources natural sciences and engineering research council discovery grant launch supplement jq university of waterloo department of civil and environmental engineering graduate student support allowance jq queen elizabeth ii graduate scholarship in science and technology dh and university of waterloo president s graduate scholarship dh the agencies institutions providing the above mentioned financial support did not influence the research contained herein or the decision to publish findings related to this research declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the authors gratefully acknowledge meteoswiss for providing the meteorological data used in this study and the swiss federal office for the environment foen for providing the streamflow data used in this research in accordance with project no 15 0054 pj o503 1381 the hbv models adopted in this paper were calibrated using the sciencecloud provided by s3it at the university of zurich appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105326 appendix b software packages the figures included in section 4 were generated using one or more of the following r packages ggplot2 wickham et al 2020a ggnewscale campitelli 2021 ggsci xiao 2018 reshape2 wickham 2020 dplyr wickham et al 2020b and foreach microsoft and weston 2020 the pdfs presented in figs 4 6 were generated using the geom density function included in the ggplot2 r package wickham et al 2020a using default settings aside from applying a multiplicative kernel bandwidth adjustment of 1 5 higher values for this parameter led to smoother pdfs while lower values led to very choppy pdfs the adopted value was used in order to strike a balance between the smoothness of the pdf and the ability to show the unique modality of the stochastic approaches pdfs 
25659,as low cost air quality sensors become more widely utilized more tools and methods are needed to help users access process sensor data identify poorly performing sensors and analyze visualize sensor data free and open source software foss packages developed for use on foss data science platforms are well suited to support this need by offering replicable and shareable tools that can be adapted to meet a user or project s specific needs this paper describes enhancements to the foss airsensor r package version 1 0 and the dataviewer web application version 1 0 1 that have been developed to support data access processing analysis and visualization for the purpleair pa ii sensor this paper also demonstrates how these enhancements may be used to track and assess the health of air sensors in real time or for large historical datasets the dataset used for this analysis was collected during a multi year project with sensors deployed from october 2017 to october 2020 involving the distribution of approximately 400 pa ii sensors across 14 communities in southern central and northern california applying the tools in the airsensor package revealed a dramatic variability in sensor performance mainly driven by seasonal trends or particulate matter source type these results also indicate that this sensor can provide useful data for at least three years with little evidence of substantial or consistent drift further high agreement was observed between co located sensors deployed at different times indicating that it may be reasonable to compare data from old and new pa ii sensors in addition to assessing the long term performance and reliability of the pa ii sensor this analysis serves as a model for how data from large sensor networks may be effectively processed evaluated interpreted and communicated keywords open source r package air quality sensors particulate matter sensors qa qc sensor networks citizen science software availability the airsensor r package version 1 0 was developed by mazama science and south coast aqmd airsensor is free and open source software available through comprehensive r archive network https cran r project org package airsensor and the github repository https github com mazamascience airsensor tree master mazama science maintains the package as part of ongoing collaborations with federal state and local air quality agencies airsensor version 1 0 was first released in 2020 under general public license v3 0 gpl 3 0 and runs on windows unix and macintosh operating systems airsensor was written in r and program files are less than 5 mbytes airsensor is designed to be used with r 3 3 and rstudio the airsensor dataviewer web based application was developed by mazama science and south coast aqmd the airsensor dataviewer is free and open source software available through the github repository https github com mazamascience airsensordataviewer the dataviewer was first released in 2019 under general public license v3 0 gpl 3 0 and runs on windows unix and macintosh operating systems dataviewer was written in r and program files are less than 7 mbytes the dataviewer requires git apache docker r and r shiny server the complete code used to conduct this analysis including accessing the data processing the data and generating the plots and statistics is available https github com acollier oxandale long term performance of purpleair pa ii sensors under an epa star grant this code will allow a user to fully replicate the analysis shared here and may serve as a model for those using data from air quality sensors in addition there are two data repositories created using the airsensor package that offer access to data collected by sensors deployed under the star grant http data mazamascience com purpleair v1 and https airsensor aqmd gov purpleair v1 1 introduction air quality sensors are often cited for their potential to provide high spatial and temporal resolution data and their accessibility in terms of cost and ease of use the increased use of this technology inevitably results in the collection of large data sets these datasets in turn require methods and tools that enable more effective processing and evaluation to meet the increased need for proper interpretation and dissemination by researchers members of the public and regulatory agencies feenstra et al 2020 currently solutions for accessing and viewing air sensor data tend to be sensor and pollutant specific and may be developed by the manufacturer or project leads alternatively they may follow a citizen science model in which data is aggregated publicly some of these approaches to data access and display are open access while others are proprietary and offered as a software as a service saas or platform as a service paas that require private user accounts and often include subscriptions costs however these solutions are often limited for example accessing and working with the unprocessed or factory calibrated sensor data requires scientific and technical expertise in some cases data displays may not include access to historical data and sensor manufacturers often provide no information on what if any data quality assessment and improvement measures are being applied feenstra et al 2020 1 1 developing data access analysis and visualization solutions for air sensors the limitations of existing data solutions as related to the use of sensors by the public became apparent during the us epa science to achieve results star grant project entitled engage educate and empower california communities on the use and applications of low cost air monitoring sensors this us epa star grant was awarded to the south coast air quality management district south coast aqmd in 2016 the project was driven by the following objective provide communities across california with the knowledge necessary to appropriately select use and maintain low cost sensors and to correctly interpret the collected data activities included the distribution of approximately 400 particulate matter sensors within 14 california communities and extensive engagement with participants which offered many opportunities to collect feedback and insights from those hosting and using the sensors and the data for example survey data indicated that users were most often checking their data sometimes as opposed to often or everyday feenstra et al 2020 discussions during workshops suggested that this was at least in part driven by the difficulty of engaging with the data using the platform available in addition participants noted that accessing and analyzing historical data was difficult for those without a scientific or technical background in response to this feedback the r based airsensor package version 0 5 and dataviewer tool version 0 9 7 were developed to enable more effective use of the data from purpleair pa ii sensors feenstra et al 2020 the airsensor package is a foss r based package available in r rstudio that allows users to easily access process and visualize data from pa ii sensors registered as public the dataviewer tool is a web based application that leverages the functionality of the airsensor package to allow users to easily engage with and visualize sensor data the dataviewer tool requires no programming experience and provides an intuitive way to explore current and historical data a previous publication describes the airsensor package version 0 5 and the dataviewer tool version 0 9 7 and their capabilities in detail feenstra et al 2020 1 2 the need for tools and methods to examine and improve sensor data quality the first phase of development of the airsensor package laid a foundation by defining the format of data objects methods of sensor data access procedures for data processing functions for accessing and formatting supplementary data and various plotting visualization functions in terms of access the airsensor package version 0 5 and the coding associated with the dataviewer tool version 0 9 7 were available through github for the next phase of development the primary goals were to expand the functionalities related to assessing and or improving data quality and increase the robustness of the package and the accessibility of the work to meet data processing needs for large sensor deployments formal methods and procedures were needed to monitor air sensor data quality in real time or assess the quality of historical data while several platforms are working to aggregate data from air quality sensors they do not currently apply any quality assurance quality control qa qc procedures the air quality data commons aqdc whose development is being led by the environmental defense fund has focused on developing standard data formats but data is uploaded as submitted by users https aqdatacommons org similarly openaq an open source open access platform that aggregates data from regulatory air monitors worldwide has begun adding data from air quality sensors https openaq org openaq is uploading data from the aqdc and two sensor manufacturers who provide open access to aggregated data from their sensors to pilot this new feature openaq 2021a b neither platform applies any qa qc or filtering and users are advised to consult manufacturers procedures and the metadata associated with specific datasets for more information environmental defense fund n d openaq 2021a b commercial platforms managed by manufacturers may apply qa qc procedures before displaying data but the details of this data processing are often not transparent regulatory agencies are beginning to explore the possibility of integrating data from air quality sensors into platforms that communicate air quality information from regulatory stations for example the us epa has developed qa qc procedures and a nationwide correction to enable data from pa ii sensors to be incorporated into the airnow fire and smoke map https fire airnow gov at the south coast aqmd staff has also developed qa qc procedures and a correction algorithm to enable the data from pa ii sensors to be incorporated into maps of data from regulatory monitoring stations http www aqmd gov home air quality current air quality data see current hourly air quality index map the combined data is able to provide more locally relevant information with high spatial resolution schulte et al 2020 for both of these examples the details of the data processing approach are described in publications but they are not necessarily readily accessible or easy to apply to the work of other sensor data users barkjohn et al 2021 schulte et al 2020 developing foss software airsensor r package on foss platforms r rstudio to gather data in an efficient format and easily apply and or alter these types of methods and procedures will increase their accessibility enabling sensor researchers users to integrate them into workflows compare different approaches and eventually optimize them 1 3 current understanding of long term sensor performance and reliability data quality is a critical issue for this technology considering continual concerns regarding sensor performance sensor reliability over time sensor lifetime and the potential for drift and sensor degradation while there have been many studies evaluating the performance of air sensors across a variety of environments as well as in controlled laboratory settings feenstra et al 2019 collier oxandale et al 2019 castell et al 2017 questions persist regarding sensor performance over long term e g one or more years field deployments feinberg et al 2018 clements et al 2017 a few studies have examined pm2 5 or particulate matter of 2 5 μm in aerodynamic diameter or smaller sensor performance during deployments occurring over approximately a year in these studies researchers observed moderate to good correlation with co located regulatory grade instruments bulot et al 2019 liu et al 2020 these researchers also suggest that the variability in performance correlation with co located regulatory grade instruments over these long term deployments may be driven by seasonal trends and the type of particulate matter pm to which the sensor is exposed sayahi et al 2018 bulot et al 2019 liu et al 2020 laboratory studies have observed pm sensors exhibit differing levels of accuracy depending on the type of pm used in the test supporting this assertion tryner et al 2020 zamora et al 2019 drift or an increase in the error over time is another important consideration for long term deployments or when sensors within a network are not deployed simultaneously studies evaluating pm sensors deployed simultaneously did not observe drift to be a significant concern during one year of network deployment mailings et al 2020 liu et al 2020 zamora et al 2019 however research has suggested that there may be issues with sensor accuracy associated with high cumulative exposures one laboratory chamber study demonstrated this potential reliability issue by exposing sensors to very high pm concentrations over short periods to simulate long term ambient exposure tryner et al 2020 the above observations are valuable and begin to answer questions related to long term sensor performance however these studies are limited in terms of the number of sensors tested and the duration of the deployments the manufacturer plantower technology 2016 of the sensor examined here suggests a lifetime of three years but none of the studies described tests sensor performance over that length of time these studies also do not consider other issues likely to occur in deployments involving participation by the public such as the comparability of sensors within a community scale network that were deployed weeks or even months apart given the accessibility of some commercial pm sensors there are networks where data from hundreds of sensors is being viewed on a single map frame data display however these sensors may vary significantly in terms of sensor age and cumulative environmental exposure developing systematic and replicable approaches to qa qc that can be applied either in real time or to historical data from large sensor networks could enable the collection of more reliable and useful data and lead to a better understanding of sensor performance 1 4 the airsensor package version 1 0 and dataviewer tool version 1 0 1 here we present enhancements to the airsensor package version 1 0 and dataviewer version 1 0 1 tool and show how these tools may be used to process sensor data and assess sensor performance historically or in real time the enhancements include expanded qc algorithms new state of health metrics for sensors additions to the dataviewer and the increased robustness and accessibility of the package through publication on the comprehensive r archive network cran repository the utility of these enhancements is demonstrated through an assessment of data collected during the star grant project period october 2017 to october 2020 as such this dataset offers the opportunity to examine sensor performance across multiple years in this project participants deployed most sensors themselves which resulted into sensors being deployed at different times during the project period thus the dataset includes many of the real world issues associated with sensor networks deployed by the public leveraging the capabilities in the airsensor package we access process and examine the data from extensive sensor networks to make observations regarding the health of these sensors over time and learn more about long term performance and reliability this analysis also offers a replicable approach to this task that could be modelled to support the use and maintenance of other large sensor networks 2 methods 2 1 enhancements and modifications to the airsensor package and the dataviewer tool as previously mentioned the first phase of development for both the airsensor package version 0 5 and the dataviewer tool version 0 9 8 is described in detail with example code in a previous publication feenstra et al 2020 in terms of specific updates and enhancements to the package there have been changes to the purple air synoptic pas data objects as well as the data archive the addition of state of health metrics and revisions additions to the qa qc algorithms these additions and enhancements provide additional functionality to make the package more robust and enable publication on cran cran is the main repository for r code packages publication on cran requires packages to undergo thorough testing and review and makes them easy to access and install in r studio the pas data objects are either created from the json data available at www purpleair com json or accessed from the local star grant data archive https airsensor aqmd gov purpleair v1 these data objects provide a summary of purpleair sensor data from a point in time the default version of this data object provides information on all sensors in the united states classified as public by users during registration and currently active users may also access historical summaries that include both active and inactive sensors the pas data objects have been modified to include an additional column devicedeploymentid a new identifier that includes both the sensor s id and the gps location this identifier was added to avoid confusion that might occur if there are two sets of timeseries data associated with an identical label or name which could arise if one sensor is moved to a new location with no changes to the sensor label or name though there are unique identifiers associated with each sensor and assigned by purpleair the labels are used to identify sensors by the airsensor package functions and these labels are assigned by the user in addition the star grant data archive has been updated to accommodate changes to the airsensor data object formats the example code below loads the package sets the archive base to the new star grant data archive and loads a pas object where the new identifier column can be seen in addition to the increased robustness of the data archive and sensor identifiers the ability to access the additional particulate matter data pm1 0 and pm10 that the sensor provides has been added library airsensor setarchivebaseurl http data mazamascience com purpleair v1 temporary archive setarchivebaseurl https airsensor aqmd gov purpleair v1 long term archive pas example pas load new functions have been added that allow the user to calculate state of health soh metrics for sensors and visualize sensor performance the soh metrics are calculated on a 24 hr basis using the purple air timeseries pat data objects as inputs for individual sensors the output provides the user with a list of metrics that can be used to understand sensor performance over a user defined timeframe table 1 lists these metrics which include factors such as completeness percent of data that is considered valid based on manufacturer s listed range of operation the level of agreement between the duplicate channel a and channel b sensors in each unit and other daily linear model fit checks between various parameters measured by the pa ii the airsensor package provides functions to generate all soh metrics listed in table 1 at once for a given sensor the user can also generate those metrics individually and a function is available to calculate the daily index values the output in all cases is essentially a time series that includes the date time and the soh metric s for each day there are also two plotting functions available to visualize sensor soh metrics the example code below illustrates the use of the package to create a pat that contains all available 2018 data for a single sensor the following lines of code produce the resulting figs 1 and 2 fig 1 provides an overview of the data collected by the sensor and indicates the data s reliability based on the calculated index value fig 2 provides multiple plots of individual soh metrics which can serve as useful diagnostic information when examining sensor soh over time for example the period of poor performance in september of 2018 fig 1 coincides with a period of missing data fig 2 see the percent reporting metric it is also evident that while the data generally shows high completeness and validity fig 2 occasional periods of lower agreement between the channel a and b data result in an index value of fair figs 1 and 2 pat example pat load pas pas example label scsb 20 startdate 20180101 enddate 20181231 timezone america los angeles pat dailysohindexplot pat example pat dailysohplot pat example in addition to the soh metrics functions to support data processing and qa qc checks have been revised and added in the airsensor package version 1 0 2 four algorithms are now available and they are described in table 2 for data that passes a qa qc check listed in table 2 the pm2 5 data from channels a and b are averaged and aggregated to an hourly time resolution as previously described these algorithms may be applied to a pat data object and the resulting output is a sensor data object the sensor data objects contain the final processed data which may be used for analysis and visualization feenstra et al 2020 new qa qc algorithms titled purpleairqc hourly ab 02 and purpleairqc hourly ab 03 have been added to provide more options and examples of algorithms furthermore the robustness of the qa qc functions has been improved by reducing the number of arguments or inputs available to users this change will result in greater consistency among those using the algorithms if users would like to change any parameters of a qa qc algorithm they are encouraged to build a new algorithm a helpful vignette that illustrates how to create new qa qc algorithms is available on github https mazamascience github io airsensor articles articles custom qc algorithms html highlighting the value of foss solutions users can easily make local copies of existing qc algorithms that can be adapted or users may create entirely new qc algorithms modelled on those offered in the package like the airsensor package version 0 5 the dataviewer tool version 0 9 8 and its features have been described in detail in a previous publication enhancements made to the dataviewer tool version 1 0 1 primarily include improved appearance usability data handling and performance the one prominent feature added in this phase of development is the interactive time slider that provides users with more control over the data displayed on the map in the overview tab fig 3 this new feature facilitates interaction with the air quality map data and thus is expected to increase user engagement 2 2 using the airsensor package version 1 0 to quantify sensor performance and improve data quality the new soh features may be used to track sensor health in real time or to examine and process historical data by applying these functions in real time sensor network managers may be able to identify poorly performing or malfunctioning sensors and more efficiently schedule maintenance or replacement resulting in higher quality sensor network data by applying the functions to historical data sensor network managers have a systematic way to evaluate long term data from extensive sensor networks the airsensor package version 1 0 is used to access and load data into the rstudio environment calculate the soh metrics and apply qa qc functions to investigate the long term performance and reliability of sensors the qa qc algorithm selected and used throughout this analysis is purpleairqc hourly ab 03 this work focuses on pm2 5 data from the cf atm channel the purpleair sensor includes a cf atm and cf 1 channel with different manufacturer defined data processing applied to each and no additional corrections were applied to the sensor data purpleair 2020 this approach ensured that the data used in this analysis was the same data most likely to be viewed by the public the soh metrics and proportion of data passing the selected qa qc algorithm were examined over time cumulative runtime and cumulative exposure this analysis included all the data collected within the project timeframe october 2017 to november 2020 for each sensor the cumulative runtime for each sensor excluded periods where the sensor appeared to be powered off or was not sending data to the online database the estimated cumulative mass exposure to pm for each sensor was calculated using equation 1 the calculation for cumulative mass exposure is based on an estimated flow of 0 5 cfm cubic feet per minute for the sensor additional details are available in the supplemental materials eq 1 c u m u l a t i v e m a s s e x p o s u r e μ g n 1 n p m 2 5 d a i l y a v e r a g e 20 3881 μ g in addition our analysis includes an assessment of how long sensors were in operation and how many experienced failures or intermittent qa qc issues one community referred to here as community z table 3 included 16 sensors all co located at a regulatory air monitoring station ams the data from this community was used to examine the performance of sensors with respect to data from regulatory grade instruments several common sensor evaluation metrics were used to compare sensor and regulatory data including rmse root mean squared error mae mean absolute error mbe mean bias error and the r2 or coefficient of determination used to indicate goodness of fit these metrics were calculated on a daily basis using a 30 day moving window this approach results in data on at the same time resolution as the soh metrics 2 3 additional details regarding air quality sensors used and the deployed networks under the star grant project sensor networks were deployed within 14 california communities these communities varied in terms of their location and nearby land use they included coastal and inland desert communities as well as urban suburban and rural communities in addition to examining the aggregate data from many sensors this range of communities provides the opportunity to explore sensor performance across different environments here we present data from 13 of the 14 communities the sensors in the 14th community were deployed earlier than the other 13 meaning the deployment dates do not cover the same timeframe however in addition to the 13 communities data from co located sensors at a regulatory ams are included as community z community z had 16 sensors deployed in batches at the ams site in parallel to the sensor deployments occurring in the other 13 communities here the communities are identified by the letters a m with the ams co located community identified as community z table 3 provides brief descriptions of each community the purpleair pa ii sensor was selected for this project based on community air pollutant concerns for pm the open access nature of the data cost of the sensor and the sensor s performance during evaluations feenstra et al 2019 south coast aqmd 2020 the pa ii uses optical particle sensor ops technology to estimate pm1 0 pm2 5 and pm10 mass concentrations and the sensor collects near ambient weather data including temperature humidity and pressure hagen and kroll 2020 each pa ii includes two opc pm sensors the pms5003 by plantower referred to as channel a and channel b while the purpleair uses the plantower pms5003 sensor our results are discussed in relation to studies using this and other similar sensors by the same manufacturer e g plantower pms1003 data from these sensors is automatically streamed over a wi fi connection to a thingspeak database which is accessible via an api application programming interface the data may also be viewed or downloaded through an online platform www purpleair com map open access to the data allowed project leads to track sensor installations by the community members and monitor the data participation was gauged through the rates of installation which varied across the communities ranging from 30 to 115 in a community where additional sensors were requested collier oxandale 2021 the data set used in this analysis includes data from 257 sensors these sensors were correctly labelled making them easily identifiable and making the data easily accessible using the functions in the airsensor package data were available from another 19 sensors but this subset was sited indoors and thus they are omitted 3 results discussion the results presented here are from an analysis of all data available from these community networks for the three year period of october 1 2017 to november 1 2020 these results provide insights into sensor performance and reliability in the context of community led deployments where many of the typical challenges associated with environmental monitoring projects involving participation by the public occurred collier oxandale 2021 given this context the findings presented may be relevant to other community based projects using air quality sensors particularly those using the same type of sensor 3 1 overview of the star grant sensor networks table 3 provides an overview of the participating communities an indication of the size of the networks and an indication of how much data was available for this analysis in all communities at least one sensor collected well over a year of data and the median for aggregate of all communities is 544 days or approximately 1 5 years expanding on the information provided in the previous table fig 4 groups sensors based on whether they exhibited very few qa qc issues during their operation group 1 exhibited periodic qa qc issues but seemed to recover group 2 or were not passing qa qc when they stopped recording data possibly due to failure or being intentionally powered off group 3 in all communities a portion of the sensors provided data that passed qa qc checks throughout their deployment though most sensors in all communities except for community m fell into group 1 or group 2 there were sensors in all 14 communities that exhibited either periodic or persistent qc issues the presence of these issues further highlights the importance of establishing procedures to identify poorly performing sensors and or data of questionable quality the sensors were distributed between october 2017 and august 2018 and community members completed the installations on a rolling basis in addition in some communities more sensors were added or replaced over time sensors were replaced when they experienced some type of failure e g not reporting data despite troubleshooting approximately 10 sensors were replaced over the course of the project across all communities and no sensors were replaced in community z fig 5 a illustrates the rolling nature of the deployment by showing the total number of operational sensors over time along with the number of sensors providing valid data and data passing qa qc checks after the first three months there were well over 100 sensors in operation at any given time fig 5b and c provide further context by showing the percent of sensors providing either valid data or data passing qa qc over time whether a sensor was providing valid data is based on the pat qc function which assesses whether the data from each sensor is within manufacturer defined specifications whether data was passing qa qc is based in part on how well the data from channels a and b agree in other words data flagged as invalid may be indicating a sensor malfunction whereas data not passing qa qc may be indicating poor sensor performance thus fig 5b and c may imply that declines in performance over time e g possibly associated with sensor degradation are of greater concern than an outright sensor failure 3 2 examining state of health with respect to time runtime and cumulative exposure utilizing the soh metrics the following plots fig 6 12 depict the average value of the indicated statistic by month for each sensor when data is available in addition the number of sensors included each month is listed across the top of the plot figs 6 8 and 10 additional figures in the supplement figs s1 s2 and s3 provide similar plots for other soh metrics fig 6 depicts the soh ab fit daily metric for r2 between channels a and b averaged by month for the plot this plot indicates variability in the ab fit r2 metric that appears to be seasonal the highest agreement between channels seems to occur in the winter months november december and january additionally the r2 values during these months are similar throughout the three years shown with median values consistently above 0 9 the following plots depict the percent of valid data soh metric for channels a and b 7a and 7c the slope value from the ab fit soh metric 7b and the overall percent of data passing qa qc 7d note the number of sensors shown each month in fig 7a d is the same as in the previous figure for each plot depicting a percentage 7a 7c and 7d each box shows the percent of data either valid or passing qa qc for each sensor by month as shown in fig 7a and c the sensors seem to exhibit high reliability in terms of providing valid data there appears to be relatively high consistency in the slope between channels a and b with values remaining close to 1 0 however the range or spread of average slope values may be increasing toward the end of the three year period finally similar to fig 5c the percent of data that passes qa qc appears to decrease over time to add further context to these results fig 8a and b summarize the pm2 5 and humidity levels observed by the sensors throughout the same three year period aggregated by month while the pm2 5 concentrations seem to be the lowest in the spring the seasonal trend from year to year is not as clear as the trend observed in fig 6 this likely indicates that more factors than simply ambient pm2 5 levels are driving the variability in the agreement between the channels a and b given that the sensor installations were ongoing as opposed to occurring simultaneously soh metrics were also examined in terms of cumulative runtime a sensor was considered powered on by the collection of data on at least one parameter e g particulate matter temperature humidity etc and all powered on periods were factored into the calculation of the cumulative runtime any period where the sensor could not be confirmed to have been powered on was excluded this analysis was driven by an interest in determining whether a recommendation regarding the typical operational period for these sensors could be established however as illustrated by figs 9 and 10 there is no clear trend of decreasing sensor performance or a point at which sensors consistently fail after approximately the first year of operation there appears to more variance in the percent of data passing qa qc from the sensors fig 10d however this trend does not seem to continue for sensors with longer operational times upwards of 100 sensors collected two years of data passing qa qc and approximately 30 sensors seem to provide data passing qa qc for three years the observation that a subset of sensors could operate successfully for three years supports the sensor manufacturer s claim of a mean time to failure mttf greater than three years plantower technology 2016 though the mttf is likely heavily dependent on the type and quantity of pm the sensor is exposed to and dependent on the operating conditions meteorological experienced by the sensor figs 11 and 12 examine the same soh metrics with respect to the estimated cumulative exposure these results suggest that high cumulative exposures do not result in increased instances of sensor failures or decreases in the amount of data that passes qa qc in a laboratory study pms5003 sensors the same sensors used in the pa ii were exposed to concentrations of pm2 5 specifically and total pm of 7 300 and 33 000 μg m3 respectively for a total of 18 h tryner et al 2020 researchers observed issues with sensor accuracy after 6 h tryner et al 2020 assuming the same flow rate used in equation 1 6 h at these concentrations would be equivalent to cumulative exposures of approximately 37 000 μg and 170 000 μg for pm2 5 and total pm respectively while comparable levels of cumulative exposure were observed in the star grant deployments in terms of the sensors soh over time there does not appear to be a relationship between diminished performance or data quality and high cumulative exposures however it is important to note the pm exposure for star grant sensors involved much lower concentrations over much longer timeframes in section 3 3 the impact of cumulative exposure on accuracy with respect to data from a regulatory monitoring station is also assessed to further explore the seasonal trend observed in fig 6 r2 has been plotted against several parameters in fig 13 including the pm2 5 concentrations 13a temperature 13b humidity 13b and month 13d these plots seem to indicate that in addition to consistently occurring in late spring and early summer months lower r2 values seem to coincide with lower levels of pm2 5 capabilities of the technology likely explain the lower correlation at low pm2 5 concentrations in field studies measurement errors for plantower sensors between approximately 3 11 μg m3 have been observed feenstra et al 2019 bulot et al 2019 sayahi et al 2018 liu et al 2020 note while figs 6 12 depict the aggregate data for all star grant community networks similar plots depicting data from individual communities are available in the supplement fig s4 s17 in the supplement as discussed studies have observed variability in sensor performance attributed to seasonal trends pm sources types and pm composition sayahi et al 2018 liu et al 2020 however access to data from multiple communities also allows for the exploration of spatial trends and for the star grant sensor networks there are interesting differences between communities the seasonal trend in the agreement between channels a and b fig 6 is more prominent in the data from communities on the western side of the south coast air basin scab such as communities c h and l see figs s6 s11 and s15 these three communities range from coastal to near downtown los angeles conversely the seasonal trend is less apparent in data from communities on the eastern side of the basin such as communities j k and z see figs s13 s14 and s17 for the sites in the eastern portion of the basin in communities j k and z the agreement between channels a and b is more consistent throughout the different months of deployment this spatial trend may provide insight into what drives the seasonal trend in the agreement between channels a and b for some communities in the scab periods of greater atmospheric stability often occur in the winter months and result in less transport of emissions and more local accumulation whereas in the warmer months emissions from the west side of the basin are often transported to the east by onshore winds hasheminassab et al 2014 vutukuru et al 2006 this transport affects pm composition in different parts of the air basin a source apportionment study that included sampling sites in downtown los angeles west and riverside east using data collected from 2002 to 2013 revealed that secondary inorganic aerosols i e ammonium nitrate and sulfate tend to dominate at both sites although secondary aerosols are generally higher in riverside hasheminassab et al 2014 hasheminassab et al 2014b in addition at the los angeles site vehicle emissions are an important contributor in the winter months whereas aged sea salt aerosols become an important contributor in the spring and summer months hasheminassab et al 2014 hasheminassab et al 2014b furthermore for the communities on the western side of the basin especially those on the coast onshore winds may also result in more dispersal and lower pollutant concentrations assuming pm compositions are similar to those measured previously the seasonal trends observed in our study indicate that the data from the plantower pms5003 sensor may be less reliable in situations with low pollutant concentrations and for specific sources such as aged sea salt aerosols fig s28 in the supplemental materials shows the distribution of r squared values from the soh ab fit metric for different ranges of pm2 5 concentrations and this plot confirms that lower r squared values tend to occur with greater frequency for smaller pm2 5 ranges e g pm2 5 less than 10 μg m3 and pm2 5 between 10 and 20 μg m3 the observation related to source type is supported by the findings of other research including a field study that suggests the plantower pms1003 is not well suited to detect marine aerosols liu et al 2020 and a laboratory study that found that the plantower pmsa003 underpredicted mass concentrations of nacl zamora et al 2019 however it would be valuable future work to continue exploring the plantower pms5003 sensor s responsiveness to a range of aerosols including aged sea salt in the meantime these results illustrate the importance of using tools such as soh metrics to systematically track variability in sensor performance for large sensor networks 3 3 performance and reliability of sensors with respect to regulatory grade data the subset of sensors co located at an ambient ams provided the opportunity to compare the sensor and regulatory grade data collected by federal reference method and federal equivalent method frm fem instruments over the three years as previously described the airsensor package leverages the pwfslsmoke package pwfsl pacific wildland fire sciences lab https github com mazamascience pwfslsmoke to provide access to pm2 5 data from the nearest ams feenstra et al 2020 thus airsensor package functions are used to access the regulatory grade data from the monitoring site at which the sensors were co located fig 14 depicts the average monthly r2 value from each of the sensors in community z the r2 is calculated using the post qa qc hourly aggregated sensor data and the regulatory grade data that is provided at an hourly time resolution fig 14 displays a strong seasonal trend with higher r2 values and less variability in the winter and lower r2 values in the warmer months this trend is similar to the one seen in fig 6 but there are some key differences first fig 14 includes only the data from the sensors in community z and as noted previously the seasonal trend in the agreement between channels a and b was not as pronounced for community z as compared to communities on the west side of the air basin fig s17 additionally if we compare the density scatter plot of this data fig 15 and the previous density scatter plot fig 13 we do not see the same association between lower r2 values lower pm2 5 concentrations and the late spring early summer months expanding on the discussion in section 3 2 studies in various cities have consistently observed higher correlation between sensors and reference data in winter months liu et al 2020 sayahi et al 2018 a deployment of two plantower pms1003 and two plantower pms5003 sensors in salt lake city ut observed correlations with reference data of 0 87 in the winter months 0 18 0 32 in march june and 0 48 0 72 in june october wildfire season sayahi et al 2018 compared to this literature the r2 values observed here are slightly lower in the winter months 0 80 0 90 similar in june october 0 40 0 80 and higher for march june 0 40 0 80 these studies attribute this seasonal variability to pollutant concentrations and pm composition the second study observed this seasonal trend for sensors deployed in beijing sydney brisbane and on the gold coast liu et al 2020 both studies attribute this trend at least in part to typically higher pm concentrations in the winter months and lower pm concentrations in the spring summer and fall months liu et al 2020 sayahi et al 2018 in terms of pm composition the researchers using the plantower pms1003 in multiple cities also observed that this sensor is less well suited for detecting fresh vehicle emissions and marine aerosols while it is more well suited to detect mixed urban background emissions aged vehicle emissions and industrial emissions liu et al 2020 this relationship to sources were determined by filtering the data by wind direction and sorting according to the nearby sources at multiple field sites liu et al 2020 similar to the previous conclusions regarding the variability in the level of agreement between the duplicate channels fig 6 the seasonal variability in performance is likely driven by some combination of pm concentrations and compositions fig 16 shows the pm2 5 and humidity values observed by the sensors and aggregated by month for community z this plot confirms that in general lower pm2 5 concentrations are observed in the spring and this seems to correspond to the reduced correlation between the sensor and regulatory data fig 14 for more information on the relationship between the r squared values and pm2 5 concentrations as well as concentrations observed by the regulatory monitoring instruments see plots s27 and s29 in the supplemental materials in addition laboratory studies have confirmed that the plantower ops sensors typically underpredict some types of pm e g ammonium sulfate arizona road dust talcum powder oleic acid and nacl while overpredicting others e g woodsmoke tryner et al 2020 zamora et al 2019 in addition the source apportionment study data from riverside illustrated the variation in dominant sources throughout the year pm in the winter is typically dominated by ammonium nitrate and vehicle emissions whereas in spring summer and fall months ammonium sulfate and aged sea salt aerosols become more prominent hasheminassab et al 2014 hasheminassab et al 2014b in addition this site is sometimes impacted by dust storms and wildfire emissions therefore the results of this co location suggest that the difference in responsiveness e g over or underestimation of the true pm2 5 levels by the sensor to different types of pm and typical seasonal trends can result in significant variability in sensor performance when compared to reference data from an ams those using sensor data should anticipate and take these seasonal trends into account further laboratory research quantifying the responsiveness of ops sensors to typical ambient aerosols might also support the development of seasonal sensor corrections or corrections informed by pm composition in the meantime we have demonstrated how the airsensor package may be used to quantify variability in sensor performance and reveal important patterns furthermore while the analysis here has been applied to a subset of sensors co located at an ams the same approach could be applied to any community this approach would enable users to compare each sensor in a network to the nearest reference monitor and would enable users to assess whether similar or different trends exist in their area we can learn more about this variability in sensor performance by examining other metrics as well fig 17 a 17f provide the monthly averages for other statistics typically used to evaluate sensor performance with respect to regulatory grade data for these statistics the seasonal variability appears more significant than any consistent linear trend for example rather than seeing a clear increase in rmse or mae over time periods where the median rmse and mae are approximately 5 μg m3 occur toward the beginning and end of the three years the ratio of mbe to mae mbe mae can indicate whether the error is primarily systematic bias or random absolute error here the mbe mae also fluctuates with a higher portion of random error typically occurring in spring or summer months in previous work by aq spec the pa ii sensor exhibited primarily random error when concentrations were 12 μg m3 and more systematic positive bias when concentrations were between 13 and 50 μg m3 feenstra et al 2019 the results here agree with these findings as the periods of primarily random error correspond to those months where pm2 5 is likely to be lower once again highlighting the relationship between sensor performance and seasonal trends fig 18 b illustrates that the magnitude of the random error is relatively consistent and centered at approximately 5 μg m3 when higher levels of error mae are observed they appear to be primarily attributable to systematic error in other words mae values above 7 μg m3 tend to be associated with higher mbe mae ratio values this distinction is important because it may be possible to reduce systematic errors by using sensor corrections which could reduce mae values additional plots examining sensor performance with respect to regulatory grade data over runtime and cumulative exposure are available in the supplement fig s18 s21 in the previously discussed laboratory study tryner et al found that three out of seven sensors began reporting high values after 6 h of exposure equivalent to cumulative exposures of approximately 37 000 μg and 170 000 μg for pm2 5 and total pm 2020 this change in performance was not observed after 3 h of exposure to very high particulate matter concentrations equivalent to cumulative exposures of approximately 19 000 μg and 84 000 for pm2 5 and total pm the deployments in this study resulted in comparable cumulative exposures for sensors however these cumulative exposures resulted from long term exposure to much lower levels of pm while fig 19 c seems to show an increase in the range and magnitude of the mbe after a cumulative exposure of approximately 100 000 μg seasonal influences are not apparent because the different installation dates are not accounted for in the plots of cumulative exposure this increase in the range and magnitude of the mbe is not sustained if the groups of sensors deployed together are viewed separately fig s22 thus we did not observe a clear relationship between cumulative exposure and sensor performance the data was filtered for pm2 5 concentrations based on the ams data between 10 and 15 μg m3 and a linear regression model was used to examine whether any consistent drift was observed for the sensors in community z over the three years note the annual average pm2 5 mass concentration at this regulatory monitoring site is approximately 12 μg m3 and this guided the selection of the range 10 15 μg m3 feenstra et al 2019 fig 20 shows the difference between the regulatory grade data and the aggregated sensor data the slope which is relatively close to zero indicates there does not appear to be any significant drift if this slope indicates drift by the sensors it would suggest a drift rate of approximately 0 23 μg m3 per year this value is well under the estimated uncertainty provided by the manufacturer of 10 μg m3 plantower technology 2016 this finding is in agreement with previous studies using purpleair sensors where drift was not observed in one study researchers applied a correction for drift but found that it did not improve the data mailings et al 2020 in another study dramatic drift was observed from one out of three deployed sensors but not the other two liu et al 2020 similar to the observation in the latter study dramatic and inconsistent drift was observed from two of the 16 sensors in community z see fig s23 in the supplement for both sensors dramatic baseline drift occurred for a period and then the baseline values for both sensors seemed to return to typical ambient levels as this type of malfunction does not appear to be consistent or predictable procedures and functions such as those in the airsensor package can be applied to identify these occurrences for example existing functions in the airsensor package could be used to periodically compare data from a sensor to data from the nearest regulatory monitor alerting the user of an increase in the difference between the two instruments currently the airsensor package supports the evaluation of a sensor s performance on an individual basis however adding functions that can compare a designated sensor to a nearby sensor or sensors could also aid in identifying this type of performance issue determining whether it is reasonable to compare or use data from sensors deployed at different times is related to the question of drift in table 4 the sensors from the co located community have been grouped based on installation date and the mean pairwise r2 and mae values for sensors within each group and between groups is shown table 5 provides the mean r2 and mae values within and between groups as well as the results from a student s t test for both r2 and mae these p values suggest that we cannot conclude that there is a significant difference between and within groups in all cases regardless of which sensors are being compared the mean pairwise r2 value is 0 96 and the mean mae value is 1 3 μg m3 therefore these results suggest that it is reasonable to compare data from sensors deployed at different times figs 9 and 10 also support this conclusion the aggregated data in figs 9 and 10 includes sensors with varied installation dates and varied run times yet the performance indicated by the soh metrics appears relatively consistent over time fig 22a and b provide the rolling r2 and mae values between each sensor and the regulatory data while there appears to be a gradual decline in r2 and a gradual increase in mae over time and variability throughout the deployment we continue to see relatively high r2 values and mae values below 10 μg m3 toward the end of the three years the results from this sensor subset suggest that these sensors are capable of operating outdoors for at least three years and providing useable data see fig 21 the typical temperatures humidity levels and concentrations experienced at this regulatory monitoring site should be considered as the feasibility of long term outdoor use may vary in other locations with harsher conditions 3 4 increasing spatial resolution through sensor networks and using the dataviewer to study this variability the star grant community sensor networks varied in their spatial coverage some communities installed nearly all sensors within a single neighborhood 1 sq mile while in other communities the sensors were distributed across multiple towns 20 sq miles thus this dataset offers the opportunity to compare the spatial variability of pollutant concentrations observed in community networks of different scales fig 23 shows the average r2 value between each sensor and the nearest regulatory grade instrument listed by community for the month indicated the colors in this figure are assigned based on the average distance between sensors showing the network s scale table s1 in the supplement provides a list of the average distances and the distance from each sensor to the nearest regulatory monitoring site averaged for each community consistently community z demonstrates the highest correlation with the nearest co located regulatory monitoring site fig s24 in the other communities both the r2 values and the spread varies between communities and over time for example in community d there is generally high agreement among the sensors in may fig 23a and c whereas there is more spread in the r2 values in december fig 23b and 23d in community k there also appears to be more agreement between sensors in may fig 23c and less agreement in december fig 23b and d while smaller scale communities tend to exhibit higher agreement between the sensors in general there are examples suggesting that the amount of variability can depend on more than the distance between sensors in fig 23c a larger scale community community m displays less variability than a smaller scale community community l several different factors could cause this observed variability in r2 values within a community local sources topography and metrological patterns can drive differences in trends among the sensors and between the sensors and the nearest ams the following figure further affirms the importance of factors other than scale in determining variability within a network fig 24 shows the average pairwise r2 and mae values for all sensor pairs with more than three months of overlapping data plotted against the average distance between sensors the highest r2 value and lowest mae is seen amongst the sensors in the co located community however the level of agreement amongst sensors in a community does not seem to be a function of distance alone this suggests that there is not a singular recommendation regarding the optimal distance between sensors in a community alternatively sensor siting should consider factors such as local sources and topography as well as research objectives note the average mae values are relatively small this is likely because the data shown is aggregated to include all available data from the deployments differences between sensors in the same network may be driven by local sources can be episodic and can occur over short timeframes figs 25 and 26 provide examples of the type of events that might drive variability among sensors in the same network community members participating in the star grant sometimes observed these influences from local sources and episodic events examples of using the dataviewer to observe these events are shared in figs 25 and 26 in fig 25 a screenshot from the airsensor dataviewer version 1 0 2 depicts what may be a plume of particulate matter passing through the community and causing elevated pm2 5 readings from a few sensors the sensors that registered the event or enhanced levels of pm2 5 are circled in yellow fig s25 in the supplement provides a pollution rose that confirms that the elevated pm2 5 levels likely originated from a source to the southwest fig 26 provides another example of a localized event potentially driving small scale variability in a community here the sensors in the southern half of the network display elevated pollutant levels elevated pm2 5 is not indicated by the active sensors to the north nor by the nearest regulatory monitoring site located 14 3 km to the southeast see fig s26 in the supplement for concurrent data from the nearest regulatory monitoring station the potential for sensors to provide information on spatial variability of pollutants at relatively small scales has been illustrated in this study as well as in other studies sadighi et al 2018 cao et al 2020 collier oxandale et al 2018 thus users should consider the factors that might drive local variability in their community as well as their research objectives when siting sensors for example if the user s intention is to provide data in an area where there are no nearby regulatory grade fem instruments users might consider siting sensors further apart to better understand how trends in the study area agree with regional trends whereas if users are interested in studying specific local sources high density networks covering smaller areas can help to meet this need 4 conclusion the work presented here describes and illustrates the value of enhancements to the airsensor package and dataviewer tool the first phase of this development resulted in version 0 5 which set the foundations for foss tools to support sensor work filling in the gap between data collection and data display feenstra et al 2020 the next phase resulting in version 1 0 has increased the functionality and made the tools more broadly accessibly with the publication of the package on cran the new soh metrics may be used to identify clear sensor failures such as days when the data from a sensor is incomplete or invalid the soh metrics may also be used to identify questionable sensor performance such as days when there is reduced agreement between the channel a and channel b data thus the tools and procedures enable users to systematically identify failing or poorly performing sensors and use replicable methods to determine the reliability of data throughout a deployment furthermore the availability of these standardized functions and qa qc algorithms through the airsensor package increases the accessibility of these procedures for communities and researchers and makes it easier for the methods to be shared replicated and discussed as r rstudio is a foss tool the soh metrics and qa qc algorithms can be customized and adapted as needed encouraging further development and exploration of these tools and concepts by users the analysis of sensor data presented here illustrates how the airsensor package may be used to quantify and ensure data quality as well as why this is critical in terms of using the data from large sensor networks this analysis suggests that for the pa ii sensors used in this study drift is not a major concern it is reasonable to compare data from sensors deployed at different times i e old versus new sensors and the sensors seem capable of providing useable data in the field for long time periods at least three years seasonal variability in sensor reliability and performance likely driven by seasonal trends in pm concentrations and compositions was found to be substantial and should be considered in the analysis and use of the data a limitation of this work is the focus on a single particulate matter sensor in addition the soh metrics and qa qc algorithms are based on this sensor s unique design i e the inclusion of duplicate sensors or channel a and channel b planned next steps for the airsensor package include adding functionality to support data from other sensors made by different manufacturers and adapting developing soh metrics and qa qc algorithms for other types of sensors this work will include developing functionalities for both other types of pm sensors and for gas phase pollutant sensors these next steps may also include adding functions to calculate data recovery apply sensor corrections and examine soh for a network of multiple sensors following these developments a similar analysis of the long term performance and reliability of other types of particulate matter sensors and gas phase sensors could be conducted as the deployment of large sensor networks continues tools like the airsensor package will enable users to better assess sensor state of health and thus improve the quality and reliability of data generated from these sensor networks data availability all the data used for this analysis is publicly available from either the data archive associated with the star grant work or from purpleair and the code used to conduct this analysis will enable access to the data and the replication of the work shared here funding information this publication was developed under assistance agreement no rd83618401 awarded by the u s environmental protection agency through the science to achieve results star program to south coast air quality management district it has not been formally reviewed by epa the views expressed in this document are solely those of the authors and do not necessarily reflect those of the epa south coast aqmd and epa do not endorse any products or commercial services mentioned in this publication declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors would like to thank dr jonathan callahan and hans martin at mazama science inc seattle wa for their collaboration and contributions in the development of the airsensor r package and the dataviewer application tools along with their valuable feedback on this manuscript the sensor data used and presented in this paper was collected by the air quality sensor performance evaluation center aq spec at south coast aqmd the authors would like to acknowledge the work of our co investigators at the university of california los angeles and at sonoma technology inc on the star grant project engage educate and empower california communities on the use and applications of low cost air monitoring sensors the authors would also like to thank the community groups leaders trainers coordinators and members sensor hosts that participated in the u s epa star grant and provided valuable feedback that allowed us to create and improve this work appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105256 
25659,as low cost air quality sensors become more widely utilized more tools and methods are needed to help users access process sensor data identify poorly performing sensors and analyze visualize sensor data free and open source software foss packages developed for use on foss data science platforms are well suited to support this need by offering replicable and shareable tools that can be adapted to meet a user or project s specific needs this paper describes enhancements to the foss airsensor r package version 1 0 and the dataviewer web application version 1 0 1 that have been developed to support data access processing analysis and visualization for the purpleair pa ii sensor this paper also demonstrates how these enhancements may be used to track and assess the health of air sensors in real time or for large historical datasets the dataset used for this analysis was collected during a multi year project with sensors deployed from october 2017 to october 2020 involving the distribution of approximately 400 pa ii sensors across 14 communities in southern central and northern california applying the tools in the airsensor package revealed a dramatic variability in sensor performance mainly driven by seasonal trends or particulate matter source type these results also indicate that this sensor can provide useful data for at least three years with little evidence of substantial or consistent drift further high agreement was observed between co located sensors deployed at different times indicating that it may be reasonable to compare data from old and new pa ii sensors in addition to assessing the long term performance and reliability of the pa ii sensor this analysis serves as a model for how data from large sensor networks may be effectively processed evaluated interpreted and communicated keywords open source r package air quality sensors particulate matter sensors qa qc sensor networks citizen science software availability the airsensor r package version 1 0 was developed by mazama science and south coast aqmd airsensor is free and open source software available through comprehensive r archive network https cran r project org package airsensor and the github repository https github com mazamascience airsensor tree master mazama science maintains the package as part of ongoing collaborations with federal state and local air quality agencies airsensor version 1 0 was first released in 2020 under general public license v3 0 gpl 3 0 and runs on windows unix and macintosh operating systems airsensor was written in r and program files are less than 5 mbytes airsensor is designed to be used with r 3 3 and rstudio the airsensor dataviewer web based application was developed by mazama science and south coast aqmd the airsensor dataviewer is free and open source software available through the github repository https github com mazamascience airsensordataviewer the dataviewer was first released in 2019 under general public license v3 0 gpl 3 0 and runs on windows unix and macintosh operating systems dataviewer was written in r and program files are less than 7 mbytes the dataviewer requires git apache docker r and r shiny server the complete code used to conduct this analysis including accessing the data processing the data and generating the plots and statistics is available https github com acollier oxandale long term performance of purpleair pa ii sensors under an epa star grant this code will allow a user to fully replicate the analysis shared here and may serve as a model for those using data from air quality sensors in addition there are two data repositories created using the airsensor package that offer access to data collected by sensors deployed under the star grant http data mazamascience com purpleair v1 and https airsensor aqmd gov purpleair v1 1 introduction air quality sensors are often cited for their potential to provide high spatial and temporal resolution data and their accessibility in terms of cost and ease of use the increased use of this technology inevitably results in the collection of large data sets these datasets in turn require methods and tools that enable more effective processing and evaluation to meet the increased need for proper interpretation and dissemination by researchers members of the public and regulatory agencies feenstra et al 2020 currently solutions for accessing and viewing air sensor data tend to be sensor and pollutant specific and may be developed by the manufacturer or project leads alternatively they may follow a citizen science model in which data is aggregated publicly some of these approaches to data access and display are open access while others are proprietary and offered as a software as a service saas or platform as a service paas that require private user accounts and often include subscriptions costs however these solutions are often limited for example accessing and working with the unprocessed or factory calibrated sensor data requires scientific and technical expertise in some cases data displays may not include access to historical data and sensor manufacturers often provide no information on what if any data quality assessment and improvement measures are being applied feenstra et al 2020 1 1 developing data access analysis and visualization solutions for air sensors the limitations of existing data solutions as related to the use of sensors by the public became apparent during the us epa science to achieve results star grant project entitled engage educate and empower california communities on the use and applications of low cost air monitoring sensors this us epa star grant was awarded to the south coast air quality management district south coast aqmd in 2016 the project was driven by the following objective provide communities across california with the knowledge necessary to appropriately select use and maintain low cost sensors and to correctly interpret the collected data activities included the distribution of approximately 400 particulate matter sensors within 14 california communities and extensive engagement with participants which offered many opportunities to collect feedback and insights from those hosting and using the sensors and the data for example survey data indicated that users were most often checking their data sometimes as opposed to often or everyday feenstra et al 2020 discussions during workshops suggested that this was at least in part driven by the difficulty of engaging with the data using the platform available in addition participants noted that accessing and analyzing historical data was difficult for those without a scientific or technical background in response to this feedback the r based airsensor package version 0 5 and dataviewer tool version 0 9 7 were developed to enable more effective use of the data from purpleair pa ii sensors feenstra et al 2020 the airsensor package is a foss r based package available in r rstudio that allows users to easily access process and visualize data from pa ii sensors registered as public the dataviewer tool is a web based application that leverages the functionality of the airsensor package to allow users to easily engage with and visualize sensor data the dataviewer tool requires no programming experience and provides an intuitive way to explore current and historical data a previous publication describes the airsensor package version 0 5 and the dataviewer tool version 0 9 7 and their capabilities in detail feenstra et al 2020 1 2 the need for tools and methods to examine and improve sensor data quality the first phase of development of the airsensor package laid a foundation by defining the format of data objects methods of sensor data access procedures for data processing functions for accessing and formatting supplementary data and various plotting visualization functions in terms of access the airsensor package version 0 5 and the coding associated with the dataviewer tool version 0 9 7 were available through github for the next phase of development the primary goals were to expand the functionalities related to assessing and or improving data quality and increase the robustness of the package and the accessibility of the work to meet data processing needs for large sensor deployments formal methods and procedures were needed to monitor air sensor data quality in real time or assess the quality of historical data while several platforms are working to aggregate data from air quality sensors they do not currently apply any quality assurance quality control qa qc procedures the air quality data commons aqdc whose development is being led by the environmental defense fund has focused on developing standard data formats but data is uploaded as submitted by users https aqdatacommons org similarly openaq an open source open access platform that aggregates data from regulatory air monitors worldwide has begun adding data from air quality sensors https openaq org openaq is uploading data from the aqdc and two sensor manufacturers who provide open access to aggregated data from their sensors to pilot this new feature openaq 2021a b neither platform applies any qa qc or filtering and users are advised to consult manufacturers procedures and the metadata associated with specific datasets for more information environmental defense fund n d openaq 2021a b commercial platforms managed by manufacturers may apply qa qc procedures before displaying data but the details of this data processing are often not transparent regulatory agencies are beginning to explore the possibility of integrating data from air quality sensors into platforms that communicate air quality information from regulatory stations for example the us epa has developed qa qc procedures and a nationwide correction to enable data from pa ii sensors to be incorporated into the airnow fire and smoke map https fire airnow gov at the south coast aqmd staff has also developed qa qc procedures and a correction algorithm to enable the data from pa ii sensors to be incorporated into maps of data from regulatory monitoring stations http www aqmd gov home air quality current air quality data see current hourly air quality index map the combined data is able to provide more locally relevant information with high spatial resolution schulte et al 2020 for both of these examples the details of the data processing approach are described in publications but they are not necessarily readily accessible or easy to apply to the work of other sensor data users barkjohn et al 2021 schulte et al 2020 developing foss software airsensor r package on foss platforms r rstudio to gather data in an efficient format and easily apply and or alter these types of methods and procedures will increase their accessibility enabling sensor researchers users to integrate them into workflows compare different approaches and eventually optimize them 1 3 current understanding of long term sensor performance and reliability data quality is a critical issue for this technology considering continual concerns regarding sensor performance sensor reliability over time sensor lifetime and the potential for drift and sensor degradation while there have been many studies evaluating the performance of air sensors across a variety of environments as well as in controlled laboratory settings feenstra et al 2019 collier oxandale et al 2019 castell et al 2017 questions persist regarding sensor performance over long term e g one or more years field deployments feinberg et al 2018 clements et al 2017 a few studies have examined pm2 5 or particulate matter of 2 5 μm in aerodynamic diameter or smaller sensor performance during deployments occurring over approximately a year in these studies researchers observed moderate to good correlation with co located regulatory grade instruments bulot et al 2019 liu et al 2020 these researchers also suggest that the variability in performance correlation with co located regulatory grade instruments over these long term deployments may be driven by seasonal trends and the type of particulate matter pm to which the sensor is exposed sayahi et al 2018 bulot et al 2019 liu et al 2020 laboratory studies have observed pm sensors exhibit differing levels of accuracy depending on the type of pm used in the test supporting this assertion tryner et al 2020 zamora et al 2019 drift or an increase in the error over time is another important consideration for long term deployments or when sensors within a network are not deployed simultaneously studies evaluating pm sensors deployed simultaneously did not observe drift to be a significant concern during one year of network deployment mailings et al 2020 liu et al 2020 zamora et al 2019 however research has suggested that there may be issues with sensor accuracy associated with high cumulative exposures one laboratory chamber study demonstrated this potential reliability issue by exposing sensors to very high pm concentrations over short periods to simulate long term ambient exposure tryner et al 2020 the above observations are valuable and begin to answer questions related to long term sensor performance however these studies are limited in terms of the number of sensors tested and the duration of the deployments the manufacturer plantower technology 2016 of the sensor examined here suggests a lifetime of three years but none of the studies described tests sensor performance over that length of time these studies also do not consider other issues likely to occur in deployments involving participation by the public such as the comparability of sensors within a community scale network that were deployed weeks or even months apart given the accessibility of some commercial pm sensors there are networks where data from hundreds of sensors is being viewed on a single map frame data display however these sensors may vary significantly in terms of sensor age and cumulative environmental exposure developing systematic and replicable approaches to qa qc that can be applied either in real time or to historical data from large sensor networks could enable the collection of more reliable and useful data and lead to a better understanding of sensor performance 1 4 the airsensor package version 1 0 and dataviewer tool version 1 0 1 here we present enhancements to the airsensor package version 1 0 and dataviewer version 1 0 1 tool and show how these tools may be used to process sensor data and assess sensor performance historically or in real time the enhancements include expanded qc algorithms new state of health metrics for sensors additions to the dataviewer and the increased robustness and accessibility of the package through publication on the comprehensive r archive network cran repository the utility of these enhancements is demonstrated through an assessment of data collected during the star grant project period october 2017 to october 2020 as such this dataset offers the opportunity to examine sensor performance across multiple years in this project participants deployed most sensors themselves which resulted into sensors being deployed at different times during the project period thus the dataset includes many of the real world issues associated with sensor networks deployed by the public leveraging the capabilities in the airsensor package we access process and examine the data from extensive sensor networks to make observations regarding the health of these sensors over time and learn more about long term performance and reliability this analysis also offers a replicable approach to this task that could be modelled to support the use and maintenance of other large sensor networks 2 methods 2 1 enhancements and modifications to the airsensor package and the dataviewer tool as previously mentioned the first phase of development for both the airsensor package version 0 5 and the dataviewer tool version 0 9 8 is described in detail with example code in a previous publication feenstra et al 2020 in terms of specific updates and enhancements to the package there have been changes to the purple air synoptic pas data objects as well as the data archive the addition of state of health metrics and revisions additions to the qa qc algorithms these additions and enhancements provide additional functionality to make the package more robust and enable publication on cran cran is the main repository for r code packages publication on cran requires packages to undergo thorough testing and review and makes them easy to access and install in r studio the pas data objects are either created from the json data available at www purpleair com json or accessed from the local star grant data archive https airsensor aqmd gov purpleair v1 these data objects provide a summary of purpleair sensor data from a point in time the default version of this data object provides information on all sensors in the united states classified as public by users during registration and currently active users may also access historical summaries that include both active and inactive sensors the pas data objects have been modified to include an additional column devicedeploymentid a new identifier that includes both the sensor s id and the gps location this identifier was added to avoid confusion that might occur if there are two sets of timeseries data associated with an identical label or name which could arise if one sensor is moved to a new location with no changes to the sensor label or name though there are unique identifiers associated with each sensor and assigned by purpleair the labels are used to identify sensors by the airsensor package functions and these labels are assigned by the user in addition the star grant data archive has been updated to accommodate changes to the airsensor data object formats the example code below loads the package sets the archive base to the new star grant data archive and loads a pas object where the new identifier column can be seen in addition to the increased robustness of the data archive and sensor identifiers the ability to access the additional particulate matter data pm1 0 and pm10 that the sensor provides has been added library airsensor setarchivebaseurl http data mazamascience com purpleair v1 temporary archive setarchivebaseurl https airsensor aqmd gov purpleair v1 long term archive pas example pas load new functions have been added that allow the user to calculate state of health soh metrics for sensors and visualize sensor performance the soh metrics are calculated on a 24 hr basis using the purple air timeseries pat data objects as inputs for individual sensors the output provides the user with a list of metrics that can be used to understand sensor performance over a user defined timeframe table 1 lists these metrics which include factors such as completeness percent of data that is considered valid based on manufacturer s listed range of operation the level of agreement between the duplicate channel a and channel b sensors in each unit and other daily linear model fit checks between various parameters measured by the pa ii the airsensor package provides functions to generate all soh metrics listed in table 1 at once for a given sensor the user can also generate those metrics individually and a function is available to calculate the daily index values the output in all cases is essentially a time series that includes the date time and the soh metric s for each day there are also two plotting functions available to visualize sensor soh metrics the example code below illustrates the use of the package to create a pat that contains all available 2018 data for a single sensor the following lines of code produce the resulting figs 1 and 2 fig 1 provides an overview of the data collected by the sensor and indicates the data s reliability based on the calculated index value fig 2 provides multiple plots of individual soh metrics which can serve as useful diagnostic information when examining sensor soh over time for example the period of poor performance in september of 2018 fig 1 coincides with a period of missing data fig 2 see the percent reporting metric it is also evident that while the data generally shows high completeness and validity fig 2 occasional periods of lower agreement between the channel a and b data result in an index value of fair figs 1 and 2 pat example pat load pas pas example label scsb 20 startdate 20180101 enddate 20181231 timezone america los angeles pat dailysohindexplot pat example pat dailysohplot pat example in addition to the soh metrics functions to support data processing and qa qc checks have been revised and added in the airsensor package version 1 0 2 four algorithms are now available and they are described in table 2 for data that passes a qa qc check listed in table 2 the pm2 5 data from channels a and b are averaged and aggregated to an hourly time resolution as previously described these algorithms may be applied to a pat data object and the resulting output is a sensor data object the sensor data objects contain the final processed data which may be used for analysis and visualization feenstra et al 2020 new qa qc algorithms titled purpleairqc hourly ab 02 and purpleairqc hourly ab 03 have been added to provide more options and examples of algorithms furthermore the robustness of the qa qc functions has been improved by reducing the number of arguments or inputs available to users this change will result in greater consistency among those using the algorithms if users would like to change any parameters of a qa qc algorithm they are encouraged to build a new algorithm a helpful vignette that illustrates how to create new qa qc algorithms is available on github https mazamascience github io airsensor articles articles custom qc algorithms html highlighting the value of foss solutions users can easily make local copies of existing qc algorithms that can be adapted or users may create entirely new qc algorithms modelled on those offered in the package like the airsensor package version 0 5 the dataviewer tool version 0 9 8 and its features have been described in detail in a previous publication enhancements made to the dataviewer tool version 1 0 1 primarily include improved appearance usability data handling and performance the one prominent feature added in this phase of development is the interactive time slider that provides users with more control over the data displayed on the map in the overview tab fig 3 this new feature facilitates interaction with the air quality map data and thus is expected to increase user engagement 2 2 using the airsensor package version 1 0 to quantify sensor performance and improve data quality the new soh features may be used to track sensor health in real time or to examine and process historical data by applying these functions in real time sensor network managers may be able to identify poorly performing or malfunctioning sensors and more efficiently schedule maintenance or replacement resulting in higher quality sensor network data by applying the functions to historical data sensor network managers have a systematic way to evaluate long term data from extensive sensor networks the airsensor package version 1 0 is used to access and load data into the rstudio environment calculate the soh metrics and apply qa qc functions to investigate the long term performance and reliability of sensors the qa qc algorithm selected and used throughout this analysis is purpleairqc hourly ab 03 this work focuses on pm2 5 data from the cf atm channel the purpleair sensor includes a cf atm and cf 1 channel with different manufacturer defined data processing applied to each and no additional corrections were applied to the sensor data purpleair 2020 this approach ensured that the data used in this analysis was the same data most likely to be viewed by the public the soh metrics and proportion of data passing the selected qa qc algorithm were examined over time cumulative runtime and cumulative exposure this analysis included all the data collected within the project timeframe october 2017 to november 2020 for each sensor the cumulative runtime for each sensor excluded periods where the sensor appeared to be powered off or was not sending data to the online database the estimated cumulative mass exposure to pm for each sensor was calculated using equation 1 the calculation for cumulative mass exposure is based on an estimated flow of 0 5 cfm cubic feet per minute for the sensor additional details are available in the supplemental materials eq 1 c u m u l a t i v e m a s s e x p o s u r e μ g n 1 n p m 2 5 d a i l y a v e r a g e 20 3881 μ g in addition our analysis includes an assessment of how long sensors were in operation and how many experienced failures or intermittent qa qc issues one community referred to here as community z table 3 included 16 sensors all co located at a regulatory air monitoring station ams the data from this community was used to examine the performance of sensors with respect to data from regulatory grade instruments several common sensor evaluation metrics were used to compare sensor and regulatory data including rmse root mean squared error mae mean absolute error mbe mean bias error and the r2 or coefficient of determination used to indicate goodness of fit these metrics were calculated on a daily basis using a 30 day moving window this approach results in data on at the same time resolution as the soh metrics 2 3 additional details regarding air quality sensors used and the deployed networks under the star grant project sensor networks were deployed within 14 california communities these communities varied in terms of their location and nearby land use they included coastal and inland desert communities as well as urban suburban and rural communities in addition to examining the aggregate data from many sensors this range of communities provides the opportunity to explore sensor performance across different environments here we present data from 13 of the 14 communities the sensors in the 14th community were deployed earlier than the other 13 meaning the deployment dates do not cover the same timeframe however in addition to the 13 communities data from co located sensors at a regulatory ams are included as community z community z had 16 sensors deployed in batches at the ams site in parallel to the sensor deployments occurring in the other 13 communities here the communities are identified by the letters a m with the ams co located community identified as community z table 3 provides brief descriptions of each community the purpleair pa ii sensor was selected for this project based on community air pollutant concerns for pm the open access nature of the data cost of the sensor and the sensor s performance during evaluations feenstra et al 2019 south coast aqmd 2020 the pa ii uses optical particle sensor ops technology to estimate pm1 0 pm2 5 and pm10 mass concentrations and the sensor collects near ambient weather data including temperature humidity and pressure hagen and kroll 2020 each pa ii includes two opc pm sensors the pms5003 by plantower referred to as channel a and channel b while the purpleair uses the plantower pms5003 sensor our results are discussed in relation to studies using this and other similar sensors by the same manufacturer e g plantower pms1003 data from these sensors is automatically streamed over a wi fi connection to a thingspeak database which is accessible via an api application programming interface the data may also be viewed or downloaded through an online platform www purpleair com map open access to the data allowed project leads to track sensor installations by the community members and monitor the data participation was gauged through the rates of installation which varied across the communities ranging from 30 to 115 in a community where additional sensors were requested collier oxandale 2021 the data set used in this analysis includes data from 257 sensors these sensors were correctly labelled making them easily identifiable and making the data easily accessible using the functions in the airsensor package data were available from another 19 sensors but this subset was sited indoors and thus they are omitted 3 results discussion the results presented here are from an analysis of all data available from these community networks for the three year period of october 1 2017 to november 1 2020 these results provide insights into sensor performance and reliability in the context of community led deployments where many of the typical challenges associated with environmental monitoring projects involving participation by the public occurred collier oxandale 2021 given this context the findings presented may be relevant to other community based projects using air quality sensors particularly those using the same type of sensor 3 1 overview of the star grant sensor networks table 3 provides an overview of the participating communities an indication of the size of the networks and an indication of how much data was available for this analysis in all communities at least one sensor collected well over a year of data and the median for aggregate of all communities is 544 days or approximately 1 5 years expanding on the information provided in the previous table fig 4 groups sensors based on whether they exhibited very few qa qc issues during their operation group 1 exhibited periodic qa qc issues but seemed to recover group 2 or were not passing qa qc when they stopped recording data possibly due to failure or being intentionally powered off group 3 in all communities a portion of the sensors provided data that passed qa qc checks throughout their deployment though most sensors in all communities except for community m fell into group 1 or group 2 there were sensors in all 14 communities that exhibited either periodic or persistent qc issues the presence of these issues further highlights the importance of establishing procedures to identify poorly performing sensors and or data of questionable quality the sensors were distributed between october 2017 and august 2018 and community members completed the installations on a rolling basis in addition in some communities more sensors were added or replaced over time sensors were replaced when they experienced some type of failure e g not reporting data despite troubleshooting approximately 10 sensors were replaced over the course of the project across all communities and no sensors were replaced in community z fig 5 a illustrates the rolling nature of the deployment by showing the total number of operational sensors over time along with the number of sensors providing valid data and data passing qa qc checks after the first three months there were well over 100 sensors in operation at any given time fig 5b and c provide further context by showing the percent of sensors providing either valid data or data passing qa qc over time whether a sensor was providing valid data is based on the pat qc function which assesses whether the data from each sensor is within manufacturer defined specifications whether data was passing qa qc is based in part on how well the data from channels a and b agree in other words data flagged as invalid may be indicating a sensor malfunction whereas data not passing qa qc may be indicating poor sensor performance thus fig 5b and c may imply that declines in performance over time e g possibly associated with sensor degradation are of greater concern than an outright sensor failure 3 2 examining state of health with respect to time runtime and cumulative exposure utilizing the soh metrics the following plots fig 6 12 depict the average value of the indicated statistic by month for each sensor when data is available in addition the number of sensors included each month is listed across the top of the plot figs 6 8 and 10 additional figures in the supplement figs s1 s2 and s3 provide similar plots for other soh metrics fig 6 depicts the soh ab fit daily metric for r2 between channels a and b averaged by month for the plot this plot indicates variability in the ab fit r2 metric that appears to be seasonal the highest agreement between channels seems to occur in the winter months november december and january additionally the r2 values during these months are similar throughout the three years shown with median values consistently above 0 9 the following plots depict the percent of valid data soh metric for channels a and b 7a and 7c the slope value from the ab fit soh metric 7b and the overall percent of data passing qa qc 7d note the number of sensors shown each month in fig 7a d is the same as in the previous figure for each plot depicting a percentage 7a 7c and 7d each box shows the percent of data either valid or passing qa qc for each sensor by month as shown in fig 7a and c the sensors seem to exhibit high reliability in terms of providing valid data there appears to be relatively high consistency in the slope between channels a and b with values remaining close to 1 0 however the range or spread of average slope values may be increasing toward the end of the three year period finally similar to fig 5c the percent of data that passes qa qc appears to decrease over time to add further context to these results fig 8a and b summarize the pm2 5 and humidity levels observed by the sensors throughout the same three year period aggregated by month while the pm2 5 concentrations seem to be the lowest in the spring the seasonal trend from year to year is not as clear as the trend observed in fig 6 this likely indicates that more factors than simply ambient pm2 5 levels are driving the variability in the agreement between the channels a and b given that the sensor installations were ongoing as opposed to occurring simultaneously soh metrics were also examined in terms of cumulative runtime a sensor was considered powered on by the collection of data on at least one parameter e g particulate matter temperature humidity etc and all powered on periods were factored into the calculation of the cumulative runtime any period where the sensor could not be confirmed to have been powered on was excluded this analysis was driven by an interest in determining whether a recommendation regarding the typical operational period for these sensors could be established however as illustrated by figs 9 and 10 there is no clear trend of decreasing sensor performance or a point at which sensors consistently fail after approximately the first year of operation there appears to more variance in the percent of data passing qa qc from the sensors fig 10d however this trend does not seem to continue for sensors with longer operational times upwards of 100 sensors collected two years of data passing qa qc and approximately 30 sensors seem to provide data passing qa qc for three years the observation that a subset of sensors could operate successfully for three years supports the sensor manufacturer s claim of a mean time to failure mttf greater than three years plantower technology 2016 though the mttf is likely heavily dependent on the type and quantity of pm the sensor is exposed to and dependent on the operating conditions meteorological experienced by the sensor figs 11 and 12 examine the same soh metrics with respect to the estimated cumulative exposure these results suggest that high cumulative exposures do not result in increased instances of sensor failures or decreases in the amount of data that passes qa qc in a laboratory study pms5003 sensors the same sensors used in the pa ii were exposed to concentrations of pm2 5 specifically and total pm of 7 300 and 33 000 μg m3 respectively for a total of 18 h tryner et al 2020 researchers observed issues with sensor accuracy after 6 h tryner et al 2020 assuming the same flow rate used in equation 1 6 h at these concentrations would be equivalent to cumulative exposures of approximately 37 000 μg and 170 000 μg for pm2 5 and total pm respectively while comparable levels of cumulative exposure were observed in the star grant deployments in terms of the sensors soh over time there does not appear to be a relationship between diminished performance or data quality and high cumulative exposures however it is important to note the pm exposure for star grant sensors involved much lower concentrations over much longer timeframes in section 3 3 the impact of cumulative exposure on accuracy with respect to data from a regulatory monitoring station is also assessed to further explore the seasonal trend observed in fig 6 r2 has been plotted against several parameters in fig 13 including the pm2 5 concentrations 13a temperature 13b humidity 13b and month 13d these plots seem to indicate that in addition to consistently occurring in late spring and early summer months lower r2 values seem to coincide with lower levels of pm2 5 capabilities of the technology likely explain the lower correlation at low pm2 5 concentrations in field studies measurement errors for plantower sensors between approximately 3 11 μg m3 have been observed feenstra et al 2019 bulot et al 2019 sayahi et al 2018 liu et al 2020 note while figs 6 12 depict the aggregate data for all star grant community networks similar plots depicting data from individual communities are available in the supplement fig s4 s17 in the supplement as discussed studies have observed variability in sensor performance attributed to seasonal trends pm sources types and pm composition sayahi et al 2018 liu et al 2020 however access to data from multiple communities also allows for the exploration of spatial trends and for the star grant sensor networks there are interesting differences between communities the seasonal trend in the agreement between channels a and b fig 6 is more prominent in the data from communities on the western side of the south coast air basin scab such as communities c h and l see figs s6 s11 and s15 these three communities range from coastal to near downtown los angeles conversely the seasonal trend is less apparent in data from communities on the eastern side of the basin such as communities j k and z see figs s13 s14 and s17 for the sites in the eastern portion of the basin in communities j k and z the agreement between channels a and b is more consistent throughout the different months of deployment this spatial trend may provide insight into what drives the seasonal trend in the agreement between channels a and b for some communities in the scab periods of greater atmospheric stability often occur in the winter months and result in less transport of emissions and more local accumulation whereas in the warmer months emissions from the west side of the basin are often transported to the east by onshore winds hasheminassab et al 2014 vutukuru et al 2006 this transport affects pm composition in different parts of the air basin a source apportionment study that included sampling sites in downtown los angeles west and riverside east using data collected from 2002 to 2013 revealed that secondary inorganic aerosols i e ammonium nitrate and sulfate tend to dominate at both sites although secondary aerosols are generally higher in riverside hasheminassab et al 2014 hasheminassab et al 2014b in addition at the los angeles site vehicle emissions are an important contributor in the winter months whereas aged sea salt aerosols become an important contributor in the spring and summer months hasheminassab et al 2014 hasheminassab et al 2014b furthermore for the communities on the western side of the basin especially those on the coast onshore winds may also result in more dispersal and lower pollutant concentrations assuming pm compositions are similar to those measured previously the seasonal trends observed in our study indicate that the data from the plantower pms5003 sensor may be less reliable in situations with low pollutant concentrations and for specific sources such as aged sea salt aerosols fig s28 in the supplemental materials shows the distribution of r squared values from the soh ab fit metric for different ranges of pm2 5 concentrations and this plot confirms that lower r squared values tend to occur with greater frequency for smaller pm2 5 ranges e g pm2 5 less than 10 μg m3 and pm2 5 between 10 and 20 μg m3 the observation related to source type is supported by the findings of other research including a field study that suggests the plantower pms1003 is not well suited to detect marine aerosols liu et al 2020 and a laboratory study that found that the plantower pmsa003 underpredicted mass concentrations of nacl zamora et al 2019 however it would be valuable future work to continue exploring the plantower pms5003 sensor s responsiveness to a range of aerosols including aged sea salt in the meantime these results illustrate the importance of using tools such as soh metrics to systematically track variability in sensor performance for large sensor networks 3 3 performance and reliability of sensors with respect to regulatory grade data the subset of sensors co located at an ambient ams provided the opportunity to compare the sensor and regulatory grade data collected by federal reference method and federal equivalent method frm fem instruments over the three years as previously described the airsensor package leverages the pwfslsmoke package pwfsl pacific wildland fire sciences lab https github com mazamascience pwfslsmoke to provide access to pm2 5 data from the nearest ams feenstra et al 2020 thus airsensor package functions are used to access the regulatory grade data from the monitoring site at which the sensors were co located fig 14 depicts the average monthly r2 value from each of the sensors in community z the r2 is calculated using the post qa qc hourly aggregated sensor data and the regulatory grade data that is provided at an hourly time resolution fig 14 displays a strong seasonal trend with higher r2 values and less variability in the winter and lower r2 values in the warmer months this trend is similar to the one seen in fig 6 but there are some key differences first fig 14 includes only the data from the sensors in community z and as noted previously the seasonal trend in the agreement between channels a and b was not as pronounced for community z as compared to communities on the west side of the air basin fig s17 additionally if we compare the density scatter plot of this data fig 15 and the previous density scatter plot fig 13 we do not see the same association between lower r2 values lower pm2 5 concentrations and the late spring early summer months expanding on the discussion in section 3 2 studies in various cities have consistently observed higher correlation between sensors and reference data in winter months liu et al 2020 sayahi et al 2018 a deployment of two plantower pms1003 and two plantower pms5003 sensors in salt lake city ut observed correlations with reference data of 0 87 in the winter months 0 18 0 32 in march june and 0 48 0 72 in june october wildfire season sayahi et al 2018 compared to this literature the r2 values observed here are slightly lower in the winter months 0 80 0 90 similar in june october 0 40 0 80 and higher for march june 0 40 0 80 these studies attribute this seasonal variability to pollutant concentrations and pm composition the second study observed this seasonal trend for sensors deployed in beijing sydney brisbane and on the gold coast liu et al 2020 both studies attribute this trend at least in part to typically higher pm concentrations in the winter months and lower pm concentrations in the spring summer and fall months liu et al 2020 sayahi et al 2018 in terms of pm composition the researchers using the plantower pms1003 in multiple cities also observed that this sensor is less well suited for detecting fresh vehicle emissions and marine aerosols while it is more well suited to detect mixed urban background emissions aged vehicle emissions and industrial emissions liu et al 2020 this relationship to sources were determined by filtering the data by wind direction and sorting according to the nearby sources at multiple field sites liu et al 2020 similar to the previous conclusions regarding the variability in the level of agreement between the duplicate channels fig 6 the seasonal variability in performance is likely driven by some combination of pm concentrations and compositions fig 16 shows the pm2 5 and humidity values observed by the sensors and aggregated by month for community z this plot confirms that in general lower pm2 5 concentrations are observed in the spring and this seems to correspond to the reduced correlation between the sensor and regulatory data fig 14 for more information on the relationship between the r squared values and pm2 5 concentrations as well as concentrations observed by the regulatory monitoring instruments see plots s27 and s29 in the supplemental materials in addition laboratory studies have confirmed that the plantower ops sensors typically underpredict some types of pm e g ammonium sulfate arizona road dust talcum powder oleic acid and nacl while overpredicting others e g woodsmoke tryner et al 2020 zamora et al 2019 in addition the source apportionment study data from riverside illustrated the variation in dominant sources throughout the year pm in the winter is typically dominated by ammonium nitrate and vehicle emissions whereas in spring summer and fall months ammonium sulfate and aged sea salt aerosols become more prominent hasheminassab et al 2014 hasheminassab et al 2014b in addition this site is sometimes impacted by dust storms and wildfire emissions therefore the results of this co location suggest that the difference in responsiveness e g over or underestimation of the true pm2 5 levels by the sensor to different types of pm and typical seasonal trends can result in significant variability in sensor performance when compared to reference data from an ams those using sensor data should anticipate and take these seasonal trends into account further laboratory research quantifying the responsiveness of ops sensors to typical ambient aerosols might also support the development of seasonal sensor corrections or corrections informed by pm composition in the meantime we have demonstrated how the airsensor package may be used to quantify variability in sensor performance and reveal important patterns furthermore while the analysis here has been applied to a subset of sensors co located at an ams the same approach could be applied to any community this approach would enable users to compare each sensor in a network to the nearest reference monitor and would enable users to assess whether similar or different trends exist in their area we can learn more about this variability in sensor performance by examining other metrics as well fig 17 a 17f provide the monthly averages for other statistics typically used to evaluate sensor performance with respect to regulatory grade data for these statistics the seasonal variability appears more significant than any consistent linear trend for example rather than seeing a clear increase in rmse or mae over time periods where the median rmse and mae are approximately 5 μg m3 occur toward the beginning and end of the three years the ratio of mbe to mae mbe mae can indicate whether the error is primarily systematic bias or random absolute error here the mbe mae also fluctuates with a higher portion of random error typically occurring in spring or summer months in previous work by aq spec the pa ii sensor exhibited primarily random error when concentrations were 12 μg m3 and more systematic positive bias when concentrations were between 13 and 50 μg m3 feenstra et al 2019 the results here agree with these findings as the periods of primarily random error correspond to those months where pm2 5 is likely to be lower once again highlighting the relationship between sensor performance and seasonal trends fig 18 b illustrates that the magnitude of the random error is relatively consistent and centered at approximately 5 μg m3 when higher levels of error mae are observed they appear to be primarily attributable to systematic error in other words mae values above 7 μg m3 tend to be associated with higher mbe mae ratio values this distinction is important because it may be possible to reduce systematic errors by using sensor corrections which could reduce mae values additional plots examining sensor performance with respect to regulatory grade data over runtime and cumulative exposure are available in the supplement fig s18 s21 in the previously discussed laboratory study tryner et al found that three out of seven sensors began reporting high values after 6 h of exposure equivalent to cumulative exposures of approximately 37 000 μg and 170 000 μg for pm2 5 and total pm 2020 this change in performance was not observed after 3 h of exposure to very high particulate matter concentrations equivalent to cumulative exposures of approximately 19 000 μg and 84 000 for pm2 5 and total pm the deployments in this study resulted in comparable cumulative exposures for sensors however these cumulative exposures resulted from long term exposure to much lower levels of pm while fig 19 c seems to show an increase in the range and magnitude of the mbe after a cumulative exposure of approximately 100 000 μg seasonal influences are not apparent because the different installation dates are not accounted for in the plots of cumulative exposure this increase in the range and magnitude of the mbe is not sustained if the groups of sensors deployed together are viewed separately fig s22 thus we did not observe a clear relationship between cumulative exposure and sensor performance the data was filtered for pm2 5 concentrations based on the ams data between 10 and 15 μg m3 and a linear regression model was used to examine whether any consistent drift was observed for the sensors in community z over the three years note the annual average pm2 5 mass concentration at this regulatory monitoring site is approximately 12 μg m3 and this guided the selection of the range 10 15 μg m3 feenstra et al 2019 fig 20 shows the difference between the regulatory grade data and the aggregated sensor data the slope which is relatively close to zero indicates there does not appear to be any significant drift if this slope indicates drift by the sensors it would suggest a drift rate of approximately 0 23 μg m3 per year this value is well under the estimated uncertainty provided by the manufacturer of 10 μg m3 plantower technology 2016 this finding is in agreement with previous studies using purpleair sensors where drift was not observed in one study researchers applied a correction for drift but found that it did not improve the data mailings et al 2020 in another study dramatic drift was observed from one out of three deployed sensors but not the other two liu et al 2020 similar to the observation in the latter study dramatic and inconsistent drift was observed from two of the 16 sensors in community z see fig s23 in the supplement for both sensors dramatic baseline drift occurred for a period and then the baseline values for both sensors seemed to return to typical ambient levels as this type of malfunction does not appear to be consistent or predictable procedures and functions such as those in the airsensor package can be applied to identify these occurrences for example existing functions in the airsensor package could be used to periodically compare data from a sensor to data from the nearest regulatory monitor alerting the user of an increase in the difference between the two instruments currently the airsensor package supports the evaluation of a sensor s performance on an individual basis however adding functions that can compare a designated sensor to a nearby sensor or sensors could also aid in identifying this type of performance issue determining whether it is reasonable to compare or use data from sensors deployed at different times is related to the question of drift in table 4 the sensors from the co located community have been grouped based on installation date and the mean pairwise r2 and mae values for sensors within each group and between groups is shown table 5 provides the mean r2 and mae values within and between groups as well as the results from a student s t test for both r2 and mae these p values suggest that we cannot conclude that there is a significant difference between and within groups in all cases regardless of which sensors are being compared the mean pairwise r2 value is 0 96 and the mean mae value is 1 3 μg m3 therefore these results suggest that it is reasonable to compare data from sensors deployed at different times figs 9 and 10 also support this conclusion the aggregated data in figs 9 and 10 includes sensors with varied installation dates and varied run times yet the performance indicated by the soh metrics appears relatively consistent over time fig 22a and b provide the rolling r2 and mae values between each sensor and the regulatory data while there appears to be a gradual decline in r2 and a gradual increase in mae over time and variability throughout the deployment we continue to see relatively high r2 values and mae values below 10 μg m3 toward the end of the three years the results from this sensor subset suggest that these sensors are capable of operating outdoors for at least three years and providing useable data see fig 21 the typical temperatures humidity levels and concentrations experienced at this regulatory monitoring site should be considered as the feasibility of long term outdoor use may vary in other locations with harsher conditions 3 4 increasing spatial resolution through sensor networks and using the dataviewer to study this variability the star grant community sensor networks varied in their spatial coverage some communities installed nearly all sensors within a single neighborhood 1 sq mile while in other communities the sensors were distributed across multiple towns 20 sq miles thus this dataset offers the opportunity to compare the spatial variability of pollutant concentrations observed in community networks of different scales fig 23 shows the average r2 value between each sensor and the nearest regulatory grade instrument listed by community for the month indicated the colors in this figure are assigned based on the average distance between sensors showing the network s scale table s1 in the supplement provides a list of the average distances and the distance from each sensor to the nearest regulatory monitoring site averaged for each community consistently community z demonstrates the highest correlation with the nearest co located regulatory monitoring site fig s24 in the other communities both the r2 values and the spread varies between communities and over time for example in community d there is generally high agreement among the sensors in may fig 23a and c whereas there is more spread in the r2 values in december fig 23b and 23d in community k there also appears to be more agreement between sensors in may fig 23c and less agreement in december fig 23b and d while smaller scale communities tend to exhibit higher agreement between the sensors in general there are examples suggesting that the amount of variability can depend on more than the distance between sensors in fig 23c a larger scale community community m displays less variability than a smaller scale community community l several different factors could cause this observed variability in r2 values within a community local sources topography and metrological patterns can drive differences in trends among the sensors and between the sensors and the nearest ams the following figure further affirms the importance of factors other than scale in determining variability within a network fig 24 shows the average pairwise r2 and mae values for all sensor pairs with more than three months of overlapping data plotted against the average distance between sensors the highest r2 value and lowest mae is seen amongst the sensors in the co located community however the level of agreement amongst sensors in a community does not seem to be a function of distance alone this suggests that there is not a singular recommendation regarding the optimal distance between sensors in a community alternatively sensor siting should consider factors such as local sources and topography as well as research objectives note the average mae values are relatively small this is likely because the data shown is aggregated to include all available data from the deployments differences between sensors in the same network may be driven by local sources can be episodic and can occur over short timeframes figs 25 and 26 provide examples of the type of events that might drive variability among sensors in the same network community members participating in the star grant sometimes observed these influences from local sources and episodic events examples of using the dataviewer to observe these events are shared in figs 25 and 26 in fig 25 a screenshot from the airsensor dataviewer version 1 0 2 depicts what may be a plume of particulate matter passing through the community and causing elevated pm2 5 readings from a few sensors the sensors that registered the event or enhanced levels of pm2 5 are circled in yellow fig s25 in the supplement provides a pollution rose that confirms that the elevated pm2 5 levels likely originated from a source to the southwest fig 26 provides another example of a localized event potentially driving small scale variability in a community here the sensors in the southern half of the network display elevated pollutant levels elevated pm2 5 is not indicated by the active sensors to the north nor by the nearest regulatory monitoring site located 14 3 km to the southeast see fig s26 in the supplement for concurrent data from the nearest regulatory monitoring station the potential for sensors to provide information on spatial variability of pollutants at relatively small scales has been illustrated in this study as well as in other studies sadighi et al 2018 cao et al 2020 collier oxandale et al 2018 thus users should consider the factors that might drive local variability in their community as well as their research objectives when siting sensors for example if the user s intention is to provide data in an area where there are no nearby regulatory grade fem instruments users might consider siting sensors further apart to better understand how trends in the study area agree with regional trends whereas if users are interested in studying specific local sources high density networks covering smaller areas can help to meet this need 4 conclusion the work presented here describes and illustrates the value of enhancements to the airsensor package and dataviewer tool the first phase of this development resulted in version 0 5 which set the foundations for foss tools to support sensor work filling in the gap between data collection and data display feenstra et al 2020 the next phase resulting in version 1 0 has increased the functionality and made the tools more broadly accessibly with the publication of the package on cran the new soh metrics may be used to identify clear sensor failures such as days when the data from a sensor is incomplete or invalid the soh metrics may also be used to identify questionable sensor performance such as days when there is reduced agreement between the channel a and channel b data thus the tools and procedures enable users to systematically identify failing or poorly performing sensors and use replicable methods to determine the reliability of data throughout a deployment furthermore the availability of these standardized functions and qa qc algorithms through the airsensor package increases the accessibility of these procedures for communities and researchers and makes it easier for the methods to be shared replicated and discussed as r rstudio is a foss tool the soh metrics and qa qc algorithms can be customized and adapted as needed encouraging further development and exploration of these tools and concepts by users the analysis of sensor data presented here illustrates how the airsensor package may be used to quantify and ensure data quality as well as why this is critical in terms of using the data from large sensor networks this analysis suggests that for the pa ii sensors used in this study drift is not a major concern it is reasonable to compare data from sensors deployed at different times i e old versus new sensors and the sensors seem capable of providing useable data in the field for long time periods at least three years seasonal variability in sensor reliability and performance likely driven by seasonal trends in pm concentrations and compositions was found to be substantial and should be considered in the analysis and use of the data a limitation of this work is the focus on a single particulate matter sensor in addition the soh metrics and qa qc algorithms are based on this sensor s unique design i e the inclusion of duplicate sensors or channel a and channel b planned next steps for the airsensor package include adding functionality to support data from other sensors made by different manufacturers and adapting developing soh metrics and qa qc algorithms for other types of sensors this work will include developing functionalities for both other types of pm sensors and for gas phase pollutant sensors these next steps may also include adding functions to calculate data recovery apply sensor corrections and examine soh for a network of multiple sensors following these developments a similar analysis of the long term performance and reliability of other types of particulate matter sensors and gas phase sensors could be conducted as the deployment of large sensor networks continues tools like the airsensor package will enable users to better assess sensor state of health and thus improve the quality and reliability of data generated from these sensor networks data availability all the data used for this analysis is publicly available from either the data archive associated with the star grant work or from purpleair and the code used to conduct this analysis will enable access to the data and the replication of the work shared here funding information this publication was developed under assistance agreement no rd83618401 awarded by the u s environmental protection agency through the science to achieve results star program to south coast air quality management district it has not been formally reviewed by epa the views expressed in this document are solely those of the authors and do not necessarily reflect those of the epa south coast aqmd and epa do not endorse any products or commercial services mentioned in this publication declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors would like to thank dr jonathan callahan and hans martin at mazama science inc seattle wa for their collaboration and contributions in the development of the airsensor r package and the dataviewer application tools along with their valuable feedback on this manuscript the sensor data used and presented in this paper was collected by the air quality sensor performance evaluation center aq spec at south coast aqmd the authors would like to acknowledge the work of our co investigators at the university of california los angeles and at sonoma technology inc on the star grant project engage educate and empower california communities on the use and applications of low cost air monitoring sensors the authors would also like to thank the community groups leaders trainers coordinators and members sensor hosts that participated in the u s epa star grant and provided valuable feedback that allowed us to create and improve this work appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105256 
