index,text
22690,this work investigates the modelling accuracy of distinct reynolds averaged navier stokes rans equations and scale resolving simulation srs methods in the computation of the flow around a circular cylinder at reynolds number 3900 the study examines the dependence of the modelling and numerical accuracies on the physical resolution range of resolved scales and turbulence closure along with their consequences for the physics behind the results six mathematical models are evaluated through validation exercises rans supplemented with linear and non linear turbulent viscosity closures delayed detached eddy simulation improved delayed detached eddy simulation extra large eddy simulation and partially averaged navier stokes equations the results confirm the ability of srs models to significantly reduce the modelling error in comparison to linear rans closures nonetheless attaining an adequate numerical accuracy with srs models is clearly more demanding than with rans models it is also shown that non linear rans models lead to smaller comparison errors than traditional linear closures all results indicate that the modelling accuracy of a given mathematical model in simulation of the present flow is determined by its aptitude to represent the spatial development of the vortex shedding coherent structure keywords scale resolving simulation reynolds averaged navier stokes equation validation uncertainty quantification vortex shedding circular cylinder 1 introduction the flow around a circular cylinder is an important problem of fluid mechanics which has motivated an extensive number of experimental and numerical investigations for reynolds numbers r e of practical interest the flow exhibits a boundary layer free shear layer and wake the dynamics and interaction of these shear layers is complex and extremely sensitive to coherent structures and turbulence in fact two distinct flow regimes are attained according to the location of turbulent transition zdravkovich 1997 a sub critical or transition in the free shear layer tsl regime for r e between 3 5 10 2 and 2 0 10 5 and a regime where transition occurs in the boundary layer for r e 2 0 10 5 at the threshold of r e 2 0 10 5 transition starts moving to the boundary layer leading to a later flow separation and consequent reduction of the resistance forces a detailed review of this class of flow configurations is provided by zdravkovich 1997 and williamson 1996 all these features make the modelling and simulation of flows around circular cylinders challenging the most accurate mathematical model to simulate such flows is evidently to solve directly the navier stokes equations i e through a direct numerical simulation dns although this formulation would lead to the most accurate representation of the mean ensemble averaged and turbulent flow fields the numerical requisites to resolve all turbulent scales are still not feasible for applications at moderate to high reynolds numbers r e 10 5 davidson 2004 the reynolds averaged navier stokes rans equations are an alternative technique to simulate such turbulent flows instead of resolving the turbulent field this mathematical model represents the effect of turbulence on the resolved velocity field mean flow through a constitutive relation named turbulence closure although this strategy reduces significantly the numerical requirements of the model most of the available rans closures were not developed to handle flows where the coherent or periodic component of the mean flow hussain and reynolds 1970 is relevant for this reason traditional linear turbulent viscosity rans closures tend to attain modest performances on such type of flows see for instance pereira et al 2018a 2019 or rosetti et al 2012 between these two distinct modelling philosophies lies the large eddy simulation les by setting the physical resolution or length scale cut off so that only the largest turbulent scales are resolved les reduces the computational requisites of dns while taking advantage of the isotropic behaviour of the smallest scales to improve the modelling accuracy yet this approach is still excessively demanding for most wall bounded flows of practical interest davidson 2004 spalart 2000 since it requires resolving a minimum of 80 of the turbulence kinetic energy pope 2000 none of the previous techniques is therefore optimal for most practical applications owing to either insufficient modelling accuracy or excessive numerical demands there are three possible approaches to overcome these issues i increase the physical resolution i e resolve larger fractions of the turbulent field ii improve the turbulence closures accuracy or iii combine the previous ideas however the improvement of the modelling accuracy of rans closures is difficult owing to the complexity of developing general and robust formulations to represent the entire turbulent field as a result and considering the increase of computational power observed in the past decades the approach relying on the increase of the physical resolution has been preferred for this reason and in order to take advantage of the first approach without the numerical demands of les or dns spalart et al 1997 have proposed a new modelling strategy based on the idea of speziale speziale et al 1997 of combining dns les and rans this class of mathematical model named hybrid method combines rans in boundary layers with a scale resolving simulation srs formulation in outer and detached regions this idea led to the development of several hybrid formulations such as detached eddy simulation des spalart et al 1997 delayed detached eddy simulation ddes spalart et al 2006 or extra large eddy simulation xles kok et al 2004 additionally multiple bridging models have been proposed in the last decade although sharing the same aim of hybrid models bridging models employ the same turbulence model in the entire computational domain whether or not the filter is constant in space and time partially averaged navier stokes pans equations girimaji 2005 and partially integrated transport model pitm chaouat and schiestel schiestel and dejoan 2005 are examples of this modelling strategy naturally hybrid and bridging methods are dependent on the physical resolution and on the quality of the rans based closure on the turbulence modelling side several approaches have been developed to improve the turbulence closures accuracy by exploiting for instance the non linearity between the reynolds stress tensor and the product of the turbulent viscosity and the strain rate tensor or the anisotropy of the turbulent field the lag model olsen and coakley 2001 v 2 f model parneix et al 1998 explicit algebraic reynolds stress model earsm wallin and johansson 2000 dol et al 2002 and reynolds stress model rsm launder et al 1975 speziale et al 1991 eisfeld 2004 wilcox 2006 hanjalić and launder 2011 are examples of such techniques the present work examines six of these rans and srs models in simulation of the flow around a circular cylinder at reynolds number 3900 note that despite the low reynolds number the combination of the complex flow physics availability of precise experimental measurements and lower numerical requisites enables the comprehensive study of turbulence models making this problem a benchmark case for turbulence research this flow has been extensively studied by experimental ong and wallace 1996 parnaudeau et al 2008 and numerical beaudan and moin 1994 breuer 1998 kravchenko and moin 2000 lysenko et al 2012 lehmkuhl et al 2013 palkin et al 2015 d alessandro et al 2016 methods a detailed numerical overview is provided in pereira et al 2018a the aim of the investigation is i evaluate the modelling accuracy of the formulations ii examine the dependence of the modelling and numerical accuracies on the physical resolution and turbulence closure and iii interpret the physics of the results the latter step is accomplished by considering the findings of pereira et al 2018b 2019 these show that the modelling accuracy of the predictions tsl regime is critically dependent on the ability of the mathematical model to represent the spatial development of the vortex shedding coherent structure the selected mathematical models are the following rans supplemented by the linear shear stress transport sst menter et al 2003 and by the earsm dol et al 2002 turbulence closures ddes gritskevich et al 2012 improved ddes iddes gritskevich et al 2012 xles kok et al 2004 and pans girimaji 2005 pereira et al 2015a the modelling accuracy of these formulations is evaluated through validation exercises that compare functional and local flow quantities with the experimental measurements of parnaudeau et al 2008 norberg 2002 2003 and son and hanratty 1969 the quantities of interest range from practical functional quantities such as the strouhal number s t or the time averaged drag coefficient c d to the time averaged pressure distribution on the cylinder s surface c p θ and the velocity field in the near wake moreover the validation uncertainty is estimated by performing grid space and time refinement studies to estimate the numerical uncertainty verification and using the available experimental uncertainties parnaudeau et al 2008 norberg 2002 2003 all calculations are carried out with the finite volume solver refresco refresco 2018 the remainder of this article is structured as follows section 2 describes the main features of the selected mathematical models whereas section 3 introduces the problem setup and the numerical techniques used to execute the validation exercises the results are then presented and discussed in section 4 the paper presents the summary of the major findings in section 5 2 mathematical models consider the existence of an arbitrary filter implicit or explicit constant preserving and commuting with the spatial and temporal differentiation the application of such filtering operator that decomposes any dependent quantity φ into a resolved φ and a modelled ϕ component 1 φ φ ϕ to the continuity and momentum conservation equations leads to 2 v i x i 0 3 d v i d t 1 ρ p x i x j ν v i x j v j x i 1 ρ τ i j v i v j x j in the previous equations it is assumed incompressible and one phase flow x i are the coordinates of a cartesian coordinate system v i are the cartesian velocity components p is the pressure ρ is the fluid density ν is the kinematic viscosity and τ i j v i v j is a tensor that represents the effect of the unresolved scales on the resolved velocity field in order to close this system of equations τ i j v i v j needs to be modelled this is accomplished here through the boussinesq hypothesis which leads to the concept of turbulent viscosity ν t 4 τ i j v i v j ρ 2 ν t s i j 2 3 k δ i j where s i j is the resolved strain rate tensor k is the modelled turbulence kinetic energy and δ i j is the kronecker symbol an additional term a i j e x k may be added to equation 4 in order to consider the effects of the anisotropy of the turbulent field on the mean flow field this is the idea behind an earsm although the srs models tested in this work have distinct frameworks their numerical implementation is quite similar due to the use of implicit filtering consequently their main difference lies in the definition of ν t turbulence closure the main features of the selected models are summarized in sections 2 1 to 2 5 their complete description is given in menter et al 2003 dol et al 2002 gritskevich et al 2012 and pereira et al 2015a section 2 6 presents general considerations about rans and srs models 2 1 reynolds averaged navier stokes equations in this mathematical model the dependent variables are the ensemble averaged velocity components and pressure furthermore τ i j v i v j corresponds to the reynolds stress tensor which is modelled through a turbulence closure two distinct rans closures are assessed in this work the k ω sst and the k ω tnt earsm throughout this article these models are designated by sst and earsm respectively 2 1 1 k ω sst model the k ω sst closure menter et al 2003 is a linear turbulent viscosity model blending the k ω and k ε models in order to combine their main advantages it also accounts for the effect of the principal shear stress transport this closure relies on two transport equations to calculate the modelled turbulence kinetic energy k and specific dissipation ω 5 d k d t p k β ω k x j ν ν t σ k k x j 6 d ω d t α ν t p k β ω 2 x j ν ν t σ ω ω x j 2 1 f 1 σ ω 2 ω k x j ω x j and defines the turbulent viscosity as 7 ν t a 1 k max a 1 ω s f 2 where s stands for the mean flow ensemble averaged or resolved strain rate magnitude p k for the production of turbulence kinetic energy p k ν t s 2 and a 1 α β β σ k σ ω σ ω 2 f 1 and f 2 are coefficients and auxiliary functions given in menter et al 2003 2 1 2 k ω tnt explicit algebraic reynolds stress model dol et al 2002 have used the framework proposed by wallin and johansson to enable the k ω tnt model kok 2000 to consider the anisotropy of the turbulent field in this closure a corrective extra anisotropy tensor a i j e x 8 a i j e x β 3 ω i k ω k j i i ω δ i j 3 β 4 s i k ω k j ω i k s k j β 6 s i k ω k l ω l j ω i k ω k l s l j β 6 i i ω s i j 2 i v δ i j 3 β 9 ω i k s k l ω l m ω m j β 9 ω i k ω k l s l m ω m j is added to equation 4 9 τ i j v i v j ρ 2 ν t s i j 2 3 k δ i j a i j e x k since this earsm closure relies on the k ω tnt model kok 2000 equations 6 and 7 are replaced by 10 d ω d t α ν t p k β ω 2 x j ν ν t σ ω ω x j 0 5 ω max k x j ω x j 0 0 and 11 ν t k ω in equation 8 ω i j is the mean flow vorticity tensor i i ω and i v are mean flow velocity invariants 12 i i ω ω k l ω l k 13 i v s k l ω l m ω m k β i are model coefficients and the superscript indicates that the quantity is normalized by 1 β ω dol et al 2002 2 2 delayed detached eddy simulation ddes gritskevich et al 2012 combines rans in boundary layers with a srs model in outer and detached regions towards this end an explicit dependence on the local spatial grid resolution is added in equation 5 in order to reduce the magnitude of k this feature enables the model to resolve the turbulent field in outer and detached turbulent flow regions therefore a turbulent length scale l t 14 l t l rans f d max l rans l les 0 0 is introduced in the dissipation term of the k transport equation 15 d k d t p k k 3 l t x j ν ν t σ k k x j in equation 14 l rans is the rans length scale l rans k β ω l les is the les length scale l les c des δ c des is a constant and δ is the largest cell characteristic length and f d is an empirical delay function 0 f d 1 defined as 16 f d 1 tanh c d 1 ν t ν κ 2 d 2 0 5 s 2 ω 2 c d 2 where ω is the resolved vorticity tensor magnitude κ is the von kármán constant d is the wall distance and c d 1 and c d 2 are coefficients given in gritskevich et al 2012 2 3 improved delayed detached eddy simulation iddes gritskevich et al 2012 is a slightly modified version of ddes contrary to ddes this model allows the use of a srs model inside the boundary layer therefore l t is defined as 17 l t l rans f d 1 f d l les where l les is the les length scale 18 l les c iddes min c w max d δ δ and f d is an empirical delay function 0 f d 1 given by 19 f d max f d t f b 20 f d t tanh c d t 1 ν t κ 2 d 2 0 5 s 2 ω 2 c d t 2 21 f b min 2 0 exp 9 0 0 25 d δ 2 1 0 in the previous equations c iddes c w c d t 1 and c d t 2 are coefficients given in gritskevich et al 2012 it is important to note that in the limit of δ the empirical delay function f d does not guarantee l t bounded by l rans for this reason and contrary to ddes this method does not necessary recover a rans behaviour in outer and detached regions in cases where the spatial resolution is insufficient to an adequate activation of the srs model 2 4 extra large eddy simulation xles kok et al 2004 is similar to the first generation of des however when in srs mode it uses a les model to determine the turbulent viscosity instead of a rans based closure in this manner it combines the rans k ω tnt closure kok 2000 with the les k sub grid scale model towards this end the turbulent length scale is defined as 22 l t min k ω c 1 δ and the turbulent viscosity by 23 ν t l t k in equation 22 c 1 is a constant given in kok et al 2 5 partially averaged navier stokes equations this mathematical model relies on two parameters to define the magnitude of the turbulence quantities being modelled f ϕ ϕ φ the pans closure used in this investigation is based on the k ω sst closure described in section 2 1 1 here we use the version proposed by pereira et al 2015a hence the parameters f k and f ω are included in equations 5 and 6 24 d k d t p k β ω k x j ν ν t σ k f ω f k k x j 25 d ω d t α ν t p k p p f ω β ω f ω ω x j ν ν t σ ω f ω f k ω x j 2 σ ω 2 ω f ω f k 1 f 1 k x j ω x j where p α β k ν t the present derivation assumes a negligible contribution of the resolved turbulent field φ on the turbulent transport of the modelled field ϕ this hypothesis is named zero transport model girimaji 2005 in this work we use constant values for f k and f ε f ω f ε f k and so the model has no explicit dependence on the spatial or temporal resolution two f k values are used in this study 0 25 similar to les and 0 50 on the other hand turbulent dissipation is assumed to occur in the unresolved turbulent scales and so f ε is set equal to 1 00 pereira et al 2015a 2 6 general considerations although the numerical implementation of the former mathematical models exhibit several similarities these models rely on distinct ideas therefore it is worth pointing out some of their relevant features the selected hybrid models ddes iddes and xles have an explicit dependence on the spatial resolution unless l t is imposed the physical resolution range of resolved scales of the model varies upon spatial grid refinement therefore modelling and numerical errors are inevitably entangled and so small differences between simulations and experiments may be a consequence of error cancelling this limits the estimation of numerical errors based on grid refinement studies contrary to these hybrid models the bridging method pans using constant values of f ϕ enables the distinction between numerical and modelling errors since its governing equations have no explicit dependence on the grid resolution yet ensuring that the spatial and temporal resolutions are sufficient for the intended f ϕ might be demanding for practical applications the former property of pans also prevents commutation errors avoiding the well recognized issues at rans srs interfaces naturally the former mathematical models lead to different physical resolutions therefore unless the results of all models are a posteriori filtered a precise comparison of these models with experimental measurements should only rely on time averaged quantities nonetheless the root mean square of the lift coefficient c l is also compared in this work due to its importance for practical applications hence the results of c l should be interpreted carefully 3 problem setup and numerical techniques 3 1 case study computational domain and boundary conditions the selected case study is the flow around a smooth circular cylinder at reynolds number 3900 the prescribed reynolds number is based on the cylinder diameter d and undisturbed incoming velocity v its magnitude places this flow in the sub critical regime zdravkovich 1997 characterized by a laminar boundary layer flow separation and free shear layer turbulence onset in the free shear layer and turbulent wake the flow in this regime also entails two major coherent structures the kelvin helmholtz rollers and the vortex shedding this flow problem has been experimentally investigated by ong and wallace 1996 and parnaudeau et al 2008 the measurements of the latter study are used as reference in the present validation exercises although at slightly different reynolds numbers and apparatus the experiments of norberg 2002 2003 and son and hanratty 1969 are also used as reference for the predicted time averaged separation angle θ s son and hanratty 1969 and the pressure coefficient on the cylinder s surface and resultant functional quantities c p θ norberg 2002 2003 these experiments are summarized in pereira et al 2018a the numerical simulations are conducted on a prismatic rectangular domain defined in a cartesian coordinate system x i centred at the cylinder s axis fig 1 the computational domain and boundary conditions prescribed in the present work are the following the velocity and turbulence quantities are set constant at the inlet boundary x 1 d 10 and the pressure is extrapolated from the interior of the domain the turbulence quantities result from a turbulence intensity equal to that measured in the experiments of parnaudeau et al 2008 i 0 2 and a ratio between turbulent and molecular kinematic viscosities of ν t ν 10 3 1 1 based on the authors experience ν t ν is selected to obtain a dimensionless ω of approximately 20 the order of magnitude of this value is similar to that recommended in menter 1994 on the other hand the streamwise derivatives of all dependent variables are zero at the outlet boundary x 1 d 40 at the top and bottom boundaries x 2 d 12 the pressure is imposed and the transverse derivatives of all the remainder dependent variables are set equal to zero symmetry conditions are applied at the spanwise boundaries x 3 d 0 and x 3 d 3 this option is discussed in pereira et al 2015b and stems from the fact that neither cyclic nor symmetry conditions are optimal to handle the spanwise wavelengths of the structures resulting from the coherent field no slip and impermeability boundary conditions are specified on the cylinder s surface furthermore the normal pressure derivative and the modelled turbulence kinetic energy are set equal to zero whereas the modelled specific dissipation is defined at the near wall cell centre from the analytical near wall solution of the ω equation wilcox 2006 ω 80 ν d 2 ω 80 ν d 2 f ω for pans 3 2 numerical settings the numerical simulations are performed in four spatial grid resolutions h i these grids range from 1 0 to 4 5 10 6 cells n c leading to a refinement ratio r n c 1 n c i 1 3 of 1 64 however due to the inherent cost the finest spatial grid resolution h 1 is only used with formulations that enable the separate assessment of numerical and modelling errors sst earsm and pans the temporal resolution is linked to the spatial resolution so that similar temporal and spatial refinement ratios are used therefore the dimensionless time step δ t v d varies from 8 526 to 5 209 10 3 time units as a result the time averaged maximum courant number c and characteristic cell length using wall coordinates on the tangential x t normal x n and spanwise x s directions are smaller than 2 7 1 8 0 38 and 38 9 based on half cell length in each direction and for the coarsest grid resolution g 4 for the sake of simplicity throughout this work the combination of the spatial and temporal resolutions h i and t i is denoted as grid g i due to the fact that the formal orders of spatial p s and temporal p t grid convergence are equal and so n c 1 1 3 n c i 1 3 p s δ t i δ t 1 p t the computational domain and the spatial grid resolution h 2 are depicted in fig 1 whereas the properties of the four grids are summarized in table 1 all the numerical simulations start from the velocity pressure and turbulence fields of an initial rans simulation of 200 time units then the calculations run for 500 dimensionless time units being the statistics calculated with at least the last 400 time units approximately eighty shedding cycles in order to minimize round off and iterative errors the numerical simulations are performed in double precision and the iterative convergence criterion c i t requires a maximum normalized residual of 10 5 for all dependent variables normalized residuals are equivalent to dimensionless variables differences in a simple jacobi iteration the discretization of the governing equations is performed with spatial and temporal second order accurate schemes p 2 0 including the convective terms of the turbulence quantities transport equations 3 3 refresco solver refresco refresco 2018 is a community based open usage cfd code for maritime applications it solves multiphase incompressible viscous flows using the continuity and navier stokes equations filtered complemented with turbulence closures the equations are discretized using a finite volume approach with cell centred collocated variables in strong conservation form and a pressure correction equation based on the simple algorithm is used to ensure mass conservation time integration is performed implicitly with second order backward schemes at each implicit time step the non linear system for velocity and pressure is linearised with picard s method and either a segregated or coupled approach can be used a segregated approach is adopted here for the solution of all transport equations the implementation is face based which permits grids with elements consisting of an arbitrary number of faces for turbulent flows rans sas des xles pans and les approaches can be used pereira et al 2015a 2015c the code is parallelized using mpi and subdomain decomposition and runs on linux workstations and hpc clusters the code is currently being developed verified and tested at marin the netherlands in collaboration with various universities refresco 2018 3 4 estimation of modelling errors the accuracy of the mathematical models described in section 2 is evaluated through the quantification of the modelling error e m ϕ i this exercise relies on the verification and validation procedures proposed by the american society of mechanical engineers asme 2009 the interval containing e m ϕ i for 95 out of 100 cases is given by 26 e c ϕ i u v ϕ i e m ϕ i e c ϕ i u v ϕ i where e c ϕ i is the comparison error defined as the difference between the numerical ϕ i and the experimental ϕ e measurement 27 e c ϕ i ϕ i ϕ e and u v ϕ i is the validation uncertainty determined as 28 u v ϕ i u i ϕ i 2 u n ϕ i 2 u e ϕ e 2 in equation 28 u i ϕ i u n ϕ i and u e ϕ e are the input numerical and experimental uncertainties respectively u i ϕ i is a consequence of the uncertainty in the quantification of several input parameters of the experiments fluid temperature turbulence intensity at the inlet of the experimental facility etc the present exercise assumes u i ϕ i 0 on the other hand u n ϕ i and u e ϕ e have their origin in the techniques used to perform the numerical and experimental measurements whereas u e ϕ e is taken from the experimental studies used in this investigation parnaudeau et al 2008 norbert 2002 2003 u n ϕ i is estimated by simply assuming that its value is equal to the estimated discretization error u n ϕ i e d ϕ i the reasoning behind this simple assumption is the fact that the use of the procedures recommended by the american society of mechanical engineers asme 2009 or eça and hoekstra 2014 would require at least three sufficiently refined grids which is excessively demanding for the available computational resources see appendix a furthermore e d ϕ i is estimated using a power series expansion where the spatial p s and temporal p t orders of convergence are imposed equal to the formal value of the selected discretization techniques p 2 0 therefore 29 e d ϕ i ϕ i 1 ϕ 1 r i 1 2 1 r i 2 where ϕ o is the estimate of the exact solution we are aware that such approach may lead to non conservative estimates of the numerical uncertainty however for the level of grid refinement available the application of procedures that rely on the observed order of spatial temporal convergence may lead to overly conservative estimates of the numerical uncertainty due to the use of data from excessively coarse grids therefore such shortcoming of the uncertainty estimation procedure would obscure the comparison of the models tested unless stated otherwise the estimated e d ϕ and u v ϕ shown in this work use the finest resolution available nonetheless the complete set of results for each mathematical model is available in appendix a 4 results this section presents the results of this investigation where the modelling accuracy of the selected mathematical models is examined in the simulation of the flow around a circular cylinder at r e 3900 towards this end section 4 1 starts by analysing some relevant features of the various rans and srs models afterwards section 4 2 discusses the outcome of the validation exercises performed with the different mathematical models whereas section 4 3 examines the physical rationale behind the results all quantities shown in this section are normalized using v d and ρ as reference quantities it is also important to mention that the time averaged data presented in contour plots considers the entire simulated data nonetheless the influence of the initial condition should not affect the qualitative conclusions taken based on these data on the contrary the statistics of integral and local profiles quantities discard the initial transient period up to 100 time units 4 1 features of the mathematical models as discussed in section 2 the selected mathematical models are based on distinct modelling concepts for this reason a correct comparison of these models requires the understanding of their main characteristics hence the influence of the extra reynolds stress anisotropy tensor in the earsm the extension and location of the srs regions in hybrid models and the physical resolution of each model are discussed below all these features are relevant to the discussion presented in sections 4 2 and 4 3 4 1 1 anisotropy of the turbulence field most turbulence closures used in practical applications are based on the boussinesq hypothesis in this manner the influence of the turbulent stresses on the mean flow field is assumed proportional to the product of the turbulent viscosity by the strain rate tensor equation 4 however this type of linear constitutive relation is usually only adequate for simple statistically steady flows or to represent the smallest turbulent scales the application of such class of closures is therefore questionable in cases where the flow exhibits for instance coherent structures or massive separation the idea of an earsm is to enhance the quality of rans closures by considering a non linear relation between the reynolds stress and the strain rate tensors towards this end an extra reynolds stress anisotropy tensor a i j e x is added to the boussinesq relation equation 9 the importance of this additional tensor to the simulation of the present flow problem is shown in fig 2 this picture presents the streamwise and transverse normal components of the time averaged ratio γ i j a i j e x k 2 ν t s i j 2 3 k obtained from an earsm solution the data indicate that according to this model the maximum values of γ i i reach 0 50 in the free shear layer although not shown here the remainder γ i j components also possess meaningful magnitudes for instance γ 12 achieves similar values to those shown in fig 2 with their maximum located in the x 2 d 0 0 region these results demonstrate that the estimated extra anisotropic components of τ i j are not negligible 4 1 2 rans and srs regions in hybrid models an important aspect in the application of hybrid models is the location and extension of the srs region although at the present r e the boundary layer is laminar the location of the rans srs interface will certainly affect the breakdown to high intensity turbulence in the free shear layer to analyse its effect in the predictions of hybrid methods discussed in section 4 2 fig 3 presents the regions where the srs mode is active l t l rans 0 999 at δ t v d 500 and the respective turbulent viscosity field as expected the plots demonstrate that iddes permits the activation of the srs mode in the boundary layers whereas in ddes such activation only occurs in the free shear layer compared to ddes xles applies the srs model closest to the cylinder s surface regarding the instantaneous ν t xles leads to the lowest magnitudes of this quantity while ddes and iddes exhibit comparable values of ν t similar trends are observed for the time averaged turbulent viscosity field 4 1 3 physical resolution the physical resolution determines the range of resolved scales which strongly influences the magnitude of the modelling error it is expected that resolving a wider range of scales enhances the modelling accuracy ideally the evaluation of the effective physical resolution of a given mathematical model should be accomplished through techniques such as energy spectra however considering the aim of this study and the complexity of applying such methods to the present flow we simply conduct a qualitative discussion by examining the instantaneous q criterion hunt et al 1988 field 30 q 1 2 ω i j ω i j s i j s i j predicted by the various models fig 4 depicts the iso surfaces of the q criterion q 0 1 0 2 and 0 5 at δ t v d 500 as expected the sst model does not exhibit a significant amount of flow structures since the turbulence field is in theory entirely modelled nonetheless it is clear that the mean flow is not bi dimensional the earsm on the other hand leads to a flow field significantly different from the sst in fact the results are very similar to pans with f k 0 50 suggesting that the reynolds stress tensor might be underpredicted by the earsm naturally the srs models exhibit flow structures that are not captured by rans furthermore the data indicate the existence of two groups of solutions ddes with iddes and pans f k 0 25 with xles overall the results demonstrate that the selected rans and srs methods possess distinct physical resolutions and the existence of similarities between some models sst earsm and pans f k 0 50 ddes and iddes and xles and pans f k 0 25 it is therefore expected that the magnitude of the modelling and numerical error follows the trends observed in fig 4 this point is illustrated in the next sections and in appendix a 4 2 mathematical models comparison this section discusses the modelling accuracy of the selected rans and srs models towards this end modelling and numerical errors are quantified for integral and local flow quantities although the computations exhibit discretization errors that are not negligible see appendix a their magnitude enables the comparison of the models performance for most of the selected quantities all the results presented in this section use the finest grid resolution common to all formulations g 2 the exceptions are the quantities for which the validation uncertainties are estimated in those cases the results use the finest grid spatial and temporal resolution available fig 5 and table 2 present the comparison error e c ϕ and validation uncertainty u v ϕ in the prediction of several quantities of practical interest these are the root mean square lift coefficient c l strouhal number s t time averaged drag coefficient c d base pressure coefficient c p b recirculation length l r minimum streamwise velocity magnitude v 1 min at the centreline x 2 d 0 0 and separation angle θ s 2 2 validation uncertainties are not calculated for this quantity since the time averaged friction coefficient along the cylinder s surface c f θ necessary to calculate θ s was only computed for the finest grid for nearly all flow quantities the largest discrepancies with the experiments are obtained for the sst closure this is particularly visible in the prediction of c l 3 3 it should be mentioned that applying ensemble averaging to the experimental data will most likely reduce c l and so e c c l for the sst model should be even larger than the present result where the comparison error exceeds 590 the experimental observation the earsm on the other hand exhibits a significantly better prediction of these flow quantities for instance the comparison errors of the drag and lift coefficients are reduced to 1 7 and 195 7 respectively it is also remarkable the similarity between the results of earsm and pans f k 0 50 nonetheless the enhancement in modelling quality e c ϕ achieved by these models is coupled with a stronger dependence on the grid resolution this is visible in the magnitude of u v ϕ the best agreement with the experimental observations of the aforementioned flow quantities is obtained by the remainder srs methods ddes iddes xles and pans f k 0 25 it is also possible to identify a trend suggesting that xles and pans f k 0 25 attain the most accurate representation lowest e c ϕ of the selected quantities of interest this improvement entails a substantial growth of the necessary grid resolution to guarantee sufficiently small magnitudes of the numerical error 4 4 ideally the numerical error should be reduced to negligible levels when compared to the modelling error similar conclusions can be drawn for the pressure coefficient on the cylinder s surface c p θ fig 6 whereas pans f k 0 25 and xles computations obtain an excellent agreement with the experimental measurements the outcome of sst leads to the largest comparison errors although the remainder rans and srs methods attain larger comparison errors than xles or pans f k 0 25 these are substantially smaller than for the sst it is also interesting to observe the influence of the mathematical model on the thickness of the boundary layer upstream flow separation the results show that the sst earsm and pans at f k 0 50 models lead to thicker boundary layers lower magnitudes of c p fig 7 depicts the time averaged velocity magnitude v i streamwise variance v 1 v 1 and covariance v 1 v 2 profiles in the near wake the results confirm once again the previous trends there is a significant reduction in the comparison error of all flow quantities between sst and xles pans f k 0 25 computations and based on the magnitude of e c ϕ it is possible to identify four groups of simulations these are i sst i i earsm and pans f k 0 50 i i i ddes and iddes and i v xles and pans f k 0 25 it should be noted that the use of time averaged quantities at the inlet boundary is certainly delaying the turbulence onset in the free shear layer contributing to a larger recirculation region with the increase of the physical resolution this has been demonstrated and discussed in norberg 1987 or pereira et al 2018a fig 7 also shows that only srs models except pans at f k 0 50 can replicate the correct magnitude and shape of the v i profiles at different cross sections however this is not necessarily true for the specific stresses velocity variance and covariance for these quantities only pans f k 0 25 and xles simulations present a good agreement with the experiments for this grid resolution the former tendencies are also observed in the time averaged streamwise velocity field on the x 1 x 2 plane depicted in fig 8 in comparison to the experiments the length and shape of the recirculation region delimited by the white line v 1 0 are clearly underpredicted by the sst however although the use of earsm and pans f k 0 50 models reduces the comparison error only the remainder srs methods lead to a good representation of the streamwise velocity field finally the comparison error of the time averaged streamwise velocity field e c v 1 is shown in fig 9 the outcome indicates that the maximum comparison error in the predictions of this flow quantity is located in the free shear layers with its magnitude following the overall trend obtained for the remainder quantities whereas the comparison error in sst computations reaches 85 of the inflow velocity pans at f k 0 25 reduces its magnitude to small levels this result points out a strong relation between the predictions quality and the manner how the free shear layer is represented this point is now examined in the following section 4 3 physical interpretation of the results the dynamics of flows around circular cylinders in the regime where turbulent transition occurs in the free shear layer is governed by the kelvin helmholtz kh and vortex shedding vs coherent structures pereira et al 2018b have shown that the accuracy of a given mathematical model to predict problems in such regime depends on the ability to replicate the spatial development of the former coherent structures this comprises four major steps i onset of the kelvin helmholtz instability in the free shear layer ii spatial development of the kelvin helmholtz rollers iii breakdown to high intensity turbulence and iv turbulent free shear layer roll up and vortex shedding the investigation of pereira et al 2018b proposes two conditions to guarantee the precise prediction of the former phenomena i an effective computational reynolds number r e e 31 r e e v d ν ν t exceeding 1200 in the free shear layer region lowest reynolds number to experimentally observe the kelvin helmholtz rollers and ii a mean to unresolved field strain rate ratio s k ε lower than 6 to assure that only fully developed turbulence is modelled by the closure these findings and conditions are now used to explain the observed differences in modelling accuracy between the rans and srs methods analysed in section 4 2 the spatial development of the kelvin helmholtz instability and structure occurs along the free shear layers and determines the flow dynamics in this region hence the extension and shape of the free shear layers are expected to be closely dependent on the representation of such coherent structure to investigate this aspect fig 10 depicts the centreline of the free shear layers predicted by the rans and srs models these lines are estimated from the inflection point of the streamwise velocity field 2 v 1 x 2 2 from fig 10 it is clear that most formulations predict the onset of the free shear layer at approximately the same location excluding the sst θ s varies less than 3 4 upon model yet its spatial development varies with the mathematical model whereas the sst exhibits an early inward curving of the inflection lines and consequent underprediction of their length the differences between predictions and experiments are significantly reduced as the physical resolution see section 4 1 3 and the closure quality increase in this manner it is once again possible to identify four groups of models i sst ii earsm and pans f k 0 50 iii ddes and iddes and iv xles and pans f k 0 25 note that these groups coincide with those observed in section 4 2 confirming the relation between modelling accuracy and the dynamics of the free shear layer pereira et al 2018b it is also interesting to observe that the extension and shape of the free shear layer is also strongly affected by the grid resolution for pans at f k 0 50 earsm and sst not shown this result is a consequence of discretization errors or numerical diffusion and explains the large discrepancies between results in grids g 1 and g 2 see appendix a the outcome of fig 10 indicates that the evolution of the free shear layer is incorrectly simulated by the one point closures used in this work as illustrated in pereira et al 2018b this stems from the overprediction of turbulence in the free shear layer region where the coherent field dominates the flow dynamics to overcome this issue it is proposed that the physical resolution of the mathematical model should guarantee i r e e 1200 and ii s k ε 6 fig 11 depicts the effective computational reynolds number r e e in the x 1 x 2 plane for the analysed rans and srs models similar to the results of pereira et al 2018b the data indicate that the beginning of the inward curving of the inflection lines coincides with the point where the free shear layers experience an effective computational reynolds number inferior to 1200 recall that such reynolds number is the lower limit to experimentally observe the kelvin helmholtz instability and structure rajagopalan and antonia 2005 additionally fig 11 shows that the point where this event occurs moves downstream upon the enhancement of the physical resolution see section 4 1 3 whereas for sst the inflection line experiences r e e 1200 in the vicinity of the cylinder only xles and pans f k 0 25 guarantee r e e 1200 in the entire free shear layer regarding the remainder models and in comparison to the sst the data exhibit a significant increase of r e e which follows the modelling accuracy e c ϕ of the models the former trends suggest that the modelling accuracy of the mathematical models is related to the spatial development of the kelvin helmholtz instability and structure in the free shear layer this is confirmed in fig 12 where the frequency energy spectra in a point located in the free shear layer region is plotted at this location it is expected that the resultant spectra clearly exhibit the characteristic frequencies of the vortex shedding and kelvin helmholtz structures fig 12h the results show that while the sst only captures the vortex shedding frequency and its harmonics the remainder models are also able to replicate the kelvin helmholtz frequency the exception is the ddes simulation where is not possible to clearly identify such frequency it is only visible for the v 2 field nonetheless solely xles and pans f k 0 25 exhibit the characteristic relatively broad band of the kelvin helmholtz frequency see for instance rajagopalan and antonia 2005 or prasad and williamson 1997 this result indicates that the enhanced modelling accuracy of these models stems from their ability to precisely predict the spatial development of the kelvin helmholtz structure and consequently of the vortex shedding although ddes and iddes have the potential to achieve similar results they require further spatial grid refinement to increase the physical resolution as for the s k ε 6 criterion the results indicate that it is less strict than r e e 1200 and so the outcome is not shown here nonetheless this condition is fulfilled by all srs models tested except pans at f k 0 50 5 conclusions the present work has investigated the aptitude of various rans and srs methods to simulate the flow around a circular cylinder at r e 3900 the objectives of this study are threefold i evaluate the accuracy of selected formulations ii examine their dependence on the physical resolution and turbulence closure and iii interpret the physical rationale of the results six mathematical models are selected rans supplemented with the sst and earsm closures ddes iddes xles and pans f k 0 50 and 0 25 the modelling accuracy of these models is assessed through validation exercises the results are then interpreted using the findings and conditions proposed in pereira et al 2018b 2019 the major conclusions of the study are the following the required grid resolution to achieve an adequate magnitude of the numerical error varies upon physical resolution and closure as expected the results indicate that both the range of resolved scales and the necessary grid resolution increase with the physical resolution similarly advanced closure strategies such as the earsm require finer grid resolutions than traditional turbulent viscosity closures to achieve the same magnitude of the discretization error although not shown the increase of physical resolution also originates a growth in the unsteadiness and irregularity of the numerical solutions which causes larger variations between time steps as a result the number of iterations to achieve a given iterative convergence criterion increases in the case of pans the reduction of f k from 1 00 to 0 25 leads to an increase of 62 in the required averaged number of iterations per time step to attain c i t 10 5 the outcome of rans and srs methods for integral and local flow quantities indicates the existence of four groups of simulation in terms of their modelling accuracy these are i sst ii earsm and pans f k 0 50 iii ddes and iddes and iv xles and pans f k 0 25 whereas the results of the sst lead to large comparison errors which may reach 590 of the experimental value xles and pans f k 0 25 achieve the best agreement with the experiments by obtaining reduced comparison errors from a practical perspective the results also suggest that the use of the earsm rans may be an efficient option for some applications due to its compromise between modelling accuracy and numerical requirements the differences in modelling accuracy of the former groups are explained by the ability of the mathematical models to replicate the major flow features most notably the spatial development of vortex shedding coherent structure which entails the onset and growth of the kelvin helmholtz rollers it has been shown that only xles and pans f k 0 25 are capable to precisely replicate these phenomena the results also reaffirm the validity of the conditions proposed in pereira et al 2018b to guarantee the correct prediction of this flow regime these are an effective computational reynolds number r e e exceeding 1200 in the free shear layer and a mean to unresolved field strain rate ratio s k ε lower than 6 acknowledgements the authors would like to thank the maritime research institute netherlands and to the laboratory for advanced computing at university of coimbra for providing the hpc resources necessary to perform the present research moreover the authors are very grateful to c norberg l carlier and e lamballais for providing their experimental measurements appendix a influence of grid resolution the evaluation of the selected rans and srs methods was preceded by grid space and time refinement exercises to estimate the discretization error and consequently the validation uncertainty the predictions for integral and local flow quantities and respective estimates of discretization errors e d ϕ are presented in tables a 3 and a 4 and figures a 13 and a 14 the data shown in the former tables demonstrate a reduced dependence of the sst results from the grid resolutions employed in this study except for the recirculation length and minimum streamwise velocity all estimates of the discretization error are small earsm and srs models on the other hand increase substantially the relevance of the grid resolution the results obtained with these formulations also indicate that the two coarsest grids do not possess enough resolution to use the selected earsm and srs models similar trends are also observed for the time averaged pressure coefficient distribution on the cylinder s surface c p θ and the streamwise velocity v 1 profiles in the near wake depicted in figures a 13 and a 14 table a3 time averaged drag coefficient c d root mean square lift coefficient c l time averaged base pressure coefficient c p b strouhal number st recirculation length l r and minimum streamwise velocity v 1 min as a function of the grid resolution g i and mathematical model table a3 model g i c d c l c p b s t l r v 1 min sst 4 1 280 0 735 1 437 0 215 0 244 0 015 3 1 258 0 693 1 370 0 217 0 258 0 022 2 1 249 0 682 1 360 0 217 0 309 0 045 1 1 245 0 664 1 405 0 211 0 505 0 227 earsm 4 1 218 0 628 1 376 0 208 0 615 0 174 3 1 111 0 436 1 174 0 217 0 832 0 235 2 1 061 0 348 1 088 0 214 0 982 0 259 1 0 998 0 225 0 976 0 217 1 217 0 259 4 1 230 0 623 1 438 0 204 0 621 0 179 pans f k 0 50 3 1 182 0 535 1 302 0 208 0 714 0 209 2 1 135 0 451 1 221 0 209 0 828 0 245 1 1 036 0 284 1 050 0 214 1 124 0 273 ddes 4 1 229 0 637 1 390 0 208 0 625 0 166 3 1 071 0 322 1 095 0 208 1 025 0 284 2 1 008 0 238 1 003 0 209 1 289 0 233 iddes 4 1 170 0 511 1 278 0 200 0 821 0 219 3 1 077 0 337 1 097 0 204 1 031 0 266 2 0 993 0 181 0 966 0 209 1 347 0 282 xles 4 1 094 0 407 1 150 0 204 1 004 0 177 3 0 976 0 170 0 950 0 206 1 428 0 251 2 0 954 0 127 0 906 0 212 1 547 0 257 4 1 103 0 380 1 157 0 200 0 985 0 267 pans f k 0 25 3 0 996 0 231 0 976 0 208 1 378 0 235 2 0 938 0 112 0 882 0 212 1 630 0 264 1 0 927 0 095 0 864 0 208 1 728 0 272 table a4 estimated discretizarion error maximum p 1 00 and minimum p 2 0 for the time averaged drag coefficient c d root mean square lift coefficient c l time averaged base pressure coefficient c p b strouhal number s t recirculation length l r and minimum streamwise velocity v 1 min as a function of the mathematical model results for grid resolutions g 1 and g 2 table a4 model ϕ c d c l c p b s t l r v 1 min sst e d ϕ 2 max 0 025 0 033 0 027 0 001 0 146 0 065 e d ϕ 2 min 0 011 0 014 0 015 0 000 0 062 0 028 e d ϕ 1 max 0 024 0 118 0 306 0 042 1 610 1 239 e d ϕ 1 min 0 011 0 055 0 143 0 019 0 750 0 577 earsm e d ϕ 2 max 0 142 0 249 0 244 0 006 0 425 0 068 e d ϕ 2 min 0 060 0 106 0 104 0 003 0 181 0 029 e d ϕ 1 max 0 429 0 836 0 762 0 015 1 602 0 004 e d ϕ 1 min 0 200 0 389 0 355 0 007 0 746 0 002 e d ϕ 2 max 0 132 0 239 0 231 0 004 0 323 0 103 pans f k 0 50 e d ϕ 2 min 0 056 0 102 0 098 0 002 0 137 0 044 e d ϕ 1 max 0 678 1 133 1 165 0 030 2 009 0 188 e d ϕ 1 min 0 316 0 528 0 543 0 014 0 935 0 087 ddes e d ϕ 2 max 0 176 0 238 0 263 0 004 0 749 0 145 e d ϕ 2 min 0 075 0 101 0 112 0 002 0 318 0 062 iddes e d ϕ 2 max 0 238 0 444 0 370 0 016 0 897 0 043 e d ϕ 2 min 0 101 0 189 0 157 0 007 0 381 0 018 xles e d ϕ 2 max 0 062 0 122 0 125 0 018 0 253 0 016 e d ϕ 2 min 0 026 0 052 0 053 0 007 0 143 0 007 e d ϕ 2 max 0 165 0 338 0 268 0 011 0 715 0 082 pans f k 0 25 e d ϕ 2 min 0 070 0 144 0 114 0 005 0 304 0 035 e d ϕ 1 max 0 079 0 115 0 118 0 027 0 655 0 057 e d ϕ 1 min 0 037 0 054 0 055 0 013 0 305 0 026 fig a13 time averaged pressure coefficient distribution on the cylinder s surface c p θ as a function of the grid resolution g i and mathematical model fig a13 fig a14 time averaged streamwise velocity magnitude v 1 profiles in the near wake as a function of the grid resolution g i and mathematical model fig a14 
22690,this work investigates the modelling accuracy of distinct reynolds averaged navier stokes rans equations and scale resolving simulation srs methods in the computation of the flow around a circular cylinder at reynolds number 3900 the study examines the dependence of the modelling and numerical accuracies on the physical resolution range of resolved scales and turbulence closure along with their consequences for the physics behind the results six mathematical models are evaluated through validation exercises rans supplemented with linear and non linear turbulent viscosity closures delayed detached eddy simulation improved delayed detached eddy simulation extra large eddy simulation and partially averaged navier stokes equations the results confirm the ability of srs models to significantly reduce the modelling error in comparison to linear rans closures nonetheless attaining an adequate numerical accuracy with srs models is clearly more demanding than with rans models it is also shown that non linear rans models lead to smaller comparison errors than traditional linear closures all results indicate that the modelling accuracy of a given mathematical model in simulation of the present flow is determined by its aptitude to represent the spatial development of the vortex shedding coherent structure keywords scale resolving simulation reynolds averaged navier stokes equation validation uncertainty quantification vortex shedding circular cylinder 1 introduction the flow around a circular cylinder is an important problem of fluid mechanics which has motivated an extensive number of experimental and numerical investigations for reynolds numbers r e of practical interest the flow exhibits a boundary layer free shear layer and wake the dynamics and interaction of these shear layers is complex and extremely sensitive to coherent structures and turbulence in fact two distinct flow regimes are attained according to the location of turbulent transition zdravkovich 1997 a sub critical or transition in the free shear layer tsl regime for r e between 3 5 10 2 and 2 0 10 5 and a regime where transition occurs in the boundary layer for r e 2 0 10 5 at the threshold of r e 2 0 10 5 transition starts moving to the boundary layer leading to a later flow separation and consequent reduction of the resistance forces a detailed review of this class of flow configurations is provided by zdravkovich 1997 and williamson 1996 all these features make the modelling and simulation of flows around circular cylinders challenging the most accurate mathematical model to simulate such flows is evidently to solve directly the navier stokes equations i e through a direct numerical simulation dns although this formulation would lead to the most accurate representation of the mean ensemble averaged and turbulent flow fields the numerical requisites to resolve all turbulent scales are still not feasible for applications at moderate to high reynolds numbers r e 10 5 davidson 2004 the reynolds averaged navier stokes rans equations are an alternative technique to simulate such turbulent flows instead of resolving the turbulent field this mathematical model represents the effect of turbulence on the resolved velocity field mean flow through a constitutive relation named turbulence closure although this strategy reduces significantly the numerical requirements of the model most of the available rans closures were not developed to handle flows where the coherent or periodic component of the mean flow hussain and reynolds 1970 is relevant for this reason traditional linear turbulent viscosity rans closures tend to attain modest performances on such type of flows see for instance pereira et al 2018a 2019 or rosetti et al 2012 between these two distinct modelling philosophies lies the large eddy simulation les by setting the physical resolution or length scale cut off so that only the largest turbulent scales are resolved les reduces the computational requisites of dns while taking advantage of the isotropic behaviour of the smallest scales to improve the modelling accuracy yet this approach is still excessively demanding for most wall bounded flows of practical interest davidson 2004 spalart 2000 since it requires resolving a minimum of 80 of the turbulence kinetic energy pope 2000 none of the previous techniques is therefore optimal for most practical applications owing to either insufficient modelling accuracy or excessive numerical demands there are three possible approaches to overcome these issues i increase the physical resolution i e resolve larger fractions of the turbulent field ii improve the turbulence closures accuracy or iii combine the previous ideas however the improvement of the modelling accuracy of rans closures is difficult owing to the complexity of developing general and robust formulations to represent the entire turbulent field as a result and considering the increase of computational power observed in the past decades the approach relying on the increase of the physical resolution has been preferred for this reason and in order to take advantage of the first approach without the numerical demands of les or dns spalart et al 1997 have proposed a new modelling strategy based on the idea of speziale speziale et al 1997 of combining dns les and rans this class of mathematical model named hybrid method combines rans in boundary layers with a scale resolving simulation srs formulation in outer and detached regions this idea led to the development of several hybrid formulations such as detached eddy simulation des spalart et al 1997 delayed detached eddy simulation ddes spalart et al 2006 or extra large eddy simulation xles kok et al 2004 additionally multiple bridging models have been proposed in the last decade although sharing the same aim of hybrid models bridging models employ the same turbulence model in the entire computational domain whether or not the filter is constant in space and time partially averaged navier stokes pans equations girimaji 2005 and partially integrated transport model pitm chaouat and schiestel schiestel and dejoan 2005 are examples of this modelling strategy naturally hybrid and bridging methods are dependent on the physical resolution and on the quality of the rans based closure on the turbulence modelling side several approaches have been developed to improve the turbulence closures accuracy by exploiting for instance the non linearity between the reynolds stress tensor and the product of the turbulent viscosity and the strain rate tensor or the anisotropy of the turbulent field the lag model olsen and coakley 2001 v 2 f model parneix et al 1998 explicit algebraic reynolds stress model earsm wallin and johansson 2000 dol et al 2002 and reynolds stress model rsm launder et al 1975 speziale et al 1991 eisfeld 2004 wilcox 2006 hanjalić and launder 2011 are examples of such techniques the present work examines six of these rans and srs models in simulation of the flow around a circular cylinder at reynolds number 3900 note that despite the low reynolds number the combination of the complex flow physics availability of precise experimental measurements and lower numerical requisites enables the comprehensive study of turbulence models making this problem a benchmark case for turbulence research this flow has been extensively studied by experimental ong and wallace 1996 parnaudeau et al 2008 and numerical beaudan and moin 1994 breuer 1998 kravchenko and moin 2000 lysenko et al 2012 lehmkuhl et al 2013 palkin et al 2015 d alessandro et al 2016 methods a detailed numerical overview is provided in pereira et al 2018a the aim of the investigation is i evaluate the modelling accuracy of the formulations ii examine the dependence of the modelling and numerical accuracies on the physical resolution and turbulence closure and iii interpret the physics of the results the latter step is accomplished by considering the findings of pereira et al 2018b 2019 these show that the modelling accuracy of the predictions tsl regime is critically dependent on the ability of the mathematical model to represent the spatial development of the vortex shedding coherent structure the selected mathematical models are the following rans supplemented by the linear shear stress transport sst menter et al 2003 and by the earsm dol et al 2002 turbulence closures ddes gritskevich et al 2012 improved ddes iddes gritskevich et al 2012 xles kok et al 2004 and pans girimaji 2005 pereira et al 2015a the modelling accuracy of these formulations is evaluated through validation exercises that compare functional and local flow quantities with the experimental measurements of parnaudeau et al 2008 norberg 2002 2003 and son and hanratty 1969 the quantities of interest range from practical functional quantities such as the strouhal number s t or the time averaged drag coefficient c d to the time averaged pressure distribution on the cylinder s surface c p θ and the velocity field in the near wake moreover the validation uncertainty is estimated by performing grid space and time refinement studies to estimate the numerical uncertainty verification and using the available experimental uncertainties parnaudeau et al 2008 norberg 2002 2003 all calculations are carried out with the finite volume solver refresco refresco 2018 the remainder of this article is structured as follows section 2 describes the main features of the selected mathematical models whereas section 3 introduces the problem setup and the numerical techniques used to execute the validation exercises the results are then presented and discussed in section 4 the paper presents the summary of the major findings in section 5 2 mathematical models consider the existence of an arbitrary filter implicit or explicit constant preserving and commuting with the spatial and temporal differentiation the application of such filtering operator that decomposes any dependent quantity φ into a resolved φ and a modelled ϕ component 1 φ φ ϕ to the continuity and momentum conservation equations leads to 2 v i x i 0 3 d v i d t 1 ρ p x i x j ν v i x j v j x i 1 ρ τ i j v i v j x j in the previous equations it is assumed incompressible and one phase flow x i are the coordinates of a cartesian coordinate system v i are the cartesian velocity components p is the pressure ρ is the fluid density ν is the kinematic viscosity and τ i j v i v j is a tensor that represents the effect of the unresolved scales on the resolved velocity field in order to close this system of equations τ i j v i v j needs to be modelled this is accomplished here through the boussinesq hypothesis which leads to the concept of turbulent viscosity ν t 4 τ i j v i v j ρ 2 ν t s i j 2 3 k δ i j where s i j is the resolved strain rate tensor k is the modelled turbulence kinetic energy and δ i j is the kronecker symbol an additional term a i j e x k may be added to equation 4 in order to consider the effects of the anisotropy of the turbulent field on the mean flow field this is the idea behind an earsm although the srs models tested in this work have distinct frameworks their numerical implementation is quite similar due to the use of implicit filtering consequently their main difference lies in the definition of ν t turbulence closure the main features of the selected models are summarized in sections 2 1 to 2 5 their complete description is given in menter et al 2003 dol et al 2002 gritskevich et al 2012 and pereira et al 2015a section 2 6 presents general considerations about rans and srs models 2 1 reynolds averaged navier stokes equations in this mathematical model the dependent variables are the ensemble averaged velocity components and pressure furthermore τ i j v i v j corresponds to the reynolds stress tensor which is modelled through a turbulence closure two distinct rans closures are assessed in this work the k ω sst and the k ω tnt earsm throughout this article these models are designated by sst and earsm respectively 2 1 1 k ω sst model the k ω sst closure menter et al 2003 is a linear turbulent viscosity model blending the k ω and k ε models in order to combine their main advantages it also accounts for the effect of the principal shear stress transport this closure relies on two transport equations to calculate the modelled turbulence kinetic energy k and specific dissipation ω 5 d k d t p k β ω k x j ν ν t σ k k x j 6 d ω d t α ν t p k β ω 2 x j ν ν t σ ω ω x j 2 1 f 1 σ ω 2 ω k x j ω x j and defines the turbulent viscosity as 7 ν t a 1 k max a 1 ω s f 2 where s stands for the mean flow ensemble averaged or resolved strain rate magnitude p k for the production of turbulence kinetic energy p k ν t s 2 and a 1 α β β σ k σ ω σ ω 2 f 1 and f 2 are coefficients and auxiliary functions given in menter et al 2003 2 1 2 k ω tnt explicit algebraic reynolds stress model dol et al 2002 have used the framework proposed by wallin and johansson to enable the k ω tnt model kok 2000 to consider the anisotropy of the turbulent field in this closure a corrective extra anisotropy tensor a i j e x 8 a i j e x β 3 ω i k ω k j i i ω δ i j 3 β 4 s i k ω k j ω i k s k j β 6 s i k ω k l ω l j ω i k ω k l s l j β 6 i i ω s i j 2 i v δ i j 3 β 9 ω i k s k l ω l m ω m j β 9 ω i k ω k l s l m ω m j is added to equation 4 9 τ i j v i v j ρ 2 ν t s i j 2 3 k δ i j a i j e x k since this earsm closure relies on the k ω tnt model kok 2000 equations 6 and 7 are replaced by 10 d ω d t α ν t p k β ω 2 x j ν ν t σ ω ω x j 0 5 ω max k x j ω x j 0 0 and 11 ν t k ω in equation 8 ω i j is the mean flow vorticity tensor i i ω and i v are mean flow velocity invariants 12 i i ω ω k l ω l k 13 i v s k l ω l m ω m k β i are model coefficients and the superscript indicates that the quantity is normalized by 1 β ω dol et al 2002 2 2 delayed detached eddy simulation ddes gritskevich et al 2012 combines rans in boundary layers with a srs model in outer and detached regions towards this end an explicit dependence on the local spatial grid resolution is added in equation 5 in order to reduce the magnitude of k this feature enables the model to resolve the turbulent field in outer and detached turbulent flow regions therefore a turbulent length scale l t 14 l t l rans f d max l rans l les 0 0 is introduced in the dissipation term of the k transport equation 15 d k d t p k k 3 l t x j ν ν t σ k k x j in equation 14 l rans is the rans length scale l rans k β ω l les is the les length scale l les c des δ c des is a constant and δ is the largest cell characteristic length and f d is an empirical delay function 0 f d 1 defined as 16 f d 1 tanh c d 1 ν t ν κ 2 d 2 0 5 s 2 ω 2 c d 2 where ω is the resolved vorticity tensor magnitude κ is the von kármán constant d is the wall distance and c d 1 and c d 2 are coefficients given in gritskevich et al 2012 2 3 improved delayed detached eddy simulation iddes gritskevich et al 2012 is a slightly modified version of ddes contrary to ddes this model allows the use of a srs model inside the boundary layer therefore l t is defined as 17 l t l rans f d 1 f d l les where l les is the les length scale 18 l les c iddes min c w max d δ δ and f d is an empirical delay function 0 f d 1 given by 19 f d max f d t f b 20 f d t tanh c d t 1 ν t κ 2 d 2 0 5 s 2 ω 2 c d t 2 21 f b min 2 0 exp 9 0 0 25 d δ 2 1 0 in the previous equations c iddes c w c d t 1 and c d t 2 are coefficients given in gritskevich et al 2012 it is important to note that in the limit of δ the empirical delay function f d does not guarantee l t bounded by l rans for this reason and contrary to ddes this method does not necessary recover a rans behaviour in outer and detached regions in cases where the spatial resolution is insufficient to an adequate activation of the srs model 2 4 extra large eddy simulation xles kok et al 2004 is similar to the first generation of des however when in srs mode it uses a les model to determine the turbulent viscosity instead of a rans based closure in this manner it combines the rans k ω tnt closure kok 2000 with the les k sub grid scale model towards this end the turbulent length scale is defined as 22 l t min k ω c 1 δ and the turbulent viscosity by 23 ν t l t k in equation 22 c 1 is a constant given in kok et al 2 5 partially averaged navier stokes equations this mathematical model relies on two parameters to define the magnitude of the turbulence quantities being modelled f ϕ ϕ φ the pans closure used in this investigation is based on the k ω sst closure described in section 2 1 1 here we use the version proposed by pereira et al 2015a hence the parameters f k and f ω are included in equations 5 and 6 24 d k d t p k β ω k x j ν ν t σ k f ω f k k x j 25 d ω d t α ν t p k p p f ω β ω f ω ω x j ν ν t σ ω f ω f k ω x j 2 σ ω 2 ω f ω f k 1 f 1 k x j ω x j where p α β k ν t the present derivation assumes a negligible contribution of the resolved turbulent field φ on the turbulent transport of the modelled field ϕ this hypothesis is named zero transport model girimaji 2005 in this work we use constant values for f k and f ε f ω f ε f k and so the model has no explicit dependence on the spatial or temporal resolution two f k values are used in this study 0 25 similar to les and 0 50 on the other hand turbulent dissipation is assumed to occur in the unresolved turbulent scales and so f ε is set equal to 1 00 pereira et al 2015a 2 6 general considerations although the numerical implementation of the former mathematical models exhibit several similarities these models rely on distinct ideas therefore it is worth pointing out some of their relevant features the selected hybrid models ddes iddes and xles have an explicit dependence on the spatial resolution unless l t is imposed the physical resolution range of resolved scales of the model varies upon spatial grid refinement therefore modelling and numerical errors are inevitably entangled and so small differences between simulations and experiments may be a consequence of error cancelling this limits the estimation of numerical errors based on grid refinement studies contrary to these hybrid models the bridging method pans using constant values of f ϕ enables the distinction between numerical and modelling errors since its governing equations have no explicit dependence on the grid resolution yet ensuring that the spatial and temporal resolutions are sufficient for the intended f ϕ might be demanding for practical applications the former property of pans also prevents commutation errors avoiding the well recognized issues at rans srs interfaces naturally the former mathematical models lead to different physical resolutions therefore unless the results of all models are a posteriori filtered a precise comparison of these models with experimental measurements should only rely on time averaged quantities nonetheless the root mean square of the lift coefficient c l is also compared in this work due to its importance for practical applications hence the results of c l should be interpreted carefully 3 problem setup and numerical techniques 3 1 case study computational domain and boundary conditions the selected case study is the flow around a smooth circular cylinder at reynolds number 3900 the prescribed reynolds number is based on the cylinder diameter d and undisturbed incoming velocity v its magnitude places this flow in the sub critical regime zdravkovich 1997 characterized by a laminar boundary layer flow separation and free shear layer turbulence onset in the free shear layer and turbulent wake the flow in this regime also entails two major coherent structures the kelvin helmholtz rollers and the vortex shedding this flow problem has been experimentally investigated by ong and wallace 1996 and parnaudeau et al 2008 the measurements of the latter study are used as reference in the present validation exercises although at slightly different reynolds numbers and apparatus the experiments of norberg 2002 2003 and son and hanratty 1969 are also used as reference for the predicted time averaged separation angle θ s son and hanratty 1969 and the pressure coefficient on the cylinder s surface and resultant functional quantities c p θ norberg 2002 2003 these experiments are summarized in pereira et al 2018a the numerical simulations are conducted on a prismatic rectangular domain defined in a cartesian coordinate system x i centred at the cylinder s axis fig 1 the computational domain and boundary conditions prescribed in the present work are the following the velocity and turbulence quantities are set constant at the inlet boundary x 1 d 10 and the pressure is extrapolated from the interior of the domain the turbulence quantities result from a turbulence intensity equal to that measured in the experiments of parnaudeau et al 2008 i 0 2 and a ratio between turbulent and molecular kinematic viscosities of ν t ν 10 3 1 1 based on the authors experience ν t ν is selected to obtain a dimensionless ω of approximately 20 the order of magnitude of this value is similar to that recommended in menter 1994 on the other hand the streamwise derivatives of all dependent variables are zero at the outlet boundary x 1 d 40 at the top and bottom boundaries x 2 d 12 the pressure is imposed and the transverse derivatives of all the remainder dependent variables are set equal to zero symmetry conditions are applied at the spanwise boundaries x 3 d 0 and x 3 d 3 this option is discussed in pereira et al 2015b and stems from the fact that neither cyclic nor symmetry conditions are optimal to handle the spanwise wavelengths of the structures resulting from the coherent field no slip and impermeability boundary conditions are specified on the cylinder s surface furthermore the normal pressure derivative and the modelled turbulence kinetic energy are set equal to zero whereas the modelled specific dissipation is defined at the near wall cell centre from the analytical near wall solution of the ω equation wilcox 2006 ω 80 ν d 2 ω 80 ν d 2 f ω for pans 3 2 numerical settings the numerical simulations are performed in four spatial grid resolutions h i these grids range from 1 0 to 4 5 10 6 cells n c leading to a refinement ratio r n c 1 n c i 1 3 of 1 64 however due to the inherent cost the finest spatial grid resolution h 1 is only used with formulations that enable the separate assessment of numerical and modelling errors sst earsm and pans the temporal resolution is linked to the spatial resolution so that similar temporal and spatial refinement ratios are used therefore the dimensionless time step δ t v d varies from 8 526 to 5 209 10 3 time units as a result the time averaged maximum courant number c and characteristic cell length using wall coordinates on the tangential x t normal x n and spanwise x s directions are smaller than 2 7 1 8 0 38 and 38 9 based on half cell length in each direction and for the coarsest grid resolution g 4 for the sake of simplicity throughout this work the combination of the spatial and temporal resolutions h i and t i is denoted as grid g i due to the fact that the formal orders of spatial p s and temporal p t grid convergence are equal and so n c 1 1 3 n c i 1 3 p s δ t i δ t 1 p t the computational domain and the spatial grid resolution h 2 are depicted in fig 1 whereas the properties of the four grids are summarized in table 1 all the numerical simulations start from the velocity pressure and turbulence fields of an initial rans simulation of 200 time units then the calculations run for 500 dimensionless time units being the statistics calculated with at least the last 400 time units approximately eighty shedding cycles in order to minimize round off and iterative errors the numerical simulations are performed in double precision and the iterative convergence criterion c i t requires a maximum normalized residual of 10 5 for all dependent variables normalized residuals are equivalent to dimensionless variables differences in a simple jacobi iteration the discretization of the governing equations is performed with spatial and temporal second order accurate schemes p 2 0 including the convective terms of the turbulence quantities transport equations 3 3 refresco solver refresco refresco 2018 is a community based open usage cfd code for maritime applications it solves multiphase incompressible viscous flows using the continuity and navier stokes equations filtered complemented with turbulence closures the equations are discretized using a finite volume approach with cell centred collocated variables in strong conservation form and a pressure correction equation based on the simple algorithm is used to ensure mass conservation time integration is performed implicitly with second order backward schemes at each implicit time step the non linear system for velocity and pressure is linearised with picard s method and either a segregated or coupled approach can be used a segregated approach is adopted here for the solution of all transport equations the implementation is face based which permits grids with elements consisting of an arbitrary number of faces for turbulent flows rans sas des xles pans and les approaches can be used pereira et al 2015a 2015c the code is parallelized using mpi and subdomain decomposition and runs on linux workstations and hpc clusters the code is currently being developed verified and tested at marin the netherlands in collaboration with various universities refresco 2018 3 4 estimation of modelling errors the accuracy of the mathematical models described in section 2 is evaluated through the quantification of the modelling error e m ϕ i this exercise relies on the verification and validation procedures proposed by the american society of mechanical engineers asme 2009 the interval containing e m ϕ i for 95 out of 100 cases is given by 26 e c ϕ i u v ϕ i e m ϕ i e c ϕ i u v ϕ i where e c ϕ i is the comparison error defined as the difference between the numerical ϕ i and the experimental ϕ e measurement 27 e c ϕ i ϕ i ϕ e and u v ϕ i is the validation uncertainty determined as 28 u v ϕ i u i ϕ i 2 u n ϕ i 2 u e ϕ e 2 in equation 28 u i ϕ i u n ϕ i and u e ϕ e are the input numerical and experimental uncertainties respectively u i ϕ i is a consequence of the uncertainty in the quantification of several input parameters of the experiments fluid temperature turbulence intensity at the inlet of the experimental facility etc the present exercise assumes u i ϕ i 0 on the other hand u n ϕ i and u e ϕ e have their origin in the techniques used to perform the numerical and experimental measurements whereas u e ϕ e is taken from the experimental studies used in this investigation parnaudeau et al 2008 norbert 2002 2003 u n ϕ i is estimated by simply assuming that its value is equal to the estimated discretization error u n ϕ i e d ϕ i the reasoning behind this simple assumption is the fact that the use of the procedures recommended by the american society of mechanical engineers asme 2009 or eça and hoekstra 2014 would require at least three sufficiently refined grids which is excessively demanding for the available computational resources see appendix a furthermore e d ϕ i is estimated using a power series expansion where the spatial p s and temporal p t orders of convergence are imposed equal to the formal value of the selected discretization techniques p 2 0 therefore 29 e d ϕ i ϕ i 1 ϕ 1 r i 1 2 1 r i 2 where ϕ o is the estimate of the exact solution we are aware that such approach may lead to non conservative estimates of the numerical uncertainty however for the level of grid refinement available the application of procedures that rely on the observed order of spatial temporal convergence may lead to overly conservative estimates of the numerical uncertainty due to the use of data from excessively coarse grids therefore such shortcoming of the uncertainty estimation procedure would obscure the comparison of the models tested unless stated otherwise the estimated e d ϕ and u v ϕ shown in this work use the finest resolution available nonetheless the complete set of results for each mathematical model is available in appendix a 4 results this section presents the results of this investigation where the modelling accuracy of the selected mathematical models is examined in the simulation of the flow around a circular cylinder at r e 3900 towards this end section 4 1 starts by analysing some relevant features of the various rans and srs models afterwards section 4 2 discusses the outcome of the validation exercises performed with the different mathematical models whereas section 4 3 examines the physical rationale behind the results all quantities shown in this section are normalized using v d and ρ as reference quantities it is also important to mention that the time averaged data presented in contour plots considers the entire simulated data nonetheless the influence of the initial condition should not affect the qualitative conclusions taken based on these data on the contrary the statistics of integral and local profiles quantities discard the initial transient period up to 100 time units 4 1 features of the mathematical models as discussed in section 2 the selected mathematical models are based on distinct modelling concepts for this reason a correct comparison of these models requires the understanding of their main characteristics hence the influence of the extra reynolds stress anisotropy tensor in the earsm the extension and location of the srs regions in hybrid models and the physical resolution of each model are discussed below all these features are relevant to the discussion presented in sections 4 2 and 4 3 4 1 1 anisotropy of the turbulence field most turbulence closures used in practical applications are based on the boussinesq hypothesis in this manner the influence of the turbulent stresses on the mean flow field is assumed proportional to the product of the turbulent viscosity by the strain rate tensor equation 4 however this type of linear constitutive relation is usually only adequate for simple statistically steady flows or to represent the smallest turbulent scales the application of such class of closures is therefore questionable in cases where the flow exhibits for instance coherent structures or massive separation the idea of an earsm is to enhance the quality of rans closures by considering a non linear relation between the reynolds stress and the strain rate tensors towards this end an extra reynolds stress anisotropy tensor a i j e x is added to the boussinesq relation equation 9 the importance of this additional tensor to the simulation of the present flow problem is shown in fig 2 this picture presents the streamwise and transverse normal components of the time averaged ratio γ i j a i j e x k 2 ν t s i j 2 3 k obtained from an earsm solution the data indicate that according to this model the maximum values of γ i i reach 0 50 in the free shear layer although not shown here the remainder γ i j components also possess meaningful magnitudes for instance γ 12 achieves similar values to those shown in fig 2 with their maximum located in the x 2 d 0 0 region these results demonstrate that the estimated extra anisotropic components of τ i j are not negligible 4 1 2 rans and srs regions in hybrid models an important aspect in the application of hybrid models is the location and extension of the srs region although at the present r e the boundary layer is laminar the location of the rans srs interface will certainly affect the breakdown to high intensity turbulence in the free shear layer to analyse its effect in the predictions of hybrid methods discussed in section 4 2 fig 3 presents the regions where the srs mode is active l t l rans 0 999 at δ t v d 500 and the respective turbulent viscosity field as expected the plots demonstrate that iddes permits the activation of the srs mode in the boundary layers whereas in ddes such activation only occurs in the free shear layer compared to ddes xles applies the srs model closest to the cylinder s surface regarding the instantaneous ν t xles leads to the lowest magnitudes of this quantity while ddes and iddes exhibit comparable values of ν t similar trends are observed for the time averaged turbulent viscosity field 4 1 3 physical resolution the physical resolution determines the range of resolved scales which strongly influences the magnitude of the modelling error it is expected that resolving a wider range of scales enhances the modelling accuracy ideally the evaluation of the effective physical resolution of a given mathematical model should be accomplished through techniques such as energy spectra however considering the aim of this study and the complexity of applying such methods to the present flow we simply conduct a qualitative discussion by examining the instantaneous q criterion hunt et al 1988 field 30 q 1 2 ω i j ω i j s i j s i j predicted by the various models fig 4 depicts the iso surfaces of the q criterion q 0 1 0 2 and 0 5 at δ t v d 500 as expected the sst model does not exhibit a significant amount of flow structures since the turbulence field is in theory entirely modelled nonetheless it is clear that the mean flow is not bi dimensional the earsm on the other hand leads to a flow field significantly different from the sst in fact the results are very similar to pans with f k 0 50 suggesting that the reynolds stress tensor might be underpredicted by the earsm naturally the srs models exhibit flow structures that are not captured by rans furthermore the data indicate the existence of two groups of solutions ddes with iddes and pans f k 0 25 with xles overall the results demonstrate that the selected rans and srs methods possess distinct physical resolutions and the existence of similarities between some models sst earsm and pans f k 0 50 ddes and iddes and xles and pans f k 0 25 it is therefore expected that the magnitude of the modelling and numerical error follows the trends observed in fig 4 this point is illustrated in the next sections and in appendix a 4 2 mathematical models comparison this section discusses the modelling accuracy of the selected rans and srs models towards this end modelling and numerical errors are quantified for integral and local flow quantities although the computations exhibit discretization errors that are not negligible see appendix a their magnitude enables the comparison of the models performance for most of the selected quantities all the results presented in this section use the finest grid resolution common to all formulations g 2 the exceptions are the quantities for which the validation uncertainties are estimated in those cases the results use the finest grid spatial and temporal resolution available fig 5 and table 2 present the comparison error e c ϕ and validation uncertainty u v ϕ in the prediction of several quantities of practical interest these are the root mean square lift coefficient c l strouhal number s t time averaged drag coefficient c d base pressure coefficient c p b recirculation length l r minimum streamwise velocity magnitude v 1 min at the centreline x 2 d 0 0 and separation angle θ s 2 2 validation uncertainties are not calculated for this quantity since the time averaged friction coefficient along the cylinder s surface c f θ necessary to calculate θ s was only computed for the finest grid for nearly all flow quantities the largest discrepancies with the experiments are obtained for the sst closure this is particularly visible in the prediction of c l 3 3 it should be mentioned that applying ensemble averaging to the experimental data will most likely reduce c l and so e c c l for the sst model should be even larger than the present result where the comparison error exceeds 590 the experimental observation the earsm on the other hand exhibits a significantly better prediction of these flow quantities for instance the comparison errors of the drag and lift coefficients are reduced to 1 7 and 195 7 respectively it is also remarkable the similarity between the results of earsm and pans f k 0 50 nonetheless the enhancement in modelling quality e c ϕ achieved by these models is coupled with a stronger dependence on the grid resolution this is visible in the magnitude of u v ϕ the best agreement with the experimental observations of the aforementioned flow quantities is obtained by the remainder srs methods ddes iddes xles and pans f k 0 25 it is also possible to identify a trend suggesting that xles and pans f k 0 25 attain the most accurate representation lowest e c ϕ of the selected quantities of interest this improvement entails a substantial growth of the necessary grid resolution to guarantee sufficiently small magnitudes of the numerical error 4 4 ideally the numerical error should be reduced to negligible levels when compared to the modelling error similar conclusions can be drawn for the pressure coefficient on the cylinder s surface c p θ fig 6 whereas pans f k 0 25 and xles computations obtain an excellent agreement with the experimental measurements the outcome of sst leads to the largest comparison errors although the remainder rans and srs methods attain larger comparison errors than xles or pans f k 0 25 these are substantially smaller than for the sst it is also interesting to observe the influence of the mathematical model on the thickness of the boundary layer upstream flow separation the results show that the sst earsm and pans at f k 0 50 models lead to thicker boundary layers lower magnitudes of c p fig 7 depicts the time averaged velocity magnitude v i streamwise variance v 1 v 1 and covariance v 1 v 2 profiles in the near wake the results confirm once again the previous trends there is a significant reduction in the comparison error of all flow quantities between sst and xles pans f k 0 25 computations and based on the magnitude of e c ϕ it is possible to identify four groups of simulations these are i sst i i earsm and pans f k 0 50 i i i ddes and iddes and i v xles and pans f k 0 25 it should be noted that the use of time averaged quantities at the inlet boundary is certainly delaying the turbulence onset in the free shear layer contributing to a larger recirculation region with the increase of the physical resolution this has been demonstrated and discussed in norberg 1987 or pereira et al 2018a fig 7 also shows that only srs models except pans at f k 0 50 can replicate the correct magnitude and shape of the v i profiles at different cross sections however this is not necessarily true for the specific stresses velocity variance and covariance for these quantities only pans f k 0 25 and xles simulations present a good agreement with the experiments for this grid resolution the former tendencies are also observed in the time averaged streamwise velocity field on the x 1 x 2 plane depicted in fig 8 in comparison to the experiments the length and shape of the recirculation region delimited by the white line v 1 0 are clearly underpredicted by the sst however although the use of earsm and pans f k 0 50 models reduces the comparison error only the remainder srs methods lead to a good representation of the streamwise velocity field finally the comparison error of the time averaged streamwise velocity field e c v 1 is shown in fig 9 the outcome indicates that the maximum comparison error in the predictions of this flow quantity is located in the free shear layers with its magnitude following the overall trend obtained for the remainder quantities whereas the comparison error in sst computations reaches 85 of the inflow velocity pans at f k 0 25 reduces its magnitude to small levels this result points out a strong relation between the predictions quality and the manner how the free shear layer is represented this point is now examined in the following section 4 3 physical interpretation of the results the dynamics of flows around circular cylinders in the regime where turbulent transition occurs in the free shear layer is governed by the kelvin helmholtz kh and vortex shedding vs coherent structures pereira et al 2018b have shown that the accuracy of a given mathematical model to predict problems in such regime depends on the ability to replicate the spatial development of the former coherent structures this comprises four major steps i onset of the kelvin helmholtz instability in the free shear layer ii spatial development of the kelvin helmholtz rollers iii breakdown to high intensity turbulence and iv turbulent free shear layer roll up and vortex shedding the investigation of pereira et al 2018b proposes two conditions to guarantee the precise prediction of the former phenomena i an effective computational reynolds number r e e 31 r e e v d ν ν t exceeding 1200 in the free shear layer region lowest reynolds number to experimentally observe the kelvin helmholtz rollers and ii a mean to unresolved field strain rate ratio s k ε lower than 6 to assure that only fully developed turbulence is modelled by the closure these findings and conditions are now used to explain the observed differences in modelling accuracy between the rans and srs methods analysed in section 4 2 the spatial development of the kelvin helmholtz instability and structure occurs along the free shear layers and determines the flow dynamics in this region hence the extension and shape of the free shear layers are expected to be closely dependent on the representation of such coherent structure to investigate this aspect fig 10 depicts the centreline of the free shear layers predicted by the rans and srs models these lines are estimated from the inflection point of the streamwise velocity field 2 v 1 x 2 2 from fig 10 it is clear that most formulations predict the onset of the free shear layer at approximately the same location excluding the sst θ s varies less than 3 4 upon model yet its spatial development varies with the mathematical model whereas the sst exhibits an early inward curving of the inflection lines and consequent underprediction of their length the differences between predictions and experiments are significantly reduced as the physical resolution see section 4 1 3 and the closure quality increase in this manner it is once again possible to identify four groups of models i sst ii earsm and pans f k 0 50 iii ddes and iddes and iv xles and pans f k 0 25 note that these groups coincide with those observed in section 4 2 confirming the relation between modelling accuracy and the dynamics of the free shear layer pereira et al 2018b it is also interesting to observe that the extension and shape of the free shear layer is also strongly affected by the grid resolution for pans at f k 0 50 earsm and sst not shown this result is a consequence of discretization errors or numerical diffusion and explains the large discrepancies between results in grids g 1 and g 2 see appendix a the outcome of fig 10 indicates that the evolution of the free shear layer is incorrectly simulated by the one point closures used in this work as illustrated in pereira et al 2018b this stems from the overprediction of turbulence in the free shear layer region where the coherent field dominates the flow dynamics to overcome this issue it is proposed that the physical resolution of the mathematical model should guarantee i r e e 1200 and ii s k ε 6 fig 11 depicts the effective computational reynolds number r e e in the x 1 x 2 plane for the analysed rans and srs models similar to the results of pereira et al 2018b the data indicate that the beginning of the inward curving of the inflection lines coincides with the point where the free shear layers experience an effective computational reynolds number inferior to 1200 recall that such reynolds number is the lower limit to experimentally observe the kelvin helmholtz instability and structure rajagopalan and antonia 2005 additionally fig 11 shows that the point where this event occurs moves downstream upon the enhancement of the physical resolution see section 4 1 3 whereas for sst the inflection line experiences r e e 1200 in the vicinity of the cylinder only xles and pans f k 0 25 guarantee r e e 1200 in the entire free shear layer regarding the remainder models and in comparison to the sst the data exhibit a significant increase of r e e which follows the modelling accuracy e c ϕ of the models the former trends suggest that the modelling accuracy of the mathematical models is related to the spatial development of the kelvin helmholtz instability and structure in the free shear layer this is confirmed in fig 12 where the frequency energy spectra in a point located in the free shear layer region is plotted at this location it is expected that the resultant spectra clearly exhibit the characteristic frequencies of the vortex shedding and kelvin helmholtz structures fig 12h the results show that while the sst only captures the vortex shedding frequency and its harmonics the remainder models are also able to replicate the kelvin helmholtz frequency the exception is the ddes simulation where is not possible to clearly identify such frequency it is only visible for the v 2 field nonetheless solely xles and pans f k 0 25 exhibit the characteristic relatively broad band of the kelvin helmholtz frequency see for instance rajagopalan and antonia 2005 or prasad and williamson 1997 this result indicates that the enhanced modelling accuracy of these models stems from their ability to precisely predict the spatial development of the kelvin helmholtz structure and consequently of the vortex shedding although ddes and iddes have the potential to achieve similar results they require further spatial grid refinement to increase the physical resolution as for the s k ε 6 criterion the results indicate that it is less strict than r e e 1200 and so the outcome is not shown here nonetheless this condition is fulfilled by all srs models tested except pans at f k 0 50 5 conclusions the present work has investigated the aptitude of various rans and srs methods to simulate the flow around a circular cylinder at r e 3900 the objectives of this study are threefold i evaluate the accuracy of selected formulations ii examine their dependence on the physical resolution and turbulence closure and iii interpret the physical rationale of the results six mathematical models are selected rans supplemented with the sst and earsm closures ddes iddes xles and pans f k 0 50 and 0 25 the modelling accuracy of these models is assessed through validation exercises the results are then interpreted using the findings and conditions proposed in pereira et al 2018b 2019 the major conclusions of the study are the following the required grid resolution to achieve an adequate magnitude of the numerical error varies upon physical resolution and closure as expected the results indicate that both the range of resolved scales and the necessary grid resolution increase with the physical resolution similarly advanced closure strategies such as the earsm require finer grid resolutions than traditional turbulent viscosity closures to achieve the same magnitude of the discretization error although not shown the increase of physical resolution also originates a growth in the unsteadiness and irregularity of the numerical solutions which causes larger variations between time steps as a result the number of iterations to achieve a given iterative convergence criterion increases in the case of pans the reduction of f k from 1 00 to 0 25 leads to an increase of 62 in the required averaged number of iterations per time step to attain c i t 10 5 the outcome of rans and srs methods for integral and local flow quantities indicates the existence of four groups of simulation in terms of their modelling accuracy these are i sst ii earsm and pans f k 0 50 iii ddes and iddes and iv xles and pans f k 0 25 whereas the results of the sst lead to large comparison errors which may reach 590 of the experimental value xles and pans f k 0 25 achieve the best agreement with the experiments by obtaining reduced comparison errors from a practical perspective the results also suggest that the use of the earsm rans may be an efficient option for some applications due to its compromise between modelling accuracy and numerical requirements the differences in modelling accuracy of the former groups are explained by the ability of the mathematical models to replicate the major flow features most notably the spatial development of vortex shedding coherent structure which entails the onset and growth of the kelvin helmholtz rollers it has been shown that only xles and pans f k 0 25 are capable to precisely replicate these phenomena the results also reaffirm the validity of the conditions proposed in pereira et al 2018b to guarantee the correct prediction of this flow regime these are an effective computational reynolds number r e e exceeding 1200 in the free shear layer and a mean to unresolved field strain rate ratio s k ε lower than 6 acknowledgements the authors would like to thank the maritime research institute netherlands and to the laboratory for advanced computing at university of coimbra for providing the hpc resources necessary to perform the present research moreover the authors are very grateful to c norberg l carlier and e lamballais for providing their experimental measurements appendix a influence of grid resolution the evaluation of the selected rans and srs methods was preceded by grid space and time refinement exercises to estimate the discretization error and consequently the validation uncertainty the predictions for integral and local flow quantities and respective estimates of discretization errors e d ϕ are presented in tables a 3 and a 4 and figures a 13 and a 14 the data shown in the former tables demonstrate a reduced dependence of the sst results from the grid resolutions employed in this study except for the recirculation length and minimum streamwise velocity all estimates of the discretization error are small earsm and srs models on the other hand increase substantially the relevance of the grid resolution the results obtained with these formulations also indicate that the two coarsest grids do not possess enough resolution to use the selected earsm and srs models similar trends are also observed for the time averaged pressure coefficient distribution on the cylinder s surface c p θ and the streamwise velocity v 1 profiles in the near wake depicted in figures a 13 and a 14 table a3 time averaged drag coefficient c d root mean square lift coefficient c l time averaged base pressure coefficient c p b strouhal number st recirculation length l r and minimum streamwise velocity v 1 min as a function of the grid resolution g i and mathematical model table a3 model g i c d c l c p b s t l r v 1 min sst 4 1 280 0 735 1 437 0 215 0 244 0 015 3 1 258 0 693 1 370 0 217 0 258 0 022 2 1 249 0 682 1 360 0 217 0 309 0 045 1 1 245 0 664 1 405 0 211 0 505 0 227 earsm 4 1 218 0 628 1 376 0 208 0 615 0 174 3 1 111 0 436 1 174 0 217 0 832 0 235 2 1 061 0 348 1 088 0 214 0 982 0 259 1 0 998 0 225 0 976 0 217 1 217 0 259 4 1 230 0 623 1 438 0 204 0 621 0 179 pans f k 0 50 3 1 182 0 535 1 302 0 208 0 714 0 209 2 1 135 0 451 1 221 0 209 0 828 0 245 1 1 036 0 284 1 050 0 214 1 124 0 273 ddes 4 1 229 0 637 1 390 0 208 0 625 0 166 3 1 071 0 322 1 095 0 208 1 025 0 284 2 1 008 0 238 1 003 0 209 1 289 0 233 iddes 4 1 170 0 511 1 278 0 200 0 821 0 219 3 1 077 0 337 1 097 0 204 1 031 0 266 2 0 993 0 181 0 966 0 209 1 347 0 282 xles 4 1 094 0 407 1 150 0 204 1 004 0 177 3 0 976 0 170 0 950 0 206 1 428 0 251 2 0 954 0 127 0 906 0 212 1 547 0 257 4 1 103 0 380 1 157 0 200 0 985 0 267 pans f k 0 25 3 0 996 0 231 0 976 0 208 1 378 0 235 2 0 938 0 112 0 882 0 212 1 630 0 264 1 0 927 0 095 0 864 0 208 1 728 0 272 table a4 estimated discretizarion error maximum p 1 00 and minimum p 2 0 for the time averaged drag coefficient c d root mean square lift coefficient c l time averaged base pressure coefficient c p b strouhal number s t recirculation length l r and minimum streamwise velocity v 1 min as a function of the mathematical model results for grid resolutions g 1 and g 2 table a4 model ϕ c d c l c p b s t l r v 1 min sst e d ϕ 2 max 0 025 0 033 0 027 0 001 0 146 0 065 e d ϕ 2 min 0 011 0 014 0 015 0 000 0 062 0 028 e d ϕ 1 max 0 024 0 118 0 306 0 042 1 610 1 239 e d ϕ 1 min 0 011 0 055 0 143 0 019 0 750 0 577 earsm e d ϕ 2 max 0 142 0 249 0 244 0 006 0 425 0 068 e d ϕ 2 min 0 060 0 106 0 104 0 003 0 181 0 029 e d ϕ 1 max 0 429 0 836 0 762 0 015 1 602 0 004 e d ϕ 1 min 0 200 0 389 0 355 0 007 0 746 0 002 e d ϕ 2 max 0 132 0 239 0 231 0 004 0 323 0 103 pans f k 0 50 e d ϕ 2 min 0 056 0 102 0 098 0 002 0 137 0 044 e d ϕ 1 max 0 678 1 133 1 165 0 030 2 009 0 188 e d ϕ 1 min 0 316 0 528 0 543 0 014 0 935 0 087 ddes e d ϕ 2 max 0 176 0 238 0 263 0 004 0 749 0 145 e d ϕ 2 min 0 075 0 101 0 112 0 002 0 318 0 062 iddes e d ϕ 2 max 0 238 0 444 0 370 0 016 0 897 0 043 e d ϕ 2 min 0 101 0 189 0 157 0 007 0 381 0 018 xles e d ϕ 2 max 0 062 0 122 0 125 0 018 0 253 0 016 e d ϕ 2 min 0 026 0 052 0 053 0 007 0 143 0 007 e d ϕ 2 max 0 165 0 338 0 268 0 011 0 715 0 082 pans f k 0 25 e d ϕ 2 min 0 070 0 144 0 114 0 005 0 304 0 035 e d ϕ 1 max 0 079 0 115 0 118 0 027 0 655 0 057 e d ϕ 1 min 0 037 0 054 0 055 0 013 0 305 0 026 fig a13 time averaged pressure coefficient distribution on the cylinder s surface c p θ as a function of the grid resolution g i and mathematical model fig a13 fig a14 time averaged streamwise velocity magnitude v 1 profiles in the near wake as a function of the grid resolution g i and mathematical model fig a14 
22691,bubbling from near wall orifices is widely used in ocean engineering such as ship drag reduction sewage treatment underwater vehicle control technology and so on but at present most studies on ventilated bubble do not take into account the influence of the wall on the development of the bubble based on the potential flow theory combined with mesh smoothing technique and mesh subdivision technique the boundary element method is adopted to study the 3d ventilated bubble growth in this paper the bubble is generated by ventilation during the vertical upward movement of the rotary body with a single horizontal orifice on the wall and the convergence of the model is studied by modifying the mesh size meanwhile the numerical simulation results agree well with the experimental results finally with the existence of the inflow the effects of different inflow velocities ventilation rates the diameters of the orifice and surface tension coefficients on the development process and flow characteristics of the bubble were studied the results show that the inflow velocity is the main influencing factor of the bubble development process the ventilation rate and the diameter of the orifice have some influence on the development process of the bubble and the surface tension coefficient has little effect on the development process of the bubble keywords bubble formation near wall single orifice boundary element method bubble dynamics 1 introduction the process of ventilated bubble formation occurs in various ocean engineering such as ship drag reduction sewage treatment underwater vehicle control technology and so on there are many ways to achieve ventilation single orifice ventilation is the basis of ventilation technology and it is also the basic problem of gas liquid two phase flow it has many influencing factors different gas flow rates liquid flow rates liquid surface tension coefficients orifice diameters gas characteristics etc can cause differences in the flow characteristics of the ventilated bubble when there is liquid inflow the horizontal single orifice ventilated bubble is mainly affected by fluid drag force buoyancy surface tension and gas momentum force different liquid inflow velocities and ventilation velocities will result in different forms of ventilated bubble such as floating bubble jet etc for single orifice ventilated bubble in theoretical and experimental research harby et al 2017 proposed and verified a mathematical model in the mathematical model the rate of entrainment is assumed to be a function of the jet centerline velocity the ratio of the mean jet and the ambient densities the experiments about bubble formation development and detachment were carried out with parameters badam et al 2007 vafaei 2010 bari and robinson 2013 hu et al 2016 besides harby et al 2017 li et al 2018 conducted an experimental study on the gas jet formed by horizontal ventilation in numerical research xu et al 2013 islam et al 2015 applied volume of fluid vof method das and das 2009b 2012 applied lagrangian smoothed particle hydrodynamics sph method to successfully simulate the development of single orifice ventilated bubble oguz and zeng 1997 higuera and medina 2006 applied the boundary element method to simulate the development of axisymmetric bubble oguz and zeng 1997 wu et al 2017 simulated the growth and detachment of single orifice ventilated three dimensional bubble under static and inflow conditions by the boundary element method for the study of single orifice ventilated bubble most of them do not consider the influence of wall on the bubble flow characteristics but in practical applications the single orifice is generally opened on the wall such as applied to ship drag reduction underwater vehicle control technology etc in experimental research kodama et al 2000 song et al 2018 verified the drag reduction effect of ventilation through experimental research bai 2001 studied the gas flow rate liquid flow rate orifice diameter and gas characteristics in the bubble formation process through horizontal single orifice ventilation experiments vigneau et al 2001 conducted an experimental study on the shapes of horizontal near wall single orifice ventilated bubble under conditions of downward flow lee 2015 studied the effect of froude number on the shape of gas jets in the vertical ventilation experiments at the bottom of the ship model the numerical simulation has high efficiency and low cost and can be compared with experiments results it is an indispensable research method in the study of near wall ventilated bubble chen et al 2009 studied the effects of the contact angle and the contact line models on the bubble formation from submerged orifices using a 2d axisymmetric numerical scheme involving a level set method for tracking the two phase interface mohanarangam et al 2009 used multiple size group musig based on population balance model to study the phenomenon of drag reduction by the injection of micro bubbles the vof method was used by rana et al 2017 to simulate the bubble bubble interaction upon formation at submerged orifices mirsandi et al 2018 extended the local front reconstruction method by incorporating contact angle dynamics and verified the model by experimental and numerical data the boundary element method is accurate for the simulation of the development of the bubble oguz and zeng 1997 wu et al 2017 it can track the interface deformation directly calculate the interface velocity and can better simulate the steady motion bubble attached to the wall moreover the dimension of the problem can be reduced by one dimension and the operation rate is higher than other algorithms the boundary element method was used by xiao and tan 2005 to simulate the development of flat plate single orifice ventilated axisymmetric bubble before detachment in this paper combined with mesh smoothing technique zhang et al 2001 and mesh subdivision technique wu et al 2017 a three dimensional near wall ventilated bubble dynamics model is established by the boundary element method then the convergence of the model is studied by modifying the mesh size and the accuracy of the model was verified by comparing with the experimental results after that the numerical simulation of the development process of horizontal near wall single orifice ventilated 3d bubble under inflow conditions was carried out and the effects of inflow velocity ventilation rate orifice diameter and surface tension coefficient on bubble flow characteristics were analyzed 2 problem formulation in this paper the potential flow theory is used to establish a numerical model of underwater near wall single orifice ventilated bubble including 1 the governing equation for bubble motion 2 the equation of gas for underwater ventilated bubble fig 1 shows the physical model of this paper the research focus of this paper is on the flow characteristics of ventilated bubble before detachment as shown in fig 1 the gas is not directly filled into the fluid through the orifice but is first filled into the air chamber c at ventilation rate q and then filled into the fluid through the orifice to form a ventilated bubble in this paper ventilation rate at the orifice is q u m represents the speed of the model movement p 0 p ρ l g h represents hydrostatic pressure at the orifice v c is the volume of the air chamber v b is the volume of the bubble the diameter of the orifice is r 0 in the numerical model the center of the orifice is the origin of the coordinate system and r axis and z axis are shown in fig 1 in addition the physical model needs to be reasonable and appropriate simplified liquid inflow velocity u equivalent model motion velocity u m meanwhile a small round orifice is opened on the wall of the cylindrical chamber with a larger radius which can be approximated as an orifice on the flat plate in addition as shown in fig 2 the length width and thickness of the bubble are respectively defined as the difference between the maximum and minimum values of the node coordinates in the corresponding direction 2 1 governing equation assuming that the flow field is irrotational non viscous and incompressible the velocity potential ϕ satisfies laplace s equation 1 2 φ 0 from the infinity boundary condition and the green function the boundary integral equation can be obtained 2 λ ϕ p s ϕ q n g p q ϕ q n g p q d s where p is a fixed point on the boundary q is the integration point on the boundary λ is the solid angle of the flow field observed at point p s is the boundary surface including the bubble surface and the wall surface green function g p q p q 1 in the system of near wall single orifice ventilated bubble the development process of the bubble and the entire flow field are affected by the wall surface in this model the influence of the wall on the development of the bubble is taken into account by using the principle of the image and the processing of the wall mesh in the model can be simplified thus the green function in equation 2 is taken as g p q p q 1 p q 1 where q is the image of q in the wall considering the inflow conditions in the flow field the total velocity potential in the flow field can be decomposed into the disturbance velocity potential ϕ d induced by the existence of the bubble and the flow incident velocity potential ϕ c as follows 3 ϕ ϕ c ϕ d u 0 z ϕ d where u 0 is the velocity of steady uniform flow since the problem of steady uniform flow is studied in this paper the key is to solve the disturbance velocity potential induced by bubble the total velocity potential ϕ and the disturbance velocity potential ϕ d in the flow field satisfy the laplace equation and ϕ d disappears in the far field to be zero so the boundary integral equation can be used to solve the disturbance velocity potential 4 λ ϕ d p s ϕ d q n g p q ϕ d q n g p q d s the integral equation 4 can be reduced to a matrix form chahine et al 1995 5 h ϕ g ϕ n substituting equation 3 into the bernoulli equation we can get the following equation wu et al 1998 koo and kim 2007 6 d ϕ d d t p p l ρ 1 2 ϕ 2 g z ϕ ϕ d where ρ is density of fluid z represents the vertical component of the node position p represents the pressure at infinity on the plane z 0 p l is the pressure at every node on the surface of the bubble ϕ ϕ c ϕ d u 0 z ϕ d this formula updates the disturbance velocity potential of the bubble surface under inflow conditions 2 2 the equation of gas for the entire ventilation system from the law of conservation of mass and the first law of thermodynamics applied to the bubble and the chamber respectively we can get the following equations xiao and tan 2005 7 v b d p b d t γ p b d v b d t γ p c q 8 v c d p c d t γ p a q γ p c q the volumetric flow q through the orifice is 9 q ρ b ρ c d v b d t substituting q into equation 8 and assuming the density is constant ρ b ρ c p a p c it follows 10 p c t p c 0 c 0 2 ρ c v c q t v b t v b 0 where c 0 2 γ r t c initially if we assume q q it follows that 11 p b 0 p c 0 q 2 ρ c k 2 π r 0 2 2 where k is the orifice coefficient according to formula 10 and 11 we can get 12 p b t p b 0 c 0 2 ρ c v c q t v b t v b 0 ρ c k 2 π r 0 2 2 q 2 d v b d t 2 the real time pressure of the bubble surface can be obtained from p b 13 p l p b σ κ where p b is the pressure in the bubble σ is the surface tension coefficient κ is the curvature of each node of the bubble 3 numerical scheme in order to obtain a numerical simulation result with good convergence and at the beginning of bubble development the bubble is transformed from spherical crown into hemisphere rapidly bai and thomas 2001 das and das 2009a islam et al 2015 thus the initial shape of the bubble is set to a hemisphere having the same diameter as the diameter of the orifice the model assumes that the initial value of the internal pressure of the bubble is the sum of the static pressure of the corresponding depth of the orifice and the surface tension of the bubble 14 p b 0 p o 2 σ r o 3 1 curvature of bubble surface in the 3d model we must face the numerical problem of solving the curvature of irregular 3d surfaces to solve it the idea of the literature liu et al 2007 is adopted and a concrete solution process of the node curvature is given as shown in fig 3 it is partial elements of the bubble surface the normal vector of the selected node h is n k and the edge connected to the point h is h h i i 1 2 m suppose a plane p passes through the normal vector n k and the edge h h i then the intersection of the plane p and the bubble surface is c s and s is the corresponding arc length in the plane p the tangential vector of the node h along the curve c s is τ k the element normal vector of the selected node is obtained by weighted averaging of the normal vectors corresponding to the surrounding triangles h h i h i 1 of the selected node plesset and chapman 1971 15 n k i 1 m n k i s i i 1 m 1 s i where s i is area of the triangular element h h i h i 1 then using the taylor formula expand the curve function c s at s s k it follows that 16 c s c s h τ s h s s h k s h n s h s s h 2 2 o s 3 then do the dot product between n k and both ends of formula 16 remove the high order infinitesimal amount and replace the arc length s s k with the length of the line segment of the adjacent node h i h we can get liu et al 2007 17 k s h 2 n h h i h h i h 2 assume that the angles of the triangles near the point h are θ i i 1 2 m respectively according to the curvature formula bronshtein and semendyayev 1997 the relationship between the curvature κ of the selected node and the corresponding curvature k n θ in the normal interface of the adjacent element is as follows 18 κ 1 π 0 2 π k n θ d θ 1 π i 1 m k i k i 1 2 θ i in the formula the integral value is approximated by the trapezoidal principle k i is obtained by 17 and finally the surface tension is obtained by multiplying the obtained curvature by the surface tension coefficient 3 2 mesh subdivision in the numerical simulation of 3d bubble when the bubble is gradually deformed the node spacing becomes larger and as a result the nodes near the orifice become sparse meanwhile the mesh at the end of the bubble is too dense these will cause the mesh distortion of the calculation process the calculation efficiency is greatly reduced and even the calculation is terminated if the numerical fault is simply eliminated by adding or removing nodes the topology of the bubble surface will also change dramatically this not only increases the difficulty of programming but also destroys the accuracy of the original mesh bringing unpredictable errors to the simulation process since the factors affecting the quality of the mesh mainly include the relative size of the mesh and the regularity of the elements in order to measure the degree of regularity of the triangular elements in the 3d model this paper introduces the concept of the triangle regularity r g 19 r g l p l 1 l p l 2 l p l 3 max l i where l p c 2 c is circumference of the triangle l i i 1 2 3 represents the length of three sides on the decision condition of the mesh subdivision it is considered that when any one edge of the triangle is larger than the specified edge size δ l 0 27 mm 0 28 mm 0 29 mm or 0 30 mm the edge l i of the triangle needs to be subdivided in the subdivision process the midpoint of the subdivision edge is taken as the subdivision point therefore there are three types of triangulation element subdivision modes subdivision of one edge two edges or three edges as shown in fig 4 for subdivision of one edge and three edges only the diagonal vertices and the midpoints need to be connected however for subdivision of two edges two subdivisions can be generated after subdividing one edge namely c d and a e to this end we select a subdivision scheme with a higher degree of element regularity 3 3 mesh smoothing technique in the process of simulating bubble development using the boundary element method since the discrete mesh does not completely conform to the smooth bubble surface the generation and accumulation of random errors will cause the mesh to be distorted and deformed resulting in non convergence of numerical calculation for this numerical problem this paper adopts a three dimensional mesh smoothing technique based on least square method zhang et al 2001 a portion of the surface elements are selected near the update node consisting of two turns of nodes around the selected node the number of nodes satisfies the minimum number of nodes required to use least square smoothing the whole process is equivalent to using a node as the origin o to establish an independent coordinate system o x y z where the direction of the axis z is the normal direction of the node according to the entire new coordinate system a function of the fourth power is selected to fit the local bubble surface 20 z a 1 x 2 a 2 x y a 3 y 2 a 4 x a 5 y a 6 f x y where a i i 1 6 are the coefficient that is optimized by calculating the error function in addition to being able to directly optimize the bubble node coordinates for a smoother surface the same method is equally suitable for optimizing the velocity potential ϕ on a node in the process of simulating the near wall single orifice ventilated 3d bubble the mesh distortion usually occurs at the edge of the orifice as shown in fig 5 a the inside of the circle is the mesh with distortion fig 5 b shows the mesh after smoothing the three dimensional smoothing technique can ensure the smooth progress of the numerical simulation process 4 convergence and validity of the model 4 1 convergence study the convergence of the numerical model about the mesh size is discussed in this section the size of the triangular element δ l is 0 27 mm 0 28 mm 0 29 mm and 0 30 mm respectively the other parameters in the model are the diameter of orifice r 0 is 4 mm inflow velocity u is 1 20 m s volumetric flow q is 15 cm3 s surface tension coefficient σ is 7 27 10 2 n m in addition in the model chamber volume v c and height of water h are constant which are 785 5 cm3 and 15 cm respectively fig 6 shows the convergence study of bubble length for different mesh sizes it can be seen from the bubble length variation curve that the length variation is extremely small as the mesh size decreases the mesh fineness increases continuously and the numerical solution converges to the same simulation result fig 7 and fig 8 respectively illustrate the convergence of bubble width and thickness for different mesh sizes they all converge to a numerical solution as the mesh size decreases it is worth mentioning that when the mesh size is approaching from 0 3 mm to 0 27 mm if a gradual calculation of the size value of 0 27 mm is the benchmark the calculation errors of the other three sizes are 9 01 7 12 and 0 62 from the reduction of the relative error it can be clearly seen that the numerical solution converges when the mesh size approaches 0 27 mm because there is no need for too many nodes to affect the computational efficiency the mesh size selected in the model is 0 28 mm 4 2 validity verification before using the bubble dynamics model to study the bubble variation characteristics it is necessary to verify its validity in this paper the self developed program is used to numerically simulate the bubble development process and compare it with the actual experimental data to verify the validity of the model the ventilation process is divided into two stages before and after the bubble is detached this paper mainly considers the development process before the bubble is detached in addition since the initial diameter of the bubble r 0 in the numerical model is the same as the diameter of the orifice r obviously this process does not exist in the generation of bubble in the experiment this may cause some errors at the beginning of the simulation the parameters are set as follows r 0 4 0 mm u 1 20 m s q 15 cm 3 s σ 7 27 10 2 n m the bubble shapes of the experimental and numerical simulation are shown in fig 9 it can be seen that the shape of the bubble obtained by numerical simulation is basically consistent with the experiment however in the experiment the gas source was supplied by a peristaltic pump and the flow rate was pulsed which could not guarantee a completely continuous stable flow supply such the shape of the bubble obtained by numerical simulation is smoother than the experiment the inflow velocity of the working condition is relatively large the liquid shearing force of the bubble is larger than the buoyancy of the bubble the bubble moves downward relative to the orifice and the bubble is vertically stretched and the bubble adheres to the wall surface due to the horizontal shearing force of the liquid in the upper part of the bubble near the orifice the thickness and width of the bubble are small and in the lower part of the bubble away from the orifice the relative velocity of the gas and the liquid is reduced the horizontal shear force is reduced the gas is diffused to both sides and the thickness and width of the bubble are increased fig 10 shows the comparison of length thickness and width of the bubble with time in experimental results and numerical simulations results according to fig 10 in the simulation results the development process and dimensional change of the bubble are basically consistent with the experimental data among them the growth rate of the width and thickness of the bubble tends to be gentle over time and the thickness even has a tendency to decrease the time at which the growth rate tends to be gentle is approximately the same reaching the state at about 30 ms in addition the length width and thickness are affected by the longitudinal inflow and there are many factors affecting the development of them for example the change of the depth causes the gas inside the bubble to expand and the influence of the vortex caused by the viscosity these cause some errors in the numerical simulation results relative to the experimental results 5 results and discussions 5 1 effect of inflow velocity in the case of downward inflow the bubble characteristics are not only affected by the internal pressure of the bubble but are mainly affected by the drag force in this section u is 0 5 m s 1 0 m s 2 0 m s and 3 0 m s respectively the other parameters are set as follows r 0 4 0 mm q 15 cm 3 s σ 7 27 10 2 n m fig 11 shows the comparison of the bubble formation evolution with different inflow velocities when the velocity is small the bubble is more fat and the relative development speed in the width direction is greater than that when the inflow velocity is large moreover when the inflow velocity is small the liquid drag force is small and as the bubble volume increases the buoyancy has a significant effect and the bubble tail is flat when the inflow velocity is large the liquid drag force plays a major role in the development of the bubble and the bubble mainly develops in the length direction as shown in fig 12 a when the uniform inflow velocity is in the range from 0 5 m s to 3 0 m s the bubble develops faster in the longitudinal direction as the inflow velocity increases as shown in fig 12 b when the inflow velocity is small the maximum volume that the bubble can reach is larger than the case where the velocity is large but the development speed of volume is slow when the inflow velocity is large the bubble volume develops faster but the maximum volume is small after reaching the maximum volume the bubble volume is reduced the main reason for this situation is that the large inflow velocity will cause the bubble to expand rapidly and the internal pressure of the bubble will decrease because the ventilation rate cannot satisfy the bubble volume expansion rate when the bubble volume reaches the peak value the bubble will shrink and rebound due to the difference between the pressure inside and outside of the bubble and eventually stabilizes near a certain value as shown in fig 13 when the inflow velocity is 3 m s and 2 m s the inflow velocity is large in the initial stage of development the width and thickness of the bubble increase at a high speed but as the flow velocity increases the maximum of width and thickness do not change much and are basically the same when the inflow velocity is 0 5 m s the thickness and width of the bubble are higher than the other three conditions and the attenuation of the thickness and the width is delayed when the volumetric flow is the same for the low velocity inflow the ventilation rate can compensate for the decrease of the pressure inside the bubble caused by the drag force and as time increases more gas flows into the bubble to form a relatively stable bubble when the inflow velocity is large the bubble rapidly expands the time to reach the peak is too short the amount of gas introduced into the bubble is little and the pressure inside the bubble is low so that shrinkage occurs in the thickness and width directions 5 2 effect of ventilation rate since the gas is regarded as an incompressible fluid throughout the process the gas flow takes a volumetric flow for the numerical model of this paper different ventilation rates will change the speed at which gas is injected into the water which in turn affects the development of the bubble in this section q is 15 cm3 s 25 cm3 s 35 cm3 s and 45 cm3 s respectively the other parameters are set as follows u 1 0 m s r 0 4 0 mm σ 7 27 10 2 n m fig 14 shows the comparison of the bubble formation evolution with different ventilation rates fig 15 a shows the time evolution of bubble volume with different ventilation rates the ventilation rate increases from 15 cm3 s to 45 cm3 s in the early stage of bubble development the effect of ventilation rate on bubble formation evolution is small before 13 ms the bubble volume was mainly affected by the drag force which increased linearly however over time after 13 ms the bubble with small ventilation rate could not satisfy the continuous increase of its volume the internal pressure of the bubble decreased and the decrease of difference between the internal and external pressure caused the volume growth rate to decrease in addition the bubble volume growth rate does not always increase with the increase of ventilation rate but tends to be gentle as the ventilation rate increases the main reason for this phenomenon is that the gas does not directly enter the inside of the bubble but first enters the gas chamber generates a buffer and then enters the inside of the bubble when the volume of the gas chamber is large even if the ventilation rate is large due to the limitation of the orifice the charged gas still cannot compensate for the pressure drop inside the bubble caused by the bubble volume increase fig 15 b shows the time evolution of bubble length with different ventilation rates the effect of different ventilation rates on the length of the bubble is small in the early and middle stages of bubble development but at each time point the length of the bubble with a large ventilation rate is large over time the bubble length no longer increases linearly only when the ventilation rate is small at which time the internal pressure of the bubble is low and the rate of growth in the length direction becomes slow fig 16 shows the time evolution of bubble thickness and width with different ventilation rates before 13 ms the effect of ventilation rate on thickness and width was small after 13 ms the bubble thickness increased with the increase of ventilation rate for the bubble width except for the case where q is 15 cm3 s is significantly smaller than the other three cases the thickness difference is small at the end of the curve of the other three cases therefore within a certain range the bubble width increases with the increase of the ventilation rate and when the ventilation rate reaches a critical value the influence of the ventilation rate on the bubble width is small 5 3 effect of orifice diameter in the numerical model of this paper the initial diameter of the bubble is equal to the diameter of the orifice by changing the diameter of the orifice and selecting the typical time the effect on the bubble formation evolution is studied in this section r 0 is 4 mm 8 mm 12 mm and 16 mm respectively the other parameters are set as follows u 1 0 m s q 15 cm 3 s σ 7 27 10 2 n m fig 17 shows the comparison of the bubble formation evolution with different diameters of the orifice when the diameter of the orifice is small the bubble is conical from top to bottom when the initial diameter is larger the initial volume of the bubble is larger and the internal pressure of the bubble is lower due to the constant ventilation due to the influence of surface tension and buoyancy the tendency of the downward movement of the bubble tail is slowed down and due to insufficient ventilation the internal pressure of the bubble is low and the shape of the bubble shrinks toward the middle it can be predicted that if the diameter of the orifice is continuously increased the bubbles will be quickly detached fig 18 a shows the time evolution of bubble thickness with different diameters of the orifice when the initial diameter is large such as 12 mm and 16 mm the bubble thickness gradually decreases and tends to 5 mm and 6 mm respectively however in the initial stage of bubble formation both the ventilation effect and the drag force of the inflow should increase the thickness of the bubble the main reason for this phenomenon is that in order to obtain a numerical simulation result with good convergence the initial shape of the bubble is set to a hemisphere having the same diameter as the diameter of the orifice but the process cannot occur in an actual situation because once a bubble is formed in the presence of an inflow the bubble will move downward forming an abdomen therefore in the initial stage of numerical simulation the simulation is not in line with the actual situation however as time progresses the thickness gradually approaches a value indicating that the model can guarantee better convergence fig 18 b shows the time evolution of bubble width with different diameters of the orifice when the diameter is 16 mm the bubble width does not substantially change when the diameter is 12 mm the width fluctuates around 12 mm in contrast when the diameter is 8 mm the bubble width also fluctuates constantly but the overall rise slightly when the diameter is 4 mm the time evolution of the width of the bubble is more consistent with the conventional change course that is the width of the bubble first increases and then becomes flat fig 19 shows the time evolution of bubble length and volume with different diameters of the orifice under the same inflow condition as the initial diameter of the bubble increases the bubble volume increases it is worth noting that when the diameter is 4 mm the growth rate of the length and volume of the bubble is much faster than the other three cases in the other three cases the bubble length tends to be flat as time is 27 ms 5 4 effect of surface tension coefficient the effect of surface tension on the development of bubble formation evolution was studied by changing the surface tension coefficient according to the weber number w e ρ r 0 u 2 σ the excessive inflow velocity will greatly increase the influence of inertial force in the system even if the surface tension coefficient is changed the influence of surface tension cannot be compared with the inertial force therefore the inflow velocity in this section takes a smaller value u 1 0 m s in this section σ is 3 0 10 2 n m 3 4 10 2 n m 4 9 10 2 n m and 7 27 10 2 n m respectively the other parameters are set as follows u 1 0 m s r 0 4 0 m m q 15 c m 3 s fig 20 shows the comparison of the bubble formation evolution with different coefficients of surface tension as the surface tension increases the bubble is more fat it can be seen from fig 21 that the surface tension has little effect on the change of bubble formation evolution in the presence of inflow especially the effect on the width and length of the bubble and the four curves almost coincide only in the thickness of the bubble as the surface tension coefficient increases the thickness decreases slightly 6 conclusions in this paper a three dimensional near wall ventilated bubble dynamics model is established by the boundary element method and mesh smoothing technique zhang et al 2001 and mesh subdivision technique wu et al 2017 are adopted in the presence of vertical downward inflow the numerical simulation of the development process of horizontal near wall single orifice ventilated 3d bubble was carried out and the simulation results are basically consistent with the experimental results then the effects of inflow velocity ventilation rate diameter of the orifice and surface tension coefficient on the flow characteristics of the ventilated bubble were studied the following conclusions were obtained in the case of inflow the inflow velocity is the main factor affecting the bubble development process as the inflow velocity increases the growth rate of bubble length thickness and width increases at the beginning of bubble development however when the inflow velocity is larger the bubble tends to shrink in later stage of development when the inflow velocity is smaller the thickness and width of the bubble are continuously increased in a period of time the length thickness and width of the bubble increase as the diameter of the orifice increases for a smaller diameter as the time increases the bubble features first increases and then decreases when the diameter is larger the bubble thickness first decreases and then tends to be stable and the bubble width is basically unchanged and the length and volume are continuously increased for the initial value of the diameter within a certain range the variation of the bubble length width and thickness with time is consistent ventilation rate is an important parameter of the ventilation system and has some influence on the development of the bubble for the ventilation model used in this paper in a certain ventilation rate range the greater the ventilation rate the greater the development speed and maximum value of bubble length thickness and width the surface tension coefficient has little effect on the bubble characteristics and the degree of influence decreases as the inflow velocity increases acknowledgements this work is supported by the national natural science foundation of china grant no 51509047 the fundamental research funds for the central universities of ministry of education of china grant no heucfp201743 and no heucfp201742 and the national defense basic scientific research program of china grant no jcky2016604c003 
22691,bubbling from near wall orifices is widely used in ocean engineering such as ship drag reduction sewage treatment underwater vehicle control technology and so on but at present most studies on ventilated bubble do not take into account the influence of the wall on the development of the bubble based on the potential flow theory combined with mesh smoothing technique and mesh subdivision technique the boundary element method is adopted to study the 3d ventilated bubble growth in this paper the bubble is generated by ventilation during the vertical upward movement of the rotary body with a single horizontal orifice on the wall and the convergence of the model is studied by modifying the mesh size meanwhile the numerical simulation results agree well with the experimental results finally with the existence of the inflow the effects of different inflow velocities ventilation rates the diameters of the orifice and surface tension coefficients on the development process and flow characteristics of the bubble were studied the results show that the inflow velocity is the main influencing factor of the bubble development process the ventilation rate and the diameter of the orifice have some influence on the development process of the bubble and the surface tension coefficient has little effect on the development process of the bubble keywords bubble formation near wall single orifice boundary element method bubble dynamics 1 introduction the process of ventilated bubble formation occurs in various ocean engineering such as ship drag reduction sewage treatment underwater vehicle control technology and so on there are many ways to achieve ventilation single orifice ventilation is the basis of ventilation technology and it is also the basic problem of gas liquid two phase flow it has many influencing factors different gas flow rates liquid flow rates liquid surface tension coefficients orifice diameters gas characteristics etc can cause differences in the flow characteristics of the ventilated bubble when there is liquid inflow the horizontal single orifice ventilated bubble is mainly affected by fluid drag force buoyancy surface tension and gas momentum force different liquid inflow velocities and ventilation velocities will result in different forms of ventilated bubble such as floating bubble jet etc for single orifice ventilated bubble in theoretical and experimental research harby et al 2017 proposed and verified a mathematical model in the mathematical model the rate of entrainment is assumed to be a function of the jet centerline velocity the ratio of the mean jet and the ambient densities the experiments about bubble formation development and detachment were carried out with parameters badam et al 2007 vafaei 2010 bari and robinson 2013 hu et al 2016 besides harby et al 2017 li et al 2018 conducted an experimental study on the gas jet formed by horizontal ventilation in numerical research xu et al 2013 islam et al 2015 applied volume of fluid vof method das and das 2009b 2012 applied lagrangian smoothed particle hydrodynamics sph method to successfully simulate the development of single orifice ventilated bubble oguz and zeng 1997 higuera and medina 2006 applied the boundary element method to simulate the development of axisymmetric bubble oguz and zeng 1997 wu et al 2017 simulated the growth and detachment of single orifice ventilated three dimensional bubble under static and inflow conditions by the boundary element method for the study of single orifice ventilated bubble most of them do not consider the influence of wall on the bubble flow characteristics but in practical applications the single orifice is generally opened on the wall such as applied to ship drag reduction underwater vehicle control technology etc in experimental research kodama et al 2000 song et al 2018 verified the drag reduction effect of ventilation through experimental research bai 2001 studied the gas flow rate liquid flow rate orifice diameter and gas characteristics in the bubble formation process through horizontal single orifice ventilation experiments vigneau et al 2001 conducted an experimental study on the shapes of horizontal near wall single orifice ventilated bubble under conditions of downward flow lee 2015 studied the effect of froude number on the shape of gas jets in the vertical ventilation experiments at the bottom of the ship model the numerical simulation has high efficiency and low cost and can be compared with experiments results it is an indispensable research method in the study of near wall ventilated bubble chen et al 2009 studied the effects of the contact angle and the contact line models on the bubble formation from submerged orifices using a 2d axisymmetric numerical scheme involving a level set method for tracking the two phase interface mohanarangam et al 2009 used multiple size group musig based on population balance model to study the phenomenon of drag reduction by the injection of micro bubbles the vof method was used by rana et al 2017 to simulate the bubble bubble interaction upon formation at submerged orifices mirsandi et al 2018 extended the local front reconstruction method by incorporating contact angle dynamics and verified the model by experimental and numerical data the boundary element method is accurate for the simulation of the development of the bubble oguz and zeng 1997 wu et al 2017 it can track the interface deformation directly calculate the interface velocity and can better simulate the steady motion bubble attached to the wall moreover the dimension of the problem can be reduced by one dimension and the operation rate is higher than other algorithms the boundary element method was used by xiao and tan 2005 to simulate the development of flat plate single orifice ventilated axisymmetric bubble before detachment in this paper combined with mesh smoothing technique zhang et al 2001 and mesh subdivision technique wu et al 2017 a three dimensional near wall ventilated bubble dynamics model is established by the boundary element method then the convergence of the model is studied by modifying the mesh size and the accuracy of the model was verified by comparing with the experimental results after that the numerical simulation of the development process of horizontal near wall single orifice ventilated 3d bubble under inflow conditions was carried out and the effects of inflow velocity ventilation rate orifice diameter and surface tension coefficient on bubble flow characteristics were analyzed 2 problem formulation in this paper the potential flow theory is used to establish a numerical model of underwater near wall single orifice ventilated bubble including 1 the governing equation for bubble motion 2 the equation of gas for underwater ventilated bubble fig 1 shows the physical model of this paper the research focus of this paper is on the flow characteristics of ventilated bubble before detachment as shown in fig 1 the gas is not directly filled into the fluid through the orifice but is first filled into the air chamber c at ventilation rate q and then filled into the fluid through the orifice to form a ventilated bubble in this paper ventilation rate at the orifice is q u m represents the speed of the model movement p 0 p ρ l g h represents hydrostatic pressure at the orifice v c is the volume of the air chamber v b is the volume of the bubble the diameter of the orifice is r 0 in the numerical model the center of the orifice is the origin of the coordinate system and r axis and z axis are shown in fig 1 in addition the physical model needs to be reasonable and appropriate simplified liquid inflow velocity u equivalent model motion velocity u m meanwhile a small round orifice is opened on the wall of the cylindrical chamber with a larger radius which can be approximated as an orifice on the flat plate in addition as shown in fig 2 the length width and thickness of the bubble are respectively defined as the difference between the maximum and minimum values of the node coordinates in the corresponding direction 2 1 governing equation assuming that the flow field is irrotational non viscous and incompressible the velocity potential ϕ satisfies laplace s equation 1 2 φ 0 from the infinity boundary condition and the green function the boundary integral equation can be obtained 2 λ ϕ p s ϕ q n g p q ϕ q n g p q d s where p is a fixed point on the boundary q is the integration point on the boundary λ is the solid angle of the flow field observed at point p s is the boundary surface including the bubble surface and the wall surface green function g p q p q 1 in the system of near wall single orifice ventilated bubble the development process of the bubble and the entire flow field are affected by the wall surface in this model the influence of the wall on the development of the bubble is taken into account by using the principle of the image and the processing of the wall mesh in the model can be simplified thus the green function in equation 2 is taken as g p q p q 1 p q 1 where q is the image of q in the wall considering the inflow conditions in the flow field the total velocity potential in the flow field can be decomposed into the disturbance velocity potential ϕ d induced by the existence of the bubble and the flow incident velocity potential ϕ c as follows 3 ϕ ϕ c ϕ d u 0 z ϕ d where u 0 is the velocity of steady uniform flow since the problem of steady uniform flow is studied in this paper the key is to solve the disturbance velocity potential induced by bubble the total velocity potential ϕ and the disturbance velocity potential ϕ d in the flow field satisfy the laplace equation and ϕ d disappears in the far field to be zero so the boundary integral equation can be used to solve the disturbance velocity potential 4 λ ϕ d p s ϕ d q n g p q ϕ d q n g p q d s the integral equation 4 can be reduced to a matrix form chahine et al 1995 5 h ϕ g ϕ n substituting equation 3 into the bernoulli equation we can get the following equation wu et al 1998 koo and kim 2007 6 d ϕ d d t p p l ρ 1 2 ϕ 2 g z ϕ ϕ d where ρ is density of fluid z represents the vertical component of the node position p represents the pressure at infinity on the plane z 0 p l is the pressure at every node on the surface of the bubble ϕ ϕ c ϕ d u 0 z ϕ d this formula updates the disturbance velocity potential of the bubble surface under inflow conditions 2 2 the equation of gas for the entire ventilation system from the law of conservation of mass and the first law of thermodynamics applied to the bubble and the chamber respectively we can get the following equations xiao and tan 2005 7 v b d p b d t γ p b d v b d t γ p c q 8 v c d p c d t γ p a q γ p c q the volumetric flow q through the orifice is 9 q ρ b ρ c d v b d t substituting q into equation 8 and assuming the density is constant ρ b ρ c p a p c it follows 10 p c t p c 0 c 0 2 ρ c v c q t v b t v b 0 where c 0 2 γ r t c initially if we assume q q it follows that 11 p b 0 p c 0 q 2 ρ c k 2 π r 0 2 2 where k is the orifice coefficient according to formula 10 and 11 we can get 12 p b t p b 0 c 0 2 ρ c v c q t v b t v b 0 ρ c k 2 π r 0 2 2 q 2 d v b d t 2 the real time pressure of the bubble surface can be obtained from p b 13 p l p b σ κ where p b is the pressure in the bubble σ is the surface tension coefficient κ is the curvature of each node of the bubble 3 numerical scheme in order to obtain a numerical simulation result with good convergence and at the beginning of bubble development the bubble is transformed from spherical crown into hemisphere rapidly bai and thomas 2001 das and das 2009a islam et al 2015 thus the initial shape of the bubble is set to a hemisphere having the same diameter as the diameter of the orifice the model assumes that the initial value of the internal pressure of the bubble is the sum of the static pressure of the corresponding depth of the orifice and the surface tension of the bubble 14 p b 0 p o 2 σ r o 3 1 curvature of bubble surface in the 3d model we must face the numerical problem of solving the curvature of irregular 3d surfaces to solve it the idea of the literature liu et al 2007 is adopted and a concrete solution process of the node curvature is given as shown in fig 3 it is partial elements of the bubble surface the normal vector of the selected node h is n k and the edge connected to the point h is h h i i 1 2 m suppose a plane p passes through the normal vector n k and the edge h h i then the intersection of the plane p and the bubble surface is c s and s is the corresponding arc length in the plane p the tangential vector of the node h along the curve c s is τ k the element normal vector of the selected node is obtained by weighted averaging of the normal vectors corresponding to the surrounding triangles h h i h i 1 of the selected node plesset and chapman 1971 15 n k i 1 m n k i s i i 1 m 1 s i where s i is area of the triangular element h h i h i 1 then using the taylor formula expand the curve function c s at s s k it follows that 16 c s c s h τ s h s s h k s h n s h s s h 2 2 o s 3 then do the dot product between n k and both ends of formula 16 remove the high order infinitesimal amount and replace the arc length s s k with the length of the line segment of the adjacent node h i h we can get liu et al 2007 17 k s h 2 n h h i h h i h 2 assume that the angles of the triangles near the point h are θ i i 1 2 m respectively according to the curvature formula bronshtein and semendyayev 1997 the relationship between the curvature κ of the selected node and the corresponding curvature k n θ in the normal interface of the adjacent element is as follows 18 κ 1 π 0 2 π k n θ d θ 1 π i 1 m k i k i 1 2 θ i in the formula the integral value is approximated by the trapezoidal principle k i is obtained by 17 and finally the surface tension is obtained by multiplying the obtained curvature by the surface tension coefficient 3 2 mesh subdivision in the numerical simulation of 3d bubble when the bubble is gradually deformed the node spacing becomes larger and as a result the nodes near the orifice become sparse meanwhile the mesh at the end of the bubble is too dense these will cause the mesh distortion of the calculation process the calculation efficiency is greatly reduced and even the calculation is terminated if the numerical fault is simply eliminated by adding or removing nodes the topology of the bubble surface will also change dramatically this not only increases the difficulty of programming but also destroys the accuracy of the original mesh bringing unpredictable errors to the simulation process since the factors affecting the quality of the mesh mainly include the relative size of the mesh and the regularity of the elements in order to measure the degree of regularity of the triangular elements in the 3d model this paper introduces the concept of the triangle regularity r g 19 r g l p l 1 l p l 2 l p l 3 max l i where l p c 2 c is circumference of the triangle l i i 1 2 3 represents the length of three sides on the decision condition of the mesh subdivision it is considered that when any one edge of the triangle is larger than the specified edge size δ l 0 27 mm 0 28 mm 0 29 mm or 0 30 mm the edge l i of the triangle needs to be subdivided in the subdivision process the midpoint of the subdivision edge is taken as the subdivision point therefore there are three types of triangulation element subdivision modes subdivision of one edge two edges or three edges as shown in fig 4 for subdivision of one edge and three edges only the diagonal vertices and the midpoints need to be connected however for subdivision of two edges two subdivisions can be generated after subdividing one edge namely c d and a e to this end we select a subdivision scheme with a higher degree of element regularity 3 3 mesh smoothing technique in the process of simulating bubble development using the boundary element method since the discrete mesh does not completely conform to the smooth bubble surface the generation and accumulation of random errors will cause the mesh to be distorted and deformed resulting in non convergence of numerical calculation for this numerical problem this paper adopts a three dimensional mesh smoothing technique based on least square method zhang et al 2001 a portion of the surface elements are selected near the update node consisting of two turns of nodes around the selected node the number of nodes satisfies the minimum number of nodes required to use least square smoothing the whole process is equivalent to using a node as the origin o to establish an independent coordinate system o x y z where the direction of the axis z is the normal direction of the node according to the entire new coordinate system a function of the fourth power is selected to fit the local bubble surface 20 z a 1 x 2 a 2 x y a 3 y 2 a 4 x a 5 y a 6 f x y where a i i 1 6 are the coefficient that is optimized by calculating the error function in addition to being able to directly optimize the bubble node coordinates for a smoother surface the same method is equally suitable for optimizing the velocity potential ϕ on a node in the process of simulating the near wall single orifice ventilated 3d bubble the mesh distortion usually occurs at the edge of the orifice as shown in fig 5 a the inside of the circle is the mesh with distortion fig 5 b shows the mesh after smoothing the three dimensional smoothing technique can ensure the smooth progress of the numerical simulation process 4 convergence and validity of the model 4 1 convergence study the convergence of the numerical model about the mesh size is discussed in this section the size of the triangular element δ l is 0 27 mm 0 28 mm 0 29 mm and 0 30 mm respectively the other parameters in the model are the diameter of orifice r 0 is 4 mm inflow velocity u is 1 20 m s volumetric flow q is 15 cm3 s surface tension coefficient σ is 7 27 10 2 n m in addition in the model chamber volume v c and height of water h are constant which are 785 5 cm3 and 15 cm respectively fig 6 shows the convergence study of bubble length for different mesh sizes it can be seen from the bubble length variation curve that the length variation is extremely small as the mesh size decreases the mesh fineness increases continuously and the numerical solution converges to the same simulation result fig 7 and fig 8 respectively illustrate the convergence of bubble width and thickness for different mesh sizes they all converge to a numerical solution as the mesh size decreases it is worth mentioning that when the mesh size is approaching from 0 3 mm to 0 27 mm if a gradual calculation of the size value of 0 27 mm is the benchmark the calculation errors of the other three sizes are 9 01 7 12 and 0 62 from the reduction of the relative error it can be clearly seen that the numerical solution converges when the mesh size approaches 0 27 mm because there is no need for too many nodes to affect the computational efficiency the mesh size selected in the model is 0 28 mm 4 2 validity verification before using the bubble dynamics model to study the bubble variation characteristics it is necessary to verify its validity in this paper the self developed program is used to numerically simulate the bubble development process and compare it with the actual experimental data to verify the validity of the model the ventilation process is divided into two stages before and after the bubble is detached this paper mainly considers the development process before the bubble is detached in addition since the initial diameter of the bubble r 0 in the numerical model is the same as the diameter of the orifice r obviously this process does not exist in the generation of bubble in the experiment this may cause some errors at the beginning of the simulation the parameters are set as follows r 0 4 0 mm u 1 20 m s q 15 cm 3 s σ 7 27 10 2 n m the bubble shapes of the experimental and numerical simulation are shown in fig 9 it can be seen that the shape of the bubble obtained by numerical simulation is basically consistent with the experiment however in the experiment the gas source was supplied by a peristaltic pump and the flow rate was pulsed which could not guarantee a completely continuous stable flow supply such the shape of the bubble obtained by numerical simulation is smoother than the experiment the inflow velocity of the working condition is relatively large the liquid shearing force of the bubble is larger than the buoyancy of the bubble the bubble moves downward relative to the orifice and the bubble is vertically stretched and the bubble adheres to the wall surface due to the horizontal shearing force of the liquid in the upper part of the bubble near the orifice the thickness and width of the bubble are small and in the lower part of the bubble away from the orifice the relative velocity of the gas and the liquid is reduced the horizontal shear force is reduced the gas is diffused to both sides and the thickness and width of the bubble are increased fig 10 shows the comparison of length thickness and width of the bubble with time in experimental results and numerical simulations results according to fig 10 in the simulation results the development process and dimensional change of the bubble are basically consistent with the experimental data among them the growth rate of the width and thickness of the bubble tends to be gentle over time and the thickness even has a tendency to decrease the time at which the growth rate tends to be gentle is approximately the same reaching the state at about 30 ms in addition the length width and thickness are affected by the longitudinal inflow and there are many factors affecting the development of them for example the change of the depth causes the gas inside the bubble to expand and the influence of the vortex caused by the viscosity these cause some errors in the numerical simulation results relative to the experimental results 5 results and discussions 5 1 effect of inflow velocity in the case of downward inflow the bubble characteristics are not only affected by the internal pressure of the bubble but are mainly affected by the drag force in this section u is 0 5 m s 1 0 m s 2 0 m s and 3 0 m s respectively the other parameters are set as follows r 0 4 0 mm q 15 cm 3 s σ 7 27 10 2 n m fig 11 shows the comparison of the bubble formation evolution with different inflow velocities when the velocity is small the bubble is more fat and the relative development speed in the width direction is greater than that when the inflow velocity is large moreover when the inflow velocity is small the liquid drag force is small and as the bubble volume increases the buoyancy has a significant effect and the bubble tail is flat when the inflow velocity is large the liquid drag force plays a major role in the development of the bubble and the bubble mainly develops in the length direction as shown in fig 12 a when the uniform inflow velocity is in the range from 0 5 m s to 3 0 m s the bubble develops faster in the longitudinal direction as the inflow velocity increases as shown in fig 12 b when the inflow velocity is small the maximum volume that the bubble can reach is larger than the case where the velocity is large but the development speed of volume is slow when the inflow velocity is large the bubble volume develops faster but the maximum volume is small after reaching the maximum volume the bubble volume is reduced the main reason for this situation is that the large inflow velocity will cause the bubble to expand rapidly and the internal pressure of the bubble will decrease because the ventilation rate cannot satisfy the bubble volume expansion rate when the bubble volume reaches the peak value the bubble will shrink and rebound due to the difference between the pressure inside and outside of the bubble and eventually stabilizes near a certain value as shown in fig 13 when the inflow velocity is 3 m s and 2 m s the inflow velocity is large in the initial stage of development the width and thickness of the bubble increase at a high speed but as the flow velocity increases the maximum of width and thickness do not change much and are basically the same when the inflow velocity is 0 5 m s the thickness and width of the bubble are higher than the other three conditions and the attenuation of the thickness and the width is delayed when the volumetric flow is the same for the low velocity inflow the ventilation rate can compensate for the decrease of the pressure inside the bubble caused by the drag force and as time increases more gas flows into the bubble to form a relatively stable bubble when the inflow velocity is large the bubble rapidly expands the time to reach the peak is too short the amount of gas introduced into the bubble is little and the pressure inside the bubble is low so that shrinkage occurs in the thickness and width directions 5 2 effect of ventilation rate since the gas is regarded as an incompressible fluid throughout the process the gas flow takes a volumetric flow for the numerical model of this paper different ventilation rates will change the speed at which gas is injected into the water which in turn affects the development of the bubble in this section q is 15 cm3 s 25 cm3 s 35 cm3 s and 45 cm3 s respectively the other parameters are set as follows u 1 0 m s r 0 4 0 mm σ 7 27 10 2 n m fig 14 shows the comparison of the bubble formation evolution with different ventilation rates fig 15 a shows the time evolution of bubble volume with different ventilation rates the ventilation rate increases from 15 cm3 s to 45 cm3 s in the early stage of bubble development the effect of ventilation rate on bubble formation evolution is small before 13 ms the bubble volume was mainly affected by the drag force which increased linearly however over time after 13 ms the bubble with small ventilation rate could not satisfy the continuous increase of its volume the internal pressure of the bubble decreased and the decrease of difference between the internal and external pressure caused the volume growth rate to decrease in addition the bubble volume growth rate does not always increase with the increase of ventilation rate but tends to be gentle as the ventilation rate increases the main reason for this phenomenon is that the gas does not directly enter the inside of the bubble but first enters the gas chamber generates a buffer and then enters the inside of the bubble when the volume of the gas chamber is large even if the ventilation rate is large due to the limitation of the orifice the charged gas still cannot compensate for the pressure drop inside the bubble caused by the bubble volume increase fig 15 b shows the time evolution of bubble length with different ventilation rates the effect of different ventilation rates on the length of the bubble is small in the early and middle stages of bubble development but at each time point the length of the bubble with a large ventilation rate is large over time the bubble length no longer increases linearly only when the ventilation rate is small at which time the internal pressure of the bubble is low and the rate of growth in the length direction becomes slow fig 16 shows the time evolution of bubble thickness and width with different ventilation rates before 13 ms the effect of ventilation rate on thickness and width was small after 13 ms the bubble thickness increased with the increase of ventilation rate for the bubble width except for the case where q is 15 cm3 s is significantly smaller than the other three cases the thickness difference is small at the end of the curve of the other three cases therefore within a certain range the bubble width increases with the increase of the ventilation rate and when the ventilation rate reaches a critical value the influence of the ventilation rate on the bubble width is small 5 3 effect of orifice diameter in the numerical model of this paper the initial diameter of the bubble is equal to the diameter of the orifice by changing the diameter of the orifice and selecting the typical time the effect on the bubble formation evolution is studied in this section r 0 is 4 mm 8 mm 12 mm and 16 mm respectively the other parameters are set as follows u 1 0 m s q 15 cm 3 s σ 7 27 10 2 n m fig 17 shows the comparison of the bubble formation evolution with different diameters of the orifice when the diameter of the orifice is small the bubble is conical from top to bottom when the initial diameter is larger the initial volume of the bubble is larger and the internal pressure of the bubble is lower due to the constant ventilation due to the influence of surface tension and buoyancy the tendency of the downward movement of the bubble tail is slowed down and due to insufficient ventilation the internal pressure of the bubble is low and the shape of the bubble shrinks toward the middle it can be predicted that if the diameter of the orifice is continuously increased the bubbles will be quickly detached fig 18 a shows the time evolution of bubble thickness with different diameters of the orifice when the initial diameter is large such as 12 mm and 16 mm the bubble thickness gradually decreases and tends to 5 mm and 6 mm respectively however in the initial stage of bubble formation both the ventilation effect and the drag force of the inflow should increase the thickness of the bubble the main reason for this phenomenon is that in order to obtain a numerical simulation result with good convergence the initial shape of the bubble is set to a hemisphere having the same diameter as the diameter of the orifice but the process cannot occur in an actual situation because once a bubble is formed in the presence of an inflow the bubble will move downward forming an abdomen therefore in the initial stage of numerical simulation the simulation is not in line with the actual situation however as time progresses the thickness gradually approaches a value indicating that the model can guarantee better convergence fig 18 b shows the time evolution of bubble width with different diameters of the orifice when the diameter is 16 mm the bubble width does not substantially change when the diameter is 12 mm the width fluctuates around 12 mm in contrast when the diameter is 8 mm the bubble width also fluctuates constantly but the overall rise slightly when the diameter is 4 mm the time evolution of the width of the bubble is more consistent with the conventional change course that is the width of the bubble first increases and then becomes flat fig 19 shows the time evolution of bubble length and volume with different diameters of the orifice under the same inflow condition as the initial diameter of the bubble increases the bubble volume increases it is worth noting that when the diameter is 4 mm the growth rate of the length and volume of the bubble is much faster than the other three cases in the other three cases the bubble length tends to be flat as time is 27 ms 5 4 effect of surface tension coefficient the effect of surface tension on the development of bubble formation evolution was studied by changing the surface tension coefficient according to the weber number w e ρ r 0 u 2 σ the excessive inflow velocity will greatly increase the influence of inertial force in the system even if the surface tension coefficient is changed the influence of surface tension cannot be compared with the inertial force therefore the inflow velocity in this section takes a smaller value u 1 0 m s in this section σ is 3 0 10 2 n m 3 4 10 2 n m 4 9 10 2 n m and 7 27 10 2 n m respectively the other parameters are set as follows u 1 0 m s r 0 4 0 m m q 15 c m 3 s fig 20 shows the comparison of the bubble formation evolution with different coefficients of surface tension as the surface tension increases the bubble is more fat it can be seen from fig 21 that the surface tension has little effect on the change of bubble formation evolution in the presence of inflow especially the effect on the width and length of the bubble and the four curves almost coincide only in the thickness of the bubble as the surface tension coefficient increases the thickness decreases slightly 6 conclusions in this paper a three dimensional near wall ventilated bubble dynamics model is established by the boundary element method and mesh smoothing technique zhang et al 2001 and mesh subdivision technique wu et al 2017 are adopted in the presence of vertical downward inflow the numerical simulation of the development process of horizontal near wall single orifice ventilated 3d bubble was carried out and the simulation results are basically consistent with the experimental results then the effects of inflow velocity ventilation rate diameter of the orifice and surface tension coefficient on the flow characteristics of the ventilated bubble were studied the following conclusions were obtained in the case of inflow the inflow velocity is the main factor affecting the bubble development process as the inflow velocity increases the growth rate of bubble length thickness and width increases at the beginning of bubble development however when the inflow velocity is larger the bubble tends to shrink in later stage of development when the inflow velocity is smaller the thickness and width of the bubble are continuously increased in a period of time the length thickness and width of the bubble increase as the diameter of the orifice increases for a smaller diameter as the time increases the bubble features first increases and then decreases when the diameter is larger the bubble thickness first decreases and then tends to be stable and the bubble width is basically unchanged and the length and volume are continuously increased for the initial value of the diameter within a certain range the variation of the bubble length width and thickness with time is consistent ventilation rate is an important parameter of the ventilation system and has some influence on the development of the bubble for the ventilation model used in this paper in a certain ventilation rate range the greater the ventilation rate the greater the development speed and maximum value of bubble length thickness and width the surface tension coefficient has little effect on the bubble characteristics and the degree of influence decreases as the inflow velocity increases acknowledgements this work is supported by the national natural science foundation of china grant no 51509047 the fundamental research funds for the central universities of ministry of education of china grant no heucfp201743 and no heucfp201742 and the national defense basic scientific research program of china grant no jcky2016604c003 
22692,to date the increasing density of water traffic has caused the ship s navigation environment to deteriorate resulting in frequent water traffic accidents in addition a majority of maritime accidents are caused by human factors and one of the important ways to solve the ship accidents caused by human factors is to utilize intelligent maneuvering of ships based on the actual crews operational data from full task handling simulation platform this study combines a 30 000 ton bulk carrier inbound navigation scenario and uses the decision tree method to propose a knowledge learning model under multiple environmental constraints to give intelligent ships the ability to make decisions like a human an intelligent ship human like decision making maneuvering decision recognition hdmdr model the decision making mechanism for the maneuvering behavior of officer on watch oow under the influence of the specific water traffic environment in the inbound scenario is analyzed and the oow s decision making knowledge is automatically acquired and represented the validation tests and the comparative analysis with the classic classification algorithms of k nearest neighbours k nn and support vector machine svm are performed to demonstrate the accuracy of the proposed hdmdr model this paper provides a feasible basis for the human like decision making analysis of intelligent ships keywords c4 5 algorithm ship maneuvering decision making intelligent ship classification rule data mining 1 introduction driven by economic globalization the volume of trade between countries around the world continues to rise and higher demands are placed on the transportation of goods due to its large volume and low cost waterway transportation plays an increasingly important role in cargo transportation it bears the main task of world cargo circulation and is the main means of trade transportation currently waterway transportation accounts for 95 of total crude oil transportation and 99 of total iron ore transportation it is an irreplaceable transportation method however with the increasing number of vessels and the increasingly busy routes the environmental pollution related to waterway transportation the high labor costs and the lack of safety have also received more attention lun et al 2016 in recent years the development of technologies such as information computers communications networks new energy artificial intelligence application of the internet of things big data integrated bridge systems and information physics systems have greatly advanced the process of ship intelligence and made real green safe efficient and unmanned intelligence ships a possibility at the same time water transportation is recognized as a high risk industry with the development of the domestic economy and world trade transportation is becoming increasingly busy the number of ships is increasing ships are becoming larger and more specialized and the speed of ships is increasing coupled with the increase in the transportation of dangerous goods the density of water traffic is increasing and the navigation environment of ships is deteriorating causing frequent water traffic accidents which causes people to pay more attention to the risk of navigation akyuz and celik 2014 goerlandt and montewka 2015 according to statistics hanzu pazara et al 2008 in ship collision accidents 89 96 of accidents are caused directly or indirectly by human factors and one of the important ways to solve ship accidents caused by human factors is to utilize intelligent maneuvering of ships in addition the safety of the crew in extreme weather conditions in recent years has also become a problem that cannot be ignored wang et al 2014 besides the number of crews is declining recently while the wages of crew are rising year by year which has become the second largest expenditure item after the fuel costs of shipping lun et al 2016 as intelligent ships have outstanding advantages in improving the safety management energy consumption management and operational efficiency of ships therefore the researches for intelligent ships have become an inevitable trend for future ship development and gained the interest of many researchers in both academia and private sectors goerlandt and montewka 2015 in addition the natural environment is an important factor affecting the safety of waterborne traffic zhang et al 2018 among the natural environmental factors surrounding the ship meteorological conditions walrus conditions topographical environments and water facilities will bring restrictions to the navigation of the ship these factors affect the ship s navigation and the crew s decisions by affecting the ship s maneuverability along with the skill and mentality of the shipper the natural environmental factors that typically affect the safe environment of maritime traffic are weather conditions and ocean conditions specifically wind current and waves intelligent ships use sensors communications internet of things the internet and other technical means to automatically sense and obtain information and data on the ship itself the marine environment logistics ports etc based on computer technology automatic control technology big data processing and analysis technology it utilizes intelligent operation in ship navigation management maintenance cargo transportation etc lazarowska 2017 making ships safer more environmentally friendly more economical and more reliable intelligent here can be understood as human like thinking it can comprehensively consider the specific tasks and various information obtained and develop a series of optimal decisions that meet the safety requirements of the ship s navigation economy and environment it takes a long transition period for an intelligent ship to fully realize unmanned maneuvering presently although the current level of ship automation is relatively high the normal operation of ships is always inseparable from human participation perera et al 2015 even in an unattended cabin the crew must be handled when an emergency occurs although the ship is maneuvered by satellite navigation electronic compass electronic channel map and automatic rudder the bridge has not been unmanned intelligent ship technology has developed rapidly in recent years however there are still many problems need to be solved in addition the existing research does not form a set of theoretical methods to solve the problem of autonomous learning of the intelligent ship for the maneuvering decision making characteristics of officer on watch oow and lacks the corresponding theoretical methods to solve the problem of intelligent ship human like maneuvering decision making modeling researchers have proposed several different decision tree algorithms see literature review for both classification and decision making problems based on different aspects and obtained good results based on the advantages of the c4 5 algorithm and the ability to analyze the characteristics of multifork trees this paper uses the c4 5 algorithm to learn the oow s maneuvering decision characteristics we regard the intelligent ship human like maneuvering decision making problem as a machine learning problem based on the oow s experience the oow s actual maneuvering data and the environmental influencing factors such as wind wave and current in specific water areas and the problem is converted using the decision tree c4 5 method to learn the oow s maneuvering decision making characteristics thus constructing a human like decision making model under multiple constraints in summary this study focuses on the concept of human like maneuvering for the intelligent ship and studies the human like decision making method of intelligent ships by establishing autonomous learning method of maneuvering decision making the maneuvering decision making rules of typical maneuvering style is explored and the processes of autonomous learning oows maneuvering decision making characteristics for intelligent ships are studied and the intelligent ship human like decision making model is constructed this study provides a new perspective and methodology for the development of intelligent ship technology in theory and practice and promotes the application and spreading of intelligent ships the main contributions of this study are as follows 1 a novel intelligent ship human like decision making maneuvering decision recognition hdmdr model is proposed 2 the standardization principle of environmental influencing factors and maneuvering decision making factors is developed 3 the decision making mechanism of the oow s maneuvering behavior is analyzed on the basis of the actual crews operational data from full task handling simulation platform and the oow s decision making knowledge under the specific environmental influencing factors in the inbound scenario is automatically acquired and represented 4 considering the high cost of using the real 30 000 ton ship to carry out this kind of experiment and the low feasibility of collecting the data of multiple voyages from the real world ship therefore it is unique and very valuable to obtain the experimental data operated by an experienced oow on the full task handling simulation platform in a certain time and space the structure of this paper is organized as follows initially section 2 reviews the literature section 3 briefly presents the proposed decision making model the experimental processes are introduced in section 4 section 5 details the experimental results and the performance of our optimization methodology the conclusions and future directions of research are addressed in section 6 2 literature review data mining is a process that uses analytical tools to extract information and knowledge including knowledge that is hidden unknown or incomplete but potentially useful from a large amount of incomplete noisy fuzzy and random data moreover data mining determines the relationship between models and data and uses it to make predictions aguiar pulido et al 2013 sanil 2001 the classification algorithm is a data analysis method belonging to predictive data mining its goal is to find models that accurately describe and distinguish data classes or concepts from important sample data sets such that they can be grouped into a data category based on the entity s attribute values and other constraints the current technologies and methods mainly include decision tree algorithms calistru et al 2015 xie et al 2003 bayesian classification and bayesian networks baksh et al 2018 neural networks kheradpisheh et al 2018 genetic algorithms peng et al 2015 rough sets zhang et al 2012 etc in the 1960s decision tree algorithm was initially proposed by hunt et al 1966 to minimize the cost of classifying an object quinlan 1986 decision trees can handle both categorical and numerical data and it is good at processing nonnumeric data which can eliminate a significant amount of data preprocessing work when dealing with numerical data through algorithms such as neural networks in addition the decision tree method is simple in structure and does not need much background knowledge in the process of learning second the decision tree model is more efficient and is more suitable for training sample sets that have large amounts of data third the computational tree algorithm has a relatively small amount of computation then the decision tree method typically does not require knowledge outside the training data and is good at processing nonnumeric data finally the decision tree method has a higher classification accuracy therefore the decision tree method is a key research direction in the field of machine learning muchoney et al proposed the classification algorithms of decision tree dt artificial neural network ann and maximum likelihood to analyze the land cover classification problem in central united states and the results show that the decision tree has the highest classification accuracy muchoney et al 2010 borak 1999 used decision trees to classify features from a large amount of data the results show that the tree based classifier can greatly reduce the dimensionality of the input data set without affecting the classification accuracy calistru et al 2015 proposed a novel parallel decision tree algorithm namely pdscart to process a larger amount of data stream records and construct the tree efficiently saunier et al 2011 used decision trees the k means algorithm and the hierarchical agglomerative clustering method to identify patterns in the traffic event database and analyze the relationship between interaction attributes and collision common decision tree algorithms are concept learning system cls angluin 1988 hunt et al 1966 iterative dichotomiser 3 id3 quinlan 1979 1986 c4 5 quinlan 1993 c5 0 bujlow et al 2012 pandya and pandya 2015 classification and regression trees cart calistru et al 2015 friedman et al 1984 chi squared automatic interaction detector chaid kass 1975 rodriguez et al 2016 etc the internal variables of each subsample are highly consistent and the corresponding variation impurity falls between different subsamples as far as possible all decision tree algorithms follow this criterion and the data set is partitioned into subsets with different statistical approaches such as entropy lakkakula et al 2014 gain ratio prasad and naidu 2013 gini coefficient prasad et al 2013 v et al 2013 etc a series of follow up decision tree programs such as id3 c4 5 and cart etc are all developed from cls among them the c4 5 algorithm developed based on id3 is currently one of the most famous and popular decision tree algorithms lu et al 2015 c4 5 is the most influential data mining algorithm identified by the ieee international conference on data mining icdm in december 2006 wu et al 2007 a comparative study of c4 5 and other learning algorithms shows that it can balance processing speed and error rate well lim et al 2000 c4 5 can convert the decision tree into an equivalent production rule solve the learning problem of continuous value data classify multiple categories increase the boosting technology and complete the processing of large databases more efficiently the c4 5 algorithm also deals well with continuous and discrete values and attributes with missing attribute values garcía laencina et al 2015 the c4 5 algorithm solves the above problem well however the id3 algorithm tends to favor more attributes and the data of discrete value attributes but not the attributes with continuous values nor the samples with missing values and is sensitive to noise hssina et al 2014 c5 0 mainly adds support for boosting which also uses less memory compared with the c4 5 algorithm it builds a smaller rule set therefore it is more accurate but c5 0 is a commercial software and the public cannot easily get the source code witten et al 2016 cart uses the training set and the cross validation set to continuously evaluate the performance of the decision tree to prune the decision tree thus achieving a good balance between training error and test error however cart and chaid only supports building binary trees while c4 5 allows two or more outcomes and supports binary or multifork trees wu et al 2007 several prior studies on the c4 5 dt could be found from the literature a prior study provost and domingos 2003 found that a c4 5 introduction learner without pruning and without node collapsing quinlan 1993 can achieve the best prediction accuracy a novel vfc4 5 was proposed by cherfi et al 2018 to build decision trees through reducing the number of cut points by using the arithmetic mean and median this algorithm could get excellent accuracy than a c4 5 algorithm reumers et al 2013 used c4 5 decision tree based model to infer activity types from global positioning system gps traces the results showed that the overfitting was minimal in addition the model enables researchers to infer activity types directly from activity start time and duration information obtained from gps data dai and ji 2014 proposed a parallel mapreduce algorithm to implement a typical c4 5 decision tree the experimental results indicated that the algorithm exhibits both time efficiency and scalability 3 methodology 3 1 decision tree a decision tree is a mathematical method that generates decision trees or decision tree rules by inductive learning of training samples and then classifies new data using decision trees or decision rules as a supervised case based inductive learning algorithm decision trees evolved from the artificial neural network method which is a method to solve complex decision problems through tree like logical thinking it can infer the classification rules of the decision tree representation from a set of unordered and irregular cases it typically forms a classifier and a prediction model which can classify predict and analyze the unknown data for knowledge discovery the decision tree consists of a root node a series of internal nodes and leaf nodes each node has only one root node and two or more leaf nodes and the nodes are connected by branches yuan and shaw 1995 each internal node of the decision tree corresponds to a collection of noncategory attributes with each edge corresponding to each possible value of the attribute the leaf nodes of the decision tree correspond to a category attribute value and different leaf nodes can correspond to the same category attribute value in addition to being represented in the form of a tree a decision tree can also be represented as a set of production rules in the form of if then each root to leaf path in the decision tree corresponds to a rule the condition of the rule is the rounding of all node attribute values on the path the rule s conclusion is the category attribute of the leaf node on the path compared with decision trees rules are more concise and easier for people to understand use and modify which form the basis of the expert system therefore in practical applications more rules are used the decision tree method consists of two main steps the first step is to use the training sample set to build and generalize a decision tree and build a decision tree model this process is actually a process of acquiring knowledge from the data and doing machine learning it is usually divided into two phases building and pruning the second step is the process of classifying new data using a built in decision tree the input of the decision tree learning algorithm is a set of training samples represented by attributes and attribute values and the output is a decision tree which can also be extended to other representations such as rule sets decision tree generation typically uses a top down recursive approach the optimal attribute is selected as the node of the tree by some method and the attribute values are compared on the node and the branch from the node is judged according to the different attribute values that correspond to the training samples the lower nodes and branches are repeatedly established in each branch subset and the growth of the tree is stopped under certain conditions and the conclusions are obtained at the leaf nodes of the decision tree to form a decision tree the decision tree is generated by performing decision tree learning on the training samples the decision tree can classify an unknown sample set according to the value of the attribute which is the decision tree classification fig 1 c shows an example of a typical binary decision tree based on the data shown in table 1 from fig 1 c we can see that a decision node attribute i e crossing orientation which represents the position of vessel 2 has two branches values i e right section and left section which represent the unique values for the specific attribute leaf node i e class which represents the crossing situation represents the class category or decision of each instance furthermore according to the colregs international regulations for preventing collisions at sea navigation rules which provide safe operation guidelines for maritime navigation as shown in fig 1 a if vessel 2 is at the left crossing section class a the vessel 2 should turn right and vessel 1 should keep its course if vessel 2 is at the right crossing section class b the vessel 1 should turn right and vessel 2 should keep its course shown as fig 1 b therefore the final decision can also be represented through the form of if then rule set shown as follows rule 1 if crossing orientation left then class a vessel 1 keeps course and vessel 2 turns right rule 2 if crossing orientation right then class b vessel 1 turns right and vessel 2 keeps course this example indicates a maritime problem of colregs situation the decision tree generated from collision avoidance operation in the encounter scenario of different crossing situations in this way the attributes taken together provide a zeroth order language for characterizing objects in the universe quinlan 1986 3 2 the proposed hdmdr model 3 2 1 theory information shannon 1948 proposed the information theory in 1948 and the amount of information on events could be calculated as follows 1 i s i p s i log 2 p s i where p s i is the probability of occurrence of event s i suppose that there are v mutually exclusive events s 1 s 2 s v and only one of them happens the average amount of information can be measured as follows 2 i s 1 s 2 s v i 1 v p s i log 2 p s i when p s i 0 then i s i p s i log 2 p s i 0 3 2 2 information entropy assume that d is the intelligent ship human like decision making training data set contains a set of m classes d stands for the total number of samples in data set d and s i is the number of samples in data set d that belongs to class s i i 1 2 m if we randomly select a sample from d and this sample belongs to class s i then we can get a prior probability of the event as follows 3 p i s i d the expected information also referred to as entropy needed to classify d into m classes is defined as 4 i s 1 s 2 s m i 1 m p i log 2 p i suppose a feature attribute a has n distinct values a 1 a 2 a n feature attribute a partitions d into n subsets d 1 d 2 d n d j is the number of samples in subset d j j 1 2 n and s j i stands for the number of samples in subset d j that belongs to class s i then the expected information is defined as 5 e a j 1 n d j d i s j 1 s j 2 s j i note that the smaller the entropy value is the higher the purity of the subset partition where m for a given subset d j 6 i s j 1 s j 2 s j i i 1 m p i j log 2 p i j 3 2 3 information gain and gain ratio the information gain of feature attribute a is expressed as follows 7 g a i n a i s 1 s 2 s m e a the split information is defined as 8 s p l i t a j 1 n d j d log 2 d j d where s p l i t a is the information generated by partitioning d based on the values of a it indicates the outcome of the test rather than the class to which the sample belongs the gain ratio could be calculated by the following 9 g a i n r a t i o a g a i n a s p l i t a 3 2 4 constructing the c4 5 decision tree c4 5 is an extension of id3 and was presented by quinlan 1993 id3 selects the attribute with the largest information gain value as the node of the tree as also shown by xue et al 2019 however c4 5 introduces the concept of information gain ratio and selects the attribute with the largest information gain ratio moreover each possible value is used as a branch of this node to recursively form a decision tree in addition c4 5 adds significant functions compared to id3 such as rules generation uncertainty processing functions and attribute discretization c4 5 overcomes the shortcomings of the id3 algorithm using information gain to select attributes when biasing the selection of more attributes and can build a decision tree with as simple a structure as possible while ensuring the accuracy of training set classification algorithm 1 depicts the procedures of the process of construction of the proposed maneuvering c4 5 decision tree of intelligent ship human like decision making maneuvering decision recognition hdmdr model algorithm 1 construct the proposed c4 5 decision tree of hdmdr model image 10380 3 2 5 pruning the decision tree the initial construction of the c4 5 decision tree is often complicated by the inclusion of a large number of classification attributes and branches and there are inevitably some errors namely noise this noise gradually accumulates in the decision classification process which will eventually cause the c4 5 decision tree to have a large deviation from the classification of the actual sample and the accuracy is reduced i e over fitting thus the c4 5 decision tree generated by the training set is very good for classifying the training set but it may not be ideal to use it to classify the new data set that does not participate in the decision tree generation process therefore the preliminary constructed c4 5 decision tree needs to be pruned and the purpose of pruning is to optimize the c4 5 decision tree or simplify the generated rules there are two kinds of decision tree pruning methods prepruning and postpruning for the problem of over fitting this study uses postpruning methods to eliminate branching anomalies caused by noise data and isolated points quinlan 1993 proposed using pessimistic error pruning to compensate for optimistic bias in tree generation during pruning due to the decision tree is generated from the training data set in most cases the decision tree is consistent with the training data set however when the decision tree is used to classify data other than the training data it is obvious that the error rate will be greatly increased the postpruning rule adopts the principle of minimum expected error rate i e starting from the root node of the tree and calculating the expected error rate that may occur for each branch node pruning no pruning if the node is clipped resulting in a higher expected error rate the subtree is retained otherwise the subtree is clipped and finally the c4 5 decision tree with the smallest expected error rate is obtained in this paper the upper limit of the quality confidence interval is used as the erroneous estimation under pessimistic conditions given a confidence level α 0 25 in the c4 5 algorithm the total number of errors obeys the bernoulli distribution then there is a probability equation 10 p f q q 1 q n μ 1 α α where n is the total number of instances under the pruned subtree e is the number of error instances that occur after pruning f e n is the actual observed error rate and q is the estimated error rate let z μ 1 α taking the upper bound of the confidence interval as the pessimistic error rate estimate of this node then the formula for calculating the false positive rate of the node 11 q f z 2 2 n z f n f 2 n z 2 4 n 2 1 z 2 n where f e n is the actual observed error rate and q is the estimated error rate set the maximum value of the expected false positive rate to c if the estimated false positive rate q after pruning is higher than c the original subtree is retained otherwise the subtree is cut and replaced with leaves after the pruning the inbound human like decision making tree is shown in fig 6 fig 2 is the basic process and framework for our proposed hdmdr model 4 experiments 4 1 scenario design and data collection in our experiment the simulator scenario was the shanghai waigaoqiao wharf and the ship was downstream of the berthing into the port we use a 30 000 ton bulk carrier as our experimental ship os1 33089 0 t 182 9 m long 22 6 m wide we define the process as when the ship s stern leaves the main channel near the port side of the boundary line in the electronic chart fig 3 b shows the initial boundary to the ship berths docked at the end of the cable fig 3 c shows the end boundary as a complete berthing process the experimental scenario is shown in fig 3 we collect the data from the full task handling simulation platform navi trainer professional 5000 which conforms to the imo stcw78 10 convention and the det norske veritas dnv from the maneuvering simulator laboratory in wuhan university of technology waterway road traffic safety control and equipment ministry of education engineering research center we collect the operational data of the exercises and assessment exams as our experimental data unlimited navigational class crew 4 groups of 96 people 32 45 years old skilled maneuvering level captain chief officer from table 2 we can get the average age of the crew participating in this experiment is 38 76 years old and their average piloting experience is 8 89 years the captains average age and piloting age are both higher than those of chief officers from fig 4 we can get the distribution of oows age and their piloting experience the ship handing and environment including inside and outside multisource information were collected on the ship s berthing process including the environment wind current wave etc control rudder order marine telegraph order 2 factors table 3 lists some of the training samples it should be noted that in our case the oow is the captain or chief officer although in the real situation the captain is not on duty the captain will go to the bridge only in special circumstances and if necessary the captain may take over the duty of the oow to maneuver the ship but it is an assessment and evaluation scenario in our experiment therefore the captain also acts as the oow in addition we regard the tugboat as a power plant system of target ship os1 to facilitate the ship s overall situation of a simplified analysis and we consider the tugs and the ship os1 as a whole dynamic model under the premise of this hypothesis the ship os1 completes the inbound operation through the combination of rudder orders and telegraph orders according to the actual navigational situation of its force and movement 4 2 standardization principle setting maneuvering decision making processes are often influenced by multisource information such as human ship and environmental factors these influencing factors act together to determine the next action strategy of the ship s oow for a particular person ship unit the overall reliability is constant for a certain period of time or during a trip therefore the person and ship factors have less influence on maneuvering decisions with the operation of the ship the oow s waterway and the environment will change with time and space and the changing waterway and environmental factors will have a greater impact on maneuvering decisions in this research we mainly focus on the environmental influencing factors and study their effect on the decision making of the oow based on the strategy and the current maneuvering environment the experienced oow can quickly and accurately make maneuver decisions thus laying the foundation for the study of human like maneuvering behavior for the application to intelligent ships we select six environmental influencing factors as the input of our proposed hdmdr model to study the decision making mechanisms for different maneuvering behaviors in order to let the maneuvering decision making knowledge to be automatically obtained and expressed along with higher decision making knowledge effectiveness it is typically necessary to divide the number of linguistic terms by experience yuan and shaw 1995 in this paper experimental data of each maneuvering decision making factor are trisected into three levels namely small a medium b and large c see table 4 to objectively describe the characteristics of each influencing factor and make it easier to describe how each factor influences final maneuvering decisions we select six environmental influencing factors as the input of our proposed model to study the decision making mechanisms for different maneuvering behaviors current direction current speed relative current direction relative wave direction relative wind direction relative wind speed in other cases the other new factors can also be upgraded according to algorithm 1 in section 3 2 4 using specific standardization principle the oow maneuvers the ship by operating different telegraph and rudder orders to change ship s speed and direction and to complete the ship s control table 5 shows the combining telegraph and rudder orders speed and course control respectively this control is a multidynamic process moreover it should be noted that in combination with the actual situation of the experimental scenario unlike the ship sailing on the open sea the oow needs to call the rudder and telegraph orders frequently in the inbound decision making ship handing process therefore in this paper we do not consider midships and stop engine regardless of the rudder angle and if the power output is 0 table 5 shows the standardization principle for output maneuvering decision making factors 5 results and discussion 5 1 standardizing of training set the data in table 3 are standardized according to the principle of standardization of maneuvering decision influencing factors in tables 4 and 5 the results are shown in table 6 5 2 constructing and pruning the decision tree the c4 5 algorithm can be divided into two phases first a certain attribute is selected according to the criterion of maximum information gain to divide the training set and the recursive call is performed until all the examples in each division belong to the same class then the established tree is pruned i e the branch established above the noise data is cut in the decision tree analysis approximately 80 of the data is randomly selected as the training set and the remaining 20 is used as the test set then through eqs 3 11 we could obtain the decision tree structure as shown in fig 6 partitioned into 3 parts part i ii and iii the number and proportion of different decisions are shown in fig 5 5 3 establishing maneuvering decision classification rules the result of our proposed hdmdr model is a set of classification rules in the form of if then each path from the root node to the leaf node constitutes a rule the characteristics of the internal nodes of the path correspond to the conditions of the rule and the classification of the leaf nodes corresponds to the conclusion of the rule as a result we can easily extract the human like decision making knowledge using the decision tree and rule set the optimized maneuvering decision recognition rule set is shown in table 7 from figs 5 and 6 and table 7 we can draw the following conclusions 1 it can be seen from the generated decision tree that y2 as the root node i e current speed is the most informative attribute of all samples in other words in the environment of the simulation experimental scenario the current speed in the environmental influencing factors has the most significant impact on the oow s maneuvering decision making followed by the relative current direction and current direction 2 the ordering of environmental factors provides the oow with a set of variables for decision making reference which has certain guiding significance for the formulation of maneuvering decisions 3 through the analysis of the rule set this designed scenario outputs a number of standardized maneuvering decision operations x1 12 50 and x3 23 50 x1 u1d1u2t2 maneuvering decision knowledge can be interpreted and conceptualized into linguistic term or operation order keep the propeller ahead and keep the current rudder angle port the same x3 u1d1u2d2 keep the propeller ahead and keep the current rudder angle starboard which is consistent with actual ship maneuvering experience 4 the specific factors and decision rule sets in a specific scenario obtained by the hdmdr model proposed in this paper can be used as an important reference for the intelligent ship human like decision making and can also be used to create a knowledge base of expert systems it has a high reference value and practical value for the development of intelligent ship s maneuvering algorithm 5 4 performances assessment 5 4 1 applying rules for classification we use the maneuvering decision making model proposed in this paper to identify the decision making data to be identified in table 8 we compare the recognition results with the actual ship maneuvering decisions and use the accuracy of the recognition to verify the validity of the model the standardized maneuvering decision making data are identified in table 8 using classification rules 33 29 and 37 and the recognition result is x13 x13 x13 x9 x9 and x15 this result is consistent with actual maneuvering decisions and demonstrates high reasoning efficiency the test data set was evaluated and validated using the generated decision tree model there were 135531 samples participating in the test accounting for 20 of the overall data set to assess the accuracy of the hdmdr model the data in the test data set is used for prediction and the degree of agreement between the test results and the actual situation is compared the accuracy of the proposed module acc could be calculated as 12 a c c t n t p t n t p f n f p where tn is true negatives tp is true positives fn is false negatives and fp is true positives the classification accuracy of our proposed hdmdr model using c4 5 decision trees based on the test data set can reach more than 81 6 5 4 2 comparative analysis to further validate the effectiveness of the hdmdr model in this paper we compare the performance of the proposed c4 5 decision tree algorithm with two classic classification algorithms k nearest neighbours k nn and support vector machine svm in our case we use the radial basis function rbf to conduct the svm and k 1 in the k nn besides we use classification accuracy shown as eq 12 to measure the proposed c4 5 algorithm in addition in this paper the code for the basic versions of k nn and svm classifiers is adopted from the waikato environment for knowledge analysis weka which is open source data mining software hall et al 2009 weka is a comprehensive software that implements many state of the art machine learning and data mining algorithms we conduct a ten fold cross validation 10 cv experiment using the data from training set 10 cv breaks data into ten sets equally then trains the classifier on nine data sets and uses it to test the remaining one data set repeating ten times like this and finally taking an average accuracy thus to compare the performance of the proposed c4 5 decision tree algorithm with k nn and svm the performance of different classifier algorithms on our data set is shown in table 9 and fig 7 according to the classification accuracy results the proposed method can achieve the highest accuracy among these three algorithms 6 conclusions with the development of the economy the continuous advancement of technology and the continuous increase of labor costs it has become an urgent trend to realize intelligent maneuvering of ships the purpose of this research is to recognize the automatic acquisition and representation of the oow s decision making knowledge and to provide a basis and reference for the development of decision making algorithms for intelligent ships in this paper a intelligent ship human like decision making maneuvering decision recognition hdmdr model and a novel standardization principle of maneuvering decision making factors are proposed for the learning of human like decision making mechanisms of intelligent ships by establishing an autonomous learning method of maneuvering decision making the processes of autonomous learning oows maneuvering decision making characteristics are studied in addition it is unique and very valuable to obtain experimental data operated by an experienced oow on the full task handling simulation platform in a certain time and space to validate the performance and effectiveness of our proposed model the assessment of applying rules for classification and the comparative analysis with the k nn and svm are compared according to the results the classification accuracy of our proposed hdmdr model can reach more than 81 6 in addition the proposed method is superior to the representative classification algorithms this study provides a new perspective and methodology for the development of intelligent ship maneuvering decision making technology in theory and practice promotes the application and spreading of intelligent ships under specific scenarios and is conducive to the development of water transportation in the direction of safety sustainability and economy nevertheless the hdmdr model still has some shortcomings which need to be improved in further research 1 the proposed method model is a data driven method we need more data to further train the machine learning model and improve recognition accuracy besides the feedback loop to inform the effect of this model still need to be optimized 2 the application scenarios of the proposed model still need to be enriched and more environmental influencing factors which may affect piloting decisions need to be added as well considering the specific situation thus to make the proposed model more widely applicable 3 the standardization principle of maneuvering decision making attributes need to be further detailed according to the actual navigation situation and more suitable for the real world ship handing orders specifically the combined rudder orders and telegraph orders thus further increasing the applicability of the model in the subsequent research we will study the classification of the influencing factors the fuzzy processing of data sets the detailed connection of our model with human behavior and their performance in specific navigational scenarios the application of multi navigation scenarios etc besides we will further collect the relevant data from the tugs and add the data to the model analysis to further optimize our proposed algorithm acknowledgments this study is supported by the national natural science foundation of china 51775396 61703319 u1764262 the major project of technological innovation of hubei province 2016aaa007 2017cfa008 and the china scholarship council 
22692,to date the increasing density of water traffic has caused the ship s navigation environment to deteriorate resulting in frequent water traffic accidents in addition a majority of maritime accidents are caused by human factors and one of the important ways to solve the ship accidents caused by human factors is to utilize intelligent maneuvering of ships based on the actual crews operational data from full task handling simulation platform this study combines a 30 000 ton bulk carrier inbound navigation scenario and uses the decision tree method to propose a knowledge learning model under multiple environmental constraints to give intelligent ships the ability to make decisions like a human an intelligent ship human like decision making maneuvering decision recognition hdmdr model the decision making mechanism for the maneuvering behavior of officer on watch oow under the influence of the specific water traffic environment in the inbound scenario is analyzed and the oow s decision making knowledge is automatically acquired and represented the validation tests and the comparative analysis with the classic classification algorithms of k nearest neighbours k nn and support vector machine svm are performed to demonstrate the accuracy of the proposed hdmdr model this paper provides a feasible basis for the human like decision making analysis of intelligent ships keywords c4 5 algorithm ship maneuvering decision making intelligent ship classification rule data mining 1 introduction driven by economic globalization the volume of trade between countries around the world continues to rise and higher demands are placed on the transportation of goods due to its large volume and low cost waterway transportation plays an increasingly important role in cargo transportation it bears the main task of world cargo circulation and is the main means of trade transportation currently waterway transportation accounts for 95 of total crude oil transportation and 99 of total iron ore transportation it is an irreplaceable transportation method however with the increasing number of vessels and the increasingly busy routes the environmental pollution related to waterway transportation the high labor costs and the lack of safety have also received more attention lun et al 2016 in recent years the development of technologies such as information computers communications networks new energy artificial intelligence application of the internet of things big data integrated bridge systems and information physics systems have greatly advanced the process of ship intelligence and made real green safe efficient and unmanned intelligence ships a possibility at the same time water transportation is recognized as a high risk industry with the development of the domestic economy and world trade transportation is becoming increasingly busy the number of ships is increasing ships are becoming larger and more specialized and the speed of ships is increasing coupled with the increase in the transportation of dangerous goods the density of water traffic is increasing and the navigation environment of ships is deteriorating causing frequent water traffic accidents which causes people to pay more attention to the risk of navigation akyuz and celik 2014 goerlandt and montewka 2015 according to statistics hanzu pazara et al 2008 in ship collision accidents 89 96 of accidents are caused directly or indirectly by human factors and one of the important ways to solve ship accidents caused by human factors is to utilize intelligent maneuvering of ships in addition the safety of the crew in extreme weather conditions in recent years has also become a problem that cannot be ignored wang et al 2014 besides the number of crews is declining recently while the wages of crew are rising year by year which has become the second largest expenditure item after the fuel costs of shipping lun et al 2016 as intelligent ships have outstanding advantages in improving the safety management energy consumption management and operational efficiency of ships therefore the researches for intelligent ships have become an inevitable trend for future ship development and gained the interest of many researchers in both academia and private sectors goerlandt and montewka 2015 in addition the natural environment is an important factor affecting the safety of waterborne traffic zhang et al 2018 among the natural environmental factors surrounding the ship meteorological conditions walrus conditions topographical environments and water facilities will bring restrictions to the navigation of the ship these factors affect the ship s navigation and the crew s decisions by affecting the ship s maneuverability along with the skill and mentality of the shipper the natural environmental factors that typically affect the safe environment of maritime traffic are weather conditions and ocean conditions specifically wind current and waves intelligent ships use sensors communications internet of things the internet and other technical means to automatically sense and obtain information and data on the ship itself the marine environment logistics ports etc based on computer technology automatic control technology big data processing and analysis technology it utilizes intelligent operation in ship navigation management maintenance cargo transportation etc lazarowska 2017 making ships safer more environmentally friendly more economical and more reliable intelligent here can be understood as human like thinking it can comprehensively consider the specific tasks and various information obtained and develop a series of optimal decisions that meet the safety requirements of the ship s navigation economy and environment it takes a long transition period for an intelligent ship to fully realize unmanned maneuvering presently although the current level of ship automation is relatively high the normal operation of ships is always inseparable from human participation perera et al 2015 even in an unattended cabin the crew must be handled when an emergency occurs although the ship is maneuvered by satellite navigation electronic compass electronic channel map and automatic rudder the bridge has not been unmanned intelligent ship technology has developed rapidly in recent years however there are still many problems need to be solved in addition the existing research does not form a set of theoretical methods to solve the problem of autonomous learning of the intelligent ship for the maneuvering decision making characteristics of officer on watch oow and lacks the corresponding theoretical methods to solve the problem of intelligent ship human like maneuvering decision making modeling researchers have proposed several different decision tree algorithms see literature review for both classification and decision making problems based on different aspects and obtained good results based on the advantages of the c4 5 algorithm and the ability to analyze the characteristics of multifork trees this paper uses the c4 5 algorithm to learn the oow s maneuvering decision characteristics we regard the intelligent ship human like maneuvering decision making problem as a machine learning problem based on the oow s experience the oow s actual maneuvering data and the environmental influencing factors such as wind wave and current in specific water areas and the problem is converted using the decision tree c4 5 method to learn the oow s maneuvering decision making characteristics thus constructing a human like decision making model under multiple constraints in summary this study focuses on the concept of human like maneuvering for the intelligent ship and studies the human like decision making method of intelligent ships by establishing autonomous learning method of maneuvering decision making the maneuvering decision making rules of typical maneuvering style is explored and the processes of autonomous learning oows maneuvering decision making characteristics for intelligent ships are studied and the intelligent ship human like decision making model is constructed this study provides a new perspective and methodology for the development of intelligent ship technology in theory and practice and promotes the application and spreading of intelligent ships the main contributions of this study are as follows 1 a novel intelligent ship human like decision making maneuvering decision recognition hdmdr model is proposed 2 the standardization principle of environmental influencing factors and maneuvering decision making factors is developed 3 the decision making mechanism of the oow s maneuvering behavior is analyzed on the basis of the actual crews operational data from full task handling simulation platform and the oow s decision making knowledge under the specific environmental influencing factors in the inbound scenario is automatically acquired and represented 4 considering the high cost of using the real 30 000 ton ship to carry out this kind of experiment and the low feasibility of collecting the data of multiple voyages from the real world ship therefore it is unique and very valuable to obtain the experimental data operated by an experienced oow on the full task handling simulation platform in a certain time and space the structure of this paper is organized as follows initially section 2 reviews the literature section 3 briefly presents the proposed decision making model the experimental processes are introduced in section 4 section 5 details the experimental results and the performance of our optimization methodology the conclusions and future directions of research are addressed in section 6 2 literature review data mining is a process that uses analytical tools to extract information and knowledge including knowledge that is hidden unknown or incomplete but potentially useful from a large amount of incomplete noisy fuzzy and random data moreover data mining determines the relationship between models and data and uses it to make predictions aguiar pulido et al 2013 sanil 2001 the classification algorithm is a data analysis method belonging to predictive data mining its goal is to find models that accurately describe and distinguish data classes or concepts from important sample data sets such that they can be grouped into a data category based on the entity s attribute values and other constraints the current technologies and methods mainly include decision tree algorithms calistru et al 2015 xie et al 2003 bayesian classification and bayesian networks baksh et al 2018 neural networks kheradpisheh et al 2018 genetic algorithms peng et al 2015 rough sets zhang et al 2012 etc in the 1960s decision tree algorithm was initially proposed by hunt et al 1966 to minimize the cost of classifying an object quinlan 1986 decision trees can handle both categorical and numerical data and it is good at processing nonnumeric data which can eliminate a significant amount of data preprocessing work when dealing with numerical data through algorithms such as neural networks in addition the decision tree method is simple in structure and does not need much background knowledge in the process of learning second the decision tree model is more efficient and is more suitable for training sample sets that have large amounts of data third the computational tree algorithm has a relatively small amount of computation then the decision tree method typically does not require knowledge outside the training data and is good at processing nonnumeric data finally the decision tree method has a higher classification accuracy therefore the decision tree method is a key research direction in the field of machine learning muchoney et al proposed the classification algorithms of decision tree dt artificial neural network ann and maximum likelihood to analyze the land cover classification problem in central united states and the results show that the decision tree has the highest classification accuracy muchoney et al 2010 borak 1999 used decision trees to classify features from a large amount of data the results show that the tree based classifier can greatly reduce the dimensionality of the input data set without affecting the classification accuracy calistru et al 2015 proposed a novel parallel decision tree algorithm namely pdscart to process a larger amount of data stream records and construct the tree efficiently saunier et al 2011 used decision trees the k means algorithm and the hierarchical agglomerative clustering method to identify patterns in the traffic event database and analyze the relationship between interaction attributes and collision common decision tree algorithms are concept learning system cls angluin 1988 hunt et al 1966 iterative dichotomiser 3 id3 quinlan 1979 1986 c4 5 quinlan 1993 c5 0 bujlow et al 2012 pandya and pandya 2015 classification and regression trees cart calistru et al 2015 friedman et al 1984 chi squared automatic interaction detector chaid kass 1975 rodriguez et al 2016 etc the internal variables of each subsample are highly consistent and the corresponding variation impurity falls between different subsamples as far as possible all decision tree algorithms follow this criterion and the data set is partitioned into subsets with different statistical approaches such as entropy lakkakula et al 2014 gain ratio prasad and naidu 2013 gini coefficient prasad et al 2013 v et al 2013 etc a series of follow up decision tree programs such as id3 c4 5 and cart etc are all developed from cls among them the c4 5 algorithm developed based on id3 is currently one of the most famous and popular decision tree algorithms lu et al 2015 c4 5 is the most influential data mining algorithm identified by the ieee international conference on data mining icdm in december 2006 wu et al 2007 a comparative study of c4 5 and other learning algorithms shows that it can balance processing speed and error rate well lim et al 2000 c4 5 can convert the decision tree into an equivalent production rule solve the learning problem of continuous value data classify multiple categories increase the boosting technology and complete the processing of large databases more efficiently the c4 5 algorithm also deals well with continuous and discrete values and attributes with missing attribute values garcía laencina et al 2015 the c4 5 algorithm solves the above problem well however the id3 algorithm tends to favor more attributes and the data of discrete value attributes but not the attributes with continuous values nor the samples with missing values and is sensitive to noise hssina et al 2014 c5 0 mainly adds support for boosting which also uses less memory compared with the c4 5 algorithm it builds a smaller rule set therefore it is more accurate but c5 0 is a commercial software and the public cannot easily get the source code witten et al 2016 cart uses the training set and the cross validation set to continuously evaluate the performance of the decision tree to prune the decision tree thus achieving a good balance between training error and test error however cart and chaid only supports building binary trees while c4 5 allows two or more outcomes and supports binary or multifork trees wu et al 2007 several prior studies on the c4 5 dt could be found from the literature a prior study provost and domingos 2003 found that a c4 5 introduction learner without pruning and without node collapsing quinlan 1993 can achieve the best prediction accuracy a novel vfc4 5 was proposed by cherfi et al 2018 to build decision trees through reducing the number of cut points by using the arithmetic mean and median this algorithm could get excellent accuracy than a c4 5 algorithm reumers et al 2013 used c4 5 decision tree based model to infer activity types from global positioning system gps traces the results showed that the overfitting was minimal in addition the model enables researchers to infer activity types directly from activity start time and duration information obtained from gps data dai and ji 2014 proposed a parallel mapreduce algorithm to implement a typical c4 5 decision tree the experimental results indicated that the algorithm exhibits both time efficiency and scalability 3 methodology 3 1 decision tree a decision tree is a mathematical method that generates decision trees or decision tree rules by inductive learning of training samples and then classifies new data using decision trees or decision rules as a supervised case based inductive learning algorithm decision trees evolved from the artificial neural network method which is a method to solve complex decision problems through tree like logical thinking it can infer the classification rules of the decision tree representation from a set of unordered and irregular cases it typically forms a classifier and a prediction model which can classify predict and analyze the unknown data for knowledge discovery the decision tree consists of a root node a series of internal nodes and leaf nodes each node has only one root node and two or more leaf nodes and the nodes are connected by branches yuan and shaw 1995 each internal node of the decision tree corresponds to a collection of noncategory attributes with each edge corresponding to each possible value of the attribute the leaf nodes of the decision tree correspond to a category attribute value and different leaf nodes can correspond to the same category attribute value in addition to being represented in the form of a tree a decision tree can also be represented as a set of production rules in the form of if then each root to leaf path in the decision tree corresponds to a rule the condition of the rule is the rounding of all node attribute values on the path the rule s conclusion is the category attribute of the leaf node on the path compared with decision trees rules are more concise and easier for people to understand use and modify which form the basis of the expert system therefore in practical applications more rules are used the decision tree method consists of two main steps the first step is to use the training sample set to build and generalize a decision tree and build a decision tree model this process is actually a process of acquiring knowledge from the data and doing machine learning it is usually divided into two phases building and pruning the second step is the process of classifying new data using a built in decision tree the input of the decision tree learning algorithm is a set of training samples represented by attributes and attribute values and the output is a decision tree which can also be extended to other representations such as rule sets decision tree generation typically uses a top down recursive approach the optimal attribute is selected as the node of the tree by some method and the attribute values are compared on the node and the branch from the node is judged according to the different attribute values that correspond to the training samples the lower nodes and branches are repeatedly established in each branch subset and the growth of the tree is stopped under certain conditions and the conclusions are obtained at the leaf nodes of the decision tree to form a decision tree the decision tree is generated by performing decision tree learning on the training samples the decision tree can classify an unknown sample set according to the value of the attribute which is the decision tree classification fig 1 c shows an example of a typical binary decision tree based on the data shown in table 1 from fig 1 c we can see that a decision node attribute i e crossing orientation which represents the position of vessel 2 has two branches values i e right section and left section which represent the unique values for the specific attribute leaf node i e class which represents the crossing situation represents the class category or decision of each instance furthermore according to the colregs international regulations for preventing collisions at sea navigation rules which provide safe operation guidelines for maritime navigation as shown in fig 1 a if vessel 2 is at the left crossing section class a the vessel 2 should turn right and vessel 1 should keep its course if vessel 2 is at the right crossing section class b the vessel 1 should turn right and vessel 2 should keep its course shown as fig 1 b therefore the final decision can also be represented through the form of if then rule set shown as follows rule 1 if crossing orientation left then class a vessel 1 keeps course and vessel 2 turns right rule 2 if crossing orientation right then class b vessel 1 turns right and vessel 2 keeps course this example indicates a maritime problem of colregs situation the decision tree generated from collision avoidance operation in the encounter scenario of different crossing situations in this way the attributes taken together provide a zeroth order language for characterizing objects in the universe quinlan 1986 3 2 the proposed hdmdr model 3 2 1 theory information shannon 1948 proposed the information theory in 1948 and the amount of information on events could be calculated as follows 1 i s i p s i log 2 p s i where p s i is the probability of occurrence of event s i suppose that there are v mutually exclusive events s 1 s 2 s v and only one of them happens the average amount of information can be measured as follows 2 i s 1 s 2 s v i 1 v p s i log 2 p s i when p s i 0 then i s i p s i log 2 p s i 0 3 2 2 information entropy assume that d is the intelligent ship human like decision making training data set contains a set of m classes d stands for the total number of samples in data set d and s i is the number of samples in data set d that belongs to class s i i 1 2 m if we randomly select a sample from d and this sample belongs to class s i then we can get a prior probability of the event as follows 3 p i s i d the expected information also referred to as entropy needed to classify d into m classes is defined as 4 i s 1 s 2 s m i 1 m p i log 2 p i suppose a feature attribute a has n distinct values a 1 a 2 a n feature attribute a partitions d into n subsets d 1 d 2 d n d j is the number of samples in subset d j j 1 2 n and s j i stands for the number of samples in subset d j that belongs to class s i then the expected information is defined as 5 e a j 1 n d j d i s j 1 s j 2 s j i note that the smaller the entropy value is the higher the purity of the subset partition where m for a given subset d j 6 i s j 1 s j 2 s j i i 1 m p i j log 2 p i j 3 2 3 information gain and gain ratio the information gain of feature attribute a is expressed as follows 7 g a i n a i s 1 s 2 s m e a the split information is defined as 8 s p l i t a j 1 n d j d log 2 d j d where s p l i t a is the information generated by partitioning d based on the values of a it indicates the outcome of the test rather than the class to which the sample belongs the gain ratio could be calculated by the following 9 g a i n r a t i o a g a i n a s p l i t a 3 2 4 constructing the c4 5 decision tree c4 5 is an extension of id3 and was presented by quinlan 1993 id3 selects the attribute with the largest information gain value as the node of the tree as also shown by xue et al 2019 however c4 5 introduces the concept of information gain ratio and selects the attribute with the largest information gain ratio moreover each possible value is used as a branch of this node to recursively form a decision tree in addition c4 5 adds significant functions compared to id3 such as rules generation uncertainty processing functions and attribute discretization c4 5 overcomes the shortcomings of the id3 algorithm using information gain to select attributes when biasing the selection of more attributes and can build a decision tree with as simple a structure as possible while ensuring the accuracy of training set classification algorithm 1 depicts the procedures of the process of construction of the proposed maneuvering c4 5 decision tree of intelligent ship human like decision making maneuvering decision recognition hdmdr model algorithm 1 construct the proposed c4 5 decision tree of hdmdr model image 10380 3 2 5 pruning the decision tree the initial construction of the c4 5 decision tree is often complicated by the inclusion of a large number of classification attributes and branches and there are inevitably some errors namely noise this noise gradually accumulates in the decision classification process which will eventually cause the c4 5 decision tree to have a large deviation from the classification of the actual sample and the accuracy is reduced i e over fitting thus the c4 5 decision tree generated by the training set is very good for classifying the training set but it may not be ideal to use it to classify the new data set that does not participate in the decision tree generation process therefore the preliminary constructed c4 5 decision tree needs to be pruned and the purpose of pruning is to optimize the c4 5 decision tree or simplify the generated rules there are two kinds of decision tree pruning methods prepruning and postpruning for the problem of over fitting this study uses postpruning methods to eliminate branching anomalies caused by noise data and isolated points quinlan 1993 proposed using pessimistic error pruning to compensate for optimistic bias in tree generation during pruning due to the decision tree is generated from the training data set in most cases the decision tree is consistent with the training data set however when the decision tree is used to classify data other than the training data it is obvious that the error rate will be greatly increased the postpruning rule adopts the principle of minimum expected error rate i e starting from the root node of the tree and calculating the expected error rate that may occur for each branch node pruning no pruning if the node is clipped resulting in a higher expected error rate the subtree is retained otherwise the subtree is clipped and finally the c4 5 decision tree with the smallest expected error rate is obtained in this paper the upper limit of the quality confidence interval is used as the erroneous estimation under pessimistic conditions given a confidence level α 0 25 in the c4 5 algorithm the total number of errors obeys the bernoulli distribution then there is a probability equation 10 p f q q 1 q n μ 1 α α where n is the total number of instances under the pruned subtree e is the number of error instances that occur after pruning f e n is the actual observed error rate and q is the estimated error rate let z μ 1 α taking the upper bound of the confidence interval as the pessimistic error rate estimate of this node then the formula for calculating the false positive rate of the node 11 q f z 2 2 n z f n f 2 n z 2 4 n 2 1 z 2 n where f e n is the actual observed error rate and q is the estimated error rate set the maximum value of the expected false positive rate to c if the estimated false positive rate q after pruning is higher than c the original subtree is retained otherwise the subtree is cut and replaced with leaves after the pruning the inbound human like decision making tree is shown in fig 6 fig 2 is the basic process and framework for our proposed hdmdr model 4 experiments 4 1 scenario design and data collection in our experiment the simulator scenario was the shanghai waigaoqiao wharf and the ship was downstream of the berthing into the port we use a 30 000 ton bulk carrier as our experimental ship os1 33089 0 t 182 9 m long 22 6 m wide we define the process as when the ship s stern leaves the main channel near the port side of the boundary line in the electronic chart fig 3 b shows the initial boundary to the ship berths docked at the end of the cable fig 3 c shows the end boundary as a complete berthing process the experimental scenario is shown in fig 3 we collect the data from the full task handling simulation platform navi trainer professional 5000 which conforms to the imo stcw78 10 convention and the det norske veritas dnv from the maneuvering simulator laboratory in wuhan university of technology waterway road traffic safety control and equipment ministry of education engineering research center we collect the operational data of the exercises and assessment exams as our experimental data unlimited navigational class crew 4 groups of 96 people 32 45 years old skilled maneuvering level captain chief officer from table 2 we can get the average age of the crew participating in this experiment is 38 76 years old and their average piloting experience is 8 89 years the captains average age and piloting age are both higher than those of chief officers from fig 4 we can get the distribution of oows age and their piloting experience the ship handing and environment including inside and outside multisource information were collected on the ship s berthing process including the environment wind current wave etc control rudder order marine telegraph order 2 factors table 3 lists some of the training samples it should be noted that in our case the oow is the captain or chief officer although in the real situation the captain is not on duty the captain will go to the bridge only in special circumstances and if necessary the captain may take over the duty of the oow to maneuver the ship but it is an assessment and evaluation scenario in our experiment therefore the captain also acts as the oow in addition we regard the tugboat as a power plant system of target ship os1 to facilitate the ship s overall situation of a simplified analysis and we consider the tugs and the ship os1 as a whole dynamic model under the premise of this hypothesis the ship os1 completes the inbound operation through the combination of rudder orders and telegraph orders according to the actual navigational situation of its force and movement 4 2 standardization principle setting maneuvering decision making processes are often influenced by multisource information such as human ship and environmental factors these influencing factors act together to determine the next action strategy of the ship s oow for a particular person ship unit the overall reliability is constant for a certain period of time or during a trip therefore the person and ship factors have less influence on maneuvering decisions with the operation of the ship the oow s waterway and the environment will change with time and space and the changing waterway and environmental factors will have a greater impact on maneuvering decisions in this research we mainly focus on the environmental influencing factors and study their effect on the decision making of the oow based on the strategy and the current maneuvering environment the experienced oow can quickly and accurately make maneuver decisions thus laying the foundation for the study of human like maneuvering behavior for the application to intelligent ships we select six environmental influencing factors as the input of our proposed hdmdr model to study the decision making mechanisms for different maneuvering behaviors in order to let the maneuvering decision making knowledge to be automatically obtained and expressed along with higher decision making knowledge effectiveness it is typically necessary to divide the number of linguistic terms by experience yuan and shaw 1995 in this paper experimental data of each maneuvering decision making factor are trisected into three levels namely small a medium b and large c see table 4 to objectively describe the characteristics of each influencing factor and make it easier to describe how each factor influences final maneuvering decisions we select six environmental influencing factors as the input of our proposed model to study the decision making mechanisms for different maneuvering behaviors current direction current speed relative current direction relative wave direction relative wind direction relative wind speed in other cases the other new factors can also be upgraded according to algorithm 1 in section 3 2 4 using specific standardization principle the oow maneuvers the ship by operating different telegraph and rudder orders to change ship s speed and direction and to complete the ship s control table 5 shows the combining telegraph and rudder orders speed and course control respectively this control is a multidynamic process moreover it should be noted that in combination with the actual situation of the experimental scenario unlike the ship sailing on the open sea the oow needs to call the rudder and telegraph orders frequently in the inbound decision making ship handing process therefore in this paper we do not consider midships and stop engine regardless of the rudder angle and if the power output is 0 table 5 shows the standardization principle for output maneuvering decision making factors 5 results and discussion 5 1 standardizing of training set the data in table 3 are standardized according to the principle of standardization of maneuvering decision influencing factors in tables 4 and 5 the results are shown in table 6 5 2 constructing and pruning the decision tree the c4 5 algorithm can be divided into two phases first a certain attribute is selected according to the criterion of maximum information gain to divide the training set and the recursive call is performed until all the examples in each division belong to the same class then the established tree is pruned i e the branch established above the noise data is cut in the decision tree analysis approximately 80 of the data is randomly selected as the training set and the remaining 20 is used as the test set then through eqs 3 11 we could obtain the decision tree structure as shown in fig 6 partitioned into 3 parts part i ii and iii the number and proportion of different decisions are shown in fig 5 5 3 establishing maneuvering decision classification rules the result of our proposed hdmdr model is a set of classification rules in the form of if then each path from the root node to the leaf node constitutes a rule the characteristics of the internal nodes of the path correspond to the conditions of the rule and the classification of the leaf nodes corresponds to the conclusion of the rule as a result we can easily extract the human like decision making knowledge using the decision tree and rule set the optimized maneuvering decision recognition rule set is shown in table 7 from figs 5 and 6 and table 7 we can draw the following conclusions 1 it can be seen from the generated decision tree that y2 as the root node i e current speed is the most informative attribute of all samples in other words in the environment of the simulation experimental scenario the current speed in the environmental influencing factors has the most significant impact on the oow s maneuvering decision making followed by the relative current direction and current direction 2 the ordering of environmental factors provides the oow with a set of variables for decision making reference which has certain guiding significance for the formulation of maneuvering decisions 3 through the analysis of the rule set this designed scenario outputs a number of standardized maneuvering decision operations x1 12 50 and x3 23 50 x1 u1d1u2t2 maneuvering decision knowledge can be interpreted and conceptualized into linguistic term or operation order keep the propeller ahead and keep the current rudder angle port the same x3 u1d1u2d2 keep the propeller ahead and keep the current rudder angle starboard which is consistent with actual ship maneuvering experience 4 the specific factors and decision rule sets in a specific scenario obtained by the hdmdr model proposed in this paper can be used as an important reference for the intelligent ship human like decision making and can also be used to create a knowledge base of expert systems it has a high reference value and practical value for the development of intelligent ship s maneuvering algorithm 5 4 performances assessment 5 4 1 applying rules for classification we use the maneuvering decision making model proposed in this paper to identify the decision making data to be identified in table 8 we compare the recognition results with the actual ship maneuvering decisions and use the accuracy of the recognition to verify the validity of the model the standardized maneuvering decision making data are identified in table 8 using classification rules 33 29 and 37 and the recognition result is x13 x13 x13 x9 x9 and x15 this result is consistent with actual maneuvering decisions and demonstrates high reasoning efficiency the test data set was evaluated and validated using the generated decision tree model there were 135531 samples participating in the test accounting for 20 of the overall data set to assess the accuracy of the hdmdr model the data in the test data set is used for prediction and the degree of agreement between the test results and the actual situation is compared the accuracy of the proposed module acc could be calculated as 12 a c c t n t p t n t p f n f p where tn is true negatives tp is true positives fn is false negatives and fp is true positives the classification accuracy of our proposed hdmdr model using c4 5 decision trees based on the test data set can reach more than 81 6 5 4 2 comparative analysis to further validate the effectiveness of the hdmdr model in this paper we compare the performance of the proposed c4 5 decision tree algorithm with two classic classification algorithms k nearest neighbours k nn and support vector machine svm in our case we use the radial basis function rbf to conduct the svm and k 1 in the k nn besides we use classification accuracy shown as eq 12 to measure the proposed c4 5 algorithm in addition in this paper the code for the basic versions of k nn and svm classifiers is adopted from the waikato environment for knowledge analysis weka which is open source data mining software hall et al 2009 weka is a comprehensive software that implements many state of the art machine learning and data mining algorithms we conduct a ten fold cross validation 10 cv experiment using the data from training set 10 cv breaks data into ten sets equally then trains the classifier on nine data sets and uses it to test the remaining one data set repeating ten times like this and finally taking an average accuracy thus to compare the performance of the proposed c4 5 decision tree algorithm with k nn and svm the performance of different classifier algorithms on our data set is shown in table 9 and fig 7 according to the classification accuracy results the proposed method can achieve the highest accuracy among these three algorithms 6 conclusions with the development of the economy the continuous advancement of technology and the continuous increase of labor costs it has become an urgent trend to realize intelligent maneuvering of ships the purpose of this research is to recognize the automatic acquisition and representation of the oow s decision making knowledge and to provide a basis and reference for the development of decision making algorithms for intelligent ships in this paper a intelligent ship human like decision making maneuvering decision recognition hdmdr model and a novel standardization principle of maneuvering decision making factors are proposed for the learning of human like decision making mechanisms of intelligent ships by establishing an autonomous learning method of maneuvering decision making the processes of autonomous learning oows maneuvering decision making characteristics are studied in addition it is unique and very valuable to obtain experimental data operated by an experienced oow on the full task handling simulation platform in a certain time and space to validate the performance and effectiveness of our proposed model the assessment of applying rules for classification and the comparative analysis with the k nn and svm are compared according to the results the classification accuracy of our proposed hdmdr model can reach more than 81 6 in addition the proposed method is superior to the representative classification algorithms this study provides a new perspective and methodology for the development of intelligent ship maneuvering decision making technology in theory and practice promotes the application and spreading of intelligent ships under specific scenarios and is conducive to the development of water transportation in the direction of safety sustainability and economy nevertheless the hdmdr model still has some shortcomings which need to be improved in further research 1 the proposed method model is a data driven method we need more data to further train the machine learning model and improve recognition accuracy besides the feedback loop to inform the effect of this model still need to be optimized 2 the application scenarios of the proposed model still need to be enriched and more environmental influencing factors which may affect piloting decisions need to be added as well considering the specific situation thus to make the proposed model more widely applicable 3 the standardization principle of maneuvering decision making attributes need to be further detailed according to the actual navigation situation and more suitable for the real world ship handing orders specifically the combined rudder orders and telegraph orders thus further increasing the applicability of the model in the subsequent research we will study the classification of the influencing factors the fuzzy processing of data sets the detailed connection of our model with human behavior and their performance in specific navigational scenarios the application of multi navigation scenarios etc besides we will further collect the relevant data from the tugs and add the data to the model analysis to further optimize our proposed algorithm acknowledgments this study is supported by the national natural science foundation of china 51775396 61703319 u1764262 the major project of technological innovation of hubei province 2016aaa007 2017cfa008 and the china scholarship council 
22693,knowledge about extreme ocean currents and their vertical structure is important when designing offshore structures we propose a method for statistical modelling of extreme vertical current velocity profiles accounting for factors such as directionality spatial and temporal dependence and non stationarity due to the tide we first pre process the data by resolving the observed vector currents at each of several water depths into orthogonal major and minor axis components by principal component analysis and use harmonic analysis to decompose the total observed current into the sum of deterministic tidal and stochastic residual currents a complete marginal model is then constructed for all residual current components and the dependence structure between the components is characterized using the conditional extremes model by heffernan and tawn 2004 by simulating under this model estimates of various extremal statistics can be acquired a simple approach for deriving design current velocity profiles is also proposed the method is tested using measured current profiles at two coastal locations in norway covering a period of 2 5 and 1 5 years it is demonstrated that the method provides good extrapolations at both locations and the estimated 10 year design current velocity profiles appear realistic compared to the most extreme velocity profiles observed in the measurements keywords current velocity profiles extreme current velocities multivariate extreme values conditional extremes model peaks over threshold structural design 1 introduction knowledge about ocean currents and their vertical structure is important as a design criterion for ocean and coastal structures for offshore structures located in shallow water waves are typically the most important load factor while in deeper water currents can actually dominate the load equation forristall and cooper 1997 this is also the case for many structures located in the coastal zone where coastal features such as islands and skerries can provide shelter from severe sea states whilst currents might retain or even increase their strength an example of the latter type of structure is aquaculture fish cages where the mooring line tension is generally dominated by current loads huang et al 2008 it is clear that simplification of the vertical current profile can introduce substantial errors in the calculated design load in such cases in a review paper on recent developments of ocean environmental description bitner gregersen et al 2014 improved accuracy of the statistical description of currents is called for particularly regarding change of the current profile with water depth this issue is addressed in the present paper unlike many other time signals in nature ocean currents include a deterministic signal due to the astronomical tide generally becoming much stronger and important near the shore and in shallow water pugh and woodworth 2014 applying standard tidal analysis techniques the tidal signal can be extracted and predicted with very high accuracy for any future time robinson and tawn 1997 currents have this in common with sea levels so the methods used for estimating the distribution of extreme currents and sea levels are therefore somewhat related two broad classes of methods exist 1 direct methods analysing extremes of the total observed current directly and 2 indirect methods exploiting the decomposition of the total current into deterministic tidal and stochastic residual currents modelling both separately before inferring the distribution of extreme total currents our focus will be on the second class sometimes referred to as the joint probabilities method jpm this method was originally introduced for estimation of extreme sea levels by 69 70 and later applied for estimating extreme currents by 67 extensions were given by 73 demonstrating substantial benefits over traditional direct methods for sea levels 22 found that the observed bias in direct methods was primarily caused by the non stationarity introduced by the tide extreme currents are far more difficult to estimate than extreme sea levels not only due to their directional and spatial variation but also because of the difficulty of obtaining sufficiently long series of observations pugh and woodworth 2014 even if a long time series is available a particular issue when dealing with extremes is that rare events are necessarily unusual so the quantity of directly relevant observations is limited this difficulty is compounded in the spatial setting such as for current profiles because forecasting then requires extrapolation into a high dimensional space with all its associated uncertainties it is thus important that the statistical models used should both be flexible and have a strong mathematical foundation so that such extrapolation has an adequate basis davison et al 2012 arguably the most useful and flexible current approach for modelling extremes in high dimensions is the conditional extremes model by 40 based on an assumption of the asymptotic form of the conditional distribution of a d dimensional variable given that it has an extreme component they present a semiparametric approach valid for extremes from a wide class of multivariate distributions applicable to problems of any dimension examples of application are spatial risk assessment of extreme river flows keef et al 2009 joint modelling of extreme significant wave height and spectral peak period jonathan et al 2010 2013 modelling of temporal dependence in river flows eastoe and tawn 2012 and modelling spatial extremal dependence of sea surface elevations at neighbouring locations eastoe et al 2013 the conditional extremes model was introduced for joint modelling of vertical current profiles by 45 and 72 applied it both for joint modelling of currents and waves and for modelling of current profiles owing to the positive experiences of the mentioned authors in using the heffernan and tawn model and its solid theoretical foundation this model will be applied here as well the additional constraints and slight change in model formulation recently proposed by 49 are also implemented to overcome a few complications that have been identified with using the heffernan and tawn model as commented by 45 any viable approach to modelling extreme vertical current velocity profiles must account for a the vector nature of the current at each depth and b the dependence between currents at different depths instead of using empirical orthogonal functions eof to pre process the data and then perform extreme value analysis on just a few energetic modes of the observed current profiles see e g 36 we choose to model orthogonal current components at each depth directly to avoid loss of information our approach is therefore closely related to that presented by 45 however we focus on modelling instantaneous mean velocity profiles rather than profiles consisting of hourly maxima and minima of the current components by considering orthogonal current components at each depth we bypass the necessity of explicitly introducing covariates as would be required for modelling current speeds and directions see 72 the proposed method accommodates the vector nature of the current by considering orthogonal current components at each depth and the dependence between the residual current components is characterized by the conditional extremes model furthermore observed temporal dependence leading to clustering of extremes is accounted for in both the marginal tail and dependence modelling by applying the peaks over threshold pot method and the non stationarity introduced by tidal currents is handled by exploiting the decomposition of the total current into tidal and residual currents the key steps of the proposed method are summarized in fig 1 we outline each of the steps herein and also propose a simple and pragmatic approach for deriving design current velocity profiles the method is believed to provide a valuable addition to existing methods for estimation of extreme current velocity profiles and we test it using adcp acoustic doppler current profiler data collected at two coastal locations in norway the paper is organized as follows in section 2 the considered locations together with the available data are described a brief general discussion on the accuracy of the measurements is also included the required pre processing of the data is described in section 3 this includes application of principal component analysis pca to resolve the current velocities into major and minor axis components at each depth and decomposition of the current velocity into tidal and residual currents by harmonic analysis in section 4 a general introduction of the conditional extremes model by 40 is given section 5 constitutes the main part of this paper here the statistical modelling of the residual current components is described in detail and applied to obtain extreme vertical residual and total current profiles at the two considered locations we present an approach for modelling the complete marginal distribution bulk and tail distribution of each residual current component describe the application of the conditional extremes model for characterizing the dependence structure between the components and outline the monte carlo procedure used to simulate extreme residual current and total current velocity profiles the simulated velocity profiles are compared with the measurements in section 6 a simple approach for deriving design current velocity profiles is proposed and the main conclusions and a discussion on assumptions and possible improvements are given in section 7 2 locations and data as indicated in fig 2 the measurements have been made at two coastal locations off the west coast of trndelag norway roughly 150 km east of the shelf break the munkskjæra site 63 8221 n 8 3836 e has a water depth of approximately 80 m and is located in vicinity of a number of small islands and skerries forming a strait in the east west direction fifteen kilometres to the northeast at salatskjæra 63 9200 n 8 5927 e the water depth is approximately 40 m this site is surrounded by a myriad of small islands underwater rocks and skerries resulting in a local bathymetry that is even more complex than at munkskjæra a simple statistical analysis of the current and wave conditions at both sites has previously been made at an earlier stage of the measurement programme kristiansen et al 2017 there are two major current systems in this coastal area the norwegian atlantic current primarily flowing along the shelf edge and the norwegian coastal current causing high current speeds near the coast sætre 2007 the larger fjord systems along the coastline are forced by freshwater runoff from land resulting in a surface outflow of brackish water that eventually adds to the norwegian coastal current broch et al 2017 from the island frya see fig 2 a chain of small islands stretches north eastward between this island chain and the mainland of norway is a deep ocean bay frohavet cutting inwards towards the entrance of trondheimsfjorden the local flow conditions at the considered sites munkskjæra and salatskjæra are dominated by water exchange between the norwegian sea to the west and frohavet primarily following the semidiurnal tidal cycle wind induced currents are of importance too and the rough topography creates a dynamic environment including tidal residual currents generated by interaction of tidal currents with coastal features and bottom topography and is also responsible for steering the current along its contours the measurements at both locations were made with a three beam acoustic doppler current profiler adcp of type nortek aquadopp 400 khz mounted on a moored oceanographic surface buoy seawatch midi 185 an adcp utilizes a physical principle called the doppler effect to measure the current speed and direction in multiple depth cells through the water column the doppler effect is exploited by emitting sound pulses from transducers beams which are reflected echoed by particulate matter moving with the water the signal is then shifted in frequency doppler shifted in proportion to the particle velocity see e g 62 for additional information note that the resulting measured velocity vector in a given depth cell by an adcp is not an instantaneous velocity at a fixed point but rather a spatial average with an inherent assumption that the flow is homogeneous in the horizontal plane over the distances separating the acoustic beams lu and lueck 1999 the current measurements were performed with a sampling rate of 1 hz over an ensemble interval of 10 min 600 samples with output once every hour this yields a time series of hourly 10 min average current speeds and directions the speed range was 0 300 cm s discretized by 256 points bin size of 1 2 cm s and the depth cell size was 3 m the data were post processed internally on the buoy before being sent to land every hour the measurement period at munkskjæra was almost 2 5 years from february 2016 to end of may 2018 and we consider the resulting hourly measurements of easterly and northerly velocity components at depths 4 m 10 m 16 m 22 m 28 m and 34 m below the surface at salatskjæra the buoy deployment lasted approximately 1 5 years from march 2016 to september 2017 and we consider the same depths as for munkskjæra apart from depth 34 m which was left out as the acoustic measurements here appeared to be occasionally affected by the proximity to the seabed the reason for focusing on the upper part of the water column is primarily a consequence of the measurement setup in addition the considered sites are located in an important area for fish farming mainly salmon farmed in open sea cages knowledge of the current velocity as a function of depth during extreme events in the upper part of the water column is important both for structural design of the cages and for the welfare of the fish an essential point when relying on measurements is their quality and validity the performance of acoustic doppler current profilers in laboratory flumes is generally found to be good for measuring mean current velocity profiles particularly in flow with low turbulence nystrom et al 2007 however in a recent 5 year measurement program in the north sea large discrepancies were observed between overlapping current speed data measured by different current profilers at the same locations and water depths suggesting that the accuracy of current profilers is not as good as the user expects see 7 also as our measurements are performed with adcps mounted on surface buoys an aspect likely to affect of their validity is the presence of surface waves an effect which is hoped to be averaged out over the ensemble interval there is only a limited literature investigating the effect of wave induced motions on buoy mounted adcps see 7 55 59 79 and 87 the overall conclusion is that the buoy motion does affect the measurements but the above references do not agree on the magnitude of the effect the latter is not really surprising considering that the buoy response depends on multiple factors such as buoy type mooring system wave conditions including stokes drift see e g 74 and 56 and ambient eulerian current this will not be discussed further however we emphasize that any statistical method assumes the input data to be valid the corresponding validity of the estimated extremes therefore depends critically on the quality of the measurements 3 pre processing of the data prior to the statistical modelling pre processing of the current velocity data is required although the techniques used are standard within the oceanographic community boon 2004 they may not be familiar to an ocean engineer working with structural design for completeness we therefore provide some level of detail in the present section specifically the mathematical techniques principal component analysis pca and harmonic analysis is introduced along with their application we mention that as presented here the order of application of the two techniques is interchangeable 3 1 principal component analysis resolving the current velocity into major and minor axis components principal component analysis pca sometimes referred to as empirical orthogonal functions eof is a statistical approach where the usual objective is to condensate the information contained in a large number of interrelated original variables into a smaller set of linearly uncorrelated variates with a minimal loss of information in terms of variance see 44 this technique has been used for decades by oceanographers and meteorologists to analyse complex time series forristall and cooper 1997 in our case the motivation for applying pca is primarily to obtain uncorrelated current components at each depth following 45 not to reduce the dimensionality of the problem unless the current to be analysed is rectilinear note that uncorrelated variables are not necessarily statistically independent consider a time series consisting of n measured horizontal current velocities at a given depth expressed as orthogonal vector components u e and u n u e k and u n k k 1 n being the velocity observed at time t k in the eastward and northward direction respectively we now want to apply pca to convert this set of generally correlated variables into a set of values of linearly uncorrelated variables the principal components this is achieved by using an orthogonal transformation defined in such a way that the first principal component has the greatest fraction possible of the total variance and consequently in the two dimensional case the second principal component has the least the practical procedure for obtaining the principal components is as follows our time series of horizontal current velocities is expressed as a n 2 data matrix u u e u n where u e and u n are the eastward and northward velocity vectors with their mean value subtracted the sample covariance matrix s is then given as 1 s 1 n 1 u t u where the diagonal elements of s is the variance of u e and u n respectively to find the principal components we must calculate the eigenvalues and corresponding eigenvectors of s the resulting 2 2 unit eigenvector matrix v is then sorted so that the first column of v is the eigenvector corresponding to the largest eigenvalue the first principal component axis then simply refers to the first column of v and the second principal axis to the second column the values or scores of the original variables onto the principal axes are then found as 2 u m u m u e u n v where u m is termed the major axis component and u m is termed the minor axis component corresponding to the first and second principal component respectively in practice v in eq 2 is nothing more than a rotation matrix this is seen in fig 3 and 4 showing the resulting major and minor axis of the surface current velocity observations at munkskjæra and at salatskjæra the dominant current direction is seen to be closely aligned with the east west axis at munkskjæra and in the southeast northwest direction at salatskjæra the direction of the highest observed velocities at salatskjæra is however not aligned with the major axis the choice of positive directions is up to the analyst to decide our choice is indicated in the figures the procedure above has been followed to resolve the current velocity into uncorrelated major and minor axis components independently at each depth at the two considered sites at the munkskjæra site the major axis component account for 90 95 of the total current velocity variance increasing with depth as shown in fig 5 a in fig 5 b it is seen that the relative major axis variance is less prominent at salatskjæra where it accounts for 75 85 of the total variance the direction of the major axis is determined by the local bathymetry and topography at both locations a slight anti clockwise and clockwise rotation for increasing depths is seen at munkskjæra and salatskjæra respectively here the direction of the major and minor axes has been decided based on the total current velocity at each depth following 45 other rational choices exists however for instance deciding their direction based on the residual current velocity at each depth and or only considering velocities whose magnitude exceeds some threshold the best choice is dependent on the data at hand 3 2 harmonic analysis decomposing the total current into tidal and residual currents the measured total current velocity is the vector sum of an essentially deterministic tidal current and a stochastic random residual current it is only meaningful to perform extreme value analysis on stochastic variables so it would be desirable to decompose the observed current into a tidal and a residual component at each depth the standard method for extracting the tidal signal from a time series is called harmonic analysis and will be briefly described in the following unlike many other time signals in nature tides and tidal currents are forced oscillations that occur only at known tidal frequencies boon 2004 the driving forces originate from the gravitational fields of the sun and moon acting on a rotating earth expressed mathematically as the tidal potential doodson 1921 this astronomical forcing can be written as a linear combination of sinusoidal terms each having a distinct amplitude phase and temporal frequency foreman and henry 1989 the oceanic response can be described in the same manner each sinusoid being referred to as a tidal constituent due to the hydrodynamic effects caused by irregular coastal boundaries and the bathymetry of the oceans the amplitudes and phases of the constituents can vary greatly but their frequencies remain the same as those in the tidal potential foreman and henry 1989 the tidal frequencies are all linear combinations of the rates of change of the mean lunar time the earth rotation with respect to the moon and five astronomical variables that uniquely specify the position of the sun and moon see 33 unlike spectral analysis harmonic analysis takes advantage of the fact that the tidal frequencies are known in advance once a suitable set of m tidal constituents has been chosen the amplitude and phase of each constituent are calculated by solving a system of linear equations in the one dimensional case this equation system takes the form 3 h t k a 0 j 1 m a j cos σ j t k ϕ j where a 0 is the mean a j ϕ j and σ j are the amplitude phase and frequency of constituent j and h t k k 1 n is the observation at time t k eq 3 is generally overdetermined there exists more equations than unknowns and is therefore solved by a least squares technique minimizing the equation residuals the utide matlab functions by 12 have been applied for the harmonic analysis of the current velocity at each depth the functions take orthogonal current components along the first and second axes in any right handed coordinate system as input conventionally the eastward and northward components the time series are permitted to be irregularly sampled and or contain gaps utide then uses a refined two dimensional complex version of eq 3 to indirectly solve for the so called current ellipse parameters the tip of the velocity vector of a constituent traces out an ellipse over its tidal period so the goal is to find the lengths of its semi major and semi minor axes its angle of inclination and greenwich phase see e g 32 for illustrative figures the equation system can be solved either by the ordinary least squares ols method or by an iteratively reweighted least squares irls method the latter limiting the sensitivity to outliers and reducing confidence intervals compared to the ols method codiga 2011 diagnostics to assess constituent independence includes among others the conventional rayleigh criterion a time series of length t is required to distinguish between two constituents with frequency separation of t 1 and its noise modified version due to 61 accounting for the amount of non tidal energy noise in the record the so called nodal satellite and astronomical argument corrections are evaluated at the exact times of each measurement removing the restriction that the analysis periods should not be much longer than one year foreman et al 2009 the nodal satellite corrections accounts for the fact that the amplitudes and phases of the constituents are generally not constant due to interaction with minor unresolved constituents called satellites while the astronomical argument simply re expresses phase lags with respect to an absolute time and space origin foreman and henry 1989 there are a maximum of 146 possible tidal constituents that can be included in utide of these 45 are astronomical in origin while the remaining 101 are shallow water tides the latter constituents arise due to distortion of the tidal wave by shallow water effects and have frequencies that are multiples sums and differences of the frequencies of the astronomical constituents see e g 71 for further details the automated decision tree constituent selection method default option in utide due to 37 and formalized by 31 was applied to decide which constituents to include in the analysis the method selected a total of 68 constituents for inclusion in the harmonic analysis at both munkskjæra and salatskjæra the resulting current ellipse parameters of the five most energetic tidal constituents are given in table 1 for both sites the tidal current at munkskjæra and salatskjæra is semidiurnal dominated by the m 2 principal lunar semidiurnal and s 2 principal solar semidiurnal constituents the seasonal low frequency constituent sa solar annual is also seen to be important particularly at munkskjæra it should be noted that this low frequency constituent is largely influenced by non tidal forcing boon 2004 once the current ellipse parameters have been obtained utide can be used to reconstruct hindcast the tidal current over the period of observations in the reconstruction we conservatively neglect non significant tidal constituents having a signal to noise ratio snr below 2 with respect to the raw signal at that frequency default option in utide see 12 the residual current in the major and minor axis direction at depth i are then given as 4 u r m i t k u m i t k u t m i t k u r m i t k u m i t k u t m i t k where subscript r refers to the residual current and t to the tidal current the latter includes the mean current velocity if the eastward and northward velocity components have been used as input to the harmonic analysis the velocities are easily transformed to components along the major and minor axis at each depth using eq 2 the decomposition of the total current into tidal and residual current is shown in fig 6 and 7 for the major axis surface current at munkskjæra and salatskjæra respectively though only a 14 day period is plotted it is clear that the tidal current is important at both locations this is confirmed in fig 5 displaying the total tidal and residual current velocity variance with depth at the considered locations the tidal current accounts for 57 71 of the total variance at munkskjæra and 59 68 of the total variance at salatskjæra accordingly the variance of the current we are to perform extreme value analysis on the residual current is reduced by the same percentages at both locations the relative importance of the residual current increases towards the sea surface it is somewhat surprising to note that the total current variance at the top bin 4 m is slightly lower than that at 10 m at both locations when comparing buoy mounted and bottom mounted adcps 59 found that the near surface measurements of the buoy mounted adcps were biased low though we have no means of verifying it this could be a possible explanation for the observed near surface velocity reduction in our measurements as well in the remaining sections the tidal current is assumed to be deterministic and known at any time due to the preceding harmonic analysis it is worth noting that an underlying assumption in tidal harmonic analysis is that the tide is stationary there are cases where this assumption can be invalid for instance as a result of nonlinear interaction between the tide and storm surges in shallow water for internal tidal currents that change with the stratification or seasonally varying ice cover that can modify both tidal elevation and current harmonics foreman et al 2009 in such cases the use of wavelet analysis allowing for tidal non stationarity can provide a better alternative for extracting the tidal signal see e g 30 4 the conditional extremes model in this section a general description of the conditional extremes model by 40 is given for additional theoretical details the reader should consult the original paper heffernan and tawn 2004 or the article by 39 the latter providing a formal mathematical framework its application for joint modelling of extreme residual currents including practical details and required modifications in addition to the marginal modelling is presented in section 5 consider a continuous d dimensional random vector variable x x 1 x d with unknown distribution function f x x being for instance simultaneously observed values of an environmental parameter at different locations from a sample of n independent and identically distributed observations from f the conditional extremes model by heffernan and tawn concerns the estimation of functionals of the distribution of x when x is extreme in at least one component specifically it describes the conditional distribution of x i x i v x i where x i denotes the vector variable x excluding component x i and v x i is a high threshold here and throughout vector algebra is to be interpreted componentwise 4 1 marginal transformation having established the marginal distribution of each x i i 1 d by univariate extreme value theory see section 5 2 the method starts by componentwise transforming all variables to follow a common distribution this is known as marginal standardization and is performed in order to separate the marginal behaviour from the dependence structure between the components drees and janßen 2017 heffernan and tawn chose the standard gumbel distribution for this purpose we will however follow 49 transforming the marginals to standard laplace distributions the motivation for choosing the laplace distribution is that the semiparametric regression model in 40 used to characterize the behaviour of y i occurring with large y i takes different functional forms for positively and negatively associated variables as the laplace distribution has both exponential tails and symmetry this captures the exponential upper tail of the gumbel distribution required for modelling positive dependence while the symmetry allows the same functional form to be used for modelling the dependence of negatively associated variables keef et al 2013 we will later see that this is particularly convenient for modelling current profiles as it generally includes modelling of both positively and negatively dependent variables using the probability integral transform our original vector variable x with marginal cumulative distributions f x i x i is thus transformed componentwise as 5 y i log 2 f x i x i for x i f x i 1 0 5 log 2 1 f x i x i for x i f x i 1 0 5 where f x i 1 q is the inverse cumulative distribution function quantile function of x i evaluated at the cumulative probability q the new vector variable y y 1 y d then has standard laplace distributed marginals with 6 pr y i y f y i y 1 2 exp y if y 0 1 1 2 exp y if y 0 meaning that both the upper and lower tails of y i are exactly exponentially distributed for clarity x and y are used throughout the paper to denote the variable with its original marginal distributions and with laplace marginals respectively following 40 the focus will now be placed on extremal dependence modelling of variables with laplace marginal distributions 4 2 dependence model the dependence model considers the asymptotic structure of the conditional distribution pr y i y i y i y i arising from a d dimensional random variable y y 1 y d with laplace marginal distributions y i denotes the vector variable y excluding component y i to examine the limiting conditional distributions as y i the growth of y i must be controlled according to its dependence on y i so that the limiting distribution has non degenerate marginals this is achieved by assuming that for a given i there exist vector normalizing functions a i y i and b i y i of the same dimension as y i such that for all fixed z i 7 lim y i pr y i a i y i b i y i z i y i y i g i z i where the limit distribution g i has non degenerate marginal distributions g j i for all j i eq 7 is assumed to hold exactly for all values of y i u y i for a suitably high threshold u y i as a consequence the random variable z i defined by 8 z i y i a i y i b i y i is independent of y i for y i u y i and has distribution function g i the extremal dependence behaviour is then characterized by location and scale functions a i y i and b i y i and the distribution function g i due to the transformation to laplace marginals the form of the normalizing functions a i y i and b i y i falls into a simple parametric family both for positively and negatively associated variables given by 9 a i y i α i y i b i y i y i β i where the vector constants α i and β i have components α j i 1 1 and β j i 1 for all j i no such simple class of parametric models exists for g i as no specific structure is imposed by the limiting operation 7 g i is therefore modelled nonparametrically the resulting dependence model is a multivariate semiparametric regression model of the form 10 y i α i y i y i β i z i for y i y i u y i where i 1 d for a large value of y i the behaviour of the remaining components in y is thus described by eq 10 the constant α j i describes the strength of dependence between y j on large values of y i while β j i describes how the variability of y j changes with increasing y i positive and negative values of α j i corresponds respectively to positive and negative association between the variables y i y j positive β j i means that the variance of y j increases as y i increases whereas negative β j i means that the variance decrease for α j i 1 and β j i 0 y i y j are said to be asymptotically positive dependent the quantiles of the distribution of y j y i y i grows at the same rate as y i for y i and for α j i 1 and β j i 0 they are asymptotically negative dependent otherwise they are asymptotically independent see e g 49 4 3 inference as stepwise estimation is generally simpler than joint estimation inference for marginal and dependence structure is undertaken stepwise in 40 first the parameters of the marginal distributions of the components of x are estimated after transformation to laplace marginals the dependence parameters are estimated assuming that the marginal parameters are known since the conditional extremes model by heffernan and tawn offers nothing new regarding marginal inference we focus here on the estimation of the conditional model parameters inference for the parametric part of the conditional model consists of estimating the values of the vector constants α i and β i based on the sample data during inference a parametric model for g i must be assumed specifically the components of z i are falsely assumed to be mutually independent and gaussian distributed the gaussian distribution was selected for its simplicity and superior performance heffernan and tawn 2004 if z i has marginal means and standard deviations denoted by the vectors μ i and s i then following eq 10 the means and standard deviations of the random variables y i y i y i for y i u y i are α i y i μ i y i β i and s i y i β i respectively from the k 1 n u y i observations of y y i u y i the maximum likelihood estimates of the unknown parameters α i β i μ i and s i are then found from the following objective function log likelihood 11 q i α i β i μ i s i j i k 1 n u y i log s j i y i i k β j i 1 2 y j i k α j i y i i k μ j i y i i k β j i s j i y i i k β j i 2 numerical maximization of q i over the parameter space of the model is required to obtain the point estimates α ˆ i β ˆ i μ ˆ i s ˆ i with μ i and s i treated as nuisance parameters the distribution g i is finally estimated nonparametrically by using the empirical distribution function g ˆ i of replicates of the random variable z ˆ i defined by 12 z ˆ i y i α ˆ i y i y i β ˆ i for y i y i u y i the resulting observations z ˆ i provide a sample from the multivariate distribution g i a problem identified by 49 is that due to the omission by heffernan and tawn of imposing joint constraints on the parameters of the semiparametric regression model α j i and β j i and the nonparametric element of the model inconsistencies with the marginal distributions can arise the strongest form of extremal dependence between two variables is asymptotic dependence coles et al 1999 given by α j i β j i 1 0 in the dependence model 10 this suggests that when α j i 1 β j i cannot be positive however as the parameter space is α j i β j i 1 1 1 such a combination of parameters is allowed in the original model this together with the nonparametric element of the model results in the possibility of the estimated joint probabilities to exceed the marginal probabilities to avoid this it is recommended to impose the constraints given in 49 on α j i and β j i if strong extremal dependence is expected between the variables for pairs y i y j these constraints follows from requiring a stochastic ordering assuring that conditional quantiles for any form of asymptotic independence are not larger than under asymptotic positive dependence nor smaller than under asymptotic negative dependence these constraints are imposed only on extrapolations i e for y i v where v is a value above the maximum observed value of y i the reader is referred to 49 for further details 4 4 conditional simulation since the dependence model 10 is semiparametric estimates for various extremal statistics must be acquired by simulation we thus generate random samples of x x i v x i where v x i f x i 1 f y i u y i for each i using the estimated conditional models from these samples monte carlo approximations of functionals of the joint tails of the distribution of x can then be obtained the sampling algorithm for each i is as follows 1 simulate y i from a laplace distribution conditional on it exceeding its cumulative probability corresponding to f x i v x i 2 sample z i from g ˆ i independent of y i 3 obtain y i α ˆ i y i y i β ˆ i z i 4 transform y y i y i to x using the inverse of transformation 5 let us say that we from the data sample at hand have n independent observations of x where x is extreme in at least one component a simulated random realization of this process covering the same period is then obtained by simulating n pseudo observation of x by the sampling algorithm above typically many of the observed x k k 1 n comprises observations where more than one of the components x i k are simultaneously extreme this raises the question of how to determine the number of times one should condition on each x i to obtain x i for all i 1 d resulting in a total of n simulated pseudo observations of x in the proceeding simulation of residual current profiles we principally follow the procedure proposed by 45 to estimate this the main argument in 45 is that since the conditional extremes model is motivated asymptotically it is most appropriately applied to the conditioning variable whose value is most extreme in its marginal distribution transformed to y this means that for the observations y k k 1 n the number of times to condition on variable y i during the conditional simulation is determined by the number of times which y i was the largest component of y k k 1 n the rate at which to condition on each y i or x i is thus found as 1 n k 1 n 1 y i k max y k where 1 a denotes the indicator function of some event a in section 5 4 a slight modification of this procedure is described the modification being introduced to account for temporal dependence 5 application in this section the statistical modelling of the residual current components is described in detail and applied to obtain extreme vertical residual and total current profiles at the two considered locations munkskjæra and salatskjæra this includes modelling the complete marginal distribution bulk and tail distribution of each residual current component characterizing the dependence structure between the components by application of the conditional extremes model heffernan and tawn 2004 and outlining the monte carlo procedure used to simulate extreme current velocity profiles following 45 67 73 the tidal and residual currents are assumed to be independent inspection of plots of observed residual currents against observed tidal currents from the measurements as in 68 confirmed that the independence assumption generally appears reasonable just as assuming tide surge independence when estimating extreme sea levels this can however be a slightly conservative assumption in some cases particularly in shallow water see e g 41 66 and 71 subsequent to the statistical modelling extreme total current velocity profiles can then be obtained by randomly adding predicted tidal current profiles to realizations of extreme residual current profiles we start by defining the considered residual current velocity components 5 1 residual current components the directionality of the extreme currents needs to be accounted for in the analysis this requires characterization of the extremal behaviour of both positive and negative principal current components the observed residual current velocities are therefore split into four velocity components at each depth specifically for each depth i we consider the positive major axis component u r m i the negative major axis component u r m i the positive minor axis component u r m i and the negative minor axis component u r m i see fig 8 since positive and negative velocity components along a given axis at a given depth are mutually exclusive events one can only observe one of the major axis and one of the minor axis components simultaneously at each depth in order to work with only positive variable values the residual current velocity components for the k 1 n observations are defined as 13 u r m i k u r m i k u r m i k 0 u r m i k u r m i k u r m i k 0 u r m i k u r m i k u r m i k 0 u r m i k u r m i k u r m i k 0 a velocity component is said to be unobserved if the condition in eq 13 is not fulfilled denoting the total number of residual current components d an observed velocity profile thus contains d 2 observed velocity components and d 2 unobserved components the total number of residual current components is equal to 24 at munkskjæra and 20 at salatskjæra as the mean current velocity is included in the tidal current the sample size of each residual current velocity component u r m i and u r m i is expected to be approximately equal to n 2 5 2 marginal modelling marginal modelling is performed independently for each of the four residual current components u r m and u r m at each depth from now we denote the full set of residual current components as x i i 1 d u r m 1 u r m 1 u r m 1 u r m 1 u r m 2 and refer to u r m and u r m only when needed since the aim is to describe all values of x i that can occur with any large x i a model for the complete marginal distribution f x i of each x i is required for this purpose we essentially follow 40 adopting the semiparametric model by 16 17 which comprises the generalized pareto distribution for x i above a high threshold u x i and the empirical distribution function below the threshold such models are sometimes referred to as mixture models the empirical distribution function describing the bulk of observations is established based on all hourly observations below the threshold while inference for the tail distribution is made by application of the peaks over threshold pot method to account for marginal temporal dependence at extreme levels as the latter distribution refers to events cluster maxima rather than individual sequential observations an approach is described to properly connect the all observation based bulk distribution and the event based tail distribution similar approaches have been used by for instance 60 in connection with estimation of extreme sea levels the marginal model of each residual current velocity component thus consists of 1 a bulk distribution describing observations below a high threshold by the empirical distribution function 2 a tail distribution identifying cluster maxima above the threshold by the peaks over threshold method and fitting these maxima to the generalized pareto distribution 3 connecting the bulk and the tail distribution to obtain the complete marginal distribution giving particular emphasis to the fitting of the tail distribution the points above will be outlined in the current subsection 5 2 1 bulk distribution marginally points below the threshold u x i are relatively dense and are therefore well described by the empirical distribution function f x i i e 14 f x i x num elem x i k k 1 n x i x n x i for x u x i where n x i is the number of observations of variable x i the denominator in the expression for f x i is sometimes written n x i 1 the difference being negligible here the threshold u x i for each residual current velocity component is decided as part of the fitting procedure for the tail distribution 5 2 2 tail distribution from univariate extreme value theory it can be shown that the generalized pareto gp distribution arises as the limiting distribution for excesses over thresholds davison and smith 1990 a result originally due to 65 and 1 1 1 many practitioners within engineering disciplines use the weibull distribution as an alternative based on empirical goodness of fit to data this is a credible candidate model for our data as well the justification for the generalized pareto distribution as the limiting distribution for excesses over thresholds and subsequent parameter and return level estimates are based on an assumption that the exceedances are independent this is not a valid assumption for our hourly measured residual current velocities temporal dependence is clearly observed particularly for the major axis velocity components resulting in a tendency of extremes to cluster however if we introduce a condition that limits the dependence structure of the sequence it can be shown that the maxima of dependent stationary series follows the same distributional limit laws as those for independent series leadbetter et al 1983 coles 2001 the peaks over threshold pot method with declustering is applied to limit marginal temporal dependence at extreme levels specifically for exceedances above the threshold u x i only the largest excess within a cluster of exceedances is considered the cluster peak excess or cluster maxima the most common definition of clusters is as runs of consecutive exceedances with an additional temporal separation criterion caires and sterl 2005 we define the cluster peak excesses simply as the highest peaks above the threshold with a minimum peak to peak separation of τ hours a peak being defined as the highest observation of consecutive exceedances this cluster peak excess definition is related to the runs method described in e g 81 though we feel it is more convenient to impose the temporal separation criterion directly on the peaks rather than on the required below threshold time it should be mentioned that such definitions are often asymptotically equivalent leadbetter 1991 the marginal tail of x i for i 1 d describing the distribution of the independent cluster peak excesses x c i conditional on x c i u x i is thus modelled by 15 g x c i x 1 1 ξ i x u x i σ i 1 ξ i ξ i 0 1 exp x u x i σ i ξ i 0 where u x i is a high threshold for x i and ξ i and σ i are shape and scale parameters respectively with σ i 0 and the operator s max s 0 if ξ i 0 the distribution of excesses has an upper bound of u x i σ i ξ i while for ξ i 0 the distribution as no upper limit the case ξ i 0 is interpreted as the limit ξ i 0 resulting in the exponential distribution with mean excess σ i before the parameters ξ i σ i of the generalized pareto distribution can be estimated a suitable threshold u x i and minimum peak to peak separation time τ x i have to be decided for each x i the minimum peak to peak separation time is usually set based on physical considerations often related to typical storm durations for parameters causing environmental loads while the threshold can be more difficult to decide the issue of threshold selection amounts to a trade off between bias and variance too low a threshold is likely to violate the asymptotic basis of the model leading to bias while too high a threshold leads to fewer excesses with which the model can be estimated leading to high variance coles 2001 we use the graphical diagnostics outlined by 13 and 78 in addition to the anderson darling statistic for deciding the threshold the so called mean residual life plot and the parameter stability plots are based on the fact that if the generalized pareto distribution is valid for cluster peak excesses of the threshold u 0 it should also be valid for all thresholds u u 0 these plots are obtained by calculating the mean of the cluster peak excesses e x u x u and estimating the gp parameters over a range of different thresholds above a threshold for which the generalized pareto distribution is valid the mean residual life plot and the scale parameter σ should be approximately linear in u while the shape parameter ξ should be approximately constant the scale parameter can alternatively be reparametrized as σ σ ξ u so that σ should also be constant with u the lowest threshold for which the above holds true taking sample uncertainty into account is then selected we found subjectively the mean residual life plot and the shape parameter stability plot to be the most informal see fig 9 for an illustration due to the large number of considered residual current components a suitable common threshold corresponding to a given non exceedance probability as measured by f x i x was selected one threshold for all the major axis components and one for the minor axis components the gp parameters were estimated by maximum likelihood see e g 38 though several alternative methods exists 20 21 and 58 are useful references in this respect return level plots were used to validate the threshold choices and more generally the validity of the gp distribution such plots where the estimated n year return level is plotted against n the latter on logarithmic scale are particularly convenient since the effect of extrapolation is highlighted for a given threshold and corresponding estimated gp parameters the n year return level for variable x i x i n is given by 16 x i n u x i σ i ξ i n c i 1 y n ξ i 1 denoting the total number of observations by n the theoretical possible number of here hourly observations during a year by n 1 y 8766 and the total number of observed cluster peak excesses of variable x i by n c i it follows that the estimated average number of cluster peak excesses per year for variable x i n c i 1 y in eq 16 is given by n c i 1 y n c i n 1 y n where n n 1 y is the effective number of years of measurements and accounts for possible missing observations gaps in the measured time series assuming they are missing at random as the graphical diagnostics discussed above can be rather subjective and sometimes difficult to interpret the anderson darling statistic was used as an additional objective goodness of fit test this statistic is a modification of the cramér von mises static giving more weight to the tail of the distribution making it particularly powerful for detecting tail discrepancies we found it very convenient to use this statistic as a quick way of assessing the overall performance of the fitted marginal models looking at the number of model rejections at the 5 significance level for a given choice of thresholds for all x i plotting the calculated value of the anderson darling statistic and the corresponding critical test value over a range of thresholds for each x i was also helpful reference is made to 11 for additional details regarding the anderson darling statistic and critical test values for the generalized pareto distribution an initial minimum peak to peak separation time τ of 30 h was selected for all variables before the diagnostics above were utilized to identify a suitable cumulative threshold for the major and minor axis components the initial separation time was decided based on time series inspection and knowledge of typical storm durations in the area in addition it seemed rational to adopt a separation time larger than the period of the semidiurnal and diurnal tidal constituents so that dependence introduced due to possible tidal residual current interaction and lack of fit of the harmonic model is limited if inertial oscillations were important the inertial period at the given latitude should probably be considered as well see generally 89 chap 5 using the initial τ a threshold corresponding to the 95 quantile for the major axis components u r m i and the 94 quantile for the minor axis components u r m i were found to give reasonably stable results at both locations this is similar to that found by 73 and 45 both used the 95 quantile though strictly the numbers are not directly comparable due to differences in the involved variables a simple assessment of the threshold sensitivity on the estimated marginal extremes were made by plotting the estimated 50 year return level for some of the velocity components for a range of thresholds these estimates were generally surprisingly stable for a broad range of reasonable threshold choices this is reassuring considering the subjectivity involved in the threshold selection process having decided the threshold the final peak to peak separation time was determined by considering different values of τ ranging from 12 48 h looking at the effect changes in τ had on the mentioned diagnostics the return level plots were particularly informal in this context as with the threshold choice the issue is a trade off between bias and variance a minimum peak to peak separation time τ 30 hours gave the most stable results and was thus selected however the results were not particularly sensitive to the precise value of τ this is in line with that reported in several other studies davison and smith 1990 such as by 82 in the case of sea levels with the final choice of thresholds and minimum peak to peak separation time the fitted gp models passed the anderson darling test at the 5 significance level for all x i at both locations a representative illustration of the mean residual life and shape parameter diagnostics are shown in fig 9 for the u r m velocity component at 22 m depth at munkskjæra above the chosen threshold indicated by the red asterisks the mean residual life plot is seen to be approximately linear in u while the shape parameter is approximately constant taking sampling variability into account indicated by the confidence intervals the maximum likelihood estimates of the gp parameters are given in fig 10 a and 10 b for all velocity components at munkskjæra and salatskjæra respectively the shape parameter is in most cases negative indicating the existence of an upper limit for these residual current velocity components the goodness of fit of the gp distribution and the effect of extrapolation is illustrated in the return level plot in fig 11 again for u r m at 22 m depth at munkskjæra 95 confidence intervals based on profile likelihood are added see e g 13 such intervals are usually more accurate than wald type intervals important points to note are the asymmetric confidence intervals reflecting the greater uncertainty about high values and the large uncertainties that arises once the model is extrapolated to values well beyond the range of observations with only 1 5 2 5 years of observations large uncertainty will thus inevitably be associated with estimates of return levels corresponding to return periods ranging from 10 100 years typically required for design the estimated 10 and 50 year return values for all residual current components including confidence intervals are found in table a 2 in the appendix together with all relevant marginal distribution parameters it is interesting to note the particularly wide confidence intervals seen for most of the velocity components with a high value of the gp shape parameter though considering the physics of our problem the most extreme intervals appears unrealistic in such cases better estimates could probably be obtained by penalized maximum likelihood see 14 5 2 3 connecting the bulk and tail distribution the bulk distribution describes all hourly observations below the thresholds for each x i i 1 d while the tail distribution describes only the independent cluster peak excesses above the threshold for consistency below and above the threshold it would therefore be favourable to transform the tail distribution into a distribution describing arbitrary hourly exceedances rather than just the cluster maxima this is further motivated by our aim of characterizing instantaneous current profiles during an extreme event simultaneously observed threshold exceeding current components of the profile might not necessarily all correspond to marginal cluster peak excesses the goal is to transform the distribution of the independent cluster peak excesses into the distribution of dependent hourly sequential residual current velocities above the thresholds u x i i 1 d to transform the distribution of the independent cluster peak excesses into the distribution of all hourly excesses it is necessary to introduce the so called extremal index θ which lies in the interval 0 1 see e g 3 10 the extremal index has a physical and intuitively appealing interpretation due to 52 as the inverse of the limiting mean cluster size if θ 1 exceedances of an increasing threshold occur singly in the limit while if θ 1 exceedances tend to cluster in the limit the finite sample approximation of θ due to 85 θ x is an index varying with the level x for large x the physical interpretation of θ holds for θ x and its inverse can thus be calculated as 17 θ ˆ x 1 x number of exceedances of x number of cluster maxima exceeding x the above equation can however not be used to directly calculate θ x 1 x for x beyond the range of the data and the uncertainty of the empirical estimates of θ ˆ x 1 x increases for increasing x also inevitably θ ˆ x 1 x 1 as x approaches the highest observation of the sample denoting the extremal index obtained at the marginal threshold u as θ u 83 notes that the distribution function of an arbitrary exceedance is influenced by the factor θ u θ x 1 following a similar notation as in 27 conditional on x u we then have the following subasymptotic relation between quantiles of the distribution function of an arbitrary exceedance g x x and that of an independent cluster peak excess g x c x 18 1 g x x θ u θ x 1 x 1 g x c x given x u θ u can be interpreted as a scale factor to account for the increased sample size of all excesses relative to the cluster peak excesses while θ x 1 x accounts for the increased probability of observing an arbitrary exceedance above x due to dependence relative to if all exceedances above the level were independent cluster peak excesses the complete marginal distribution f x i of each x i can thus be taken as 19 f x i x 1 p u x i θ u x i θ x i 1 x 1 g x c i x for x u x i f x i x for x u x i where g x c i x is the generalized pareto distribution given by eq 15 f x i x is the empirical distribution function and p u x i 1 f x i u x i is the probability of exceeding the threshold u x i clearly an estimate of θ x 1 x is required beyond the range of the data a rational approach would then be to estimate θ x 1 by eq 17 at regularly spaced levels fit a function to these estimates and extrapolate as commented in 85 the choice of an appropriate function is not immediately obvious but results from extreme value theory leadbetter 1983 suggest that in the limit a constant is the correct function denoting the asymptotic limiting extremal index θ l i m and the associated level x l i m for which θ x x θ l i m for all x x l i m we have tested three different approaches for estimating θ l i m and x l i m in addition to θ x x on the intermediate range u x x l i m 1 simply take θ x x θ u for all x u implying θ l i m θ u from eq 18 it can be seen that this is equivalent to assuming that the distribution function of an arbitrary exceedance equals that of an independent cluster maxima 2 using an approach inspired by 60 taking θ l i m 1 and having θ x 1 x varying linearly from θ u 1 at x u to 1 at x x l i m where x l i m is the lowest level x for which eq 17 gives θ ˆ x 1 1 3 following 85 estimating θ x 1 by eq 17 at regularly spaced levels for u x x m a x where x m a x is the highest observation and using simple weighted least squares to estimate θ l i m 1 the same approach can be used to estimate x l i m we adopted the estimators 20 θ ˆ l i m 1 k j θ ˆ x 1 x j k j 21 x ˆ l i m k j x j k j where k j is the number of cluster maxima above the level x j on u x x ˆ l i m θ x 1 x is taken to vary linearly between θ u 1 at x u and θ ˆ l i m 1 at x x ˆ l i m the resulting estimates of θ x 1 x are displayed in fig 12 a for residual current component u r m at 4 m depth at munkskjæra highlighting the differences between the three considered approaches in fig 12 b using eq 19 the resulting survival function f x 1 f x is plotted against the empirical all observation estimates in line with results in 27 approach 1 fits the empirical estimates poorly overpredicting the probability of exceeding a particular level x overall approach 2 mazas et al and approach 3 tawn and vassie performed more or less equally well however as approach 3 has a better theoretical underpinning when extrapolating beyond the range of observations this is our preferred approach the complete marginal distribution function described by eq 19 for each residual current component x i is thus calculated with θ x i 1 x as determined by approach 3 as an anecdote to the above discussion it is worth mentioning that we performed the dependence modelling and the conditional simulations described in the following subsections using all the three approaches described above for estimation of θ x 1 x somewhat surprisingly at both considered locations only small differences were observed in the resulting predicted extreme current velocity profiles this seems to suggest that provided the same approach is used to estimate θ x i 1 x for all x i accurate estimation of the extremal index is not the most critical step in the current statistical model 5 3 dependence modelling we now turn to the modelling of the extremal dependence between the residual current velocity components most of the details have already been described in section 4 however some modifications to the original model heffernan and tawn 2004 are needed to allow modelling of instantaneously observed mean current velocity profiles the conditional extremes model by heffernan and tawn concerns the estimation of functionals of the conditional distribution of x i x i v x i as originally formulated each observation of x is assumed to be independent and with no components of x missing both these assumptions are violated for our data calling for an extension of the original model solutions to similar problems were given by 50 however due to differences in the involved variables the proposed methods are unfortunately not applicable to our data more specifically the issues which needs to be addressed are 1 the hourly recorded current velocity profiles displays temporal dependence requiring a condition to be introduced to limit the effect of dependence owing to the multidimensionality of the problem it is however difficult to introduce a very strict condition to reduce dependence as the observed individual residual current components do not necessarily peak at the same time during an event with strong consecutive currents it is as opposed to for univariate peaks over threshold modelling difficult to decide which of the observed current profiles that should be considered as the peak observation 2 the d dimensional vector of simultaneously observations x includes d 2 observed and d 2 unobserved residual current components this is not an issue of missing values due to measurement malfunction but simply a consequence of how the current components are defined cf section 5 1 the dependence model is applied to the data after transformation to laplace margins the original vector variable x with marginal distributions defined by eq 19 is thus transformed componentwise by transformation 5 to a vector variable y with laplace distributed marginals point 1 above is taken care of marginally by application of the peaks over threshold method in the same way as when fitting the marginal tail distributions in section 5 2 2 this means that the semiparametric regression model 10 which for large y i describes the behaviour of the remaining components in y is only fitted to observations of y y i u y i where y i corresponds to a cluster peak excess for each conditioning variate y i i 1 d of independent cluster peak excesses the resulting observations y y i can then also be considered independent the minimum peak to peak separation time τ y i is set equal to the marginal minimum peak to peak separation time τ x i here 30 h the dependence thresholds u y i which does not need to equal the marginal thresholds u x i in the sense that f y i u y i f x i u x i can be decided based on assessing the stability of the estimates of α i β i for a range of thresholds and examining the independence of z i and y i see 40 for further details regarding diagnostics point 2 constitutes an issue for structure and consistency in the system of equations and for the construction of and simulation from the distribution function g i consistency in the system of equations can be obtained by careful vector treatment ensuring that the conditional model 10 is fitted only to the components y j i of y i which were observed simultaneously as y i in the construction of g i z i we take advantage of the lack of imposed structure on g i in 40 and the false assumption that the components of z i are mutually independent and gaussian distributed during inference the independence assumption renders possible componentwise estimation of the vector constants in eq 10 this makes it easier to allow for a few difficulties that arises because of how the residual current components making up y are defined eq 13 specifically for a given conditioning variate y i corresponding to a cluster peak excess there are certain restrictions on which components in y i that can be observed simultaneously as y i for instance if y i corresponds to a positive major axis velocity component at a particular depth it is not possible to simultaneously observe a negative major axis velocity component at the same depth and vice versa also due to spatial dependence there will generally be other combinations of y i y j i j i that are highly unlikely to occur together physically it is quite intuitive that given an extreme current velocity in a particular direction at one depth the current would be expected to flow in approximately the same direction at adjacent depths the practical consequence of this is that for some y j i we have no or very few observations to estimate the corresponding dependence parameters α j i β j i for the y j i with no observations this is not an issue g i is modelled by its empirical distribution function so for a given y i such y j will have zero probability of occurrence making dependence modelling redundant for the y j i with just a few observations estimates of α j i β j i will be unreliable due to the small sample size after careful inspection of the data at the two considered locations it was found that such y j i always corresponded to velocity components in the opposite direction 2 2 if for instance y i corresponds to a positive major axis component we mean by opposite direction y j i corresponding to negative major axis components of y i at adjacent depths physical considerations suggests that these pairs y i y j are negatively dependent denoting the number of observations of y j i by n y j i we therefore set α j i β j i 1 0 corresponding to negative asymptotic dependence for all y j i with just a few observations 3 3 if found to be unimportant for extremes an alternative can be to simply neglect ignore these observations for the data at hand we defined a few as n y j i 12 the remaining α j i and β j i and nuisance parameters μ j i and s j i are estimated by numerical minimization of the negative of the components q j i of the objective function 11 over the parameter space of the model only summing over the y i i and y j i observed simultaneously for both computational efficiency and stability it was found favourable to first perform the minimization without imposing the constraints in 49 and then using the resulting estimates as a starting point run the minimization again with the additional constraints in 49 the constraints were imposed on y i v where for each y i v was taken equal to the estimated marginal 50 year return level of y i as found in 49 the results showed little sensitivity to the precise choice of v as long as v was taken above the maximum observed value of y i based on the point estimates α ˆ i β ˆ i the observations z ˆ i of the random normalized residual z i are given by eq 12 the number of observations of z ˆ i is equal to the number of cluster peak excesses of y i denoted n y i and provide a sample from g i since each y i formally contains d 2 1 observed and d 2 unobserved components so will z ˆ i the dependence between the components partly in terms of magnitude but more importantly in terms of which components are simultaneously observed and which are unobserved is taken care of by modelling g i by its empirical distribution function g ˆ i the probability of observing a given component z j i from g ˆ i is equal to n y j i n y i when sampling from g ˆ i in the subsequent conditional simulation the number of observed z j i and y j i will thus follow a binomial distribution by conditioning on the cluster peak excesses of each of the d variates in turn the dependence model was estimated as in 45 and for many of the parameters examined in 40 it was found that conditional thresholds equalling the marginal thresholds in terms of quantiles gave reasonable stable results a dependence threshold equivalent to the 95 quantile for y i corresponding to major axis components and the 94 quantile for y i corresponding to minor axis components was thus selected the resulting estimates of the dependence parameters α j i and β j i when conditioning on y 1 and y 3 at munkskjæra are shown in fig 13 including the effect of imposing the 49 constraints for conditioning variate y 1 corresponding to the major axis velocity component u r m 1 strong dependence is seen with u r m at all other depths weaker dependence is seen with minor axis components there are no observations of simultaneously reversing currents at other depths u r m except one low velocity observation 1 cm s at depth number 3 16 m indicated by the single black dot with α 1 β 1 1 0 in fig 13 a for conditioning variate y 3 corresponding to the minor axis component u r m 1 strong dependence is seen for other u r m getting weaker with depth only weak dependence with other components is seen current reversals occur relatively frequent from depths 3 6 u r m the frequency increasing with depth qualitatively similar results as in fig 13 were observed when conditioning on other major and minor axis components at munkskjæra it is noticeable that the estimated dependence parameters are generally smoother with depth for velocity components with strong dependence e g for u r m in fig 13 a than for those with weak dependence as the estimated parameters are expected to vary relatively smoothly with depth this indicates that it can be more challenging to accurately estimate the dependence parameters for velocity components with weak association in addition the estimated α are usually smoother than those for β 47 48 notes that there are some redundancy between β and the nuisance parameters μ and s which can probably help explain this as a consequence it is observed that the non smooth parameter estimates are to some extent compensated by the residuals z j i in the conditional simulation yielding smoother simulated current profiles an illustration of the fitting of the dependence model 10 by log likelihood 11 is shown in fig 14 simultaneous observations of y 13 corresponding to u r m 4 and cluster peak excesses of conditioning variate y 1 corresponding to u r m 1 are displayed together with the fitted mean and standard deviation of y 13 as a function of y 1 the estimated near asymptotic dependence between these two variables as indicated in fig 13 a is seen to be well supported by the data in fig 14 b the resulting normalized residuals z ˆ 13 1 plotted against y 1 are shown such plots where the independence of z j i and y i is examined independence is a modelling assumption are part of the diagnostics to assess model fit no obvious dependence between z 13 1 and y 1 is seen 5 4 conditional simulation as previously mentioned estimates of various extremal statistics must be acquired by simulation the simulation is performed using the monte carlo sampling algorithm described in section 4 4 to account for temporal dependence a slight modification of how to determine the number of times which to condition on each variable has to be made compared to that proposed by 45 following the methodology from the marginal and dependence modelling we consider only the marginal cluster peak excesses of each x i i 1 d transformed to laplace distributed variables y i the yearly number of times which to condition on each y i is then determined by the number of times which y i is the only or the largest simultaneously observed y corresponding to a cluster peak excess divided by the effective number of years of observations we thus ensure that the number of simulated current profiles per year is equal to the average number of yearly observed residual current profiles with at least one velocity component corresponding to a marginal cluster peak excess when performing the conditional simulation a critical point has to be made clear in order to ensure consistency regarding how to determine which components of the simulated y i that should be observed and which should not the key point is that subsequent to having simulated a value of the conditioning variate y i we sample z i by randomly drawing with replacement one of the n y i observed vectors z ˆ i making up g ˆ i the d 2 1 observed components of the drawn z ˆ i then determines which components of the simulated vector y i obtained by eq 10 that are observed and which are not in that way the generally strong dependence between the components in terms of which are simultaneously observed are taken care of physically this ensures that the directional misalignment of the current velocity at different depths for a simulated current profile is approximately within the range observed in the measurements the components of z ˆ i can thus not be modelled as being independent i e g ˆ i z i j i g ˆ j i z j i a simplification involved using this approach is that the probability of observing a particular y j i does not depend on the magnitude of the simulated value of y i though quite laborious it would be possible to also model such dependence but as long as the conditional threshold is set sufficiently high it does not appear to affect the results in any critical way a simulated total current profile is obtained by adding a simulated residual current profile and a predicted tidal current profile assuming the residual and tidal current to be independent since the tidal current for practical purposes can be considered as periodic with period 18 6 years the nodal cycle predicting hourly tidal current profiles over this period is sufficient to fully describe the distribution of the tidal current profiles the predictions are made by the harmonic analysis described in section 3 2 and the tidal current profiles are drawn at random from the resulting sample of size 18 6 8766 the number of hourly observations during 18 6 years having decided the yearly number of times which to condition on each y i i 1 d the full sampling algorithm for each y i for simulating total current velocity profiles over a desired period is as follows 1 simulate a cluster peak excess of x i u x i from the generalized pareto distribution eq 15 transformation 5 via eq 19 yields a simulated y i 2 sample z i by drawing with replacement one of the n y i observed vectors z ˆ i 3 obtain y i α ˆ i y i y i β ˆ i z ˆ i the d 2 1 observed components of z ˆ i determines which components of y i that are observed and which are not 4 transform y y i y i to x using the inverse of transformation 5 to obtain a simulated residual current velocity profile 5 draw with replacement a tidal current velocity profile from the sample of 18 6 years of predicted tidal current profiles 6 add the realizations of residual and tidal current profiles to obtain a simulated total current velocity profile due to the introduction of an extremal index varying with level x in eq 19 the inverse of transformation 5 in step 4 above used to transform y to the original scale x cannot always be solved analytically to avoid the use of time consuming numerical solvers in the simulation procedure it is favourable to in advance fit a higher order polynomial to describe the relationship between x i and y i on the range u x i x x l i m i the range for which θ x i 1 x is not a constant if interest is in submodels obvious modifications to the simulation procedure must be made 5 5 simulated current profiles with the estimated marginal and conditional extremes models arbitrary long time series of extreme current velocity profiles can be simulated by the sampling algorithm described in section 5 4 to illustrate some results we have simulated 1000 years of extreme current profiles at both munkskjæra and salatskjæra 4 4 the single core computational time was between 15 and 20 min on an intel i7 3 ghz processor for each location particular focus will be placed on the residual current profiles as it is only the residual current that is described by the conditional extremes model in fig 15 and 16 the resulting simulated median marginal 10 year return values for all four residual current components at each depth are illustrated for munkskjæra and salatskjæra respectively the median 10 year return values and the quantile intervals are found by splitting the 1000 years of simulated residual current profiles into 10 year periods extracting componentwise maxima fitting a generalized extreme value gev distribution to the resulting 100 block maxima for each variable and extracting the 2 5 50 and 97 5 quantiles from the gev distributions 5 5 simple empirical quantile estimates gave almost identical results for comparison and partial validation of the dependence modelling and simulation procedure the median 10 year return values and corresponding 95 profile likelihood confidence intervals calculated from the fitted marginals from section 5 2 are also shown note that the calculated median 10 year return values from the fitted marginals are not equal to the 10 year return values estimated by eq 16 as the latter correspond to 37 quantiles the simulated median return values are seen to closely follow those calculated from the fitted marginals the simulated values consistently being slightly higher the tendency of a small positive bias is primarily related to the choice of only accounting for temporal dependence marginally when determining the number of current profiles to be simulated per year though compared to the confidence intervals of the fitted marginals the difference is generally negligible the tighter confidence intervals from the simulations are a natural consequence of the parametric bootstrap procedure used to obtain them emphasizing the stochastic nature of the plots simulated and observed residual tidal and total current velocities at 4 m depth at both munkskjæra and salatskjæra are illustrated in fig 17 note that the velocities have been transformed back to eastward and northward components only the first 10 year period of the simulated velocities are shown to resemble the simulated velocities only observations from the measurements where at least one of the residual current components of the velocity profile corresponds to a marginal cluster peak excess are plotted the simulated residual current velocities based on the conditional extremes model are seen to closely follow the trends of the observations looking at the simulated total currents they are also seen to match the observed total currents quite well particularly at munkskjæra at salatskjæra fig 17 f there are apparent evidence of some interaction between the tidal and the residual current the main effect being suppression of total current velocities in the northeastern direction similar interaction though less pronounced is apparent in the eastward direction at munkskjæra such interactions are not captured by the statistical model as independence between the tidal and the residual current is an inherent assumption however when including all observations from the measurements in the plot not shown the interaction is less pronounced at salatskjæra in the context of extreme value analysis it is reassuring that the most extreme simulated total current velocities both in terms of magnitude and direction coincide reasonably well with those from the observations similar agreement between simulated and observed velocities are seen at all other considered depths so far only marginal and single depth results have been presented in isolation these results are of limited interest as our main objective is to characterize instantaneous extreme current velocity profiles with depth presenting such multivariate data in a visually appealing manner is however challenging and we therefore present the instantaneous current profile results in terms of integrated variables since we are usually interested in the resulting force vector due to the current velocity profile for design of coastal and ocean structures such a measure is considered to be the most informal two dimensional summary of the current velocity profiles let us consider the resulting force on a fixed surface piercing vertical circular cylinder with diameter d 1 m and submerged length h the length h is taken equal to 35 m for munkskjæra and 30 m for salatskjæra so that the bottom of the cylinder is at a depth slightly lower than the deepest measurement point at the respective locations 34 m and 28 m the resulting cartesian force components or loads l e and l n in the eastward and northward direction respectively are given by the quadratic drag equation which for a vertical cylinder exposed to a horizontal current takes the form 22 l e 1 2 ρ c d d h 0 u e z u e z 2 u n z 2 d z l n 1 2 ρ c d d h 0 u n z u e z 2 u n z 2 d z where we take the water density ρ 1000 kg m 3 and the drag coefficient c d 1 the eastward and northward velocities u e and u n are functions of the vertical coordinate z z 0 at the sea surface a two dimensional illustration is shown in fig 20 for all observed and simulated current velocity profiles eq 22 is solved by numerical integration over the length of the cylinder assuming a linear velocity profile between the discrete measurement points and a constant velocity above the highest and below the lowest measurement point by applying bivariate kernel density estimation see e g 80 a smooth empirical estimate of the joint probability density function f l e l n l e l n can be obtained the resulting joint probability densities are then transformed to polar coordinates force magnitude l and direction φ the conditional cumulative distribution function of force given direction f l φ l ϕ can then be estimated allowing for estimation of extreme quantiles of the resulting force given direction in fig 18 estimates of the 0 95 0 99 and 0 995 quantiles of the conditional distribution of force given direction due to the residual current and total current velocity profiles at munkskjæra are presented both from the simulations and the observations the observation samples are also plotted again only observations where at least one of the residual current components of the velocity profile corresponds to a cluster peak excess are considered fig 19 displays a similar plot for salatskjæra at both sites the quantiles of the forces from the simulated current profiles are seen to follow the trends of the observations well particularly the residual current forces the estimates of the 0 95 quantile residual current forces are very similar as is expected due to the limited sample size the higher quantiles estimated from the observations are expected to be poorly estimated empirically and are dominated by outlying data and the choice of kernel bandwidth parameter however in terms of both magnitude and direction the observed and simulated quantiles are still in reasonable agreement the slightly rough quantile curves from the simulations are a consequence of the semiparametric nature of the conditional extremes model though smoother estimates can obviously be obtained by increasing the kernel bandwidth the additional discrepancies for the total current forces compared to the residual current forces alone are primarily related to the increased variability randomness introduced when combining residual and tidal current profiles and residual tidal current interaction at munkskjæra fig 18 b there are some evidence of interaction between the tidal and residual current in the eastward direction making the independence assumption appear slightly conservative at salatskjæra fig 19 b similar effects are seen in the northeastern direction in addition to an apparent clustering of extreme total current forces in the eastward direction both these effects can be also be seen in fig 17 f notwithstanding these effects the simulated total current forces are generally in reasonable agreement with those from the measurements as it has already been demonstrated that the simulated current velocity components are consistent both marginally and jointly at each depth the agreement between simulated and observed resulting forces suggests that the simulated current velocity profiles are also consistent 6 design current velocity profiles from a structural design perspective the key output from a statistical analysis of current profiles is so called design current velocity profiles in principle such design profiles should not be estimated in isolation of other relevant environmental parameters however as the focus of this paper is on current velocity profiles only we do not consider the additional effect of other environmental phenomena such as waves and wind when deciding these reference is made to offshore standards and recommended practices such as 43 and 23 for important joint environmental and structure specific considerations an example of a pragmatic way of accounting for the effect of the additional wave induced fluid velocity is found in 88 resulting in increased importance weighting of near surface currents when designing coastal or ocean structures design values of important metocean parameters are required to ensure that the structure in question can withstand the extreme environmental conditions expected to occur at the location s where it is to be operated the required design values are usually specified in terms of return periods or annual probabilities of occurrence the reciprocal of the return period for a univariate metocean parameter the value corresponding to a given return period is uniquely defined once its marginal distribution has been established in the case of multivariate metocean data like current velocity profiles this is not so however and some sort of sub ordering principle see 2 for an overview is useful to employ for easier identification of multivariate extreme design values in the context of structural design sub ordering of multivariate environmental phenomena is usually achieved through a generic load model that effectively combines the environmental variables into a relevant response tromans and vanderschuren 1995 the resulting total drag force on the structure in question is among the most informal summary variables of the effect of the current velocity profiles on a wide range of marine structures since the current can usually be considered as a steady flow field where the velocity vector is only a function of depth iso 2015 dnv 2014 this force can usually be calculated without much effort at least within the degree of accuracy required here the first step in our approach for deciding design current velocity profiles therefore involves calculating the total drag force or another relevant response function due to the simulated current velocity profiles we outline the approach below by considering a simple example it should however be mentioned that due to the flexibility of the conditional extremes model a range of alternative strategies will also be applicable besides the one presented here 6 1 example design current velocity profiles on a vertical circular cylinder following the example in section 5 5 we consider the total drag force vector due to the current velocity profiles acting on a vertical circular cylinder with diameter d 1 m and submerged length h see fig 20 for a two dimensional illustration again h is taken equal to 35 m for munkskjæra and 30 m for salatskjæra this is a simplified but representative example of a floating moored structure exposed to currents for all the simulated extreme total current velocity profiles covering the 1000 year simulation period the cartesian total drag force components are obtained by solving eq 22 by numerical integration over the length of the cylinder at the munkskjæra site this means calculating the resulting force due to approximately 755000 current profiles on a state of the art laptop this required only a few minutes of cpu time when using a discrete step size d z 1 m in the numerical integration the cartesian force components are conveniently transformed to polar coordinates yielding force magnitudes l and associated directions φ by looking at the resulting response the total drag force vector instead of the current velocity profiles directly our multivariate problem has effectively been reduced to a two dimensional one 6 6 the entire extreme value analysis could indeed have been performed on the structural response load directly as opposed to the current profiles but this would have left us with little knowledge about the actual shape of the current velocity profiles causing the extreme responses n year design current velocity profiles can then be obtained by first estimating the n year drag force favourably as a function of direction and subsequently deriving corresponding current velocity profiles yielding an equivalent force 6 1 1 estimating design drag forces 73 describe various ways of estimating design values of a two dimensional vector variable with a magnitude and a direction applying these methods both omni directional and directional return values of the simulated drag forces can be obtained two aspects greatly simplifying this step are 1 since temporal dependence has already been partly accounted for in the monte carlo sampling algorithm the simulated drag forces can be considered as independent 2 provided the simulation period is substantially longer than the return periods of interest say at least 10 20 times longer return values can be estimated directly from the simulated sample without the need of first fitting a parametric probability distribution denoting the yearly number of simulated current velocity profiles n s 1 y the estimated omni directional n year return value of the drag force l n is obtained by solving for the load that satisfies 23 f l l n 1 1 n s 1 y n as we only consider return periods n that are substantially shorter than the simulation period the distribution function of the drag load f l l can conveniently be approximated by its empirical distribution function directional return values can be obtained by looking at the probability that the drag load is exceeded in a particular direction providing information on the required directional structural strength following the discussion in 73 each simulated observation of l φ has a resolved drag load in every non orthogonal direction i e the observation l ϕ has a resolved load l ψ l cos ϕ ψ in each direction ψ 0 2 π for regularly spaced directions ψ j the resolved load l ψ j can be calculated for all the simulated observations l φ the n year return value l n ψ j for the resolved loads in direction ψ j are then calculated by solving 24 f l n ψ j 1 1 n s 1 y n with f l ψ j approximated by its empirical distribution function instead of resolved loads one might be interested in the load l n ψ δ which is exceeded on average once every n years conditional on the direction being in sector ψ δ ψ δ being a sector of width δ centred on ϕ ψ such estimates will depend on the sector width δ which should be decided taking both the structure and the directional variation of the load into consideration note that the sectors do not need to be equal in size but for design iso 19901 1 recommends that the sector width should not be smaller than 45 to avoid over optimization iso 2015 to decide l n ψ δ the conditional distribution of l ψ δ given ψ δ is required i e 25 f l ψ δ ψ δ 0 l ψ δ ψ δ 2 ψ δ 2 f r φ r ϕ d ϕ d r p ψ δ where p ψ δ is the probability of being in sector ψ δ both p ψ δ and f l ψ δ ψ δ can be estimated empirically particularly for small δ a bivariate kernel density estimate of f r φ r ϕ can provide smoother directional estimates finally the return level l n ψ δ can be estimated by solving 26 f l n ψ δ ψ δ 1 1 n s 1 y n p ψ δ applying the three approaches described above figs 21 and 22 shows estimates of extreme drag forces corresponding to return periods n 10 50 and 100 years at munkskjæra and salatskjæra respectively at munkskjæra the critical directions are clearly bounded to the east and west looking at the resolved force estimates l n ψ we note that the 10 year force is approximately of equal magnitude in the eastward and westward direction however for increasing n the difference magnifies reflecting the long tail higher gp shape parameter of the westward flowing current components cf fig 10 a at salatskjæra the critical direction is in a quite wide sector to the southeast an interesting feature is the local force minima at approximately 135 seen for the conditional force estimates l n ψ δ this local minima is also apparent in the measurements fig 19 b and is probably related to the presence of a few small islands and skerries upstream for flow in this direction it is pleasing to observe that the proposed statistical model is able to capture such complex directional behaviour the estimates of l n ψ δ force conditional on direction have similar directional characteristics as those of the resolved forces l n ψ but are smaller in magnitude because large forces in other directions can have resolved forces which exceed observed forces in direction sector ϕ δ also the estimates of l n ψ δ have been made with a sector width δ 1 to better illustrate the shape of the simulated sample using such a small sector width as environmental criteria for design is highly unconservative see e g 35 for a discussion on the use of directional environmental criteria 6 1 2 estimating design current velocity profiles taking advantage of the large sample size 1000 years a simple and pragmatic way of obtaining n year design current velocity profiles is to identify a subsample of observations in proximity of a prescribed n year force level and direction and then use the corresponding subsample of current velocity profiles directly to construct a design profile focusing on critical directional sectors along the 10 year return value line of the resolved forces l n ψ this approach is used below to obtain estimates of 10 year design current velocity profiles at both munkskjæra and salatskjæra from fig 21 and 22 it is noted that at least for n 50 a longer simulation period would generally be required so that the number of observations at such extreme levels is increased at munkskjæra we concentrate on the eastward sector 60 100 and the westward sector 255 280 in both sectors 21 simulated force observations have been identified near the respective peaks of the 10 year return level of l n ψ as indicated by the black crosses in fig 21 we thus have a sample of 21 current velocity profiles in each sector all of which approximately give rise to the same total drag force in the same directional sector to decide the design profiles we first resolve the current velocity at each depth of each profile into a velocity component in the same direction as the resulting drag force of the profile the longitudinal current velocity and a component orthogonal to the direction of this force the orthogonal current velocity the median longitudinal current velocity at each depth then serves as a reasonable two dimensional estimate of the 10 year design current velocity profile in the respective sectors we preferred the median over the mean as it is less sensitive to outliers if a three dimensional profile is desired the same operation can be performed on the orthogonal current components as well even though the resulting vertically integrated force due to the orthogonal current profiles is zero their local effect might be of importance in fig 23 the resulting 10 year design current velocity profiles are shown for both directional sectors together with empirical 10 and 90 quantiles of the velocity components at each depth obtained from the respective samples of 21 current velocity profiles as a simple way of validating if the shape of the profiles appears reasonable the velocity profiles from the measurements causing the 5 largest forces in the two sectors are also included though one should be careful not to put too much weight on such a validation it is reassuring that the shape of the design current profiles is in agreement with the measurements in sector 60 100 the median longitudinal velocity profile in the total force direction in fig 23 a is seen to be relatively uniform with depth it is somewhat surprising that the lowest velocity of the profile is at the top bin 4 m however this is actually supported by the measurements the orthogonal velocity components in fig 23 b are generally of minor importance indicating an approximately unidirectional flow in sector 255 280 the median longitudinal velocity profile in fig 23 c peaks with a velocity of 1 m s at depth 10 m decreasing for deeper levels again the orthogonal velocity components in fig 23 d are of minor importance at salatskjæra the focus is on the main critical sector 95 130 in addition we investigate sector 145 170 to see if there is any particular difference in the shape of the current profiles south of the local minima at 135 as for munkskjæra 21 current velocity profiles have been identified in each sector in proximity of the 10 year return level of l n ψ see fig 22 following the same procedure of resolving the velocity components at each depth into longitudinal and orthogonal velocity components relative to the total drag force the resulting 10 year design current profiles for both sectors are shown in fig 24 in sector 95 130 the longitudinal design profile in fig 24 a contains its highest velocities at intermediate depths a shape that is supported by the measurements a minor anticlockwise rotation is seen for the orthogonal profile in fig 24 b for sector 145 170 the highest velocities of the longitudinal design profile in fig 24 c is found at the deepest levels a consequence of the high gp shape parameters of the positive major axis components at these depths cf fig 10 b this shape is not fully supported by the measurements however there seems to be a trend of the lower level velocities increasing more than those at the upper levels so as we are extrapolating beyond the measurements it does not appear unrealistic either compared to sector 95 130 both the design profile and the measurements indicate a decrease in the upper level velocities increasing the relative importance of the lower level velocities overall the performance of the proposed statistical model and the approach for deriving design current velocity profiles appears to be satisfactory both at munkskjæra and salatskjæra 7 conclusions and discussion in this paper we have described a method for statistical modelling of extreme vertical current velocity profiles the method is based on the conditional extremes model by 40 arguably providing the most useful and flexible current approach for modelling extremes in high dimensions as illustrated in fig 1 we consider current velocity components resolved along major and minor axes at each depth and decompose the total current into tidal and residual currents using harmonic analysis a complete marginal model for each of the both positive and negative residual current components is then constructed and the dependence structure is characterized using the conditional extremes model estimates for various extremal statistics are acquired by simulating under this model and extreme total current velocity profiles can be obtained by randomly adding predicted deterministic tidal current profiles to realizations of stochastic extreme residual current profiles as implemented these steps accounts for directionality spatial and temporal dependence and non stationarity introduced by the tide based on a long period of simulated current velocity profiles a simple approach for deriving design current velocity profiles is also proposed the statistical method has been tested using adcp data from two coastal locations in norway we have shown that the method provides good extrapolations at both locations this is confirmed both marginally for each velocity component jointly for the velocity components at each depth and for the full velocity profiles the latter is primarily illustrated in terms of integrated variables resulting force vector on a circular cylinder due to the current profiles compared to the most extreme velocity profiles observed from the measurements the derived 10 year design current velocity profiles appear realistic both considering their shape magnitude and direction even though we have only considered the upper part of the water column significant change in the velocity as a function of depth is seen for most of the design profiles the shape of the design profiles is also found to be quite different in the two considered critical directional sectors at both locations we believe that the proposed method represents a valuable addition to existing methods for deriving extreme current velocity profiles as the vertical current structure is expected to be more variable in deeper and or stratified waters the proposed method will be even more beneficial to apply there as presented here the method makes a number of assumptions which should be commented upon for instance the residual current velocities are assumed to be stationary processes for the data at hand this is a reasonable assumption however at other locations this might not be the case if there are important seasonal changes a separate model might be specified for each season as in 40 with the obvious drawback that each model has less available data for inference important aspects regarding modelling of extremes of non stationary sequences can be found in 13 chap 6 further we have assumed that tidal and residual currents are independent albeit this appeared reasonable for our data independence might be a conservative assumption particularly in shallow water if tidal residual current interaction is evident analogous methods as when estimating extreme sea levels could probably be applied to account for this see 22 84 and 60 due to the increased dimensionality of the data and the additional dependence modelling this is however expected to be more challenging to implement for current profiles than for sea levels inference for marginal and dependence structure has been undertaken componentwise using the conventional peaks over threshold method to account for temporal dependence a major problem for estimation of extreme currents is often to obtain data of adequate length so using only the cluster peak excesses might seem rather wasteful of data 28 29 argue that more accurate and precise parameter and return level estimates can be obtained by using all threshold excesses this could provide an interesting alternative to the peaks over threshold method used here and it would also remove the complication of connecting the bulk and tail distribution cf section 5 2 3 it should be noted that temporal dependence still needs to be accounted for when constructing confidence intervals and estimating return levels this is relatively straightforward in the univariate case but might prove more defiant for multivariate data such as current profiles also we do not know how this affects the performance of the conditional extremes model another aspect both relevant for the marginal and the dependence modelling is the possibility of applying joint estimation instead of componentwise parameter estimation this is a valuable alternative when there are constraints between the parameters enabling inferential efficiency to be gained heffernan and tawn 2004 for instance one might expect the tail behaviour of a particular velocity component e g positive major axis components at adjacent depths to display similar tail behaviour this is equivalent to saying that the generalized pareto parameters of the velocity component varies relatively smoothly with depth similarly the dependence parameters for a given velocity component are also expected to vary relatively smoothly with depth treating depth as a covariate parameter smoothness could for instance be controlled within a penalized likelihood framework see e g 9 47 in this respect ideas from the piecewise stationary extreme value model of 76 seems particularly interesting to use together with the modelling strategy proposed herein though clearly adding complexity to the analysis threshold selection is an important step in the proposed method both when fitting the marginal tail model and the dependence model due to the large number of considered residual current components a suitable common threshold corresponding to a given non exceedance probability was selected in the present work one for the major axis components and one for the minor axis components common threshold selection or other automated procedures is inevitable due to the large number of possible threshold choices still as also commented by 45 a more satisfactory procedure than the one presented here would generally be desirable estimating models for different threshold levels and then averaging over the models to incorporate uncertainty in threshold selection is a possible but comprehensive solution see e g 63 or 75 in the conditional simulation cf section 5 4 there is one assumption which should be mentioned in particular subsequent to having simulated a value of the conditioning variate y i it concerns how to determine which components of the remaining profile y i that should be observed and which should not we have assumed that it is appropriate to determine this by drawing an arbitrary z ˆ i and let the d 2 1 observed components of z ˆ i control which components of y i that are observed as commented in section 5 4 a simplification involved here is that the probability of observing a particular y j i does not depend on the magnitude of the simulated y i the number of observed y j i in the conditional simulation thus follows a binomial distribution it is therefore important that the conditional threshold is set sufficiently high in the dependence modelling for the data at hand this assumption did not affect the results in any critical way however we expect that the robustness of the method could be improved by also modelling this type of dependence this is probably easier to accommodate if the current direction is explicitly considered as a covariate in the statistical model acknowledgements we would like to thank three anonymous reviewers for thorough and thoughtful comments this work was supported by the research council of norway through the centres of excellence funding scheme project number 223254 ntnu amos and through the centres for research based innovation funding scheme project number 237790 exposed appendix a residual current marginal distribution parameters and return levels estimated residual current marginal distribution parameters and return levels are given in table a 2 
22693,knowledge about extreme ocean currents and their vertical structure is important when designing offshore structures we propose a method for statistical modelling of extreme vertical current velocity profiles accounting for factors such as directionality spatial and temporal dependence and non stationarity due to the tide we first pre process the data by resolving the observed vector currents at each of several water depths into orthogonal major and minor axis components by principal component analysis and use harmonic analysis to decompose the total observed current into the sum of deterministic tidal and stochastic residual currents a complete marginal model is then constructed for all residual current components and the dependence structure between the components is characterized using the conditional extremes model by heffernan and tawn 2004 by simulating under this model estimates of various extremal statistics can be acquired a simple approach for deriving design current velocity profiles is also proposed the method is tested using measured current profiles at two coastal locations in norway covering a period of 2 5 and 1 5 years it is demonstrated that the method provides good extrapolations at both locations and the estimated 10 year design current velocity profiles appear realistic compared to the most extreme velocity profiles observed in the measurements keywords current velocity profiles extreme current velocities multivariate extreme values conditional extremes model peaks over threshold structural design 1 introduction knowledge about ocean currents and their vertical structure is important as a design criterion for ocean and coastal structures for offshore structures located in shallow water waves are typically the most important load factor while in deeper water currents can actually dominate the load equation forristall and cooper 1997 this is also the case for many structures located in the coastal zone where coastal features such as islands and skerries can provide shelter from severe sea states whilst currents might retain or even increase their strength an example of the latter type of structure is aquaculture fish cages where the mooring line tension is generally dominated by current loads huang et al 2008 it is clear that simplification of the vertical current profile can introduce substantial errors in the calculated design load in such cases in a review paper on recent developments of ocean environmental description bitner gregersen et al 2014 improved accuracy of the statistical description of currents is called for particularly regarding change of the current profile with water depth this issue is addressed in the present paper unlike many other time signals in nature ocean currents include a deterministic signal due to the astronomical tide generally becoming much stronger and important near the shore and in shallow water pugh and woodworth 2014 applying standard tidal analysis techniques the tidal signal can be extracted and predicted with very high accuracy for any future time robinson and tawn 1997 currents have this in common with sea levels so the methods used for estimating the distribution of extreme currents and sea levels are therefore somewhat related two broad classes of methods exist 1 direct methods analysing extremes of the total observed current directly and 2 indirect methods exploiting the decomposition of the total current into deterministic tidal and stochastic residual currents modelling both separately before inferring the distribution of extreme total currents our focus will be on the second class sometimes referred to as the joint probabilities method jpm this method was originally introduced for estimation of extreme sea levels by 69 70 and later applied for estimating extreme currents by 67 extensions were given by 73 demonstrating substantial benefits over traditional direct methods for sea levels 22 found that the observed bias in direct methods was primarily caused by the non stationarity introduced by the tide extreme currents are far more difficult to estimate than extreme sea levels not only due to their directional and spatial variation but also because of the difficulty of obtaining sufficiently long series of observations pugh and woodworth 2014 even if a long time series is available a particular issue when dealing with extremes is that rare events are necessarily unusual so the quantity of directly relevant observations is limited this difficulty is compounded in the spatial setting such as for current profiles because forecasting then requires extrapolation into a high dimensional space with all its associated uncertainties it is thus important that the statistical models used should both be flexible and have a strong mathematical foundation so that such extrapolation has an adequate basis davison et al 2012 arguably the most useful and flexible current approach for modelling extremes in high dimensions is the conditional extremes model by 40 based on an assumption of the asymptotic form of the conditional distribution of a d dimensional variable given that it has an extreme component they present a semiparametric approach valid for extremes from a wide class of multivariate distributions applicable to problems of any dimension examples of application are spatial risk assessment of extreme river flows keef et al 2009 joint modelling of extreme significant wave height and spectral peak period jonathan et al 2010 2013 modelling of temporal dependence in river flows eastoe and tawn 2012 and modelling spatial extremal dependence of sea surface elevations at neighbouring locations eastoe et al 2013 the conditional extremes model was introduced for joint modelling of vertical current profiles by 45 and 72 applied it both for joint modelling of currents and waves and for modelling of current profiles owing to the positive experiences of the mentioned authors in using the heffernan and tawn model and its solid theoretical foundation this model will be applied here as well the additional constraints and slight change in model formulation recently proposed by 49 are also implemented to overcome a few complications that have been identified with using the heffernan and tawn model as commented by 45 any viable approach to modelling extreme vertical current velocity profiles must account for a the vector nature of the current at each depth and b the dependence between currents at different depths instead of using empirical orthogonal functions eof to pre process the data and then perform extreme value analysis on just a few energetic modes of the observed current profiles see e g 36 we choose to model orthogonal current components at each depth directly to avoid loss of information our approach is therefore closely related to that presented by 45 however we focus on modelling instantaneous mean velocity profiles rather than profiles consisting of hourly maxima and minima of the current components by considering orthogonal current components at each depth we bypass the necessity of explicitly introducing covariates as would be required for modelling current speeds and directions see 72 the proposed method accommodates the vector nature of the current by considering orthogonal current components at each depth and the dependence between the residual current components is characterized by the conditional extremes model furthermore observed temporal dependence leading to clustering of extremes is accounted for in both the marginal tail and dependence modelling by applying the peaks over threshold pot method and the non stationarity introduced by tidal currents is handled by exploiting the decomposition of the total current into tidal and residual currents the key steps of the proposed method are summarized in fig 1 we outline each of the steps herein and also propose a simple and pragmatic approach for deriving design current velocity profiles the method is believed to provide a valuable addition to existing methods for estimation of extreme current velocity profiles and we test it using adcp acoustic doppler current profiler data collected at two coastal locations in norway the paper is organized as follows in section 2 the considered locations together with the available data are described a brief general discussion on the accuracy of the measurements is also included the required pre processing of the data is described in section 3 this includes application of principal component analysis pca to resolve the current velocities into major and minor axis components at each depth and decomposition of the current velocity into tidal and residual currents by harmonic analysis in section 4 a general introduction of the conditional extremes model by 40 is given section 5 constitutes the main part of this paper here the statistical modelling of the residual current components is described in detail and applied to obtain extreme vertical residual and total current profiles at the two considered locations we present an approach for modelling the complete marginal distribution bulk and tail distribution of each residual current component describe the application of the conditional extremes model for characterizing the dependence structure between the components and outline the monte carlo procedure used to simulate extreme residual current and total current velocity profiles the simulated velocity profiles are compared with the measurements in section 6 a simple approach for deriving design current velocity profiles is proposed and the main conclusions and a discussion on assumptions and possible improvements are given in section 7 2 locations and data as indicated in fig 2 the measurements have been made at two coastal locations off the west coast of trndelag norway roughly 150 km east of the shelf break the munkskjæra site 63 8221 n 8 3836 e has a water depth of approximately 80 m and is located in vicinity of a number of small islands and skerries forming a strait in the east west direction fifteen kilometres to the northeast at salatskjæra 63 9200 n 8 5927 e the water depth is approximately 40 m this site is surrounded by a myriad of small islands underwater rocks and skerries resulting in a local bathymetry that is even more complex than at munkskjæra a simple statistical analysis of the current and wave conditions at both sites has previously been made at an earlier stage of the measurement programme kristiansen et al 2017 there are two major current systems in this coastal area the norwegian atlantic current primarily flowing along the shelf edge and the norwegian coastal current causing high current speeds near the coast sætre 2007 the larger fjord systems along the coastline are forced by freshwater runoff from land resulting in a surface outflow of brackish water that eventually adds to the norwegian coastal current broch et al 2017 from the island frya see fig 2 a chain of small islands stretches north eastward between this island chain and the mainland of norway is a deep ocean bay frohavet cutting inwards towards the entrance of trondheimsfjorden the local flow conditions at the considered sites munkskjæra and salatskjæra are dominated by water exchange between the norwegian sea to the west and frohavet primarily following the semidiurnal tidal cycle wind induced currents are of importance too and the rough topography creates a dynamic environment including tidal residual currents generated by interaction of tidal currents with coastal features and bottom topography and is also responsible for steering the current along its contours the measurements at both locations were made with a three beam acoustic doppler current profiler adcp of type nortek aquadopp 400 khz mounted on a moored oceanographic surface buoy seawatch midi 185 an adcp utilizes a physical principle called the doppler effect to measure the current speed and direction in multiple depth cells through the water column the doppler effect is exploited by emitting sound pulses from transducers beams which are reflected echoed by particulate matter moving with the water the signal is then shifted in frequency doppler shifted in proportion to the particle velocity see e g 62 for additional information note that the resulting measured velocity vector in a given depth cell by an adcp is not an instantaneous velocity at a fixed point but rather a spatial average with an inherent assumption that the flow is homogeneous in the horizontal plane over the distances separating the acoustic beams lu and lueck 1999 the current measurements were performed with a sampling rate of 1 hz over an ensemble interval of 10 min 600 samples with output once every hour this yields a time series of hourly 10 min average current speeds and directions the speed range was 0 300 cm s discretized by 256 points bin size of 1 2 cm s and the depth cell size was 3 m the data were post processed internally on the buoy before being sent to land every hour the measurement period at munkskjæra was almost 2 5 years from february 2016 to end of may 2018 and we consider the resulting hourly measurements of easterly and northerly velocity components at depths 4 m 10 m 16 m 22 m 28 m and 34 m below the surface at salatskjæra the buoy deployment lasted approximately 1 5 years from march 2016 to september 2017 and we consider the same depths as for munkskjæra apart from depth 34 m which was left out as the acoustic measurements here appeared to be occasionally affected by the proximity to the seabed the reason for focusing on the upper part of the water column is primarily a consequence of the measurement setup in addition the considered sites are located in an important area for fish farming mainly salmon farmed in open sea cages knowledge of the current velocity as a function of depth during extreme events in the upper part of the water column is important both for structural design of the cages and for the welfare of the fish an essential point when relying on measurements is their quality and validity the performance of acoustic doppler current profilers in laboratory flumes is generally found to be good for measuring mean current velocity profiles particularly in flow with low turbulence nystrom et al 2007 however in a recent 5 year measurement program in the north sea large discrepancies were observed between overlapping current speed data measured by different current profilers at the same locations and water depths suggesting that the accuracy of current profilers is not as good as the user expects see 7 also as our measurements are performed with adcps mounted on surface buoys an aspect likely to affect of their validity is the presence of surface waves an effect which is hoped to be averaged out over the ensemble interval there is only a limited literature investigating the effect of wave induced motions on buoy mounted adcps see 7 55 59 79 and 87 the overall conclusion is that the buoy motion does affect the measurements but the above references do not agree on the magnitude of the effect the latter is not really surprising considering that the buoy response depends on multiple factors such as buoy type mooring system wave conditions including stokes drift see e g 74 and 56 and ambient eulerian current this will not be discussed further however we emphasize that any statistical method assumes the input data to be valid the corresponding validity of the estimated extremes therefore depends critically on the quality of the measurements 3 pre processing of the data prior to the statistical modelling pre processing of the current velocity data is required although the techniques used are standard within the oceanographic community boon 2004 they may not be familiar to an ocean engineer working with structural design for completeness we therefore provide some level of detail in the present section specifically the mathematical techniques principal component analysis pca and harmonic analysis is introduced along with their application we mention that as presented here the order of application of the two techniques is interchangeable 3 1 principal component analysis resolving the current velocity into major and minor axis components principal component analysis pca sometimes referred to as empirical orthogonal functions eof is a statistical approach where the usual objective is to condensate the information contained in a large number of interrelated original variables into a smaller set of linearly uncorrelated variates with a minimal loss of information in terms of variance see 44 this technique has been used for decades by oceanographers and meteorologists to analyse complex time series forristall and cooper 1997 in our case the motivation for applying pca is primarily to obtain uncorrelated current components at each depth following 45 not to reduce the dimensionality of the problem unless the current to be analysed is rectilinear note that uncorrelated variables are not necessarily statistically independent consider a time series consisting of n measured horizontal current velocities at a given depth expressed as orthogonal vector components u e and u n u e k and u n k k 1 n being the velocity observed at time t k in the eastward and northward direction respectively we now want to apply pca to convert this set of generally correlated variables into a set of values of linearly uncorrelated variables the principal components this is achieved by using an orthogonal transformation defined in such a way that the first principal component has the greatest fraction possible of the total variance and consequently in the two dimensional case the second principal component has the least the practical procedure for obtaining the principal components is as follows our time series of horizontal current velocities is expressed as a n 2 data matrix u u e u n where u e and u n are the eastward and northward velocity vectors with their mean value subtracted the sample covariance matrix s is then given as 1 s 1 n 1 u t u where the diagonal elements of s is the variance of u e and u n respectively to find the principal components we must calculate the eigenvalues and corresponding eigenvectors of s the resulting 2 2 unit eigenvector matrix v is then sorted so that the first column of v is the eigenvector corresponding to the largest eigenvalue the first principal component axis then simply refers to the first column of v and the second principal axis to the second column the values or scores of the original variables onto the principal axes are then found as 2 u m u m u e u n v where u m is termed the major axis component and u m is termed the minor axis component corresponding to the first and second principal component respectively in practice v in eq 2 is nothing more than a rotation matrix this is seen in fig 3 and 4 showing the resulting major and minor axis of the surface current velocity observations at munkskjæra and at salatskjæra the dominant current direction is seen to be closely aligned with the east west axis at munkskjæra and in the southeast northwest direction at salatskjæra the direction of the highest observed velocities at salatskjæra is however not aligned with the major axis the choice of positive directions is up to the analyst to decide our choice is indicated in the figures the procedure above has been followed to resolve the current velocity into uncorrelated major and minor axis components independently at each depth at the two considered sites at the munkskjæra site the major axis component account for 90 95 of the total current velocity variance increasing with depth as shown in fig 5 a in fig 5 b it is seen that the relative major axis variance is less prominent at salatskjæra where it accounts for 75 85 of the total variance the direction of the major axis is determined by the local bathymetry and topography at both locations a slight anti clockwise and clockwise rotation for increasing depths is seen at munkskjæra and salatskjæra respectively here the direction of the major and minor axes has been decided based on the total current velocity at each depth following 45 other rational choices exists however for instance deciding their direction based on the residual current velocity at each depth and or only considering velocities whose magnitude exceeds some threshold the best choice is dependent on the data at hand 3 2 harmonic analysis decomposing the total current into tidal and residual currents the measured total current velocity is the vector sum of an essentially deterministic tidal current and a stochastic random residual current it is only meaningful to perform extreme value analysis on stochastic variables so it would be desirable to decompose the observed current into a tidal and a residual component at each depth the standard method for extracting the tidal signal from a time series is called harmonic analysis and will be briefly described in the following unlike many other time signals in nature tides and tidal currents are forced oscillations that occur only at known tidal frequencies boon 2004 the driving forces originate from the gravitational fields of the sun and moon acting on a rotating earth expressed mathematically as the tidal potential doodson 1921 this astronomical forcing can be written as a linear combination of sinusoidal terms each having a distinct amplitude phase and temporal frequency foreman and henry 1989 the oceanic response can be described in the same manner each sinusoid being referred to as a tidal constituent due to the hydrodynamic effects caused by irregular coastal boundaries and the bathymetry of the oceans the amplitudes and phases of the constituents can vary greatly but their frequencies remain the same as those in the tidal potential foreman and henry 1989 the tidal frequencies are all linear combinations of the rates of change of the mean lunar time the earth rotation with respect to the moon and five astronomical variables that uniquely specify the position of the sun and moon see 33 unlike spectral analysis harmonic analysis takes advantage of the fact that the tidal frequencies are known in advance once a suitable set of m tidal constituents has been chosen the amplitude and phase of each constituent are calculated by solving a system of linear equations in the one dimensional case this equation system takes the form 3 h t k a 0 j 1 m a j cos σ j t k ϕ j where a 0 is the mean a j ϕ j and σ j are the amplitude phase and frequency of constituent j and h t k k 1 n is the observation at time t k eq 3 is generally overdetermined there exists more equations than unknowns and is therefore solved by a least squares technique minimizing the equation residuals the utide matlab functions by 12 have been applied for the harmonic analysis of the current velocity at each depth the functions take orthogonal current components along the first and second axes in any right handed coordinate system as input conventionally the eastward and northward components the time series are permitted to be irregularly sampled and or contain gaps utide then uses a refined two dimensional complex version of eq 3 to indirectly solve for the so called current ellipse parameters the tip of the velocity vector of a constituent traces out an ellipse over its tidal period so the goal is to find the lengths of its semi major and semi minor axes its angle of inclination and greenwich phase see e g 32 for illustrative figures the equation system can be solved either by the ordinary least squares ols method or by an iteratively reweighted least squares irls method the latter limiting the sensitivity to outliers and reducing confidence intervals compared to the ols method codiga 2011 diagnostics to assess constituent independence includes among others the conventional rayleigh criterion a time series of length t is required to distinguish between two constituents with frequency separation of t 1 and its noise modified version due to 61 accounting for the amount of non tidal energy noise in the record the so called nodal satellite and astronomical argument corrections are evaluated at the exact times of each measurement removing the restriction that the analysis periods should not be much longer than one year foreman et al 2009 the nodal satellite corrections accounts for the fact that the amplitudes and phases of the constituents are generally not constant due to interaction with minor unresolved constituents called satellites while the astronomical argument simply re expresses phase lags with respect to an absolute time and space origin foreman and henry 1989 there are a maximum of 146 possible tidal constituents that can be included in utide of these 45 are astronomical in origin while the remaining 101 are shallow water tides the latter constituents arise due to distortion of the tidal wave by shallow water effects and have frequencies that are multiples sums and differences of the frequencies of the astronomical constituents see e g 71 for further details the automated decision tree constituent selection method default option in utide due to 37 and formalized by 31 was applied to decide which constituents to include in the analysis the method selected a total of 68 constituents for inclusion in the harmonic analysis at both munkskjæra and salatskjæra the resulting current ellipse parameters of the five most energetic tidal constituents are given in table 1 for both sites the tidal current at munkskjæra and salatskjæra is semidiurnal dominated by the m 2 principal lunar semidiurnal and s 2 principal solar semidiurnal constituents the seasonal low frequency constituent sa solar annual is also seen to be important particularly at munkskjæra it should be noted that this low frequency constituent is largely influenced by non tidal forcing boon 2004 once the current ellipse parameters have been obtained utide can be used to reconstruct hindcast the tidal current over the period of observations in the reconstruction we conservatively neglect non significant tidal constituents having a signal to noise ratio snr below 2 with respect to the raw signal at that frequency default option in utide see 12 the residual current in the major and minor axis direction at depth i are then given as 4 u r m i t k u m i t k u t m i t k u r m i t k u m i t k u t m i t k where subscript r refers to the residual current and t to the tidal current the latter includes the mean current velocity if the eastward and northward velocity components have been used as input to the harmonic analysis the velocities are easily transformed to components along the major and minor axis at each depth using eq 2 the decomposition of the total current into tidal and residual current is shown in fig 6 and 7 for the major axis surface current at munkskjæra and salatskjæra respectively though only a 14 day period is plotted it is clear that the tidal current is important at both locations this is confirmed in fig 5 displaying the total tidal and residual current velocity variance with depth at the considered locations the tidal current accounts for 57 71 of the total variance at munkskjæra and 59 68 of the total variance at salatskjæra accordingly the variance of the current we are to perform extreme value analysis on the residual current is reduced by the same percentages at both locations the relative importance of the residual current increases towards the sea surface it is somewhat surprising to note that the total current variance at the top bin 4 m is slightly lower than that at 10 m at both locations when comparing buoy mounted and bottom mounted adcps 59 found that the near surface measurements of the buoy mounted adcps were biased low though we have no means of verifying it this could be a possible explanation for the observed near surface velocity reduction in our measurements as well in the remaining sections the tidal current is assumed to be deterministic and known at any time due to the preceding harmonic analysis it is worth noting that an underlying assumption in tidal harmonic analysis is that the tide is stationary there are cases where this assumption can be invalid for instance as a result of nonlinear interaction between the tide and storm surges in shallow water for internal tidal currents that change with the stratification or seasonally varying ice cover that can modify both tidal elevation and current harmonics foreman et al 2009 in such cases the use of wavelet analysis allowing for tidal non stationarity can provide a better alternative for extracting the tidal signal see e g 30 4 the conditional extremes model in this section a general description of the conditional extremes model by 40 is given for additional theoretical details the reader should consult the original paper heffernan and tawn 2004 or the article by 39 the latter providing a formal mathematical framework its application for joint modelling of extreme residual currents including practical details and required modifications in addition to the marginal modelling is presented in section 5 consider a continuous d dimensional random vector variable x x 1 x d with unknown distribution function f x x being for instance simultaneously observed values of an environmental parameter at different locations from a sample of n independent and identically distributed observations from f the conditional extremes model by heffernan and tawn concerns the estimation of functionals of the distribution of x when x is extreme in at least one component specifically it describes the conditional distribution of x i x i v x i where x i denotes the vector variable x excluding component x i and v x i is a high threshold here and throughout vector algebra is to be interpreted componentwise 4 1 marginal transformation having established the marginal distribution of each x i i 1 d by univariate extreme value theory see section 5 2 the method starts by componentwise transforming all variables to follow a common distribution this is known as marginal standardization and is performed in order to separate the marginal behaviour from the dependence structure between the components drees and janßen 2017 heffernan and tawn chose the standard gumbel distribution for this purpose we will however follow 49 transforming the marginals to standard laplace distributions the motivation for choosing the laplace distribution is that the semiparametric regression model in 40 used to characterize the behaviour of y i occurring with large y i takes different functional forms for positively and negatively associated variables as the laplace distribution has both exponential tails and symmetry this captures the exponential upper tail of the gumbel distribution required for modelling positive dependence while the symmetry allows the same functional form to be used for modelling the dependence of negatively associated variables keef et al 2013 we will later see that this is particularly convenient for modelling current profiles as it generally includes modelling of both positively and negatively dependent variables using the probability integral transform our original vector variable x with marginal cumulative distributions f x i x i is thus transformed componentwise as 5 y i log 2 f x i x i for x i f x i 1 0 5 log 2 1 f x i x i for x i f x i 1 0 5 where f x i 1 q is the inverse cumulative distribution function quantile function of x i evaluated at the cumulative probability q the new vector variable y y 1 y d then has standard laplace distributed marginals with 6 pr y i y f y i y 1 2 exp y if y 0 1 1 2 exp y if y 0 meaning that both the upper and lower tails of y i are exactly exponentially distributed for clarity x and y are used throughout the paper to denote the variable with its original marginal distributions and with laplace marginals respectively following 40 the focus will now be placed on extremal dependence modelling of variables with laplace marginal distributions 4 2 dependence model the dependence model considers the asymptotic structure of the conditional distribution pr y i y i y i y i arising from a d dimensional random variable y y 1 y d with laplace marginal distributions y i denotes the vector variable y excluding component y i to examine the limiting conditional distributions as y i the growth of y i must be controlled according to its dependence on y i so that the limiting distribution has non degenerate marginals this is achieved by assuming that for a given i there exist vector normalizing functions a i y i and b i y i of the same dimension as y i such that for all fixed z i 7 lim y i pr y i a i y i b i y i z i y i y i g i z i where the limit distribution g i has non degenerate marginal distributions g j i for all j i eq 7 is assumed to hold exactly for all values of y i u y i for a suitably high threshold u y i as a consequence the random variable z i defined by 8 z i y i a i y i b i y i is independent of y i for y i u y i and has distribution function g i the extremal dependence behaviour is then characterized by location and scale functions a i y i and b i y i and the distribution function g i due to the transformation to laplace marginals the form of the normalizing functions a i y i and b i y i falls into a simple parametric family both for positively and negatively associated variables given by 9 a i y i α i y i b i y i y i β i where the vector constants α i and β i have components α j i 1 1 and β j i 1 for all j i no such simple class of parametric models exists for g i as no specific structure is imposed by the limiting operation 7 g i is therefore modelled nonparametrically the resulting dependence model is a multivariate semiparametric regression model of the form 10 y i α i y i y i β i z i for y i y i u y i where i 1 d for a large value of y i the behaviour of the remaining components in y is thus described by eq 10 the constant α j i describes the strength of dependence between y j on large values of y i while β j i describes how the variability of y j changes with increasing y i positive and negative values of α j i corresponds respectively to positive and negative association between the variables y i y j positive β j i means that the variance of y j increases as y i increases whereas negative β j i means that the variance decrease for α j i 1 and β j i 0 y i y j are said to be asymptotically positive dependent the quantiles of the distribution of y j y i y i grows at the same rate as y i for y i and for α j i 1 and β j i 0 they are asymptotically negative dependent otherwise they are asymptotically independent see e g 49 4 3 inference as stepwise estimation is generally simpler than joint estimation inference for marginal and dependence structure is undertaken stepwise in 40 first the parameters of the marginal distributions of the components of x are estimated after transformation to laplace marginals the dependence parameters are estimated assuming that the marginal parameters are known since the conditional extremes model by heffernan and tawn offers nothing new regarding marginal inference we focus here on the estimation of the conditional model parameters inference for the parametric part of the conditional model consists of estimating the values of the vector constants α i and β i based on the sample data during inference a parametric model for g i must be assumed specifically the components of z i are falsely assumed to be mutually independent and gaussian distributed the gaussian distribution was selected for its simplicity and superior performance heffernan and tawn 2004 if z i has marginal means and standard deviations denoted by the vectors μ i and s i then following eq 10 the means and standard deviations of the random variables y i y i y i for y i u y i are α i y i μ i y i β i and s i y i β i respectively from the k 1 n u y i observations of y y i u y i the maximum likelihood estimates of the unknown parameters α i β i μ i and s i are then found from the following objective function log likelihood 11 q i α i β i μ i s i j i k 1 n u y i log s j i y i i k β j i 1 2 y j i k α j i y i i k μ j i y i i k β j i s j i y i i k β j i 2 numerical maximization of q i over the parameter space of the model is required to obtain the point estimates α ˆ i β ˆ i μ ˆ i s ˆ i with μ i and s i treated as nuisance parameters the distribution g i is finally estimated nonparametrically by using the empirical distribution function g ˆ i of replicates of the random variable z ˆ i defined by 12 z ˆ i y i α ˆ i y i y i β ˆ i for y i y i u y i the resulting observations z ˆ i provide a sample from the multivariate distribution g i a problem identified by 49 is that due to the omission by heffernan and tawn of imposing joint constraints on the parameters of the semiparametric regression model α j i and β j i and the nonparametric element of the model inconsistencies with the marginal distributions can arise the strongest form of extremal dependence between two variables is asymptotic dependence coles et al 1999 given by α j i β j i 1 0 in the dependence model 10 this suggests that when α j i 1 β j i cannot be positive however as the parameter space is α j i β j i 1 1 1 such a combination of parameters is allowed in the original model this together with the nonparametric element of the model results in the possibility of the estimated joint probabilities to exceed the marginal probabilities to avoid this it is recommended to impose the constraints given in 49 on α j i and β j i if strong extremal dependence is expected between the variables for pairs y i y j these constraints follows from requiring a stochastic ordering assuring that conditional quantiles for any form of asymptotic independence are not larger than under asymptotic positive dependence nor smaller than under asymptotic negative dependence these constraints are imposed only on extrapolations i e for y i v where v is a value above the maximum observed value of y i the reader is referred to 49 for further details 4 4 conditional simulation since the dependence model 10 is semiparametric estimates for various extremal statistics must be acquired by simulation we thus generate random samples of x x i v x i where v x i f x i 1 f y i u y i for each i using the estimated conditional models from these samples monte carlo approximations of functionals of the joint tails of the distribution of x can then be obtained the sampling algorithm for each i is as follows 1 simulate y i from a laplace distribution conditional on it exceeding its cumulative probability corresponding to f x i v x i 2 sample z i from g ˆ i independent of y i 3 obtain y i α ˆ i y i y i β ˆ i z i 4 transform y y i y i to x using the inverse of transformation 5 let us say that we from the data sample at hand have n independent observations of x where x is extreme in at least one component a simulated random realization of this process covering the same period is then obtained by simulating n pseudo observation of x by the sampling algorithm above typically many of the observed x k k 1 n comprises observations where more than one of the components x i k are simultaneously extreme this raises the question of how to determine the number of times one should condition on each x i to obtain x i for all i 1 d resulting in a total of n simulated pseudo observations of x in the proceeding simulation of residual current profiles we principally follow the procedure proposed by 45 to estimate this the main argument in 45 is that since the conditional extremes model is motivated asymptotically it is most appropriately applied to the conditioning variable whose value is most extreme in its marginal distribution transformed to y this means that for the observations y k k 1 n the number of times to condition on variable y i during the conditional simulation is determined by the number of times which y i was the largest component of y k k 1 n the rate at which to condition on each y i or x i is thus found as 1 n k 1 n 1 y i k max y k where 1 a denotes the indicator function of some event a in section 5 4 a slight modification of this procedure is described the modification being introduced to account for temporal dependence 5 application in this section the statistical modelling of the residual current components is described in detail and applied to obtain extreme vertical residual and total current profiles at the two considered locations munkskjæra and salatskjæra this includes modelling the complete marginal distribution bulk and tail distribution of each residual current component characterizing the dependence structure between the components by application of the conditional extremes model heffernan and tawn 2004 and outlining the monte carlo procedure used to simulate extreme current velocity profiles following 45 67 73 the tidal and residual currents are assumed to be independent inspection of plots of observed residual currents against observed tidal currents from the measurements as in 68 confirmed that the independence assumption generally appears reasonable just as assuming tide surge independence when estimating extreme sea levels this can however be a slightly conservative assumption in some cases particularly in shallow water see e g 41 66 and 71 subsequent to the statistical modelling extreme total current velocity profiles can then be obtained by randomly adding predicted tidal current profiles to realizations of extreme residual current profiles we start by defining the considered residual current velocity components 5 1 residual current components the directionality of the extreme currents needs to be accounted for in the analysis this requires characterization of the extremal behaviour of both positive and negative principal current components the observed residual current velocities are therefore split into four velocity components at each depth specifically for each depth i we consider the positive major axis component u r m i the negative major axis component u r m i the positive minor axis component u r m i and the negative minor axis component u r m i see fig 8 since positive and negative velocity components along a given axis at a given depth are mutually exclusive events one can only observe one of the major axis and one of the minor axis components simultaneously at each depth in order to work with only positive variable values the residual current velocity components for the k 1 n observations are defined as 13 u r m i k u r m i k u r m i k 0 u r m i k u r m i k u r m i k 0 u r m i k u r m i k u r m i k 0 u r m i k u r m i k u r m i k 0 a velocity component is said to be unobserved if the condition in eq 13 is not fulfilled denoting the total number of residual current components d an observed velocity profile thus contains d 2 observed velocity components and d 2 unobserved components the total number of residual current components is equal to 24 at munkskjæra and 20 at salatskjæra as the mean current velocity is included in the tidal current the sample size of each residual current velocity component u r m i and u r m i is expected to be approximately equal to n 2 5 2 marginal modelling marginal modelling is performed independently for each of the four residual current components u r m and u r m at each depth from now we denote the full set of residual current components as x i i 1 d u r m 1 u r m 1 u r m 1 u r m 1 u r m 2 and refer to u r m and u r m only when needed since the aim is to describe all values of x i that can occur with any large x i a model for the complete marginal distribution f x i of each x i is required for this purpose we essentially follow 40 adopting the semiparametric model by 16 17 which comprises the generalized pareto distribution for x i above a high threshold u x i and the empirical distribution function below the threshold such models are sometimes referred to as mixture models the empirical distribution function describing the bulk of observations is established based on all hourly observations below the threshold while inference for the tail distribution is made by application of the peaks over threshold pot method to account for marginal temporal dependence at extreme levels as the latter distribution refers to events cluster maxima rather than individual sequential observations an approach is described to properly connect the all observation based bulk distribution and the event based tail distribution similar approaches have been used by for instance 60 in connection with estimation of extreme sea levels the marginal model of each residual current velocity component thus consists of 1 a bulk distribution describing observations below a high threshold by the empirical distribution function 2 a tail distribution identifying cluster maxima above the threshold by the peaks over threshold method and fitting these maxima to the generalized pareto distribution 3 connecting the bulk and the tail distribution to obtain the complete marginal distribution giving particular emphasis to the fitting of the tail distribution the points above will be outlined in the current subsection 5 2 1 bulk distribution marginally points below the threshold u x i are relatively dense and are therefore well described by the empirical distribution function f x i i e 14 f x i x num elem x i k k 1 n x i x n x i for x u x i where n x i is the number of observations of variable x i the denominator in the expression for f x i is sometimes written n x i 1 the difference being negligible here the threshold u x i for each residual current velocity component is decided as part of the fitting procedure for the tail distribution 5 2 2 tail distribution from univariate extreme value theory it can be shown that the generalized pareto gp distribution arises as the limiting distribution for excesses over thresholds davison and smith 1990 a result originally due to 65 and 1 1 1 many practitioners within engineering disciplines use the weibull distribution as an alternative based on empirical goodness of fit to data this is a credible candidate model for our data as well the justification for the generalized pareto distribution as the limiting distribution for excesses over thresholds and subsequent parameter and return level estimates are based on an assumption that the exceedances are independent this is not a valid assumption for our hourly measured residual current velocities temporal dependence is clearly observed particularly for the major axis velocity components resulting in a tendency of extremes to cluster however if we introduce a condition that limits the dependence structure of the sequence it can be shown that the maxima of dependent stationary series follows the same distributional limit laws as those for independent series leadbetter et al 1983 coles 2001 the peaks over threshold pot method with declustering is applied to limit marginal temporal dependence at extreme levels specifically for exceedances above the threshold u x i only the largest excess within a cluster of exceedances is considered the cluster peak excess or cluster maxima the most common definition of clusters is as runs of consecutive exceedances with an additional temporal separation criterion caires and sterl 2005 we define the cluster peak excesses simply as the highest peaks above the threshold with a minimum peak to peak separation of τ hours a peak being defined as the highest observation of consecutive exceedances this cluster peak excess definition is related to the runs method described in e g 81 though we feel it is more convenient to impose the temporal separation criterion directly on the peaks rather than on the required below threshold time it should be mentioned that such definitions are often asymptotically equivalent leadbetter 1991 the marginal tail of x i for i 1 d describing the distribution of the independent cluster peak excesses x c i conditional on x c i u x i is thus modelled by 15 g x c i x 1 1 ξ i x u x i σ i 1 ξ i ξ i 0 1 exp x u x i σ i ξ i 0 where u x i is a high threshold for x i and ξ i and σ i are shape and scale parameters respectively with σ i 0 and the operator s max s 0 if ξ i 0 the distribution of excesses has an upper bound of u x i σ i ξ i while for ξ i 0 the distribution as no upper limit the case ξ i 0 is interpreted as the limit ξ i 0 resulting in the exponential distribution with mean excess σ i before the parameters ξ i σ i of the generalized pareto distribution can be estimated a suitable threshold u x i and minimum peak to peak separation time τ x i have to be decided for each x i the minimum peak to peak separation time is usually set based on physical considerations often related to typical storm durations for parameters causing environmental loads while the threshold can be more difficult to decide the issue of threshold selection amounts to a trade off between bias and variance too low a threshold is likely to violate the asymptotic basis of the model leading to bias while too high a threshold leads to fewer excesses with which the model can be estimated leading to high variance coles 2001 we use the graphical diagnostics outlined by 13 and 78 in addition to the anderson darling statistic for deciding the threshold the so called mean residual life plot and the parameter stability plots are based on the fact that if the generalized pareto distribution is valid for cluster peak excesses of the threshold u 0 it should also be valid for all thresholds u u 0 these plots are obtained by calculating the mean of the cluster peak excesses e x u x u and estimating the gp parameters over a range of different thresholds above a threshold for which the generalized pareto distribution is valid the mean residual life plot and the scale parameter σ should be approximately linear in u while the shape parameter ξ should be approximately constant the scale parameter can alternatively be reparametrized as σ σ ξ u so that σ should also be constant with u the lowest threshold for which the above holds true taking sample uncertainty into account is then selected we found subjectively the mean residual life plot and the shape parameter stability plot to be the most informal see fig 9 for an illustration due to the large number of considered residual current components a suitable common threshold corresponding to a given non exceedance probability as measured by f x i x was selected one threshold for all the major axis components and one for the minor axis components the gp parameters were estimated by maximum likelihood see e g 38 though several alternative methods exists 20 21 and 58 are useful references in this respect return level plots were used to validate the threshold choices and more generally the validity of the gp distribution such plots where the estimated n year return level is plotted against n the latter on logarithmic scale are particularly convenient since the effect of extrapolation is highlighted for a given threshold and corresponding estimated gp parameters the n year return level for variable x i x i n is given by 16 x i n u x i σ i ξ i n c i 1 y n ξ i 1 denoting the total number of observations by n the theoretical possible number of here hourly observations during a year by n 1 y 8766 and the total number of observed cluster peak excesses of variable x i by n c i it follows that the estimated average number of cluster peak excesses per year for variable x i n c i 1 y in eq 16 is given by n c i 1 y n c i n 1 y n where n n 1 y is the effective number of years of measurements and accounts for possible missing observations gaps in the measured time series assuming they are missing at random as the graphical diagnostics discussed above can be rather subjective and sometimes difficult to interpret the anderson darling statistic was used as an additional objective goodness of fit test this statistic is a modification of the cramér von mises static giving more weight to the tail of the distribution making it particularly powerful for detecting tail discrepancies we found it very convenient to use this statistic as a quick way of assessing the overall performance of the fitted marginal models looking at the number of model rejections at the 5 significance level for a given choice of thresholds for all x i plotting the calculated value of the anderson darling statistic and the corresponding critical test value over a range of thresholds for each x i was also helpful reference is made to 11 for additional details regarding the anderson darling statistic and critical test values for the generalized pareto distribution an initial minimum peak to peak separation time τ of 30 h was selected for all variables before the diagnostics above were utilized to identify a suitable cumulative threshold for the major and minor axis components the initial separation time was decided based on time series inspection and knowledge of typical storm durations in the area in addition it seemed rational to adopt a separation time larger than the period of the semidiurnal and diurnal tidal constituents so that dependence introduced due to possible tidal residual current interaction and lack of fit of the harmonic model is limited if inertial oscillations were important the inertial period at the given latitude should probably be considered as well see generally 89 chap 5 using the initial τ a threshold corresponding to the 95 quantile for the major axis components u r m i and the 94 quantile for the minor axis components u r m i were found to give reasonably stable results at both locations this is similar to that found by 73 and 45 both used the 95 quantile though strictly the numbers are not directly comparable due to differences in the involved variables a simple assessment of the threshold sensitivity on the estimated marginal extremes were made by plotting the estimated 50 year return level for some of the velocity components for a range of thresholds these estimates were generally surprisingly stable for a broad range of reasonable threshold choices this is reassuring considering the subjectivity involved in the threshold selection process having decided the threshold the final peak to peak separation time was determined by considering different values of τ ranging from 12 48 h looking at the effect changes in τ had on the mentioned diagnostics the return level plots were particularly informal in this context as with the threshold choice the issue is a trade off between bias and variance a minimum peak to peak separation time τ 30 hours gave the most stable results and was thus selected however the results were not particularly sensitive to the precise value of τ this is in line with that reported in several other studies davison and smith 1990 such as by 82 in the case of sea levels with the final choice of thresholds and minimum peak to peak separation time the fitted gp models passed the anderson darling test at the 5 significance level for all x i at both locations a representative illustration of the mean residual life and shape parameter diagnostics are shown in fig 9 for the u r m velocity component at 22 m depth at munkskjæra above the chosen threshold indicated by the red asterisks the mean residual life plot is seen to be approximately linear in u while the shape parameter is approximately constant taking sampling variability into account indicated by the confidence intervals the maximum likelihood estimates of the gp parameters are given in fig 10 a and 10 b for all velocity components at munkskjæra and salatskjæra respectively the shape parameter is in most cases negative indicating the existence of an upper limit for these residual current velocity components the goodness of fit of the gp distribution and the effect of extrapolation is illustrated in the return level plot in fig 11 again for u r m at 22 m depth at munkskjæra 95 confidence intervals based on profile likelihood are added see e g 13 such intervals are usually more accurate than wald type intervals important points to note are the asymmetric confidence intervals reflecting the greater uncertainty about high values and the large uncertainties that arises once the model is extrapolated to values well beyond the range of observations with only 1 5 2 5 years of observations large uncertainty will thus inevitably be associated with estimates of return levels corresponding to return periods ranging from 10 100 years typically required for design the estimated 10 and 50 year return values for all residual current components including confidence intervals are found in table a 2 in the appendix together with all relevant marginal distribution parameters it is interesting to note the particularly wide confidence intervals seen for most of the velocity components with a high value of the gp shape parameter though considering the physics of our problem the most extreme intervals appears unrealistic in such cases better estimates could probably be obtained by penalized maximum likelihood see 14 5 2 3 connecting the bulk and tail distribution the bulk distribution describes all hourly observations below the thresholds for each x i i 1 d while the tail distribution describes only the independent cluster peak excesses above the threshold for consistency below and above the threshold it would therefore be favourable to transform the tail distribution into a distribution describing arbitrary hourly exceedances rather than just the cluster maxima this is further motivated by our aim of characterizing instantaneous current profiles during an extreme event simultaneously observed threshold exceeding current components of the profile might not necessarily all correspond to marginal cluster peak excesses the goal is to transform the distribution of the independent cluster peak excesses into the distribution of dependent hourly sequential residual current velocities above the thresholds u x i i 1 d to transform the distribution of the independent cluster peak excesses into the distribution of all hourly excesses it is necessary to introduce the so called extremal index θ which lies in the interval 0 1 see e g 3 10 the extremal index has a physical and intuitively appealing interpretation due to 52 as the inverse of the limiting mean cluster size if θ 1 exceedances of an increasing threshold occur singly in the limit while if θ 1 exceedances tend to cluster in the limit the finite sample approximation of θ due to 85 θ x is an index varying with the level x for large x the physical interpretation of θ holds for θ x and its inverse can thus be calculated as 17 θ ˆ x 1 x number of exceedances of x number of cluster maxima exceeding x the above equation can however not be used to directly calculate θ x 1 x for x beyond the range of the data and the uncertainty of the empirical estimates of θ ˆ x 1 x increases for increasing x also inevitably θ ˆ x 1 x 1 as x approaches the highest observation of the sample denoting the extremal index obtained at the marginal threshold u as θ u 83 notes that the distribution function of an arbitrary exceedance is influenced by the factor θ u θ x 1 following a similar notation as in 27 conditional on x u we then have the following subasymptotic relation between quantiles of the distribution function of an arbitrary exceedance g x x and that of an independent cluster peak excess g x c x 18 1 g x x θ u θ x 1 x 1 g x c x given x u θ u can be interpreted as a scale factor to account for the increased sample size of all excesses relative to the cluster peak excesses while θ x 1 x accounts for the increased probability of observing an arbitrary exceedance above x due to dependence relative to if all exceedances above the level were independent cluster peak excesses the complete marginal distribution f x i of each x i can thus be taken as 19 f x i x 1 p u x i θ u x i θ x i 1 x 1 g x c i x for x u x i f x i x for x u x i where g x c i x is the generalized pareto distribution given by eq 15 f x i x is the empirical distribution function and p u x i 1 f x i u x i is the probability of exceeding the threshold u x i clearly an estimate of θ x 1 x is required beyond the range of the data a rational approach would then be to estimate θ x 1 by eq 17 at regularly spaced levels fit a function to these estimates and extrapolate as commented in 85 the choice of an appropriate function is not immediately obvious but results from extreme value theory leadbetter 1983 suggest that in the limit a constant is the correct function denoting the asymptotic limiting extremal index θ l i m and the associated level x l i m for which θ x x θ l i m for all x x l i m we have tested three different approaches for estimating θ l i m and x l i m in addition to θ x x on the intermediate range u x x l i m 1 simply take θ x x θ u for all x u implying θ l i m θ u from eq 18 it can be seen that this is equivalent to assuming that the distribution function of an arbitrary exceedance equals that of an independent cluster maxima 2 using an approach inspired by 60 taking θ l i m 1 and having θ x 1 x varying linearly from θ u 1 at x u to 1 at x x l i m where x l i m is the lowest level x for which eq 17 gives θ ˆ x 1 1 3 following 85 estimating θ x 1 by eq 17 at regularly spaced levels for u x x m a x where x m a x is the highest observation and using simple weighted least squares to estimate θ l i m 1 the same approach can be used to estimate x l i m we adopted the estimators 20 θ ˆ l i m 1 k j θ ˆ x 1 x j k j 21 x ˆ l i m k j x j k j where k j is the number of cluster maxima above the level x j on u x x ˆ l i m θ x 1 x is taken to vary linearly between θ u 1 at x u and θ ˆ l i m 1 at x x ˆ l i m the resulting estimates of θ x 1 x are displayed in fig 12 a for residual current component u r m at 4 m depth at munkskjæra highlighting the differences between the three considered approaches in fig 12 b using eq 19 the resulting survival function f x 1 f x is plotted against the empirical all observation estimates in line with results in 27 approach 1 fits the empirical estimates poorly overpredicting the probability of exceeding a particular level x overall approach 2 mazas et al and approach 3 tawn and vassie performed more or less equally well however as approach 3 has a better theoretical underpinning when extrapolating beyond the range of observations this is our preferred approach the complete marginal distribution function described by eq 19 for each residual current component x i is thus calculated with θ x i 1 x as determined by approach 3 as an anecdote to the above discussion it is worth mentioning that we performed the dependence modelling and the conditional simulations described in the following subsections using all the three approaches described above for estimation of θ x 1 x somewhat surprisingly at both considered locations only small differences were observed in the resulting predicted extreme current velocity profiles this seems to suggest that provided the same approach is used to estimate θ x i 1 x for all x i accurate estimation of the extremal index is not the most critical step in the current statistical model 5 3 dependence modelling we now turn to the modelling of the extremal dependence between the residual current velocity components most of the details have already been described in section 4 however some modifications to the original model heffernan and tawn 2004 are needed to allow modelling of instantaneously observed mean current velocity profiles the conditional extremes model by heffernan and tawn concerns the estimation of functionals of the conditional distribution of x i x i v x i as originally formulated each observation of x is assumed to be independent and with no components of x missing both these assumptions are violated for our data calling for an extension of the original model solutions to similar problems were given by 50 however due to differences in the involved variables the proposed methods are unfortunately not applicable to our data more specifically the issues which needs to be addressed are 1 the hourly recorded current velocity profiles displays temporal dependence requiring a condition to be introduced to limit the effect of dependence owing to the multidimensionality of the problem it is however difficult to introduce a very strict condition to reduce dependence as the observed individual residual current components do not necessarily peak at the same time during an event with strong consecutive currents it is as opposed to for univariate peaks over threshold modelling difficult to decide which of the observed current profiles that should be considered as the peak observation 2 the d dimensional vector of simultaneously observations x includes d 2 observed and d 2 unobserved residual current components this is not an issue of missing values due to measurement malfunction but simply a consequence of how the current components are defined cf section 5 1 the dependence model is applied to the data after transformation to laplace margins the original vector variable x with marginal distributions defined by eq 19 is thus transformed componentwise by transformation 5 to a vector variable y with laplace distributed marginals point 1 above is taken care of marginally by application of the peaks over threshold method in the same way as when fitting the marginal tail distributions in section 5 2 2 this means that the semiparametric regression model 10 which for large y i describes the behaviour of the remaining components in y is only fitted to observations of y y i u y i where y i corresponds to a cluster peak excess for each conditioning variate y i i 1 d of independent cluster peak excesses the resulting observations y y i can then also be considered independent the minimum peak to peak separation time τ y i is set equal to the marginal minimum peak to peak separation time τ x i here 30 h the dependence thresholds u y i which does not need to equal the marginal thresholds u x i in the sense that f y i u y i f x i u x i can be decided based on assessing the stability of the estimates of α i β i for a range of thresholds and examining the independence of z i and y i see 40 for further details regarding diagnostics point 2 constitutes an issue for structure and consistency in the system of equations and for the construction of and simulation from the distribution function g i consistency in the system of equations can be obtained by careful vector treatment ensuring that the conditional model 10 is fitted only to the components y j i of y i which were observed simultaneously as y i in the construction of g i z i we take advantage of the lack of imposed structure on g i in 40 and the false assumption that the components of z i are mutually independent and gaussian distributed during inference the independence assumption renders possible componentwise estimation of the vector constants in eq 10 this makes it easier to allow for a few difficulties that arises because of how the residual current components making up y are defined eq 13 specifically for a given conditioning variate y i corresponding to a cluster peak excess there are certain restrictions on which components in y i that can be observed simultaneously as y i for instance if y i corresponds to a positive major axis velocity component at a particular depth it is not possible to simultaneously observe a negative major axis velocity component at the same depth and vice versa also due to spatial dependence there will generally be other combinations of y i y j i j i that are highly unlikely to occur together physically it is quite intuitive that given an extreme current velocity in a particular direction at one depth the current would be expected to flow in approximately the same direction at adjacent depths the practical consequence of this is that for some y j i we have no or very few observations to estimate the corresponding dependence parameters α j i β j i for the y j i with no observations this is not an issue g i is modelled by its empirical distribution function so for a given y i such y j will have zero probability of occurrence making dependence modelling redundant for the y j i with just a few observations estimates of α j i β j i will be unreliable due to the small sample size after careful inspection of the data at the two considered locations it was found that such y j i always corresponded to velocity components in the opposite direction 2 2 if for instance y i corresponds to a positive major axis component we mean by opposite direction y j i corresponding to negative major axis components of y i at adjacent depths physical considerations suggests that these pairs y i y j are negatively dependent denoting the number of observations of y j i by n y j i we therefore set α j i β j i 1 0 corresponding to negative asymptotic dependence for all y j i with just a few observations 3 3 if found to be unimportant for extremes an alternative can be to simply neglect ignore these observations for the data at hand we defined a few as n y j i 12 the remaining α j i and β j i and nuisance parameters μ j i and s j i are estimated by numerical minimization of the negative of the components q j i of the objective function 11 over the parameter space of the model only summing over the y i i and y j i observed simultaneously for both computational efficiency and stability it was found favourable to first perform the minimization without imposing the constraints in 49 and then using the resulting estimates as a starting point run the minimization again with the additional constraints in 49 the constraints were imposed on y i v where for each y i v was taken equal to the estimated marginal 50 year return level of y i as found in 49 the results showed little sensitivity to the precise choice of v as long as v was taken above the maximum observed value of y i based on the point estimates α ˆ i β ˆ i the observations z ˆ i of the random normalized residual z i are given by eq 12 the number of observations of z ˆ i is equal to the number of cluster peak excesses of y i denoted n y i and provide a sample from g i since each y i formally contains d 2 1 observed and d 2 unobserved components so will z ˆ i the dependence between the components partly in terms of magnitude but more importantly in terms of which components are simultaneously observed and which are unobserved is taken care of by modelling g i by its empirical distribution function g ˆ i the probability of observing a given component z j i from g ˆ i is equal to n y j i n y i when sampling from g ˆ i in the subsequent conditional simulation the number of observed z j i and y j i will thus follow a binomial distribution by conditioning on the cluster peak excesses of each of the d variates in turn the dependence model was estimated as in 45 and for many of the parameters examined in 40 it was found that conditional thresholds equalling the marginal thresholds in terms of quantiles gave reasonable stable results a dependence threshold equivalent to the 95 quantile for y i corresponding to major axis components and the 94 quantile for y i corresponding to minor axis components was thus selected the resulting estimates of the dependence parameters α j i and β j i when conditioning on y 1 and y 3 at munkskjæra are shown in fig 13 including the effect of imposing the 49 constraints for conditioning variate y 1 corresponding to the major axis velocity component u r m 1 strong dependence is seen with u r m at all other depths weaker dependence is seen with minor axis components there are no observations of simultaneously reversing currents at other depths u r m except one low velocity observation 1 cm s at depth number 3 16 m indicated by the single black dot with α 1 β 1 1 0 in fig 13 a for conditioning variate y 3 corresponding to the minor axis component u r m 1 strong dependence is seen for other u r m getting weaker with depth only weak dependence with other components is seen current reversals occur relatively frequent from depths 3 6 u r m the frequency increasing with depth qualitatively similar results as in fig 13 were observed when conditioning on other major and minor axis components at munkskjæra it is noticeable that the estimated dependence parameters are generally smoother with depth for velocity components with strong dependence e g for u r m in fig 13 a than for those with weak dependence as the estimated parameters are expected to vary relatively smoothly with depth this indicates that it can be more challenging to accurately estimate the dependence parameters for velocity components with weak association in addition the estimated α are usually smoother than those for β 47 48 notes that there are some redundancy between β and the nuisance parameters μ and s which can probably help explain this as a consequence it is observed that the non smooth parameter estimates are to some extent compensated by the residuals z j i in the conditional simulation yielding smoother simulated current profiles an illustration of the fitting of the dependence model 10 by log likelihood 11 is shown in fig 14 simultaneous observations of y 13 corresponding to u r m 4 and cluster peak excesses of conditioning variate y 1 corresponding to u r m 1 are displayed together with the fitted mean and standard deviation of y 13 as a function of y 1 the estimated near asymptotic dependence between these two variables as indicated in fig 13 a is seen to be well supported by the data in fig 14 b the resulting normalized residuals z ˆ 13 1 plotted against y 1 are shown such plots where the independence of z j i and y i is examined independence is a modelling assumption are part of the diagnostics to assess model fit no obvious dependence between z 13 1 and y 1 is seen 5 4 conditional simulation as previously mentioned estimates of various extremal statistics must be acquired by simulation the simulation is performed using the monte carlo sampling algorithm described in section 4 4 to account for temporal dependence a slight modification of how to determine the number of times which to condition on each variable has to be made compared to that proposed by 45 following the methodology from the marginal and dependence modelling we consider only the marginal cluster peak excesses of each x i i 1 d transformed to laplace distributed variables y i the yearly number of times which to condition on each y i is then determined by the number of times which y i is the only or the largest simultaneously observed y corresponding to a cluster peak excess divided by the effective number of years of observations we thus ensure that the number of simulated current profiles per year is equal to the average number of yearly observed residual current profiles with at least one velocity component corresponding to a marginal cluster peak excess when performing the conditional simulation a critical point has to be made clear in order to ensure consistency regarding how to determine which components of the simulated y i that should be observed and which should not the key point is that subsequent to having simulated a value of the conditioning variate y i we sample z i by randomly drawing with replacement one of the n y i observed vectors z ˆ i making up g ˆ i the d 2 1 observed components of the drawn z ˆ i then determines which components of the simulated vector y i obtained by eq 10 that are observed and which are not in that way the generally strong dependence between the components in terms of which are simultaneously observed are taken care of physically this ensures that the directional misalignment of the current velocity at different depths for a simulated current profile is approximately within the range observed in the measurements the components of z ˆ i can thus not be modelled as being independent i e g ˆ i z i j i g ˆ j i z j i a simplification involved using this approach is that the probability of observing a particular y j i does not depend on the magnitude of the simulated value of y i though quite laborious it would be possible to also model such dependence but as long as the conditional threshold is set sufficiently high it does not appear to affect the results in any critical way a simulated total current profile is obtained by adding a simulated residual current profile and a predicted tidal current profile assuming the residual and tidal current to be independent since the tidal current for practical purposes can be considered as periodic with period 18 6 years the nodal cycle predicting hourly tidal current profiles over this period is sufficient to fully describe the distribution of the tidal current profiles the predictions are made by the harmonic analysis described in section 3 2 and the tidal current profiles are drawn at random from the resulting sample of size 18 6 8766 the number of hourly observations during 18 6 years having decided the yearly number of times which to condition on each y i i 1 d the full sampling algorithm for each y i for simulating total current velocity profiles over a desired period is as follows 1 simulate a cluster peak excess of x i u x i from the generalized pareto distribution eq 15 transformation 5 via eq 19 yields a simulated y i 2 sample z i by drawing with replacement one of the n y i observed vectors z ˆ i 3 obtain y i α ˆ i y i y i β ˆ i z ˆ i the d 2 1 observed components of z ˆ i determines which components of y i that are observed and which are not 4 transform y y i y i to x using the inverse of transformation 5 to obtain a simulated residual current velocity profile 5 draw with replacement a tidal current velocity profile from the sample of 18 6 years of predicted tidal current profiles 6 add the realizations of residual and tidal current profiles to obtain a simulated total current velocity profile due to the introduction of an extremal index varying with level x in eq 19 the inverse of transformation 5 in step 4 above used to transform y to the original scale x cannot always be solved analytically to avoid the use of time consuming numerical solvers in the simulation procedure it is favourable to in advance fit a higher order polynomial to describe the relationship between x i and y i on the range u x i x x l i m i the range for which θ x i 1 x is not a constant if interest is in submodels obvious modifications to the simulation procedure must be made 5 5 simulated current profiles with the estimated marginal and conditional extremes models arbitrary long time series of extreme current velocity profiles can be simulated by the sampling algorithm described in section 5 4 to illustrate some results we have simulated 1000 years of extreme current profiles at both munkskjæra and salatskjæra 4 4 the single core computational time was between 15 and 20 min on an intel i7 3 ghz processor for each location particular focus will be placed on the residual current profiles as it is only the residual current that is described by the conditional extremes model in fig 15 and 16 the resulting simulated median marginal 10 year return values for all four residual current components at each depth are illustrated for munkskjæra and salatskjæra respectively the median 10 year return values and the quantile intervals are found by splitting the 1000 years of simulated residual current profiles into 10 year periods extracting componentwise maxima fitting a generalized extreme value gev distribution to the resulting 100 block maxima for each variable and extracting the 2 5 50 and 97 5 quantiles from the gev distributions 5 5 simple empirical quantile estimates gave almost identical results for comparison and partial validation of the dependence modelling and simulation procedure the median 10 year return values and corresponding 95 profile likelihood confidence intervals calculated from the fitted marginals from section 5 2 are also shown note that the calculated median 10 year return values from the fitted marginals are not equal to the 10 year return values estimated by eq 16 as the latter correspond to 37 quantiles the simulated median return values are seen to closely follow those calculated from the fitted marginals the simulated values consistently being slightly higher the tendency of a small positive bias is primarily related to the choice of only accounting for temporal dependence marginally when determining the number of current profiles to be simulated per year though compared to the confidence intervals of the fitted marginals the difference is generally negligible the tighter confidence intervals from the simulations are a natural consequence of the parametric bootstrap procedure used to obtain them emphasizing the stochastic nature of the plots simulated and observed residual tidal and total current velocities at 4 m depth at both munkskjæra and salatskjæra are illustrated in fig 17 note that the velocities have been transformed back to eastward and northward components only the first 10 year period of the simulated velocities are shown to resemble the simulated velocities only observations from the measurements where at least one of the residual current components of the velocity profile corresponds to a marginal cluster peak excess are plotted the simulated residual current velocities based on the conditional extremes model are seen to closely follow the trends of the observations looking at the simulated total currents they are also seen to match the observed total currents quite well particularly at munkskjæra at salatskjæra fig 17 f there are apparent evidence of some interaction between the tidal and the residual current the main effect being suppression of total current velocities in the northeastern direction similar interaction though less pronounced is apparent in the eastward direction at munkskjæra such interactions are not captured by the statistical model as independence between the tidal and the residual current is an inherent assumption however when including all observations from the measurements in the plot not shown the interaction is less pronounced at salatskjæra in the context of extreme value analysis it is reassuring that the most extreme simulated total current velocities both in terms of magnitude and direction coincide reasonably well with those from the observations similar agreement between simulated and observed velocities are seen at all other considered depths so far only marginal and single depth results have been presented in isolation these results are of limited interest as our main objective is to characterize instantaneous extreme current velocity profiles with depth presenting such multivariate data in a visually appealing manner is however challenging and we therefore present the instantaneous current profile results in terms of integrated variables since we are usually interested in the resulting force vector due to the current velocity profile for design of coastal and ocean structures such a measure is considered to be the most informal two dimensional summary of the current velocity profiles let us consider the resulting force on a fixed surface piercing vertical circular cylinder with diameter d 1 m and submerged length h the length h is taken equal to 35 m for munkskjæra and 30 m for salatskjæra so that the bottom of the cylinder is at a depth slightly lower than the deepest measurement point at the respective locations 34 m and 28 m the resulting cartesian force components or loads l e and l n in the eastward and northward direction respectively are given by the quadratic drag equation which for a vertical cylinder exposed to a horizontal current takes the form 22 l e 1 2 ρ c d d h 0 u e z u e z 2 u n z 2 d z l n 1 2 ρ c d d h 0 u n z u e z 2 u n z 2 d z where we take the water density ρ 1000 kg m 3 and the drag coefficient c d 1 the eastward and northward velocities u e and u n are functions of the vertical coordinate z z 0 at the sea surface a two dimensional illustration is shown in fig 20 for all observed and simulated current velocity profiles eq 22 is solved by numerical integration over the length of the cylinder assuming a linear velocity profile between the discrete measurement points and a constant velocity above the highest and below the lowest measurement point by applying bivariate kernel density estimation see e g 80 a smooth empirical estimate of the joint probability density function f l e l n l e l n can be obtained the resulting joint probability densities are then transformed to polar coordinates force magnitude l and direction φ the conditional cumulative distribution function of force given direction f l φ l ϕ can then be estimated allowing for estimation of extreme quantiles of the resulting force given direction in fig 18 estimates of the 0 95 0 99 and 0 995 quantiles of the conditional distribution of force given direction due to the residual current and total current velocity profiles at munkskjæra are presented both from the simulations and the observations the observation samples are also plotted again only observations where at least one of the residual current components of the velocity profile corresponds to a cluster peak excess are considered fig 19 displays a similar plot for salatskjæra at both sites the quantiles of the forces from the simulated current profiles are seen to follow the trends of the observations well particularly the residual current forces the estimates of the 0 95 quantile residual current forces are very similar as is expected due to the limited sample size the higher quantiles estimated from the observations are expected to be poorly estimated empirically and are dominated by outlying data and the choice of kernel bandwidth parameter however in terms of both magnitude and direction the observed and simulated quantiles are still in reasonable agreement the slightly rough quantile curves from the simulations are a consequence of the semiparametric nature of the conditional extremes model though smoother estimates can obviously be obtained by increasing the kernel bandwidth the additional discrepancies for the total current forces compared to the residual current forces alone are primarily related to the increased variability randomness introduced when combining residual and tidal current profiles and residual tidal current interaction at munkskjæra fig 18 b there are some evidence of interaction between the tidal and residual current in the eastward direction making the independence assumption appear slightly conservative at salatskjæra fig 19 b similar effects are seen in the northeastern direction in addition to an apparent clustering of extreme total current forces in the eastward direction both these effects can be also be seen in fig 17 f notwithstanding these effects the simulated total current forces are generally in reasonable agreement with those from the measurements as it has already been demonstrated that the simulated current velocity components are consistent both marginally and jointly at each depth the agreement between simulated and observed resulting forces suggests that the simulated current velocity profiles are also consistent 6 design current velocity profiles from a structural design perspective the key output from a statistical analysis of current profiles is so called design current velocity profiles in principle such design profiles should not be estimated in isolation of other relevant environmental parameters however as the focus of this paper is on current velocity profiles only we do not consider the additional effect of other environmental phenomena such as waves and wind when deciding these reference is made to offshore standards and recommended practices such as 43 and 23 for important joint environmental and structure specific considerations an example of a pragmatic way of accounting for the effect of the additional wave induced fluid velocity is found in 88 resulting in increased importance weighting of near surface currents when designing coastal or ocean structures design values of important metocean parameters are required to ensure that the structure in question can withstand the extreme environmental conditions expected to occur at the location s where it is to be operated the required design values are usually specified in terms of return periods or annual probabilities of occurrence the reciprocal of the return period for a univariate metocean parameter the value corresponding to a given return period is uniquely defined once its marginal distribution has been established in the case of multivariate metocean data like current velocity profiles this is not so however and some sort of sub ordering principle see 2 for an overview is useful to employ for easier identification of multivariate extreme design values in the context of structural design sub ordering of multivariate environmental phenomena is usually achieved through a generic load model that effectively combines the environmental variables into a relevant response tromans and vanderschuren 1995 the resulting total drag force on the structure in question is among the most informal summary variables of the effect of the current velocity profiles on a wide range of marine structures since the current can usually be considered as a steady flow field where the velocity vector is only a function of depth iso 2015 dnv 2014 this force can usually be calculated without much effort at least within the degree of accuracy required here the first step in our approach for deciding design current velocity profiles therefore involves calculating the total drag force or another relevant response function due to the simulated current velocity profiles we outline the approach below by considering a simple example it should however be mentioned that due to the flexibility of the conditional extremes model a range of alternative strategies will also be applicable besides the one presented here 6 1 example design current velocity profiles on a vertical circular cylinder following the example in section 5 5 we consider the total drag force vector due to the current velocity profiles acting on a vertical circular cylinder with diameter d 1 m and submerged length h see fig 20 for a two dimensional illustration again h is taken equal to 35 m for munkskjæra and 30 m for salatskjæra this is a simplified but representative example of a floating moored structure exposed to currents for all the simulated extreme total current velocity profiles covering the 1000 year simulation period the cartesian total drag force components are obtained by solving eq 22 by numerical integration over the length of the cylinder at the munkskjæra site this means calculating the resulting force due to approximately 755000 current profiles on a state of the art laptop this required only a few minutes of cpu time when using a discrete step size d z 1 m in the numerical integration the cartesian force components are conveniently transformed to polar coordinates yielding force magnitudes l and associated directions φ by looking at the resulting response the total drag force vector instead of the current velocity profiles directly our multivariate problem has effectively been reduced to a two dimensional one 6 6 the entire extreme value analysis could indeed have been performed on the structural response load directly as opposed to the current profiles but this would have left us with little knowledge about the actual shape of the current velocity profiles causing the extreme responses n year design current velocity profiles can then be obtained by first estimating the n year drag force favourably as a function of direction and subsequently deriving corresponding current velocity profiles yielding an equivalent force 6 1 1 estimating design drag forces 73 describe various ways of estimating design values of a two dimensional vector variable with a magnitude and a direction applying these methods both omni directional and directional return values of the simulated drag forces can be obtained two aspects greatly simplifying this step are 1 since temporal dependence has already been partly accounted for in the monte carlo sampling algorithm the simulated drag forces can be considered as independent 2 provided the simulation period is substantially longer than the return periods of interest say at least 10 20 times longer return values can be estimated directly from the simulated sample without the need of first fitting a parametric probability distribution denoting the yearly number of simulated current velocity profiles n s 1 y the estimated omni directional n year return value of the drag force l n is obtained by solving for the load that satisfies 23 f l l n 1 1 n s 1 y n as we only consider return periods n that are substantially shorter than the simulation period the distribution function of the drag load f l l can conveniently be approximated by its empirical distribution function directional return values can be obtained by looking at the probability that the drag load is exceeded in a particular direction providing information on the required directional structural strength following the discussion in 73 each simulated observation of l φ has a resolved drag load in every non orthogonal direction i e the observation l ϕ has a resolved load l ψ l cos ϕ ψ in each direction ψ 0 2 π for regularly spaced directions ψ j the resolved load l ψ j can be calculated for all the simulated observations l φ the n year return value l n ψ j for the resolved loads in direction ψ j are then calculated by solving 24 f l n ψ j 1 1 n s 1 y n with f l ψ j approximated by its empirical distribution function instead of resolved loads one might be interested in the load l n ψ δ which is exceeded on average once every n years conditional on the direction being in sector ψ δ ψ δ being a sector of width δ centred on ϕ ψ such estimates will depend on the sector width δ which should be decided taking both the structure and the directional variation of the load into consideration note that the sectors do not need to be equal in size but for design iso 19901 1 recommends that the sector width should not be smaller than 45 to avoid over optimization iso 2015 to decide l n ψ δ the conditional distribution of l ψ δ given ψ δ is required i e 25 f l ψ δ ψ δ 0 l ψ δ ψ δ 2 ψ δ 2 f r φ r ϕ d ϕ d r p ψ δ where p ψ δ is the probability of being in sector ψ δ both p ψ δ and f l ψ δ ψ δ can be estimated empirically particularly for small δ a bivariate kernel density estimate of f r φ r ϕ can provide smoother directional estimates finally the return level l n ψ δ can be estimated by solving 26 f l n ψ δ ψ δ 1 1 n s 1 y n p ψ δ applying the three approaches described above figs 21 and 22 shows estimates of extreme drag forces corresponding to return periods n 10 50 and 100 years at munkskjæra and salatskjæra respectively at munkskjæra the critical directions are clearly bounded to the east and west looking at the resolved force estimates l n ψ we note that the 10 year force is approximately of equal magnitude in the eastward and westward direction however for increasing n the difference magnifies reflecting the long tail higher gp shape parameter of the westward flowing current components cf fig 10 a at salatskjæra the critical direction is in a quite wide sector to the southeast an interesting feature is the local force minima at approximately 135 seen for the conditional force estimates l n ψ δ this local minima is also apparent in the measurements fig 19 b and is probably related to the presence of a few small islands and skerries upstream for flow in this direction it is pleasing to observe that the proposed statistical model is able to capture such complex directional behaviour the estimates of l n ψ δ force conditional on direction have similar directional characteristics as those of the resolved forces l n ψ but are smaller in magnitude because large forces in other directions can have resolved forces which exceed observed forces in direction sector ϕ δ also the estimates of l n ψ δ have been made with a sector width δ 1 to better illustrate the shape of the simulated sample using such a small sector width as environmental criteria for design is highly unconservative see e g 35 for a discussion on the use of directional environmental criteria 6 1 2 estimating design current velocity profiles taking advantage of the large sample size 1000 years a simple and pragmatic way of obtaining n year design current velocity profiles is to identify a subsample of observations in proximity of a prescribed n year force level and direction and then use the corresponding subsample of current velocity profiles directly to construct a design profile focusing on critical directional sectors along the 10 year return value line of the resolved forces l n ψ this approach is used below to obtain estimates of 10 year design current velocity profiles at both munkskjæra and salatskjæra from fig 21 and 22 it is noted that at least for n 50 a longer simulation period would generally be required so that the number of observations at such extreme levels is increased at munkskjæra we concentrate on the eastward sector 60 100 and the westward sector 255 280 in both sectors 21 simulated force observations have been identified near the respective peaks of the 10 year return level of l n ψ as indicated by the black crosses in fig 21 we thus have a sample of 21 current velocity profiles in each sector all of which approximately give rise to the same total drag force in the same directional sector to decide the design profiles we first resolve the current velocity at each depth of each profile into a velocity component in the same direction as the resulting drag force of the profile the longitudinal current velocity and a component orthogonal to the direction of this force the orthogonal current velocity the median longitudinal current velocity at each depth then serves as a reasonable two dimensional estimate of the 10 year design current velocity profile in the respective sectors we preferred the median over the mean as it is less sensitive to outliers if a three dimensional profile is desired the same operation can be performed on the orthogonal current components as well even though the resulting vertically integrated force due to the orthogonal current profiles is zero their local effect might be of importance in fig 23 the resulting 10 year design current velocity profiles are shown for both directional sectors together with empirical 10 and 90 quantiles of the velocity components at each depth obtained from the respective samples of 21 current velocity profiles as a simple way of validating if the shape of the profiles appears reasonable the velocity profiles from the measurements causing the 5 largest forces in the two sectors are also included though one should be careful not to put too much weight on such a validation it is reassuring that the shape of the design current profiles is in agreement with the measurements in sector 60 100 the median longitudinal velocity profile in the total force direction in fig 23 a is seen to be relatively uniform with depth it is somewhat surprising that the lowest velocity of the profile is at the top bin 4 m however this is actually supported by the measurements the orthogonal velocity components in fig 23 b are generally of minor importance indicating an approximately unidirectional flow in sector 255 280 the median longitudinal velocity profile in fig 23 c peaks with a velocity of 1 m s at depth 10 m decreasing for deeper levels again the orthogonal velocity components in fig 23 d are of minor importance at salatskjæra the focus is on the main critical sector 95 130 in addition we investigate sector 145 170 to see if there is any particular difference in the shape of the current profiles south of the local minima at 135 as for munkskjæra 21 current velocity profiles have been identified in each sector in proximity of the 10 year return level of l n ψ see fig 22 following the same procedure of resolving the velocity components at each depth into longitudinal and orthogonal velocity components relative to the total drag force the resulting 10 year design current profiles for both sectors are shown in fig 24 in sector 95 130 the longitudinal design profile in fig 24 a contains its highest velocities at intermediate depths a shape that is supported by the measurements a minor anticlockwise rotation is seen for the orthogonal profile in fig 24 b for sector 145 170 the highest velocities of the longitudinal design profile in fig 24 c is found at the deepest levels a consequence of the high gp shape parameters of the positive major axis components at these depths cf fig 10 b this shape is not fully supported by the measurements however there seems to be a trend of the lower level velocities increasing more than those at the upper levels so as we are extrapolating beyond the measurements it does not appear unrealistic either compared to sector 95 130 both the design profile and the measurements indicate a decrease in the upper level velocities increasing the relative importance of the lower level velocities overall the performance of the proposed statistical model and the approach for deriving design current velocity profiles appears to be satisfactory both at munkskjæra and salatskjæra 7 conclusions and discussion in this paper we have described a method for statistical modelling of extreme vertical current velocity profiles the method is based on the conditional extremes model by 40 arguably providing the most useful and flexible current approach for modelling extremes in high dimensions as illustrated in fig 1 we consider current velocity components resolved along major and minor axes at each depth and decompose the total current into tidal and residual currents using harmonic analysis a complete marginal model for each of the both positive and negative residual current components is then constructed and the dependence structure is characterized using the conditional extremes model estimates for various extremal statistics are acquired by simulating under this model and extreme total current velocity profiles can be obtained by randomly adding predicted deterministic tidal current profiles to realizations of stochastic extreme residual current profiles as implemented these steps accounts for directionality spatial and temporal dependence and non stationarity introduced by the tide based on a long period of simulated current velocity profiles a simple approach for deriving design current velocity profiles is also proposed the statistical method has been tested using adcp data from two coastal locations in norway we have shown that the method provides good extrapolations at both locations this is confirmed both marginally for each velocity component jointly for the velocity components at each depth and for the full velocity profiles the latter is primarily illustrated in terms of integrated variables resulting force vector on a circular cylinder due to the current profiles compared to the most extreme velocity profiles observed from the measurements the derived 10 year design current velocity profiles appear realistic both considering their shape magnitude and direction even though we have only considered the upper part of the water column significant change in the velocity as a function of depth is seen for most of the design profiles the shape of the design profiles is also found to be quite different in the two considered critical directional sectors at both locations we believe that the proposed method represents a valuable addition to existing methods for deriving extreme current velocity profiles as the vertical current structure is expected to be more variable in deeper and or stratified waters the proposed method will be even more beneficial to apply there as presented here the method makes a number of assumptions which should be commented upon for instance the residual current velocities are assumed to be stationary processes for the data at hand this is a reasonable assumption however at other locations this might not be the case if there are important seasonal changes a separate model might be specified for each season as in 40 with the obvious drawback that each model has less available data for inference important aspects regarding modelling of extremes of non stationary sequences can be found in 13 chap 6 further we have assumed that tidal and residual currents are independent albeit this appeared reasonable for our data independence might be a conservative assumption particularly in shallow water if tidal residual current interaction is evident analogous methods as when estimating extreme sea levels could probably be applied to account for this see 22 84 and 60 due to the increased dimensionality of the data and the additional dependence modelling this is however expected to be more challenging to implement for current profiles than for sea levels inference for marginal and dependence structure has been undertaken componentwise using the conventional peaks over threshold method to account for temporal dependence a major problem for estimation of extreme currents is often to obtain data of adequate length so using only the cluster peak excesses might seem rather wasteful of data 28 29 argue that more accurate and precise parameter and return level estimates can be obtained by using all threshold excesses this could provide an interesting alternative to the peaks over threshold method used here and it would also remove the complication of connecting the bulk and tail distribution cf section 5 2 3 it should be noted that temporal dependence still needs to be accounted for when constructing confidence intervals and estimating return levels this is relatively straightforward in the univariate case but might prove more defiant for multivariate data such as current profiles also we do not know how this affects the performance of the conditional extremes model another aspect both relevant for the marginal and the dependence modelling is the possibility of applying joint estimation instead of componentwise parameter estimation this is a valuable alternative when there are constraints between the parameters enabling inferential efficiency to be gained heffernan and tawn 2004 for instance one might expect the tail behaviour of a particular velocity component e g positive major axis components at adjacent depths to display similar tail behaviour this is equivalent to saying that the generalized pareto parameters of the velocity component varies relatively smoothly with depth similarly the dependence parameters for a given velocity component are also expected to vary relatively smoothly with depth treating depth as a covariate parameter smoothness could for instance be controlled within a penalized likelihood framework see e g 9 47 in this respect ideas from the piecewise stationary extreme value model of 76 seems particularly interesting to use together with the modelling strategy proposed herein though clearly adding complexity to the analysis threshold selection is an important step in the proposed method both when fitting the marginal tail model and the dependence model due to the large number of considered residual current components a suitable common threshold corresponding to a given non exceedance probability was selected in the present work one for the major axis components and one for the minor axis components common threshold selection or other automated procedures is inevitable due to the large number of possible threshold choices still as also commented by 45 a more satisfactory procedure than the one presented here would generally be desirable estimating models for different threshold levels and then averaging over the models to incorporate uncertainty in threshold selection is a possible but comprehensive solution see e g 63 or 75 in the conditional simulation cf section 5 4 there is one assumption which should be mentioned in particular subsequent to having simulated a value of the conditioning variate y i it concerns how to determine which components of the remaining profile y i that should be observed and which should not we have assumed that it is appropriate to determine this by drawing an arbitrary z ˆ i and let the d 2 1 observed components of z ˆ i control which components of y i that are observed as commented in section 5 4 a simplification involved here is that the probability of observing a particular y j i does not depend on the magnitude of the simulated y i the number of observed y j i in the conditional simulation thus follows a binomial distribution it is therefore important that the conditional threshold is set sufficiently high in the dependence modelling for the data at hand this assumption did not affect the results in any critical way however we expect that the robustness of the method could be improved by also modelling this type of dependence this is probably easier to accommodate if the current direction is explicitly considered as a covariate in the statistical model acknowledgements we would like to thank three anonymous reviewers for thorough and thoughtful comments this work was supported by the research council of norway through the centres of excellence funding scheme project number 223254 ntnu amos and through the centres for research based innovation funding scheme project number 237790 exposed appendix a residual current marginal distribution parameters and return levels estimated residual current marginal distribution parameters and return levels are given in table a 2 
22694,aeration effects on wave loading are of considerable importance for offshore design this paper describes experimental work to investigate four types of wave impact on a truncated vertical wall representative of a plate in an fpso vessel in pure and aerated water investigations showed that high aeration and flip through wave impacts are the most severe impact types and should therefore be considered for offshore structure design it was also observed that there is a significant reduction in peak impact loads both pressure and force for impacts in aerated water compared to those in pure water however there was almost no reduction in impulsive loadings in aerated water compared to those in pure water and therefore maximum instantaneous loads may be conservative in the presence of aerated water in the implementation for offshore structure design this paper is a companion paper to aeration effects on water structure impacts part 1 drop plate impacts keywords aeration effect wave impact physical model 1 introduction breaking wave impacts on vertical structures can produce very high loads which may lead to structural failure and damage wave impacts on coastal and offshore structures have been investigated experimentally and numerically for several decades most investigations have been carried out to improve understanding of the physics and characteristics of wave impacts on a vertical wall chan 1994 oumeraci et al 1992 1993 hattori et al 1994 bullock et al 2007 bredmose et al 2010 hofland et al 2011 lafeber et al 2012 guilcher et al 2013 chuang et al 2017 2018 or a vertical cylinder wienke and oumeraci 2005 the physics and characteristics of the impact loading have been shown by researchers to depend significantly on the breaking wave conditions oumeraci et al 1993 hattori et al 1994 hattori et al 1994 found in their experimental results that the smaller the amount of entrapped air between the breaking wave and the wall at the collision the higher the impact pressure however the distinctions between low aeration small amount of entrapped air and high aeration large amount of entrapped air were considered by bullock et al 2007 who found that a high aeration level which is known as a large air pocket wave impact does not always reduce the peak pressure and indeed can increase both the force and impulse on the structure the highest impact pressures were found to occur around still water level swl under regular wave conditions by hattori et al 1994 and bullock et al 2007 other researchers found the maximum peak pressure occurred at swl for certain impact types chan and meville 1988 chan 1994 hull and müller 2002 oumeraci et al 1993 whereas hofland et al 2011 found that the location of the pressure peak was above swl in their tests limited research has been conducted on the effect of aerated water on wave impacts kimmoun et al 2012 carried out experiments to investigate the influence of a bubble curtain on wave impacts on a vertical wall with the soliton and wave focusing techniques they found that for cases using the wave focusing technique the location of the bubble generator and the injected air flow rate affect the wave breaking process the variation of loads is increased while the area corresponding to the high loads is reduced in addition kimmoun et al 2012 found that the compressibility of the aerated water does not seem to be of significant influence on wave impacts generated using the soliton wave technique here we investigate the effects of aeration on wave impact by generating wave impacts of different types on a truncated vertical wall representing a section of fpso floating production storage and offloading vessel hull in pure and aerated water the companion paper aeration effects on water structure impacts part 1 drop plate impacts deals with the related and perhaps converse problem of a plate hitting an otherwise still water surface 2 methodology 2 1 wave impact test 2 1 1 physical model of wave impact test the wave impact experiments were carried out in the sediment wave flume at the university of plymouth coast lab the flume is 35 m long with a working section 0 6 m wide 1 2 m high and maximum still water depth of 0 8 m a schematic of the physical model setup is given in fig 1 the truncated vertical wall plate 1 is an aluminium plate of 0 56 m width by 0 6 m height and 0 012 m thickness connected to rigid plates plates 2 and 3 plates 2 and 3 were mounted on a support frame by a low profile load cell and plate 4 there were 0 02 m gaps on both sides of the tested model to ensure no friction between the model and the flume side walls where such a force would affect load measurement pressures under wave impact were measured by fgp xpm10 pressure sensors installed at 7 locations on the impact wall and a low profile load cell stainless steel series with an inline dc amplifier model 140 used to measure the total force on the wall an accelerometer model 4610 was used to measure vibration of the structure under wave impact fig 1c presents the configuration of the instrumentation on the impact plate pressure force and acceleration data were sampled at 35 khz frequency thirteen resistance type wave gauges were used to measure water elevation along the wave flume sampled at 128 hz in addition a photron sa4 high speed camera frame rate up to 3600 fps at a resolution of 1024 1024 was used to visualise the air pockets wave run up and jets produced at impact the same bubble generation system was used as for the drop tests to generate aerated water with 0 6 void fraction for these flume tests located 0 m from the impact wall fig 1a and b the 0 6 aeration level is the lowest aeration level we could produce in our wave flume to minimize distortion of water surface and water flow under tested waves the bubble generator was switched off for the first condition mai 2017 and mai et al 2019 present further details of the bubble generation system 2 1 2 wave conditions focused wave groups were generated using newwave tromans et al 1991 focusing with an underlying jonswap spectrum γ 3 3 a focusing technique was applied to generate different types of wave impact by changing the focus location x f kimmoun et al 2009 wave conditions were scaled from prototype by a factor of 1 65 of the 100 year extreme significant wave height at the cleeton platform in the southern north sea williams 2008 four different types of wave impact were generated by changing the focal location from an absorbing piston paddle in the wave flume the wave paddle is 0 5 m wide by 1 m high the distance between the front impact plate plate 1 and the wave paddle is 26 9 m the target focus points are located at x f 28 54 m 28 84 m 29 04 m and 29 44 m from the wave paddle for the broken high aeration flip through and slightly breaking waves respectively the tested wave impacts are a broken wave impact x f 28 54 m fig 2 a this wave breaks near the front face of the wall this produces a small aerated water mass which hits the wall and is similar to that described by bullock et al 2007 b high aeration wave impact x f 28 84 m fig 2b at impact the wall and wave enclose a combined cloud of bubbles and large air pocket bullock et al 2007 c flip through wave impact x f 29 04 m fig 2c at impact uprush on the wall causes a jet to rush vertically upwards just before the crest hits the wall bredmose et al 2010 kimmoun et al 2009 d slightly breaking wave x f 29 44 m fig 2d this has a higher run up than its crest which is slightly broken when it reaches the wall bullock et al 2007 3 results and discussion 3 1 characteristics of wave impacts in pure water 3 1 1 broken wave impact in pure water fig 3 a b c presents typical time histories of acceleration and force under broken wave impact focus distance x f 28 54 m on the wall in pure water time t 0 s corresponds to the time of maximum force in this case the wave breaks just as it hits the wall accelerations up to 3g were recorded for the broken wave impacts and are presented in fig 3a the maximum impact force due to the broken wave for this particular test was 1 07 kn fig 3b the low frequency oscillation superimposed on the force time history is found to be close to the natural frequency of the model f n 40 hz this low frequency oscillation was observed in all the force time histories of the broken wave impacts due to the chaotic nature of the turbulent flow associated with the broken wave there is considerable randomness apparent in the pressure time history as can be seen in fig 3c at level z d 0 32 there was a high impact pressure of 0 27 bar the highest value that was observed for broken waves at t 8 71 ms see fig 3c many other test runs of broken wave impacts show high impact pressures up to 0 22 bar that were attained at early times around t 100 ms there are high frequency oscillations superimposed on the pressure signals these oscillations are likely to be due to the alternate expansion and compression of the dense cloud of bubbles seen in the image sequence of fig a 1 in appendix a 1 and also noted by hattori et al 1994 and bullock et al 2007 fig a 1 shows sequential snapshots of the broken wave impact and a small volume of aerated water is evident the wave approaches from the left hand side and the wall is located at the right hand end of each snapshot as shown in fig a 1 at t 100 ms the breaking wave hit the wall causing high random impact pressures between t 100 ms and t 0 repeatability of acceleration force and pressures observed in five broken wave impact tests is presented in fig b 1 in appendix b 1 acceleration and pressures on the wall have significant variation between repeats fig b 1a and fig b 1c h but the total force on the wall seems to be repeatable fig b 1b 3 1 2 high aeration wave impact in pure water under the high aeration wave impact focus distance x f 28 84 m the maximum acceleration of the wall is up to 3 5g for the case illustrated in fig 3d the time history of the measured force on the wall under high aeration impact is presented in fig 3e the low frequency oscillations of the acceleration and force traces after the impact between t 0 ms and t 500 ms in fig 3d and e can be seen and they are identical to the 40 hz natural frequency of the structure fig 3f presents associated time histories of pressures on the wall at different levels for the test case presented in fig 3d and e higher impact pressures in comparison with the previous wave impact types are found and the time of peak pressure for each gauge is now nearly simultaneous under this wave impact type the low frequency oscillation found in acceleration and force traces can also be observed from the pressure traces under the high aeration wave impact type see fig 3 it is shown that there are high frequency 170 hz 880 hz pressure oscillations after the impact between t 0 ms and t 11 ms in fig 3f these high frequency oscillations may be due to the acoustic wave reflecting from the flume bottom and or due to air pocket and bubble oscillations using the theoretical natural frequency of air bubbles in water as derived by minnaert 1933 and hattori et al 1994 the frequency of oscillation observed here corresponds to air pocket and bubble radius of between 19 2 mm and 3 7 mm this behaviour of the pressure time histories after the impact is similar to the previous findings for large air pocket wave impacts hattori et al 1994 or high aeration wave impacts bullock et al 2007 where the variation of pressure peaks are very large fig a 2 shows the high aeration wave impact in sequential snapshots taken from t 100 ms to t 50 ms it can be seen that the wave crest started to overturn at t 100 ms and as time proceeded the jet of the breaking wave hit the wall and entrapped a large air volume and a cloud of bubbles see at time between t 30 ms and t 10 ms measurements of acceleration force and pressures on the wall are shown in fig b 2 for five high aeration wave impact tests the total force on the wall shows a high level of repeatability for the high aeration wave impact fig b 2b low frequency oscillation after impact on the acceleration and pressure traces is found to be repeatable but the maximum acceleration and the impact pressure show some variation fig b 2a c h 3 1 3 flip through wave impact in pure water flip through impacts occur when the wave front is kept from striking the wall by a jet coming from below and this type lies between air pocket impact and sloshing flip through impact was first identified by cooker and peregrine 1990 from fully non linear potential flow computation fig 4 a b c presents time histories of the accelerations force and pressures of a flip through impact test case which caused the largest impact pressure 0 54 bar at level z d 0 39 in fig 4c obtained from any experiment presented here the focus distance of x f 29 04 m was used to generate this flip through wave impact type the maximum acceleration of the wall on impact was recorded up to 4 5g for this particular case fig 4a similar to the broken wave and high aeration impacts evidence of the low frequency oscillation 37 hz due to the natural frequency vibration of the impact wall was found in the acceleration force and pressure signals under the flip through impacts a high frequency oscillation 620 hz was also observed after impact in pressure signals at all measured levels fig 4c this high frequency oscillation may be due to oscillation of 10 6 mm diameter air bubble or a bubble cloud which was enclosed during the flip through impact due to turbulence of the water surface the high frequency oscillation of pressures after impact might also be caused by a sound wave reflected from the flume bed phase differences of the high frequency oscillations in the pressure traces were observed and this may be due to the different distances between the pressure sensors and the impact point where the air bubble was formed and the sound wave started to transmit fig a 3 in appendix a 1 presents snapshots of the flip through impact at different time instants and it shows clearly the turbulence associated with the flip through wave impact similar to the high aeration wave impact the total force on the wall is shown to have a high level of repeatability fig b 3b however acceleration and pressures are not repeatable except the low frequency oscillation after the impact which is known to be due to the natural frequency vibration of the wall fig b 3a c h 3 1 4 slightly breaking wave impact in pure water increasing the focus distance up to x f 29 44 m leads to the run up on the wall being higher than the crest and the crest is slightly broken when it reaches the wall fig 4d e f present accelerations force and pressures under the slightly breaking wave impact a maximum acceleration of 0 7 g was recorded after the impact for this particular slightly breaking wave impact fig 4d there is also a low frequency oscillation evident in the force signal see fig 4e but the amplitude of this oscillation is much lower than that due to other wave impacts broken high aeration and flip through impacts as can be seen in the pressure time histories presented in fig 4f the pressure peak tends to decrease as the level z d increases except at the level z d 0 32 and 0 36 where the wave crest hits the wall causing high pressures a preceding single impact was found on the pressure trace at level z d 0 36 that is due to slight breaking of the wave crest which can be seen in the snapshots in fig a 4 in appendix a 1 acceleration total force and pressures on the wall due to five slightly breaking wave impacts are illustrated in fig b 4 in appendix b 1 and show clear lack of repeatability of acceleration and pressures on the wall as findings from other wave impact types have shown fig b 4a c h however total force on the wall seems to be repeatable for the slightly breaking wave impact see fig b 4b 3 2 characteristics of wave impacts in aerated water fig 5 and fig 6 illustrate typical time histories of acceleration force and pressures on the wall in the aerated water for the four wave impact types comparisons of water surface elevation time histories between the tests in pure and aerated water have been made and presented in fig 7 for the four wave impact types at two different flume locations fig a 5 to a 8 in appendix a 2 show the sequential snapshots during the wave impacts in aerated water fig 5 a b d f and fig 6a b d f show that during the impacts the low frequency oscillation 37 hz is evident in the acceleration and force traces but its amplitude is smaller than that in pure water looking at the pressure traces in fig 5c f and fig 6c f there are no significant high pressure peaks such as obtained in pure water under the impacts presented in section 3 1 high frequency oscillations after the impacts were also observed in the pressure traces particularly evident in fig 5f and these oscillations are considered to be due to the expansion and compression and pressure wave transmitted through the water air mixture repeatability of wave impacts in aerated water is presented in appendix b 2 for four wave impact types similar to wave impact in pure water the total force on the wall shows good repeatability and the accelerations and pressures are more variable with no clear repeatable behaviour water surface elevation time histories of the same wave impact types in pure and aerated water at location x 0 15 m x 0 corresponds to the front of the impact wall are presented in fig 7 air flow expelled from the bubble generator came into the water body and generated a flow which was first in the vertical direction and then horizontally consequently there was a surface current opposite to the incoming wave direction this effect on the incoming waves is clearly shown in fig 7 it tended to change the incoming wave in the aerated water dashed line in plots so that it arrived 0 08 s later than that in the pure water solid line furthermore it slowed down the wave leading to an increase in the wave steepness the difference between the maximum wave crest elevations in pure and aerated waters at x 0 15 m was up to 0 01 m the disturbance of the wave crest due to the presence of a bubble curtain was studied by kimmoun et al 2012 who showed that the disturbance could lead to a small decrease in impact pressure on a vertical wall such as is found in the experiments presented here 3 3 wave impact pressure and its impulse the maximum pressures at various levels on the wall are presented in fig 8 for all wave impact types in pure and aerated water the vertical axis is the dimensionless level z d of the measured points on the wall and the swl is represented by z d 0 the horizontal axis is the logarithm of dimensionless impact pressure p max ρgd the black diamonds are the impact pressures in pure water and the red pluses are the impact pressures in the aerated water with 0 6 of void fraction the solid and dashed lines join mean values of data points at each level on the wall in pure and aerated water respectively maximum impact pressures were found to occur at elevations above the swl for impacts in 0 625 m water depth it is clearly seen from fig 8 that there is a significant reduction of the impact pressures from pure water to aerated water the maximum impact pressures measured in pure water were 0 27 bar 0 38 bar 0 56 bar and 0 2 bar for the broken high aeration flip through and slightly breaking wave impacts whereas those maximum impact pressures were only from 0 03 bar to 0 13 bar for wave impacts in aerated water the incident wave was also affected by the current and turbulence induced by the bubble generator which further reduces the impact pressures significant reduction of the impact pressure in aerated water was also found in the experimental work of kimmoun et al 2012 fig 9 shows pressure impulses on the vertical wall for pure and aerated water the impulse value is integrated over the duration of the measured signal cooker and peregrine 1995 the black diamond markers and solid line are the pressure impulse and its mean values at the measured levels in pure water respectively the red plus marker and dashed line are for the aerated water the results show that the pressure impulses in aerated water are a little smaller than those in pure water for all types of wave impacts broken high aeration flip through and slightly breaking pressure impulses decrease with increasing elevation on the wall for both pure and aerated water this is because the pressure oscillations were sustained for a long duration 3 4 wave impact force and its impulse fig 10 presents the impact forces and force impulses on the wall in pure and aerated water as seen for the pressure results impact forces in pure water are significantly higher than in aerated water fig 10a in both pure and aerated waters the flip through impact resulted in the highest impact force and the broken wave impact caused the lowest impact force on the hull section similar findings were reported by kimmoun et al 2012 for a vertical wall and bubble curtain arrangement in general the average values of the total force impulses in pure water are higher than those in aerated water but their variation is quite large and in some events the total force impulse in aerated water is larger than that in pure water fig 10b 4 conclusions this paper examined the role of aeration in wave impacts experimental work was conducted by means of wave impact on a truncated vertical rigid wall hull section in pure and aerated water wave impacts on a truncated vertical wall designed to represent a vertical section of an fpso hull were explored for various wave impact types including broken high aeration large air pocket flip through and slightly breaking wave impacts it is found that the high aeration and flip through wave impacts are the most severe impact types and they should be considered for offshore structure design post impact high frequency oscillations of pressures were observed for wave impacts and these oscillations are due to repeated compression and expansion of the trapped air between wave front and the truncated wall therefore the fatigue analysis of a section of the hull or whole ship structure may need to assess those local high frequency loading oscillations this study observed that aeration has an important effect on wave impacts a significant reduction in the hydrodynamic impact loadings pressure and force is found for those measured in aerated water compared to those in pure water though there is almost no reduction of the total loading impulses for wave impacts therefore maximum instantaneous loads calculated for pure water may be conservative in the presence of aerated water in the implication for offshore structure design currently standard design practices det norske veritas 2012 2016 have not taken into account aerated water in design the effects of aeration should be taken into consideration for the offshore environment where wave breaking is commonplace leading to aerated wave interactions with structures acknowledgements this study was a part of the froth fundamentals and reliability of offshore structure hydrodynamics project supported by the engineering and physical science research council epsrc grant ep j012866 1 the froth project was led by plymouth university and the collaborative partners included oxford university university of bath city university london and the manchester metropolitan university the authors gratefully acknowledge the financial support provided by epsrc and useful discussions with the project partners appendix a visualisation of wave impacts table a1 description of figures presented in appendix a table a1 figures description fig a 1 snapshots of broken wave impact in pure water fig a 2 snapshots of high aeration wave impact in pure water fig a 3 snapshots of flip through wave impact in pure water fig a 4 snapshots of slightly breaking wave impact in pure water fig a 5 snapshots of broken wave impact in aerated water fig a 6 snapshots of high aeration wave impact in aerated water fig a 7 snapshots of flip through wave impact in aerated water fig a 8 snapshots of slightly breaking wave impact in aerated water a 1visualisation of wave impacts in pure water fig a1 snapshots of broken wave impact in pure water in each snapshot the wave was coming from the left and the wall was located on the right fig a1 fig a2 snapshots of high aeration wave impact in pure water in each snapshot the wave was coming from the left and the wall was located on the right 2 fig a2 fig a3 snapshots of flip through wave impact in pure water in each snapshot the wave was coming from the left and the wall was located on the right 3 fig a3 fig a4 snapshots of slightly breaking wave impact in pure water in each snapshot the wave was coming from the left and the wall was located on the right 4 fig a4 visualisation of wave impacts in aerated water fig a5 snapshots of broken wave impact in aerated water in each snapshot the wave was coming from the left and the wall was located on the right 5 fig a5 fig a6 snapshots of high aeration wave impact in aerated water in each snapshot the wave was coming from the left and the wall was located on the right 6 fig a6 fig a7 snapshots of flip through wave impact in aerated water in each snapshot the wave was coming from the left and the wall was located on the right 7 fig a7 fig a8 snapshots of slightly breaking wave impact in aerated water in each snapshot the wave was coming from the left and the wall was located on the right 8 fig a8 appendix b repeatability of wave impacts table b1 description of figures presented in appendix b table b1 figures description fig b 1 repeatability of acceleration force and pressures on the wall under broken wave impact in pure water fig b 2 repeatability of acceleration force and pressures on the wall under high aeration wave impact in pure water fig b 3 repeatability of acceleration force and pressures on the wall under flip through wave impact in pure water fig b 4 repeatability of acceleration force and pressures the wall under slightly breaking wave impact in pure water fig b 5 repeatability of acceleration force and pressures on the wall under broken wave impact in aerated water fig b 6 repeatability of acceleration force and pressures on the wall under high aeration wave impact in aerated water fig b 7 repeatability of acceleration force and pressures on the wall under flip through wave impact in aerated water fig b 8 repeatability of acceleration force and pressures the wall under slightly breaking wave impact in aerated water b 1repeatability of wave impacts in pure water fig b1 repeatability of acceleration force and pressures on the wall under broken wave impact in pure water 9 fig b1 fig b2 repeatability of acceleration force and pressures on the wall under high aeration wave impact in pure water 0 fig b2 fig b3 repeatability of acceleration force and pressures on the wall under flip through wave impact in pure water 1 fig b3 fig b4 repeatability of acceleration force and pressures the wall under slightly breaking wave impact in pure water 2 fig b4 repeatability of wave impacts in aerated water fig b5repeatability of acceleration force and pressures on the wall under broken wave impact in aerated water 3 figure b6repeatability of acceleration force and pressures on the wall under high aeration wave impact in aerated water 4 fig b7 repeatability of acceleration force and pressures on the wall under flip through wave impact in aerated water 5 fig b7 fig b8 repeatability of acceleration force and pressures on the wall under slightly breaking wave impact in aerated water 6 fig b8 
22694,aeration effects on wave loading are of considerable importance for offshore design this paper describes experimental work to investigate four types of wave impact on a truncated vertical wall representative of a plate in an fpso vessel in pure and aerated water investigations showed that high aeration and flip through wave impacts are the most severe impact types and should therefore be considered for offshore structure design it was also observed that there is a significant reduction in peak impact loads both pressure and force for impacts in aerated water compared to those in pure water however there was almost no reduction in impulsive loadings in aerated water compared to those in pure water and therefore maximum instantaneous loads may be conservative in the presence of aerated water in the implementation for offshore structure design this paper is a companion paper to aeration effects on water structure impacts part 1 drop plate impacts keywords aeration effect wave impact physical model 1 introduction breaking wave impacts on vertical structures can produce very high loads which may lead to structural failure and damage wave impacts on coastal and offshore structures have been investigated experimentally and numerically for several decades most investigations have been carried out to improve understanding of the physics and characteristics of wave impacts on a vertical wall chan 1994 oumeraci et al 1992 1993 hattori et al 1994 bullock et al 2007 bredmose et al 2010 hofland et al 2011 lafeber et al 2012 guilcher et al 2013 chuang et al 2017 2018 or a vertical cylinder wienke and oumeraci 2005 the physics and characteristics of the impact loading have been shown by researchers to depend significantly on the breaking wave conditions oumeraci et al 1993 hattori et al 1994 hattori et al 1994 found in their experimental results that the smaller the amount of entrapped air between the breaking wave and the wall at the collision the higher the impact pressure however the distinctions between low aeration small amount of entrapped air and high aeration large amount of entrapped air were considered by bullock et al 2007 who found that a high aeration level which is known as a large air pocket wave impact does not always reduce the peak pressure and indeed can increase both the force and impulse on the structure the highest impact pressures were found to occur around still water level swl under regular wave conditions by hattori et al 1994 and bullock et al 2007 other researchers found the maximum peak pressure occurred at swl for certain impact types chan and meville 1988 chan 1994 hull and müller 2002 oumeraci et al 1993 whereas hofland et al 2011 found that the location of the pressure peak was above swl in their tests limited research has been conducted on the effect of aerated water on wave impacts kimmoun et al 2012 carried out experiments to investigate the influence of a bubble curtain on wave impacts on a vertical wall with the soliton and wave focusing techniques they found that for cases using the wave focusing technique the location of the bubble generator and the injected air flow rate affect the wave breaking process the variation of loads is increased while the area corresponding to the high loads is reduced in addition kimmoun et al 2012 found that the compressibility of the aerated water does not seem to be of significant influence on wave impacts generated using the soliton wave technique here we investigate the effects of aeration on wave impact by generating wave impacts of different types on a truncated vertical wall representing a section of fpso floating production storage and offloading vessel hull in pure and aerated water the companion paper aeration effects on water structure impacts part 1 drop plate impacts deals with the related and perhaps converse problem of a plate hitting an otherwise still water surface 2 methodology 2 1 wave impact test 2 1 1 physical model of wave impact test the wave impact experiments were carried out in the sediment wave flume at the university of plymouth coast lab the flume is 35 m long with a working section 0 6 m wide 1 2 m high and maximum still water depth of 0 8 m a schematic of the physical model setup is given in fig 1 the truncated vertical wall plate 1 is an aluminium plate of 0 56 m width by 0 6 m height and 0 012 m thickness connected to rigid plates plates 2 and 3 plates 2 and 3 were mounted on a support frame by a low profile load cell and plate 4 there were 0 02 m gaps on both sides of the tested model to ensure no friction between the model and the flume side walls where such a force would affect load measurement pressures under wave impact were measured by fgp xpm10 pressure sensors installed at 7 locations on the impact wall and a low profile load cell stainless steel series with an inline dc amplifier model 140 used to measure the total force on the wall an accelerometer model 4610 was used to measure vibration of the structure under wave impact fig 1c presents the configuration of the instrumentation on the impact plate pressure force and acceleration data were sampled at 35 khz frequency thirteen resistance type wave gauges were used to measure water elevation along the wave flume sampled at 128 hz in addition a photron sa4 high speed camera frame rate up to 3600 fps at a resolution of 1024 1024 was used to visualise the air pockets wave run up and jets produced at impact the same bubble generation system was used as for the drop tests to generate aerated water with 0 6 void fraction for these flume tests located 0 m from the impact wall fig 1a and b the 0 6 aeration level is the lowest aeration level we could produce in our wave flume to minimize distortion of water surface and water flow under tested waves the bubble generator was switched off for the first condition mai 2017 and mai et al 2019 present further details of the bubble generation system 2 1 2 wave conditions focused wave groups were generated using newwave tromans et al 1991 focusing with an underlying jonswap spectrum γ 3 3 a focusing technique was applied to generate different types of wave impact by changing the focus location x f kimmoun et al 2009 wave conditions were scaled from prototype by a factor of 1 65 of the 100 year extreme significant wave height at the cleeton platform in the southern north sea williams 2008 four different types of wave impact were generated by changing the focal location from an absorbing piston paddle in the wave flume the wave paddle is 0 5 m wide by 1 m high the distance between the front impact plate plate 1 and the wave paddle is 26 9 m the target focus points are located at x f 28 54 m 28 84 m 29 04 m and 29 44 m from the wave paddle for the broken high aeration flip through and slightly breaking waves respectively the tested wave impacts are a broken wave impact x f 28 54 m fig 2 a this wave breaks near the front face of the wall this produces a small aerated water mass which hits the wall and is similar to that described by bullock et al 2007 b high aeration wave impact x f 28 84 m fig 2b at impact the wall and wave enclose a combined cloud of bubbles and large air pocket bullock et al 2007 c flip through wave impact x f 29 04 m fig 2c at impact uprush on the wall causes a jet to rush vertically upwards just before the crest hits the wall bredmose et al 2010 kimmoun et al 2009 d slightly breaking wave x f 29 44 m fig 2d this has a higher run up than its crest which is slightly broken when it reaches the wall bullock et al 2007 3 results and discussion 3 1 characteristics of wave impacts in pure water 3 1 1 broken wave impact in pure water fig 3 a b c presents typical time histories of acceleration and force under broken wave impact focus distance x f 28 54 m on the wall in pure water time t 0 s corresponds to the time of maximum force in this case the wave breaks just as it hits the wall accelerations up to 3g were recorded for the broken wave impacts and are presented in fig 3a the maximum impact force due to the broken wave for this particular test was 1 07 kn fig 3b the low frequency oscillation superimposed on the force time history is found to be close to the natural frequency of the model f n 40 hz this low frequency oscillation was observed in all the force time histories of the broken wave impacts due to the chaotic nature of the turbulent flow associated with the broken wave there is considerable randomness apparent in the pressure time history as can be seen in fig 3c at level z d 0 32 there was a high impact pressure of 0 27 bar the highest value that was observed for broken waves at t 8 71 ms see fig 3c many other test runs of broken wave impacts show high impact pressures up to 0 22 bar that were attained at early times around t 100 ms there are high frequency oscillations superimposed on the pressure signals these oscillations are likely to be due to the alternate expansion and compression of the dense cloud of bubbles seen in the image sequence of fig a 1 in appendix a 1 and also noted by hattori et al 1994 and bullock et al 2007 fig a 1 shows sequential snapshots of the broken wave impact and a small volume of aerated water is evident the wave approaches from the left hand side and the wall is located at the right hand end of each snapshot as shown in fig a 1 at t 100 ms the breaking wave hit the wall causing high random impact pressures between t 100 ms and t 0 repeatability of acceleration force and pressures observed in five broken wave impact tests is presented in fig b 1 in appendix b 1 acceleration and pressures on the wall have significant variation between repeats fig b 1a and fig b 1c h but the total force on the wall seems to be repeatable fig b 1b 3 1 2 high aeration wave impact in pure water under the high aeration wave impact focus distance x f 28 84 m the maximum acceleration of the wall is up to 3 5g for the case illustrated in fig 3d the time history of the measured force on the wall under high aeration impact is presented in fig 3e the low frequency oscillations of the acceleration and force traces after the impact between t 0 ms and t 500 ms in fig 3d and e can be seen and they are identical to the 40 hz natural frequency of the structure fig 3f presents associated time histories of pressures on the wall at different levels for the test case presented in fig 3d and e higher impact pressures in comparison with the previous wave impact types are found and the time of peak pressure for each gauge is now nearly simultaneous under this wave impact type the low frequency oscillation found in acceleration and force traces can also be observed from the pressure traces under the high aeration wave impact type see fig 3 it is shown that there are high frequency 170 hz 880 hz pressure oscillations after the impact between t 0 ms and t 11 ms in fig 3f these high frequency oscillations may be due to the acoustic wave reflecting from the flume bottom and or due to air pocket and bubble oscillations using the theoretical natural frequency of air bubbles in water as derived by minnaert 1933 and hattori et al 1994 the frequency of oscillation observed here corresponds to air pocket and bubble radius of between 19 2 mm and 3 7 mm this behaviour of the pressure time histories after the impact is similar to the previous findings for large air pocket wave impacts hattori et al 1994 or high aeration wave impacts bullock et al 2007 where the variation of pressure peaks are very large fig a 2 shows the high aeration wave impact in sequential snapshots taken from t 100 ms to t 50 ms it can be seen that the wave crest started to overturn at t 100 ms and as time proceeded the jet of the breaking wave hit the wall and entrapped a large air volume and a cloud of bubbles see at time between t 30 ms and t 10 ms measurements of acceleration force and pressures on the wall are shown in fig b 2 for five high aeration wave impact tests the total force on the wall shows a high level of repeatability for the high aeration wave impact fig b 2b low frequency oscillation after impact on the acceleration and pressure traces is found to be repeatable but the maximum acceleration and the impact pressure show some variation fig b 2a c h 3 1 3 flip through wave impact in pure water flip through impacts occur when the wave front is kept from striking the wall by a jet coming from below and this type lies between air pocket impact and sloshing flip through impact was first identified by cooker and peregrine 1990 from fully non linear potential flow computation fig 4 a b c presents time histories of the accelerations force and pressures of a flip through impact test case which caused the largest impact pressure 0 54 bar at level z d 0 39 in fig 4c obtained from any experiment presented here the focus distance of x f 29 04 m was used to generate this flip through wave impact type the maximum acceleration of the wall on impact was recorded up to 4 5g for this particular case fig 4a similar to the broken wave and high aeration impacts evidence of the low frequency oscillation 37 hz due to the natural frequency vibration of the impact wall was found in the acceleration force and pressure signals under the flip through impacts a high frequency oscillation 620 hz was also observed after impact in pressure signals at all measured levels fig 4c this high frequency oscillation may be due to oscillation of 10 6 mm diameter air bubble or a bubble cloud which was enclosed during the flip through impact due to turbulence of the water surface the high frequency oscillation of pressures after impact might also be caused by a sound wave reflected from the flume bed phase differences of the high frequency oscillations in the pressure traces were observed and this may be due to the different distances between the pressure sensors and the impact point where the air bubble was formed and the sound wave started to transmit fig a 3 in appendix a 1 presents snapshots of the flip through impact at different time instants and it shows clearly the turbulence associated with the flip through wave impact similar to the high aeration wave impact the total force on the wall is shown to have a high level of repeatability fig b 3b however acceleration and pressures are not repeatable except the low frequency oscillation after the impact which is known to be due to the natural frequency vibration of the wall fig b 3a c h 3 1 4 slightly breaking wave impact in pure water increasing the focus distance up to x f 29 44 m leads to the run up on the wall being higher than the crest and the crest is slightly broken when it reaches the wall fig 4d e f present accelerations force and pressures under the slightly breaking wave impact a maximum acceleration of 0 7 g was recorded after the impact for this particular slightly breaking wave impact fig 4d there is also a low frequency oscillation evident in the force signal see fig 4e but the amplitude of this oscillation is much lower than that due to other wave impacts broken high aeration and flip through impacts as can be seen in the pressure time histories presented in fig 4f the pressure peak tends to decrease as the level z d increases except at the level z d 0 32 and 0 36 where the wave crest hits the wall causing high pressures a preceding single impact was found on the pressure trace at level z d 0 36 that is due to slight breaking of the wave crest which can be seen in the snapshots in fig a 4 in appendix a 1 acceleration total force and pressures on the wall due to five slightly breaking wave impacts are illustrated in fig b 4 in appendix b 1 and show clear lack of repeatability of acceleration and pressures on the wall as findings from other wave impact types have shown fig b 4a c h however total force on the wall seems to be repeatable for the slightly breaking wave impact see fig b 4b 3 2 characteristics of wave impacts in aerated water fig 5 and fig 6 illustrate typical time histories of acceleration force and pressures on the wall in the aerated water for the four wave impact types comparisons of water surface elevation time histories between the tests in pure and aerated water have been made and presented in fig 7 for the four wave impact types at two different flume locations fig a 5 to a 8 in appendix a 2 show the sequential snapshots during the wave impacts in aerated water fig 5 a b d f and fig 6a b d f show that during the impacts the low frequency oscillation 37 hz is evident in the acceleration and force traces but its amplitude is smaller than that in pure water looking at the pressure traces in fig 5c f and fig 6c f there are no significant high pressure peaks such as obtained in pure water under the impacts presented in section 3 1 high frequency oscillations after the impacts were also observed in the pressure traces particularly evident in fig 5f and these oscillations are considered to be due to the expansion and compression and pressure wave transmitted through the water air mixture repeatability of wave impacts in aerated water is presented in appendix b 2 for four wave impact types similar to wave impact in pure water the total force on the wall shows good repeatability and the accelerations and pressures are more variable with no clear repeatable behaviour water surface elevation time histories of the same wave impact types in pure and aerated water at location x 0 15 m x 0 corresponds to the front of the impact wall are presented in fig 7 air flow expelled from the bubble generator came into the water body and generated a flow which was first in the vertical direction and then horizontally consequently there was a surface current opposite to the incoming wave direction this effect on the incoming waves is clearly shown in fig 7 it tended to change the incoming wave in the aerated water dashed line in plots so that it arrived 0 08 s later than that in the pure water solid line furthermore it slowed down the wave leading to an increase in the wave steepness the difference between the maximum wave crest elevations in pure and aerated waters at x 0 15 m was up to 0 01 m the disturbance of the wave crest due to the presence of a bubble curtain was studied by kimmoun et al 2012 who showed that the disturbance could lead to a small decrease in impact pressure on a vertical wall such as is found in the experiments presented here 3 3 wave impact pressure and its impulse the maximum pressures at various levels on the wall are presented in fig 8 for all wave impact types in pure and aerated water the vertical axis is the dimensionless level z d of the measured points on the wall and the swl is represented by z d 0 the horizontal axis is the logarithm of dimensionless impact pressure p max ρgd the black diamonds are the impact pressures in pure water and the red pluses are the impact pressures in the aerated water with 0 6 of void fraction the solid and dashed lines join mean values of data points at each level on the wall in pure and aerated water respectively maximum impact pressures were found to occur at elevations above the swl for impacts in 0 625 m water depth it is clearly seen from fig 8 that there is a significant reduction of the impact pressures from pure water to aerated water the maximum impact pressures measured in pure water were 0 27 bar 0 38 bar 0 56 bar and 0 2 bar for the broken high aeration flip through and slightly breaking wave impacts whereas those maximum impact pressures were only from 0 03 bar to 0 13 bar for wave impacts in aerated water the incident wave was also affected by the current and turbulence induced by the bubble generator which further reduces the impact pressures significant reduction of the impact pressure in aerated water was also found in the experimental work of kimmoun et al 2012 fig 9 shows pressure impulses on the vertical wall for pure and aerated water the impulse value is integrated over the duration of the measured signal cooker and peregrine 1995 the black diamond markers and solid line are the pressure impulse and its mean values at the measured levels in pure water respectively the red plus marker and dashed line are for the aerated water the results show that the pressure impulses in aerated water are a little smaller than those in pure water for all types of wave impacts broken high aeration flip through and slightly breaking pressure impulses decrease with increasing elevation on the wall for both pure and aerated water this is because the pressure oscillations were sustained for a long duration 3 4 wave impact force and its impulse fig 10 presents the impact forces and force impulses on the wall in pure and aerated water as seen for the pressure results impact forces in pure water are significantly higher than in aerated water fig 10a in both pure and aerated waters the flip through impact resulted in the highest impact force and the broken wave impact caused the lowest impact force on the hull section similar findings were reported by kimmoun et al 2012 for a vertical wall and bubble curtain arrangement in general the average values of the total force impulses in pure water are higher than those in aerated water but their variation is quite large and in some events the total force impulse in aerated water is larger than that in pure water fig 10b 4 conclusions this paper examined the role of aeration in wave impacts experimental work was conducted by means of wave impact on a truncated vertical rigid wall hull section in pure and aerated water wave impacts on a truncated vertical wall designed to represent a vertical section of an fpso hull were explored for various wave impact types including broken high aeration large air pocket flip through and slightly breaking wave impacts it is found that the high aeration and flip through wave impacts are the most severe impact types and they should be considered for offshore structure design post impact high frequency oscillations of pressures were observed for wave impacts and these oscillations are due to repeated compression and expansion of the trapped air between wave front and the truncated wall therefore the fatigue analysis of a section of the hull or whole ship structure may need to assess those local high frequency loading oscillations this study observed that aeration has an important effect on wave impacts a significant reduction in the hydrodynamic impact loadings pressure and force is found for those measured in aerated water compared to those in pure water though there is almost no reduction of the total loading impulses for wave impacts therefore maximum instantaneous loads calculated for pure water may be conservative in the presence of aerated water in the implication for offshore structure design currently standard design practices det norske veritas 2012 2016 have not taken into account aerated water in design the effects of aeration should be taken into consideration for the offshore environment where wave breaking is commonplace leading to aerated wave interactions with structures acknowledgements this study was a part of the froth fundamentals and reliability of offshore structure hydrodynamics project supported by the engineering and physical science research council epsrc grant ep j012866 1 the froth project was led by plymouth university and the collaborative partners included oxford university university of bath city university london and the manchester metropolitan university the authors gratefully acknowledge the financial support provided by epsrc and useful discussions with the project partners appendix a visualisation of wave impacts table a1 description of figures presented in appendix a table a1 figures description fig a 1 snapshots of broken wave impact in pure water fig a 2 snapshots of high aeration wave impact in pure water fig a 3 snapshots of flip through wave impact in pure water fig a 4 snapshots of slightly breaking wave impact in pure water fig a 5 snapshots of broken wave impact in aerated water fig a 6 snapshots of high aeration wave impact in aerated water fig a 7 snapshots of flip through wave impact in aerated water fig a 8 snapshots of slightly breaking wave impact in aerated water a 1visualisation of wave impacts in pure water fig a1 snapshots of broken wave impact in pure water in each snapshot the wave was coming from the left and the wall was located on the right fig a1 fig a2 snapshots of high aeration wave impact in pure water in each snapshot the wave was coming from the left and the wall was located on the right 2 fig a2 fig a3 snapshots of flip through wave impact in pure water in each snapshot the wave was coming from the left and the wall was located on the right 3 fig a3 fig a4 snapshots of slightly breaking wave impact in pure water in each snapshot the wave was coming from the left and the wall was located on the right 4 fig a4 visualisation of wave impacts in aerated water fig a5 snapshots of broken wave impact in aerated water in each snapshot the wave was coming from the left and the wall was located on the right 5 fig a5 fig a6 snapshots of high aeration wave impact in aerated water in each snapshot the wave was coming from the left and the wall was located on the right 6 fig a6 fig a7 snapshots of flip through wave impact in aerated water in each snapshot the wave was coming from the left and the wall was located on the right 7 fig a7 fig a8 snapshots of slightly breaking wave impact in aerated water in each snapshot the wave was coming from the left and the wall was located on the right 8 fig a8 appendix b repeatability of wave impacts table b1 description of figures presented in appendix b table b1 figures description fig b 1 repeatability of acceleration force and pressures on the wall under broken wave impact in pure water fig b 2 repeatability of acceleration force and pressures on the wall under high aeration wave impact in pure water fig b 3 repeatability of acceleration force and pressures on the wall under flip through wave impact in pure water fig b 4 repeatability of acceleration force and pressures the wall under slightly breaking wave impact in pure water fig b 5 repeatability of acceleration force and pressures on the wall under broken wave impact in aerated water fig b 6 repeatability of acceleration force and pressures on the wall under high aeration wave impact in aerated water fig b 7 repeatability of acceleration force and pressures on the wall under flip through wave impact in aerated water fig b 8 repeatability of acceleration force and pressures the wall under slightly breaking wave impact in aerated water b 1repeatability of wave impacts in pure water fig b1 repeatability of acceleration force and pressures on the wall under broken wave impact in pure water 9 fig b1 fig b2 repeatability of acceleration force and pressures on the wall under high aeration wave impact in pure water 0 fig b2 fig b3 repeatability of acceleration force and pressures on the wall under flip through wave impact in pure water 1 fig b3 fig b4 repeatability of acceleration force and pressures the wall under slightly breaking wave impact in pure water 2 fig b4 repeatability of wave impacts in aerated water fig b5repeatability of acceleration force and pressures on the wall under broken wave impact in aerated water 3 figure b6repeatability of acceleration force and pressures on the wall under high aeration wave impact in aerated water 4 fig b7 repeatability of acceleration force and pressures on the wall under flip through wave impact in aerated water 5 fig b7 fig b8 repeatability of acceleration force and pressures on the wall under slightly breaking wave impact in aerated water 6 fig b8 
