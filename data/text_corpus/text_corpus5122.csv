index,text
25610,optimization problems in hydrological modeling are frequently solved using local or global search strategies which either maximize exploitation or exploration thus the elevated performance of one strategy for one class of problems is often offset by poor performance for another class to overcome this issue we propose a hybrid strategy g clpso that combines the global search characteristics of the comprehensive learning particle swarm optimization clpso with the exploitation capability of the marquardt levenberg ml method and implement it into the hydrological model hydrus benchmarks involving optimizing non separable unimodal and multimodal functions demonstrate that g clpso outperforms clpso in terms of accuracy and convergence synthetic modeling scenarios involving the inverse estimation of soil hydraulic properties are used to compare the g clpso against the original hydrus ml solver the gradient based algorithm pest and the stochastic sce ua strategy results demonstrate the superior performance of the g clpso suggesting a potential use in other environmental problems keywords optimization model calibration hydrus environmental modeling 1 introduction due to the poor transferability of laboratory estimated parameters e g ritter et al 2003 hydrological models are generally calibrated against transient field measurements for predictive purposes the main goal is to inversely estimate model parameters by minimizing the deviations between model predictions and observations i e the objective function the same strategy can also be used for designing purposes e g irrigation system optimization roy et al 2019 these optimization problems are frequently dealt with using local or global search strategies which either maximize exploitation or exploration thus the elevated performance of one strategy for one class of problems is often offset by poor performance for another class a classic example is the hydrological model hydrus šimůnek et al 2016 one of the most widespread mechanistic models for analyzing water flow and transport processes in the vadose zone hydrus includes the marquardt levenberg ml algorithm levenberg 1944 marquardt 1963 to solve a nonlinear least squares minimization problem arising from the model calibration šimůnek and hopmans 2002 the ml is a gradient based local search method which has been used extensively in the past in soil physics and vadose zone hydrology e g kool and parker 1988 šimůnek et al 1998 šimůnek and van genuchten 1996 van dam et al 1994 due to its efficiency for well posed low dimensional unimodal inverse problems however calibration problems are often ill posed and non separable due to measurement errors and model inadequacy thus leading to response surfaces characterized by long banana shaped narrow valleys or multiple local optima e g abbaspour et al 2004 arora et al 2012 brunetti et al 2016 šimůnek and van genuchten 1996 in these conditions the ml algorithm becomes extremely sensitive to the initial starting point and can easily converge to a local minimum transtrum and sethna 2012 to deal with irregularities in the response surface in high dimensions the pest software white et al 2020 couples an ml solver with a subspace parameter dimensionality reduction based on the singular value decomposition the resulting optimization strategy generated significant computational gains and was successfully applied in combination with hydrus watson et al 2013 nevertheless its local search behavior can still lead to a modest performance in complex optimization tasks e g sedaghatdoost et al 2019 to overcome this limitation multiple studies have externally coupled hydrus with global optimization strategies such as the annealing simplex method pan and wu 1998 genetic algorithms ines and droogers 2002 ant colony optimization abbaspour et al 2001 shuffled complex methods duan et al 1993 vrugt et al 2003 and particle swarm optimization brunetti et al 2016 2018 among many others global optimization algorithms outperform local search methods for high dimensional complex problems due to their high exploration capability however according to the so called no free lunch theorem wolpert and macready 1997 for any algorithm any elevated performance over one class of problems is offset by poor performance over another class thus the high performance of global search strategies on complex multimodal problems is counterbalanced by their slow convergence on unimodal problems nevertheless since the shape of the response surface i e objective function is not known a priori when solving real world problems it is preferable to apply global optimization methods particle swarm optimizers psos are a promising tool due to their ease of implementation and good performance on moderately high dimensional inverse problems to improve their efficiency multiple studies have coupled them with local search methods e g cao et al 2019 han and liu 2015 noel 2012 zhao et al 2008 for instance in noel 2012 the steepest descent scheme is used at every iteration or periodically to survey the vicinity around the swarm s best solution exploitation to complement the pso s global exploration shi and eberhart 1998 however if the local search is performed too early the swarm can lose diversity and converge to a local optimum recently cao et al 2019 coupled the comprehensive learning particle swarm optimization clpso liang et al 2006 with both the nelder mead and broyden fletch goldfarb shannon algorithms and proposed to start the local search only when all particles in the clpso enter the global optimum basin by doing so both the convergence time and accuracy of the clpso improve however the limited interaction between the clpso and the local search during the optimization process suggests that there is still room for improvement especially for unimodal problems starting from this background the main goal of this study is to develop an optimization strategy that combines the exploitation capability of the ml algorithm with the exploration features of the clpso which was shown to outperform other algorithms for multimodal problems liang et al 2006 we aim to obtain an algorithm that is flexible enough to be applied to multiple optimization tasks without deteriorating its performance the developed approach is implemented in the hydrus source code as an inverse solver to extend its applicability to moderately high dimensional multimodal optimization problems the internal coupling overcomes the usability limitations of external coupling routines by making the approach readily available to all hydrus users the problem is addressed in the following way first we describe the ml and clpso algorithms next we explain their combination in the gradient based comprehensive learning particle swarm optimization g clpso and its implementation into the hydrus source code non separable and ill conditioned unimodal and multimodal test functions are then used to compare the original clpso and newly developed strategies and assess achieved improvements finally synthetic modeling scenarios involving the inverse estimation of a layered lysimeter s soil hydraulic properties are used to test the g clpso performance and assess its benefit against the original hydrus ml solver the gradient based pest strategy white et al 2020 and the stochastic search algorithm sce ua duan et al 1993 2 materials and methods 2 1 optimization strategy in this section we first provide a general description of the clpso and ml algorithms and then explain their combination in the g clpso as well as its implementation into the hydrus source code considered is the following optimization problem in a bounded parameter space of dimension d 1 minimize f x 1 x 2 x d subject to x i l x i x i u with i 1 2 d where x i l and x i u are the lower and upper bounds of the ith parameter respectively since the optimization strategy includes the ml algorithm the fitness f is the sum of squared residuals ssq 2 1 1 comprehensive learning particle swarm optimization the pso is a metaheuristic and gradient free global search strategy based on a social psychological metaphor involving individuals of a swarm that interact with each other in a social world to achieve an optimum state kennedy and eberhart 1995 shi and eberhart 1998 by making no assumptions on the problem being optimized it is especially suited for black box optimization problems often encountered in environmental sciences e g brunetti et al 2018 2016 chu and chang 2009 xi et al 2017 the swarm consists of s individuals or particles i e potential solutions each particle j is characterized by three d dimensional vectors and a scalar value these are its current position x j its best position x j b e s t its velocity v j and its best objective function f j b e s t the current position is a set of coordinates in the parameters space which is updated at each iteration using a specific learning strategy that involves an information exchange with some neighboring individuals or the whole swarm this communication allows the swarm to progressively locate better solutions and eventually converge to the global optimum intuitively the core of the pso is social interaction in the original pso the particle s position is updated by random averaging of the best position encountered by the particle so far and the best position found by the neighboring particles however this trade off between individual and social interests can reduce the swarm s diversity thus leading to premature convergence to a local optimum in complex multimodal problems the clpso liang et al 2006 overcomes this limitation by using a different learning strategy that preserves diversity and reduces swarm behavior the jth particle s movement in the ith dimension is described as follows 2 v j i ω v j i u i 0 1 c x f j i i b e s t x j i x j i x j i v j i where v j i and x j i are the velocity and position of the ith parameter of the jth particle respectively ω is an inertia weight that is reduced as the number of iterations grows to favor exploitation u i is a random number in the range 0 1 c is a learning parameter typically set to 1 4995 and x f j i i b e s t defines which particles x j b e s t the particle j should follow x f j i i b e s t can be the corresponding dimension of any particle s x j b e s t including its own and the decision depends on probability pc referred to as the learning probability which takes different values for different particles 3 p c j 0 05 0 45 exp 10 j 1 s 1 1 exp 10 1 for each dimension of the particle j a random number is generated if this number is lower than pc j the corresponding dimension learns from other particles based on a tournament selection procedure otherwise it will learn from its own x j b e s t the learning process is continuously monitored if a particle ceases to improve for a certain number of iterations i e refreshing gap m the exemplar from which the particle is learning is reassigned a thorough description of the selection procedure is reported in liang et al 2006 as problem dimensionality increases the particles tend to leave the search space and exhibit unwanted roaming behavior to address this issue both the particle s velocity and position should be corrected in the clpso the particle s velocity in each dimension is bounded by a maximum magnitude v max if v j i exceeds a positive constant value v max j then the velocity of that dimension is set to s i g n v j i v max j simultaneously the fitness value of a particle is calculated and its x j b e s t is updated only if its position is within the limits of the search range this guarantees that the particle will return in the search range in the following iterations the main advantage of the clpso learning strategy is that all particles can be potentially used to guide the search direction of other individuals and this learning process can be different for each dimension of the particle such reduced emphasis on the global best position preserves the swarm s diversity and enhances the algorithm s performance on complex multimodal problems liang et al 2006 however this high exploration capability delays the convergence for unimodal problems 2 1 2 marquardt levenberg algorithm the marquardt levenberg algorithm is a gradient based technique that combines the steepest descent and gauss newton methods levenberg 1944 marquardt 1963 to solve nonlinear least squares minimization problems in the classic steepest descent scheme the algorithm is started at a selected position in the search space and small steps are made against the direction of the gradient according to 4 x k 1 x k β c x k where x k 1 is the approximation of the local minimum at the iteration k 1 β is the step size and c x k is the gradient of the function evaluated at x k the gradient consists of first order derivatives which are calculated using the forward difference approximation this step can be relatively computationally intensive when the inverse problem s dimensionality is high and can lead to inaccuracies when the response surface is topologically complex large β will result in a faster convergence towards the local minimum yet oscillations will appear once the algorithm reaches the sub optimal regions as x k overshoots the minimum on the other hand a small step size will improve stability but reduce the algorithm s convergence rate the ml algorithm solves these issues by using the steepest descent method when the objective function is far from its minimum and the gauss newton method as the minimum is approached this switch is operated by the parameter λ which is set to a modest value at the beginning e g 0 02 and then reduced as the solution approaches the minimum the algorithm stops when a certain number of iterations n max is reached or no further reductions of the fitness value are obtained results are then used to calculate the correlation matrix between estimated parameters and their confidence intervals thus providing a statistical basis for uncertainty assessment the ml algorithm outperforms stochastic search algorithms for unimodal response surfaces characterized by a single well defined optimum or when the initial point lies near the global optimum in such circumstances the high exploration ability of stochastic search methods is redundant however when the problem s dimensionality is high and the response surface is noisy and topologically complex the ml is likely to converge early to a local optimum transtrum and sethna 2012 2 1 3 gradient based comprehensive learning particle swarm optimization the critical point in hybrid global local search strategies is to find a trade off between their conflicting explorative and exploitative interests one of the main advantages of the clpso over other psos is the use of selective pressure in the learning strategy sutton et al 2007 noted how the application of selective pressure to the optimization of a non separable function leads to significant improvements in differential evolution strategies by focusing the search this was further confirmed by stanovov et al 2019 who concluded that rank and tournament selection could significantly enhance the performance of differential evolution strategies by increasing their exploitation capabilities while not affecting exploration significantly however one inherent danger with high selective pressure is the rapid loss of diversity within the population thus it is reasonable to assume that the coupling with a local search algorithm will further increase selective pressure with detrimental effects for multimodal fitness landscapes in the gradient based comprehensive learning particle swarm optimization g clpso developed here we propose a loose coupling strategy between the clpso and ml the clpso is used first for n l iterations then one random individual rand is selected from the swarm and x r a n d is used as the starting point for the ml local search if the calculated fitness value f x r a n d is lower than the corresponding personal best f r a n d b e s t then x r a n d b e s t is replaced by the optimum found by the local search x r a n d n e w by doing so the new x r a n d b e s t can enter the clpso tournament selection procedure and improve the swarm without significantly reducing the diversity of the swarm the main advantage is the possibility to start the ml algorithm after every n l iterations at different points with practical implications for different types of problems multimodal problems this allows to progressively identify different local minima which will then enter and improve the tournament selection procedure without hampering the swarm s diversity unimodal problems the ml will calculate similar fitness values for different starting points thus driving the entire swarm towards the global optimum in a few iterations the algorithm stops when the total number of function evaluations exceeds a user defined threshold value n max f e s or the difference between the best and worst objective function values is below a user defined tolerance value tol the latter criterion has proven to be reliable for multiple problems zielinski and laur 2007 the pseudocode of the g clpso is shown in fig 1 2 2 algorithm testing this section describes the test functions and synthetic case studies used to assess the performance of the g clpso for both widely used benchmark functions and classic vadose zone inverse estimation problems in particular test functions are used only to highlight numerical improvements of the g clpso against the original clpso and investigate the influence of g clpso input parameters s and n l on the algorithm s performance conversely the synthetic scenarios serve to compare the g clpso the original hydrus inverse solver the gradient based pest optimization strategy white et al 2020 and the stochastic search algorithm sce ua duan et al 1993 for specific hydrological problems algorithms are evaluated in terms of accuracy which is the capability to approximate better the global optimum and convergence speed which is the number of function calls required to identify the optimum or meet the termination criteria the use of test functions and synthetic scenarios is an often used practice in hydrological modeling for testing newly developed algorithms for instance duan et al 1993 tested the shuffled complex evolution approach against six benchmark functions and one synthetic dataset obtained using the sixpar hydrological model in another study duan et al 1994 used a synthetic sequence of streamflow to test the sce ua optimization strategy multiple numerical experiments were used by vrugt and robinson 2007 and ter braak and vrugt 2008 to assess the performance of the amalgam and differential evolution monte carlo algorithms respectively the a priori knowledge of the global optimum and topological features of the response surface guarantees a fair assessment of the proposed optimization strategy 2 2 1 test functions as the g clpso is supposed to outperform the original clpso in unimodal problems while preserving its performance for multimodal functions we selected one unimodal and two multimodal functions from the list of benchmark problems used at the ieee congress on evolutionary computation liang et al 2014 to test the proposed strategy the choice of the test functions is not casual but intentionally directed towards the inclusion of mathematical functions whose landscapes closely resemble the response surfaces of real hydrological optimization problems all functions are non separable since model parameters often exhibit high interactions in hydrological inverse problems to this aim different rotation matrices m are assigned to each function which is also shifted by using a vector o the search range is 100 100 d for all problems which are tested in ten and thirty dimensions the maximum number of function evaluations n max f e s and the convergence tolerance tol are set to 10 000d and 0 001 respectively the properties and formulas of selected functions are listed in table 1 while their fitness landscapes are shown in fig 2 twenty five combinations of s and n l are generated for each of them the average final fitness and the total number of function evaluations for each test function are calculated by averaging the results of ten independent g clpso optimizations finally a color map is used to explain the sensitivity of the g clpso performance to changes in the population size and local search frequency the better combination is identified and then it is used in the following comparison between the g clpso and the clpso 2 2 2 implementation of g clpso into the hydrus code the main reason to implement the g clpso directly into the hydrus code is to extend the capabilities of the existing hydrus inverse solver and overcome the usability limitations of the external coupling routines furthermore the addition of a few new parameters will not require extensive changes to the hydrus graphical user interface the new g clpso algorithm has been implemented in a separate fortran file gclpso for to preserve the original hydrus code s structure as much as possible the file contains only four subroutines gclpso gradient tournament and fitness the first two names are self explanatory and the subroutines contain the instructions to run the gpso and ml algorithms as described above in sections 2 1 1 and 2 1 2 tournament includes the clpso learning strategy while fitness is a subroutine used to call the hydrus model and calculate ssq the main file is modified to load the g clpso parameters from an external input file i e fit in which are then passed to the gclpso subroutine at this stage of development an integer ioptimization loaded from an external input file is used to choose between the original hydrus inverse solver i e ioptimization 0 or the new g clpso algorithm i e ioptimization 1 modifications of the graphical user interface are currently ongoing to facilitate the input at each g clpso iteration k the number of hydrus calls the lowest ssq calculated thus far and its corresponding best position in the parameter space are saved in an external output file i e gclpso out 2 2 3 synthetic modeling scenarios and model settings we use three synthetic modeling scenarios focused on the inverse estimation of a layered lysimeter s soil hydraulic properties to test the performance of the g clpso in vadose zone related problems and assess its benefit against the original hydrus ml solver the gradient based pest optimization strategy white et al 2020 and the stochastic search algorithm sce ua duan et al 1993 the synthetic lysimeter is a widely used numerical experiment for the inverse estimation of soil hydraulic properties e g durner et al 2008 schelle et al 2012 a 150 cm deep monolithic lysimeter installed in gumpenstein austria has been used as a reference herndl et al 2011 the lysimeter consists of three soil layers classified from top to bottom as loam silt loam and silt the vegetation consists of grass with homogeneous roots distributed to a depth of 20 cm climatic data collected by a weather station are used to calculate daily potential evapotranspiration hargreaves 1994 the evapotranspiration demand is then partitioned into potential evaporation and transpiration fluxes using the leaf area index sutanto et al 2012 which is constant during the simulation period and equal to 2 0 precipitation is measured using a tipping bucket rain collector placed nearby data from january to september 2012 are used in the numerical simulations hydrus 1d simulates variably saturated water flow and root water uptake in the lysimeter by numerically solving the richards equation richards 1931 and the feddes macroscopic model feddes et al 1978 respectively the van genuchten mualem functions vgm van genuchten 1980 describe the unsaturated soil hydraulic properties of the three layers and their parameters are initially set according to carsel and parrish 1988 to generate synthetic data table 2 field volumetric water content measurements at a pressure head of 15 000 cm are used to adjust the residual water content θ r the domain is discretized into 100 finite elements an atmospheric boundary condition is applied at the soil surface while a seepage face boundary condition is set at the bottom warm up simulations covering a period of 3 months 1st jan 31st mar 2012 are performed to reduce the influence of the initial condition which is set constant and equal to a pressure head of 100 cm water flow from 1st apr to 30th sept 2012 is calculated to generate the synthetic data used in the following inverse modeling scenarios in particular simulated daily seepage outflow and volumetric water contents at three different depths i e z 10 45 130 cm are retrieved and combined in different modeling scenarios forward simulation results are perturbed with a normally distributed measurement error with zero mean and standard deviation σ we used σ θ 1 1 10 4 cm3cm 3 and σ q 0 01 cm for the water content and cumulative outflow respectively schelle et al 2012 synthetic perturbed data are then combined in three different modeling scenarios focused on the inverse estimation of the saturated water content and conductivity θ s and k s respectively and the vgm shape parameters α and n for the three soil layers i e a total of 12 soil hydraulic parameters scenarios m1 and m2 use separately synthetic seepage outflow and volumetric water contents at three different depths respectively while scenario m3 combines the two time series 2 2 4 algorithms settings the pest software white et al 2020 is based on a gauss levenberg marquardt algorithm similar to that implemented in hydrus which calculates the gradient through numerical differentiation the main differences are the treatment of the parameter λ and the pest use of regularization to restrict the parameter search to identifiable parameters either by adding additional constraints to the parameters tikhonov regularization or separating identifiable parameters from non identifiable parameters subspace regularization in the present study pest with subspace regularization based on singular value decomposition is used optimization control parameters are set according to the pest documentation pest 2021 the pest software suite also implements the global optimizers sce ua which uses the same instructions and control files therefore this implementation is applied in the present study termination criteria for all algorithms are reported in table 3 pest is connected to a python python software foundation 2013 batch file that externally executes hydrus and checks its convergence if the simulation is not convergent hydrus output files read by pest are replaced with existing similar files having all outputs of interest set to a very large value this will lead to a very high calculated ssq value for this simulation which can slow the calculations but avoids their interruption to conduct a fair comparison between multiple optimization strategies we compare them by averaging the results of ten independent optimizations e g duan et al 1994 raj shrestha and rode 2008 sorooshian et al 1993 in particular the local search methods i e ml and pest are started at ten different random points in the search space to resemble the initialization of the particles in the g clpso which are randomly spread in the parameters bounds similarly the stochastic sce ua is run with ten different seeds 3 results and discussion 3 1 g clpso sensitivity analysis fig 3 shows the mutual influence of the swarm size s and local search frequency n l on the normalized final ssq and fes function evaluations for each 10d test function results indicate that n l has a small effect on the final fitness value ssq for both the elliptic and rosenbrock functions while it influences more the outcomes for the rastrigin problem the combined effect of s and n l on the latter is nonlinear suggesting that specific parameter combinations can yield better results for multimodal problems however in general a more frequent local search 1 n l 10 leads to better results for both unimodal and multimodal problems independently from the population size this also holds for the total number of function evaluations fes though the behavior is more variegated among different functions a frequent local ml search significantly accelerates the convergence of the algorithm for the rosenbrock problem while its effect is more nuanced for the elliptic and rastrigin functions this is intuitive since frequently initializing the ml solver at different points in the curved narrow valley of the rosenbrock function improves the swarm s convergence towards the global optimum this behavior is less critical but still appreciable for the unimodal elliptic function characterized by a more regular fitness landscape these findings are exacerbated when the dimensionality 30d of the problem increases fig 4 on the one hand a more frequent local search is again enormously beneficial for both the elliptic and rosenbrock functions which are only minimally affected by the swarm size on the other hand the interaction between s and n l is nonlinear for the multimodal rastrigin problem the main findings of the sensitivity analysis are it is not possible to identify a single combination s n l that guarantees the best performance for all problems conversely the algorithm consistently exhibits good performance for multiple parameters combinations for all benchmark functions thus suggesting the overall robustness of the optimization approach the use of only two parameters confirms the flexibility of the proposed search strategy the algorithm shows limited sensitivity to the swarm size for low and moderately high dimensional unimodal and multimodal problems piotrowski et al 2020 compared the clpso with other psos and concluded that a swarm of 20 individuals is optimal for the clpso with a larger population particles movement in the clpso becomes too chaotic reducing the convergence to any optima we do not observe this problem in the g clpso this may be mainly related to the effects of the ml solver on the search behavior of the particles and the corresponding reduction in their roaming behavior however no general conclusions can be drawn since the maximum investigated swarm size was 40 the analysis suggests that performing the local search every 1 to 10 g clpso iterations is beneficial in terms of convergence and computational cost for low and moderately high dimensional unimodal and multimodal problems we argue that this is again related to the local search s magnifying effect on selective pressure in the tournament selection procedure of the clpso in the following comparison s and n l in the g clpso are set to 10 and 1 respectively this choice is a compromise based on the sensitivity analysis results and is intended to highlight the effect of a very frequent local search on the algorithm performance the population size in the clpso is set to 20 according to piotrowski et al 2020 who carried out an extensive analysis on the effect of the population size in multiple particle swarm optimization strategies 3 2 test functions comparison between g clpso and clpso the convergence characteristics of the g clpso black and clpso grey on the selected test functions in both 10 and 30 dimensions are shown in fig 5 which reports the fitness value vs the number of function evaluations for ten independent optimizations the mean and the standard deviation of the final fitness value for both competing algorithms are reported in table 4 results indicate that the g clpso significantly outperforms the clpso in terms of both accuracy and convergence speed for low and moderately high dimensional unimodal problems i e f 1 the local search quickly focuses the search of the swarm on the global optimum basin and reduces the roaming behavior of the clpso this also happens although to a lower extent for the multimodal rosenbrock problems where more function evaluations are needed to identify the global minimum nevertheless the accuracy gain of the g clpso against the clpso remains significant also for this function table 4 similar behavior is encountered for the multimodal rastrigin problem for which the g clpso identifies a better minimum interestingly the use of the local search does not hinder the exploration of the individuals in the early stage of the optimization while it focuses their search towards the end when the global optimum basin is likely to be identified this is a desirable property to avoid premature convergence in local minima finally table 4 confirms the better performance of the g clpso for all problems thus indicating good flexibility of the developed strategy over a wide range of potential applications 3 3 synthetic modeling scenarios comparison between g clpso ml pest and sce ua the convergence characteristics of the ml grey pest red sce ua blue and g clpso black for the three inverse modeling scenarios m1 m2 and m3 are shown in fig 6 which reports the fitness value vs the number of hydrus evaluations for ten independent optimizations the mean and standard deviation of the final fitness value for all competing algorithms are reported in table 4 on the first inspection it is evident that the g clpso outperforms other solvers in terms of accuracy and convergence speed in all scenarios the local search strategies ml and pest have similar behaviors and exhibit early convergence in local minima due to their sensitivity to the initialization point their accuracy is comparable table 4 with pest outperforming ml in scenario m2 conversely ml is generally less computationally expensive than pest and this can be related to the scaling of the parameter λ during the optimization process the global stochastic optimization strategy sce ua ranks second in terms of accuracy table 4 but exhibits slow convergence fig 6 multiple optimization runs were terminated because the algorithm exceeded the maximum number of allowed model executions i e 10 000 the algorithm can consistently identify the global optimum basin but then a significant number of model executions is required to achieve small accuracy gains this behavior was already observed by wöhling et al 2008 and it is common for global optimization methods which generally lack exploitation capabilities compared to ml and pest sce ua has a much higher computational cost hindering its applicability when the model execution time increases e g in reactive solute transport models on the other hand the g clpso always reaches the global optimum region in a reasonable amount of function evaluations in particular the number of hydrus model executions is on average only slightly higher than for ml and pest and significantly lower than for sce ua this is extremely important since the computational cost is generally a bottleneck in the calibration of environmental models especially when using global optimization strategies table 4 confirms the superior performance of the g clpso the convergence characteristics of which exhibit a recurrent pattern characterized by early exploration followed by an exploitation phase induced by the local search after this minor improvements are achieved suggesting that the optimal region has been reached and that this region is likely to be flat and similar to the rosenbrock fitness landscape fig 2 this shape of the objective function is frequent in inverse hydrological problems abbaspour et al 2004 šimůnek and van genuchten 1996 as fig 5 shows more iterations are needed to identify precisely the global minimum in these cases more stringent convergence criteria would have further reduced the spread of the final optimized value among different runs however this is not particularly important for practical applications since the modeler is more interested in knowing the fitness landscape s shape near the optimum than a single value characterizing the parameter s uncertainty this is automatically done in the g clpso which calculates the parameters confidence interval around the final global optimum 3 4 strengths and limitations we consider this study to be innovative in three main aspects performance the analysis demonstrates that the newly developed strategy outperforms both its parent algorithms the clpso and ml and the gradient based and stochastic search strategies pest and sce ua respectively in terms of accuracy and convergence speed in particular it preserves the ml solver s exploitation and robustness without sacrificing the global search capabilities of the clpso this allows the algorithm to be successfully used in moderately high dimensional unimodal and multimodal problems frequently encountered in vadose zone hydrology and environmental sciences computational cost the g clpso does not replace the well tested and robust ml solver but extends its capabilities without dramatically increasing the run time this is crucial for several applications but especially for multi dimensional problems and problems involving reactive solute transport for which high computational costs make global search strategies impractical ease of implementation and coupling the g clpso needs only a few additional parameters i e s and n l compared to the original hydrus local search solver this makes it easy to use and simplifies its implementation into the hydrus graphical user interface which makes the ml algorithm to be favored by hydrus users due to its internal coupling into the hydrus source code despite these major benefits there are still some aspects that must be further investigated the synthetic lysimeter experiment is limited and does not represent other inverse modeling scenarios frequently encountered in vadose zone hydrology therefore the algorithm should be further tested on synthetic and real case studies to identify potential issues and then resolve them the effect of the local search on selective pressure in the tournament procedure is empirically hypothesized but not mathematically explained more numerical tests are needed to fully understand the interaction between the local and global search particularly how the refreshing gap m of the clpso might interact with n l 4 conclusions this study s main aim was to extend the capabilities of the hydrus inverse solver to moderately high dimensional multimodal calibration problems by combining its exploitation capability with the global search characteristics of the pso to this aim we developed a new loosely coupled strategy the g clpso which couples the clpso and ml algorithms the g clpso has been internally coupled into the hydrus source code facilitating its use and implementation in the hydrus graphical user interface multiple numerical and synthetic tests confirmed that the hybrid strategy outperforms both the clpso and ml on unimodal and multimodal problems by appropriately balancing exploration and exploitation furthermore synthetic modeling scenarios on the inverse estimation of soil hydraulic properties showed g clpso outperforming the original hydrus ml solver the gradient based strategy pest and the stochastic global optimization method sce ua in terms of accuracy and convergence speed despite the promising results presented in this study more research is needed to test the developed optimization strategy further better understand its structure and improve its performance while this is generally accomplished by using a probabilistic framework the inclusion of an approximate appraisal of the parameters uncertainty in the numerical framework is desirable to better characterize the model predictive performance nevertheless this represents an important step toward efficiently embodying global search strategies in vadose zone model calibration finally we would like to emphasize that while we have developed the g clpso strategy for the hydrus model the algorithm can be easily adapted for other hydrological or environmental models declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments partial financial support from the austrian academy of sciences öaw is acknowledged for funding the project rechaut variability of groundwater recharge and its implication for sustainable land use in austria within the earth system sciences research programme a python implementation of the developed algorithm is available upon request from the corresponding author 
25610,optimization problems in hydrological modeling are frequently solved using local or global search strategies which either maximize exploitation or exploration thus the elevated performance of one strategy for one class of problems is often offset by poor performance for another class to overcome this issue we propose a hybrid strategy g clpso that combines the global search characteristics of the comprehensive learning particle swarm optimization clpso with the exploitation capability of the marquardt levenberg ml method and implement it into the hydrological model hydrus benchmarks involving optimizing non separable unimodal and multimodal functions demonstrate that g clpso outperforms clpso in terms of accuracy and convergence synthetic modeling scenarios involving the inverse estimation of soil hydraulic properties are used to compare the g clpso against the original hydrus ml solver the gradient based algorithm pest and the stochastic sce ua strategy results demonstrate the superior performance of the g clpso suggesting a potential use in other environmental problems keywords optimization model calibration hydrus environmental modeling 1 introduction due to the poor transferability of laboratory estimated parameters e g ritter et al 2003 hydrological models are generally calibrated against transient field measurements for predictive purposes the main goal is to inversely estimate model parameters by minimizing the deviations between model predictions and observations i e the objective function the same strategy can also be used for designing purposes e g irrigation system optimization roy et al 2019 these optimization problems are frequently dealt with using local or global search strategies which either maximize exploitation or exploration thus the elevated performance of one strategy for one class of problems is often offset by poor performance for another class a classic example is the hydrological model hydrus šimůnek et al 2016 one of the most widespread mechanistic models for analyzing water flow and transport processes in the vadose zone hydrus includes the marquardt levenberg ml algorithm levenberg 1944 marquardt 1963 to solve a nonlinear least squares minimization problem arising from the model calibration šimůnek and hopmans 2002 the ml is a gradient based local search method which has been used extensively in the past in soil physics and vadose zone hydrology e g kool and parker 1988 šimůnek et al 1998 šimůnek and van genuchten 1996 van dam et al 1994 due to its efficiency for well posed low dimensional unimodal inverse problems however calibration problems are often ill posed and non separable due to measurement errors and model inadequacy thus leading to response surfaces characterized by long banana shaped narrow valleys or multiple local optima e g abbaspour et al 2004 arora et al 2012 brunetti et al 2016 šimůnek and van genuchten 1996 in these conditions the ml algorithm becomes extremely sensitive to the initial starting point and can easily converge to a local minimum transtrum and sethna 2012 to deal with irregularities in the response surface in high dimensions the pest software white et al 2020 couples an ml solver with a subspace parameter dimensionality reduction based on the singular value decomposition the resulting optimization strategy generated significant computational gains and was successfully applied in combination with hydrus watson et al 2013 nevertheless its local search behavior can still lead to a modest performance in complex optimization tasks e g sedaghatdoost et al 2019 to overcome this limitation multiple studies have externally coupled hydrus with global optimization strategies such as the annealing simplex method pan and wu 1998 genetic algorithms ines and droogers 2002 ant colony optimization abbaspour et al 2001 shuffled complex methods duan et al 1993 vrugt et al 2003 and particle swarm optimization brunetti et al 2016 2018 among many others global optimization algorithms outperform local search methods for high dimensional complex problems due to their high exploration capability however according to the so called no free lunch theorem wolpert and macready 1997 for any algorithm any elevated performance over one class of problems is offset by poor performance over another class thus the high performance of global search strategies on complex multimodal problems is counterbalanced by their slow convergence on unimodal problems nevertheless since the shape of the response surface i e objective function is not known a priori when solving real world problems it is preferable to apply global optimization methods particle swarm optimizers psos are a promising tool due to their ease of implementation and good performance on moderately high dimensional inverse problems to improve their efficiency multiple studies have coupled them with local search methods e g cao et al 2019 han and liu 2015 noel 2012 zhao et al 2008 for instance in noel 2012 the steepest descent scheme is used at every iteration or periodically to survey the vicinity around the swarm s best solution exploitation to complement the pso s global exploration shi and eberhart 1998 however if the local search is performed too early the swarm can lose diversity and converge to a local optimum recently cao et al 2019 coupled the comprehensive learning particle swarm optimization clpso liang et al 2006 with both the nelder mead and broyden fletch goldfarb shannon algorithms and proposed to start the local search only when all particles in the clpso enter the global optimum basin by doing so both the convergence time and accuracy of the clpso improve however the limited interaction between the clpso and the local search during the optimization process suggests that there is still room for improvement especially for unimodal problems starting from this background the main goal of this study is to develop an optimization strategy that combines the exploitation capability of the ml algorithm with the exploration features of the clpso which was shown to outperform other algorithms for multimodal problems liang et al 2006 we aim to obtain an algorithm that is flexible enough to be applied to multiple optimization tasks without deteriorating its performance the developed approach is implemented in the hydrus source code as an inverse solver to extend its applicability to moderately high dimensional multimodal optimization problems the internal coupling overcomes the usability limitations of external coupling routines by making the approach readily available to all hydrus users the problem is addressed in the following way first we describe the ml and clpso algorithms next we explain their combination in the gradient based comprehensive learning particle swarm optimization g clpso and its implementation into the hydrus source code non separable and ill conditioned unimodal and multimodal test functions are then used to compare the original clpso and newly developed strategies and assess achieved improvements finally synthetic modeling scenarios involving the inverse estimation of a layered lysimeter s soil hydraulic properties are used to test the g clpso performance and assess its benefit against the original hydrus ml solver the gradient based pest strategy white et al 2020 and the stochastic search algorithm sce ua duan et al 1993 2 materials and methods 2 1 optimization strategy in this section we first provide a general description of the clpso and ml algorithms and then explain their combination in the g clpso as well as its implementation into the hydrus source code considered is the following optimization problem in a bounded parameter space of dimension d 1 minimize f x 1 x 2 x d subject to x i l x i x i u with i 1 2 d where x i l and x i u are the lower and upper bounds of the ith parameter respectively since the optimization strategy includes the ml algorithm the fitness f is the sum of squared residuals ssq 2 1 1 comprehensive learning particle swarm optimization the pso is a metaheuristic and gradient free global search strategy based on a social psychological metaphor involving individuals of a swarm that interact with each other in a social world to achieve an optimum state kennedy and eberhart 1995 shi and eberhart 1998 by making no assumptions on the problem being optimized it is especially suited for black box optimization problems often encountered in environmental sciences e g brunetti et al 2018 2016 chu and chang 2009 xi et al 2017 the swarm consists of s individuals or particles i e potential solutions each particle j is characterized by three d dimensional vectors and a scalar value these are its current position x j its best position x j b e s t its velocity v j and its best objective function f j b e s t the current position is a set of coordinates in the parameters space which is updated at each iteration using a specific learning strategy that involves an information exchange with some neighboring individuals or the whole swarm this communication allows the swarm to progressively locate better solutions and eventually converge to the global optimum intuitively the core of the pso is social interaction in the original pso the particle s position is updated by random averaging of the best position encountered by the particle so far and the best position found by the neighboring particles however this trade off between individual and social interests can reduce the swarm s diversity thus leading to premature convergence to a local optimum in complex multimodal problems the clpso liang et al 2006 overcomes this limitation by using a different learning strategy that preserves diversity and reduces swarm behavior the jth particle s movement in the ith dimension is described as follows 2 v j i ω v j i u i 0 1 c x f j i i b e s t x j i x j i x j i v j i where v j i and x j i are the velocity and position of the ith parameter of the jth particle respectively ω is an inertia weight that is reduced as the number of iterations grows to favor exploitation u i is a random number in the range 0 1 c is a learning parameter typically set to 1 4995 and x f j i i b e s t defines which particles x j b e s t the particle j should follow x f j i i b e s t can be the corresponding dimension of any particle s x j b e s t including its own and the decision depends on probability pc referred to as the learning probability which takes different values for different particles 3 p c j 0 05 0 45 exp 10 j 1 s 1 1 exp 10 1 for each dimension of the particle j a random number is generated if this number is lower than pc j the corresponding dimension learns from other particles based on a tournament selection procedure otherwise it will learn from its own x j b e s t the learning process is continuously monitored if a particle ceases to improve for a certain number of iterations i e refreshing gap m the exemplar from which the particle is learning is reassigned a thorough description of the selection procedure is reported in liang et al 2006 as problem dimensionality increases the particles tend to leave the search space and exhibit unwanted roaming behavior to address this issue both the particle s velocity and position should be corrected in the clpso the particle s velocity in each dimension is bounded by a maximum magnitude v max if v j i exceeds a positive constant value v max j then the velocity of that dimension is set to s i g n v j i v max j simultaneously the fitness value of a particle is calculated and its x j b e s t is updated only if its position is within the limits of the search range this guarantees that the particle will return in the search range in the following iterations the main advantage of the clpso learning strategy is that all particles can be potentially used to guide the search direction of other individuals and this learning process can be different for each dimension of the particle such reduced emphasis on the global best position preserves the swarm s diversity and enhances the algorithm s performance on complex multimodal problems liang et al 2006 however this high exploration capability delays the convergence for unimodal problems 2 1 2 marquardt levenberg algorithm the marquardt levenberg algorithm is a gradient based technique that combines the steepest descent and gauss newton methods levenberg 1944 marquardt 1963 to solve nonlinear least squares minimization problems in the classic steepest descent scheme the algorithm is started at a selected position in the search space and small steps are made against the direction of the gradient according to 4 x k 1 x k β c x k where x k 1 is the approximation of the local minimum at the iteration k 1 β is the step size and c x k is the gradient of the function evaluated at x k the gradient consists of first order derivatives which are calculated using the forward difference approximation this step can be relatively computationally intensive when the inverse problem s dimensionality is high and can lead to inaccuracies when the response surface is topologically complex large β will result in a faster convergence towards the local minimum yet oscillations will appear once the algorithm reaches the sub optimal regions as x k overshoots the minimum on the other hand a small step size will improve stability but reduce the algorithm s convergence rate the ml algorithm solves these issues by using the steepest descent method when the objective function is far from its minimum and the gauss newton method as the minimum is approached this switch is operated by the parameter λ which is set to a modest value at the beginning e g 0 02 and then reduced as the solution approaches the minimum the algorithm stops when a certain number of iterations n max is reached or no further reductions of the fitness value are obtained results are then used to calculate the correlation matrix between estimated parameters and their confidence intervals thus providing a statistical basis for uncertainty assessment the ml algorithm outperforms stochastic search algorithms for unimodal response surfaces characterized by a single well defined optimum or when the initial point lies near the global optimum in such circumstances the high exploration ability of stochastic search methods is redundant however when the problem s dimensionality is high and the response surface is noisy and topologically complex the ml is likely to converge early to a local optimum transtrum and sethna 2012 2 1 3 gradient based comprehensive learning particle swarm optimization the critical point in hybrid global local search strategies is to find a trade off between their conflicting explorative and exploitative interests one of the main advantages of the clpso over other psos is the use of selective pressure in the learning strategy sutton et al 2007 noted how the application of selective pressure to the optimization of a non separable function leads to significant improvements in differential evolution strategies by focusing the search this was further confirmed by stanovov et al 2019 who concluded that rank and tournament selection could significantly enhance the performance of differential evolution strategies by increasing their exploitation capabilities while not affecting exploration significantly however one inherent danger with high selective pressure is the rapid loss of diversity within the population thus it is reasonable to assume that the coupling with a local search algorithm will further increase selective pressure with detrimental effects for multimodal fitness landscapes in the gradient based comprehensive learning particle swarm optimization g clpso developed here we propose a loose coupling strategy between the clpso and ml the clpso is used first for n l iterations then one random individual rand is selected from the swarm and x r a n d is used as the starting point for the ml local search if the calculated fitness value f x r a n d is lower than the corresponding personal best f r a n d b e s t then x r a n d b e s t is replaced by the optimum found by the local search x r a n d n e w by doing so the new x r a n d b e s t can enter the clpso tournament selection procedure and improve the swarm without significantly reducing the diversity of the swarm the main advantage is the possibility to start the ml algorithm after every n l iterations at different points with practical implications for different types of problems multimodal problems this allows to progressively identify different local minima which will then enter and improve the tournament selection procedure without hampering the swarm s diversity unimodal problems the ml will calculate similar fitness values for different starting points thus driving the entire swarm towards the global optimum in a few iterations the algorithm stops when the total number of function evaluations exceeds a user defined threshold value n max f e s or the difference between the best and worst objective function values is below a user defined tolerance value tol the latter criterion has proven to be reliable for multiple problems zielinski and laur 2007 the pseudocode of the g clpso is shown in fig 1 2 2 algorithm testing this section describes the test functions and synthetic case studies used to assess the performance of the g clpso for both widely used benchmark functions and classic vadose zone inverse estimation problems in particular test functions are used only to highlight numerical improvements of the g clpso against the original clpso and investigate the influence of g clpso input parameters s and n l on the algorithm s performance conversely the synthetic scenarios serve to compare the g clpso the original hydrus inverse solver the gradient based pest optimization strategy white et al 2020 and the stochastic search algorithm sce ua duan et al 1993 for specific hydrological problems algorithms are evaluated in terms of accuracy which is the capability to approximate better the global optimum and convergence speed which is the number of function calls required to identify the optimum or meet the termination criteria the use of test functions and synthetic scenarios is an often used practice in hydrological modeling for testing newly developed algorithms for instance duan et al 1993 tested the shuffled complex evolution approach against six benchmark functions and one synthetic dataset obtained using the sixpar hydrological model in another study duan et al 1994 used a synthetic sequence of streamflow to test the sce ua optimization strategy multiple numerical experiments were used by vrugt and robinson 2007 and ter braak and vrugt 2008 to assess the performance of the amalgam and differential evolution monte carlo algorithms respectively the a priori knowledge of the global optimum and topological features of the response surface guarantees a fair assessment of the proposed optimization strategy 2 2 1 test functions as the g clpso is supposed to outperform the original clpso in unimodal problems while preserving its performance for multimodal functions we selected one unimodal and two multimodal functions from the list of benchmark problems used at the ieee congress on evolutionary computation liang et al 2014 to test the proposed strategy the choice of the test functions is not casual but intentionally directed towards the inclusion of mathematical functions whose landscapes closely resemble the response surfaces of real hydrological optimization problems all functions are non separable since model parameters often exhibit high interactions in hydrological inverse problems to this aim different rotation matrices m are assigned to each function which is also shifted by using a vector o the search range is 100 100 d for all problems which are tested in ten and thirty dimensions the maximum number of function evaluations n max f e s and the convergence tolerance tol are set to 10 000d and 0 001 respectively the properties and formulas of selected functions are listed in table 1 while their fitness landscapes are shown in fig 2 twenty five combinations of s and n l are generated for each of them the average final fitness and the total number of function evaluations for each test function are calculated by averaging the results of ten independent g clpso optimizations finally a color map is used to explain the sensitivity of the g clpso performance to changes in the population size and local search frequency the better combination is identified and then it is used in the following comparison between the g clpso and the clpso 2 2 2 implementation of g clpso into the hydrus code the main reason to implement the g clpso directly into the hydrus code is to extend the capabilities of the existing hydrus inverse solver and overcome the usability limitations of the external coupling routines furthermore the addition of a few new parameters will not require extensive changes to the hydrus graphical user interface the new g clpso algorithm has been implemented in a separate fortran file gclpso for to preserve the original hydrus code s structure as much as possible the file contains only four subroutines gclpso gradient tournament and fitness the first two names are self explanatory and the subroutines contain the instructions to run the gpso and ml algorithms as described above in sections 2 1 1 and 2 1 2 tournament includes the clpso learning strategy while fitness is a subroutine used to call the hydrus model and calculate ssq the main file is modified to load the g clpso parameters from an external input file i e fit in which are then passed to the gclpso subroutine at this stage of development an integer ioptimization loaded from an external input file is used to choose between the original hydrus inverse solver i e ioptimization 0 or the new g clpso algorithm i e ioptimization 1 modifications of the graphical user interface are currently ongoing to facilitate the input at each g clpso iteration k the number of hydrus calls the lowest ssq calculated thus far and its corresponding best position in the parameter space are saved in an external output file i e gclpso out 2 2 3 synthetic modeling scenarios and model settings we use three synthetic modeling scenarios focused on the inverse estimation of a layered lysimeter s soil hydraulic properties to test the performance of the g clpso in vadose zone related problems and assess its benefit against the original hydrus ml solver the gradient based pest optimization strategy white et al 2020 and the stochastic search algorithm sce ua duan et al 1993 the synthetic lysimeter is a widely used numerical experiment for the inverse estimation of soil hydraulic properties e g durner et al 2008 schelle et al 2012 a 150 cm deep monolithic lysimeter installed in gumpenstein austria has been used as a reference herndl et al 2011 the lysimeter consists of three soil layers classified from top to bottom as loam silt loam and silt the vegetation consists of grass with homogeneous roots distributed to a depth of 20 cm climatic data collected by a weather station are used to calculate daily potential evapotranspiration hargreaves 1994 the evapotranspiration demand is then partitioned into potential evaporation and transpiration fluxes using the leaf area index sutanto et al 2012 which is constant during the simulation period and equal to 2 0 precipitation is measured using a tipping bucket rain collector placed nearby data from january to september 2012 are used in the numerical simulations hydrus 1d simulates variably saturated water flow and root water uptake in the lysimeter by numerically solving the richards equation richards 1931 and the feddes macroscopic model feddes et al 1978 respectively the van genuchten mualem functions vgm van genuchten 1980 describe the unsaturated soil hydraulic properties of the three layers and their parameters are initially set according to carsel and parrish 1988 to generate synthetic data table 2 field volumetric water content measurements at a pressure head of 15 000 cm are used to adjust the residual water content θ r the domain is discretized into 100 finite elements an atmospheric boundary condition is applied at the soil surface while a seepage face boundary condition is set at the bottom warm up simulations covering a period of 3 months 1st jan 31st mar 2012 are performed to reduce the influence of the initial condition which is set constant and equal to a pressure head of 100 cm water flow from 1st apr to 30th sept 2012 is calculated to generate the synthetic data used in the following inverse modeling scenarios in particular simulated daily seepage outflow and volumetric water contents at three different depths i e z 10 45 130 cm are retrieved and combined in different modeling scenarios forward simulation results are perturbed with a normally distributed measurement error with zero mean and standard deviation σ we used σ θ 1 1 10 4 cm3cm 3 and σ q 0 01 cm for the water content and cumulative outflow respectively schelle et al 2012 synthetic perturbed data are then combined in three different modeling scenarios focused on the inverse estimation of the saturated water content and conductivity θ s and k s respectively and the vgm shape parameters α and n for the three soil layers i e a total of 12 soil hydraulic parameters scenarios m1 and m2 use separately synthetic seepage outflow and volumetric water contents at three different depths respectively while scenario m3 combines the two time series 2 2 4 algorithms settings the pest software white et al 2020 is based on a gauss levenberg marquardt algorithm similar to that implemented in hydrus which calculates the gradient through numerical differentiation the main differences are the treatment of the parameter λ and the pest use of regularization to restrict the parameter search to identifiable parameters either by adding additional constraints to the parameters tikhonov regularization or separating identifiable parameters from non identifiable parameters subspace regularization in the present study pest with subspace regularization based on singular value decomposition is used optimization control parameters are set according to the pest documentation pest 2021 the pest software suite also implements the global optimizers sce ua which uses the same instructions and control files therefore this implementation is applied in the present study termination criteria for all algorithms are reported in table 3 pest is connected to a python python software foundation 2013 batch file that externally executes hydrus and checks its convergence if the simulation is not convergent hydrus output files read by pest are replaced with existing similar files having all outputs of interest set to a very large value this will lead to a very high calculated ssq value for this simulation which can slow the calculations but avoids their interruption to conduct a fair comparison between multiple optimization strategies we compare them by averaging the results of ten independent optimizations e g duan et al 1994 raj shrestha and rode 2008 sorooshian et al 1993 in particular the local search methods i e ml and pest are started at ten different random points in the search space to resemble the initialization of the particles in the g clpso which are randomly spread in the parameters bounds similarly the stochastic sce ua is run with ten different seeds 3 results and discussion 3 1 g clpso sensitivity analysis fig 3 shows the mutual influence of the swarm size s and local search frequency n l on the normalized final ssq and fes function evaluations for each 10d test function results indicate that n l has a small effect on the final fitness value ssq for both the elliptic and rosenbrock functions while it influences more the outcomes for the rastrigin problem the combined effect of s and n l on the latter is nonlinear suggesting that specific parameter combinations can yield better results for multimodal problems however in general a more frequent local search 1 n l 10 leads to better results for both unimodal and multimodal problems independently from the population size this also holds for the total number of function evaluations fes though the behavior is more variegated among different functions a frequent local ml search significantly accelerates the convergence of the algorithm for the rosenbrock problem while its effect is more nuanced for the elliptic and rastrigin functions this is intuitive since frequently initializing the ml solver at different points in the curved narrow valley of the rosenbrock function improves the swarm s convergence towards the global optimum this behavior is less critical but still appreciable for the unimodal elliptic function characterized by a more regular fitness landscape these findings are exacerbated when the dimensionality 30d of the problem increases fig 4 on the one hand a more frequent local search is again enormously beneficial for both the elliptic and rosenbrock functions which are only minimally affected by the swarm size on the other hand the interaction between s and n l is nonlinear for the multimodal rastrigin problem the main findings of the sensitivity analysis are it is not possible to identify a single combination s n l that guarantees the best performance for all problems conversely the algorithm consistently exhibits good performance for multiple parameters combinations for all benchmark functions thus suggesting the overall robustness of the optimization approach the use of only two parameters confirms the flexibility of the proposed search strategy the algorithm shows limited sensitivity to the swarm size for low and moderately high dimensional unimodal and multimodal problems piotrowski et al 2020 compared the clpso with other psos and concluded that a swarm of 20 individuals is optimal for the clpso with a larger population particles movement in the clpso becomes too chaotic reducing the convergence to any optima we do not observe this problem in the g clpso this may be mainly related to the effects of the ml solver on the search behavior of the particles and the corresponding reduction in their roaming behavior however no general conclusions can be drawn since the maximum investigated swarm size was 40 the analysis suggests that performing the local search every 1 to 10 g clpso iterations is beneficial in terms of convergence and computational cost for low and moderately high dimensional unimodal and multimodal problems we argue that this is again related to the local search s magnifying effect on selective pressure in the tournament selection procedure of the clpso in the following comparison s and n l in the g clpso are set to 10 and 1 respectively this choice is a compromise based on the sensitivity analysis results and is intended to highlight the effect of a very frequent local search on the algorithm performance the population size in the clpso is set to 20 according to piotrowski et al 2020 who carried out an extensive analysis on the effect of the population size in multiple particle swarm optimization strategies 3 2 test functions comparison between g clpso and clpso the convergence characteristics of the g clpso black and clpso grey on the selected test functions in both 10 and 30 dimensions are shown in fig 5 which reports the fitness value vs the number of function evaluations for ten independent optimizations the mean and the standard deviation of the final fitness value for both competing algorithms are reported in table 4 results indicate that the g clpso significantly outperforms the clpso in terms of both accuracy and convergence speed for low and moderately high dimensional unimodal problems i e f 1 the local search quickly focuses the search of the swarm on the global optimum basin and reduces the roaming behavior of the clpso this also happens although to a lower extent for the multimodal rosenbrock problems where more function evaluations are needed to identify the global minimum nevertheless the accuracy gain of the g clpso against the clpso remains significant also for this function table 4 similar behavior is encountered for the multimodal rastrigin problem for which the g clpso identifies a better minimum interestingly the use of the local search does not hinder the exploration of the individuals in the early stage of the optimization while it focuses their search towards the end when the global optimum basin is likely to be identified this is a desirable property to avoid premature convergence in local minima finally table 4 confirms the better performance of the g clpso for all problems thus indicating good flexibility of the developed strategy over a wide range of potential applications 3 3 synthetic modeling scenarios comparison between g clpso ml pest and sce ua the convergence characteristics of the ml grey pest red sce ua blue and g clpso black for the three inverse modeling scenarios m1 m2 and m3 are shown in fig 6 which reports the fitness value vs the number of hydrus evaluations for ten independent optimizations the mean and standard deviation of the final fitness value for all competing algorithms are reported in table 4 on the first inspection it is evident that the g clpso outperforms other solvers in terms of accuracy and convergence speed in all scenarios the local search strategies ml and pest have similar behaviors and exhibit early convergence in local minima due to their sensitivity to the initialization point their accuracy is comparable table 4 with pest outperforming ml in scenario m2 conversely ml is generally less computationally expensive than pest and this can be related to the scaling of the parameter λ during the optimization process the global stochastic optimization strategy sce ua ranks second in terms of accuracy table 4 but exhibits slow convergence fig 6 multiple optimization runs were terminated because the algorithm exceeded the maximum number of allowed model executions i e 10 000 the algorithm can consistently identify the global optimum basin but then a significant number of model executions is required to achieve small accuracy gains this behavior was already observed by wöhling et al 2008 and it is common for global optimization methods which generally lack exploitation capabilities compared to ml and pest sce ua has a much higher computational cost hindering its applicability when the model execution time increases e g in reactive solute transport models on the other hand the g clpso always reaches the global optimum region in a reasonable amount of function evaluations in particular the number of hydrus model executions is on average only slightly higher than for ml and pest and significantly lower than for sce ua this is extremely important since the computational cost is generally a bottleneck in the calibration of environmental models especially when using global optimization strategies table 4 confirms the superior performance of the g clpso the convergence characteristics of which exhibit a recurrent pattern characterized by early exploration followed by an exploitation phase induced by the local search after this minor improvements are achieved suggesting that the optimal region has been reached and that this region is likely to be flat and similar to the rosenbrock fitness landscape fig 2 this shape of the objective function is frequent in inverse hydrological problems abbaspour et al 2004 šimůnek and van genuchten 1996 as fig 5 shows more iterations are needed to identify precisely the global minimum in these cases more stringent convergence criteria would have further reduced the spread of the final optimized value among different runs however this is not particularly important for practical applications since the modeler is more interested in knowing the fitness landscape s shape near the optimum than a single value characterizing the parameter s uncertainty this is automatically done in the g clpso which calculates the parameters confidence interval around the final global optimum 3 4 strengths and limitations we consider this study to be innovative in three main aspects performance the analysis demonstrates that the newly developed strategy outperforms both its parent algorithms the clpso and ml and the gradient based and stochastic search strategies pest and sce ua respectively in terms of accuracy and convergence speed in particular it preserves the ml solver s exploitation and robustness without sacrificing the global search capabilities of the clpso this allows the algorithm to be successfully used in moderately high dimensional unimodal and multimodal problems frequently encountered in vadose zone hydrology and environmental sciences computational cost the g clpso does not replace the well tested and robust ml solver but extends its capabilities without dramatically increasing the run time this is crucial for several applications but especially for multi dimensional problems and problems involving reactive solute transport for which high computational costs make global search strategies impractical ease of implementation and coupling the g clpso needs only a few additional parameters i e s and n l compared to the original hydrus local search solver this makes it easy to use and simplifies its implementation into the hydrus graphical user interface which makes the ml algorithm to be favored by hydrus users due to its internal coupling into the hydrus source code despite these major benefits there are still some aspects that must be further investigated the synthetic lysimeter experiment is limited and does not represent other inverse modeling scenarios frequently encountered in vadose zone hydrology therefore the algorithm should be further tested on synthetic and real case studies to identify potential issues and then resolve them the effect of the local search on selective pressure in the tournament procedure is empirically hypothesized but not mathematically explained more numerical tests are needed to fully understand the interaction between the local and global search particularly how the refreshing gap m of the clpso might interact with n l 4 conclusions this study s main aim was to extend the capabilities of the hydrus inverse solver to moderately high dimensional multimodal calibration problems by combining its exploitation capability with the global search characteristics of the pso to this aim we developed a new loosely coupled strategy the g clpso which couples the clpso and ml algorithms the g clpso has been internally coupled into the hydrus source code facilitating its use and implementation in the hydrus graphical user interface multiple numerical and synthetic tests confirmed that the hybrid strategy outperforms both the clpso and ml on unimodal and multimodal problems by appropriately balancing exploration and exploitation furthermore synthetic modeling scenarios on the inverse estimation of soil hydraulic properties showed g clpso outperforming the original hydrus ml solver the gradient based strategy pest and the stochastic global optimization method sce ua in terms of accuracy and convergence speed despite the promising results presented in this study more research is needed to test the developed optimization strategy further better understand its structure and improve its performance while this is generally accomplished by using a probabilistic framework the inclusion of an approximate appraisal of the parameters uncertainty in the numerical framework is desirable to better characterize the model predictive performance nevertheless this represents an important step toward efficiently embodying global search strategies in vadose zone model calibration finally we would like to emphasize that while we have developed the g clpso strategy for the hydrus model the algorithm can be easily adapted for other hydrological or environmental models declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments partial financial support from the austrian academy of sciences öaw is acknowledged for funding the project rechaut variability of groundwater recharge and its implication for sustainable land use in austria within the earth system sciences research programme a python implementation of the developed algorithm is available upon request from the corresponding author 
25611,time lapse gravimetry repeat microgravity measurement is a powerful tool for monitoring temporal mass distribution variations including seasonal and long term groundwater storage changes gwsc this geophysical method for measuring changes in gravity δg is potentially applicable to any groundwater system here i present gravi4gw a python tool for the site adapted calculation of β the conversion factor between δg and gwsc also known as topographic admittance alpine catchments in particular are ideal target sites as they are highly sensitive to climate variations and can experience significant gwsc while often lacking groundwater monitoring infrastructure therefore to illustrate the usage of gravi4gw i investigate a detailed example of an alpine catchment and examine spatial variations and the effects of depth assumptions this novel and accessible tool is designed to be useful in both the planning and data processing stages of time lapse gravimetric field studies keywords hydrogeophysics gravimetry time lapse gravimetry groundwater storage alpine hydrology topographic admittance 1 introduction change in groundwater storage is often the largest source of uncertainty in catchment scale hydrological models on the sub catchment scale spatial and temporal variations are difficult to constrain in the absence of direct piezometric measurements addressing these issues directly by drilling multiple piezometers or bores throughout a catchment involves significant financial costs and in the case of alpine and other remote fieldsites non negligible logistical challenges because of this there is significant interest in non invasive methods for measuring fluctuations in groundwater levels time lapse gravimetry is a promising geophysical method that is well suited to such investigations transport of matter such as water rock or hydrocarbons involves a change in mass distribution which in turn affects the gravitational force experienced at any given point in the domain which is theoretically infinite this small change in gravity g can be measured at the same point in space and at two or more points in time in a technique referred to as time lapse gravimetry repeat microgravity measurement or sometimes simply microgravimetry time lapse gravimetry measures δg at a given point over a given time interval this approach has been used to investigate the transport of hydrocarbons eiken et al 2008 abbasi et al 2016 reitz et al 2015 sediment and rock mass jongmans and garambois 2007 mouyen et al 2020 and magma bonforte et al 2017 carbone et al 2017 in hydrogeology the technique has been used to investigate transport of groundwater on the basin pool and eychaner 1995 and field scale creutzfeldt et al 2010 masson et al 2012 mcclymont et al 2012 arnoux et al 2020 furthermore the same fundamental principles underlying time lapse gravimetry have also been employed on a global scale in the gravity recovery and climate experiment grace grace fo tapley et al 2004 ramillien et al 2008 thomas et al 2014 logistical technical and financial challenges related to the installation of boreholes or piezometers are particularly salient in alpine catchments these mountainous regions act as water towers for much of humanity and thus a deep understanding of the hydrological state of alpine catchments is necessary for long term water management viviroli et al 2007 immerzeel et al 2020 voigt et al 2021 time lapse gravimetry therefore has the potential to be particularly impactful in hydrological investigations in alpine catchments the technique has already been employed to measure seasonal changes in gravity due to groundwater storage changes gwsc in unconfined superficial alpine aquifers in the canadian rocky mountains mcclymont et al 2012 and in the swiss alps arnoux et al 2020 both of these studies employed portable gravimeters to measure gravity at multiple locations at the beginning of the post snowmelt period and just before the onset of winter snow accumulation importantly during this period groundwater has been shown to be the major hydrological component ensuring baseflow in streams at lower elevations in alpine catchments clow et al 2003 glas et al 2018 hayashi 2020 in a talus moraine field alongside an alpine lake mcclymont et al 2012 measured predominantly negative δg values without a discernible spatial trend leading the authors to hypothesise that decrease in groundwater storage occurred in small pockets rather than continuously across the aquifer in a sloping talus field above superficial moraine deposits down gradient arnoux et al 2020 found a more pronounced decrease in g in the talus which supported a conceptual model involving greater gwsc in the higher permeability talus than in the lower elevation moraine in both of these studies the authors recognised that direct conversion of δg measurements to gwsc was not possible due to the non uniformity of the groundwater topography with arnoux et al 2020 suggesting that a numerical model would be required to provide accurate estimates of groundwater level changes the difficulty in obtaining quantitative estimates of gwsc from gravity measurements at the field scale has been recognised by several groups e g creutzfeldt et al 2008 jacob et al 2009 creutzfeldt et al 2010 noted explicitly that topography determines the hydrological mass distribution and directly influences the relationship between δg and gwsc the influence of topography on the groundwater table was investigated by haitjema and mitchell bruker 2005 and their results should be considered in gravimetric investigations of gwsc some authors have used the groundwater version of the bouguer plate approximation bpa which assumes a flat and infinite plane to convert between δg and gwsc e g pool and eychaner 1995 jacob et al 2009 while the bpa approach may be applicable in certain instances e g flat topography or deep confined water table the more its assumptions are violated the less applicable it becomes to improve upon the bpa assumption for the δg gwsc conversion creutzfeldt et al 2008 calculated the topography informed conversion factor at the wettzell geodetic observatory in germany and found it to differ by 24 from that of the bpa a subsequent study integrated continuous absolute gravity measurements into a hydrological model as a calibrating dataset creutzfeldt et al 2010 the capabilities of the time lapse gravimetry approach in monitoring changes in water storage were confirmed by masson et al 2012 who calculated topography informed conversion factors across the highly instrumented sub alpine strengbach catchment pierret et al 2018 of the vosges ne france in a non alpine context el diasty 2016 used repeat surveys to estimate yearly gwsc in a moraine aquifer in ontario canada very recently voigt et al 2021 presented a study of snowpack and karstic groundwater storage changes using a stationary superconducting gravimeter near the summit of the zugspitze 2962 m a s l on the german austrian border this study demonstrated the utility of gravimetry as a monitoring tool for both surface and subsurface water resources in alpine regions at the same site as masson et al 2012 and others chaffaut et al 2022 carried out a continuous superconducting gravimetry study they observed generally good agreement between measured δg values and the output of water balance models while promising modelling studies have been performed to facilitate conversion and comparison between δg and gwsc they have remained site specific and have often focused on highly instrumented catchments for both data processing and the planning of targeted time lapse gravimetry campaigns there is thus a need for a general tool to supply accurate δg gwsc conversion factors this need is heightened at sites with significant topographical relief and slope such as alpine areas where the divergence from the bpa will be greatest here i present a novel python software tool for the estimation of gwsc from time lapse gravimetry measurements this improves on the bpa and provides a more accurate translation between gravitational measurements and gwsc the utility of this study extends well beyond alpine hydrogeology and is pertinent for many unconfined aquifers it requires only limited input from the user and should be of interest in both the planning and data processing stages for any site with non planar topography such as that characteristic of alpine catchments where time lapse gravimetry is particularly advantageous 2 theory and technical considerations 2 1 gravity and groundwater here i use the classical definition of g the measurable quantity of gravity as the gravitational force f g per mass m experienced by an object of mass m the value of g varies between approximately 9 78 and 9 83 m s2 across the surface of the earth its value is affected by all matter and under classical physics can be formulated as 1 g g ρ s s 2 s d 3 s where g is the universal gravitational constant 6 674 10 11 m3 kg 1 s 2 ρ s the mass density at point s and s the unit vector between the integration point and the point at which g is calculated for groundwater time lapse gravimetry the underlying assumption is that gravity is affected only by local changes in mass distribution due to the movement of water or that any other mass distribution changes have been accounted for thus by measuring g at the same location at two or more points in time the change in mass distribution over the period is indirectly measured fig 1 following equation 1 the change in gravity at a given point due to a change in mass distribution δ ρ s can be defined as 2 δ g g δ ρ s s 2 s d 3 x i define β as the change in gravity as the water table decreases by a unit height i e the effective height of an equivalent free water column 3 β g h where β is written without the vector arrow i refer to the absolute value of β i e 4 β β correspondingly β z is defined as the vertical component of the gravity change and β r as the radial component 5 β z g h z 6 β r g h x 2 g h y 2 1 2 to describe the degree to which changes in gravity due to gwsc vary from vertical one can also define following the coordinate system definitions of fig 2 7 θ β arccos β z β arcsin β r β to illustrate the principle and to make first order estimates of error introduced by finite integral ranges i simplify calculations here to planar water table topography using the coordinate system as defined in fig 2 the change in gravity at point o due to an infinitesimal change in the water table height of δh gives 8 δ g δ h g s d m s 2 where s is a unit vector in the direction of integration point o i note that if due to symmetry changes in groundwater storage influence only the vertical component β z β and θ β 0 as the thickness of the plane is decreased 9 β g s z s 2 d m assuming uniform density ρ one obtains 10 β g ρ r cos θ s 2 d φ d r taking advantage of radial symmetry and integrating up to a radial distance of r 0 one obtains 11 β 2 π g ρ 0 r 0 r z r 2 z 2 3 2 d r 2 π g ρ 1 1 1 r 0 z 2 as has been remarked by several e g leirião et al 2009 arnoux et al 2020 for water of density 1000 kg m3 when r 0 this evaluates to β 4 193 10 7 s 2 or 41 93 μ g a l m h 2 o expressed otherwise under the infinite plane assumption a gwsc of 2 38 cm will lead to a change in vertical gravity of 1 μgal or 1 10 8 m s2 independent of porosity and water table depth this is the groundwater bouguer plate approximation bpa importantly this derivation enables estimation of the error in β imparted by evaluating to a finite radial distance r 0 as is necessary in a numerical implementation 12 ε 1 1 r 0 z 2 z r 0 where the approximation is valid for r 0 z this mirrors the remarks of leirião et al 2009 who described the gravitational footprint of gwsc as 10 times the depth to the water table beneath the measurement location which corresponds to 90 of the change in gravity this approximation is used in the gravi4gw software presented here to determine the numerical integration domain as a function of defined acceptable error subject to data availability this conversion factor β can be expressed in units of s 2 or μgal m as the calculation is made in terms of gravitational change per unit of equivalent free water column one can also write μ g a l m h 2 o to be explicit to convert to units of gravity per hydraulic head μgal m β should be divided by porosity n 2 2 gravimeters and gravimetric surveys gravimeters measure the amplitude of the gravitational field at their measurement location with precision in the μgal 10 8 m s2 range currently achievable both absolute and relative gravimeters exist absolute gravimeters such as the μquans absolute quantum gravimeter or micro g lacoste fg5 x generally require continuous ac power and precise calibration these are generally suited to stationary measurements at a fixed location where they measure near continuous temporal variations in gravity the micro g lacoste a10 is an absolute gravimeter which can be used for vehicle accessible field measurements measurements made with absolute gravimeters both stationary and portable will benefit from the approach detailed in this work if the target application is gwsc in unconfined aquifers or even though it is not the focus here water storage changes due to snowpack e g voigt et al 2021 relative gravimeters such as the scintrex cg 5 and cg 6 scintrex 2018 provide a relative measurement of gravity and are suited to field applications even in rugged terrain as they can be readily transported from one location to another by a single person on foot they require repeated measurements at a fixed location throughout each survey to enable drift correction as well as a reference measurement for true quantification of gwsc the reference measurement or measurements are made at an absolute gravity station in each survey for time lapse gravimetry the location of the gravimeter at repeat measurements must be well constrained in particular vertically through corrections for small elevation differences between surveys a 1 cm error in elevation equates to a 3 1 μgal error in gravity seibert and brady 2003 arnoux et al 2020 which is within the range of accuracy offered by portable gravimeters as centimetre level accuracy and precision is available in modern global navigation satellite system real time kinematic gnss rtk surveying systems they should thus be considered a necessity to ensure stable vertical position in any time lapse gravimetry campaign even when cemented platforms have been installed a final correction to gravimetry data is that related to earth tides these variations in gravity are generally 100 μgal in amplitude and their impact on groundwater pressures can be exploited to estimate hydro physical properties of aquifers cutillo and bredehoeft 2011 acworth et al 2016 rau et al 2018 for time lapse gravimetry earth tides must be accounted for over the course of a single survey as well as across repeat surveys some gravimeters implement the longman 1959 formulae and can perform internal corrections however the current state of the art is eterna wenzel 1996 kudryavtsev 2004 and its python implementation pygtide rau 2018 significant differences between these and the longman 1959 formulae have been noted arnoux et al 2020 thus due to the precision required in estimating gwsc it is recommended that these corrections are made in the most accurate and precise manner possible 3 software gravi4gw implements the calculation of β equation 4 based on user provided data this data includes a digital elevation model or water table model and locations of gravity stations the program can be supplied the coordinates of a single gravimetry station or a grid of multiple gravimetry stations which enables the creation of a map of β additional plotting facilities for the input data and for the numerical integral are also included fig 3 shows the basic process flow of the software the program can be executed with a single line of python code although some short preamble code is required to define the input data the software is written in the python language which is reputed for its clean readable syntax that should facilitate modifications by end users the dependencies of gravi4gw numpy harris et al 2020 matplotlib hunter 2007 scipy virtanen et al 2020 and gdal gdal ogr contributors 2020 are well maintained and extensively documented standard packages the software makes use of the gdal framework to read input files of geotiff format these files may contain data from digital elevation models dems or groundwater models a meter based coordinate reference system crs is assumed and thus data using other types of crs e g longitude latitude in degrees must be converted prior to use with gravi4gw the dem should extend a distance beyond all evaluation points of at least h eff ε equation 12 where h eff is an assumed effective water table depth to minimize error the dem resolution is recommended to be on the order of h eff or finer although for a relatively uniform surfaces a coarser dem will of course suffice further information regarding program inputs is contained in the software documentation several utility functions are integrated into the software the most important of these are pointmaker and dg additionally a simple plot of the input file and created x and y coordinate matrices is optionally outputted using the quickplotterxyz function the pointmaker function creates a mesh grid for numerical integration of equation 2 it is called once for each gravimetry station and operates using radial coordinates centred at the station before converting to the cartesian coordinate system of the input data inputs for the function include parameters controlling radial extent and radial and azimuthal point density fig 4 the radial coordinate point density follows a logarithmic distribution and the maximum radius is defined based on a user defined acceptable error criterion equation 12 the function also returns the representative area for each point the approach of pointmaker avoids bias from evaluation locations that do not coincide with dem pixel coordinates and given the r 2 dependence of gravitational force employs a computationally efficient radial point density the gravity integral is evaluated using the dg function which is called once for each integration mesh point created by pointmaker for each of these points the gravity vector 13 δ g δ g x x δ g y y δ g z z is evaluated in cartesian coordinates see fig 2 14a δ g x δ g cos θ sin φ 14b δ g y δ g cos θ cos φ 14c δ g z δ g sin θ where δg is calculated following equation 2 a sufficiently small value 1 cm is used for δh to evaluate the β vector equation 4 in terms of execution time i provide indicative times using python 3 7 6 in anaconda spyder 3 3 6 on a windows 10 laptop with 16 gb of ram and an i7 7600u 2 8 ghz processor using 2 m resolution input data and 1257 integration mesh points corresponding to a maximum integration radius of 500 m with 40 points radially gravi4gw takes 1 8 s to evaluate β at a single gravity station point without plotting options and 16 3 s to evaluate it at 100 gravimetry station points with plotting options efficiency is gained by exploiting a regular grid and thus using the rectbivariatespline class in scipy to estimate the elevations in the δg numerical integral additional details regarding the use of gravi4gw are available in the software repository readme file and in the program functions themselves 4 application 4 1 site description as an illustrative example i use 2 m resolution digital elevation data from the swiss federal office of topography swisstopo with an approximate centre of 7 53 e 46 19 n and of pixel dimensions 4670 2357 the dataset follows a regular grid and spans 2602 058 to 2611 396 east west and 1113 079 to 1117 791 north south in the swiss ch1903 lv95 crs it includes part of the vallon de réchy mari et al 2013 cochand et al 2019 and the entirety of the tsalet catchment arnoux et al 2020 cochand et al 2019 provide a thorough geological and hydrological description of the greater vallon de réchy in which the catchment is located the tsalet catchment is characterised by moraine and talus deposits and undergoes a seasonal cycle of groundwater storage arnoux et al 2020 this cycle wherein groundwater levels decrease over the period between the end of spring snowmelt and at the earliest the onset of winter snow accumulation is typical of alpine catchments cartwright et al 2020 hayashi 2020 arnoux et al 2021 the spatial and temporal variations in groundwater storage are of interest here primarily due to their importance in ensuring baseflow in streams down gradient 4 2 calculating β and its spatial variability in order to illustrate the usage of gravi4gw and to investigate spatial variance and the influence of assumed effective water table depth i generate maps of β equation 4 assuming depths to the effective water table of 2 m 5 m and 10 m vectors of spatial locations in the same crs as the input data in this case a dem are supplied to gravi4gw which then evaluates the δg integral numerically for each location fig 3 the numerical integral is calculated by generating a set of point area pairs i e an integration mesh i set an acceptable residual of 2 therefore requiring that the integration mesh be extended to 50 the assumed effective water table depth beneath each gravimetric station h eff as per equation 12 i also set the number of radial distances at which mesh points are to be created to 40 and use the default azimuthal density setting of 8 these settings result in integration points for h eff 2 m with the area represented by a mesh point ranging from 2 1 10 3 m2 for the points nearest to the gravimetry station location to 241 m2 for the farthest ones fig 4 these values depend on the input parameters for the pointmaker function and can be thus controlled by the user the density of points decreases radially while a minimum azimuthal point density is also maintained fig 4 this type of integration mesh point density is suitable due to the r 2 nature of gravitational force and provides an appropriate balance between numerical efficiency and accuracy for each of the h eff values and for each gravimeter station coordinate pair a set of mesh points is generated and β is evaluated in the same cartesian crs as the input ch1903 lv95 this enables the creation of a β or β z map fig 5 generally values of β are greatest in regions of convex topography mounds hills ridges etc and smallest in those of concave topographies dolines depressions channels etc this can be seen in fig 5 where the lowest values are observed in the beds of intermittent streams and the highest values are observed on mounds at the highly concave locations there is likely to be the greatest divergence between topographically informed β values and the true dependence of gravity on gwsc as the realistically attainable measurement uncertainty for time lapse gravimetry measurements using a portable gravimeter is likely to be in the 5 μgal range mcclymont et al 2012 arnoux et al 2020 it is thus advisable that one avoid areas of extreme concave curvature and instead target gravity measurement locations with moderately convex or planar topography spatial variations in β are most abrupt when h eff is relatively shallow this is due to the more pronounced effect of local curvature as the assumed depth to the water table increases the effective radius of influence or footprint leirião et al 2009 increases as illustrated for planar topography in equation 11 wherein 90 of the δg signal can be attributed to the area within a radial distance of 10 times the depth to the water table below the gravimetric measurement point thus with increasing h eff the influence of features in the immediate vicinity of the gravimetry station is weakened 4 3 dependence of β on groundwater table depth assumed effective groundwater depth h eff is a necessity in calculating β yet in practical applications it will not be known and its true value will not be constant indeed this presents a circuitous problem as the depth to the groundwater table and its change over time constitutes in part what one seeks to understand when using time lapse gravimetry additionally as the depth to the groundwater table changes β may also change in this way accurate conversion between δg and gwsc as well as quantification of uncertainty may require an understanding of the dependence of β on the assumed effective depth to the groundwater table when the changes in groundwater storage are significant it is therefore useful to investigate the effect of this assumed groundwater depth parameter on β furthermore the directionality of δg is not necessarily co directional with the gravity vector which defines the z direction in a geodetic crs this will generally be the case in any sloped terrain although the difference between β and β z will be constrained to 5 for slopes with gradients 18 33 grade gravi4gw calculates the β vector thus it is possible to investigate its the vertical and non vertical components while it is interesting to consider the non vertical components of β in practical time lapse gravimetry field surveys β z will be the quantity of interest i define a series of 21 evenly spaced points spanning 510 m horizontally roughly aligned nw se fig 6 a this transect covers a central part of the catchment and sits between two intermittent stream channels at each of these points i evaluate β at h eff of 2 30 m and additionally calculate θ β equation 7 the angle between β and the vertical vector z fig 6 the highest variability in β both across the points and as a function of h eff occurs for small values of h eff i e when the assumed effective depth to the groundwater table is shallow this is due to the greater influence of small scale local topography at low h eff values as h eff increases so does the extent of the gwsc footprint affecting β a most extreme example of this is the sixth lowest point with the lowest β value at h eff 2 m fig 6b in contrast to the neighbouring points this location lies at the bottom of a local depression in the direction perpendicular to the transect as expected this local concave topography causes the value of β to be less than that stemming from the groundwater bouguer plate approximation 41 93 μ g a l m h 2 o however as h eff increases this effect is dampened as the local depression represents an increasingly smaller proportion of the region of influence the vertical component and consequently the direction of the δ g vector also change as a function of h eff fig 6c and d the lower half of this transect has a steeper gradient than the upper half fig 6a inset along the transect the lowest six points have local gradients of between 14 4 and 20 5 while the next four points have slopes between 24 9 and 29 8 above this the slope along the direction of the transect is between 3 6 and 12 4 one observes that θ β has the greatest variance for small h eff as h eff increases the θ β values of the upper region where the topography is more uniform approach values similar to the local topographical slope in the hypothetical case of a uniform sloping plane one would expect the orientation of the δ g vector to be perpendicular to the gradient however as topography is non planar θ β does not tend exactly towards the local slope of the terrain 4 4 using β with time lapse gravimetric data relative time lapse gravimetry measurements were carried out in the tsalet catchment in july and october 2019 arnoux et al 2020 these dates correspond to the end of the spring summer snow melt period and just prior to the onset of autumn snowfall and thus were expected to correspond to a period of significant decrease in groundwater storage typical of small alpine catchments with complex topography cowie et al 2017 cochand et al 2019 hayashi 2020 there are no wells or piezometers in the catchment however the drying of intermittent streams indicated negative gwsc arnoux et al 2020 observed greater decreases in gravity in the upper talus dominated portion of the surveyed area the authors stated that the results could only be used to look at relative differences across different parts of the catchment due in major part to the likely variability of β from one gravimeter station to another here i use gravi4gw to make δg to gwsc conversions using β z fig 7 additionally i test the influence of assumed effective depth to the water table whose precise value is unknown calculated with h eff 5 m the value of β z at all but two of the thirteen time lapse gravimetry stations is less than that of the groundwater bpa and significant variability between locations exists notably for most stations uncertainty in h eff has less of an effect on the calculated gwsc values than does the choice to use the more advanced method of gravi4gw in place of the bpa at station g1 for example gravi4gw determined gwsc values vary between 2 06 and 2 09 m for the range of h eff explored whereas the bpa determined value is 1 56 m these calculations combined with typical talus and moraine porosity ranges agree with the observed drying of intermittent groundwater fed streams over the snow free period arnoux et al 2020 while the converted gwsc do not change the overall conclusions of greater gwsc in the upper talus area of the catchment compared to those in the lower moraine region they do provide a means for making more quantitative interpretations 5 discussion time lapse gravimetry is an established and powerful tool for the indirect measurement of changes in mass distribution over time however more widespread and quantitative use in groundwater studies has been held back in part by the lack of straightforward tools for conversion between δg and gwsc gravi4gw seeks to fill this void while gravimetry is not a direct substitute for piezometric measurements it does provide valuable information where drilling bores or piezometers is not feasible due to logistical or financial constraints the tsalet catchment presented as our example application site typifies an instance where time lapse gravimetry can offer significant quantitative insight into hydrogeological processes when using the outlined topographically informed conversion factors between δg measurements and gwsc it is useful to consider the meaning of β as compared to β z on sloping terrain in particular this consideration is important as the values of these two may differ significantly this occurs due to the directionality of the change in gravity imparted by the increase or decrease in local groundwater storage which may be non vertical the precision that would be required to directly measure the directionality of this vector would be excessively fine for example a δg value of 100 μgal with a θ β value of 20 would only impart a change in the direction of g of 2 10 6 this means that the off vertical components in δg due to gwsc are not measurable what is actually measured in time lapse gravimetry is the vertical component of δg or δg z and therefore the quantity calculated by gravi4gw that is of greatest interest for conversion between δg and gwsc is β z equation 5 defining a maximum radius for the δg integral numerical calculation is necessary this is done via the acceptable error criterion equation 12 in gravi4gw due to the r 2 dependence of the gravitational force this radius need not be prohibitively large but its finite value will nonetheless impart some error the software does not calculate the effects on gravity beyond the user defined radius and does not modify the β value after its calculation some users may nonetheless wish to normalise the resultant β values by a factor of 1 1 ε which implicitly assumes that integrating to infinity would result in an increase in β this however may not be strictly true if an increase in the integration radius results in the integration of mass at a higher average elevation than the gravimetry station thus decreasing β albeit slightly it is thus suggested that the integration radius as defined through equation 12 not extend to zones where groundwater response is likely to be weaker than that of the unconfined aquifer of interest to go beyond this level of precision the input data to gravi4gw would need to be informed by a groundwater model of some sort time lapse gravimetry is particularly useful for the investigation of unconfined shallow aquifers unconfined aquifers experience much greater changes in water content due to the piezometric surface being equivalent to the water level in the aquifer they are also more likely than confined aquifers to experience significant seasonal changes in groundwater storage due to their direct connection with the earth s surface one could potentially also monitor deep confined aquifers with gravimetric methods while the bpa may be applicable in some cases the possible deformation of a confined aquifer due to gwsc acworth et al 2017 the generally larger gravity footprint due to confined aquifers generally being situated deeper than unconfined ones and influence on g from shallower unconfined aquifers may complicate interpretation as shown where the water table is shallow the effect of local variations in gwsc on gravity is pronounced as the groundwater gravitational footprint is smaller leirião et al 2009 thus the spatial resolution possible through the use of time lapse gravimetry is greatest for shallow unconfined systems incidentally this concept can also be applied to snowpack whose gravitational footprint will theoretically be smaller due to it being located at the surface voigt et al 2021 nonetheless calculated a snowpack gravity footprint with significant azimuthal variability extending up to 4 km using a continuous superconducting gravimeter at a highly instrumented alpine site this calculation used an ε value see equation 12 of 0 1 which would imply a footprint on the order of 40 m under the 10 criteria of leirião et al 2009 another consideration that those combining geodesy with hydrogeology may need to make in clay rich soils in particular is the vadose zone vz if large variations in vz water content are expected then this may affect h eff and thus β in relatively well drained soils or shallow water tables vz effects are likely to be insignificant compared to other sources of uncertainty the piezometric surface that defines the water table will not generally be known a priori in a study that uses time lapse gravimetry the present results show that despite this uncertainty in h eff there is much to be gained from the use of the approach implemented in gravi4gw especially so when the alternative is the constant bpa without direct knowledge of the water table geometry it is impossible to know exactly what the true value of β z is although it is clear that β z will have less spatial variability for deeper water tables in the absence of other information about the water table gravi4gw provides the user with a range of β z values and hence a range of gwsc as expressed in meters of water equivalent this is ultimately the quantity of interest in time lapse gravimetry groundwater studies the linear topographical influence on water table elevation that is calculated when a dem is provided as input data can be viewed as a first order approximation with the true topology of the surface possibly exhibiting less curvature than the land surface thus digital elevation data of lower resolution than that used as an example in this work 2 m resolution may also provide consistent results due to the smoothing effect it follows from the earlier discussion of the effective spatial resolution of time lapse gravimetry that the influence of local surface curvature on β will decrease as the depth to the water table increases for a greater order of accuracy in calculating β a groundwater model e g beven and kirkby 1979 thompson and moore 1996 brunner and simmons 2012 into which additional knowledge of local hydrogeological conditions could be integrated could be used in place of a dem finally while i have focused primarily on the evaluation of β to estimate gwsc using time lapse gravimetric measurements gravi4gw can also be used in the geophysical fieldwork planning phase maps of β such as those of fig 5 can serve to guide researchers towards locations where gravimetric measurements are likely to provide the greatest value locations with higher values of β specifically β z should be targeted as they are likely to yield the greatest and thus most readily measurable δg values 6 conclusions gravi4gw provides a flexible tool for the calculation of groundwater storage changes based on observed changes in gravity integration of complimentary data into it will likely be of interest to certain users depending on the nature of their study and zone of interest i have provided an example of an alpine catchment with limited hydrological monitoring in place and shown how groundwater depth and topographical curvature and slope can effect β z for highly instrumented laboratory catchments gravi4gw may provide an additional tool that supports the conclusions of other hydrological measurements and water balance models for the vast majority of catchments which do not have a high density of instrumentation the tool provides a straightforward means for estimating gwsc more accurately than possible the bpa method i envision the software being used in both the preparation and the processing phases of time lapse gravimetry field campaigns for preparation dem or groundwater model informed β maps created by gravi4gw can be used to target zones where gwsc will have the greatest gravimetric signal in the processing phase gravi4gw enables conversion between measured δg and gwsc as well as an analysis of uncertainty due to the unknown depth to the groundwater table additionally operators of continuous monitoring superconducting gravimeters e g voigt et al 2021 chaffaut et al 2022 may find utility in gravi4gw in assessing changes in topographically influenced snowpack and groundwater storage as well as exploring direction dependent sensitivity or admittance the tool can be executed in a single line of code by even novice python users due to its utility and simplicity of use it is expected that gravi4gw will assist hydrogeologists geophysicists and environmental scientists in using gravimetry to make quantitative assessments and to arrive at better informed conclusions when investigating changes in groundwater storage software availability gravi4gw is developed by landon halloran www ljsh ca and is available at www github com lhalloran gravi4gw it is distributed under the gnu general public license v3 0 gravi4gw is written in python 3 7 and requires standard packages numpy harris et al 2020 matplotlib hunter 2007 scipy virtanen et al 2020 and osgeo gdal gdal ogr contributors 2020 the core gravi4gw py program is 20 kb in size while the package with included example scripts input data and output is 87 mb it has been built and tested in the spyder 3 3 6 ide www spyder ide org as included in the open source python distribution platform anaconda individual edition www anaconda com declaration of competing interest the author declares that he has no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements most figures were made using matplotlib hunter 2007 hillshading code was adapted from github user roger veciana i rovira rveciana i thank the editors and the three anonymous reviewers whose encouraging feedback assisted me in refining the manuscript 
25611,time lapse gravimetry repeat microgravity measurement is a powerful tool for monitoring temporal mass distribution variations including seasonal and long term groundwater storage changes gwsc this geophysical method for measuring changes in gravity δg is potentially applicable to any groundwater system here i present gravi4gw a python tool for the site adapted calculation of β the conversion factor between δg and gwsc also known as topographic admittance alpine catchments in particular are ideal target sites as they are highly sensitive to climate variations and can experience significant gwsc while often lacking groundwater monitoring infrastructure therefore to illustrate the usage of gravi4gw i investigate a detailed example of an alpine catchment and examine spatial variations and the effects of depth assumptions this novel and accessible tool is designed to be useful in both the planning and data processing stages of time lapse gravimetric field studies keywords hydrogeophysics gravimetry time lapse gravimetry groundwater storage alpine hydrology topographic admittance 1 introduction change in groundwater storage is often the largest source of uncertainty in catchment scale hydrological models on the sub catchment scale spatial and temporal variations are difficult to constrain in the absence of direct piezometric measurements addressing these issues directly by drilling multiple piezometers or bores throughout a catchment involves significant financial costs and in the case of alpine and other remote fieldsites non negligible logistical challenges because of this there is significant interest in non invasive methods for measuring fluctuations in groundwater levels time lapse gravimetry is a promising geophysical method that is well suited to such investigations transport of matter such as water rock or hydrocarbons involves a change in mass distribution which in turn affects the gravitational force experienced at any given point in the domain which is theoretically infinite this small change in gravity g can be measured at the same point in space and at two or more points in time in a technique referred to as time lapse gravimetry repeat microgravity measurement or sometimes simply microgravimetry time lapse gravimetry measures δg at a given point over a given time interval this approach has been used to investigate the transport of hydrocarbons eiken et al 2008 abbasi et al 2016 reitz et al 2015 sediment and rock mass jongmans and garambois 2007 mouyen et al 2020 and magma bonforte et al 2017 carbone et al 2017 in hydrogeology the technique has been used to investigate transport of groundwater on the basin pool and eychaner 1995 and field scale creutzfeldt et al 2010 masson et al 2012 mcclymont et al 2012 arnoux et al 2020 furthermore the same fundamental principles underlying time lapse gravimetry have also been employed on a global scale in the gravity recovery and climate experiment grace grace fo tapley et al 2004 ramillien et al 2008 thomas et al 2014 logistical technical and financial challenges related to the installation of boreholes or piezometers are particularly salient in alpine catchments these mountainous regions act as water towers for much of humanity and thus a deep understanding of the hydrological state of alpine catchments is necessary for long term water management viviroli et al 2007 immerzeel et al 2020 voigt et al 2021 time lapse gravimetry therefore has the potential to be particularly impactful in hydrological investigations in alpine catchments the technique has already been employed to measure seasonal changes in gravity due to groundwater storage changes gwsc in unconfined superficial alpine aquifers in the canadian rocky mountains mcclymont et al 2012 and in the swiss alps arnoux et al 2020 both of these studies employed portable gravimeters to measure gravity at multiple locations at the beginning of the post snowmelt period and just before the onset of winter snow accumulation importantly during this period groundwater has been shown to be the major hydrological component ensuring baseflow in streams at lower elevations in alpine catchments clow et al 2003 glas et al 2018 hayashi 2020 in a talus moraine field alongside an alpine lake mcclymont et al 2012 measured predominantly negative δg values without a discernible spatial trend leading the authors to hypothesise that decrease in groundwater storage occurred in small pockets rather than continuously across the aquifer in a sloping talus field above superficial moraine deposits down gradient arnoux et al 2020 found a more pronounced decrease in g in the talus which supported a conceptual model involving greater gwsc in the higher permeability talus than in the lower elevation moraine in both of these studies the authors recognised that direct conversion of δg measurements to gwsc was not possible due to the non uniformity of the groundwater topography with arnoux et al 2020 suggesting that a numerical model would be required to provide accurate estimates of groundwater level changes the difficulty in obtaining quantitative estimates of gwsc from gravity measurements at the field scale has been recognised by several groups e g creutzfeldt et al 2008 jacob et al 2009 creutzfeldt et al 2010 noted explicitly that topography determines the hydrological mass distribution and directly influences the relationship between δg and gwsc the influence of topography on the groundwater table was investigated by haitjema and mitchell bruker 2005 and their results should be considered in gravimetric investigations of gwsc some authors have used the groundwater version of the bouguer plate approximation bpa which assumes a flat and infinite plane to convert between δg and gwsc e g pool and eychaner 1995 jacob et al 2009 while the bpa approach may be applicable in certain instances e g flat topography or deep confined water table the more its assumptions are violated the less applicable it becomes to improve upon the bpa assumption for the δg gwsc conversion creutzfeldt et al 2008 calculated the topography informed conversion factor at the wettzell geodetic observatory in germany and found it to differ by 24 from that of the bpa a subsequent study integrated continuous absolute gravity measurements into a hydrological model as a calibrating dataset creutzfeldt et al 2010 the capabilities of the time lapse gravimetry approach in monitoring changes in water storage were confirmed by masson et al 2012 who calculated topography informed conversion factors across the highly instrumented sub alpine strengbach catchment pierret et al 2018 of the vosges ne france in a non alpine context el diasty 2016 used repeat surveys to estimate yearly gwsc in a moraine aquifer in ontario canada very recently voigt et al 2021 presented a study of snowpack and karstic groundwater storage changes using a stationary superconducting gravimeter near the summit of the zugspitze 2962 m a s l on the german austrian border this study demonstrated the utility of gravimetry as a monitoring tool for both surface and subsurface water resources in alpine regions at the same site as masson et al 2012 and others chaffaut et al 2022 carried out a continuous superconducting gravimetry study they observed generally good agreement between measured δg values and the output of water balance models while promising modelling studies have been performed to facilitate conversion and comparison between δg and gwsc they have remained site specific and have often focused on highly instrumented catchments for both data processing and the planning of targeted time lapse gravimetry campaigns there is thus a need for a general tool to supply accurate δg gwsc conversion factors this need is heightened at sites with significant topographical relief and slope such as alpine areas where the divergence from the bpa will be greatest here i present a novel python software tool for the estimation of gwsc from time lapse gravimetry measurements this improves on the bpa and provides a more accurate translation between gravitational measurements and gwsc the utility of this study extends well beyond alpine hydrogeology and is pertinent for many unconfined aquifers it requires only limited input from the user and should be of interest in both the planning and data processing stages for any site with non planar topography such as that characteristic of alpine catchments where time lapse gravimetry is particularly advantageous 2 theory and technical considerations 2 1 gravity and groundwater here i use the classical definition of g the measurable quantity of gravity as the gravitational force f g per mass m experienced by an object of mass m the value of g varies between approximately 9 78 and 9 83 m s2 across the surface of the earth its value is affected by all matter and under classical physics can be formulated as 1 g g ρ s s 2 s d 3 s where g is the universal gravitational constant 6 674 10 11 m3 kg 1 s 2 ρ s the mass density at point s and s the unit vector between the integration point and the point at which g is calculated for groundwater time lapse gravimetry the underlying assumption is that gravity is affected only by local changes in mass distribution due to the movement of water or that any other mass distribution changes have been accounted for thus by measuring g at the same location at two or more points in time the change in mass distribution over the period is indirectly measured fig 1 following equation 1 the change in gravity at a given point due to a change in mass distribution δ ρ s can be defined as 2 δ g g δ ρ s s 2 s d 3 x i define β as the change in gravity as the water table decreases by a unit height i e the effective height of an equivalent free water column 3 β g h where β is written without the vector arrow i refer to the absolute value of β i e 4 β β correspondingly β z is defined as the vertical component of the gravity change and β r as the radial component 5 β z g h z 6 β r g h x 2 g h y 2 1 2 to describe the degree to which changes in gravity due to gwsc vary from vertical one can also define following the coordinate system definitions of fig 2 7 θ β arccos β z β arcsin β r β to illustrate the principle and to make first order estimates of error introduced by finite integral ranges i simplify calculations here to planar water table topography using the coordinate system as defined in fig 2 the change in gravity at point o due to an infinitesimal change in the water table height of δh gives 8 δ g δ h g s d m s 2 where s is a unit vector in the direction of integration point o i note that if due to symmetry changes in groundwater storage influence only the vertical component β z β and θ β 0 as the thickness of the plane is decreased 9 β g s z s 2 d m assuming uniform density ρ one obtains 10 β g ρ r cos θ s 2 d φ d r taking advantage of radial symmetry and integrating up to a radial distance of r 0 one obtains 11 β 2 π g ρ 0 r 0 r z r 2 z 2 3 2 d r 2 π g ρ 1 1 1 r 0 z 2 as has been remarked by several e g leirião et al 2009 arnoux et al 2020 for water of density 1000 kg m3 when r 0 this evaluates to β 4 193 10 7 s 2 or 41 93 μ g a l m h 2 o expressed otherwise under the infinite plane assumption a gwsc of 2 38 cm will lead to a change in vertical gravity of 1 μgal or 1 10 8 m s2 independent of porosity and water table depth this is the groundwater bouguer plate approximation bpa importantly this derivation enables estimation of the error in β imparted by evaluating to a finite radial distance r 0 as is necessary in a numerical implementation 12 ε 1 1 r 0 z 2 z r 0 where the approximation is valid for r 0 z this mirrors the remarks of leirião et al 2009 who described the gravitational footprint of gwsc as 10 times the depth to the water table beneath the measurement location which corresponds to 90 of the change in gravity this approximation is used in the gravi4gw software presented here to determine the numerical integration domain as a function of defined acceptable error subject to data availability this conversion factor β can be expressed in units of s 2 or μgal m as the calculation is made in terms of gravitational change per unit of equivalent free water column one can also write μ g a l m h 2 o to be explicit to convert to units of gravity per hydraulic head μgal m β should be divided by porosity n 2 2 gravimeters and gravimetric surveys gravimeters measure the amplitude of the gravitational field at their measurement location with precision in the μgal 10 8 m s2 range currently achievable both absolute and relative gravimeters exist absolute gravimeters such as the μquans absolute quantum gravimeter or micro g lacoste fg5 x generally require continuous ac power and precise calibration these are generally suited to stationary measurements at a fixed location where they measure near continuous temporal variations in gravity the micro g lacoste a10 is an absolute gravimeter which can be used for vehicle accessible field measurements measurements made with absolute gravimeters both stationary and portable will benefit from the approach detailed in this work if the target application is gwsc in unconfined aquifers or even though it is not the focus here water storage changes due to snowpack e g voigt et al 2021 relative gravimeters such as the scintrex cg 5 and cg 6 scintrex 2018 provide a relative measurement of gravity and are suited to field applications even in rugged terrain as they can be readily transported from one location to another by a single person on foot they require repeated measurements at a fixed location throughout each survey to enable drift correction as well as a reference measurement for true quantification of gwsc the reference measurement or measurements are made at an absolute gravity station in each survey for time lapse gravimetry the location of the gravimeter at repeat measurements must be well constrained in particular vertically through corrections for small elevation differences between surveys a 1 cm error in elevation equates to a 3 1 μgal error in gravity seibert and brady 2003 arnoux et al 2020 which is within the range of accuracy offered by portable gravimeters as centimetre level accuracy and precision is available in modern global navigation satellite system real time kinematic gnss rtk surveying systems they should thus be considered a necessity to ensure stable vertical position in any time lapse gravimetry campaign even when cemented platforms have been installed a final correction to gravimetry data is that related to earth tides these variations in gravity are generally 100 μgal in amplitude and their impact on groundwater pressures can be exploited to estimate hydro physical properties of aquifers cutillo and bredehoeft 2011 acworth et al 2016 rau et al 2018 for time lapse gravimetry earth tides must be accounted for over the course of a single survey as well as across repeat surveys some gravimeters implement the longman 1959 formulae and can perform internal corrections however the current state of the art is eterna wenzel 1996 kudryavtsev 2004 and its python implementation pygtide rau 2018 significant differences between these and the longman 1959 formulae have been noted arnoux et al 2020 thus due to the precision required in estimating gwsc it is recommended that these corrections are made in the most accurate and precise manner possible 3 software gravi4gw implements the calculation of β equation 4 based on user provided data this data includes a digital elevation model or water table model and locations of gravity stations the program can be supplied the coordinates of a single gravimetry station or a grid of multiple gravimetry stations which enables the creation of a map of β additional plotting facilities for the input data and for the numerical integral are also included fig 3 shows the basic process flow of the software the program can be executed with a single line of python code although some short preamble code is required to define the input data the software is written in the python language which is reputed for its clean readable syntax that should facilitate modifications by end users the dependencies of gravi4gw numpy harris et al 2020 matplotlib hunter 2007 scipy virtanen et al 2020 and gdal gdal ogr contributors 2020 are well maintained and extensively documented standard packages the software makes use of the gdal framework to read input files of geotiff format these files may contain data from digital elevation models dems or groundwater models a meter based coordinate reference system crs is assumed and thus data using other types of crs e g longitude latitude in degrees must be converted prior to use with gravi4gw the dem should extend a distance beyond all evaluation points of at least h eff ε equation 12 where h eff is an assumed effective water table depth to minimize error the dem resolution is recommended to be on the order of h eff or finer although for a relatively uniform surfaces a coarser dem will of course suffice further information regarding program inputs is contained in the software documentation several utility functions are integrated into the software the most important of these are pointmaker and dg additionally a simple plot of the input file and created x and y coordinate matrices is optionally outputted using the quickplotterxyz function the pointmaker function creates a mesh grid for numerical integration of equation 2 it is called once for each gravimetry station and operates using radial coordinates centred at the station before converting to the cartesian coordinate system of the input data inputs for the function include parameters controlling radial extent and radial and azimuthal point density fig 4 the radial coordinate point density follows a logarithmic distribution and the maximum radius is defined based on a user defined acceptable error criterion equation 12 the function also returns the representative area for each point the approach of pointmaker avoids bias from evaluation locations that do not coincide with dem pixel coordinates and given the r 2 dependence of gravitational force employs a computationally efficient radial point density the gravity integral is evaluated using the dg function which is called once for each integration mesh point created by pointmaker for each of these points the gravity vector 13 δ g δ g x x δ g y y δ g z z is evaluated in cartesian coordinates see fig 2 14a δ g x δ g cos θ sin φ 14b δ g y δ g cos θ cos φ 14c δ g z δ g sin θ where δg is calculated following equation 2 a sufficiently small value 1 cm is used for δh to evaluate the β vector equation 4 in terms of execution time i provide indicative times using python 3 7 6 in anaconda spyder 3 3 6 on a windows 10 laptop with 16 gb of ram and an i7 7600u 2 8 ghz processor using 2 m resolution input data and 1257 integration mesh points corresponding to a maximum integration radius of 500 m with 40 points radially gravi4gw takes 1 8 s to evaluate β at a single gravity station point without plotting options and 16 3 s to evaluate it at 100 gravimetry station points with plotting options efficiency is gained by exploiting a regular grid and thus using the rectbivariatespline class in scipy to estimate the elevations in the δg numerical integral additional details regarding the use of gravi4gw are available in the software repository readme file and in the program functions themselves 4 application 4 1 site description as an illustrative example i use 2 m resolution digital elevation data from the swiss federal office of topography swisstopo with an approximate centre of 7 53 e 46 19 n and of pixel dimensions 4670 2357 the dataset follows a regular grid and spans 2602 058 to 2611 396 east west and 1113 079 to 1117 791 north south in the swiss ch1903 lv95 crs it includes part of the vallon de réchy mari et al 2013 cochand et al 2019 and the entirety of the tsalet catchment arnoux et al 2020 cochand et al 2019 provide a thorough geological and hydrological description of the greater vallon de réchy in which the catchment is located the tsalet catchment is characterised by moraine and talus deposits and undergoes a seasonal cycle of groundwater storage arnoux et al 2020 this cycle wherein groundwater levels decrease over the period between the end of spring snowmelt and at the earliest the onset of winter snow accumulation is typical of alpine catchments cartwright et al 2020 hayashi 2020 arnoux et al 2021 the spatial and temporal variations in groundwater storage are of interest here primarily due to their importance in ensuring baseflow in streams down gradient 4 2 calculating β and its spatial variability in order to illustrate the usage of gravi4gw and to investigate spatial variance and the influence of assumed effective water table depth i generate maps of β equation 4 assuming depths to the effective water table of 2 m 5 m and 10 m vectors of spatial locations in the same crs as the input data in this case a dem are supplied to gravi4gw which then evaluates the δg integral numerically for each location fig 3 the numerical integral is calculated by generating a set of point area pairs i e an integration mesh i set an acceptable residual of 2 therefore requiring that the integration mesh be extended to 50 the assumed effective water table depth beneath each gravimetric station h eff as per equation 12 i also set the number of radial distances at which mesh points are to be created to 40 and use the default azimuthal density setting of 8 these settings result in integration points for h eff 2 m with the area represented by a mesh point ranging from 2 1 10 3 m2 for the points nearest to the gravimetry station location to 241 m2 for the farthest ones fig 4 these values depend on the input parameters for the pointmaker function and can be thus controlled by the user the density of points decreases radially while a minimum azimuthal point density is also maintained fig 4 this type of integration mesh point density is suitable due to the r 2 nature of gravitational force and provides an appropriate balance between numerical efficiency and accuracy for each of the h eff values and for each gravimeter station coordinate pair a set of mesh points is generated and β is evaluated in the same cartesian crs as the input ch1903 lv95 this enables the creation of a β or β z map fig 5 generally values of β are greatest in regions of convex topography mounds hills ridges etc and smallest in those of concave topographies dolines depressions channels etc this can be seen in fig 5 where the lowest values are observed in the beds of intermittent streams and the highest values are observed on mounds at the highly concave locations there is likely to be the greatest divergence between topographically informed β values and the true dependence of gravity on gwsc as the realistically attainable measurement uncertainty for time lapse gravimetry measurements using a portable gravimeter is likely to be in the 5 μgal range mcclymont et al 2012 arnoux et al 2020 it is thus advisable that one avoid areas of extreme concave curvature and instead target gravity measurement locations with moderately convex or planar topography spatial variations in β are most abrupt when h eff is relatively shallow this is due to the more pronounced effect of local curvature as the assumed depth to the water table increases the effective radius of influence or footprint leirião et al 2009 increases as illustrated for planar topography in equation 11 wherein 90 of the δg signal can be attributed to the area within a radial distance of 10 times the depth to the water table below the gravimetric measurement point thus with increasing h eff the influence of features in the immediate vicinity of the gravimetry station is weakened 4 3 dependence of β on groundwater table depth assumed effective groundwater depth h eff is a necessity in calculating β yet in practical applications it will not be known and its true value will not be constant indeed this presents a circuitous problem as the depth to the groundwater table and its change over time constitutes in part what one seeks to understand when using time lapse gravimetry additionally as the depth to the groundwater table changes β may also change in this way accurate conversion between δg and gwsc as well as quantification of uncertainty may require an understanding of the dependence of β on the assumed effective depth to the groundwater table when the changes in groundwater storage are significant it is therefore useful to investigate the effect of this assumed groundwater depth parameter on β furthermore the directionality of δg is not necessarily co directional with the gravity vector which defines the z direction in a geodetic crs this will generally be the case in any sloped terrain although the difference between β and β z will be constrained to 5 for slopes with gradients 18 33 grade gravi4gw calculates the β vector thus it is possible to investigate its the vertical and non vertical components while it is interesting to consider the non vertical components of β in practical time lapse gravimetry field surveys β z will be the quantity of interest i define a series of 21 evenly spaced points spanning 510 m horizontally roughly aligned nw se fig 6 a this transect covers a central part of the catchment and sits between two intermittent stream channels at each of these points i evaluate β at h eff of 2 30 m and additionally calculate θ β equation 7 the angle between β and the vertical vector z fig 6 the highest variability in β both across the points and as a function of h eff occurs for small values of h eff i e when the assumed effective depth to the groundwater table is shallow this is due to the greater influence of small scale local topography at low h eff values as h eff increases so does the extent of the gwsc footprint affecting β a most extreme example of this is the sixth lowest point with the lowest β value at h eff 2 m fig 6b in contrast to the neighbouring points this location lies at the bottom of a local depression in the direction perpendicular to the transect as expected this local concave topography causes the value of β to be less than that stemming from the groundwater bouguer plate approximation 41 93 μ g a l m h 2 o however as h eff increases this effect is dampened as the local depression represents an increasingly smaller proportion of the region of influence the vertical component and consequently the direction of the δ g vector also change as a function of h eff fig 6c and d the lower half of this transect has a steeper gradient than the upper half fig 6a inset along the transect the lowest six points have local gradients of between 14 4 and 20 5 while the next four points have slopes between 24 9 and 29 8 above this the slope along the direction of the transect is between 3 6 and 12 4 one observes that θ β has the greatest variance for small h eff as h eff increases the θ β values of the upper region where the topography is more uniform approach values similar to the local topographical slope in the hypothetical case of a uniform sloping plane one would expect the orientation of the δ g vector to be perpendicular to the gradient however as topography is non planar θ β does not tend exactly towards the local slope of the terrain 4 4 using β with time lapse gravimetric data relative time lapse gravimetry measurements were carried out in the tsalet catchment in july and october 2019 arnoux et al 2020 these dates correspond to the end of the spring summer snow melt period and just prior to the onset of autumn snowfall and thus were expected to correspond to a period of significant decrease in groundwater storage typical of small alpine catchments with complex topography cowie et al 2017 cochand et al 2019 hayashi 2020 there are no wells or piezometers in the catchment however the drying of intermittent streams indicated negative gwsc arnoux et al 2020 observed greater decreases in gravity in the upper talus dominated portion of the surveyed area the authors stated that the results could only be used to look at relative differences across different parts of the catchment due in major part to the likely variability of β from one gravimeter station to another here i use gravi4gw to make δg to gwsc conversions using β z fig 7 additionally i test the influence of assumed effective depth to the water table whose precise value is unknown calculated with h eff 5 m the value of β z at all but two of the thirteen time lapse gravimetry stations is less than that of the groundwater bpa and significant variability between locations exists notably for most stations uncertainty in h eff has less of an effect on the calculated gwsc values than does the choice to use the more advanced method of gravi4gw in place of the bpa at station g1 for example gravi4gw determined gwsc values vary between 2 06 and 2 09 m for the range of h eff explored whereas the bpa determined value is 1 56 m these calculations combined with typical talus and moraine porosity ranges agree with the observed drying of intermittent groundwater fed streams over the snow free period arnoux et al 2020 while the converted gwsc do not change the overall conclusions of greater gwsc in the upper talus area of the catchment compared to those in the lower moraine region they do provide a means for making more quantitative interpretations 5 discussion time lapse gravimetry is an established and powerful tool for the indirect measurement of changes in mass distribution over time however more widespread and quantitative use in groundwater studies has been held back in part by the lack of straightforward tools for conversion between δg and gwsc gravi4gw seeks to fill this void while gravimetry is not a direct substitute for piezometric measurements it does provide valuable information where drilling bores or piezometers is not feasible due to logistical or financial constraints the tsalet catchment presented as our example application site typifies an instance where time lapse gravimetry can offer significant quantitative insight into hydrogeological processes when using the outlined topographically informed conversion factors between δg measurements and gwsc it is useful to consider the meaning of β as compared to β z on sloping terrain in particular this consideration is important as the values of these two may differ significantly this occurs due to the directionality of the change in gravity imparted by the increase or decrease in local groundwater storage which may be non vertical the precision that would be required to directly measure the directionality of this vector would be excessively fine for example a δg value of 100 μgal with a θ β value of 20 would only impart a change in the direction of g of 2 10 6 this means that the off vertical components in δg due to gwsc are not measurable what is actually measured in time lapse gravimetry is the vertical component of δg or δg z and therefore the quantity calculated by gravi4gw that is of greatest interest for conversion between δg and gwsc is β z equation 5 defining a maximum radius for the δg integral numerical calculation is necessary this is done via the acceptable error criterion equation 12 in gravi4gw due to the r 2 dependence of the gravitational force this radius need not be prohibitively large but its finite value will nonetheless impart some error the software does not calculate the effects on gravity beyond the user defined radius and does not modify the β value after its calculation some users may nonetheless wish to normalise the resultant β values by a factor of 1 1 ε which implicitly assumes that integrating to infinity would result in an increase in β this however may not be strictly true if an increase in the integration radius results in the integration of mass at a higher average elevation than the gravimetry station thus decreasing β albeit slightly it is thus suggested that the integration radius as defined through equation 12 not extend to zones where groundwater response is likely to be weaker than that of the unconfined aquifer of interest to go beyond this level of precision the input data to gravi4gw would need to be informed by a groundwater model of some sort time lapse gravimetry is particularly useful for the investigation of unconfined shallow aquifers unconfined aquifers experience much greater changes in water content due to the piezometric surface being equivalent to the water level in the aquifer they are also more likely than confined aquifers to experience significant seasonal changes in groundwater storage due to their direct connection with the earth s surface one could potentially also monitor deep confined aquifers with gravimetric methods while the bpa may be applicable in some cases the possible deformation of a confined aquifer due to gwsc acworth et al 2017 the generally larger gravity footprint due to confined aquifers generally being situated deeper than unconfined ones and influence on g from shallower unconfined aquifers may complicate interpretation as shown where the water table is shallow the effect of local variations in gwsc on gravity is pronounced as the groundwater gravitational footprint is smaller leirião et al 2009 thus the spatial resolution possible through the use of time lapse gravimetry is greatest for shallow unconfined systems incidentally this concept can also be applied to snowpack whose gravitational footprint will theoretically be smaller due to it being located at the surface voigt et al 2021 nonetheless calculated a snowpack gravity footprint with significant azimuthal variability extending up to 4 km using a continuous superconducting gravimeter at a highly instrumented alpine site this calculation used an ε value see equation 12 of 0 1 which would imply a footprint on the order of 40 m under the 10 criteria of leirião et al 2009 another consideration that those combining geodesy with hydrogeology may need to make in clay rich soils in particular is the vadose zone vz if large variations in vz water content are expected then this may affect h eff and thus β in relatively well drained soils or shallow water tables vz effects are likely to be insignificant compared to other sources of uncertainty the piezometric surface that defines the water table will not generally be known a priori in a study that uses time lapse gravimetry the present results show that despite this uncertainty in h eff there is much to be gained from the use of the approach implemented in gravi4gw especially so when the alternative is the constant bpa without direct knowledge of the water table geometry it is impossible to know exactly what the true value of β z is although it is clear that β z will have less spatial variability for deeper water tables in the absence of other information about the water table gravi4gw provides the user with a range of β z values and hence a range of gwsc as expressed in meters of water equivalent this is ultimately the quantity of interest in time lapse gravimetry groundwater studies the linear topographical influence on water table elevation that is calculated when a dem is provided as input data can be viewed as a first order approximation with the true topology of the surface possibly exhibiting less curvature than the land surface thus digital elevation data of lower resolution than that used as an example in this work 2 m resolution may also provide consistent results due to the smoothing effect it follows from the earlier discussion of the effective spatial resolution of time lapse gravimetry that the influence of local surface curvature on β will decrease as the depth to the water table increases for a greater order of accuracy in calculating β a groundwater model e g beven and kirkby 1979 thompson and moore 1996 brunner and simmons 2012 into which additional knowledge of local hydrogeological conditions could be integrated could be used in place of a dem finally while i have focused primarily on the evaluation of β to estimate gwsc using time lapse gravimetric measurements gravi4gw can also be used in the geophysical fieldwork planning phase maps of β such as those of fig 5 can serve to guide researchers towards locations where gravimetric measurements are likely to provide the greatest value locations with higher values of β specifically β z should be targeted as they are likely to yield the greatest and thus most readily measurable δg values 6 conclusions gravi4gw provides a flexible tool for the calculation of groundwater storage changes based on observed changes in gravity integration of complimentary data into it will likely be of interest to certain users depending on the nature of their study and zone of interest i have provided an example of an alpine catchment with limited hydrological monitoring in place and shown how groundwater depth and topographical curvature and slope can effect β z for highly instrumented laboratory catchments gravi4gw may provide an additional tool that supports the conclusions of other hydrological measurements and water balance models for the vast majority of catchments which do not have a high density of instrumentation the tool provides a straightforward means for estimating gwsc more accurately than possible the bpa method i envision the software being used in both the preparation and the processing phases of time lapse gravimetry field campaigns for preparation dem or groundwater model informed β maps created by gravi4gw can be used to target zones where gwsc will have the greatest gravimetric signal in the processing phase gravi4gw enables conversion between measured δg and gwsc as well as an analysis of uncertainty due to the unknown depth to the groundwater table additionally operators of continuous monitoring superconducting gravimeters e g voigt et al 2021 chaffaut et al 2022 may find utility in gravi4gw in assessing changes in topographically influenced snowpack and groundwater storage as well as exploring direction dependent sensitivity or admittance the tool can be executed in a single line of code by even novice python users due to its utility and simplicity of use it is expected that gravi4gw will assist hydrogeologists geophysicists and environmental scientists in using gravimetry to make quantitative assessments and to arrive at better informed conclusions when investigating changes in groundwater storage software availability gravi4gw is developed by landon halloran www ljsh ca and is available at www github com lhalloran gravi4gw it is distributed under the gnu general public license v3 0 gravi4gw is written in python 3 7 and requires standard packages numpy harris et al 2020 matplotlib hunter 2007 scipy virtanen et al 2020 and osgeo gdal gdal ogr contributors 2020 the core gravi4gw py program is 20 kb in size while the package with included example scripts input data and output is 87 mb it has been built and tested in the spyder 3 3 6 ide www spyder ide org as included in the open source python distribution platform anaconda individual edition www anaconda com declaration of competing interest the author declares that he has no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements most figures were made using matplotlib hunter 2007 hillshading code was adapted from github user roger veciana i rovira rveciana i thank the editors and the three anonymous reviewers whose encouraging feedback assisted me in refining the manuscript 
25612,numerical models of lakes and reservoirs have been widely applied to provide quantitative forecasts of pathogen occurrence and persistence in source water to inform quantitative microbial risk assessment qmra there is an emerging need in the water supply industry for a set of best practice modelling guidelines supporting consistent and repeatable use of lake modelling approaches that could be used to provide quality assurance to water authorities and regulators to aid in the development of these guidelines we conducted a literature review to summarise common modelling steps from existing water modelling guidelines as the basis for best practice guidelines for lake modelling we also report on the results of a workshop expert interviews and online surveys that identify common challenges and requirements for each step the summarised lake modelling steps and key requirements pave the way to complete the development of best practice guidelines for lake modelling to inform qmra keywords lake hydrodynamic model guidance pathogen water quality health based targets 1 introduction contamination of drinking water by pathogens constitutes a major threat to human health globally leading to significant disease burden to communities or even loss of life braeye et al 2015 craun et al 2010 who 2016 to assess drinking water safety and protect public health the world health organisation guidelines for drinking water quality who 2017 and national guidelines including the canadian guidance for providing safe drinking water in areas of federal jurisdiction health canada 2013 the australian drinking water guidelines nhmrc 2011 and the new zealand drinking water safety plan framework ministry of health 2018 recommend a multi barrier source to tap approach to prevent waterborne pathogens from entering the drinking water supply system in this context the quantitative microbial risk assessment qmra framework serves as an important risk assessment method in the guidelines and has been widely used by water utilities around the world to assess whether the health based targets can be met with treatment storage and distribution systems medema and smeets 2009 tolouei et al 2019 qmra is a framework that allows for quantitative scientific data e g pathogen occurrence and persistence barrier efficiency and individual susceptibility to be interpreted in the context of estimated health outcomes since its first use in the early 1990s for water management regli et al 1991 qmra has been applied in different levels of detail for drinking water recreational water e g outdoor and swimming pools reuse of domestic wastewater in agriculture rainwater harvesting greywater recycling and many other forms of human water use medema 2012 qmra comprises four steps problem formulation exposure assessment health effects assessment and risk characterisation fig 1 for all the steps except problem formulation quantitative data inputs are needed to inform the processes through defining various model assumptions estimating consumption volume developing dose response relationships for reference pathogens characterising exposure frequency and establishing appropriate disability adjusted life year health impact weightings fig 1 petterson and ashbolt 2016 knowledge of pathogen concentrations in source water is a critical requirement for exposure assessment represented as the second step in qmra to evaluate the ability of a drinking water treatment plant to provide safe drinking water sokolova et al 2015 however pathogen enumeration in source water samples alone is not sufficient to describe the source water quality the monitoring of pathogen concentrations in source water is often infrequent and not well suited to the highly variable nature of pathogen concentrations in many water sources which are often driven by catchment loading events as pathogens appear and disappear within a few days astrom et al 2007 westrell et al 2006 this intermittency means that monitoring processes may fail to detect potentially hazardous peaks in concentrations of pathogens furthermore detection limits of analytical methods may be higher than the pathogen concentrations relevant to public health effects smeets et al 2007 only relying on monitoring data may provide misleading and often underestimates of pathogen levels relevant to qmra and undermine the safety of drinking water and the health of consumers sokolova et al 2015 to address the limitation of monitoring data and provide reliable inputs to qmra lake hydrodynamic models with a pathogen module termed lake models hereafter often coupled with a catchment model that estimates pathogen concentrations transported from the catchment to the lake are increasingly used to simulate the advection and dispersion of pathogens in source water and predict pathogen fate and transport in the source water over a given time period fig 1 hipsey et al 2004 jalliffier verne et al 2016 mcbride et al 2012 mohammed et al 2019 sokolova et al 2012 tolouei et al 2019 thus lake models can usefully complement monitoring data assessments the quality and outcomes of a modelling process depend on several critical steps some of which are implicitly used by modellers but which are often not adopted routinely or communicated widely as part of a quality assurance process in many cases the modelling approaches and interpretation of simulation results require specific expertise and protocols to be applied to enable the implications of different management strategies to be well understood there is an emerging need in the water supply industry for information on the appropriate use of lake models to support qmra ideally the information is a set of best practice guidelines that demonstrate a series of quality assurance principles and actions to ensure that model development implementation and application represent best practice and are commensurate with the intended purpose fig 1 black et al 2011 the best practice guidelines allow the consistent and repeatable use of lake modelling approaches which provide quality assurance to water authorities and regulators best practice guidelines have been developed for a series of modelling practices including environmental modelling generally jakeman et al 2006 us environmental protection agency 2009 rainfall and runoff modelling ball et al 2019 wong 2006 groundwater modelling barnett et al 2012 water management modelling black et al 2014 conceptual modelling argent et al 2016 bayesian network modelling chen and pollino 2012 and catchment hydrological modelling engel et al 2007 harmel et al 2014 however guidelines have so far provided little information about specific types of water models lake models in particular that could be used to inform decision making and develop techniques aligned to improving water management lakes and reservoirs have different characteristics and provide different ecosystem services than groundwater stormwater and rivers many lake modelling specific elements are missing in existing guidelines such as the suitability of 1d and 3d lake models for different intended uses coupling with a catchment model lake model calibration sequence and simulations of reference pathogen groups this means that existing guidelines cannot be applied directly for lake models however there are generalities applicable for modelling practices across the water domain and common modelling steps can be summarised from existing guidelines to form a valuable basis to develop specific lake modelling guidelines this study aims to map out the key steps in lake modelling and identify common challenges and requirements for each step to inform the future development of best practice guidelines for lake modelling in qmra applications to achieve this objective we carried out a literature review to summarise common steps for water modelling practice the review was used as a basis for further development of modelling steps specifically for lake models we then conducted three activities to refine the summarised steps and identify common challenges and key requirements for each of the steps the first activity was a key stakeholder workshop activity 1 in which participants from various water authorities local governments and water utilities around australia met to discuss and synthesise the common issues and needs of lake model users and stakeholders in qmra related projects building on this a series of expert interviews activity 2 and industry wide surveys activity 3 were conducted to better refine the content and resourcing needs to advance a publication on best practice guidelines for the application of lake models to support qmras the refined common steps with associated requirements are presented and can form the basis for developing best practice guidelines for lake modelling to inform qmra 2 literature review of common steps for water modelling a review of recently published best practice guidelines for water modelling across diverse disciplines and domains including groundwater water resources management and drinking water treatment was completed supplementary table s1 provides a subset of the reviewed literature the number of modelling steps varies among guidelines ranging from three in us environmental protection agency 2009 to nine in rietveld et al 2010 barnett et al 2012 in the australian groundwater modelling guidelines had two separate steps of model prediction and uncertainty analysis which are often presented as one step in many best practice guidelines see table 1 but when we chose to use the seven steps proposed in barnett et al 2012 to align with those in other guidelines a remarkable consistency was identified regarding accepted standard modelling approaches and the detailed modelling tasks were similar among the existing guidelines table 1 given the consistency in modelling steps across multiple issued best practice guidelines we adjusted the modelling steps given in barnett et al 2012 by aggregating model prediction and uncertainty analysis into one step and then summarised the resulting six modelling steps for our recommended best practice guidelines for lake modelling the resulting six steps are as follows step 1 project planning this process focuses on gaining clarity on the project objectives related to qmra the intended use of the model including model domain and model scope and the type of model needed to meet the project objectives step 2 lake model conceptualisation this step involves using all available data and knowledge of the region of interest to develop a conceptual lake model which is a description of the known in lake physical biochemical features within the area of interest step 3 lake model design and construction this process includes deciding on how to best represent the conceptual lake model in mathematical terms and defining the inputs required for model implementation the model confidence level classification should be addressed and the model performance criteria should be agreed at this stage the classification is a benchmarking exercise to demonstrate the level of confidence in the lake model prediction and generally reflects the level of data available to support model development step 4 lake model calibration and testing this step occurs through a process of matching lake model outputs to a historical record of observed data step 5 lake model application and uncertainty analysis this process comprises those model simulations that provide modelled pathogen concentrations to address the question s defined in the modelling objectives the predictive analysis is followed by an analysis of the implications of the uncertainty associated with the modelling outputs step 6 final reporting and archiving this step can be achieved by clear communication of the model development and quality of outputs allowing stakeholders and reviewers to assess whether the model is fit for its purpose i e meeting the modelling objectives all the model files and supporting data should be archived so the modelling results can be reproduced in the future if necessary these common steps summarised from the existing best practice guidelines provide a valuable basis to develop similar best practices guidelines for specific modelling practices including lake modelling 3 refining modelling steps and identifying key requirements 3 1 activity 1 stakeholder workshop many water utilities rely on qmra to inform the assessment and management of their treatment storage and distribution systems for meeting health based targets water utilities and their stakeholders are often the end users of lake models for qmra projects it is therefore essential to include stakeholder insights in the development of best practice guidelines for lake modelling a workshop was held on 29 september 2020 with 19 stakeholder participants from various water utilities around australia including hunter water melbourne water seqwater sa water sydney water tropwater and waternsw and local governments including cairns regional council city of gold coast and toowoomba regional council the workshop was organised following the principle of the delphi method dalkey and helmer 1963 to help the stakeholder group reach consensus on the summarised modelling steps and key requirements for each step during the workshop participants were introduced to a summary of existing best practice guidelines for water modelling and were also presented with the aforementioned six common steps for water modelling practices which were proposed to form the structure of the best practice guidelines for lake modelling all participants then discussed the proposed steps and the challenges each may pose the discussion process was facilitated by the interactive meeting software mentimeter mentimeter ab stockholm which allowed participants to provide anticipated challenges and requirements for each of the proposed modelling steps challenges and requirements were displayed in real time to all stakeholders enabling them to adjust subsequent responses based on their interpretation of the group response see supplementary material for a description of the discussion process the main output from the workshop was a list of challenges and requirements in the application of lake models to inform qmra see supplementary material for the output while similar workshops have been held in related fields such as flood modelling and water sensitive urban design around australia this meeting was the first time that a group of lake model users and water stakeholders from various organisations around australia had discussed best practice guidelines for lake modelling in qmra projects the output was used to create a more detailed interview structure and online survey questions for the next two research activities 1 individualised expert interviews and 2 industry wide online survey 3 2 activity 2 expert interviews after the workshop four water quality specialists were invited for a one on one virtual interview where confirmation of the challenges identified during the workshop activity 1 was sought and potential solutions were discussed the interview participants were selected based on their extensive experience in applying lake models to inform qmra with three of them sitting as water quality advisory committee members for the australian national health and medical research council and the fourth participant possessing more than 15 years of experience in wastewater and recycled water management interview questions comprised of a general set of questions and a set of customised questions for each participant were designed specifically according to each participant s experience the responses were recorded and collated in correspondence to each of the six modelling steps the duration of the four interviews ranged from 33 to 50 minutes the main output from the interviews was a list of confirmed challenges accompanied by potential solutions and general suggestions on the best practice guidelines for example to address the highly variable nature of pathogen decay rate one expert recommended using a range of pathogen decay rates in model simulations and reporting the variability of pathogen concentrations to inform risk assessment all four experts also provided general suggestions on drafting the best practice guidelines for lake modelling such as 1 including basic information about different lake model types e g 1d vs 3d will help stakeholders better understand the model construction process and 2 listing the desired data requirements e g data variables sample frequency for developing lake models to inform water utilities in developing their data monitoring program the main output from the interviews was used to help in the design of activity 3 the online survey 3 3 activity 3 online survey with the outputs from the stakeholder workshop activity 1 and expert interviews activity 2 a broader community of lake model users and stakeholders was invited for an online survey see supplementary material for more detail the survey used a combination of closed and open ended questions to ascertain participant responses to questions developed from activities 1 and 2 the closed ended questions asked participants to rank the importance of each of the identified common challenges and requirements on a likert scale from 1 not important at all to 5 very important participants were also asked eight optional open ended questions which included providing additional requirements for each of the six steps i e six questions and the following two general questions 1 six common steps have been identified when using lake models to inform quantitative microbial risk assessment qmra see fig 1 1 the figure was presented in the online survey not referring to any figure in the paper the figure was essentially a flow chart format of the six summarised steps described in section 2 do you agree with the 6 steps if not please give a reason and 2 how useful is it for your organisation to have best practice guidelines for lake modelling to inform qmras for this survey we targeted professionals and experts in the water sector across australia including government industry and non governmental organisation employees with experience in using lake models to inform qmra potential participants were chosen from the authors extensive network of professionals in water utilities academia water authorities and local governments around australia the online survey was distributed to 50 selected participants of which 18 participants including model developers 11 lake modellers 33 those using model output e g water quality experts microbiologists 28 stakeholders 28 and decision makers 17 provided responses 2 2 some participants were classified into more than one category these participants provided representative opinions across the water industry on the identified common challenges and key requirements for the best practice guidelines fourteen out of the 18 participants provided responses for all survey questions while the responses from the remaining four participants were for the two general questions survey results showed that almost all identified challenges and requirements 98 were considered important scored 4 or 5 by the majority 50 of participants the full survey results were provided in the supplementary material it should also be noted that a few requirements were scored 2 or 3 i e of little importance by a substantial number of participants in some cases there was even a balance between those identifying the challenges and requirements as important and those that did not for example the same number of participants considered the requirements related to step 2 lake model conceptualisation as important or of little importance which included 1 determine the appropriate format of conceptual models depending on the purpose of the conceptual model fig 2 a and 2 make decisions on how to best implement the conceptualisation fig 2b given that most participants agreed that identifying key physical and biochemical processes influencing lake reservoir mixing patterns and water quality are important fig 2c this suggests that these two requirements are addressed implicitly in the modelling practice and could therefore be deleted another group of requirements that were ranked important or of little importance by the same or similar number of participants related to technical details of lake modelling practice step 3 lake model design and construction the requirement of determine whether to couple a catchment model with a lake model was scored 3 by seven participants and 4 or 5 by the other seven participants fig 3 a a catchment model can be useful in providing ungauged inflow estimates to a study lake or reservoir and thus can improve the accuracy of capturing hydrological patterns including lake residence time and nutrient and sediment loads to a lake mohammed et al 2019 the inclusion of a catchment model may however necessitate a substantial increase in project workload and budget we retained this requirement to allow stakeholders and lake modellers to decide on the advantages and disadvantages of including a catchment model another requirement of consider the sequence of model calibration for step 4 lake model calibration and testing was ranked as being of little importance by 6 participants but important by the other 8 participants fig 3b lake modelling calibration is widely recognised as an important step to adjust model parameters for reliable simulation outputs given that advection and dispersion critically influence the fate and transport of biochemical variables such as dissolved oxygen and nutrients an appropriate sequence of model calibration is to begin with water temperature and velocity comparisons to assure good hydrodynamic process representation although the requirement of sequential calibration may be less relevant to stakeholders than to modellers we kept this requirement in the best practice guidelines the requirement of using bulk pathogen decay rates to communicate model uncertainty for step 5 lake model application and uncertainty analysis was considered to be of little importance by seven participants but important by the other seven participants fig 3c pathogen decay rate is an important parameter in simulating the fate and transport of pathogens in lakes reservoirs but measured values for a specific pathogen can vary widely with environmental conditions which may not necessarily be captured accurately or at all in many models which use a fixed parameter value assigning a range of decay rates may better reflect the natural variability in pathogen decay rates and should be retained in the guidelines to communicate model uncertainty survey results also showed that 15 participants 83 agreed on the proposed six steps for best practice lake modelling while some participants provided additional steps for consideration fig 4 all the proposed additional steps were incorporated into the six step system seventeen participants 94 regarded developing the best practice guidelines as useful of which six participants thought they would be very useful and urgently needed fig 5 4 identified requirements and challenges for best practice guidelines for lake modelling a list of common requirements challenges faced by lake modellers to inform qmra was identified and refined through the three research activities the list has been agreed upon by a relatively small but diverse and representative group of lake model users around australia and can therefore form a basis of best practice guidelines for lake modelling to inform qmra the refined list of common requirements challenges for each of the six steps is provided below fig 6 4 1 step 1 project planning stakeholders and lake modellers should clarify the actual questions to be answered to inform qmra and identify the project objectives and intended uses of the model as recommended by harmel et al 2014 this is necessary to ensure that the right modelling framework is used and that the model is set up to answer those questions stakeholders usually use lake models to test their assumptions in that case stakeholders need to clarify what assumptions they want to test with consultation with lake modellers stakeholders should also determine project timelines budget and expertise required to build the model and keep modellers up to date on any changes to initial project objectives timelines and budgets it is strongly recommended that in the best practice guidelines the desired data requirements for developing a lake model should be outlined and that modellers communicate with stakeholders about the data requirements and help them decide on whether a lake model is the right tool and whether the desired data requirements are met for developing a lake model jones et al 2020 if the answer is yes to both questions ensure that all key area experts particularly on data analysis water quality and microbiology are fully involved at a necessary frequency as lake modelling is a multi disciplinary process all parties should determine how complex the model should be 1d vs 3d modellers should clarify model limitations and agree with stakeholders on what is achievable within the constraints of the project and the level of uncertainty that is acceptable in model output it is suggested in the complete best practice guidelines a checklist of key decisions be provided which may include 1 which pathogen groups viruses bacteria and protozoa to be simulated 2 1d or 3d lake model 3 linking a catchment model or not and 4 whether simulated sediment dynamics are required all parties should have a shared understanding of the characteristics of the study lake s and discuss the potential worst case scenarios for pathogen risk e g short circuiting low water level flood event in the catchment and dam operations they also need to agree on how to present model outputs to non modellers e g look up tables and identify ownership of the data and developed models at this stage 4 2 step 2 lake model conceptualisation conceptual lake models can be an extensive interface diagram or a simple pen and paper drawing depending on the complexity the model scope requires and are useful to capture our current understanding of the structure and processes of a lake system argent et al 2016 they can help engage stakeholders reach consensus and are often a first step of quantitative modelling model conceptualisation requires all parties to identify and discuss boundary conditions e g catchment inflow locations surrounding land uses water treatment plant offtake locations and meteorological conditions and key physical and biochemical processes of the study lake s to be represented in the conceptual model this discussion enables a range of stakeholders to better understand the lake modelling process and how the modelling outputs to be used to inform qmra following the characterisation of key processes in the conceptual model data availability and knowledge gaps associated with each process can be identified 1d and 3d lake models require similar types of input data but the data requirements for 3d models are much more extensive this distinction is not covered in existing guidelines modellers and stakeholders need to agree on model assumptions made to tackle the data and knowledge gaps gupta et al 2012 the model assumptions are often lake model specific such as the depth range of water treatment plant offtake catchment inflow temperature and hydro power dam operation to facilitate the process it is recommended that stakeholders set up a clear repository of monitoring data and that key stakeholders who have knowledge of the required data be involved the conceptual model should be progressively refined through an iterative process as a given qmra project progresses it is important to have clear ongoing communications as part of the modelling process and allocate appropriate project resources e g stakeholder time commitments 4 3 step 3 lake model design and construction this step is essentially about representing the conceptual model in mathematical terms with a suitable lake model the desired data requirements for the following step of model calibration and testing should be identified depending on the complexity and scope of the model stakeholders and modellers should then evaluate the availability and quality of data required for lake modelling based on the lake model conceptualisation if some data are lacking alternatives or methods to fill the gaps should be decided upon nutrient sediment and pathogen loads from the lake catchment often have substantial impacts on the lake conditions when monitoring data on the catchment are not adequate for lake model development a catchment model may provide an option to generate data synthetically but can significantly increase workload and budget although existing guidelines can be used to develop a catchment model e g engel et al 2007 harmel et al 2014 they provide little information on its coupling with a lake model the best practice guidelines for lake modelling should clearly demonstrate the benefits and shortcomings of developing a catchment model in the project and provide specific guidance to help make the decision pathogen decay rates are important model parameters and can be sourced from previous literature or lab experiment e g ahmed et al 2021 hipsey et al 2008 literature decay rates can however be variable so best practice should ensure whatever is used is justified and appropriate additionally locally measured pathogen decay rates should be interpreted in the light of international understanding it may be worthwhile to set up recommended upper and lower bound values of pathogen decay rates for different types of lakes so that lake modellers would have default pathogen decay rates as a starting point when calibrating a lake model in this step all parties should decide what management options scenarios should be considered to address project objectives climate change impacts on pathogen risk attract increasing attention from water utilities and inclusion of climate change study may affect the model design and type of required data 4 4 step 4 lake model calibration and testing for model calibration it is recommended that a sequential process is adopted starting with physical quantities such as temperature water level and water velocity before focusing on biochemical variables such as dissolved oxygen nutrients phytoplankton and pathogens lehmann and hamilton 2018 the rationale for this approach is to first attempt to calibrate the model to adequately simulate the physical processes that affect the fate and transport of biochemical processes while it is recognised that there is inherent feedback between biochemical and physical processes in a lake the strength of feedback from the biochemical to the physical processes is often weaker than vice versa this sequential approach can reduce the time required to achieve a satisfactory calibration but it should not seek to rigorously fix model parameters as iterative adjustments of physical and biochemical parameters may be useful for each quantity of interest qoi the initial order of calibration should be decided through preliminary sensitivity analysis to focus calibration on the most sensitive qois in the first instance the sensitivity analysis technique used can be either a simple one at a time approach or one of the many global methods that are available see for example saltelli et al 2008 with ongoing improvements in computational power combined global sensitivity analysis and parameter estimation techniques will increasingly be used for the calibration of these types of water quality models in the future it is important to consult with water quality and microbiology experts and stakeholders to determine the desired goodness of fit and or statistical results needed to meet the objectives of the health based target guidance for qmras some model performance measures and indicators as well as their common benchmarks as proposed in existing guidelines may be useful in the calibration of lake models but different characteristics e g thermal stratification of lakes and reservoirs to rivers and groundwater require model performance to be assessed throughout the whole water column appropriate spatial and temporal extent of data for lake model calibration and testing should also be identified for a new reservoir model application calibration and testing data may not be available alternative ways of validating model output include sensitivity analysis and use of data from reservoirs lakes that have similar geographical and morphological characteristics it is recommended to produce a report at this point and have it reviewed preferably by a third party with a suitable skill set and allow time for beneficial suggestions to be incorporated into the modelling effort the report should provide general modelling information for non modellers so they can understand boundary condition model extent etc with regards to their qmra objectives 4 5 step 5 lake model application and uncertainty analysis the purpose of this step is to apply developed lake models to answer questions posed in the project objectives such as understanding simulated pathogen concentrations and the associated uncertainty under different management scenarios stakeholders and modellers should agree on the ways of effectively presenting complex model outputs and uncertainties harmel et al 2014 as there is currently no accepted or agreed approach for this it is recommended that potential management options be related to resulting in lake pathogen decay rates this would assist non modellers and non microbiologists to understand the impact of different management options on the barrier effect of the study lake in addition the vertical dimension i e depth is a key characteristic of lakes and simulated pathogen concentrations are often distributed heterogeneously in the vertical direction due to thermal stratification it is thus suggested that presentation of lake model outputs account for the depth variable it is important to undertake uncertainty analysis to better assess pathogen risks petterson and ashbolt 2016 the analyses may include identifying potential source of uncertainty understanding and communicating uncertainty propagation e g additive or multiplicative significant uncertainty may exist in monitored reference pathogen concentrations by lake monitoring programs it is recommended to use a range of pathogen decay rates to reflect the natural variability in pathogen decay rates 4 6 step 6 final reporting and file archiving modellers should prepare a technical report describing different stages of model development and application for review the report should also clearly describe how model outputs address project objectives and what knowledge data gaps have been identified modellers are encouraged to be involved in post modelling process community meeting government discussion to help interpret and communicate model outputs limitations all model files and supporting data should be archived with a clear metadata statement of key inputs outputs to make results reproducible clear standards on what should be in a hand over package for qmra projects are still lacking in existing guidelines and should be set up in the complete guidelines a hand over package for a qmra project may include files that allow for the exact reproduction of the results presented in the model report such as all data used to create the lake model and the associated metadata e g the data source and date of capture the package should also document the software used in the modelling process including the version of the software if possible the software itself or the model executables should be included in the package it might be beneficial to introduce in the complete best practice guidelines the idea of transitioning from project to project models to a system of operational models if the end product was a well documented operational model of the system rather than an archived set of files then the investment in the modelling processes would be more cost effective in the long term note that communication and consultation should happen throughout the proposed six steps and that steps 2 5 usually occur in an iterative loop in combination with stakeholder consultation before the project is finalised step 6 5 discussion lake modelling is a valuable tool to provide pathogen data that complement field measurements to inform qmra activities however widely varying practices have been applied to lake model development by modellers who may have different perspectives and background on pathogen fate and transport most water utilities around the world lack in house modellers so this study is important for informing non specialist modellers including understanding and assurance about lake models that are being applied for key management decisions the outcomes of the stakeholder engagement activities completed as part of this research identified a need for a set of best practice guidelines for lake modelling to provide a basis for consistent and repeatable use of modelling approaches which provide quality assurance to water authorities and regulators this study takes the first step to identify the common lake modelling steps and associated requirements to be considered in guidelines for lake modelling to inform qmra given the broad generality in water modelling practice the six lake modelling steps identified in this study draw on material in recent guidelines particularly the australian groundwater modelling guidelines barnett et al 2012 and are consistent in principle with the earlier guidelines table 1 on the other hand considering the uniqueness of lake modelling practice for each modelling step we identified specific requirements directly relevant to lake modelling such as application of 1d or 3d lake models with associated data requirements whether or not to link to a catchment model and model calibration sequence this two step approach applied in this study enables the proposed guidelines to be consistent with the framework of existing guidelines easy for stakeholders and modellers to adopt and to also reflect the unique aspects of lake modelling practice in the development of the best practice guidelines for lake modelling it is important to include experiences and knowledge from groups that extend beyond those typically involved in the development and application of models and should include groups such as water utility stakeholders and water quality experts in this study we incorporated knowledge from lake modellers by summarising the common steps for water modelling from recent literature on best practices and included practical wisdom from water utility stakeholders by holding a stakeholder workshop to summarise model application steps and collate thinking on the challenges for each step additionally we considered knowledge from water quality experts by interviewing four scientists to seek solutions to the identified challenges and requirements importantly we also surveyed a representative group of experts in the water sector across the australian water industry on the proposed steps the inclusion of diverse groups of experts ensured the lake modelling steps are suitable and targeted to the main concerns in using lake models to inform qmra some of the identified requirements need more information to specify best practice guidance this can be achieved through extensive literature review and discussions with relevant parties for example the project administration practices proposed in black et al 2014 give detailed recommendations on how to develop baseline and key scenarios to address project objectives which is directly relevant to step 1 in this study similarly the conceptual modelling framework proposed in robinson 2008a b is relevant to step 2 lake model conceptualisation the list of available aquatic ecosystem models in janssen et al 2015 and discussion on selection of models based on level of complexity in hipsey et al 2015 are relevant to step 3 lake model design and construction model evaluation metrics in hipsey et al 2020 and sensitivity analysis practice in saltelli et al 2019 are relevant to step 4 lake model calibration and testing and the suggested general uncertainty sources for environmental modelling and proposed reporting structure in barnett et al 2012 provide a basis for developing step 5 lake model application and uncertainty analysis and step 6 final reporting and archiving lake models are undergoing continuous development including better representation of existing processes and adding newly identified processes frassl et al 2019 they also increasingly become publicly available through open source policies janssen et al 2015 coordinated community efforts supporting their application and visualisation technology such as r packages lakeensembler https github com aemon j lakeensemblr and dycdtools yu et al 2020 this study paves the way to develop a complete set of best practice guidelines for lake modelling to inform qmra the complete guidelines are expected to be a reference source rather than a rigid standard they will provide direction on the scope and approaches common to modelling projects and encourage the continual evolution of modelling techniques through adaptation and innovation 6 conclusions to meet the industry wide need for a set of best practice guidelines for lake modelling to support qmra this paper takes the first step to map out six common lake modelling steps and associated requirements to be considered in the guidelines these six steps are 1 project planning 2 lake model conceptualisation 3 lake model design and construction 4 lake model calibration and testing 5 lake model application and uncertainty analysis and 6 final reporting and file archiving the six steps were confirmed by the 83 of online survey participants more studies are needed to propose specific best practice guidance for identified requirements for each modelling step and to identify an appropriate format to present the completed document of best practice guidelines for lake modelling to support qmra declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this study was funded by the queensland water modelling network capacity development small grants marlene van der sterren kevin boland and sandya nanayakkara are gratefully acknowledged for critical comments on an earlier version of the manuscript we thank all participants of the workshop interviews and online survey for providing their opinions and suggestions the expert interviews and online survey involved in this study were approved by the ethics committee of griffith university reference number 2020 792 appendix a supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105334 
25612,numerical models of lakes and reservoirs have been widely applied to provide quantitative forecasts of pathogen occurrence and persistence in source water to inform quantitative microbial risk assessment qmra there is an emerging need in the water supply industry for a set of best practice modelling guidelines supporting consistent and repeatable use of lake modelling approaches that could be used to provide quality assurance to water authorities and regulators to aid in the development of these guidelines we conducted a literature review to summarise common modelling steps from existing water modelling guidelines as the basis for best practice guidelines for lake modelling we also report on the results of a workshop expert interviews and online surveys that identify common challenges and requirements for each step the summarised lake modelling steps and key requirements pave the way to complete the development of best practice guidelines for lake modelling to inform qmra keywords lake hydrodynamic model guidance pathogen water quality health based targets 1 introduction contamination of drinking water by pathogens constitutes a major threat to human health globally leading to significant disease burden to communities or even loss of life braeye et al 2015 craun et al 2010 who 2016 to assess drinking water safety and protect public health the world health organisation guidelines for drinking water quality who 2017 and national guidelines including the canadian guidance for providing safe drinking water in areas of federal jurisdiction health canada 2013 the australian drinking water guidelines nhmrc 2011 and the new zealand drinking water safety plan framework ministry of health 2018 recommend a multi barrier source to tap approach to prevent waterborne pathogens from entering the drinking water supply system in this context the quantitative microbial risk assessment qmra framework serves as an important risk assessment method in the guidelines and has been widely used by water utilities around the world to assess whether the health based targets can be met with treatment storage and distribution systems medema and smeets 2009 tolouei et al 2019 qmra is a framework that allows for quantitative scientific data e g pathogen occurrence and persistence barrier efficiency and individual susceptibility to be interpreted in the context of estimated health outcomes since its first use in the early 1990s for water management regli et al 1991 qmra has been applied in different levels of detail for drinking water recreational water e g outdoor and swimming pools reuse of domestic wastewater in agriculture rainwater harvesting greywater recycling and many other forms of human water use medema 2012 qmra comprises four steps problem formulation exposure assessment health effects assessment and risk characterisation fig 1 for all the steps except problem formulation quantitative data inputs are needed to inform the processes through defining various model assumptions estimating consumption volume developing dose response relationships for reference pathogens characterising exposure frequency and establishing appropriate disability adjusted life year health impact weightings fig 1 petterson and ashbolt 2016 knowledge of pathogen concentrations in source water is a critical requirement for exposure assessment represented as the second step in qmra to evaluate the ability of a drinking water treatment plant to provide safe drinking water sokolova et al 2015 however pathogen enumeration in source water samples alone is not sufficient to describe the source water quality the monitoring of pathogen concentrations in source water is often infrequent and not well suited to the highly variable nature of pathogen concentrations in many water sources which are often driven by catchment loading events as pathogens appear and disappear within a few days astrom et al 2007 westrell et al 2006 this intermittency means that monitoring processes may fail to detect potentially hazardous peaks in concentrations of pathogens furthermore detection limits of analytical methods may be higher than the pathogen concentrations relevant to public health effects smeets et al 2007 only relying on monitoring data may provide misleading and often underestimates of pathogen levels relevant to qmra and undermine the safety of drinking water and the health of consumers sokolova et al 2015 to address the limitation of monitoring data and provide reliable inputs to qmra lake hydrodynamic models with a pathogen module termed lake models hereafter often coupled with a catchment model that estimates pathogen concentrations transported from the catchment to the lake are increasingly used to simulate the advection and dispersion of pathogens in source water and predict pathogen fate and transport in the source water over a given time period fig 1 hipsey et al 2004 jalliffier verne et al 2016 mcbride et al 2012 mohammed et al 2019 sokolova et al 2012 tolouei et al 2019 thus lake models can usefully complement monitoring data assessments the quality and outcomes of a modelling process depend on several critical steps some of which are implicitly used by modellers but which are often not adopted routinely or communicated widely as part of a quality assurance process in many cases the modelling approaches and interpretation of simulation results require specific expertise and protocols to be applied to enable the implications of different management strategies to be well understood there is an emerging need in the water supply industry for information on the appropriate use of lake models to support qmra ideally the information is a set of best practice guidelines that demonstrate a series of quality assurance principles and actions to ensure that model development implementation and application represent best practice and are commensurate with the intended purpose fig 1 black et al 2011 the best practice guidelines allow the consistent and repeatable use of lake modelling approaches which provide quality assurance to water authorities and regulators best practice guidelines have been developed for a series of modelling practices including environmental modelling generally jakeman et al 2006 us environmental protection agency 2009 rainfall and runoff modelling ball et al 2019 wong 2006 groundwater modelling barnett et al 2012 water management modelling black et al 2014 conceptual modelling argent et al 2016 bayesian network modelling chen and pollino 2012 and catchment hydrological modelling engel et al 2007 harmel et al 2014 however guidelines have so far provided little information about specific types of water models lake models in particular that could be used to inform decision making and develop techniques aligned to improving water management lakes and reservoirs have different characteristics and provide different ecosystem services than groundwater stormwater and rivers many lake modelling specific elements are missing in existing guidelines such as the suitability of 1d and 3d lake models for different intended uses coupling with a catchment model lake model calibration sequence and simulations of reference pathogen groups this means that existing guidelines cannot be applied directly for lake models however there are generalities applicable for modelling practices across the water domain and common modelling steps can be summarised from existing guidelines to form a valuable basis to develop specific lake modelling guidelines this study aims to map out the key steps in lake modelling and identify common challenges and requirements for each step to inform the future development of best practice guidelines for lake modelling in qmra applications to achieve this objective we carried out a literature review to summarise common steps for water modelling practice the review was used as a basis for further development of modelling steps specifically for lake models we then conducted three activities to refine the summarised steps and identify common challenges and key requirements for each of the steps the first activity was a key stakeholder workshop activity 1 in which participants from various water authorities local governments and water utilities around australia met to discuss and synthesise the common issues and needs of lake model users and stakeholders in qmra related projects building on this a series of expert interviews activity 2 and industry wide surveys activity 3 were conducted to better refine the content and resourcing needs to advance a publication on best practice guidelines for the application of lake models to support qmras the refined common steps with associated requirements are presented and can form the basis for developing best practice guidelines for lake modelling to inform qmra 2 literature review of common steps for water modelling a review of recently published best practice guidelines for water modelling across diverse disciplines and domains including groundwater water resources management and drinking water treatment was completed supplementary table s1 provides a subset of the reviewed literature the number of modelling steps varies among guidelines ranging from three in us environmental protection agency 2009 to nine in rietveld et al 2010 barnett et al 2012 in the australian groundwater modelling guidelines had two separate steps of model prediction and uncertainty analysis which are often presented as one step in many best practice guidelines see table 1 but when we chose to use the seven steps proposed in barnett et al 2012 to align with those in other guidelines a remarkable consistency was identified regarding accepted standard modelling approaches and the detailed modelling tasks were similar among the existing guidelines table 1 given the consistency in modelling steps across multiple issued best practice guidelines we adjusted the modelling steps given in barnett et al 2012 by aggregating model prediction and uncertainty analysis into one step and then summarised the resulting six modelling steps for our recommended best practice guidelines for lake modelling the resulting six steps are as follows step 1 project planning this process focuses on gaining clarity on the project objectives related to qmra the intended use of the model including model domain and model scope and the type of model needed to meet the project objectives step 2 lake model conceptualisation this step involves using all available data and knowledge of the region of interest to develop a conceptual lake model which is a description of the known in lake physical biochemical features within the area of interest step 3 lake model design and construction this process includes deciding on how to best represent the conceptual lake model in mathematical terms and defining the inputs required for model implementation the model confidence level classification should be addressed and the model performance criteria should be agreed at this stage the classification is a benchmarking exercise to demonstrate the level of confidence in the lake model prediction and generally reflects the level of data available to support model development step 4 lake model calibration and testing this step occurs through a process of matching lake model outputs to a historical record of observed data step 5 lake model application and uncertainty analysis this process comprises those model simulations that provide modelled pathogen concentrations to address the question s defined in the modelling objectives the predictive analysis is followed by an analysis of the implications of the uncertainty associated with the modelling outputs step 6 final reporting and archiving this step can be achieved by clear communication of the model development and quality of outputs allowing stakeholders and reviewers to assess whether the model is fit for its purpose i e meeting the modelling objectives all the model files and supporting data should be archived so the modelling results can be reproduced in the future if necessary these common steps summarised from the existing best practice guidelines provide a valuable basis to develop similar best practices guidelines for specific modelling practices including lake modelling 3 refining modelling steps and identifying key requirements 3 1 activity 1 stakeholder workshop many water utilities rely on qmra to inform the assessment and management of their treatment storage and distribution systems for meeting health based targets water utilities and their stakeholders are often the end users of lake models for qmra projects it is therefore essential to include stakeholder insights in the development of best practice guidelines for lake modelling a workshop was held on 29 september 2020 with 19 stakeholder participants from various water utilities around australia including hunter water melbourne water seqwater sa water sydney water tropwater and waternsw and local governments including cairns regional council city of gold coast and toowoomba regional council the workshop was organised following the principle of the delphi method dalkey and helmer 1963 to help the stakeholder group reach consensus on the summarised modelling steps and key requirements for each step during the workshop participants were introduced to a summary of existing best practice guidelines for water modelling and were also presented with the aforementioned six common steps for water modelling practices which were proposed to form the structure of the best practice guidelines for lake modelling all participants then discussed the proposed steps and the challenges each may pose the discussion process was facilitated by the interactive meeting software mentimeter mentimeter ab stockholm which allowed participants to provide anticipated challenges and requirements for each of the proposed modelling steps challenges and requirements were displayed in real time to all stakeholders enabling them to adjust subsequent responses based on their interpretation of the group response see supplementary material for a description of the discussion process the main output from the workshop was a list of challenges and requirements in the application of lake models to inform qmra see supplementary material for the output while similar workshops have been held in related fields such as flood modelling and water sensitive urban design around australia this meeting was the first time that a group of lake model users and water stakeholders from various organisations around australia had discussed best practice guidelines for lake modelling in qmra projects the output was used to create a more detailed interview structure and online survey questions for the next two research activities 1 individualised expert interviews and 2 industry wide online survey 3 2 activity 2 expert interviews after the workshop four water quality specialists were invited for a one on one virtual interview where confirmation of the challenges identified during the workshop activity 1 was sought and potential solutions were discussed the interview participants were selected based on their extensive experience in applying lake models to inform qmra with three of them sitting as water quality advisory committee members for the australian national health and medical research council and the fourth participant possessing more than 15 years of experience in wastewater and recycled water management interview questions comprised of a general set of questions and a set of customised questions for each participant were designed specifically according to each participant s experience the responses were recorded and collated in correspondence to each of the six modelling steps the duration of the four interviews ranged from 33 to 50 minutes the main output from the interviews was a list of confirmed challenges accompanied by potential solutions and general suggestions on the best practice guidelines for example to address the highly variable nature of pathogen decay rate one expert recommended using a range of pathogen decay rates in model simulations and reporting the variability of pathogen concentrations to inform risk assessment all four experts also provided general suggestions on drafting the best practice guidelines for lake modelling such as 1 including basic information about different lake model types e g 1d vs 3d will help stakeholders better understand the model construction process and 2 listing the desired data requirements e g data variables sample frequency for developing lake models to inform water utilities in developing their data monitoring program the main output from the interviews was used to help in the design of activity 3 the online survey 3 3 activity 3 online survey with the outputs from the stakeholder workshop activity 1 and expert interviews activity 2 a broader community of lake model users and stakeholders was invited for an online survey see supplementary material for more detail the survey used a combination of closed and open ended questions to ascertain participant responses to questions developed from activities 1 and 2 the closed ended questions asked participants to rank the importance of each of the identified common challenges and requirements on a likert scale from 1 not important at all to 5 very important participants were also asked eight optional open ended questions which included providing additional requirements for each of the six steps i e six questions and the following two general questions 1 six common steps have been identified when using lake models to inform quantitative microbial risk assessment qmra see fig 1 1 the figure was presented in the online survey not referring to any figure in the paper the figure was essentially a flow chart format of the six summarised steps described in section 2 do you agree with the 6 steps if not please give a reason and 2 how useful is it for your organisation to have best practice guidelines for lake modelling to inform qmras for this survey we targeted professionals and experts in the water sector across australia including government industry and non governmental organisation employees with experience in using lake models to inform qmra potential participants were chosen from the authors extensive network of professionals in water utilities academia water authorities and local governments around australia the online survey was distributed to 50 selected participants of which 18 participants including model developers 11 lake modellers 33 those using model output e g water quality experts microbiologists 28 stakeholders 28 and decision makers 17 provided responses 2 2 some participants were classified into more than one category these participants provided representative opinions across the water industry on the identified common challenges and key requirements for the best practice guidelines fourteen out of the 18 participants provided responses for all survey questions while the responses from the remaining four participants were for the two general questions survey results showed that almost all identified challenges and requirements 98 were considered important scored 4 or 5 by the majority 50 of participants the full survey results were provided in the supplementary material it should also be noted that a few requirements were scored 2 or 3 i e of little importance by a substantial number of participants in some cases there was even a balance between those identifying the challenges and requirements as important and those that did not for example the same number of participants considered the requirements related to step 2 lake model conceptualisation as important or of little importance which included 1 determine the appropriate format of conceptual models depending on the purpose of the conceptual model fig 2 a and 2 make decisions on how to best implement the conceptualisation fig 2b given that most participants agreed that identifying key physical and biochemical processes influencing lake reservoir mixing patterns and water quality are important fig 2c this suggests that these two requirements are addressed implicitly in the modelling practice and could therefore be deleted another group of requirements that were ranked important or of little importance by the same or similar number of participants related to technical details of lake modelling practice step 3 lake model design and construction the requirement of determine whether to couple a catchment model with a lake model was scored 3 by seven participants and 4 or 5 by the other seven participants fig 3 a a catchment model can be useful in providing ungauged inflow estimates to a study lake or reservoir and thus can improve the accuracy of capturing hydrological patterns including lake residence time and nutrient and sediment loads to a lake mohammed et al 2019 the inclusion of a catchment model may however necessitate a substantial increase in project workload and budget we retained this requirement to allow stakeholders and lake modellers to decide on the advantages and disadvantages of including a catchment model another requirement of consider the sequence of model calibration for step 4 lake model calibration and testing was ranked as being of little importance by 6 participants but important by the other 8 participants fig 3b lake modelling calibration is widely recognised as an important step to adjust model parameters for reliable simulation outputs given that advection and dispersion critically influence the fate and transport of biochemical variables such as dissolved oxygen and nutrients an appropriate sequence of model calibration is to begin with water temperature and velocity comparisons to assure good hydrodynamic process representation although the requirement of sequential calibration may be less relevant to stakeholders than to modellers we kept this requirement in the best practice guidelines the requirement of using bulk pathogen decay rates to communicate model uncertainty for step 5 lake model application and uncertainty analysis was considered to be of little importance by seven participants but important by the other seven participants fig 3c pathogen decay rate is an important parameter in simulating the fate and transport of pathogens in lakes reservoirs but measured values for a specific pathogen can vary widely with environmental conditions which may not necessarily be captured accurately or at all in many models which use a fixed parameter value assigning a range of decay rates may better reflect the natural variability in pathogen decay rates and should be retained in the guidelines to communicate model uncertainty survey results also showed that 15 participants 83 agreed on the proposed six steps for best practice lake modelling while some participants provided additional steps for consideration fig 4 all the proposed additional steps were incorporated into the six step system seventeen participants 94 regarded developing the best practice guidelines as useful of which six participants thought they would be very useful and urgently needed fig 5 4 identified requirements and challenges for best practice guidelines for lake modelling a list of common requirements challenges faced by lake modellers to inform qmra was identified and refined through the three research activities the list has been agreed upon by a relatively small but diverse and representative group of lake model users around australia and can therefore form a basis of best practice guidelines for lake modelling to inform qmra the refined list of common requirements challenges for each of the six steps is provided below fig 6 4 1 step 1 project planning stakeholders and lake modellers should clarify the actual questions to be answered to inform qmra and identify the project objectives and intended uses of the model as recommended by harmel et al 2014 this is necessary to ensure that the right modelling framework is used and that the model is set up to answer those questions stakeholders usually use lake models to test their assumptions in that case stakeholders need to clarify what assumptions they want to test with consultation with lake modellers stakeholders should also determine project timelines budget and expertise required to build the model and keep modellers up to date on any changes to initial project objectives timelines and budgets it is strongly recommended that in the best practice guidelines the desired data requirements for developing a lake model should be outlined and that modellers communicate with stakeholders about the data requirements and help them decide on whether a lake model is the right tool and whether the desired data requirements are met for developing a lake model jones et al 2020 if the answer is yes to both questions ensure that all key area experts particularly on data analysis water quality and microbiology are fully involved at a necessary frequency as lake modelling is a multi disciplinary process all parties should determine how complex the model should be 1d vs 3d modellers should clarify model limitations and agree with stakeholders on what is achievable within the constraints of the project and the level of uncertainty that is acceptable in model output it is suggested in the complete best practice guidelines a checklist of key decisions be provided which may include 1 which pathogen groups viruses bacteria and protozoa to be simulated 2 1d or 3d lake model 3 linking a catchment model or not and 4 whether simulated sediment dynamics are required all parties should have a shared understanding of the characteristics of the study lake s and discuss the potential worst case scenarios for pathogen risk e g short circuiting low water level flood event in the catchment and dam operations they also need to agree on how to present model outputs to non modellers e g look up tables and identify ownership of the data and developed models at this stage 4 2 step 2 lake model conceptualisation conceptual lake models can be an extensive interface diagram or a simple pen and paper drawing depending on the complexity the model scope requires and are useful to capture our current understanding of the structure and processes of a lake system argent et al 2016 they can help engage stakeholders reach consensus and are often a first step of quantitative modelling model conceptualisation requires all parties to identify and discuss boundary conditions e g catchment inflow locations surrounding land uses water treatment plant offtake locations and meteorological conditions and key physical and biochemical processes of the study lake s to be represented in the conceptual model this discussion enables a range of stakeholders to better understand the lake modelling process and how the modelling outputs to be used to inform qmra following the characterisation of key processes in the conceptual model data availability and knowledge gaps associated with each process can be identified 1d and 3d lake models require similar types of input data but the data requirements for 3d models are much more extensive this distinction is not covered in existing guidelines modellers and stakeholders need to agree on model assumptions made to tackle the data and knowledge gaps gupta et al 2012 the model assumptions are often lake model specific such as the depth range of water treatment plant offtake catchment inflow temperature and hydro power dam operation to facilitate the process it is recommended that stakeholders set up a clear repository of monitoring data and that key stakeholders who have knowledge of the required data be involved the conceptual model should be progressively refined through an iterative process as a given qmra project progresses it is important to have clear ongoing communications as part of the modelling process and allocate appropriate project resources e g stakeholder time commitments 4 3 step 3 lake model design and construction this step is essentially about representing the conceptual model in mathematical terms with a suitable lake model the desired data requirements for the following step of model calibration and testing should be identified depending on the complexity and scope of the model stakeholders and modellers should then evaluate the availability and quality of data required for lake modelling based on the lake model conceptualisation if some data are lacking alternatives or methods to fill the gaps should be decided upon nutrient sediment and pathogen loads from the lake catchment often have substantial impacts on the lake conditions when monitoring data on the catchment are not adequate for lake model development a catchment model may provide an option to generate data synthetically but can significantly increase workload and budget although existing guidelines can be used to develop a catchment model e g engel et al 2007 harmel et al 2014 they provide little information on its coupling with a lake model the best practice guidelines for lake modelling should clearly demonstrate the benefits and shortcomings of developing a catchment model in the project and provide specific guidance to help make the decision pathogen decay rates are important model parameters and can be sourced from previous literature or lab experiment e g ahmed et al 2021 hipsey et al 2008 literature decay rates can however be variable so best practice should ensure whatever is used is justified and appropriate additionally locally measured pathogen decay rates should be interpreted in the light of international understanding it may be worthwhile to set up recommended upper and lower bound values of pathogen decay rates for different types of lakes so that lake modellers would have default pathogen decay rates as a starting point when calibrating a lake model in this step all parties should decide what management options scenarios should be considered to address project objectives climate change impacts on pathogen risk attract increasing attention from water utilities and inclusion of climate change study may affect the model design and type of required data 4 4 step 4 lake model calibration and testing for model calibration it is recommended that a sequential process is adopted starting with physical quantities such as temperature water level and water velocity before focusing on biochemical variables such as dissolved oxygen nutrients phytoplankton and pathogens lehmann and hamilton 2018 the rationale for this approach is to first attempt to calibrate the model to adequately simulate the physical processes that affect the fate and transport of biochemical processes while it is recognised that there is inherent feedback between biochemical and physical processes in a lake the strength of feedback from the biochemical to the physical processes is often weaker than vice versa this sequential approach can reduce the time required to achieve a satisfactory calibration but it should not seek to rigorously fix model parameters as iterative adjustments of physical and biochemical parameters may be useful for each quantity of interest qoi the initial order of calibration should be decided through preliminary sensitivity analysis to focus calibration on the most sensitive qois in the first instance the sensitivity analysis technique used can be either a simple one at a time approach or one of the many global methods that are available see for example saltelli et al 2008 with ongoing improvements in computational power combined global sensitivity analysis and parameter estimation techniques will increasingly be used for the calibration of these types of water quality models in the future it is important to consult with water quality and microbiology experts and stakeholders to determine the desired goodness of fit and or statistical results needed to meet the objectives of the health based target guidance for qmras some model performance measures and indicators as well as their common benchmarks as proposed in existing guidelines may be useful in the calibration of lake models but different characteristics e g thermal stratification of lakes and reservoirs to rivers and groundwater require model performance to be assessed throughout the whole water column appropriate spatial and temporal extent of data for lake model calibration and testing should also be identified for a new reservoir model application calibration and testing data may not be available alternative ways of validating model output include sensitivity analysis and use of data from reservoirs lakes that have similar geographical and morphological characteristics it is recommended to produce a report at this point and have it reviewed preferably by a third party with a suitable skill set and allow time for beneficial suggestions to be incorporated into the modelling effort the report should provide general modelling information for non modellers so they can understand boundary condition model extent etc with regards to their qmra objectives 4 5 step 5 lake model application and uncertainty analysis the purpose of this step is to apply developed lake models to answer questions posed in the project objectives such as understanding simulated pathogen concentrations and the associated uncertainty under different management scenarios stakeholders and modellers should agree on the ways of effectively presenting complex model outputs and uncertainties harmel et al 2014 as there is currently no accepted or agreed approach for this it is recommended that potential management options be related to resulting in lake pathogen decay rates this would assist non modellers and non microbiologists to understand the impact of different management options on the barrier effect of the study lake in addition the vertical dimension i e depth is a key characteristic of lakes and simulated pathogen concentrations are often distributed heterogeneously in the vertical direction due to thermal stratification it is thus suggested that presentation of lake model outputs account for the depth variable it is important to undertake uncertainty analysis to better assess pathogen risks petterson and ashbolt 2016 the analyses may include identifying potential source of uncertainty understanding and communicating uncertainty propagation e g additive or multiplicative significant uncertainty may exist in monitored reference pathogen concentrations by lake monitoring programs it is recommended to use a range of pathogen decay rates to reflect the natural variability in pathogen decay rates 4 6 step 6 final reporting and file archiving modellers should prepare a technical report describing different stages of model development and application for review the report should also clearly describe how model outputs address project objectives and what knowledge data gaps have been identified modellers are encouraged to be involved in post modelling process community meeting government discussion to help interpret and communicate model outputs limitations all model files and supporting data should be archived with a clear metadata statement of key inputs outputs to make results reproducible clear standards on what should be in a hand over package for qmra projects are still lacking in existing guidelines and should be set up in the complete guidelines a hand over package for a qmra project may include files that allow for the exact reproduction of the results presented in the model report such as all data used to create the lake model and the associated metadata e g the data source and date of capture the package should also document the software used in the modelling process including the version of the software if possible the software itself or the model executables should be included in the package it might be beneficial to introduce in the complete best practice guidelines the idea of transitioning from project to project models to a system of operational models if the end product was a well documented operational model of the system rather than an archived set of files then the investment in the modelling processes would be more cost effective in the long term note that communication and consultation should happen throughout the proposed six steps and that steps 2 5 usually occur in an iterative loop in combination with stakeholder consultation before the project is finalised step 6 5 discussion lake modelling is a valuable tool to provide pathogen data that complement field measurements to inform qmra activities however widely varying practices have been applied to lake model development by modellers who may have different perspectives and background on pathogen fate and transport most water utilities around the world lack in house modellers so this study is important for informing non specialist modellers including understanding and assurance about lake models that are being applied for key management decisions the outcomes of the stakeholder engagement activities completed as part of this research identified a need for a set of best practice guidelines for lake modelling to provide a basis for consistent and repeatable use of modelling approaches which provide quality assurance to water authorities and regulators this study takes the first step to identify the common lake modelling steps and associated requirements to be considered in guidelines for lake modelling to inform qmra given the broad generality in water modelling practice the six lake modelling steps identified in this study draw on material in recent guidelines particularly the australian groundwater modelling guidelines barnett et al 2012 and are consistent in principle with the earlier guidelines table 1 on the other hand considering the uniqueness of lake modelling practice for each modelling step we identified specific requirements directly relevant to lake modelling such as application of 1d or 3d lake models with associated data requirements whether or not to link to a catchment model and model calibration sequence this two step approach applied in this study enables the proposed guidelines to be consistent with the framework of existing guidelines easy for stakeholders and modellers to adopt and to also reflect the unique aspects of lake modelling practice in the development of the best practice guidelines for lake modelling it is important to include experiences and knowledge from groups that extend beyond those typically involved in the development and application of models and should include groups such as water utility stakeholders and water quality experts in this study we incorporated knowledge from lake modellers by summarising the common steps for water modelling from recent literature on best practices and included practical wisdom from water utility stakeholders by holding a stakeholder workshop to summarise model application steps and collate thinking on the challenges for each step additionally we considered knowledge from water quality experts by interviewing four scientists to seek solutions to the identified challenges and requirements importantly we also surveyed a representative group of experts in the water sector across the australian water industry on the proposed steps the inclusion of diverse groups of experts ensured the lake modelling steps are suitable and targeted to the main concerns in using lake models to inform qmra some of the identified requirements need more information to specify best practice guidance this can be achieved through extensive literature review and discussions with relevant parties for example the project administration practices proposed in black et al 2014 give detailed recommendations on how to develop baseline and key scenarios to address project objectives which is directly relevant to step 1 in this study similarly the conceptual modelling framework proposed in robinson 2008a b is relevant to step 2 lake model conceptualisation the list of available aquatic ecosystem models in janssen et al 2015 and discussion on selection of models based on level of complexity in hipsey et al 2015 are relevant to step 3 lake model design and construction model evaluation metrics in hipsey et al 2020 and sensitivity analysis practice in saltelli et al 2019 are relevant to step 4 lake model calibration and testing and the suggested general uncertainty sources for environmental modelling and proposed reporting structure in barnett et al 2012 provide a basis for developing step 5 lake model application and uncertainty analysis and step 6 final reporting and archiving lake models are undergoing continuous development including better representation of existing processes and adding newly identified processes frassl et al 2019 they also increasingly become publicly available through open source policies janssen et al 2015 coordinated community efforts supporting their application and visualisation technology such as r packages lakeensembler https github com aemon j lakeensemblr and dycdtools yu et al 2020 this study paves the way to develop a complete set of best practice guidelines for lake modelling to inform qmra the complete guidelines are expected to be a reference source rather than a rigid standard they will provide direction on the scope and approaches common to modelling projects and encourage the continual evolution of modelling techniques through adaptation and innovation 6 conclusions to meet the industry wide need for a set of best practice guidelines for lake modelling to support qmra this paper takes the first step to map out six common lake modelling steps and associated requirements to be considered in the guidelines these six steps are 1 project planning 2 lake model conceptualisation 3 lake model design and construction 4 lake model calibration and testing 5 lake model application and uncertainty analysis and 6 final reporting and file archiving the six steps were confirmed by the 83 of online survey participants more studies are needed to propose specific best practice guidance for identified requirements for each modelling step and to identify an appropriate format to present the completed document of best practice guidelines for lake modelling to support qmra declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this study was funded by the queensland water modelling network capacity development small grants marlene van der sterren kevin boland and sandya nanayakkara are gratefully acknowledged for critical comments on an earlier version of the manuscript we thank all participants of the workshop interviews and online survey for providing their opinions and suggestions the expert interviews and online survey involved in this study were approved by the ethics committee of griffith university reference number 2020 792 appendix a supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105334 
25613,environmental time series are often affected by missing data namely data unavailability at certain time points this paper presents the iterated imputation and prediction algorithm that allows the prediction of time series with missing data the algorithm uses iteratively the correlation dimension estimation of the underlying dynamic system generating the time series to fix the model order i e how many past samples are required to model the time series accurately and the support vector machine regression to estimate the skeleton of time series experimental validation of the algorithm on three environmental time series with missing data expressing the concentration of ozone in three european sites shows a small average percentage prediction error for all time series on the test set keywords missing data model order estimation grassberger procaccia algorithm correlation dimension estimation hough transform support vector machine regression 1 introduction a time series is a sequence of data enumerated in time order in environmental sciences time series are used to study how certain environmental measures e g air pollution data hooyberghs et al 2005 garcia nieto et al 2018 chianese et al 2019 ozone concentration sousa et al 2007 feng et al 2011 population of a given animal gerber et al 2015 plant growth paine et al 2012 evolve over time environmental time series are usually damaged by missing data namely by presence in time series of samples whose values are not known i e missing this is due to temporary unavailability caused for instance by failure of measurement devices that feed the time series the prediction of an environmental time series with missing data has relevance for a twofold reason the former is to get even in presence of missing data trustworthy future time series predictions the latter is to obtain a reliable estimation of the values of missing data of time series the prediction of time series involves to cope with two problems the former consists in estimating the model order of the time series i e how many past samples are required to model the time series reliably the latter is dealing with missing data methods that deal with missing data can be divided in two big families allison 2002 methods ignoring missing data and methods with imputation the former deletes the missing data but this approach cannot be applied to a time series since by doing so the temporal dependence among samples of time series would be removed the latter consists in making a data imputation namely in replacing each missing data with a known data imputed data fixed according to any method or technique this second strategy is the only missing data strategy that can be applied to a time series with missing data without deleting the temporal dependence among time series samples comprehensive and comparative overviews of data imputation methods can be found in lang and little 2016 osman et al 2018 junger and ponce de leon junger and ponce de leon 2015 proposed an expectation maximization em like strategy after having fixed the model order for the time series prediction with missing data in particular they proposed to make the data imputation iteratively using the predicted values by an arima model but they provided no method for fixing the model order of the time series this paper proposes the iterated imputation and prediction iip algorithm that enpowers junger and ponce de leon s method replacing arima model with support vector machine regression svmr for time series skeleton estimation and estimates iteratively the model order by using the novel grassberger procaccia hough gph algorithm summing up the main contributions of the papers are the iterated imputation and prediction and grassberger procaccia hough algorithms to the best of our knowledge both are novelties the paper is organized as follows section 2 describes how the model order of a time series can be fixed by means of grassberger procaccia hough algorithm that estimates the correlation dimension of the attractor 1 1 in dynamical systems an attractor is a set of numerical values toward a system tends to evolve asymptotically as time increases for a wide variety of initial conditions of the system ott 2002 of dynamic system that gives rise to time series in section 3 support vector machine regression is briefly reviewed section 4 describes the proposed iterated imputation and prediction algorithm section 5 presents some experimental results finally in section 6 some conclusions are drawn and future possible developments are discussed 2 model order computation by correlation dimension estimation a deterministic time series x t with t 1 ℓ can be described effectively by an autoregressive model as follows x t f x t 1 x t p 1 where f and p 1 are the so called skeleton and model order of time series respectively the model order i e the number of past samples required to model correctly the time series can be estimated through several methods hirshberg and merhav 1996 although cross validation duda et al 2001 hastie et al 2009 could be the simplest solution just picking the model order which gives the lowest prediction error it is generally unfeasible due to its computational cost an alternative and effective way for estimating the model order in time series camastra and filippone 2009 is provided by nonlinear dynamics methods eckmann and ruelle 1985 that allow the reconstruction of the underlying dynamic systems that generated the time series the so called model reconstruction of time series can be obtained by using methods of delays packard et al 1980 using this approach the time series can be represented as a set of points ω x t x t x t x t 1 x t p 1 t p ℓ in a p dimensional space if p is large enough there is a diffeomorphism between the manifold 2 2 a manifold in math is a space in which every point has a neighborhood which resembles euclidean space but in which the global structure may be more complicated in a one dimensional manifold every point has a neighborhood that looks like a line segment in a two dimensional manifold the neighborhood looks like a disk r n is a n dimensional manifold m so generated by ω and the attractor u of the underlying dynamic system that generated the time series x t i e a map φ m u and its inverse φ 1 exist and both of them are differentiable if a diffeomorphism exists between m and u then they share the same mathematical properties and it is adequate to study the manifold m to get all the information about the dynamic system that generated the time series the takens embedding theorem mañé 1981 takens 1981 asserts that to construct a diffeomorphism between m and u the following inequality must hold 1 2 d u 1 p where d u denotes the dimension of the attractor u and p is called the embedding dimension of the system ott 2002 therefore it is adequate to estimate the correlation dimension d u to infer the value of the model order p 1 that can be fixed equal to 2 d u i e p 1 2 d u among several definitions of dimension of a set that are available in the literature see camastra and staiano 2016 for a review the most popular is the box counting dimension ott 2002 defined as follows let s x 1 x ℓ be a set of points in r n ν r denotes the number of the boxes of size r required to cover s the box counting dimension or kolmogorov capacity d s is given by 2 d s lim r 0 ln ν r ln r the computation of box counting dimension can require huge computational resources therefore it is generally replaced by correlation dimension grassberger and procaccia 1983 that is quite easy to compute the correlation dimension of a set s is defined as follows let the correlation integral c r be defined as follows 3 c r lim ℓ 1 ℓ 2 i 1 ℓ j i 1 ℓ i x j x i r where i is an indicator function i e its value is 1 if the condition is fulfilled 0 otherwise and r is a distance simply the correlation integral computes the percentage of point pairs whose mutual distance is not larger than r then the correlation dimension d s is given by 4 d s lim r 0 ln c r ln r eckmann and ruelle 1992 proved that to get an unbiased estimate of the correlation dimension the s cardinality ℓ should fulfill the so called eckmann ruelle s inequality 5 d s 2 log 10 ℓ it can be proven ott 2002 that the correlation dimension is a lower bound of box counting dimension and in many cases their values are the same or very close however if the attractor is multifractal box counting and correlation dimension might differ and therefore replacing box counting dimension with correlation dimension might slightly underestimate the attractor dimension and consequently the model order of time series for the sake of correctness in the rest of the manuscript it is assumed that the attractor of the dynamic system generating the time series is not multifractal a popular method to estimate correlation dimension is the grassberger procaccia algorithm gp algorithm grassberger and procaccia 1983 that consists in plotting ln c r versus ln r see fig 1 the correlation dimension is given by the slope of the linear portion of the curve manually detected camastra et al 2018 in this paper it proposes to perform the computation of the correlation dimension by a slightly modified version of hough transform for lines trucco and verri 1998 that takes as input the log log plot and returns the slope of the line that receives the maximum of votes in the accumulation matrix of the hough transform therefore we replace gp algorithm with the so defined grassberger procaccia hough gph algorithm whose pseudocode is as follows image 1 finally we proceed to discuss the computational complexity of grassberger procaccia hough algorithm the computational complexity of the log log plot i e gp algorithm is o ℓ 2 ns where ℓ and n denote the cardinality and the dimensionality of the data set s respectively and s the number of times that the correlation integral is computed the loop at line six has a complexity o str where t and r are the number of samples of θ and ρ defined at lines three and four respectively to find the slope of the line it is necessary to find the maximum that requires a complexity o tr if we consider a fixed resolution i e t and r are constant the complexity can be considered costant and it can be omitted hence the overall complexity of gph algorithm reduces to o ℓ 2 sn s since the latter term is negligible w r t the former it deduces that grassberger procaccia hough algorithm has the same complexity of gp algorithm an implementation in c language of grassberger procaccia hough algorithm is available at https github com vincap modelorderestimationgph 3 support vector machine regression the skeleton f of the time series can be estimated by support vector machine svm regression vapnik 1998 we briefly describe support vector machine regression in the case of quadratic ε insensitive loss cristianini and shawe taylor 2000 given a data set d x 1 y 1 x ℓ y ℓ where x 1 x ℓ r n the task consists in estimating the function f r n r that takes x as input and yields y as output if it is assumed that f is an hyperplane it can be described by f x w x b to solve the problem the approach of computing the optimal hyperplane in svm for classification cristianini and shawe taylor 2000 is used therefore the following functional is minimized 6 τ w 1 2 w 2 c i 1 ℓ y i f x i ε 2 where ε 2 is the quadratic ε insensitive loss 3 3 u ε 2 is u 2 if u ε 0 otherwise and c is the regularization constant that controls the trade off between the former term namely the margin and the latter one i e a quadratic ε insensitive loss it is worth noting that the presence of both the ε insensitive loss and the regularization constant makes svm regression quite robust w r t overfitting since all errors that are lower than ε are neglected the minimization problem of equation 6 can be solved by kuhn tucker theorem kuhn and tucker 1951 obtaining after some mathematical steps the following formulation 7 max α i 1 ℓ y i α i ε i 1 ℓ α i 1 2 i 1 ℓ j 1 ℓ α i α j x i x j 1 c δ i j s u b j e c t t o i 1 ℓ α i 0 the algorithm above can be reinforced by kernel trick schölkopf and smola 2002 namely by replacing the inner product x i x j in equation 7 with k x i x j where k is a proper mercer kernel vapnik 1998 yielding the final form 8 max α i 1 ℓ y i α i ε i 1 ℓ α i 1 2 i 1 ℓ j 1 ℓ α i α j k x i x j 1 c δ i j s u b j e c t t o i 1 ℓ α i 0 the vectors whose respective multipliers α i are non null are called support vectors justifying the name of the algorithm svm regression implements the following regressor f x i 1 ℓ α i k x i x b examples of mercer kernels used in svm regression are the so called linear k x i x j x i x j and gaussian k x i x j exp x i x j 2 σ 2 with σ r in this work svm regression trials have been carried out with the svmlight joachim 1999 software package nevertheless other svm software packages could be used e g libsvm chang and lin 2011 or scikit pedregosa et al 2011 4 iterated imputation and prediction algorithm the iterated imputation and prediction iip algorithm allows the time series prediction with missing data after an initialization stage iip uses an em like strategy i e it computes iteratively the model order by using the gph algorithm and performs the data imputation by using the predicted values of the time series by svmr the iip algorithm can be summarized as follows in the initialization step that can be performed by using one of the data imputation strategies allison 2002 osman et al 2018 a first imputation of missing data is performed in this paper the initialization of the missing data has been performed by a linear interpolation between the last known time series sample coming before the missing data and the first known time series sample after the missing data then at each iteration the following actions are done 1 the model order of the time series is computed by grassberger procaccia hough algorithm 2 the prediction by svm regression svmr is performed 3 if the test error decreases w r t the previous iteration missing data imputations are updated with the predicted values by svmr and a further iteration restarts otherwise the loop terminates returning the predicted time series and the test error computed in the previous stage the pseudocode of the iip algorithm is reported below image 2 it must be underlined that unlike em algorithms dempster et al 1977 the iip algorithm does not guarantee that a local minimum of the test error is reached 5 experimental results the proposed algorithm has been validated on m356 m395 and m778 4 4 the data are available on request for further investigations and research environmental time series that measure hourly the concentration of ozone expressed in μg m 3 in three different european atmospheric stations for one year the missing data characteristics of m356 m395 and m778 time series are shown in table 1 the time series were divided in training and test sets in all time series the training and the test set were formed by 28 and 24 weeks respectively all of the missing data were concentrated in the training set the skeleton of all time series was estimated by svm regression svmr for m356 and m778 time series a svmr with a linear kernel was used whereas for m778 time series a svmr with a gaussian kernel was applied the svmr parameters i e ε c σ were estimated by cross validation duda et al 2001 hastie et al 2009 the iip algorithm was applied to m356 m395 and m778 time series model orders for all time series computed by correlation dimension estimation are described in table 2 it is worthwhile remarking that all estimates of the correlation dimensions fulfill the so called eckmann ruelle s inequality see section 2 thus guaranteeing that both correlation dimensions and consequently the respective model orders are not underestimated in order to prove the determinism of the time series the scheinkman le baron test scheinkman and le baron 1989 was applied to m356 m395 and m778 time series all time series were permuted randomly destroying in this way any time dependency in the time series after that the correlation dimension was measured again and as shown in table 3 a much higher value was obtained thus confirming what expected by the scheinkman le baron test the performance of iip on test set for m356 m395 and m778 time series are reported in table 4 table 5 and table 6 respectively the overall average percentage error for m356 m395 and m778 time series are 0 45 1 17 and 4 70 in fig 2 the predictions for a full week of m356 m395 and m778 are reported finally in order to assess how much the imputation data affects the time series prediction we have considered paris 14e parc montsouris time series wijngaard et al 2003 widely discussed in camastra and filippone 2009 we have picked only the first 8760 samples generating a paris 14e parc montsouris time series whose length is the same of the m356 m395 and m778 time series then we generated four new time series with missing data denoted by paris missing1 paris missing2 paris missing3 and paris missing4 with the same length of paris 14e parc montsouris time series the missing data characteristics of four new time series with missing data are shown in table 7 we used for paris 14e parc montsouris time series and all missing data time series the same experimental protocol all time series were divided in training and test sets in the same manner all of the missing data were concentrated in the training set the prediction of paris 14e parc montsouris the skeleton and the model order of the time series were estimated by using the svmr and grassberger procaccia hough algorithms respectively whereas the predictions of missing data time series were performed by using iip algorithm for all time series predictions the svmr parameters were estimated by cross validation the performance on test sets for all time series measured in term of mean square error and mean absolute error are reported in table 8 since the differences for both indicators for all time series are not statistically significant we can conclude that the effect of data imputation on time series prediction is negligible even when the percentage of missing data on the training set is more than 30 6 conclusions in this paper the iterated imputation and prediction iip algorithm that allows predicting time series affected by missing data is described the iip algorithm uses iteratively the correlation dimension estimation of the attractor of the underlying dynamic system generating the time series to compute the time series model order and support vector machine regression to estimate the time series skeleton tests of the algorithm on three time series with missing data expressing the concentration of ozone in three different european sites show a small average percentage prediction error for all time series on the test sets having no missing data as future work we plan to investigate for the time series skeleton estimation the possible replacement of svm regression with gaussian processes williams and rasmussen 2006 or deep neural networks goodfellow et al 2017 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements first we would like to thank the anonymous reviewers and frances mary donegan for the useful comments and the proofreading the manuscript respectively vincenzo capone developed part of the work as his final dissertation for b sc in computer science under the supervision of francesco camastra at university parthenope of naples francesco camastra and antonino staiano were funded by ffabr 2017 program by anvur italian national agency for the evaluation of universities and research institutes 
25613,environmental time series are often affected by missing data namely data unavailability at certain time points this paper presents the iterated imputation and prediction algorithm that allows the prediction of time series with missing data the algorithm uses iteratively the correlation dimension estimation of the underlying dynamic system generating the time series to fix the model order i e how many past samples are required to model the time series accurately and the support vector machine regression to estimate the skeleton of time series experimental validation of the algorithm on three environmental time series with missing data expressing the concentration of ozone in three european sites shows a small average percentage prediction error for all time series on the test set keywords missing data model order estimation grassberger procaccia algorithm correlation dimension estimation hough transform support vector machine regression 1 introduction a time series is a sequence of data enumerated in time order in environmental sciences time series are used to study how certain environmental measures e g air pollution data hooyberghs et al 2005 garcia nieto et al 2018 chianese et al 2019 ozone concentration sousa et al 2007 feng et al 2011 population of a given animal gerber et al 2015 plant growth paine et al 2012 evolve over time environmental time series are usually damaged by missing data namely by presence in time series of samples whose values are not known i e missing this is due to temporary unavailability caused for instance by failure of measurement devices that feed the time series the prediction of an environmental time series with missing data has relevance for a twofold reason the former is to get even in presence of missing data trustworthy future time series predictions the latter is to obtain a reliable estimation of the values of missing data of time series the prediction of time series involves to cope with two problems the former consists in estimating the model order of the time series i e how many past samples are required to model the time series reliably the latter is dealing with missing data methods that deal with missing data can be divided in two big families allison 2002 methods ignoring missing data and methods with imputation the former deletes the missing data but this approach cannot be applied to a time series since by doing so the temporal dependence among samples of time series would be removed the latter consists in making a data imputation namely in replacing each missing data with a known data imputed data fixed according to any method or technique this second strategy is the only missing data strategy that can be applied to a time series with missing data without deleting the temporal dependence among time series samples comprehensive and comparative overviews of data imputation methods can be found in lang and little 2016 osman et al 2018 junger and ponce de leon junger and ponce de leon 2015 proposed an expectation maximization em like strategy after having fixed the model order for the time series prediction with missing data in particular they proposed to make the data imputation iteratively using the predicted values by an arima model but they provided no method for fixing the model order of the time series this paper proposes the iterated imputation and prediction iip algorithm that enpowers junger and ponce de leon s method replacing arima model with support vector machine regression svmr for time series skeleton estimation and estimates iteratively the model order by using the novel grassberger procaccia hough gph algorithm summing up the main contributions of the papers are the iterated imputation and prediction and grassberger procaccia hough algorithms to the best of our knowledge both are novelties the paper is organized as follows section 2 describes how the model order of a time series can be fixed by means of grassberger procaccia hough algorithm that estimates the correlation dimension of the attractor 1 1 in dynamical systems an attractor is a set of numerical values toward a system tends to evolve asymptotically as time increases for a wide variety of initial conditions of the system ott 2002 of dynamic system that gives rise to time series in section 3 support vector machine regression is briefly reviewed section 4 describes the proposed iterated imputation and prediction algorithm section 5 presents some experimental results finally in section 6 some conclusions are drawn and future possible developments are discussed 2 model order computation by correlation dimension estimation a deterministic time series x t with t 1 ℓ can be described effectively by an autoregressive model as follows x t f x t 1 x t p 1 where f and p 1 are the so called skeleton and model order of time series respectively the model order i e the number of past samples required to model correctly the time series can be estimated through several methods hirshberg and merhav 1996 although cross validation duda et al 2001 hastie et al 2009 could be the simplest solution just picking the model order which gives the lowest prediction error it is generally unfeasible due to its computational cost an alternative and effective way for estimating the model order in time series camastra and filippone 2009 is provided by nonlinear dynamics methods eckmann and ruelle 1985 that allow the reconstruction of the underlying dynamic systems that generated the time series the so called model reconstruction of time series can be obtained by using methods of delays packard et al 1980 using this approach the time series can be represented as a set of points ω x t x t x t x t 1 x t p 1 t p ℓ in a p dimensional space if p is large enough there is a diffeomorphism between the manifold 2 2 a manifold in math is a space in which every point has a neighborhood which resembles euclidean space but in which the global structure may be more complicated in a one dimensional manifold every point has a neighborhood that looks like a line segment in a two dimensional manifold the neighborhood looks like a disk r n is a n dimensional manifold m so generated by ω and the attractor u of the underlying dynamic system that generated the time series x t i e a map φ m u and its inverse φ 1 exist and both of them are differentiable if a diffeomorphism exists between m and u then they share the same mathematical properties and it is adequate to study the manifold m to get all the information about the dynamic system that generated the time series the takens embedding theorem mañé 1981 takens 1981 asserts that to construct a diffeomorphism between m and u the following inequality must hold 1 2 d u 1 p where d u denotes the dimension of the attractor u and p is called the embedding dimension of the system ott 2002 therefore it is adequate to estimate the correlation dimension d u to infer the value of the model order p 1 that can be fixed equal to 2 d u i e p 1 2 d u among several definitions of dimension of a set that are available in the literature see camastra and staiano 2016 for a review the most popular is the box counting dimension ott 2002 defined as follows let s x 1 x ℓ be a set of points in r n ν r denotes the number of the boxes of size r required to cover s the box counting dimension or kolmogorov capacity d s is given by 2 d s lim r 0 ln ν r ln r the computation of box counting dimension can require huge computational resources therefore it is generally replaced by correlation dimension grassberger and procaccia 1983 that is quite easy to compute the correlation dimension of a set s is defined as follows let the correlation integral c r be defined as follows 3 c r lim ℓ 1 ℓ 2 i 1 ℓ j i 1 ℓ i x j x i r where i is an indicator function i e its value is 1 if the condition is fulfilled 0 otherwise and r is a distance simply the correlation integral computes the percentage of point pairs whose mutual distance is not larger than r then the correlation dimension d s is given by 4 d s lim r 0 ln c r ln r eckmann and ruelle 1992 proved that to get an unbiased estimate of the correlation dimension the s cardinality ℓ should fulfill the so called eckmann ruelle s inequality 5 d s 2 log 10 ℓ it can be proven ott 2002 that the correlation dimension is a lower bound of box counting dimension and in many cases their values are the same or very close however if the attractor is multifractal box counting and correlation dimension might differ and therefore replacing box counting dimension with correlation dimension might slightly underestimate the attractor dimension and consequently the model order of time series for the sake of correctness in the rest of the manuscript it is assumed that the attractor of the dynamic system generating the time series is not multifractal a popular method to estimate correlation dimension is the grassberger procaccia algorithm gp algorithm grassberger and procaccia 1983 that consists in plotting ln c r versus ln r see fig 1 the correlation dimension is given by the slope of the linear portion of the curve manually detected camastra et al 2018 in this paper it proposes to perform the computation of the correlation dimension by a slightly modified version of hough transform for lines trucco and verri 1998 that takes as input the log log plot and returns the slope of the line that receives the maximum of votes in the accumulation matrix of the hough transform therefore we replace gp algorithm with the so defined grassberger procaccia hough gph algorithm whose pseudocode is as follows image 1 finally we proceed to discuss the computational complexity of grassberger procaccia hough algorithm the computational complexity of the log log plot i e gp algorithm is o ℓ 2 ns where ℓ and n denote the cardinality and the dimensionality of the data set s respectively and s the number of times that the correlation integral is computed the loop at line six has a complexity o str where t and r are the number of samples of θ and ρ defined at lines three and four respectively to find the slope of the line it is necessary to find the maximum that requires a complexity o tr if we consider a fixed resolution i e t and r are constant the complexity can be considered costant and it can be omitted hence the overall complexity of gph algorithm reduces to o ℓ 2 sn s since the latter term is negligible w r t the former it deduces that grassberger procaccia hough algorithm has the same complexity of gp algorithm an implementation in c language of grassberger procaccia hough algorithm is available at https github com vincap modelorderestimationgph 3 support vector machine regression the skeleton f of the time series can be estimated by support vector machine svm regression vapnik 1998 we briefly describe support vector machine regression in the case of quadratic ε insensitive loss cristianini and shawe taylor 2000 given a data set d x 1 y 1 x ℓ y ℓ where x 1 x ℓ r n the task consists in estimating the function f r n r that takes x as input and yields y as output if it is assumed that f is an hyperplane it can be described by f x w x b to solve the problem the approach of computing the optimal hyperplane in svm for classification cristianini and shawe taylor 2000 is used therefore the following functional is minimized 6 τ w 1 2 w 2 c i 1 ℓ y i f x i ε 2 where ε 2 is the quadratic ε insensitive loss 3 3 u ε 2 is u 2 if u ε 0 otherwise and c is the regularization constant that controls the trade off between the former term namely the margin and the latter one i e a quadratic ε insensitive loss it is worth noting that the presence of both the ε insensitive loss and the regularization constant makes svm regression quite robust w r t overfitting since all errors that are lower than ε are neglected the minimization problem of equation 6 can be solved by kuhn tucker theorem kuhn and tucker 1951 obtaining after some mathematical steps the following formulation 7 max α i 1 ℓ y i α i ε i 1 ℓ α i 1 2 i 1 ℓ j 1 ℓ α i α j x i x j 1 c δ i j s u b j e c t t o i 1 ℓ α i 0 the algorithm above can be reinforced by kernel trick schölkopf and smola 2002 namely by replacing the inner product x i x j in equation 7 with k x i x j where k is a proper mercer kernel vapnik 1998 yielding the final form 8 max α i 1 ℓ y i α i ε i 1 ℓ α i 1 2 i 1 ℓ j 1 ℓ α i α j k x i x j 1 c δ i j s u b j e c t t o i 1 ℓ α i 0 the vectors whose respective multipliers α i are non null are called support vectors justifying the name of the algorithm svm regression implements the following regressor f x i 1 ℓ α i k x i x b examples of mercer kernels used in svm regression are the so called linear k x i x j x i x j and gaussian k x i x j exp x i x j 2 σ 2 with σ r in this work svm regression trials have been carried out with the svmlight joachim 1999 software package nevertheless other svm software packages could be used e g libsvm chang and lin 2011 or scikit pedregosa et al 2011 4 iterated imputation and prediction algorithm the iterated imputation and prediction iip algorithm allows the time series prediction with missing data after an initialization stage iip uses an em like strategy i e it computes iteratively the model order by using the gph algorithm and performs the data imputation by using the predicted values of the time series by svmr the iip algorithm can be summarized as follows in the initialization step that can be performed by using one of the data imputation strategies allison 2002 osman et al 2018 a first imputation of missing data is performed in this paper the initialization of the missing data has been performed by a linear interpolation between the last known time series sample coming before the missing data and the first known time series sample after the missing data then at each iteration the following actions are done 1 the model order of the time series is computed by grassberger procaccia hough algorithm 2 the prediction by svm regression svmr is performed 3 if the test error decreases w r t the previous iteration missing data imputations are updated with the predicted values by svmr and a further iteration restarts otherwise the loop terminates returning the predicted time series and the test error computed in the previous stage the pseudocode of the iip algorithm is reported below image 2 it must be underlined that unlike em algorithms dempster et al 1977 the iip algorithm does not guarantee that a local minimum of the test error is reached 5 experimental results the proposed algorithm has been validated on m356 m395 and m778 4 4 the data are available on request for further investigations and research environmental time series that measure hourly the concentration of ozone expressed in μg m 3 in three different european atmospheric stations for one year the missing data characteristics of m356 m395 and m778 time series are shown in table 1 the time series were divided in training and test sets in all time series the training and the test set were formed by 28 and 24 weeks respectively all of the missing data were concentrated in the training set the skeleton of all time series was estimated by svm regression svmr for m356 and m778 time series a svmr with a linear kernel was used whereas for m778 time series a svmr with a gaussian kernel was applied the svmr parameters i e ε c σ were estimated by cross validation duda et al 2001 hastie et al 2009 the iip algorithm was applied to m356 m395 and m778 time series model orders for all time series computed by correlation dimension estimation are described in table 2 it is worthwhile remarking that all estimates of the correlation dimensions fulfill the so called eckmann ruelle s inequality see section 2 thus guaranteeing that both correlation dimensions and consequently the respective model orders are not underestimated in order to prove the determinism of the time series the scheinkman le baron test scheinkman and le baron 1989 was applied to m356 m395 and m778 time series all time series were permuted randomly destroying in this way any time dependency in the time series after that the correlation dimension was measured again and as shown in table 3 a much higher value was obtained thus confirming what expected by the scheinkman le baron test the performance of iip on test set for m356 m395 and m778 time series are reported in table 4 table 5 and table 6 respectively the overall average percentage error for m356 m395 and m778 time series are 0 45 1 17 and 4 70 in fig 2 the predictions for a full week of m356 m395 and m778 are reported finally in order to assess how much the imputation data affects the time series prediction we have considered paris 14e parc montsouris time series wijngaard et al 2003 widely discussed in camastra and filippone 2009 we have picked only the first 8760 samples generating a paris 14e parc montsouris time series whose length is the same of the m356 m395 and m778 time series then we generated four new time series with missing data denoted by paris missing1 paris missing2 paris missing3 and paris missing4 with the same length of paris 14e parc montsouris time series the missing data characteristics of four new time series with missing data are shown in table 7 we used for paris 14e parc montsouris time series and all missing data time series the same experimental protocol all time series were divided in training and test sets in the same manner all of the missing data were concentrated in the training set the prediction of paris 14e parc montsouris the skeleton and the model order of the time series were estimated by using the svmr and grassberger procaccia hough algorithms respectively whereas the predictions of missing data time series were performed by using iip algorithm for all time series predictions the svmr parameters were estimated by cross validation the performance on test sets for all time series measured in term of mean square error and mean absolute error are reported in table 8 since the differences for both indicators for all time series are not statistically significant we can conclude that the effect of data imputation on time series prediction is negligible even when the percentage of missing data on the training set is more than 30 6 conclusions in this paper the iterated imputation and prediction iip algorithm that allows predicting time series affected by missing data is described the iip algorithm uses iteratively the correlation dimension estimation of the attractor of the underlying dynamic system generating the time series to compute the time series model order and support vector machine regression to estimate the time series skeleton tests of the algorithm on three time series with missing data expressing the concentration of ozone in three different european sites show a small average percentage prediction error for all time series on the test sets having no missing data as future work we plan to investigate for the time series skeleton estimation the possible replacement of svm regression with gaussian processes williams and rasmussen 2006 or deep neural networks goodfellow et al 2017 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements first we would like to thank the anonymous reviewers and frances mary donegan for the useful comments and the proofreading the manuscript respectively vincenzo capone developed part of the work as his final dissertation for b sc in computer science under the supervision of francesco camastra at university parthenope of naples francesco camastra and antonino staiano were funded by ffabr 2017 program by anvur italian national agency for the evaluation of universities and research institutes 
25614,terrestrial laser scanning tls devices show great potential for application in forest inventories fis as they are capable of registering high resolution point clouds rapidly and automatically nevertheless operational use of tls for fi purposes has been hampered by the absence of algorithms for processing the acquired data particularly in the single scan mode as occlusions result in loss of information the r package fortls has been developed to overcome this obstacle as it automates the processing of single scan tls point cloud data for forestry purposes and includes several features that deal with occlusions fortls makes use of the main advantage of the single scan scenario in fi thus improving the efficiency of data acquisition and post processing all of these features of the fortls package are potentially valuable for the operational use of tls in fis in combination with inference techniques derived from model based and model assisted approaches keywords forest monitoring forest stands parameters lidar precision forestry remote sensing terrestrial based technologies 1 introduction information about forest resources is essential for sustainable forest management and development of forest policies and forest inventories fis are fundamental for estimating and monitoring the state and evolution of forest resources at both global and regional scales tomppo et al 2010 fis have improved since they were first introduced owing to the continuous appearance of new technologies especially in the last few decades since the emergence of remote and proximal sensing in this technological context light detection and ranging lidar systems provide 3 dimensional point clouds which are suitable for estimating tree attributes and are very useful for many forestry applications dubayah and drake 2000 this technology has proved operationally viable for estimating essential fi variables at stand level such as arithmetic mean height h m basal area g m2 ha 1 and volume v m3 ha 1 with airborne laser scanning als devices wulder et al 2012 white et al 2016 terrestrial laser scanning tls devices such as lidar devices with millimetric precision are considered to show great potential for enhancing fis dassot et al 2011 white et al 2016 and also forest ecology research calders et al 2020 danson et al 2018 apart from much higher spatial resolution under canopy the main advantages of using tls data rather than als data are better observation of near ground vegetation white et al 2016 and thus better trunk coverage for estimating the woody component which is one of the most important components in fis in fact tls based approaches can provide very accurate estimates of the diameter at breast height dbh measured at 1 3 m from the ground and the stem curve with the single scan approach yielding values of 1 4 and 1 3 6 cm respectively depending on the stand conditions liang et al 2018a which are close to the values required in practical applications such as fis however tls devices have not been yet adopted in fis for several reasons i difficulties in the automation of data processing to provide reliable measurements of important forest variables ii high acquisition costs iii limited software and iv lack of trained personnel liang et al 2016 many researchers agree that affordability is the main key challenge to overcome emphasizing that automation of point cloud processing with attainable and easy to use software able to extract information related to important forest attributes is essential dassot et al 2011 newnham et al 2015 white et al 2016 liang et al 2016 2018a as tls data sets comprise millions of points sophisticated methods for automatic processing are required many algorithms with a high level of automation and that are able to extract tree attributes such as dbh total height h m and stem volume v m3 have been developed in the last few decades cabo et al 2018 liang et al 2012 2018b olofsson et al 2014 olofsson and holmgren 2016 zhang et al 2019 although most algorithms yield acceptable dbh and stem curve estimations according to fi requirements stem detection and estimation of h cause bottlenecks in the process especially with single scans krok et al 2020 liang et al 2018a some of the algorithms developed have also been included in software applications such as simpleforest hackenberg et al 2015 3d forest trochta et al 2017 and autostem bienert et al 2007 among others krok et al 2020 however these programs have some drawbacks for use in fis i they focus on single tree rather than stand level approaches simpleforest ii they involve semi automatic processing 3d forest and iii the software is only available commercially autostem i e it is not free or open source furthermore the previous studies have mainly focused on replicating plot based measurements which does not extend conventional inventory approaches from a sampling perspective and thus limits the utility of tls in fis the main purpose of which is to estimate important forest variables at larger scales e g stand and regional than tree and plot levels newnham et al 2015 white et al 2016 methods that enable tls to be used for fi purposes must therefore contemplate the use of different approaches newnham et al 2015 and other procedures in which not all trees in the sample plots are measured may be feasible liang et al 2018a thus further research is required to address the challenges in the operational use of tls liang et al 2018a here we present fortls molina valero et al 2021 an r package developed with the objective of automating tls point cloud data processing and estimating variables for forestry purposes to fulfil this objective fortls enables i detection of trees and estimation of dbh and other tree attributes ii estimation of some stand variables e g h g v iii computation of metrics related to important tree attributes estimated in fis at stand level and iv optimization of plot design for combining tls data and field measured data the package also includes several features for correcting occlusion problems to improve the estimation of stand variables the current version of fortls is based on single scan tls data with the aim of facilitating operational use and it has been designed as relatively easy to use open source software aimed at use by both scientists and technical users relative to multi scan and multi single scan approaches the single scan approach improves data acquisition shortens the processing time and increases the sample size in a cost efficient manner mainly because it does not require pre scanning tasks involving location of artificial reference objects holopainen et al 2014 or automated post processing matching methods liu et al 2017 finally and as a case study we have tested fortls for estimating common forestry variables in an experimental plot of 1 ha located in an even aged pinus sylvestris l stand in northern spain these features of the fortls package may enable the operational use of tls in fis in combination with model based or model assisted inference approaches 2 methods 2 1 software design fortls molina valero et al 2021 has been developed as an r package r core team 2021 because r is free statistical software which is accessible to any user interested in this tool the initial stages of development of this package were outlined in molina valero et al 2020 although the first version of fortls was not available until march 2021 currently both the most recent stable version of the package and the most up to date version can be downloaded free of charge from respectively the cran https cran r project org package fortls and github development https github com molina valero fortls tree devel repositories the r package fortls has been optimized by implementing c code in the most demanding computing processes by means of the rcpp package eddelbuettel 2013 eddelbuettel and balamuta 2018 eddelbuettel and françois 2011 and the rcppeigen package bates and eddelbuettel 2013 which enables integration of the eigen c library for specific matrix calculation for operations with objects in spatial data classes both the raster hijmans 2020 and sp bivand et al 2013 pebesma and bivand 2005 packages have been used for obtaining voronoi polygons the ggvoronoi package garrett et al 2021 has been used in the simulations and metrics variables functions as tls point clouds represent large data sets fortls also imports the vroom package hester and wickham 2020 for accelerating loading and saving txt files we have used other important packages to generate and save interactive graphics namely plotly sievert 2020 and htmlwidgets vaidyanathan et al 2020 apart from the packages included in r base distribution and other accessory packages such as progress csárdi and fitzjohn 2019 scales wickham and seidel 2020 and tidyr wickham 2021 the other external r packages used for more specific functions are mentioned below with their respective functions the functions and results compiled in this work are based on the stable version 1 0 6 of the fortls package available in cran in the following sections all steps involved in tls point cloud data processing with fortls as well as the most relevant algorithms are described i normalization ii tree detection and iii estimation of metrics and variables at stand level 2 1 1 normalization the normalization process is a necessary first step in processing point cloud data and it is implemented in the normalize function table 1 which for some processes uses the functions readlas clip circle classify ground grid terrain and normalize height included in the lidr package roussel et al 2020 roussel and auty 2020 normalization involves obtaining the coordinates relative to plot centre for tls point clouds supplied as las or laz files the process includes the following steps i classification of points as ground ii generation of a digital terrain model dtm iii computation of coordinates relative to dtm cartesian cylindrical and spherical and iv reduction of point cloud density by the point cropping process pcp in the initial step points are classified as ground or not ground with the cloth simulation filter csf algorithm zhang et al 2016 the dtm is then generated by spatial interpolation of ground points two methods are available for executing this process i spatial interpolation based on delaunay triangulation by default and ii spatial interpolation using a k nearest neighbour approach with inverse distance weighting the point cloud is then normalized by subtracting the dtm created once the point cloud has been normalized cartesian cylindrical and spherical coordinates are calculated relative to the sampling point tls device establishment point finally the normalize function applies the pcp algorithm developed by molina valero et al 2019 to reduce the point density and thus produce a spatially homogeneous point cloud in which the distribution of points is proportional to the object size during execution of the pcp a selection probability p r o b p is assigned to each cloud point p according to eq 1 fig 1 1 p r o b p r p r m a x where r p is the radial distance of the point p from the plot centre and r m a x is the radial distance of the farthest point from the plot centre finally the point is selected if the selection probability is equal to or higher than a random value generated from the uniform distribution for the interval 0 1 2 1 2 tree detection tree detection represents a very important and challenging step in estimating variables of interest in tls assisted fis this is partly due to occlusions which are much more important when working with single scans this process of tree detection is implemented in the tree detection function table 1 which has been designed to detect as many trees as possible from point cloud data obtained as normalize function output in the tree detection process one or several horizontal slices from the original point cloud are extracted slices at heights of 1 0 1 3 and 1 6 m 5 cm over the terrain all around 1 3 m as reference section to estimate dbh are considered by default in tree detection the probability of detecting trees increases when more than one slice is considered however the tree detection function can also extract if specified in the arguments slices at other heights when trees are not identifiable at these pre established values each horizontal slice is processed by algorithms that are able to i remove branches and foliage points ii detect point clusters corresponding to potential tree sections and iii classify detected point clusters as tree sections or not according to several tests the tree sections detected for all horizontal slices are then merged and for each tree detected the tree detection function estimates certain attributes by i calculating the coordinates corresponding to tree normal section centre 1 3 m above ground level and its horizontal distance from plot centre ii estimating the dbh iii classifying the tree as fully visible or partially occluded and iv obtaining the number of points corresponding to normal section slice 1 3 m 5 cm for both original and reduced by applying pcp point clouds the main steps involved in the previously mentioned algorithms for detecting tree sections for each horizontal slice and for estimating metrics and variables related to detected trees attributes are described in the following subsections and summarized in fig 2 2 1 2 1 removing branches and foliage points for each horizontal slice this first step aims to remove points corresponding to fine branches and foliage e g leaves and shrubs and mainly to retain stem points for which we considered local surface variation also known as the normal change rate ncr this is a quantitative measure of curvature feature useful for discerning some noisy points pauly et al 2002 with higher values representing more curved surfaces predictably fine branches and foliage the ncr index is estimated at point level considering a local neighbourhood as follows given a fixed radius r the set of local neighbours of a point p is denoted by p i i n p r where n p r is the index set of all cloud points satisfying the condition that d p p i r where d is the euclidean distance the ncr for point p is then estimated by eigenanalysis of the 3 3 covariance matrix c p of its local neighbourhood eq 2 2 c p 1 n p r i n p r p i p p i p t where p is the centroid of the local neighbourhood of p obtaining the eigenvalues λ i i 0 2 by singular value decomposition of c p and assuming that λ 0 λ 1 λ 2 λ 0 describes the variation along the normal surface the extent to which the points deviate from the tangent plane is estimated pauly et al 2002 hence the ncr index for radius r at point p is defined as follows eq 3 3 n c r r p λ 0 λ 0 λ 1 λ 2 once ncr is computed p is retained as a stem point if its ncr value is lower than a pre established threshold fig 3 a in accordance with other studies in the tree detection function the neighbourhood was established by a radius of 5 cm as suitable for calculating ncr for the stem separation in forests ma et al 2016 xia et al 2015 the threshold value of ncr used to remove branches and foliage points was established as 0 1 by default also according to other studies in which the index has already been used with the same objective and obtaining good results e g jin et al 2016 zhang et al 2019 nevertheless other ncr thresholds can be specified by users in the corresponding argument of the tree detection function 2 1 2 2 detection of point clusters once branches and foliage points have been removed from the horizontal slice the next step is to detect point clusters corresponding to potential tree sections this process involves several steps i a clustering process is first applied to the horizontal projection of cartesian coordinates of points ii points corresponding to possible branches are then removed using surface density approaches and iii clusters receiving fewer points than expected for a full visible stem are discarded as mentioned above potential tree sections are first detected through a clustering process applied to the horizontal projection of cartesian coordinates this clustering process is performed by the density based spatial clustering of applications based on the noise dbscan method ester et al 1996 and applied with the dbscan function in the r package dbscan hahsler et al 2019 the size of the epsilon neighbourhood is established as the minimum distance between two consecutive points at the farthest distance from tls in the respective horizontal slice and a minimum of 5 points required in that epsilon neighbourhood this algorithm detects as many as possible sections corresponding to trees according to occlusion conditions as well as other possible clusters generated by other items branches shrubs etc and it has been used in previous studies of tls with the same objective ferrara et al 2018 molina valero et al 2019 for refining extracted stem points we used a similar approach to that proposed by zhang et al 2019 in order to remove any branches remaining in the clusters based on the principle that stems should generate more points than other parts of the tree due to the fractal size distribution according to west et al 1999 in addition these points should have a predominantly vertical distribution thus if the point cloud is vertically projected and rasterized in a grid cells over stems usually include more points than those located over branches and foliage each cluster is thus rasterized on the horizontal plane cartesian coordinates with an adapted grid step size of twice the distance between two consecutive points at mean cluster distance from tls and those points included in cells with fewer points than the median value of the number of points per cell will be removed fig 3b at this point of the process several tests were used to distinguish those clusters belonging to tree sections each cluster was first projected vertically with cylindrical coordinates φ z and divided into regular strips bound by vertical scan resolution α v fig 3c as stems completely visibly from tls must generate the maximum possible number of points according to scan resolution and distance from the tls instrument we estimated the approximate number of points that each strip must include if the section corresponds to a fully visible tree as normalized coordinates are projected on a horizontal plane the slope effect was first corrected to calculate the vertical resolution δ v in coordinate z at cluster real distance from tls m as follows eq 4 4 δ v 2 t a n α v 2 r c l u s t e r cos s l o p e where α v is the vertical scan resolution rad r c l u s t e r is the mean radial distance in spherical coordinates from tls to cluster m and s l o p e is the mean slope of cluster according to dtm rad the number of points n that each strip must contain in fully visibility conditions is then computed as follows eq 5 5 n δ z δ v where δ z is the slice thickness m and δ v is the previously defined vertical resolution eq 4 finally we reduced the predicted number of points by 30 in very stepped terrains 0 5 rad only those clusters with at least one strip containing the number of points n that every strip must contain in fully visibility conditions were selected once clusters have fulfilled all of the previous checks the next step involves obtaining the centre of the potential tree section with this aim regular square grids of 1 cm were overlapped on each cluster selected the tree section centre was then considered as the intersection grid point where the variance of the distances between this intersection and all the cluster points reaches the lowest value fig 3d considered to occur when the coefficient of variation of distances is smaller than 0 1 otherwise the cluster is dismissed finally the radius of the tree section was computed as the average of all the distances between the estimated centre and remaining cluster points at the moment of algorithm processing 2 1 2 3 cluster classification this step consists of checking multiple geometrics features and indices to verify which clusters finally correspond to tree sections most criteria used are based on those determined by molina valero et al 2019 and they were applied to each cluster the first test involves checking whether the centre is located behind the cluster points relative to tls this is considered fulfilled when at least 95 of the cluster points have a lower cylindrical coordinate ρ hence closer than plot centre than the tree centre the second test consists of checking for the absence of points behind the tree surface which is considered true when at least 95 of the distances between cluster points and the tree centre are greater than half of the estimated radius this may be visually checked by means of a distance histogram fig 4 a the similarity between the cluster shape and the circumference arc is then assessed checking that extreme points in cylindrical coordinate ρ are farther from tls than central points fig 4b this is possible when trees are largely visible from tls but otherwise will not be possible due to partial occlusions in such cases the clusters were checked to determine whether they form a smaller arc of a circle fig 4c for this purpose we calculated the pearson coefficient correlations for φ values in increasing order and the correlative numbering fig 4d those clusters with values below 0 995 were removed 2 1 2 4 estimating tree attributes when several sections are identified at different heights those corresponding to the same trees are joined using the dbscan algorithm on the horizontal projection and some tree attributes are obtained coordinates of normal section centre and horizontal distance from plot centre estimated dbh indicator of partial occlusion and number of points corresponding to normal section for original and reduced point clouds when trees are exclusively detected at 1 3 m the dbh is estimated directly as twice the radius estimated at this height conversely when trees are detected from other section s including 1 3 m or not a linear taper equation is fitted with radius as the response variable r a d i u s and section height h s e c h sec as the explanatory variable eq 6 6 r a d i u s β 0 β 1 h s e c the radius at 1 3 m is then predicted as follows eq 7 7 r a d i u s 1 3 ˆ r a d i u s i β ˆ 1 1 3 h s e c i where r a d i u s i and h s e c i are the estimated radius and the height corresponding to section i and β ˆ 1 is the slope parameter fitted in the linear regression eq 6 hence dbh is computed as twice the averaged predicted radius finally the number of points n u m p o i n t s corresponding to a normal section 5 cm in the original point cloud is computed and also estimated for each detected tree j by using the dbh values previously computed as follows eq 8 8 n u m p o i n t s e s t j d b h j i i n u m p o i n t s i d b h i i where the index set i corresponds to all detected trees fully visible at 1 3 m and i is the number of trees fully visible at 1 3 m the number of points and the estimated number of points for the point cloud reduced by pcp are obtained in a similar way 2 1 3 computing tls metrics and variables at stand level once normalization and tree detection processes for each point cloud are completed tls metrics and variables can be estimated at stand level for three different plot designs all of which are included in the metrics variables function table 1 these plot designs are circular fixed area k tree and angle count bitterlich 1948 plots fig 5 each of which is defined by a unique design parameter radius k and basal area factor baf respectively that must be specified in the function arguments the metrics and variables computed for each plot design are summarized in table 2 and more details are compiled below the two approaches for optimizing plot design by means of fortls functions are also described 2 1 3 1 stand level metrics metrics are computed using points directly from normalized point cloud in a relatively similar way as in the fusion ldv software for lidar data analysis and visualization mcgaughey 2009 these are statistical descriptive measures such as percentiles or only number of points belonging to specific sections of point cloud the following metrics are available total number of points corresponding to the normal section 5 cm of trees detected after removing some noisy points with ncr and refinement of extracted stem points processes see section 2 1 2 these can be computed for raw point clouds num points and reduced point clouds num points hom number of estimated points corresponding to the normal section 5 cm of trees detected the number of points corresponding to each tree can be estimated according to eq 8 for raw point clouds num points est and analogously for reduced point clouds num points hom est percentiles of z coordinate m computed percentiles are p01 p05 p10 p20 p25 p30 p40 p50 p60 p70 p75 p80 p90 p95 and p99 descriptive statistics of z coordinate distribution mean maximum max minimum min standard deviation sd variance var mode kurtosis and skewness percentage of points above mode perc on mode and mean perc on mean values of z coordinates scale weibull b and shape weibull c parameters of a weibull distribution fitted to z coordinates distribution 2 1 3 2 stand level variables variables represent estimates based on the attributes of trees detected from tls point cloud data further aggregated at stand level and finally expanded to unit area ha the following variables are available apparent stand density n tls trees ha 1 which is estimated for trees detected in a similar procedure to that used in conventional inventories for circular fixed area and k tree plots eq 9 and angle count plots eq 10 9 n t l s 10000 π r 2 n 10 n t l s i 1 n baf g i where r is the plot radius m n is the number of trees detected in the corresponding plot design baf is the basal area factor m2 ha 1 and g i is the basal area of the tree i m2 apparent stand basal area g tls m2 ha 1 which is estimated for trees detected in a similar procedure as that used in conventional inventories for circular fixed area and k tree plots eq 11 and angle count plots eq 12 11 g t l s 10000 π r 2 i 1 n g i 12 g t l s baf n apparent stand stem volume v tls m3 ha 1 which is estimated for trees detected by modelling stem profile as a paraboloid and calculating the volumes of revolution for fixed area and k tree plots eq 13 and angle count plots eq 14 13 v t l s 10000 π r 2 i 1 n π h p 99 2 i 2 d b h i 2 2 h p 99 i 1 3 2 14 v t l s i 1 n baf g i π h p 99 2 i 2 d b h i 2 2 h p 99 i 1 3 2 where h p 99 i and d b h i are the 99th percentile of points delimited by voronoi polygons m i e estimates of h and dbh m for tree i respectively mean and dominant diameters cm which are estimated for arithmetic quadratic geometric and harmonic means in the case of dominant diameters only the n largest trees per ha according to dbh are considered although it can be specified in the arguments the 100 largest trees per ha are considered by default mean and dominant heights m which are estimated for arithmetic quadratic geometric and harmonic means in the case of dominant heights only the n largest trees per ha according to dbh are considered although the number of trees can be specified in the arguments the 100 largest trees per ha are considered by default in the previous calculations for the k tree design the plot radius r is defined as the mean of horizontal distances of trees k and k 1 kleinn and vilčko 2006 2 1 3 3 dealing with occlusions all fortls functions used for estimating stand variables also include correction of occlusions approaches in the case of angle count plots occlusion corrections are based on gap probability attenuation with distance from tls depending on a poisson distribution in the case of circular fixed area and k tree plots distance sampling methods and shadowing effect correction are considered in order to obtain occlusion corrections based on distance sampling methods the distance sampling function must be executed previously and the values obtained must be incorporated as an argument for functions which compute stand variables whereas the other corrections are computed by default a brief description of the implemented occlusion corrections is given below 2 1 3 4 poisson attenuation model this method has been used in measurements with tls strahler et al 2008 lovell et al 2011 and optical montes et al 2019 instruments to reduce the device related bias in the relascope based approach it is based on geometric gap probability p g a p which decreases exponentially following a poisson distribution eq 15 15 p g a p λ d e r e λ d e r where λ is the number of trees per m2 d e is the effective dbh and r is the horizontal distance to the farthest tree the corrected stand density n pam trees ha 1 for angle count plots is then obtained as follows eq 16 16 n p a m n t l s f λ d e r where f is a function defined as f t 2 t 2 1 e t 1 t with t λ d e d 2 baf see strahler et al 2008 and lovell et al 2011 for further details similarly corrected stand basal area g pam m2 ha 1 and volume v pam m3 ha 1 are computed 2 1 3 5 point transect sampling this approach is based on the point transects method from distance sampling methods buckland et al 2001 these methods use detection functions g r θ with variable r distance from sampling point and parameter θ which describe how the probability of detection decreases as distance increases we used half normal eq 17 and hazard rate eq 18 functions as these have been successfully used in measurements with tls astrup et al 2014 and optical montes et al 2019 devices 17 g r θ e r 2 2 σ 2 18 g r θ 1 e r σ b parameter θ includes the shape b only in the hazard rate function and the scale σ which was also expanded with dbh as a covariate into an exponential function eq 19 according to ducey and astrup 2013 and astrup et al 2014 19 σ α 0 e α 1 d b h parameter θ is estimated by maximum likelihood marques and buckland 2003 miller and thomas 2015 clark 2016 with data left truncated at 1 m according to astrup et al 2014 the fitting process is carried out by means of the ds function included in the r package distance miller et al 2019 once the parameters of detection functions are estimated the probability of tree detection p i is estimated with eq 20 which is implemented in the expansion factor of circular fixed area and k tree plots as in eq 21 20 p i 2 r 2 0 r r g r θ ˆ 21 e f i 1 n 10000 p i π r 2 where r is the plot radius m and ef is the expansion factor multiplying n tls by these efs yields corrected stand densities n hn n hr n hn cov n hr cov trees ha 1 similarly corrected stand basal area g hn g hr g hn cov g hr cov m2 ha 1 and volume v hn v hr v hn cov v hr cov m3 ha 1 are computed all of these estimates are obtained by previously executing the distance sampling function table 1 which returns p i and also values of parameter estimates of detection functions and the corresponding akaike information criterion aic of these fits 2 1 3 6 correcting the shadowing effect this approach was developed by seidel and ammer 2014 for single scan mode these authors used the approach to correct the shadowing effect which generates shaded unsampled areas eq 22 according to the shaded area percentage related to the total area sampled 22 a s h a d o w π r 2 π r t r e e 2 360 d b h r t r e e π d b h 2 2 2 where r is the radius of the plot m and r t r e e is the distance between the tls instrument and the tree centre this method is implemented for circular fixed area and k tree plots and yields an expansion factor used to compute corrected estimates for stand density n sh trees ha 1 basal area g sh m2 ha 1 and volume v sh m3 ha 1 2 1 3 7 optimizing the plot design two different approaches can be used to find the best possible plot design depending on whether validation field data are available or not the approaches are represented in the fortls workflow fig 6 and detailed below 2 1 3 8 analysis of estimation stability the function estimation plot size estimates both apparent tree density n tls trees ha 1 and apparent basal area g tls m2 ha 1 for all of the aforementioned plot designs in the case of circular fixed area plot design concentric plots in regular increments of 0 1 m radius by default to the maximum radius specified in the arguments are simulated for computing n tls and g tls as a result line charts with estimates through plot size are obtained for k tree design all possible plots are defined by k 1 2 n where 1 is the nearest tree and n the farthest tree considered in the argument k tree max or the farthest detected existing tree if the argument is not specified finally for the angle count design variables will be estimated for regular baf increments comprised from 0 1 to the baf max specified in the arguments all of these line charts were inspired by fig 3 in brunner and gizachew 2014 2 1 3 9 validation with field measurements for cases when field data are available we designed a set of interconnected functions able to assess the performance of processed tls data relative to the corresponding field data simulations relative bias and correlations the field data necessary to conduct the analysis described hereinafter are tree dimensions dbh and h and positions relative to the tls scanner analysis of the performance is based on comparisons between these two data sources for the different plot designs and sizes the first function is simulations which computes in a similar way as estimation plot size all of the metrics and variables aforementioned for tls data 2 1 3and the corresponding variables based on field data table 2 the relative bias function was designed for direct comparison of tls based estimates and field based measurements by means of relative bias eq 23 23 r e l a t i v e b i a s 1 n i 1 n y i 1 n i 1 n x i 1 n i 1 n x i where x i and y i are the values of the field estimate and its tls counterpart respectively corresponding to plot i for i 1 n relative bias is assessed for all the simulations to find the best possible plot design for each variable of interest for other possible approaches apart from direct variables estimations the package has other functions that assess the best possible plot designs according to the correlations between variable estimates from field data and metrics variables derived from tls the correlations function computes both pearson and spearman correlation coefficients for common set of plots and all simulations and plot designs considered for each variable of interest this function produces the optimum correlations for all simulations the optimize plot design function then produces a graphical representation of the strongest correlations for all variables of interest 2 2 case study the functionality of the fortls package was tested in a fully mapped case study plot of 1 ha 100 100 m in a pure even aged p sylvestris stand located in la rioja spain fig 7 all trees in the plot live and standing dead with dbh greater than 7 5 cm were measured utm coordinates and elevation of the plot corners were measured with a high accuracy gnss receiver trimble r2 and trees were located in the plot with a total station nikon dtm 332 the dbh of all trees was measured with a diameter tape to the nearest 0 1 cm and for live trees only h was measured with a digital hypsometer vertex iv haglöf sweden to the nearest 0 1 m main stand variables estimated from field data are shown in table 3 after conventional inventory we scanned each intersection point defined by a regular 20 m squared grid 16 points sampling with a tls faro laser scanner focus 3d x 130 covering the full horizontal 0 360 and vertical ranges 60 90 with a resolution of 7 67 mm at 10 m in both horizontal and vertical angular apertures all of the single scans were then processed with fortls clipping the point clouds at 20 m radius from the plot centre finally to compare stand variables derived from tls data and from field inventory we also extracted the trees measured at the same 16 points sampling scanned considering 20 m radius all of these data can be found as example data in fortls as a list named rioja data in which the first element corresponds to the list of trees detected from tls in each plot tree list tls and the second element corresponds to the list of trees measured in each plot in the field tree list field 3 results the steps involved in processing the tls data with fortls are described below along with the different operations and arguments of the functions finally the results of tls data processing with fortls are presented for the aforementioned case study 3 1 fortls implementation users can install the released version of fortls from cran https cran r project org package fortls or they can install the most currently developed version from github https github com molina valero fortls tree devel using the install github function of the devtools package wickham et al 2021 a list with a brief description of the 10 main functions available in fortls 1 0 6 together with their default argument values is given in table 1 the functions were designed to obtain two main uses from tls data 1 conservative tree detection in which the algorithm gives preference to accuracy and 2 computation of metrics and variables related to forest attributes 2 1 with no field data available and 2 2 with the corresponding field data available the workflow of functions involved in these approaches is illustrated schematically in fig 6 although the workflow can be processed directly in the current r session most functions are designed to import data and save results to a specific working directory that should be specified in dir data and dir result arguments to facilitate efficient operation in this respect it is important to highlight that outputs from previous functions in the workflow fig 6 are usually inputs in the following functions thus systematization of the work will be enhanced when the data working directory corresponds to the results working directory script with a full example of workflow is supplied as supplementary material s1 which should be consulted while reading the following sections 3 1 1 tree detection this approach is encompassed by point cloud normalization section 2 1 1 and tree detection processes section 2 1 2 which are executed by the normalize and tree detection or tree detection multiple for the cases with several scans functions respectively tls data must be supplied as las or laz files in normalize note that point cloud centre cartesian x y coordinates relative to tls sampling point can be defined otherwise the function will use the file centre coordinates as default the maximum radial distance from the plot centre and the minimum and maximum heights of the z coordinate can also be defined in the arguments this enables possible outliers and unnecessary information to be discarded and decreases the computing time the normalize function will return a data frame with a normalized point cloud which will be saved in dir result as a txt file otherwise specified save result false but with point density reduced by applying pcp this normalized point cloud is necessary as data input in tree detection note that supplying the original not the reduced point cloud is highly recommended for detection of a greater number of trees parameters of tls resolution are necessary in the tree detection function defined according to either angle resolution rad or distance between two consecutive points mm at a determinate distance from tls m some inventory parameters such as minimum and maximum dbh can then be specified in the arguments to prevent detection of smaller and or larger desirable trees other parameters of the algorithm such as ncr see section 2 1 2 and plot level attributes can be specified in the respective arguments ncr threshold and plot attributes respectively this function will return a data frame with the tree centre location in cartesian x y and cylindrical ρ as horizontal distance φ coordinates number of points corresponding to normal section raw and estimated and partial occlusion for all detected trees the data frame will be saved as a csv file in the working directory provided in the dir result argument otherwise specified save result false in the first plot of the example top left in fig 7 and considering a radius of 20 m 38 out of 44 trees were detected however some angular deviation due to the internal compass of tls can be observed fig 8 considering all of the simulated plots our algorithm detected 598 out 659 trees i e 91 of detected trees when sequential analysis of several scans is required tree detection multiple will be the most appropriate function this function includes both normalize and tree detection processes enabling detection of trees for a set of plots in a single function these must be included in the same working directory specified in dir data as las or laz files the output will be a similar data frame as the aforementioned data frame but that includes all of the plots processed as before a csv file containing the data frame information will be saved by default in the working directory specified in dir result and the normalized point clouds reduced by pcp will be saved as txt files an example of a reduced point cloud named pcd corresponding to the first plot in the case study can be found in the supplementary material s5 as a txt file all trees detected with tree detection multiple for the 16 tls sampling points up to 20 m radius in the case study fig 7 and the list of trees measured in the field for these sampling points are included in supplementary material s5 as csv files and named tree list tls and tree list field respectively 3 1 2 estimating metrics and variables related to forest attributes the metrics variables function performs this process by computing a set of tls metrics and variables from point cloud data however other complementary functions play an important role in improving these estimates based on correction of the occlusion effect and choice of a more appropriate plot design in the first case distance sampling approaches based on point transect sampling methods buckland et al 2001 can be applied by means of the distance sampling function these approaches are used for the list of trees detected which must be provided to the function as a data frame with the same format and include the tree detection value the function works for all the plot designs included by default if no arguments are specified but can also only consider a set of specified plots argument id plots must contain this information or even plots distinguished by strata reachable by using the argument strata attributes the function will generate a list with 3 elements i probability of tree detection for the different detection functions and strata if specified ii parameters estimated for detection functions and iii aic estimator obtained for each detection probability function as the criteria for selecting the best fit regarding the second case i e the optimization of plot design more details can be found below at this stage the input data sets for metrics variables will be as follows i list of trees detected ii reduced normalized point clouds loaded from dir data and optionally iii the list with probabilities of tree detection according to distance sampling methods plot parameters considered for estimating metrics and variables are implemented in the plot parameters argument this is a data frame containing the columns named radius k tree and baf in reference to circular fixed area k tree and angle count plot design parameters the absence of parameter values rules out the corresponding plot design it is also possible to include a column named num trees that specifies the number of dominant trees per ha considered to estimate dominant dbh and h otherwise it will be considered as 100 largest trees ha 1 in terms of dbh any plots that are grouped by strata in the tree list input can be identified in a column named stratum to consider different plot design parameters for each stratum in this case another column also named stratum and coinciding with its homologue from the tree list data in the strata coding can be included in the plot parameters argument after execution of metrics variables a list with as many elements as plot designs considered will be returned the elements will include the tls metrics and variables computed for each plot design indicated by default this list will be saved as separate csv files for each specified plot design in dir result otherwise specified save result false an example corresponding to circular fixed area k tree and angle count plots of 15 m radius 12 trees and baf 1 respectively from the case study can be found in supplementary material s5 as csv files named metrics variables fixed area plot metrics variables k tree plot and metrics variables angle count plot 3 1 2 1 field data not available when field data are not available it is not possible to determine how well the tls metrics and variables perform in estimating the corresponding variables in this case the most reliable estimates can be obtained with those plot designs in which estimates are stable for different plot design sizes the estimation plot size function can be used to assess whether the plot designs considered will reach stable values and can thus use the corresponding parameters in metrics variables this function contemplates a fixed circular area k tree and angle count plot designs represented by line charts with estimated values of n tls and g tls y axes through plot sizes x axes the maximum sizes considered for each plot design must be specified in the plot parameters argument as in other functions the absence of any of these arguments rules out the corresponding plot design these charts can be generated for all the plots individually no arguments specified or for mean values across all plots or strata including the standard deviation area argument mean true it is also possible to distinguish strata if they are included in a column named stratum which must be given in the tree list input the last function enables representation in the same chart of estimates of mean values for the different plot designs considered fig 9 which can be executed by setting the argument all plot designs as true 3 1 2 2 field data available when field data are available fortls also includes functions for determining the best possible plot design for yielding maximum correspondence between tls based and field estimates these functions are based on comparison between tls and their corresponding field data and they form one of the main branches in the workflow fig 6 the simulations function represents the first compulsory step which generates estimates of metrics and variables for a set of field plots and their tls counterparts for circular fixed area k tree and angle count plots thus the list of trees detected in the point cloud and list of trees measured in the field are necessary as input data in addition probability based on distance sampling methods can be included as an optional argument if these methods are applied simulations are computed for continuous plot size increments of 0 1 m 1 tree and 0 1 baf by default although other increments can be specified in the plot parameters argument the maximum value of plot parameter simulation must be specified for the respective elements radius max k tree max and baf max of the same argument and as in other functions the absence of any of these arguments rules out the corresponding plot design the number of largest trees per ha implemented in dominant variables can thus be specified in the num trees element of the plot parameters argument by default 100 trees per ha the simulations function will return a list with many elements data frame objects as plot designs considered in which each row will correspond to a simulated pair plot radius k baf and the columns will include all the metrics and variables estimates based on field and tls data table 2 by default these elements will be saved as separate csv files in dir result otherwise specifying save result false after generating simulations two possible processes can be conducted fig 6 one process assesses bias among variables estimated from tls data and their counterpart estimated from field data and is carried out by the relative bias function which computes relative bias between field and tls estimates eq 23 this will be executed with the simulated data generated previously in simulations which are the input data of this function the objective variables can be indicated in the variables argument but by default those most conventionally used in forestry will be considered n g v d dg d 0 h and h 0 this function will return a list with as many elements data frame objects as plot designs considered with rows corresponding to simulated pairs plot radius k baf and columns including relative bias between field and tls variables estimates by default these elements will be saved as separate csv files in dir result otherwise specified save result false in addition interactive line charts with relative bias through size plot design will be generated by groups of variables n g v etc and plot design supplementary material s2 these charts are very intuitive for assessing the best plot design and size for estimating variables directly from tls data the other approach consists of evaluating the correlations between all metrics and variables estimated from tls data and variables of interest estimated from field data both pearson and spearman correlation coefficients are available this is achieved with the correlations function by means of the simulations with different previously generated plot design sizes the target variables can be indicated in the variables argument but by default those most commonly used in conventional forest inventories will be considered n g v d dg d 0 h and h 0 this function will return a list including the following i correlation values for each method and plot design ii another element with the same structure including the p values of test for association corresponding to these correlations and iii the strongest correlations for each set of simulations and the names of the tls metrics or variables to which they correspond the first and third elements will be saved as independent csv files one per correlation method and plot design otherwise specified save result false this function will also generate interactive line charts which will show the correlations between field variables of interest and all the tls metrics and variables computed through regular continuous increase of plot size supplementary material s3 finally all the variables of interest can be assessed together according to the highest correlations achieved by means of interactive heatmaps using the optimize plot design function supplementary material s4 the input data will be the list including the strongest correlations obtained previously third element of the list returned by correlations the objective variables can be indicated in the variables argument but by default those most commonly used in forest inventories will be considered n g v d dg d 0 h and h 0 the optimize plot design function is very useful for determining the most suitable plot design and size for all the variables of interest considered 3 2 experimental test case we used the study plot to explore the potential of fortls to estimate forestry variables at stand level we present some of the most important results here but all of the analyses and outputs are reported in more detail in supplementary material s1 s5 3 2 1 estimation without field data available as tls plots are established on the basis of a systematic design sampling inventory and covering the same stand conditions we considered assessing mean values across all tls sampling points using the estimation plot size function and comparing all the plot designs in the same chart regarding n tls all of the plot designs yielded stable estimates from a given plot design size onwards these estimates were 12 16 m for circular fixed area 3 16 trees for k tree and 0 8 2 baf in angle count plots fig 9 according to the standard deviation different trends were observed in plot parameter values which were quite stable in a circular fixed area decreasing in k tree and increasing in angle count plots the patterns were very similar for g tls estimates in this case study estimates in the stable zones reached very similar values for the three different designs the stable zones showed similar pattern estimates for both n tls and g tls for n tls estimates the estimated patterns almost completely coincided for a fixed radius of 13 17 m 12 36 for k with the k tree approach and 1 2 1 6 for baf reaching approximately the value of 315 trees ha 1 in all of these plot design which is very close to 322 live trees ha 1 measured in the field table 3 regarding g tls estimates were similar for fixed radius of 13 17 m 12 36 for k and 0 8 1 2 for baf reaching a slightly higher value than 23 m2 ha 1 which is lower than 25 29 m2 ha 1 estimated from field measurements table 3 thus in both cases the estimates slightly underestimated the stable sections of the curves relative to stand variables obtained from field measures 3 2 2 estimation with field data available although all the variables and plot designs are assessed in supplementary material s2 we focused here on g and circular fixed area plots to study relative bias in more detail all of the variables estimated from tls data showed positive values of relative bias for smaller plot sizes and negative values as plot size increased with peaks reaching even higher than 50 for very small plots fig 10 these peaks were followed by a first sharp decrease until approximately 8 m radius and then a less steep decrease until the largest plot size the best results were achieved with variables estimated with occlusion corrections methods with most of them reaching low constant values of relative bias of radius between 15 and 17 m however the most stable estimates and lowest relative bias were based on half normal detection function between 17 5 and 19 m radius with relative bias below 1 although all of the variables plot designs and correlations methods pearson and spearman are assessed in supplementary material s3 here we evaluated the pearson correlation between arithmetic mean dominant height estimates based on field data h 0 and the other metrics and variables obtained for tls data for angle count plots in this case the highest correlations were obtained with metrics corresponding to z coordinate percentiles especially p95 which yielded correlations above 0 94 and h 0 estimates based on tls data fig 11 the general trend for all the metrics and variables was a slow decrease until a value of 1 9 baf with the highest correlations achieved in this baf range the decrease then became steeper until reaching values below 0 8 interestingly p95 performed better than the other metrics with a less pronounced decrease in terms of correlation the most suitable plot design considering performance of all the variables of interest can be assessed with optimize plot design again this is evaluated for all plot design and correlation in supplementary information s4 but here we only assessed the case of k tree plot and pearson correlation different trends differentiated groups of field variables first both h 0 and h yielded higher correlations when more trees were included and were reached for h earlier than for h 0 fig 12 once high correlations were reached they remained stable above 0 9 until the largest k tree plot the diameter variables d 0 dg and d remained fairly stable across all plot sizes with slightly higher values in smaller plots however the correlations were not very high with values of between 0 6 and 0 8 for the other variables v g and n the strongest correlations were obtained for smaller plots with the highest values reached for plots of 6 10 trees and they then decreased slightly before remaining stable in the case of n good correlations were also achieved for between 13 and 17 trees 4 discussion although some algorithms and applications have been developed for processing tls point clouds for forestry purposes liang et al 2018a krok et al 2020 tls has not yet been established as an operational device in fis liang et al 2016 to overcome this challenge most researchers agree that automation of point cloud processing with attainable and easy to use software able to extract information related to important forest attributes is essential dassot et al 2011 newnham et al 2015 white et al 2016 liang et al 2016 2018a fortls contributes to making progress in this challenge by automating the processing of tls point clouds for estimating fi variables this is achieved by detecting trees and estimating some of their attributes and also generating metrics and or variables related to conventional forestry variables at stand level in addition some of the functions enable assessment of the performance of metrics and variables estimates for different plot designs based on comparison with corresponding field data thus allowing the best possible plot design to be established in each situation this functionality confers flexibility to fortls in statistical inference for fis representing the main difference relative to similar applications bienert et al 2007 trochta et al 2017 unlike other existing applications which extract tree attributes in high detail such as computree othmani et al 2013 simpleforest hackenberg et al 2015 adtree du et al 2019 and treegsm raumonen et al 2013 fortls mainly focuses on providing metrics and variables related to forestry attributes at stand level this approach together with field implementation based on single scans enables larger areas to be covered although at the expense of lower detail at tree level we therefore consider that our approach perceives tls devices as sampling rather than measurement instruments and applications with similar perspective to ours are very scarce e g 3d forest mainly developed for describing forest 3d structure and autostem mainly focused on timber production although use of these tools cannot be directly compared with our case study because the methods and stands conditions are different some useful information can be obtained regarding 3d forest the case study was framed in a stand characterized by highly variable canopy openness and dominated by sessile oak quercus petraea matt with a mixture of other broadleaf species trochta et al 2017 the authors sampled a plot of 2 4 ha in which the dbh of all the trees with dbh 10 cm 824 and h of 181 trees were measured the plot was then scanned with a resolution of 2 mm at 10 m by multiple scans set at 44 44 m which were co registered before processing data in 3d forest the best results yielded a relative bias of 1 positive and 0 8 negative for d and h respectively in our case the relative bias for d was negative and slightly higher than in trochta et al 2017 with the best performance obtained for angle count plot design reaching peaks with lower relative bias than 1 for low values of baf s2 however although the quadratic mean dbh d g yielded the best performance in all cases h was always underestimated reaching similar relative bias only for circular fixed area plots of around 18 m radius s2 this again shows that height variables are systematically underestimated liang et al 2016 2018a krok et al 2020 on the other hand autostem was assessed for a stand planted with picea sitchensis bong carr with a current density of 600 trees per ha mengesha et al 2015 this was measured in nine randomly located plots of 15 m radius in which all trees were measured by conventional methods and single tls scans were made from the plot centre with a resolution of 6 28 mm at 10 m the overall difference between tls derived and conventional volume estimates was 5 6 when occluded trees not detected by tls were excluded from the analysis of both sources i e tls and field data and 10 2 when estimates based on tls were corrected by simple correction factors fortls yielded much lower relative bias in volume estimates especially for the k tree plot design and estimates corrected with distance sampling methods g hr cov which yielded values of around 0 for 12 14 trees s2 in any case comparison with these findings should be done with caution as we used paraboloid function for estimating volume in contrast to mengesha et al 2015 who applied methods based on retrieved stem profiles nevertheless this is the most interesting comparison because the aforementioned authors used a very similar sampling methodology with single randomly located scans systematic in our case and the corresponding field plot measurements all plot designs considered circular fixed area k tree and angle count plots yielded stable estimates in n tls and g tls for certain plot size ranges fig 9 as occurred for g estimates using a 2d tls device fig 3 brunner and gizachew 2014 however the variables were slightly underestimated unlike in brunner and gizachew 2014 in which unbiased g estimates between real and scan basal area were observed for 5 10 m radius plot size only for one of the studied stands however our findings are generally consistent with the most recent findings which indicate underestimation of n and g due to deficit in tree detection caused by occlusions especially for single scans liang et al 2016 2018a krok et al 2020 in our study uncorrected estimates g tls yielded lower bias for smaller plot sizes due to lower occlusion rates which is consistent with the findings of corona et al 2019 who concluded that under easy to measure stand conditions plots of 10 m radius in which occlusion corrections are not considered may be good enough for estimating g to overcome underestimates derived from occlusion effects we incorporated several methods applied in tls single scans in other studies which improved estimations in larger plot sizes where g hn yielded the best estimates for 17 5 19 m radius fig 10 this finding is consistent with those of astrup et al 2014 who reported that a larger detection radius seems to improve estimates when distance sampling methods are applied the results presented here indicate that correction occlusion methods can improve estimates as observed for g in angle count strahler et al 2008 lovell et al 2011 and circular fixed area plots seidel and ammer 2014 astrup et al 2014 because these methods can be assessed in fortls in terms of relative bias through continuous plot size increment and different plot designs this represents an advantage for determining the best possible plot design in execution of a single workflow fortls also evaluates correlations between variables of interest and tls derived metrics and variables this contribution implies a new perspective enabling selection of the best possible plot design according to statistical correlation measures instead of measurement accuracy this approach may be considered for estimating forestry variables assisted by or based on models in a similar way as aba inference developed for als devices næsset 2002 in some cases simple linear regressions may be fitted when strong relationships are observed for only one tls metrics and or variables as between h 0 and p95 for the study case with correlations above 0 94 fig 11 in this case the plot design considered was an angle count plot for a baf value of 1 3 in addition this concept provides an opportunity to solve the systematic problem regarding underestimation of height variables derived from tls measurements liang et al 2016 2018a krok et al 2020 different groups of variables yielded the highest correlations at different plot sizes fig 12 when the correlations for height variables were strongest in larger plots diameter variables retained approximate stable correlations and the highest correlations for other variables n g and v were reached in smaller plots thus the plot design could be adapted to our stand conditions and target variables for more efficient sampling here we have demonstrated the utility of the r package fortls in fis in a case study as fortls works with single scan data co registration of point clouds in specific software and placement of targets for field measurements are not required this improves data acquisition and shortens the processing time as well as enabling the sample size to be increased in a cost efficient manner which is one of the most desirable features of tls in fis liang et al 2016 further research to consolidate fortls for the approaches mentioned here should encompass the following i larger and more complex study cases ii consideration of more metrics and variables with high potential for correlation with other forest attributes leaf area index species etc iii exploration of the possibility of making inferences assisted by models by developing an adequate sampling methodology and iv improvement of the computation process as much as possible in relation to both algorithms and computing time 5 conclusions the r package fortls is useful software for processing tls data for forestry purposes it has the advantage of working with single scans and conducting automatic data processing which may overcome the major challenge of affordability in data acquisition and data processing it has yielded good results for conventional variables based on a preliminary case study with direct estimates as well as good correlations between field derived variables and tls derived metrics and variables however its potential for producing model assisted inferences from metrics and or variables has not yet been demonstrated in addition one of the most valuable features of the software is its flexibility to adapt to the best possible plot design for each variable enabling multiple plot designs to be used in a single sampling design further research considering larger and more complex case studies is necessary to consolidate fortls as an operational tool in fis as well as to develop new metrics and variables 6 software availability name of software fortls 1 0 6 developers juan alberto molina valero maría josé ginzo villamayor manuel antonio novo pérez adela martínez calvo juan gabriel álvarez gonzález fernando montes césar pérez cruzado contact address unit for sustainable environmental and forest management uxafores department of agroforestry engineering higher polytechnic engineering school universidade de santiago de compostela benigno ledo s n campus terra 27002 lugo spain email juanalberto molina valero usc es software required r 3 5 0 first available march 2 2021 availability https cran r project org package fortls installation in r install packages fortls program languages r and c license gpl 3 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work was supported by the spanish ministry of science and innovation agl2016 76769 c2 2 r pid2020 119204rb c22 and galician regional government 2020 cp031 ed431f 2020 02 jamv was supported by the spanish ministry of science innovation and universities through the fpu program fpu16 03057 amc was supported by galician regional government within the framework of the agreement development of the galician continuous forest inventory 2020 cp031 cpc was supported by the spanish ministry of science and innovation ryc2018 024939 i the authors thank diego lombardero barrera joel rodríguez ruiz mario lópez fernández and óscar lópez álvarez for help with fieldwork we are also grateful for the comments made by anonymous reviewers which helped us to improve the quality of the paper appendix a supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 multimedia component 3 multimedia component 3 multimedia component 4 multimedia component 4 multimedia component 5 multimedia component 5 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105337 
25614,terrestrial laser scanning tls devices show great potential for application in forest inventories fis as they are capable of registering high resolution point clouds rapidly and automatically nevertheless operational use of tls for fi purposes has been hampered by the absence of algorithms for processing the acquired data particularly in the single scan mode as occlusions result in loss of information the r package fortls has been developed to overcome this obstacle as it automates the processing of single scan tls point cloud data for forestry purposes and includes several features that deal with occlusions fortls makes use of the main advantage of the single scan scenario in fi thus improving the efficiency of data acquisition and post processing all of these features of the fortls package are potentially valuable for the operational use of tls in fis in combination with inference techniques derived from model based and model assisted approaches keywords forest monitoring forest stands parameters lidar precision forestry remote sensing terrestrial based technologies 1 introduction information about forest resources is essential for sustainable forest management and development of forest policies and forest inventories fis are fundamental for estimating and monitoring the state and evolution of forest resources at both global and regional scales tomppo et al 2010 fis have improved since they were first introduced owing to the continuous appearance of new technologies especially in the last few decades since the emergence of remote and proximal sensing in this technological context light detection and ranging lidar systems provide 3 dimensional point clouds which are suitable for estimating tree attributes and are very useful for many forestry applications dubayah and drake 2000 this technology has proved operationally viable for estimating essential fi variables at stand level such as arithmetic mean height h m basal area g m2 ha 1 and volume v m3 ha 1 with airborne laser scanning als devices wulder et al 2012 white et al 2016 terrestrial laser scanning tls devices such as lidar devices with millimetric precision are considered to show great potential for enhancing fis dassot et al 2011 white et al 2016 and also forest ecology research calders et al 2020 danson et al 2018 apart from much higher spatial resolution under canopy the main advantages of using tls data rather than als data are better observation of near ground vegetation white et al 2016 and thus better trunk coverage for estimating the woody component which is one of the most important components in fis in fact tls based approaches can provide very accurate estimates of the diameter at breast height dbh measured at 1 3 m from the ground and the stem curve with the single scan approach yielding values of 1 4 and 1 3 6 cm respectively depending on the stand conditions liang et al 2018a which are close to the values required in practical applications such as fis however tls devices have not been yet adopted in fis for several reasons i difficulties in the automation of data processing to provide reliable measurements of important forest variables ii high acquisition costs iii limited software and iv lack of trained personnel liang et al 2016 many researchers agree that affordability is the main key challenge to overcome emphasizing that automation of point cloud processing with attainable and easy to use software able to extract information related to important forest attributes is essential dassot et al 2011 newnham et al 2015 white et al 2016 liang et al 2016 2018a as tls data sets comprise millions of points sophisticated methods for automatic processing are required many algorithms with a high level of automation and that are able to extract tree attributes such as dbh total height h m and stem volume v m3 have been developed in the last few decades cabo et al 2018 liang et al 2012 2018b olofsson et al 2014 olofsson and holmgren 2016 zhang et al 2019 although most algorithms yield acceptable dbh and stem curve estimations according to fi requirements stem detection and estimation of h cause bottlenecks in the process especially with single scans krok et al 2020 liang et al 2018a some of the algorithms developed have also been included in software applications such as simpleforest hackenberg et al 2015 3d forest trochta et al 2017 and autostem bienert et al 2007 among others krok et al 2020 however these programs have some drawbacks for use in fis i they focus on single tree rather than stand level approaches simpleforest ii they involve semi automatic processing 3d forest and iii the software is only available commercially autostem i e it is not free or open source furthermore the previous studies have mainly focused on replicating plot based measurements which does not extend conventional inventory approaches from a sampling perspective and thus limits the utility of tls in fis the main purpose of which is to estimate important forest variables at larger scales e g stand and regional than tree and plot levels newnham et al 2015 white et al 2016 methods that enable tls to be used for fi purposes must therefore contemplate the use of different approaches newnham et al 2015 and other procedures in which not all trees in the sample plots are measured may be feasible liang et al 2018a thus further research is required to address the challenges in the operational use of tls liang et al 2018a here we present fortls molina valero et al 2021 an r package developed with the objective of automating tls point cloud data processing and estimating variables for forestry purposes to fulfil this objective fortls enables i detection of trees and estimation of dbh and other tree attributes ii estimation of some stand variables e g h g v iii computation of metrics related to important tree attributes estimated in fis at stand level and iv optimization of plot design for combining tls data and field measured data the package also includes several features for correcting occlusion problems to improve the estimation of stand variables the current version of fortls is based on single scan tls data with the aim of facilitating operational use and it has been designed as relatively easy to use open source software aimed at use by both scientists and technical users relative to multi scan and multi single scan approaches the single scan approach improves data acquisition shortens the processing time and increases the sample size in a cost efficient manner mainly because it does not require pre scanning tasks involving location of artificial reference objects holopainen et al 2014 or automated post processing matching methods liu et al 2017 finally and as a case study we have tested fortls for estimating common forestry variables in an experimental plot of 1 ha located in an even aged pinus sylvestris l stand in northern spain these features of the fortls package may enable the operational use of tls in fis in combination with model based or model assisted inference approaches 2 methods 2 1 software design fortls molina valero et al 2021 has been developed as an r package r core team 2021 because r is free statistical software which is accessible to any user interested in this tool the initial stages of development of this package were outlined in molina valero et al 2020 although the first version of fortls was not available until march 2021 currently both the most recent stable version of the package and the most up to date version can be downloaded free of charge from respectively the cran https cran r project org package fortls and github development https github com molina valero fortls tree devel repositories the r package fortls has been optimized by implementing c code in the most demanding computing processes by means of the rcpp package eddelbuettel 2013 eddelbuettel and balamuta 2018 eddelbuettel and françois 2011 and the rcppeigen package bates and eddelbuettel 2013 which enables integration of the eigen c library for specific matrix calculation for operations with objects in spatial data classes both the raster hijmans 2020 and sp bivand et al 2013 pebesma and bivand 2005 packages have been used for obtaining voronoi polygons the ggvoronoi package garrett et al 2021 has been used in the simulations and metrics variables functions as tls point clouds represent large data sets fortls also imports the vroom package hester and wickham 2020 for accelerating loading and saving txt files we have used other important packages to generate and save interactive graphics namely plotly sievert 2020 and htmlwidgets vaidyanathan et al 2020 apart from the packages included in r base distribution and other accessory packages such as progress csárdi and fitzjohn 2019 scales wickham and seidel 2020 and tidyr wickham 2021 the other external r packages used for more specific functions are mentioned below with their respective functions the functions and results compiled in this work are based on the stable version 1 0 6 of the fortls package available in cran in the following sections all steps involved in tls point cloud data processing with fortls as well as the most relevant algorithms are described i normalization ii tree detection and iii estimation of metrics and variables at stand level 2 1 1 normalization the normalization process is a necessary first step in processing point cloud data and it is implemented in the normalize function table 1 which for some processes uses the functions readlas clip circle classify ground grid terrain and normalize height included in the lidr package roussel et al 2020 roussel and auty 2020 normalization involves obtaining the coordinates relative to plot centre for tls point clouds supplied as las or laz files the process includes the following steps i classification of points as ground ii generation of a digital terrain model dtm iii computation of coordinates relative to dtm cartesian cylindrical and spherical and iv reduction of point cloud density by the point cropping process pcp in the initial step points are classified as ground or not ground with the cloth simulation filter csf algorithm zhang et al 2016 the dtm is then generated by spatial interpolation of ground points two methods are available for executing this process i spatial interpolation based on delaunay triangulation by default and ii spatial interpolation using a k nearest neighbour approach with inverse distance weighting the point cloud is then normalized by subtracting the dtm created once the point cloud has been normalized cartesian cylindrical and spherical coordinates are calculated relative to the sampling point tls device establishment point finally the normalize function applies the pcp algorithm developed by molina valero et al 2019 to reduce the point density and thus produce a spatially homogeneous point cloud in which the distribution of points is proportional to the object size during execution of the pcp a selection probability p r o b p is assigned to each cloud point p according to eq 1 fig 1 1 p r o b p r p r m a x where r p is the radial distance of the point p from the plot centre and r m a x is the radial distance of the farthest point from the plot centre finally the point is selected if the selection probability is equal to or higher than a random value generated from the uniform distribution for the interval 0 1 2 1 2 tree detection tree detection represents a very important and challenging step in estimating variables of interest in tls assisted fis this is partly due to occlusions which are much more important when working with single scans this process of tree detection is implemented in the tree detection function table 1 which has been designed to detect as many trees as possible from point cloud data obtained as normalize function output in the tree detection process one or several horizontal slices from the original point cloud are extracted slices at heights of 1 0 1 3 and 1 6 m 5 cm over the terrain all around 1 3 m as reference section to estimate dbh are considered by default in tree detection the probability of detecting trees increases when more than one slice is considered however the tree detection function can also extract if specified in the arguments slices at other heights when trees are not identifiable at these pre established values each horizontal slice is processed by algorithms that are able to i remove branches and foliage points ii detect point clusters corresponding to potential tree sections and iii classify detected point clusters as tree sections or not according to several tests the tree sections detected for all horizontal slices are then merged and for each tree detected the tree detection function estimates certain attributes by i calculating the coordinates corresponding to tree normal section centre 1 3 m above ground level and its horizontal distance from plot centre ii estimating the dbh iii classifying the tree as fully visible or partially occluded and iv obtaining the number of points corresponding to normal section slice 1 3 m 5 cm for both original and reduced by applying pcp point clouds the main steps involved in the previously mentioned algorithms for detecting tree sections for each horizontal slice and for estimating metrics and variables related to detected trees attributes are described in the following subsections and summarized in fig 2 2 1 2 1 removing branches and foliage points for each horizontal slice this first step aims to remove points corresponding to fine branches and foliage e g leaves and shrubs and mainly to retain stem points for which we considered local surface variation also known as the normal change rate ncr this is a quantitative measure of curvature feature useful for discerning some noisy points pauly et al 2002 with higher values representing more curved surfaces predictably fine branches and foliage the ncr index is estimated at point level considering a local neighbourhood as follows given a fixed radius r the set of local neighbours of a point p is denoted by p i i n p r where n p r is the index set of all cloud points satisfying the condition that d p p i r where d is the euclidean distance the ncr for point p is then estimated by eigenanalysis of the 3 3 covariance matrix c p of its local neighbourhood eq 2 2 c p 1 n p r i n p r p i p p i p t where p is the centroid of the local neighbourhood of p obtaining the eigenvalues λ i i 0 2 by singular value decomposition of c p and assuming that λ 0 λ 1 λ 2 λ 0 describes the variation along the normal surface the extent to which the points deviate from the tangent plane is estimated pauly et al 2002 hence the ncr index for radius r at point p is defined as follows eq 3 3 n c r r p λ 0 λ 0 λ 1 λ 2 once ncr is computed p is retained as a stem point if its ncr value is lower than a pre established threshold fig 3 a in accordance with other studies in the tree detection function the neighbourhood was established by a radius of 5 cm as suitable for calculating ncr for the stem separation in forests ma et al 2016 xia et al 2015 the threshold value of ncr used to remove branches and foliage points was established as 0 1 by default also according to other studies in which the index has already been used with the same objective and obtaining good results e g jin et al 2016 zhang et al 2019 nevertheless other ncr thresholds can be specified by users in the corresponding argument of the tree detection function 2 1 2 2 detection of point clusters once branches and foliage points have been removed from the horizontal slice the next step is to detect point clusters corresponding to potential tree sections this process involves several steps i a clustering process is first applied to the horizontal projection of cartesian coordinates of points ii points corresponding to possible branches are then removed using surface density approaches and iii clusters receiving fewer points than expected for a full visible stem are discarded as mentioned above potential tree sections are first detected through a clustering process applied to the horizontal projection of cartesian coordinates this clustering process is performed by the density based spatial clustering of applications based on the noise dbscan method ester et al 1996 and applied with the dbscan function in the r package dbscan hahsler et al 2019 the size of the epsilon neighbourhood is established as the minimum distance between two consecutive points at the farthest distance from tls in the respective horizontal slice and a minimum of 5 points required in that epsilon neighbourhood this algorithm detects as many as possible sections corresponding to trees according to occlusion conditions as well as other possible clusters generated by other items branches shrubs etc and it has been used in previous studies of tls with the same objective ferrara et al 2018 molina valero et al 2019 for refining extracted stem points we used a similar approach to that proposed by zhang et al 2019 in order to remove any branches remaining in the clusters based on the principle that stems should generate more points than other parts of the tree due to the fractal size distribution according to west et al 1999 in addition these points should have a predominantly vertical distribution thus if the point cloud is vertically projected and rasterized in a grid cells over stems usually include more points than those located over branches and foliage each cluster is thus rasterized on the horizontal plane cartesian coordinates with an adapted grid step size of twice the distance between two consecutive points at mean cluster distance from tls and those points included in cells with fewer points than the median value of the number of points per cell will be removed fig 3b at this point of the process several tests were used to distinguish those clusters belonging to tree sections each cluster was first projected vertically with cylindrical coordinates φ z and divided into regular strips bound by vertical scan resolution α v fig 3c as stems completely visibly from tls must generate the maximum possible number of points according to scan resolution and distance from the tls instrument we estimated the approximate number of points that each strip must include if the section corresponds to a fully visible tree as normalized coordinates are projected on a horizontal plane the slope effect was first corrected to calculate the vertical resolution δ v in coordinate z at cluster real distance from tls m as follows eq 4 4 δ v 2 t a n α v 2 r c l u s t e r cos s l o p e where α v is the vertical scan resolution rad r c l u s t e r is the mean radial distance in spherical coordinates from tls to cluster m and s l o p e is the mean slope of cluster according to dtm rad the number of points n that each strip must contain in fully visibility conditions is then computed as follows eq 5 5 n δ z δ v where δ z is the slice thickness m and δ v is the previously defined vertical resolution eq 4 finally we reduced the predicted number of points by 30 in very stepped terrains 0 5 rad only those clusters with at least one strip containing the number of points n that every strip must contain in fully visibility conditions were selected once clusters have fulfilled all of the previous checks the next step involves obtaining the centre of the potential tree section with this aim regular square grids of 1 cm were overlapped on each cluster selected the tree section centre was then considered as the intersection grid point where the variance of the distances between this intersection and all the cluster points reaches the lowest value fig 3d considered to occur when the coefficient of variation of distances is smaller than 0 1 otherwise the cluster is dismissed finally the radius of the tree section was computed as the average of all the distances between the estimated centre and remaining cluster points at the moment of algorithm processing 2 1 2 3 cluster classification this step consists of checking multiple geometrics features and indices to verify which clusters finally correspond to tree sections most criteria used are based on those determined by molina valero et al 2019 and they were applied to each cluster the first test involves checking whether the centre is located behind the cluster points relative to tls this is considered fulfilled when at least 95 of the cluster points have a lower cylindrical coordinate ρ hence closer than plot centre than the tree centre the second test consists of checking for the absence of points behind the tree surface which is considered true when at least 95 of the distances between cluster points and the tree centre are greater than half of the estimated radius this may be visually checked by means of a distance histogram fig 4 a the similarity between the cluster shape and the circumference arc is then assessed checking that extreme points in cylindrical coordinate ρ are farther from tls than central points fig 4b this is possible when trees are largely visible from tls but otherwise will not be possible due to partial occlusions in such cases the clusters were checked to determine whether they form a smaller arc of a circle fig 4c for this purpose we calculated the pearson coefficient correlations for φ values in increasing order and the correlative numbering fig 4d those clusters with values below 0 995 were removed 2 1 2 4 estimating tree attributes when several sections are identified at different heights those corresponding to the same trees are joined using the dbscan algorithm on the horizontal projection and some tree attributes are obtained coordinates of normal section centre and horizontal distance from plot centre estimated dbh indicator of partial occlusion and number of points corresponding to normal section for original and reduced point clouds when trees are exclusively detected at 1 3 m the dbh is estimated directly as twice the radius estimated at this height conversely when trees are detected from other section s including 1 3 m or not a linear taper equation is fitted with radius as the response variable r a d i u s and section height h s e c h sec as the explanatory variable eq 6 6 r a d i u s β 0 β 1 h s e c the radius at 1 3 m is then predicted as follows eq 7 7 r a d i u s 1 3 ˆ r a d i u s i β ˆ 1 1 3 h s e c i where r a d i u s i and h s e c i are the estimated radius and the height corresponding to section i and β ˆ 1 is the slope parameter fitted in the linear regression eq 6 hence dbh is computed as twice the averaged predicted radius finally the number of points n u m p o i n t s corresponding to a normal section 5 cm in the original point cloud is computed and also estimated for each detected tree j by using the dbh values previously computed as follows eq 8 8 n u m p o i n t s e s t j d b h j i i n u m p o i n t s i d b h i i where the index set i corresponds to all detected trees fully visible at 1 3 m and i is the number of trees fully visible at 1 3 m the number of points and the estimated number of points for the point cloud reduced by pcp are obtained in a similar way 2 1 3 computing tls metrics and variables at stand level once normalization and tree detection processes for each point cloud are completed tls metrics and variables can be estimated at stand level for three different plot designs all of which are included in the metrics variables function table 1 these plot designs are circular fixed area k tree and angle count bitterlich 1948 plots fig 5 each of which is defined by a unique design parameter radius k and basal area factor baf respectively that must be specified in the function arguments the metrics and variables computed for each plot design are summarized in table 2 and more details are compiled below the two approaches for optimizing plot design by means of fortls functions are also described 2 1 3 1 stand level metrics metrics are computed using points directly from normalized point cloud in a relatively similar way as in the fusion ldv software for lidar data analysis and visualization mcgaughey 2009 these are statistical descriptive measures such as percentiles or only number of points belonging to specific sections of point cloud the following metrics are available total number of points corresponding to the normal section 5 cm of trees detected after removing some noisy points with ncr and refinement of extracted stem points processes see section 2 1 2 these can be computed for raw point clouds num points and reduced point clouds num points hom number of estimated points corresponding to the normal section 5 cm of trees detected the number of points corresponding to each tree can be estimated according to eq 8 for raw point clouds num points est and analogously for reduced point clouds num points hom est percentiles of z coordinate m computed percentiles are p01 p05 p10 p20 p25 p30 p40 p50 p60 p70 p75 p80 p90 p95 and p99 descriptive statistics of z coordinate distribution mean maximum max minimum min standard deviation sd variance var mode kurtosis and skewness percentage of points above mode perc on mode and mean perc on mean values of z coordinates scale weibull b and shape weibull c parameters of a weibull distribution fitted to z coordinates distribution 2 1 3 2 stand level variables variables represent estimates based on the attributes of trees detected from tls point cloud data further aggregated at stand level and finally expanded to unit area ha the following variables are available apparent stand density n tls trees ha 1 which is estimated for trees detected in a similar procedure to that used in conventional inventories for circular fixed area and k tree plots eq 9 and angle count plots eq 10 9 n t l s 10000 π r 2 n 10 n t l s i 1 n baf g i where r is the plot radius m n is the number of trees detected in the corresponding plot design baf is the basal area factor m2 ha 1 and g i is the basal area of the tree i m2 apparent stand basal area g tls m2 ha 1 which is estimated for trees detected in a similar procedure as that used in conventional inventories for circular fixed area and k tree plots eq 11 and angle count plots eq 12 11 g t l s 10000 π r 2 i 1 n g i 12 g t l s baf n apparent stand stem volume v tls m3 ha 1 which is estimated for trees detected by modelling stem profile as a paraboloid and calculating the volumes of revolution for fixed area and k tree plots eq 13 and angle count plots eq 14 13 v t l s 10000 π r 2 i 1 n π h p 99 2 i 2 d b h i 2 2 h p 99 i 1 3 2 14 v t l s i 1 n baf g i π h p 99 2 i 2 d b h i 2 2 h p 99 i 1 3 2 where h p 99 i and d b h i are the 99th percentile of points delimited by voronoi polygons m i e estimates of h and dbh m for tree i respectively mean and dominant diameters cm which are estimated for arithmetic quadratic geometric and harmonic means in the case of dominant diameters only the n largest trees per ha according to dbh are considered although it can be specified in the arguments the 100 largest trees per ha are considered by default mean and dominant heights m which are estimated for arithmetic quadratic geometric and harmonic means in the case of dominant heights only the n largest trees per ha according to dbh are considered although the number of trees can be specified in the arguments the 100 largest trees per ha are considered by default in the previous calculations for the k tree design the plot radius r is defined as the mean of horizontal distances of trees k and k 1 kleinn and vilčko 2006 2 1 3 3 dealing with occlusions all fortls functions used for estimating stand variables also include correction of occlusions approaches in the case of angle count plots occlusion corrections are based on gap probability attenuation with distance from tls depending on a poisson distribution in the case of circular fixed area and k tree plots distance sampling methods and shadowing effect correction are considered in order to obtain occlusion corrections based on distance sampling methods the distance sampling function must be executed previously and the values obtained must be incorporated as an argument for functions which compute stand variables whereas the other corrections are computed by default a brief description of the implemented occlusion corrections is given below 2 1 3 4 poisson attenuation model this method has been used in measurements with tls strahler et al 2008 lovell et al 2011 and optical montes et al 2019 instruments to reduce the device related bias in the relascope based approach it is based on geometric gap probability p g a p which decreases exponentially following a poisson distribution eq 15 15 p g a p λ d e r e λ d e r where λ is the number of trees per m2 d e is the effective dbh and r is the horizontal distance to the farthest tree the corrected stand density n pam trees ha 1 for angle count plots is then obtained as follows eq 16 16 n p a m n t l s f λ d e r where f is a function defined as f t 2 t 2 1 e t 1 t with t λ d e d 2 baf see strahler et al 2008 and lovell et al 2011 for further details similarly corrected stand basal area g pam m2 ha 1 and volume v pam m3 ha 1 are computed 2 1 3 5 point transect sampling this approach is based on the point transects method from distance sampling methods buckland et al 2001 these methods use detection functions g r θ with variable r distance from sampling point and parameter θ which describe how the probability of detection decreases as distance increases we used half normal eq 17 and hazard rate eq 18 functions as these have been successfully used in measurements with tls astrup et al 2014 and optical montes et al 2019 devices 17 g r θ e r 2 2 σ 2 18 g r θ 1 e r σ b parameter θ includes the shape b only in the hazard rate function and the scale σ which was also expanded with dbh as a covariate into an exponential function eq 19 according to ducey and astrup 2013 and astrup et al 2014 19 σ α 0 e α 1 d b h parameter θ is estimated by maximum likelihood marques and buckland 2003 miller and thomas 2015 clark 2016 with data left truncated at 1 m according to astrup et al 2014 the fitting process is carried out by means of the ds function included in the r package distance miller et al 2019 once the parameters of detection functions are estimated the probability of tree detection p i is estimated with eq 20 which is implemented in the expansion factor of circular fixed area and k tree plots as in eq 21 20 p i 2 r 2 0 r r g r θ ˆ 21 e f i 1 n 10000 p i π r 2 where r is the plot radius m and ef is the expansion factor multiplying n tls by these efs yields corrected stand densities n hn n hr n hn cov n hr cov trees ha 1 similarly corrected stand basal area g hn g hr g hn cov g hr cov m2 ha 1 and volume v hn v hr v hn cov v hr cov m3 ha 1 are computed all of these estimates are obtained by previously executing the distance sampling function table 1 which returns p i and also values of parameter estimates of detection functions and the corresponding akaike information criterion aic of these fits 2 1 3 6 correcting the shadowing effect this approach was developed by seidel and ammer 2014 for single scan mode these authors used the approach to correct the shadowing effect which generates shaded unsampled areas eq 22 according to the shaded area percentage related to the total area sampled 22 a s h a d o w π r 2 π r t r e e 2 360 d b h r t r e e π d b h 2 2 2 where r is the radius of the plot m and r t r e e is the distance between the tls instrument and the tree centre this method is implemented for circular fixed area and k tree plots and yields an expansion factor used to compute corrected estimates for stand density n sh trees ha 1 basal area g sh m2 ha 1 and volume v sh m3 ha 1 2 1 3 7 optimizing the plot design two different approaches can be used to find the best possible plot design depending on whether validation field data are available or not the approaches are represented in the fortls workflow fig 6 and detailed below 2 1 3 8 analysis of estimation stability the function estimation plot size estimates both apparent tree density n tls trees ha 1 and apparent basal area g tls m2 ha 1 for all of the aforementioned plot designs in the case of circular fixed area plot design concentric plots in regular increments of 0 1 m radius by default to the maximum radius specified in the arguments are simulated for computing n tls and g tls as a result line charts with estimates through plot size are obtained for k tree design all possible plots are defined by k 1 2 n where 1 is the nearest tree and n the farthest tree considered in the argument k tree max or the farthest detected existing tree if the argument is not specified finally for the angle count design variables will be estimated for regular baf increments comprised from 0 1 to the baf max specified in the arguments all of these line charts were inspired by fig 3 in brunner and gizachew 2014 2 1 3 9 validation with field measurements for cases when field data are available we designed a set of interconnected functions able to assess the performance of processed tls data relative to the corresponding field data simulations relative bias and correlations the field data necessary to conduct the analysis described hereinafter are tree dimensions dbh and h and positions relative to the tls scanner analysis of the performance is based on comparisons between these two data sources for the different plot designs and sizes the first function is simulations which computes in a similar way as estimation plot size all of the metrics and variables aforementioned for tls data 2 1 3and the corresponding variables based on field data table 2 the relative bias function was designed for direct comparison of tls based estimates and field based measurements by means of relative bias eq 23 23 r e l a t i v e b i a s 1 n i 1 n y i 1 n i 1 n x i 1 n i 1 n x i where x i and y i are the values of the field estimate and its tls counterpart respectively corresponding to plot i for i 1 n relative bias is assessed for all the simulations to find the best possible plot design for each variable of interest for other possible approaches apart from direct variables estimations the package has other functions that assess the best possible plot designs according to the correlations between variable estimates from field data and metrics variables derived from tls the correlations function computes both pearson and spearman correlation coefficients for common set of plots and all simulations and plot designs considered for each variable of interest this function produces the optimum correlations for all simulations the optimize plot design function then produces a graphical representation of the strongest correlations for all variables of interest 2 2 case study the functionality of the fortls package was tested in a fully mapped case study plot of 1 ha 100 100 m in a pure even aged p sylvestris stand located in la rioja spain fig 7 all trees in the plot live and standing dead with dbh greater than 7 5 cm were measured utm coordinates and elevation of the plot corners were measured with a high accuracy gnss receiver trimble r2 and trees were located in the plot with a total station nikon dtm 332 the dbh of all trees was measured with a diameter tape to the nearest 0 1 cm and for live trees only h was measured with a digital hypsometer vertex iv haglöf sweden to the nearest 0 1 m main stand variables estimated from field data are shown in table 3 after conventional inventory we scanned each intersection point defined by a regular 20 m squared grid 16 points sampling with a tls faro laser scanner focus 3d x 130 covering the full horizontal 0 360 and vertical ranges 60 90 with a resolution of 7 67 mm at 10 m in both horizontal and vertical angular apertures all of the single scans were then processed with fortls clipping the point clouds at 20 m radius from the plot centre finally to compare stand variables derived from tls data and from field inventory we also extracted the trees measured at the same 16 points sampling scanned considering 20 m radius all of these data can be found as example data in fortls as a list named rioja data in which the first element corresponds to the list of trees detected from tls in each plot tree list tls and the second element corresponds to the list of trees measured in each plot in the field tree list field 3 results the steps involved in processing the tls data with fortls are described below along with the different operations and arguments of the functions finally the results of tls data processing with fortls are presented for the aforementioned case study 3 1 fortls implementation users can install the released version of fortls from cran https cran r project org package fortls or they can install the most currently developed version from github https github com molina valero fortls tree devel using the install github function of the devtools package wickham et al 2021 a list with a brief description of the 10 main functions available in fortls 1 0 6 together with their default argument values is given in table 1 the functions were designed to obtain two main uses from tls data 1 conservative tree detection in which the algorithm gives preference to accuracy and 2 computation of metrics and variables related to forest attributes 2 1 with no field data available and 2 2 with the corresponding field data available the workflow of functions involved in these approaches is illustrated schematically in fig 6 although the workflow can be processed directly in the current r session most functions are designed to import data and save results to a specific working directory that should be specified in dir data and dir result arguments to facilitate efficient operation in this respect it is important to highlight that outputs from previous functions in the workflow fig 6 are usually inputs in the following functions thus systematization of the work will be enhanced when the data working directory corresponds to the results working directory script with a full example of workflow is supplied as supplementary material s1 which should be consulted while reading the following sections 3 1 1 tree detection this approach is encompassed by point cloud normalization section 2 1 1 and tree detection processes section 2 1 2 which are executed by the normalize and tree detection or tree detection multiple for the cases with several scans functions respectively tls data must be supplied as las or laz files in normalize note that point cloud centre cartesian x y coordinates relative to tls sampling point can be defined otherwise the function will use the file centre coordinates as default the maximum radial distance from the plot centre and the minimum and maximum heights of the z coordinate can also be defined in the arguments this enables possible outliers and unnecessary information to be discarded and decreases the computing time the normalize function will return a data frame with a normalized point cloud which will be saved in dir result as a txt file otherwise specified save result false but with point density reduced by applying pcp this normalized point cloud is necessary as data input in tree detection note that supplying the original not the reduced point cloud is highly recommended for detection of a greater number of trees parameters of tls resolution are necessary in the tree detection function defined according to either angle resolution rad or distance between two consecutive points mm at a determinate distance from tls m some inventory parameters such as minimum and maximum dbh can then be specified in the arguments to prevent detection of smaller and or larger desirable trees other parameters of the algorithm such as ncr see section 2 1 2 and plot level attributes can be specified in the respective arguments ncr threshold and plot attributes respectively this function will return a data frame with the tree centre location in cartesian x y and cylindrical ρ as horizontal distance φ coordinates number of points corresponding to normal section raw and estimated and partial occlusion for all detected trees the data frame will be saved as a csv file in the working directory provided in the dir result argument otherwise specified save result false in the first plot of the example top left in fig 7 and considering a radius of 20 m 38 out of 44 trees were detected however some angular deviation due to the internal compass of tls can be observed fig 8 considering all of the simulated plots our algorithm detected 598 out 659 trees i e 91 of detected trees when sequential analysis of several scans is required tree detection multiple will be the most appropriate function this function includes both normalize and tree detection processes enabling detection of trees for a set of plots in a single function these must be included in the same working directory specified in dir data as las or laz files the output will be a similar data frame as the aforementioned data frame but that includes all of the plots processed as before a csv file containing the data frame information will be saved by default in the working directory specified in dir result and the normalized point clouds reduced by pcp will be saved as txt files an example of a reduced point cloud named pcd corresponding to the first plot in the case study can be found in the supplementary material s5 as a txt file all trees detected with tree detection multiple for the 16 tls sampling points up to 20 m radius in the case study fig 7 and the list of trees measured in the field for these sampling points are included in supplementary material s5 as csv files and named tree list tls and tree list field respectively 3 1 2 estimating metrics and variables related to forest attributes the metrics variables function performs this process by computing a set of tls metrics and variables from point cloud data however other complementary functions play an important role in improving these estimates based on correction of the occlusion effect and choice of a more appropriate plot design in the first case distance sampling approaches based on point transect sampling methods buckland et al 2001 can be applied by means of the distance sampling function these approaches are used for the list of trees detected which must be provided to the function as a data frame with the same format and include the tree detection value the function works for all the plot designs included by default if no arguments are specified but can also only consider a set of specified plots argument id plots must contain this information or even plots distinguished by strata reachable by using the argument strata attributes the function will generate a list with 3 elements i probability of tree detection for the different detection functions and strata if specified ii parameters estimated for detection functions and iii aic estimator obtained for each detection probability function as the criteria for selecting the best fit regarding the second case i e the optimization of plot design more details can be found below at this stage the input data sets for metrics variables will be as follows i list of trees detected ii reduced normalized point clouds loaded from dir data and optionally iii the list with probabilities of tree detection according to distance sampling methods plot parameters considered for estimating metrics and variables are implemented in the plot parameters argument this is a data frame containing the columns named radius k tree and baf in reference to circular fixed area k tree and angle count plot design parameters the absence of parameter values rules out the corresponding plot design it is also possible to include a column named num trees that specifies the number of dominant trees per ha considered to estimate dominant dbh and h otherwise it will be considered as 100 largest trees ha 1 in terms of dbh any plots that are grouped by strata in the tree list input can be identified in a column named stratum to consider different plot design parameters for each stratum in this case another column also named stratum and coinciding with its homologue from the tree list data in the strata coding can be included in the plot parameters argument after execution of metrics variables a list with as many elements as plot designs considered will be returned the elements will include the tls metrics and variables computed for each plot design indicated by default this list will be saved as separate csv files for each specified plot design in dir result otherwise specified save result false an example corresponding to circular fixed area k tree and angle count plots of 15 m radius 12 trees and baf 1 respectively from the case study can be found in supplementary material s5 as csv files named metrics variables fixed area plot metrics variables k tree plot and metrics variables angle count plot 3 1 2 1 field data not available when field data are not available it is not possible to determine how well the tls metrics and variables perform in estimating the corresponding variables in this case the most reliable estimates can be obtained with those plot designs in which estimates are stable for different plot design sizes the estimation plot size function can be used to assess whether the plot designs considered will reach stable values and can thus use the corresponding parameters in metrics variables this function contemplates a fixed circular area k tree and angle count plot designs represented by line charts with estimated values of n tls and g tls y axes through plot sizes x axes the maximum sizes considered for each plot design must be specified in the plot parameters argument as in other functions the absence of any of these arguments rules out the corresponding plot design these charts can be generated for all the plots individually no arguments specified or for mean values across all plots or strata including the standard deviation area argument mean true it is also possible to distinguish strata if they are included in a column named stratum which must be given in the tree list input the last function enables representation in the same chart of estimates of mean values for the different plot designs considered fig 9 which can be executed by setting the argument all plot designs as true 3 1 2 2 field data available when field data are available fortls also includes functions for determining the best possible plot design for yielding maximum correspondence between tls based and field estimates these functions are based on comparison between tls and their corresponding field data and they form one of the main branches in the workflow fig 6 the simulations function represents the first compulsory step which generates estimates of metrics and variables for a set of field plots and their tls counterparts for circular fixed area k tree and angle count plots thus the list of trees detected in the point cloud and list of trees measured in the field are necessary as input data in addition probability based on distance sampling methods can be included as an optional argument if these methods are applied simulations are computed for continuous plot size increments of 0 1 m 1 tree and 0 1 baf by default although other increments can be specified in the plot parameters argument the maximum value of plot parameter simulation must be specified for the respective elements radius max k tree max and baf max of the same argument and as in other functions the absence of any of these arguments rules out the corresponding plot design the number of largest trees per ha implemented in dominant variables can thus be specified in the num trees element of the plot parameters argument by default 100 trees per ha the simulations function will return a list with many elements data frame objects as plot designs considered in which each row will correspond to a simulated pair plot radius k baf and the columns will include all the metrics and variables estimates based on field and tls data table 2 by default these elements will be saved as separate csv files in dir result otherwise specifying save result false after generating simulations two possible processes can be conducted fig 6 one process assesses bias among variables estimated from tls data and their counterpart estimated from field data and is carried out by the relative bias function which computes relative bias between field and tls estimates eq 23 this will be executed with the simulated data generated previously in simulations which are the input data of this function the objective variables can be indicated in the variables argument but by default those most conventionally used in forestry will be considered n g v d dg d 0 h and h 0 this function will return a list with as many elements data frame objects as plot designs considered with rows corresponding to simulated pairs plot radius k baf and columns including relative bias between field and tls variables estimates by default these elements will be saved as separate csv files in dir result otherwise specified save result false in addition interactive line charts with relative bias through size plot design will be generated by groups of variables n g v etc and plot design supplementary material s2 these charts are very intuitive for assessing the best plot design and size for estimating variables directly from tls data the other approach consists of evaluating the correlations between all metrics and variables estimated from tls data and variables of interest estimated from field data both pearson and spearman correlation coefficients are available this is achieved with the correlations function by means of the simulations with different previously generated plot design sizes the target variables can be indicated in the variables argument but by default those most commonly used in conventional forest inventories will be considered n g v d dg d 0 h and h 0 this function will return a list including the following i correlation values for each method and plot design ii another element with the same structure including the p values of test for association corresponding to these correlations and iii the strongest correlations for each set of simulations and the names of the tls metrics or variables to which they correspond the first and third elements will be saved as independent csv files one per correlation method and plot design otherwise specified save result false this function will also generate interactive line charts which will show the correlations between field variables of interest and all the tls metrics and variables computed through regular continuous increase of plot size supplementary material s3 finally all the variables of interest can be assessed together according to the highest correlations achieved by means of interactive heatmaps using the optimize plot design function supplementary material s4 the input data will be the list including the strongest correlations obtained previously third element of the list returned by correlations the objective variables can be indicated in the variables argument but by default those most commonly used in forest inventories will be considered n g v d dg d 0 h and h 0 the optimize plot design function is very useful for determining the most suitable plot design and size for all the variables of interest considered 3 2 experimental test case we used the study plot to explore the potential of fortls to estimate forestry variables at stand level we present some of the most important results here but all of the analyses and outputs are reported in more detail in supplementary material s1 s5 3 2 1 estimation without field data available as tls plots are established on the basis of a systematic design sampling inventory and covering the same stand conditions we considered assessing mean values across all tls sampling points using the estimation plot size function and comparing all the plot designs in the same chart regarding n tls all of the plot designs yielded stable estimates from a given plot design size onwards these estimates were 12 16 m for circular fixed area 3 16 trees for k tree and 0 8 2 baf in angle count plots fig 9 according to the standard deviation different trends were observed in plot parameter values which were quite stable in a circular fixed area decreasing in k tree and increasing in angle count plots the patterns were very similar for g tls estimates in this case study estimates in the stable zones reached very similar values for the three different designs the stable zones showed similar pattern estimates for both n tls and g tls for n tls estimates the estimated patterns almost completely coincided for a fixed radius of 13 17 m 12 36 for k with the k tree approach and 1 2 1 6 for baf reaching approximately the value of 315 trees ha 1 in all of these plot design which is very close to 322 live trees ha 1 measured in the field table 3 regarding g tls estimates were similar for fixed radius of 13 17 m 12 36 for k and 0 8 1 2 for baf reaching a slightly higher value than 23 m2 ha 1 which is lower than 25 29 m2 ha 1 estimated from field measurements table 3 thus in both cases the estimates slightly underestimated the stable sections of the curves relative to stand variables obtained from field measures 3 2 2 estimation with field data available although all the variables and plot designs are assessed in supplementary material s2 we focused here on g and circular fixed area plots to study relative bias in more detail all of the variables estimated from tls data showed positive values of relative bias for smaller plot sizes and negative values as plot size increased with peaks reaching even higher than 50 for very small plots fig 10 these peaks were followed by a first sharp decrease until approximately 8 m radius and then a less steep decrease until the largest plot size the best results were achieved with variables estimated with occlusion corrections methods with most of them reaching low constant values of relative bias of radius between 15 and 17 m however the most stable estimates and lowest relative bias were based on half normal detection function between 17 5 and 19 m radius with relative bias below 1 although all of the variables plot designs and correlations methods pearson and spearman are assessed in supplementary material s3 here we evaluated the pearson correlation between arithmetic mean dominant height estimates based on field data h 0 and the other metrics and variables obtained for tls data for angle count plots in this case the highest correlations were obtained with metrics corresponding to z coordinate percentiles especially p95 which yielded correlations above 0 94 and h 0 estimates based on tls data fig 11 the general trend for all the metrics and variables was a slow decrease until a value of 1 9 baf with the highest correlations achieved in this baf range the decrease then became steeper until reaching values below 0 8 interestingly p95 performed better than the other metrics with a less pronounced decrease in terms of correlation the most suitable plot design considering performance of all the variables of interest can be assessed with optimize plot design again this is evaluated for all plot design and correlation in supplementary information s4 but here we only assessed the case of k tree plot and pearson correlation different trends differentiated groups of field variables first both h 0 and h yielded higher correlations when more trees were included and were reached for h earlier than for h 0 fig 12 once high correlations were reached they remained stable above 0 9 until the largest k tree plot the diameter variables d 0 dg and d remained fairly stable across all plot sizes with slightly higher values in smaller plots however the correlations were not very high with values of between 0 6 and 0 8 for the other variables v g and n the strongest correlations were obtained for smaller plots with the highest values reached for plots of 6 10 trees and they then decreased slightly before remaining stable in the case of n good correlations were also achieved for between 13 and 17 trees 4 discussion although some algorithms and applications have been developed for processing tls point clouds for forestry purposes liang et al 2018a krok et al 2020 tls has not yet been established as an operational device in fis liang et al 2016 to overcome this challenge most researchers agree that automation of point cloud processing with attainable and easy to use software able to extract information related to important forest attributes is essential dassot et al 2011 newnham et al 2015 white et al 2016 liang et al 2016 2018a fortls contributes to making progress in this challenge by automating the processing of tls point clouds for estimating fi variables this is achieved by detecting trees and estimating some of their attributes and also generating metrics and or variables related to conventional forestry variables at stand level in addition some of the functions enable assessment of the performance of metrics and variables estimates for different plot designs based on comparison with corresponding field data thus allowing the best possible plot design to be established in each situation this functionality confers flexibility to fortls in statistical inference for fis representing the main difference relative to similar applications bienert et al 2007 trochta et al 2017 unlike other existing applications which extract tree attributes in high detail such as computree othmani et al 2013 simpleforest hackenberg et al 2015 adtree du et al 2019 and treegsm raumonen et al 2013 fortls mainly focuses on providing metrics and variables related to forestry attributes at stand level this approach together with field implementation based on single scans enables larger areas to be covered although at the expense of lower detail at tree level we therefore consider that our approach perceives tls devices as sampling rather than measurement instruments and applications with similar perspective to ours are very scarce e g 3d forest mainly developed for describing forest 3d structure and autostem mainly focused on timber production although use of these tools cannot be directly compared with our case study because the methods and stands conditions are different some useful information can be obtained regarding 3d forest the case study was framed in a stand characterized by highly variable canopy openness and dominated by sessile oak quercus petraea matt with a mixture of other broadleaf species trochta et al 2017 the authors sampled a plot of 2 4 ha in which the dbh of all the trees with dbh 10 cm 824 and h of 181 trees were measured the plot was then scanned with a resolution of 2 mm at 10 m by multiple scans set at 44 44 m which were co registered before processing data in 3d forest the best results yielded a relative bias of 1 positive and 0 8 negative for d and h respectively in our case the relative bias for d was negative and slightly higher than in trochta et al 2017 with the best performance obtained for angle count plot design reaching peaks with lower relative bias than 1 for low values of baf s2 however although the quadratic mean dbh d g yielded the best performance in all cases h was always underestimated reaching similar relative bias only for circular fixed area plots of around 18 m radius s2 this again shows that height variables are systematically underestimated liang et al 2016 2018a krok et al 2020 on the other hand autostem was assessed for a stand planted with picea sitchensis bong carr with a current density of 600 trees per ha mengesha et al 2015 this was measured in nine randomly located plots of 15 m radius in which all trees were measured by conventional methods and single tls scans were made from the plot centre with a resolution of 6 28 mm at 10 m the overall difference between tls derived and conventional volume estimates was 5 6 when occluded trees not detected by tls were excluded from the analysis of both sources i e tls and field data and 10 2 when estimates based on tls were corrected by simple correction factors fortls yielded much lower relative bias in volume estimates especially for the k tree plot design and estimates corrected with distance sampling methods g hr cov which yielded values of around 0 for 12 14 trees s2 in any case comparison with these findings should be done with caution as we used paraboloid function for estimating volume in contrast to mengesha et al 2015 who applied methods based on retrieved stem profiles nevertheless this is the most interesting comparison because the aforementioned authors used a very similar sampling methodology with single randomly located scans systematic in our case and the corresponding field plot measurements all plot designs considered circular fixed area k tree and angle count plots yielded stable estimates in n tls and g tls for certain plot size ranges fig 9 as occurred for g estimates using a 2d tls device fig 3 brunner and gizachew 2014 however the variables were slightly underestimated unlike in brunner and gizachew 2014 in which unbiased g estimates between real and scan basal area were observed for 5 10 m radius plot size only for one of the studied stands however our findings are generally consistent with the most recent findings which indicate underestimation of n and g due to deficit in tree detection caused by occlusions especially for single scans liang et al 2016 2018a krok et al 2020 in our study uncorrected estimates g tls yielded lower bias for smaller plot sizes due to lower occlusion rates which is consistent with the findings of corona et al 2019 who concluded that under easy to measure stand conditions plots of 10 m radius in which occlusion corrections are not considered may be good enough for estimating g to overcome underestimates derived from occlusion effects we incorporated several methods applied in tls single scans in other studies which improved estimations in larger plot sizes where g hn yielded the best estimates for 17 5 19 m radius fig 10 this finding is consistent with those of astrup et al 2014 who reported that a larger detection radius seems to improve estimates when distance sampling methods are applied the results presented here indicate that correction occlusion methods can improve estimates as observed for g in angle count strahler et al 2008 lovell et al 2011 and circular fixed area plots seidel and ammer 2014 astrup et al 2014 because these methods can be assessed in fortls in terms of relative bias through continuous plot size increment and different plot designs this represents an advantage for determining the best possible plot design in execution of a single workflow fortls also evaluates correlations between variables of interest and tls derived metrics and variables this contribution implies a new perspective enabling selection of the best possible plot design according to statistical correlation measures instead of measurement accuracy this approach may be considered for estimating forestry variables assisted by or based on models in a similar way as aba inference developed for als devices næsset 2002 in some cases simple linear regressions may be fitted when strong relationships are observed for only one tls metrics and or variables as between h 0 and p95 for the study case with correlations above 0 94 fig 11 in this case the plot design considered was an angle count plot for a baf value of 1 3 in addition this concept provides an opportunity to solve the systematic problem regarding underestimation of height variables derived from tls measurements liang et al 2016 2018a krok et al 2020 different groups of variables yielded the highest correlations at different plot sizes fig 12 when the correlations for height variables were strongest in larger plots diameter variables retained approximate stable correlations and the highest correlations for other variables n g and v were reached in smaller plots thus the plot design could be adapted to our stand conditions and target variables for more efficient sampling here we have demonstrated the utility of the r package fortls in fis in a case study as fortls works with single scan data co registration of point clouds in specific software and placement of targets for field measurements are not required this improves data acquisition and shortens the processing time as well as enabling the sample size to be increased in a cost efficient manner which is one of the most desirable features of tls in fis liang et al 2016 further research to consolidate fortls for the approaches mentioned here should encompass the following i larger and more complex study cases ii consideration of more metrics and variables with high potential for correlation with other forest attributes leaf area index species etc iii exploration of the possibility of making inferences assisted by models by developing an adequate sampling methodology and iv improvement of the computation process as much as possible in relation to both algorithms and computing time 5 conclusions the r package fortls is useful software for processing tls data for forestry purposes it has the advantage of working with single scans and conducting automatic data processing which may overcome the major challenge of affordability in data acquisition and data processing it has yielded good results for conventional variables based on a preliminary case study with direct estimates as well as good correlations between field derived variables and tls derived metrics and variables however its potential for producing model assisted inferences from metrics and or variables has not yet been demonstrated in addition one of the most valuable features of the software is its flexibility to adapt to the best possible plot design for each variable enabling multiple plot designs to be used in a single sampling design further research considering larger and more complex case studies is necessary to consolidate fortls as an operational tool in fis as well as to develop new metrics and variables 6 software availability name of software fortls 1 0 6 developers juan alberto molina valero maría josé ginzo villamayor manuel antonio novo pérez adela martínez calvo juan gabriel álvarez gonzález fernando montes césar pérez cruzado contact address unit for sustainable environmental and forest management uxafores department of agroforestry engineering higher polytechnic engineering school universidade de santiago de compostela benigno ledo s n campus terra 27002 lugo spain email juanalberto molina valero usc es software required r 3 5 0 first available march 2 2021 availability https cran r project org package fortls installation in r install packages fortls program languages r and c license gpl 3 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work was supported by the spanish ministry of science and innovation agl2016 76769 c2 2 r pid2020 119204rb c22 and galician regional government 2020 cp031 ed431f 2020 02 jamv was supported by the spanish ministry of science innovation and universities through the fpu program fpu16 03057 amc was supported by galician regional government within the framework of the agreement development of the galician continuous forest inventory 2020 cp031 cpc was supported by the spanish ministry of science and innovation ryc2018 024939 i the authors thank diego lombardero barrera joel rodríguez ruiz mario lópez fernández and óscar lópez álvarez for help with fieldwork we are also grateful for the comments made by anonymous reviewers which helped us to improve the quality of the paper appendix a supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 multimedia component 3 multimedia component 3 multimedia component 4 multimedia component 4 multimedia component 5 multimedia component 5 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105337 
