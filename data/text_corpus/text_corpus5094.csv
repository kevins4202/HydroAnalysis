index,text
25470,in this manuscript we present b ama basic data driven models for all an easy flexible fully coded python written protocol for the application of data driven models ddm in hydrology the protocol which is open source and freely available for academic and non commercial purposes has been realized to allow early career scientists with a basic background in programming to develop ddm ensuring that no stones are left unturned through their implementation b ama embeds data splitting feature selection hyperparameter optimization and performance metrics a jupyter notebook with a practical workflow is available to guide the users through the protocol employment while visualization tools allow efficient investigation and communication of results we tested b ama across four hydrological applications to explore ddm applicability across temporal resolutions time series lengths and autocorrelations b ama showed great accuracy and reasonable computational time making the protocol ideal for educational purposes and for the development of ddm based forecasts of hydrological time series keywords modeling protocol data driven models hydrological predictions data availability data and codes used to develop this research are available for non commercial research and educational purposes at https github com alessandroamaranto b ama 1 introduction hydrological extremes have significantly affected society and environment over the last decades ficklin et al 2022 satoh et al 2022 during the european 2003 drought more than 70 000 fatalities and 8 7 billion eur damage were recorded due to the extreme heat conditions sutanto et al 2020 recently the 2021 flood event occurred in germany caused at least 220 casualties and about 17 billion eur of economic damage koks et al 2021 unfortunately the impacts of these hydrological extremes are expected to further increase in the near future also due to anthropogenic climate change hirabayashi et al 2013 and increasing human exposure jongman et al 2012 mazzoleni et al 2021 thus it is of pivotal importance to design sustainable adaptation actions to minimize the future occurrence and impacts of hydrological extremes however water management strategies are highly dependent on the correct prediction and forecasting of floods and droughts brunner et al 2021 for example the hydraulic design of levee systems used for coping with floods requires the accurate estimate of the river water level corresponding to the probability of occurrence of a certain return period for this reason over the last decades a number of tools have been developed for assessing crucial hydrological variables such as soil moisture water level and river discharge and the consequent drought and flood risk candido et al 2022 in particular hydrological and land surfaces models have been widely implemented to assess river streamflow based on current and future climate condition brunner et al 2021 in their recent review horton et al 2022 analyzed the implementation of a number of hydrological models for characterization and quantification of floods and droughts climate change and land use change impact analysis uncertainty analysis operational forecasting and large scale modelling despite this broad range of applications hydrological models are still affected by certain sources of uncertainty linked to model calibration data quality and model structure the increasing availability of remote sensing observations global reanalysis data internet of things and crowdsourced observations together with improvement in computational resources graphic processor units and artificial intelligent have revolutionized water science and opened the way for pioneering the spreading of data driven models lecun et al 2015 reichstein et al 2019 sit et al 2020 among these models we can find multi linear regression lasso regression random forests decision trees support vector machine artificial neural networks and recurrent neural networks algorithm such long short term memory hauswirth et al 2021 data driven models have been extensively developed for a variety of environmental and hydrological applications solomatine and ostfeld 2008 in an early application of data driven model campolo et al 2003 used an artificial neural network to predict water level variation up to 6 h in advance in an unregulated river basin wu and chau 2010 compared four data driven models i e auto regressive moving average k nearest neighbors and artificial neural networks for monthly streamflow prediction in deo et al 2017 multivariate adaptive regression splines least square support vector machine and m5tree models were used for drought forecasting purposes in eastern australia the authors highlighted the importance of periodicity as a predictor variable for drought modelling support vector machine was used by roodposhti et al 2017 to extract reliable spatio temporal patterns of drought sensitivity in response to vegetation changes amaranto et al 2018 used multiple data driven models e g random forests support vector machines artificial neural networks extreme learning machines and genetic programming to improve the forecast of groundwater tables up to five months a nonlinear auto regressive with exogenous inputs and self organizing map approach was used by kim and han 2020 for real time urban flood maps and improve flood response capabilities similarly guo et al 2022 explored the applicability of convolutional neural networks for fast flood prediction and flood extent mapping the authors found that this approach can generalize the information from the training data to other unseen terrains suggesting its potential use as rapid surrogate model for flood predictions recent studies proposed new hybrid approaches for integrating hydrological model with data driven methods for example yang et al 2019 coupled a machine learning model i e long short term memory units with a global hydrological model i e ghms cama flood model to improve the flood simulations for a more robust and confident flood risk assessment the results of this study show that while the global hydrological model can predict peak discharge the proposed hybrid approach outperforms the original ones when prediction the timing of the peak and drastically improve the performance of global flood simulations in their study ghaith et al 2020 achieved better representation of daily streamflow prediction by integrating physically based and data driven approaches sezen and partal 2022 proved that hybrid models that combines advantages of conceptual and data driven models can provide more accurate forecasting results than hydrological and data driven models similar improvements were also found by althoff et al 2021 when coupling machine learning model with the gr4j hydrological model advances in data science has led to the growth of programming libraries frameworks and toolboxes for the implementation of data driven machine learning and deep learning algorithms python and r packages like svars lange et al 2021 tensorflow pang et al 2020 keras géron 2019 mxnet chen et al 2015 superml and theano brownlee 2017 have been widely implemented similarly modelling frameworks such as ludwig molino et al 2019 and mlflow zaharia et al 2018 have been developed to provide a general modularized machine and deep learning architecture to a broad audience of engineers and scientists for performing a vast amount of tasks moreover libraries like tslearn tavenard et al 2020 and pyts faouzi and janati 2020 have used for processing building and training machine learning models of time series data a few toolboxes have been developed specifically for hydrological applications among them the neuralhydrology library kratzert et al 2019 aims at proving deep learning models for rainfall runoff modeling furthermore abbas et al 2022 developed a python based framework for pre processing hydrological data building and training machine learning hydrological model and post processing the model results while these new frameworks help in applying machine learning based models in hydrological sciences they are also mostly focused on building the optimal model architectures while more research should be devoted in integrating an automated input variable selection algorithm in a ddm framework wu et al 2014 abbas et al 2022 considering the high impact that proper water resources management can have on society it is crucial to empower non expert users early career scientists and students with accessible tools to explore hydrological predictions however none of existing ddm toolboxes are designed for educational and dissemination purposes for these reasons we developed b ama an easy flexible fully python coded and open access protocol that will guide users through ddms implementation ensuring that none of the fundamental steps optimal data split input variable selection cross validation is overlooked in the process b ama also embeds a jupyter notebook with a practical workflow to enhance the learning by doing and guide the user through the protocol employment to real world problems the visualization aid implemented in b ama will empower the users with an effective dissemination of the results to a broad audience b ama complements existing modeling framework thanks to 1 its simplicity by allowing scientists and students with very little coding background to build ddm for time series forecasting using a single line of code 2 its adaptability to educational purpose thanks to a jupyter notebook it can guide non expert users through all the required steps to build ddms 3 its modular structure by allowing expert users to include in the protocol any data driven modeling technique by developing a simple function file and 4 its vertical structure b ama is not solely focused on optimal training and architecture definition but embeds all the steps required to build ddm it especially includes three automated input variable selection algorithms for multi dimensionality control and optimal feature extraction 2 protocol description 2 1 general framework the general framework of the python based b ama protocol is illustrated in fig 1 and it is designed to be easy to implement flexible and open source easy the protocol is specifically designed for users that have little experience with ddms and limited programming skills with only a single line of code anyone will be able to predict the desired hydrological variable with a given lead time value flexible b ama allows expert users to define more advanced configuration settings based on their preferences its modular structure allows the easy definition and integration of new modeling techniques into the protocol open source b ama is fully coded in python commented to facilitate the code understanding for non expert users and the source code is open to everyone so that expert users could also add different modelling techniques not included in b ama yet to operate the protocol the user should provide the input data and a configuration settings file specifying the application name i e the case study name the type of ddm to be employed the time series length and its periodicity however expert users could provide an advanced configuration settings selecting alternative options regarding each step of the b ama protocol described in the following sections and represented in fig 1 once input data and configuration settings are provided b ama imports the data and optimizes the split between training and test set data division step in fig 1 training and test set are then normalized using the training set minimum and maximum as normalization parameters on both sets data transformation step in fig 1 a model based forward input variable selection procedure is developed to select the most relevant input variable and lags such feature also returns visual analytics highlighting how modeling performances on the cross validation set are affected by the input combination the selected training input subset is then used to optimize via k fold cross validation the ddm architecture and the model s hyperparameters model training step in fig 1 cross validation performances are stored and visualized to provide insights upon the interdependence between tuning parameters model structure and accuracy the optimally trained models are evaluated and tested using the nash sutcliffe efficiency index testing step in fig 1 finally the user can assess predictions accuracy and gain qualitative insights about extrapolation ability through training vs test scatterplots time series analysis and other types of visual aid to compare model to observed vs predicted variables 2 2 b ama protocol structure and methods employed 2 2 1 data division the correct implementation of ddm requires the statistical distributions of the training and the test sets to be approximately the same bhattacharya et al 2007 this could be done by performing an exhaustive iterative random selection of subsets until statistical homogeneity is achieved however the randomity of the experiment prevents reproducibility of the results therefore to ensure reproducibility of modeling results two different data division techniques are employed here optimal division default option b ama constrains the randomization of the splits by limiting the iterative search of the test set only to consecutive years corresponding to 30 of the total number of time steps see amaranto et al 2019 2020 for more information among all the possible splits the protocol selects the optimal one s by satisfying the following rule 1 s arg min s μ r s 1 2 σ r s 1 2 where μ and σ are the ratios between means and standard deviations of the training and the testing set outputs after normalization respectively and the optimal split s is selected by solving equation 1 through an exhaustive search procedure this procedure ensures statistical homogeneity through the training and the test set and it is therefore selected as default while running the protocol however a user could be interested in assessing modelling performance on most recent data or on a particular event consequently b ama allows for a second split technique custom division b ama allows the user to define the initial year of the validation period once the selection is performed the protocol automatically assigns the n where n 30 of the total time steps following time steps to the test set 2 2 2 data transformation in this step b ama transforms each input and the output variable to ensure extrapolation ability of ddm especially those employing logistic or sigmoid transfer function again two methods are employed normalization default option data are constrained to the range 0 1 according to the equation 2 z x min x max x min x where x min x a n d max x are a generic input its minimum and maximum value in the training set respectively seasonal adjustment considering how hydrological time series are often characterized by strong seasonality b ama allows also to remove the cyclostationary deterministic component soncini sessa et al 2007 from each variable as 3 z x μ t mod t σ t mod t where μ t mod t and σ t mod t are the cyclostationary mean and standard deviation of the variable x t is the time index and t is the time series periodicity 2 2 3 input variable selection the selection of the best input subset represents a pillar component in ddms implementation but often overlooked wu et al 2014 the proper selection of the ddm input allows for maximizing input output interdependence leading to an increase in modeling accuracy may et al 2011 expert users can already identify the ideal input vector space if the dynamics of the system are clearly understood however when this is not the case input variable selection ivs needs to be implemented leading to a computationally expensive task for example considering a n dimensional input space there are 2n 1 possible combinations the complexity of the problem grows further to 2nk 1 when considering also the appropriate lags to be chosen among k candidates it is therefore pillar to find an algorithm balancing the tradeoff between computational effectiveness and efficient exploration of the input space to minimize such tradeoffs b ama employs three different ivs approaches forward selection default option the forward selection algorithm is a linear iterative search strategy selecting an individual candidate variable at each iteration may et al 2011 it starts by training n where n is the number of inputs single input single output models the optimal input variable is selected by maximizing the model performances based on some optimality criteria e g the nash sutcliffe efficiency index on the cross validation set the procedure then continues by training iteratively n 1 multiple input single output models adding each time one of the remaining candidates to the previously selected variable convergence is reached when no improvement in modeling accuracy is achieved with the addition of other input variables exhaustive search the forward selection approach is considered overall computationally efficient often resulting in relatively small input variable sets it is therefore selected as default configuration however in case of low input number a user could be interested in exploring all possible input combinations to select the most informative for this reason b ama also employs exhaustive search as ivs method even though exhaustive search basically prevents the algorithm to find a local optimum it is computationally extremely expensive and its application is recommended only when the data dimensionality is relatively small amaranto et al 2019 correlation based selection as a third option the user could select a correlation based ivs this method starts by ranking input candidates upon their correlation with the dependent variable and training a single input single output model using the input at the top of the rank the procedure then continues by iteratively training multiple input single output models adding at each iteration the highest correlated variable among those remaining the method stops when the addition of variables fails to ensure improvements in modelling accuracy since it only trains one model at each step the correlation model based ivs is faster than forward selection which being sv the number of selected variables in a certain step trains n sv models on the other hand by only searching through a very small subset the chances of encountering a local optimum are much higher 2 2 4 model training the protocol employs artificial neural networks ann and support vectors machines svm two among the most widespread and easy to use ddm in hydrology with applications among others on groundwater forecasts taormina et al 2012 sun et al 2016 sahoo et al 2017 amaranto et al 2018 2019 2020 lee et al 2019 pham et al 2022 flood prediction elshorbagy et al 2010b elsafi 2014 aziz et al 2014 falah et al 2019 dhunny et al 2020 precipitation forecasts di nunno et al 2022 and water quality modeling wu et al 2014 rehana 2019 bisht et al 2019 support vector machines default option svms are a supervised modeling technique developed by vapnik 1999 2013 for classification and regression purposes raghavendra and deka 2014 their goal is to find a function f x that in addition to being as flat as possible to ensure generalization ability has at most ε deviation from the target variable for all the training data this practically means that all the errors within the ε tube in fig 2 a are ignored providing a better generalization ability of the model muller et al 1997 instances located at the edges of the ε tube are called support vectors of course it is not always possible to approximate a hydrological process with ε precision in order to allow for larger errors while still ensuring the flatness of f x an additional parameter is introduced in the svm training process it is a constant term c representing the tradeoff between the flatness of the function and the extent up to which errors larger than ε are tolerated one of the main advantages of svm regression is their ability of mapping the input vector in a higher dimensional space hyperplane and keeping at the same time regression in the linear form such operation is performed through the kernel function which is often linear sigmoidal or a radial basis function artificial neural networks multi layer perceptron mlp are one of the most common among the machine learning techniques for hydrological applications oyebode and stretch 2019 they consist of an input layer a hidden layer and an output layer fig 2b the first has the sole purpose of distributing the input further and is constituted by as many nodes as the number of inputs the connection between the input and the hidden layer is ensured by weights a see fig 2b which represent the strength of the interdependence between each feature and the output the hidden layer is constituted by nodes whose number often depends on the complexity of the system analyzed their determination often requires optimization each node in the hidden layer comes with a transfer function f zoomed rectangle in fig 2b which introduces nonlinearity by means of a transfer function often represented by a sigmoid or a hyperbolic tangent the process is repeated in a very similar fashion through the connection between the hidden layer and the output layer which is however constituted by a single node or as many nodes as the number of outputs and often embeds a linear transfer function 2 2 4 1 architecture and hyperparameter optimization from a conceptual perspective training ddm follows the same sequence of steps no matter the choice of the modelling technique the training set is split in proper training and cross validation set at each split every candidate architecture is tested and the one minimizing the error statistics in the test set stored the procedure therefore generates an ensemble of models one for each fold in the cross validation set constituted by the most performing architecture and hyper parameter values each of the ensemble member subsequently generates a forecast in the test set the ensemble average is then used for evaluation purposes as far as the svm are concerned it is evident from the above discussion that optimal tuning requires determining the kernel type in addition to the ε and c parameters their value together with the maximum tolerance as a stopping criterion is determined via exhaustive search of their exponential combination with the b ama default search space for individual hyperparameter summarized in table 1 the transfer function and the number of nodes n in the mlp hidden layer are determined in a similar fashion the weights instead are optimized using the levenberg marquardt back propagation algorithm an iterative procedure requiring the definition of a regularization parameter α to avoid overfitting a learning rate and a maximum number of iterations the b ama default search space of each ann parameter is summarized in table 2 for each modeling technique b ama searches through every possible combination and selects those minimizing the error in the cross validation set experience users could modify the feasibility space of ddm hyperparameters via the advanced configuration settings file 2 2 5 testing modelling performances are tested in the test set by means of the nash sutcliffe efficiency index nse and the normalized root mean squared error rmsep these comparative estimators provide an assessment of the modelling accuracy with respect to the mean rmsep and to the standard deviation nse of the process their mathematical formulation can be expressed as 3a r m s e p i 1 n o i p i 2 n o 3b n s e 1 i 1 n o i p i 2 i 1 n o i o 2 where n is the number of observations and o i p i and o are the observed values the predicted variables and the observations mean respectively 2 2 6 visualization and post processing features to allow for a better understanding and communication of the modeling results b ama embeds a number of visual aids aimed at representing the variability in accuracy through each step in the modeling process thanks to such visualization features we aim to expand the classic representation of the results which is often limited to the analysis of the error in the test set by providing insights on the effect of variable selection and model architecture on predictive performances furthermore more experienced users can access the csv output files containing the observed and predicted time series to develop their own visual analytics and communicate results to the community 2 3 implementation of the b ama protocol fig 3 shows how the algorithmic implementation of the b ama protocol is organized into folders regardless of the type of input data and the case study under investigation b ama consists of the following straightforward five steps installation install all the required pre requisites libraries see appendix i for a complete list input loading load the input data in the input case study folder make sure that the dependent variable is in the last column of your csv file in particular the protocol communicates with the input data once they have been loaded into csv format in the input folder through the default configuration settings file left panel in fig 4 define settings in the configuration file the information to be specified in the default configuration file left panel in fig 4 are the case study name the periodicity of the variable to be forecasted expressed in number of time steps the initial year and the final year with observations and the modelling techniques to be used case study name is used by b ama to link the protocol to the input file and the periodicity is used to perform a sequence of internal operations such as optimal split and k fold cross validation it is worth mentioning that considering a 70 30 ration between training and test set and to ensure that all the dynamics under investigation are captured in the modelling process the time series length should be at least four time longer than the periodicity initial and final year are optional parameters used by the protocol to develop time series plot of observed vs predicted output while the modelling techniques provides information on the hyperparameters and modeling architecture to be optimized more experienced users could modify the default configuration through the advanced configuration settings file right panel in fig 4 and select among different data split data transformation and input variable selection options as well as for enlarging the candidate s feasibility set and modeling architectures to be explored during training see section 2 1 run the protocol open the anaconda prompt terminal and navigate to the b ama folder type python ddm run py in the terminal window b ama will then run all the protocol steps as described in section 2 2 alternatively one could also open the ddm run py script and after navigating to the b ama folder run it through spyder from a practical perspective b ama reads the five aforementioned values implements all the steps described in section 2 1 in the default configuration and returns to the user a combination of output text files for further users investigation and of visual aid to be inserted in a presentation or a scientific paper generate model output text files and visual aids are generated to better describe and understand modelling results after performing steps one to four depending on the computational time the message process completed will appear in the terminal window the outputs are then saved in the output case study where case study corresponds to the one specified in the configuration settings file folder to be created by the user the user can navigate to that folder to find output text files and a sequence of graphics describing modeling performances during input variable selection training and model testing more details and illustrative examples of such visual aids are provided in section 4 4 2 3 1 modular structure of the b ama protocol the modular structure of b ama allows the integration of new modeling techniques in the protocol with just three steps 1 definition of a model specific configuration settings file 2 definition of a python script including the functions for training and testing the model 3 insertion of the new model s name in the original configuration settings file in particular the model specific configuration settings file must be named as newmodel module config txt where newmodel corresponds to the model s name assigned by the user and placed in the protocol folder the user should include the dimensionality of the hyper parameter optimization problem and the candidate hyper parameters value fig 5 shows an example of a model specific configuration settings definition for a hypothetical model called am from the names of the authors that requires the optimization of two parameters dimensionality 2 p0 to be selected between 2 6 8 and 10 and p1 to be selected between 50 100 and 200 the python script instead must be named newmodel module py where newmodel corresponds to the model s name placed in the main folder and only includes two functions one for training and one for testing fig 6 shows an example on how such functions must be defined including the input data dimensionality they should be adapted to it is worth noticing that to add a new module to the protocol no new data or no modification to the main code must be performed and the only necessary tools are the model specific configuration settings file and the function script the new model s name must also be specified in the main configuration settings txt file i e to add the hypothetical am module the instance model in the left panel of fig 4 should be equal to am to guide the user towards the inclusion of a new module the online version of the protocol also includes an illustrative workflow for the addition of a lstm neural network module 2 4 the jupyter notebook learning water resources managemental and risk assessment can be challenging for someone with limited background in modelling and data analysis for this reason it is extremely important teach complex programming and modelling tools in a comprehensive and accessible way however traditional teaching comes with a number of disadvantages linked to little interaction among students their passive involvement surface learning and the rapid decrease of students attention schwerdt and wuppermann 2011 more interacting approaches have been increasingly implemented over the last years for incentive active learning and higher order cognitive tasks sivarajah et al 2019 driven by the increasing availability of technological tools and online platforms jupyter notebooks have been increasingly used in education for programming and modelling applications lane et al 2021 jupyter notebooks are programming environments integrating programming scripts explanatory text and computational output in an online document freely available to everyone because of their easy comprehension and vast range of applications jupyter notebooks are often used also for educational purposes peñuela et al 2021 for instance students ecs and non expert users can explore and execute the code visualize the results assess and modify the code in an interactive way perkel 2018 pérez and granger 2007 kluyver et al 2016 b ama taking advantage of the interactive nature of jupyter notebooks provides a workflow example to guide early career scientists through the protocol employment to real world problems in particular we included a rainfall runoff forecasting application which given the flexible and modular structure of b ama could also serve as a starting point for the user s own application workflows peñuela et al 2021 the aim of using b ama in education is to promote a critical understating of ml application to water resources management realizing both their opportunities and limitations b ama jupyter notebook is suitable for students allow them to learn by doing and eventually run different ml models on their own after furthermore this could allow teachers to give an introduction and application of ml techniques in just a few teaching hours 3 test datasets to assess the ability of b ama to generate accurate forecasts of hydrological time series the protocol is tested with four datasets differing among each other for time step data series length and hydrological applications 3 1 rainfall runoff hydro meteorological data are used in this study to perform daily runoff forecasts at the outlet of the rhine river basin table 3 at lobith in the netherlands in particular discharge is modelled here as a function of temperature precipitation evapotranspiration and measured runoff in two upstream gauges across the river autoregressive terms previous days discharge are included as well considering a maximum lag k of three days per each variable n the total number of candidate input subsets for the rainfall runoff application would be 2 n k 1 2 18 1 daily discharge data were obtained by the global runoff data centre grdc https www bafg de grdc en home homepage node html for the time period 2008 2014 the gauge corrected dataset chirps v2 0 funk et al 2015 provided the precipitation while the gldas reanalysis model rodell et al 2004 is used for the temperature and evapotranspiration inputs 3 2 water table forecasts monthly water table forecasts are performed across the high plains aquifer usa considering both a confined table 4 and unconfined table 5 aquifer conditions the high plains aquifer extends for about 450 000 km2 and 343 encompass over eight states south dakota nebraska colorado kansas oklahoma wyoming new mexico and texas groundwater level is predicted here as a response to t snowmelt snm et p irrigation demand irr and an autoregressive term water level in the previous steps considering the relatively slow dynamics which characterize groundwater systems each variable is considered up to a maximum lag of six months leading to a pool of 38 potential input variables and 2 36 1 potential input combinations 3 3 hydropower production total monthly hydropower production hp table 6 forecasts in the northern italy market region an area encompassing eight regions of the country and extending for about 120 000 km2 for the 2015 2019 period are performed here as a response to discharge q and time described as a combination of sine and cosine function sin 2 π t 12 and cos 2 π t 12 as in amaranto et al 2022 runoff data were obtained by the istituto superiore per la protezione e ricerca ambientale ispra https www isprambiente gov it which provided monthly data for the seven basins characterized by the highest hydropower installed capacity considering the strong dependence between runoff and production only the last two lags are considered as potential candidate input variables leading to 2 16 1 potential combinations it is worth mentioning that to test the ability of the model to forecasts without the autoregressive component previous month production is not included in the input set for this particular application terna https www terna it en provided hydropower production trajectories aggregated across the while northern italy market region 4 protocol tests and evaluation fig 7 shows ivs output produced by b ama when run in default configuration for the rhine case study in particular the plot represents stepwise results of the forward selection procedure used for the investigation of the input choice on predicting performance the input index is on the x axis the iteration step in the y axis and the cross validation nash sutcliffe efficiency index nse is identified by the different colors it is clear how the most accurate results are ensured by selecting input eleven discharge in the upstream station at time t 1 first and input six evapotranspiration then the bottom panel also highlights the increase in accuracy when the i t h input is introduced in the optimal input subset fig 8 highlights the interdependence between hyper parameters model architecture and modeling accuracy measured in terms of r m s e p parameters are presented in the same order as in table 1 for this application a rbf kernel with ε 0 05 and c 1 appears the best choices with the sigmoid kernel low ε and high c consistently producing substantially higher r m s e p values modeling performances appeared to be not sensitive to stopping criteria variation a comparison between training and test performance is shown in fig 9 a allowing a potential b ama user to assess for model overfitting in the rhine application the protocol shows consistency in the accuracy calculated among the two sets with the high nse values higher than 0 9 highlighting high forecasting ability for an assessment of the variation through time specific events and division sets of the modeling accuracy fig 9b provides a time series plot of observed vs predicted values once again it emerges the consistency in the model s accuracy across training and test sets together with a good ability to forecast both baseflow and peak conditions all the results shown above are tailored upon the rhine case study and representative only of b ama run in the default configurations svm model optimal split standard normalization forward selection ivs to allow a more general view of the protocol performance table 7 shows models accuracies and computational times ct obtained across case studies and advanced configurations all experiments are carried in the anaconda environment running on a four cores 2 6 ghz cpus with 8 gb ram per core considering the negligible impact of the normalization choice on both the performance indicators the table shows only results obtained in the default data transformation configuration seasonal normalization results are to be however considered very similar to those shown below the most impacting factor on the computational time is undoubtfully the ivs technique with the exhaustive search as expected consistently outperformed by forward selection and correlation which in turn ensures very similar results considering the high dimensional nature of the water table ivs problems 236 an exhaustive search has not been tested for these applications modeling choice also affects ct with svm apparently being faster than ann however this might also be due to the dimensionality of the training problem with the order of exponential combination of the svm hyperparameters being four hundred while the one of the ann is about four times higher 1600 therefore more ann models are tested to optimize the architecture leading to an increase in computational time which is consistent in the rhine case study about four times however when the number of observations decreases svm ct drops while remaining high for ann as far as accuracy is concerned the modeling performances reported in table 7 are in line with those obtained by le et al 2019 hasan and basak 2020 and khozani et al 2022 which employed similar techniques for similar hydrological applications overall forward selection seems to ensure stable results across case studies being the dominant technique in three out of four with nse values consistently higher than 0 9 also exhaustive search and correlation provided accurate results with the performance of the latter dropping only in the hydropower case study when the algorithm leaves out the highly informative poorly correlated time components marginally dominated exhaustive search performance might be surprising since this technique iterates across all possible feature combination however the intrinsic inter annual variability of the examined hydrological process which cannot be fully accounted for in the data split process might favor less intensive but more generalization prone techniques ann and svm instead lead to similar results in terms of forecasting ability with svm slightly dominating ann performances for some case study ivs combinations 5 conclusions b ama is a fully python coded protocol developed to facilitate the application of data driven models in the hydrological field for non expert users early career scientists and students the b ama protocol is designed to be easy to implement flexible and open source b ama includes the data division data transformation input variable selection model training cross validation and model testing steps to be performed in hydrological prediction applications this protocol allows users to easily build ddm ensuring that no fundamental step is left out in the process and provides several visualization tools for both the investigation of modeling performance and their communication to a broader the community moreover we developed a jupyter notebook based on the b ama framework for educational and dissemination purposes to empower any end users regardless their expertise in ml to perform reliable hydrological predictions for better decision making purposes more experienced modelers can still take advantage of b ama for its easy to use structure and can optimally tailor the protocol settings to their best convenience by tuning the split technique the ivs algorithm the model s choice and the modeling architecture when tested in four different hydrological applications b ama consistently showed both great accuracy nse always higher than 0 9 when forward selection is the ivs technique and depending on the choice of the input variable selection method reasonable computational time between few seconds to less than 10 min if no exhaustive search is implemented even for high dimensional longer datasets our framework is inevitably subject to a number of caveats b ama do not consider for distributed input as it is designed for introduce non expert to ml in case of lumped systems for spatially distributed application we recommend more advanced frameworks as those presented by abbas et al 2022 moreover to facilitate early career scientist to approach the field of ddms the modelling techniques included within the protocol are selected to be among the simplest between those available in the literature however more expert users might wish to perform analysis on complex nonlinear systems employing advanced machine learning techniques a possible future research direction facilitated by the modular structure of the protocol is to include additional methods within b ama a first step in this regard is represented by the embedding of the lstm module already developed and available to the users for illustrative purposes in addition to reduce computational complexity the model based feature selection techniques are constrained by the fixed structure of the model employed which is then generalized in the training phase future analysis can be directed towards finding a tradeoff between computational time and joint optimization of features and structure the python implementation of b ama is freely available for non commercial research and educational purposes at https github com alessandroamaranto b ama we encourage users to provide their impression and suggestion with the aim of creating a strong feedback loop between b ama and the users and to include when possible their needs in new releases early career scientists can take advantage of both the protocol and the manuscript and by properly understanding both theory and practice can help the diffusion of quality ddm research in the hydrological modeling community software availability software name b ama development team alessandro amaranto programming language python 3 9 systems windows macos year first available 2022 software required anaconda recommended availability https github com alessandroamaranto b ama cost free declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work has been financed by the research fund for the italian electrical system under the contract agreement between rse s p a and the ministry of economic development general directorate for the electricity market renewable energy and energy efficiency nuclear energy in compliance with the decree of april 16th 2018 all codes and data used to run the experiments are stored at https doi org 10 5281 zenodo 7304566 amaranto et al 2022b appendix i list of pre requisites and third party libraries b ama has been tested on windows and macos the easiest way to run b ama is to use the free anaconda package manager making sure to download the version for python 3 once anaconda is up and running the user is required to install the third party libraries as shown in table 8 table 8 list of third party libraries necessary to run the b ama protocol table 8 name version description dill 0 3 4 save results hydroeval1 0 1 0 modeling performance matplotlib 3 5 1 visualization numpy 1 20 3 array processing pandas 1 4 2 array processing scipy2 1 8 0 residual statistics seaborn 0 11 2 advanced plots sklearn 1 0 2 building machine learning methods 1 to install the hydroeval library type python m pip install hydroeval in the prompt 2 for the installation of scipy it is recommended to use the command pip install scipy rather than conda install scipy in the prompt 
25470,in this manuscript we present b ama basic data driven models for all an easy flexible fully coded python written protocol for the application of data driven models ddm in hydrology the protocol which is open source and freely available for academic and non commercial purposes has been realized to allow early career scientists with a basic background in programming to develop ddm ensuring that no stones are left unturned through their implementation b ama embeds data splitting feature selection hyperparameter optimization and performance metrics a jupyter notebook with a practical workflow is available to guide the users through the protocol employment while visualization tools allow efficient investigation and communication of results we tested b ama across four hydrological applications to explore ddm applicability across temporal resolutions time series lengths and autocorrelations b ama showed great accuracy and reasonable computational time making the protocol ideal for educational purposes and for the development of ddm based forecasts of hydrological time series keywords modeling protocol data driven models hydrological predictions data availability data and codes used to develop this research are available for non commercial research and educational purposes at https github com alessandroamaranto b ama 1 introduction hydrological extremes have significantly affected society and environment over the last decades ficklin et al 2022 satoh et al 2022 during the european 2003 drought more than 70 000 fatalities and 8 7 billion eur damage were recorded due to the extreme heat conditions sutanto et al 2020 recently the 2021 flood event occurred in germany caused at least 220 casualties and about 17 billion eur of economic damage koks et al 2021 unfortunately the impacts of these hydrological extremes are expected to further increase in the near future also due to anthropogenic climate change hirabayashi et al 2013 and increasing human exposure jongman et al 2012 mazzoleni et al 2021 thus it is of pivotal importance to design sustainable adaptation actions to minimize the future occurrence and impacts of hydrological extremes however water management strategies are highly dependent on the correct prediction and forecasting of floods and droughts brunner et al 2021 for example the hydraulic design of levee systems used for coping with floods requires the accurate estimate of the river water level corresponding to the probability of occurrence of a certain return period for this reason over the last decades a number of tools have been developed for assessing crucial hydrological variables such as soil moisture water level and river discharge and the consequent drought and flood risk candido et al 2022 in particular hydrological and land surfaces models have been widely implemented to assess river streamflow based on current and future climate condition brunner et al 2021 in their recent review horton et al 2022 analyzed the implementation of a number of hydrological models for characterization and quantification of floods and droughts climate change and land use change impact analysis uncertainty analysis operational forecasting and large scale modelling despite this broad range of applications hydrological models are still affected by certain sources of uncertainty linked to model calibration data quality and model structure the increasing availability of remote sensing observations global reanalysis data internet of things and crowdsourced observations together with improvement in computational resources graphic processor units and artificial intelligent have revolutionized water science and opened the way for pioneering the spreading of data driven models lecun et al 2015 reichstein et al 2019 sit et al 2020 among these models we can find multi linear regression lasso regression random forests decision trees support vector machine artificial neural networks and recurrent neural networks algorithm such long short term memory hauswirth et al 2021 data driven models have been extensively developed for a variety of environmental and hydrological applications solomatine and ostfeld 2008 in an early application of data driven model campolo et al 2003 used an artificial neural network to predict water level variation up to 6 h in advance in an unregulated river basin wu and chau 2010 compared four data driven models i e auto regressive moving average k nearest neighbors and artificial neural networks for monthly streamflow prediction in deo et al 2017 multivariate adaptive regression splines least square support vector machine and m5tree models were used for drought forecasting purposes in eastern australia the authors highlighted the importance of periodicity as a predictor variable for drought modelling support vector machine was used by roodposhti et al 2017 to extract reliable spatio temporal patterns of drought sensitivity in response to vegetation changes amaranto et al 2018 used multiple data driven models e g random forests support vector machines artificial neural networks extreme learning machines and genetic programming to improve the forecast of groundwater tables up to five months a nonlinear auto regressive with exogenous inputs and self organizing map approach was used by kim and han 2020 for real time urban flood maps and improve flood response capabilities similarly guo et al 2022 explored the applicability of convolutional neural networks for fast flood prediction and flood extent mapping the authors found that this approach can generalize the information from the training data to other unseen terrains suggesting its potential use as rapid surrogate model for flood predictions recent studies proposed new hybrid approaches for integrating hydrological model with data driven methods for example yang et al 2019 coupled a machine learning model i e long short term memory units with a global hydrological model i e ghms cama flood model to improve the flood simulations for a more robust and confident flood risk assessment the results of this study show that while the global hydrological model can predict peak discharge the proposed hybrid approach outperforms the original ones when prediction the timing of the peak and drastically improve the performance of global flood simulations in their study ghaith et al 2020 achieved better representation of daily streamflow prediction by integrating physically based and data driven approaches sezen and partal 2022 proved that hybrid models that combines advantages of conceptual and data driven models can provide more accurate forecasting results than hydrological and data driven models similar improvements were also found by althoff et al 2021 when coupling machine learning model with the gr4j hydrological model advances in data science has led to the growth of programming libraries frameworks and toolboxes for the implementation of data driven machine learning and deep learning algorithms python and r packages like svars lange et al 2021 tensorflow pang et al 2020 keras géron 2019 mxnet chen et al 2015 superml and theano brownlee 2017 have been widely implemented similarly modelling frameworks such as ludwig molino et al 2019 and mlflow zaharia et al 2018 have been developed to provide a general modularized machine and deep learning architecture to a broad audience of engineers and scientists for performing a vast amount of tasks moreover libraries like tslearn tavenard et al 2020 and pyts faouzi and janati 2020 have used for processing building and training machine learning models of time series data a few toolboxes have been developed specifically for hydrological applications among them the neuralhydrology library kratzert et al 2019 aims at proving deep learning models for rainfall runoff modeling furthermore abbas et al 2022 developed a python based framework for pre processing hydrological data building and training machine learning hydrological model and post processing the model results while these new frameworks help in applying machine learning based models in hydrological sciences they are also mostly focused on building the optimal model architectures while more research should be devoted in integrating an automated input variable selection algorithm in a ddm framework wu et al 2014 abbas et al 2022 considering the high impact that proper water resources management can have on society it is crucial to empower non expert users early career scientists and students with accessible tools to explore hydrological predictions however none of existing ddm toolboxes are designed for educational and dissemination purposes for these reasons we developed b ama an easy flexible fully python coded and open access protocol that will guide users through ddms implementation ensuring that none of the fundamental steps optimal data split input variable selection cross validation is overlooked in the process b ama also embeds a jupyter notebook with a practical workflow to enhance the learning by doing and guide the user through the protocol employment to real world problems the visualization aid implemented in b ama will empower the users with an effective dissemination of the results to a broad audience b ama complements existing modeling framework thanks to 1 its simplicity by allowing scientists and students with very little coding background to build ddm for time series forecasting using a single line of code 2 its adaptability to educational purpose thanks to a jupyter notebook it can guide non expert users through all the required steps to build ddms 3 its modular structure by allowing expert users to include in the protocol any data driven modeling technique by developing a simple function file and 4 its vertical structure b ama is not solely focused on optimal training and architecture definition but embeds all the steps required to build ddm it especially includes three automated input variable selection algorithms for multi dimensionality control and optimal feature extraction 2 protocol description 2 1 general framework the general framework of the python based b ama protocol is illustrated in fig 1 and it is designed to be easy to implement flexible and open source easy the protocol is specifically designed for users that have little experience with ddms and limited programming skills with only a single line of code anyone will be able to predict the desired hydrological variable with a given lead time value flexible b ama allows expert users to define more advanced configuration settings based on their preferences its modular structure allows the easy definition and integration of new modeling techniques into the protocol open source b ama is fully coded in python commented to facilitate the code understanding for non expert users and the source code is open to everyone so that expert users could also add different modelling techniques not included in b ama yet to operate the protocol the user should provide the input data and a configuration settings file specifying the application name i e the case study name the type of ddm to be employed the time series length and its periodicity however expert users could provide an advanced configuration settings selecting alternative options regarding each step of the b ama protocol described in the following sections and represented in fig 1 once input data and configuration settings are provided b ama imports the data and optimizes the split between training and test set data division step in fig 1 training and test set are then normalized using the training set minimum and maximum as normalization parameters on both sets data transformation step in fig 1 a model based forward input variable selection procedure is developed to select the most relevant input variable and lags such feature also returns visual analytics highlighting how modeling performances on the cross validation set are affected by the input combination the selected training input subset is then used to optimize via k fold cross validation the ddm architecture and the model s hyperparameters model training step in fig 1 cross validation performances are stored and visualized to provide insights upon the interdependence between tuning parameters model structure and accuracy the optimally trained models are evaluated and tested using the nash sutcliffe efficiency index testing step in fig 1 finally the user can assess predictions accuracy and gain qualitative insights about extrapolation ability through training vs test scatterplots time series analysis and other types of visual aid to compare model to observed vs predicted variables 2 2 b ama protocol structure and methods employed 2 2 1 data division the correct implementation of ddm requires the statistical distributions of the training and the test sets to be approximately the same bhattacharya et al 2007 this could be done by performing an exhaustive iterative random selection of subsets until statistical homogeneity is achieved however the randomity of the experiment prevents reproducibility of the results therefore to ensure reproducibility of modeling results two different data division techniques are employed here optimal division default option b ama constrains the randomization of the splits by limiting the iterative search of the test set only to consecutive years corresponding to 30 of the total number of time steps see amaranto et al 2019 2020 for more information among all the possible splits the protocol selects the optimal one s by satisfying the following rule 1 s arg min s μ r s 1 2 σ r s 1 2 where μ and σ are the ratios between means and standard deviations of the training and the testing set outputs after normalization respectively and the optimal split s is selected by solving equation 1 through an exhaustive search procedure this procedure ensures statistical homogeneity through the training and the test set and it is therefore selected as default while running the protocol however a user could be interested in assessing modelling performance on most recent data or on a particular event consequently b ama allows for a second split technique custom division b ama allows the user to define the initial year of the validation period once the selection is performed the protocol automatically assigns the n where n 30 of the total time steps following time steps to the test set 2 2 2 data transformation in this step b ama transforms each input and the output variable to ensure extrapolation ability of ddm especially those employing logistic or sigmoid transfer function again two methods are employed normalization default option data are constrained to the range 0 1 according to the equation 2 z x min x max x min x where x min x a n d max x are a generic input its minimum and maximum value in the training set respectively seasonal adjustment considering how hydrological time series are often characterized by strong seasonality b ama allows also to remove the cyclostationary deterministic component soncini sessa et al 2007 from each variable as 3 z x μ t mod t σ t mod t where μ t mod t and σ t mod t are the cyclostationary mean and standard deviation of the variable x t is the time index and t is the time series periodicity 2 2 3 input variable selection the selection of the best input subset represents a pillar component in ddms implementation but often overlooked wu et al 2014 the proper selection of the ddm input allows for maximizing input output interdependence leading to an increase in modeling accuracy may et al 2011 expert users can already identify the ideal input vector space if the dynamics of the system are clearly understood however when this is not the case input variable selection ivs needs to be implemented leading to a computationally expensive task for example considering a n dimensional input space there are 2n 1 possible combinations the complexity of the problem grows further to 2nk 1 when considering also the appropriate lags to be chosen among k candidates it is therefore pillar to find an algorithm balancing the tradeoff between computational effectiveness and efficient exploration of the input space to minimize such tradeoffs b ama employs three different ivs approaches forward selection default option the forward selection algorithm is a linear iterative search strategy selecting an individual candidate variable at each iteration may et al 2011 it starts by training n where n is the number of inputs single input single output models the optimal input variable is selected by maximizing the model performances based on some optimality criteria e g the nash sutcliffe efficiency index on the cross validation set the procedure then continues by training iteratively n 1 multiple input single output models adding each time one of the remaining candidates to the previously selected variable convergence is reached when no improvement in modeling accuracy is achieved with the addition of other input variables exhaustive search the forward selection approach is considered overall computationally efficient often resulting in relatively small input variable sets it is therefore selected as default configuration however in case of low input number a user could be interested in exploring all possible input combinations to select the most informative for this reason b ama also employs exhaustive search as ivs method even though exhaustive search basically prevents the algorithm to find a local optimum it is computationally extremely expensive and its application is recommended only when the data dimensionality is relatively small amaranto et al 2019 correlation based selection as a third option the user could select a correlation based ivs this method starts by ranking input candidates upon their correlation with the dependent variable and training a single input single output model using the input at the top of the rank the procedure then continues by iteratively training multiple input single output models adding at each iteration the highest correlated variable among those remaining the method stops when the addition of variables fails to ensure improvements in modelling accuracy since it only trains one model at each step the correlation model based ivs is faster than forward selection which being sv the number of selected variables in a certain step trains n sv models on the other hand by only searching through a very small subset the chances of encountering a local optimum are much higher 2 2 4 model training the protocol employs artificial neural networks ann and support vectors machines svm two among the most widespread and easy to use ddm in hydrology with applications among others on groundwater forecasts taormina et al 2012 sun et al 2016 sahoo et al 2017 amaranto et al 2018 2019 2020 lee et al 2019 pham et al 2022 flood prediction elshorbagy et al 2010b elsafi 2014 aziz et al 2014 falah et al 2019 dhunny et al 2020 precipitation forecasts di nunno et al 2022 and water quality modeling wu et al 2014 rehana 2019 bisht et al 2019 support vector machines default option svms are a supervised modeling technique developed by vapnik 1999 2013 for classification and regression purposes raghavendra and deka 2014 their goal is to find a function f x that in addition to being as flat as possible to ensure generalization ability has at most ε deviation from the target variable for all the training data this practically means that all the errors within the ε tube in fig 2 a are ignored providing a better generalization ability of the model muller et al 1997 instances located at the edges of the ε tube are called support vectors of course it is not always possible to approximate a hydrological process with ε precision in order to allow for larger errors while still ensuring the flatness of f x an additional parameter is introduced in the svm training process it is a constant term c representing the tradeoff between the flatness of the function and the extent up to which errors larger than ε are tolerated one of the main advantages of svm regression is their ability of mapping the input vector in a higher dimensional space hyperplane and keeping at the same time regression in the linear form such operation is performed through the kernel function which is often linear sigmoidal or a radial basis function artificial neural networks multi layer perceptron mlp are one of the most common among the machine learning techniques for hydrological applications oyebode and stretch 2019 they consist of an input layer a hidden layer and an output layer fig 2b the first has the sole purpose of distributing the input further and is constituted by as many nodes as the number of inputs the connection between the input and the hidden layer is ensured by weights a see fig 2b which represent the strength of the interdependence between each feature and the output the hidden layer is constituted by nodes whose number often depends on the complexity of the system analyzed their determination often requires optimization each node in the hidden layer comes with a transfer function f zoomed rectangle in fig 2b which introduces nonlinearity by means of a transfer function often represented by a sigmoid or a hyperbolic tangent the process is repeated in a very similar fashion through the connection between the hidden layer and the output layer which is however constituted by a single node or as many nodes as the number of outputs and often embeds a linear transfer function 2 2 4 1 architecture and hyperparameter optimization from a conceptual perspective training ddm follows the same sequence of steps no matter the choice of the modelling technique the training set is split in proper training and cross validation set at each split every candidate architecture is tested and the one minimizing the error statistics in the test set stored the procedure therefore generates an ensemble of models one for each fold in the cross validation set constituted by the most performing architecture and hyper parameter values each of the ensemble member subsequently generates a forecast in the test set the ensemble average is then used for evaluation purposes as far as the svm are concerned it is evident from the above discussion that optimal tuning requires determining the kernel type in addition to the ε and c parameters their value together with the maximum tolerance as a stopping criterion is determined via exhaustive search of their exponential combination with the b ama default search space for individual hyperparameter summarized in table 1 the transfer function and the number of nodes n in the mlp hidden layer are determined in a similar fashion the weights instead are optimized using the levenberg marquardt back propagation algorithm an iterative procedure requiring the definition of a regularization parameter α to avoid overfitting a learning rate and a maximum number of iterations the b ama default search space of each ann parameter is summarized in table 2 for each modeling technique b ama searches through every possible combination and selects those minimizing the error in the cross validation set experience users could modify the feasibility space of ddm hyperparameters via the advanced configuration settings file 2 2 5 testing modelling performances are tested in the test set by means of the nash sutcliffe efficiency index nse and the normalized root mean squared error rmsep these comparative estimators provide an assessment of the modelling accuracy with respect to the mean rmsep and to the standard deviation nse of the process their mathematical formulation can be expressed as 3a r m s e p i 1 n o i p i 2 n o 3b n s e 1 i 1 n o i p i 2 i 1 n o i o 2 where n is the number of observations and o i p i and o are the observed values the predicted variables and the observations mean respectively 2 2 6 visualization and post processing features to allow for a better understanding and communication of the modeling results b ama embeds a number of visual aids aimed at representing the variability in accuracy through each step in the modeling process thanks to such visualization features we aim to expand the classic representation of the results which is often limited to the analysis of the error in the test set by providing insights on the effect of variable selection and model architecture on predictive performances furthermore more experienced users can access the csv output files containing the observed and predicted time series to develop their own visual analytics and communicate results to the community 2 3 implementation of the b ama protocol fig 3 shows how the algorithmic implementation of the b ama protocol is organized into folders regardless of the type of input data and the case study under investigation b ama consists of the following straightforward five steps installation install all the required pre requisites libraries see appendix i for a complete list input loading load the input data in the input case study folder make sure that the dependent variable is in the last column of your csv file in particular the protocol communicates with the input data once they have been loaded into csv format in the input folder through the default configuration settings file left panel in fig 4 define settings in the configuration file the information to be specified in the default configuration file left panel in fig 4 are the case study name the periodicity of the variable to be forecasted expressed in number of time steps the initial year and the final year with observations and the modelling techniques to be used case study name is used by b ama to link the protocol to the input file and the periodicity is used to perform a sequence of internal operations such as optimal split and k fold cross validation it is worth mentioning that considering a 70 30 ration between training and test set and to ensure that all the dynamics under investigation are captured in the modelling process the time series length should be at least four time longer than the periodicity initial and final year are optional parameters used by the protocol to develop time series plot of observed vs predicted output while the modelling techniques provides information on the hyperparameters and modeling architecture to be optimized more experienced users could modify the default configuration through the advanced configuration settings file right panel in fig 4 and select among different data split data transformation and input variable selection options as well as for enlarging the candidate s feasibility set and modeling architectures to be explored during training see section 2 1 run the protocol open the anaconda prompt terminal and navigate to the b ama folder type python ddm run py in the terminal window b ama will then run all the protocol steps as described in section 2 2 alternatively one could also open the ddm run py script and after navigating to the b ama folder run it through spyder from a practical perspective b ama reads the five aforementioned values implements all the steps described in section 2 1 in the default configuration and returns to the user a combination of output text files for further users investigation and of visual aid to be inserted in a presentation or a scientific paper generate model output text files and visual aids are generated to better describe and understand modelling results after performing steps one to four depending on the computational time the message process completed will appear in the terminal window the outputs are then saved in the output case study where case study corresponds to the one specified in the configuration settings file folder to be created by the user the user can navigate to that folder to find output text files and a sequence of graphics describing modeling performances during input variable selection training and model testing more details and illustrative examples of such visual aids are provided in section 4 4 2 3 1 modular structure of the b ama protocol the modular structure of b ama allows the integration of new modeling techniques in the protocol with just three steps 1 definition of a model specific configuration settings file 2 definition of a python script including the functions for training and testing the model 3 insertion of the new model s name in the original configuration settings file in particular the model specific configuration settings file must be named as newmodel module config txt where newmodel corresponds to the model s name assigned by the user and placed in the protocol folder the user should include the dimensionality of the hyper parameter optimization problem and the candidate hyper parameters value fig 5 shows an example of a model specific configuration settings definition for a hypothetical model called am from the names of the authors that requires the optimization of two parameters dimensionality 2 p0 to be selected between 2 6 8 and 10 and p1 to be selected between 50 100 and 200 the python script instead must be named newmodel module py where newmodel corresponds to the model s name placed in the main folder and only includes two functions one for training and one for testing fig 6 shows an example on how such functions must be defined including the input data dimensionality they should be adapted to it is worth noticing that to add a new module to the protocol no new data or no modification to the main code must be performed and the only necessary tools are the model specific configuration settings file and the function script the new model s name must also be specified in the main configuration settings txt file i e to add the hypothetical am module the instance model in the left panel of fig 4 should be equal to am to guide the user towards the inclusion of a new module the online version of the protocol also includes an illustrative workflow for the addition of a lstm neural network module 2 4 the jupyter notebook learning water resources managemental and risk assessment can be challenging for someone with limited background in modelling and data analysis for this reason it is extremely important teach complex programming and modelling tools in a comprehensive and accessible way however traditional teaching comes with a number of disadvantages linked to little interaction among students their passive involvement surface learning and the rapid decrease of students attention schwerdt and wuppermann 2011 more interacting approaches have been increasingly implemented over the last years for incentive active learning and higher order cognitive tasks sivarajah et al 2019 driven by the increasing availability of technological tools and online platforms jupyter notebooks have been increasingly used in education for programming and modelling applications lane et al 2021 jupyter notebooks are programming environments integrating programming scripts explanatory text and computational output in an online document freely available to everyone because of their easy comprehension and vast range of applications jupyter notebooks are often used also for educational purposes peñuela et al 2021 for instance students ecs and non expert users can explore and execute the code visualize the results assess and modify the code in an interactive way perkel 2018 pérez and granger 2007 kluyver et al 2016 b ama taking advantage of the interactive nature of jupyter notebooks provides a workflow example to guide early career scientists through the protocol employment to real world problems in particular we included a rainfall runoff forecasting application which given the flexible and modular structure of b ama could also serve as a starting point for the user s own application workflows peñuela et al 2021 the aim of using b ama in education is to promote a critical understating of ml application to water resources management realizing both their opportunities and limitations b ama jupyter notebook is suitable for students allow them to learn by doing and eventually run different ml models on their own after furthermore this could allow teachers to give an introduction and application of ml techniques in just a few teaching hours 3 test datasets to assess the ability of b ama to generate accurate forecasts of hydrological time series the protocol is tested with four datasets differing among each other for time step data series length and hydrological applications 3 1 rainfall runoff hydro meteorological data are used in this study to perform daily runoff forecasts at the outlet of the rhine river basin table 3 at lobith in the netherlands in particular discharge is modelled here as a function of temperature precipitation evapotranspiration and measured runoff in two upstream gauges across the river autoregressive terms previous days discharge are included as well considering a maximum lag k of three days per each variable n the total number of candidate input subsets for the rainfall runoff application would be 2 n k 1 2 18 1 daily discharge data were obtained by the global runoff data centre grdc https www bafg de grdc en home homepage node html for the time period 2008 2014 the gauge corrected dataset chirps v2 0 funk et al 2015 provided the precipitation while the gldas reanalysis model rodell et al 2004 is used for the temperature and evapotranspiration inputs 3 2 water table forecasts monthly water table forecasts are performed across the high plains aquifer usa considering both a confined table 4 and unconfined table 5 aquifer conditions the high plains aquifer extends for about 450 000 km2 and 343 encompass over eight states south dakota nebraska colorado kansas oklahoma wyoming new mexico and texas groundwater level is predicted here as a response to t snowmelt snm et p irrigation demand irr and an autoregressive term water level in the previous steps considering the relatively slow dynamics which characterize groundwater systems each variable is considered up to a maximum lag of six months leading to a pool of 38 potential input variables and 2 36 1 potential input combinations 3 3 hydropower production total monthly hydropower production hp table 6 forecasts in the northern italy market region an area encompassing eight regions of the country and extending for about 120 000 km2 for the 2015 2019 period are performed here as a response to discharge q and time described as a combination of sine and cosine function sin 2 π t 12 and cos 2 π t 12 as in amaranto et al 2022 runoff data were obtained by the istituto superiore per la protezione e ricerca ambientale ispra https www isprambiente gov it which provided monthly data for the seven basins characterized by the highest hydropower installed capacity considering the strong dependence between runoff and production only the last two lags are considered as potential candidate input variables leading to 2 16 1 potential combinations it is worth mentioning that to test the ability of the model to forecasts without the autoregressive component previous month production is not included in the input set for this particular application terna https www terna it en provided hydropower production trajectories aggregated across the while northern italy market region 4 protocol tests and evaluation fig 7 shows ivs output produced by b ama when run in default configuration for the rhine case study in particular the plot represents stepwise results of the forward selection procedure used for the investigation of the input choice on predicting performance the input index is on the x axis the iteration step in the y axis and the cross validation nash sutcliffe efficiency index nse is identified by the different colors it is clear how the most accurate results are ensured by selecting input eleven discharge in the upstream station at time t 1 first and input six evapotranspiration then the bottom panel also highlights the increase in accuracy when the i t h input is introduced in the optimal input subset fig 8 highlights the interdependence between hyper parameters model architecture and modeling accuracy measured in terms of r m s e p parameters are presented in the same order as in table 1 for this application a rbf kernel with ε 0 05 and c 1 appears the best choices with the sigmoid kernel low ε and high c consistently producing substantially higher r m s e p values modeling performances appeared to be not sensitive to stopping criteria variation a comparison between training and test performance is shown in fig 9 a allowing a potential b ama user to assess for model overfitting in the rhine application the protocol shows consistency in the accuracy calculated among the two sets with the high nse values higher than 0 9 highlighting high forecasting ability for an assessment of the variation through time specific events and division sets of the modeling accuracy fig 9b provides a time series plot of observed vs predicted values once again it emerges the consistency in the model s accuracy across training and test sets together with a good ability to forecast both baseflow and peak conditions all the results shown above are tailored upon the rhine case study and representative only of b ama run in the default configurations svm model optimal split standard normalization forward selection ivs to allow a more general view of the protocol performance table 7 shows models accuracies and computational times ct obtained across case studies and advanced configurations all experiments are carried in the anaconda environment running on a four cores 2 6 ghz cpus with 8 gb ram per core considering the negligible impact of the normalization choice on both the performance indicators the table shows only results obtained in the default data transformation configuration seasonal normalization results are to be however considered very similar to those shown below the most impacting factor on the computational time is undoubtfully the ivs technique with the exhaustive search as expected consistently outperformed by forward selection and correlation which in turn ensures very similar results considering the high dimensional nature of the water table ivs problems 236 an exhaustive search has not been tested for these applications modeling choice also affects ct with svm apparently being faster than ann however this might also be due to the dimensionality of the training problem with the order of exponential combination of the svm hyperparameters being four hundred while the one of the ann is about four times higher 1600 therefore more ann models are tested to optimize the architecture leading to an increase in computational time which is consistent in the rhine case study about four times however when the number of observations decreases svm ct drops while remaining high for ann as far as accuracy is concerned the modeling performances reported in table 7 are in line with those obtained by le et al 2019 hasan and basak 2020 and khozani et al 2022 which employed similar techniques for similar hydrological applications overall forward selection seems to ensure stable results across case studies being the dominant technique in three out of four with nse values consistently higher than 0 9 also exhaustive search and correlation provided accurate results with the performance of the latter dropping only in the hydropower case study when the algorithm leaves out the highly informative poorly correlated time components marginally dominated exhaustive search performance might be surprising since this technique iterates across all possible feature combination however the intrinsic inter annual variability of the examined hydrological process which cannot be fully accounted for in the data split process might favor less intensive but more generalization prone techniques ann and svm instead lead to similar results in terms of forecasting ability with svm slightly dominating ann performances for some case study ivs combinations 5 conclusions b ama is a fully python coded protocol developed to facilitate the application of data driven models in the hydrological field for non expert users early career scientists and students the b ama protocol is designed to be easy to implement flexible and open source b ama includes the data division data transformation input variable selection model training cross validation and model testing steps to be performed in hydrological prediction applications this protocol allows users to easily build ddm ensuring that no fundamental step is left out in the process and provides several visualization tools for both the investigation of modeling performance and their communication to a broader the community moreover we developed a jupyter notebook based on the b ama framework for educational and dissemination purposes to empower any end users regardless their expertise in ml to perform reliable hydrological predictions for better decision making purposes more experienced modelers can still take advantage of b ama for its easy to use structure and can optimally tailor the protocol settings to their best convenience by tuning the split technique the ivs algorithm the model s choice and the modeling architecture when tested in four different hydrological applications b ama consistently showed both great accuracy nse always higher than 0 9 when forward selection is the ivs technique and depending on the choice of the input variable selection method reasonable computational time between few seconds to less than 10 min if no exhaustive search is implemented even for high dimensional longer datasets our framework is inevitably subject to a number of caveats b ama do not consider for distributed input as it is designed for introduce non expert to ml in case of lumped systems for spatially distributed application we recommend more advanced frameworks as those presented by abbas et al 2022 moreover to facilitate early career scientist to approach the field of ddms the modelling techniques included within the protocol are selected to be among the simplest between those available in the literature however more expert users might wish to perform analysis on complex nonlinear systems employing advanced machine learning techniques a possible future research direction facilitated by the modular structure of the protocol is to include additional methods within b ama a first step in this regard is represented by the embedding of the lstm module already developed and available to the users for illustrative purposes in addition to reduce computational complexity the model based feature selection techniques are constrained by the fixed structure of the model employed which is then generalized in the training phase future analysis can be directed towards finding a tradeoff between computational time and joint optimization of features and structure the python implementation of b ama is freely available for non commercial research and educational purposes at https github com alessandroamaranto b ama we encourage users to provide their impression and suggestion with the aim of creating a strong feedback loop between b ama and the users and to include when possible their needs in new releases early career scientists can take advantage of both the protocol and the manuscript and by properly understanding both theory and practice can help the diffusion of quality ddm research in the hydrological modeling community software availability software name b ama development team alessandro amaranto programming language python 3 9 systems windows macos year first available 2022 software required anaconda recommended availability https github com alessandroamaranto b ama cost free declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work has been financed by the research fund for the italian electrical system under the contract agreement between rse s p a and the ministry of economic development general directorate for the electricity market renewable energy and energy efficiency nuclear energy in compliance with the decree of april 16th 2018 all codes and data used to run the experiments are stored at https doi org 10 5281 zenodo 7304566 amaranto et al 2022b appendix i list of pre requisites and third party libraries b ama has been tested on windows and macos the easiest way to run b ama is to use the free anaconda package manager making sure to download the version for python 3 once anaconda is up and running the user is required to install the third party libraries as shown in table 8 table 8 list of third party libraries necessary to run the b ama protocol table 8 name version description dill 0 3 4 save results hydroeval1 0 1 0 modeling performance matplotlib 3 5 1 visualization numpy 1 20 3 array processing pandas 1 4 2 array processing scipy2 1 8 0 residual statistics seaborn 0 11 2 advanced plots sklearn 1 0 2 building machine learning methods 1 to install the hydroeval library type python m pip install hydroeval in the prompt 2 for the installation of scipy it is recommended to use the command pip install scipy rather than conda install scipy in the prompt 
25471,watershed delineation is one of the fundamental tasks in hydrological studies tools for extracting watersheds from digital elevation models and flow direction rasters are commonly implemented in gis software packages however the performance of available techniques and algorithms often turns out to be far from sufficient especially when working with large datasets while modern hardware offers high computing performance through massive parallelism there is still a need for algorithms that can effectively use these capabilities this paper proposes an algorithm for rapid watershed delineation directly from flow direction rasters using the possibilities offered by modern gpu devices performance measurements show a significant reduction in execution time compared to other parallel solutions proposed for this task in the literature moreover this implementation makes it possible to delineate multiple watersheds from the same dataset simultaneously each having one or more outlet cells with virtually no additional computational cost keywords watershed delineation gis parallel algorithms gpu cuda openmp data availability the source code is available in a public repository the data used in performance measurement originate from publicly available resources referenced in the manuscript 1 introduction watershed also referred to as drainage area basin or catchment is considered one of the basic concepts in hydrological studies tesfa et al 2011 it is defined as the area of land whose drainage eventually concentrates in a single location called the watershed outlet chow et al 1988 delineating watersheds and their boundaries is one of the fundamental tasks in this field and is used in many different contexts within and beyond the area of hydrology daniel 2011 singh 2018 while watersheds can be delineated manually using topographic data automatic techniques are widely accepted as being much faster and more precise karimipour et al 2013 procedures using digital elevation models dems as basic input data are well known and widely implemented in gis software barták 2009 in recent decades the availability and precision of this type of data has increased significantly which considerably facilitates accurate calculations and simulations but at the same time creates new challenges related to the processing of large datasets tang and wang 2020 currently the market offers hardware architectures that enable achieving previously unavailable high computing performance wide access to multicore and many core processors and graphics processing units gpus creates new possibilities for parallel processing making it possible to solve complex tasks on large datasets in a much shorter time parallel programming standards like openmp and cuda allow access to these hardware capabilities with relatively little effort chapman et al 2007 cheng et al 2014 still the effective use of the possibilities offered by modern devices requires creating suitable parallel algorithms which remains a challenging task there is a significant time gap between the available technological solutions and their practical applications tang and wang 2020 existing software often turns out to be insufficiently scalable or simply unsuitable for working with modern datasets the motivation behind this research was to investigate and address issues related to the performance of existing watershed delineation algorithms concluding from the available literature this problem has been discussed in multiple studies but there is still room and need for significant improvement the goal of this work was to develop and present a new raster based algorithm for delineating watersheds operating on dems and derivative data allowing this task to be performed more efficiently compared to existing alternatives particular attention was paid to the possibilities offered by modern gpu devices along with parallel processing capabilities on the host side 1 1 existing techniques many different methods and approaches for automatic watershed delineation have been described in the literature they differ significantly in terms of technology and architecture applied the type of input and data structures used as well as the computational complexity of implemented algorithms most of the existing research and practical implementations in this area use square grid dems and their derivatives such as flow direction rasters as the basis for further operations the foundations of this approach are well known o callaghan and mark 1984 jenson and domingue 1988 however it is worth noting that this is not the only way to address the task some research works are focused on delineating watershed areas from triangle based terrain models jones et al 1990 nelson et al 1994 de azeredo freitas et al 2016 or hexagonal grids liao et al 2020 there are also alternative methods that may be useful when accurate dems are not available karimipour et al 2013 the most common well established workflow for delineating watersheds from square grid dems consists of several separate processing stages baker et al 2006 eränen et al 2014 before a proper hydrological analysis can be carried out the elevation data must be corrected by removing spurious sinks and local depressions this is due to the fact that many existing models and algorithms require each dem cell to have a downslope path leading to the edge of the raster wang and liu 2006 depression filling is the most common method used to satisfy this condition tarboton et al 2009 however many recent papers suggest using other approaches as they can produce the desired result with much less modification to the original elevation values lindsay 2016 chen et al 2021 once the depressions are removed the hydrologically corrected dem can be used to calculate the flow direction data lindsay et al 2008 at this stage each cell is assigned a value corresponding to the expected direction of its further downstream flow a variety of approaches and specific algorithms for determining flow directions exist in the literature o callaghan and mark 1984 fairfield and leymarie 1991 freeman 1991 tarboton 1997 seibert and mcglynn 2007 in essence the purpose of this step is to determine how the outflow from each cell is distributed to its immediate neighbors wilson et al 2008 the next step is usually to calculate the flow accumulation data where each cell is assigned the total number of cells that eventually flow to it jenson and domingue 1988 martz and garbrecht 1993 these values can be used to delineate the drainage network as well as to precisely locate the watershed outlet cells lindsay et al 2008 once the flow direction is determined and the outlet cells are located it is possible to perform the delineation of selected watersheds in general this stage is usually carried out using algorithms that identify all upslope cells connected to chosen outlet points by overland flow paths lindsay et al 2008 the remaining sections of this paper mainly deal with the last stage of this workflow assuming that flow direction data and outlet cell locations are already available 1 1 1 recursive algorithms among the algorithms that utilize square grid dems one of the first and best known is the recursive approach the earliest presentation of this concept that the author is aware of comes from marks et al 1984 in this approach calculations start from a designated cell of the watershed outlet the algorithm analyzes the immediate neighbors of this location identifying the upstream cells flowing to a given point and classifying them as part of the watershed area the same procedure is then performed recursively for each cell classified this way the algorithm moves in a bottom up manner starting from the outlet point and following the flow paths upwards opposite to the flow direction the main advantages of the recursive approach are its simplicity and relative efficiency only cells belonging to the watershed area and their direct neighborhood are analyzed other cells are not entered through the procedure however the nature of recursive algorithms in general often leads to memory issues especially on larger datasets and proves difficult to effectively parallelize wallis et al 2009 qin and zhan 2012 1 1 2 iterative algorithms there are multiple references to iterative implementations of watershed delineation algorithms in the literature unfortunately not all papers discuss their approach in detail jenson and domingue 1988 described a procedure that uses a flow direction dataset and a starter raster where outlet points are marked with numerical values the algorithm uses flow direction data to iteratively fill all cells with the start values of the outlet to which they flow martz and garbrecht 1993 mentioned a watershed boundary delineation procedure based on flow vectors determined with the use of the d8 method the algorithm identifies all cells that eventually flow into the user specified outlet cell the details of this stage are not extensively discussed but it can be inferred from the context that the procedure is based on following the steepest descent path starting from each cell individually this method was used there for accumulated drainage area calculations a similar concept was used for tin data in nelson et al 1994 the algorithm starts at the centroid of each triangle and follows the flow path until it hits one of the terminus points another approach is described in choi and engel 2003 here an iterative method using flow direction was used which avoids scanning the entire raster the procedure starts with a single outlet cell selected by the user in each iteration the algorithm considers only the closest neighborhood of cells classified as part of the watershed in the previous step using the flow direction data successive upstream cells are identified this procedure is repeated until no more matching cells can be found 1 1 3 sorting and priority queues another type of approach is based on sorting dem cells and visiting them in a specific order arge et al 2003 described an algorithm that processes cells in a bottom up manner reverse topological order gradually propagating watershed labels from lower to higher cells however it should be noted that computing watersheds is understood here as part of a complex flooding procedure rather than the actual delineation of watershed areas unique labels are assigned to each local sink and then propagated to the remaining cells the idea is to identify areas concentrating their drainage in common sinks and to construct a graph of relationships between these areas it is then used to raise the elevations to ensure that for each cell there is a non ascending path to the edge of the terrain barnes et al 2014 integrated and improved on multiple related works by several authors and presented a unified algorithm based on a priority queue primarily intended for filling depressions in dems it was pointed out that this concept can be adapted and used for labeling watersheds as well in this variant of the algorithm unlabeled cells at the edges of the available data are assumed to be watershed outlets and are assigned unique labels these values are then propagated to the remaining cells by flooding the dem inwards it is worth noting that this group of algorithms processes cells in a relatively straightforward manner but this comes at the expense of additional computational time needed to perform the ordering it is also important to note that this type of approach is based on distinct assumptions placing it outside the typical watershed delineation workflow 1 1 4 other data models yet another approach to the problem can be found in haag et al 2018 2020 the algorithms presented here are intended to significantly reduce the computational time by marching around the watershed boundary without entering or leaving its area however it is necessary to note that these techniques require converting the flow direction to other data models specifically designed for this task which entails additional computational and storage costs performing such an operation directly on the flow direction is not possible castronova and goodall 2014 highlighted the issues related to processing large datasets and demonstrated an alternative approach to watershed delineation however this technique relies on the availability of additional datasets without which the dem based approach still appears to be a valid choice 1 1 5 parallel algorithms parallel algorithms are a separate category in general this approach aims to make better use of hardware capabilities and reduce computational time the earliest application of simd single instruction multiple data computers to watershed delineation that the author is aware of can be found in mower 1994 the data parallel approach was used in two different iterative algorithms developed for thinking machines cm 5 both ideas are based on copying watershed labels from neighboring cells conceptually in the first algorithm each thread pushes its own label to uphill cells and in the second one it pulls the label up from lower neighbors in both cases each iteration of the algorithm propagates the watershed label by only one cell along the flow path but for multiple paths in parallel according to the results presented the label pulling approach turned out to be significantly more effective some of the more recent works use the capabilities of simd architectures available on the graphics processing units one of the steps described in eränen et al 2014 is the watershed delineation algorithm that performs all computations on the gpu in this implementation each thread starts at the assigned cell and follows the entire flow path until a defined stream section or dem boundary is reached cells whose flow path ends in the defined stream section are marked as belonging to the watershed area the work presented in makinen et al 2016 adapts a similar concept but extends its use to a multi gpu environment another work addressing the problem of fast delineation of the watershed area was presented in sit et al 2019 both sequential and parallel approaches were described here the basic iterative sequential algorithm finds cells belonging to the watershed in a bottom up manner starting from the outlet in subsequent iterations the procedure analyzes the flow direction of the nearest neighborhood of cells classified in the previous step the parallel approach takes advantage of the gpu capabilities using webgl shaders the algorithm runs for each cell independently its main idea is to use flow direction to identify the downstream neighbor and repeatedly read its label in each cycle if the downstream cell is identified as belonging to the catchment area the current cell is also marked as such this procedure is repeated iteratively until there are no more updates 1 2 other related studies many published papers focus on developing efficient algorithms related to other aspects of hydrological modeling aside from delineating watershed areas issues such as filling depressions in dems or calculating flow accumulation are often considered planchon and darboux 2002 wang and liu 2006 barnes et al 2014 zhou et al 2016 2019 particularly noteworthy are works considering the parallelization of computations wallis et al 2009 do et al 2011 barnes 2017 zhu et al 2019 including the use of graphics processing units ortega and rueda 2010 qin and zhan 2012 rueda et al 2016 sten et al 2016 wu et al 2019 1 2 1 simd pointer processing although this research focuses on hydrological modeling topics some related work outside of this area should also be mentioned in particular a specific decades old simd technique for processing pointers is relevant to the rest of this work hillis and steele 1986 described a set of data parallel algorithms designed to be performed on machines with a large number of processors one of the concepts presented here was dedicated to locating the last element in a linear linked list it was pointed out that although this task appears to be inherently sequential the work can be organized in a parallel manner allowing it to be completed in less time the core idea starts with expressing the order of elements as a series of pointers each referring to the one immediately behind it these are then repetitively reassigned by acquiring the addresses stored in the pointers to which they currently refer this operation is performed for all elements in parallel quickly leading to a state where all pointers refer to the last element of the list except for the last one which is marked with a special null value hillis and steele 1986 described this technique as surprising and counterintuitive but also pointed out that it had been discovered in other contexts before while coming from different fields some of the problems considered in mathematical morphology digital image processing and computer vision share certain similarities with gis related issues traces of the same simd technique can also be found in these areas one of the best known and most important problems in digital image processing is called segmentation beucher and meyer 1993 it can be broadly defined as the task of separating objects present in the image from their background roerdink and meijster 2000 over the years many techniques and approaches to this task have been developed ranging from the simplest grayscale threshold methods to solutions based on neural networks and deep learning yuheng and hao 2017 dmitruk et al 2021 minaee et al 2022 one of the classic approaches to image segmentation first introduced in digabel and lantuéjoul 1978 is called the watershed transformation both the name and the intuitive idea come from the area of hydrology metaphorically referring to a landscape being flooded by water or immersed in a lake roerdink and meijster 2000 the method operates on grayscale images aiming at segmenting them into regions representing separate objects the key idea is to interpret gray value discontinuity points as object boundaries despite significant advancements in the development of modern techniques the watershed transformation can still be used for some specific issues kornilov et al 2022 many noteworthy publications focus on developing efficient implementations of the watershed transformation often taking advantage of parallel processing and gpu capabilities the amount of research in this area is substantial and has been reviewed and summarized multiple times over the years roerdink and meijster 2000 kornilov and safonov 2018 kornilov et al 2022 some of these works implement variants of the pointer processing technique described previously the key idea here is to treat the pixels of a two dimensional image as pointers to one another and reduce the dependencies between them in a way similar to the one presented in hillis and steele 1986 vitor et al 2010 referred to this concept as path compressing and representative propagation while yeghiazaryan and voiculescu 2018 called it path reduction and label propagation the only adaptation of this technique to a gis related context that the author was able to find comes from mcgough et al 2012 the paper focuses on applying parallel processing to the landscape evolution model in order to reduce the required computational time one of the repetitively performed steps of the model is the sink filling procedure which was implemented here using the general workflow presented in arge et al 2003 described in earlier sections the procedure consists of assigning unique identifiers to each local sink propagating them to all connected cells building a relationship graph between the identified areas and raising the elevation values so that each cell has a non ascending path to the edge of the dem similarly to arge et al 2003 watersheds are understood here as collections of cells flowing into common local sinks in a pre filled dem rather than actual hydrologic units the authors point out that thousands of watersheds can be identified during this procedure the propagation of identifiers was implemented using a variant of the discussed simd technique here two rasters are processed simultaneously one with interrelated pointers and the other containing progressively propagated identifiers the authors refer to this concept as index pointer jumping since the flow direction rasters can be interpreted as matrices of pointers referring to their neighboring cells the discussed technique can be seamlessly adapted to the watershed delineation workflow used in hydrological modeling 2 watershed delineation algorithms 2 1 problem specification considering the numerous references to watershed delineation algorithms appearing in the literature as well as many attempts to develop more efficient methods it can be concluded that there is a need for a highly efficient algorithm allowing this operation to be performed on large datasets in a relatively short time when specifying the problem to be solved by the algorithm it was assumed that the input data consists of a two dimensional flow direction raster and a set of labeled outlet cell locations the possibility of delineating multiple watersheds in a single algorithm run as well as marking more than one cell as the outlet of the same watershed was taken into account many techniques existing in the literature consider delineating only a single watershed at a time and as noted in haag et al 2020 most of them use only a single outlet point which is not always suitable the author of this paper believes that it is worth extending the problem specification to include the possibility of using many such points belonging to the same or different watersheds this will not only address specific practical use cases but also allow for a more detailed analysis of the differences between algorithms especially in the context of parallel data processing there are two main ways to define flow direction in the literature in the single flow approach each raster cell points to only one downstream neighbor thus all drainage is directed to a single neighboring cell in the multiple flow approach drainage can be transferred proportionally to more than one neighbor there are papers comparing the two approaches and pointing to the advantages and specific applications of both barták 2009 lópez vicente et al 2014 the algorithms presented in this work use single flow direction rasters as the main input data as noted in barták 2009 this approach seems more appropriate for watershed delineation as it avoids flow dispersion and catchment overlap specific flow direction algorithms are beyond the scope of this work and are not discussed here in fact any single flow direction algorithm could be used to prepare input rasters which is one of the main reasons why this form of data was chosen as the main input over raw dems the only requirement is that the input dataset is hydrologically correct and does not contain recurring flow paths it is assumed that no additional input datasets other than the flow direction raster and the set of outlet point markers are needed in this work any possible transformation of this basic data or conversion to alternative models is considered as part of the watershed delineation procedure any processing of raw flow direction data is considered an internal part of the algorithm and taken into account when measuring its performance the output of the algorithm will be a raster of numerical values in a correct solution each cell will contain the index of the watershed it belongs to or the none value if its flow path does not end at any of the specified outlet points or goes beyond the dem boundary since it should be possible to delineate multiple watersheds in a single algorithm run a scenario of simultaneously delineating a larger watershed and its smaller sub watersheds must be considered in such cases the cells in the output raster will be labeled as part of the innermost sub watershed to which they belong it is assumed that the algorithm is executed on a single gpu equipped machine and the size of available memory is sufficient for the data to be processed 2 2 implementation details all implementations developed as part of this work were written in c 11 the code dedicated to the gpu was implemented using cuda additionally some parts of the code executed on the host were parallelized using openmp for faster performance only the fundamental constructs introduced in the early versions of these two standards were used the implementations store the watershed labels in 8 bit unsigned char cells this makes it possible to delineate up to 255 watersheds in a single algorithm run one of the 256 possible values is reserved for the none value in case more unique labels were needed implementations could be easily extended by changing the variable type wherever it was necessary to index raster cells with single numbers unsigned 32 bit ints were used to store the indices this imposes a technical constraint on these implementations limiting the maximum number of cells to 2 32 again in case larger data sizes are needed it is possible to increase this limit by changing the variable type 2 3 flow path reduction algorithm the algorithm developed as a key part of this work is designed for gpu devices and takes advantage of the specific properties associated with their massive parallelism in essence each raster cell is processed in parallel by a separate thread the core idea behind the algorithm can be divided into three main stages first an array of cell indices is prepared using the flow direction data initially each index points to its nearest downstream neighbor the flow paths are then reduced so that each cell ultimately contains the index of the outlet cell to which it eventually flows finally these indices are converted to watershed labels 2 3 1 data preparation host side before executing the key parts of the algorithm the host reshapes and adjusts the data to the required form in this work the data structure developed in kotyra et al 2021 was used to store flow direction rasters on the host in essence the data is stored in a two dimensional array with an additional frame of cells containing neutral values a single row or column of neutral cells on each of the four edges of the raster as a result rows and columns containing relevant data are indexed from 1 one based indexing this design was developed due to the large number of raster algorithms using the values of neighboring cells the frame of neutral values often makes it possible to simplify the code and reduce execution time while this feature has no significant application in this paper this structure was chosen to maintain work continuity and consistency in the published code the first step in preparing the data for the gpu is to flatten a two dimensional flow direction raster into a one dimensional array this approach aims to simplify and speed up the transfer and data processing on the device consecutive rows are rewritten without gaps skipping the frame of neutral cells while this operation is trivial it can take a noticeable amount of time for large datasets to accelerate this step the code was parallelized using openmp clauses the next step is to remove the direction from all cells marked as outlets setting their direction values to none due to the design of some parts of the algorithm it is required to ensure that the outlet cells are the endpoints of the flow paths and do not point to subsequent cells as the number of the outlet cells was assumed to be rather small a single cell being the most common scenario this operation did not seem worth parallelizing the prepared one dimensional flow direction array is then transferred to the gpu where it can be further processed 2 3 2 calculation of downstream cell indices the first kernel to run on the device performs a relatively simple transformation of the flow direction data into cell indices a new one dimensional array is prepared where each cell contains the index of its downstream neighbor the cell immediately next in the flow path for this purpose the index of each cell is shifted by an appropriate offset according to the corresponding flow direction the outlet cells marked with the none direction receive their own index and thus point to themselves the cells directing the flow outside the raster boundaries are treated in the same way consequently the endpoint cell of each flow path is marked with its own index the hydrological correctness of the input data is assumed therefore all remaining cells should receive the index of one of their neighbors this stage requires a single kernel run each thread processes a single cell by reading its flow direction value calculating the index and storing it in a new array as each cell can be processed independently parallelizing this operation was straightforward 2 3 3 reduction of flow paths when the indices of all downstream neighbors are calculated the data is ready to be processed by the main loop of the algorithm conceptually it is now possible to traverse a flow path from any cell to its endpoint by reading the indices of successive cells each pointing to the next downstream neighbor until reaching a cell that points to itself the aim of this stage is to reduce these flow paths to just a single step the index array is transformed so that each cell points to the final endpoint of its flow path when this stage is done each cell should either point to itself being an endpoint or directly to another cell pointing to itself after this transformation by reading the index stored in any cell it will be possible to immediately identify the watershed outlet to which it flows the transformation of the array is done by executing the path reduction kernel multiple times this is considered the main loop of the algorithm in a single kernel run the index of each cell is established by reaching its target cell reading the index stored there and updating its own if necessary conceptually the following procedure is performed on each cell read your target index reach the target cell and read its index if the target cell does not point to itself save its target index as your own the kernel is iteratively executed as long as any changes to the array are made it is important to emphasize why this approach is highly efficient the gpu architecture allows this operation to be performed in a massively parallel manner each cell is processed in parallel by a separate thread conceptually the moment a cell is updated with a new target index that target cell is also updated in a similar way although the first iteration of the kernel shifts the target indices only a single step in the flow paths each subsequent execution allows for a jump approximately twice as long using the knowledge accumulated in the previous steps as a result the processing of even large datasets is possible in a relatively small number of iterations it is worth noting that the number of iterations needed is directly dependent on the longest flow path as measured by the number of cells present in the input data it can be shown that the time complexity of this stage of the algorithm is sublinear as a flow path containing twice as many cells requires just one additional iteration conceptually the number of cells identified as flowing into a given endpoint grows exponentially with each step it is also worth emphasizing that the number of outlet cells belonging to the same or different watersheds does not affect the execution time the first implementation of this idea labeled back buffer uses two data buffers in each iteration the current state of the cells is read from one buffer while new values are written into the other the roles of the buffers are swapped before the next step this approach easily eliminates the scenario where some cells might be read by one thread and modified by another in a single iteration this guarantees completely predictable results at every step however the obvious disadvantage of this implementation is the additional memory requirement as data is continuously transferred between two buffers of the same size the second implementation uses a single buffer allowing both reading and writing of the same cells within the same iteration although this makes the result of a single step less predictable it is not obvious whether a given cell will be read before or after being modified by another thread it does not have a negative impact on the final result of the entire procedure in fact this implementation allows not only lower memory consumption but in some scenarios also fewer iterations required and consequently faster execution time a similar approach was discussed in golub and ortega 1993 where bypassing synchronization in iterative methods was used to reduce the overhead the authors referred to this solution as the asynchronous method in assessing this solution some concern may be related to the atomicity of the cell modification while it is not possible to predict whether a cell will be read before or after being modified by another thread it is necessary to guarantee that the value will be correct in both cases being either the value from the previous or the current step the device memory accesses section of the cuda c programming guide nvidia 2022 states that an access to data in global memory is compiled into a single instruction under certain conditions namely when the data size is 1 2 4 8 or 16 bytes and when the data is naturally aligned meaning the memory address is a multiple of that size if these conditions are not met the data access can be compiled into multiple instructions creating potential problems in a multithreaded context as the presented implementation meets the above conditions any additional synchronization of data access would be redundant 2 3 4 assignment of watershed labels the last step performed on the gpu is converting the target indices to watershed labels at this stage each cell points to its flow path endpoint conceptually this operation assigns each cell a label associated with the watershed outlet it points to or none if its flow path does not end at any of the specified outlets in the simplest case assuming a single watershed with only one outlet point the operation would be to compare the target index of each cell with the outlet location cells pointing to it would be labeled as belonging to the watershed area while the remaining ones as being located outside of it however it was decided that the algorithm should be able to delineate multiple watersheds at once each with one or more outlet cells so a more general solution had to be implemented first the algorithm prepares an array where the outlet cells contain the labels of the corresponding watersheds and all other cells are marked as none in order to reduce the memory consumption on the gpu the same array that originally stored the flow direction values is used the array is cleared filled with none values in parallel by a simple kernel then the watershed labels are copied into the outlet cells the conversion is performed in a single kernel execution all cells are processed independently of each other in a parallel manner for each cell a thread reads its target index and then assigns it a label located at that index in the label array consequently cells with flow paths ending at specified outlet points receive valid labels all other cells are marked with the none value the result of this operation is saved in the label array it is worth noting that the outlet cells point to themselves so their labels essentially do not change next the array prepared in this way is transferred back to the host memory it is then unpacked into a two dimensional raster containing a frame of neutral cells similar to the structure used to store the original flow direction data early measurements showed that parallelizing this operation can noticeably reduce its execution time therefore openmp clauses were used for this section of the code as well it is worth noting that the data arrays transferred to the gpu flow direction and from it watershed labels are both of 8 bit values the index array which requires considerably more memory is created and processed solely on the gpu never being transferred this design aims to minimize the time needed for data transfers 2 4 reference algorithms in order to be able to compare the performance of the proposed algorithm to the existing solutions several reference algorithms were implemented these implementations were prepared by the author of this paper but are based on concepts from the existing literature special attention was paid to related studies involving parallel processing and gpu devices the main techniques described there were adapted and used in performance comparisons 2 4 1 sequential recursive algorithm as one of the simplest and most intuitive solutions to the problem the recursive algorithm was implemented in a straightforward single threaded version in the first step the inverse flow direction matrix is prepared with each cell pointing to its inflow neighbors then starting with the outlet cells a recursive procedure is performed the procedure climbs in a bottom up manner recursively invoking itself for each inflow neighbor each cell traversed in this way is marked as part of the watershed area as it eventually flows down to the outlet point where the procedure started since the reference algorithms should be able to work with multiple outlet points as well it was necessary to handle a scenario where one of the outlet cells belongs to another larger watershed area marked with some other outlet point a simple mechanism was implemented to limit recursive bottom up climbing when one of these cells is reached this approach eliminates scenarios where any sub area is traversed more than once it is worth noting that the execution time of this algorithm depends on the total size of the watershed areas marked in the input data the recursive procedure only moves within these regions ignoring the cells outside it is also worth emphasizing that in practical applications this class of algorithms can lead to memory problems such as stack overflows in this paper these issues are not discussed in detail as this implementation is treated purely as a simple benchmark for performance measurements 2 4 2 flow path tracing algorithm gpu this implementation is based on the concept described in eränen et al 2014 in this algorithm each raster cell is assigned its own individual thread on the gpu device the main idea is to follow the flow path of each cell individually until a specified stream section or the dem boundary is reached depending on where the cell s flow path ends a corresponding watershed label is assigned to it the algorithm begins with the preparation and transfer of input data flow direction and outlet cell markers to the gpu memory there a working array is prepared where the outlet cells are initialized with their watershed labels initially all other cells contain the none value the goal of the algorithm is to assign each cell to the correct watershed and update its label the main part of this implementation is the path tracing kernel based on an inner loop the procedure is performed for each cell that initially does not have a watershed label assigned to it each thread starts by reading the flow direction value of its cell and then follows it to its downstream neighbor this step is repeated until a given thread reaches the outlet point marked with no flow direction value or crosses the dem boundary if the flow path ends at one of the outlet points the watershed label is copied from there to the thread s starting cell when this stage is completed the array can be transferred back to the host memory it should be highlighted that this idea involves different threads doing the same work multiple times a single cell in the flow path will be repeatedly traversed by different threads starting from its upstream cells while the parallel nature of gpu devices accelerates processing this way of organizing work is a significant drawback of this algorithm as in the flow path reduction algorithm data reshaping and manipulation steps on the host side both at the first and the last stage were parallelized using openmp 2 4 3 label pulling algorithm gpu another reference implementation is inspired by the concepts presented in mower 1994 and sit et al 2019 again each cell in the raster is assigned its own individual thread on the gpu device the main idea is to use the flow direction of a cell to locate its downstream neighbor and then iteratively reach for the neighbor s watershed label when a valid label becomes available it is copied pulled up to the thread s cell and consequently becomes available to other threads the algorithm starts by adjusting the input data format and transferring it to the gpu again data manipulation on the host side is performed in parallel there the flow direction is used to calculate and store the downstream neighbor index for each cell next a working array is prepared where the outlet cells are initialized with the corresponding labels and all other cells are initially set to none the goal of the algorithm is to propagate the labels from these outlet points gradually updating successive upstream cells the main loop of this algorithm iteratively calls the gpu kernel each thread reads its own cell first if the cell does not have a valid other than none label yet the label of its downstream neighbor is read if this label is found to be valid it is copied into the current cell the kernel is iteratively invoked as long as at least one thread reports that a modification has been made it should be emphasized that although many threads work in parallel here only a small fraction of all operations are meaningful in each iteration a given thread reads the label from its cell and reaches its downstream neighbor if needed of all these cycles only one results in an actual modification of that thread s cell in all other iterations the thread either does not yet have a valid label from its neighbor or has already copied it the number of kernel calls needed to produce a complete result is directly linearly related to the length of the flow path with the highest cell count 3 performance measurements 3 1 data the largest dataset used in this work was a dem with over two billion cells originating from publicly available resources of the head office of geodesy and cartography gugik the data covered a part of south eastern poland one meter resolution pl 1992 coordinate system the dem underwent a depression filling procedure to ensure its hydrological correctness more than 90 of all cells in this dataset belong to a single watershed area this dem was used to prepare flow direction rasters later used as input data for the algorithms the largest raster was generated directly from the entire dem next the dataset was scaled down to prepare smaller test cases with approximately the same internal characteristics covering the same area of land but at different scales a total of 30 input datasets were prepared one with original size and 29 scaled down with the number of cells increasing linearly between approximately 67 5 million and 2 billion this approach was adopted to make it easier to examine the relationship between data size and algorithm performance while minimizing other differences in data characteristics 3 2 testing procedure the direct input for all tested algorithms were flow direction rasters loaded from previously prepared files before starting the actual tests their correctness was verified before the measurements each algorithm passed through multiple series of automated tests to confirm that its implementation was correct various scenarios including specific corner cases different data sizes and multithreading configurations were taken into account the performance of the algorithms was measured by repeatedly running each one on every dataset and recording execution times in each test the measurement application was restarted to reflect a realistic use case and eliminate the potential distortion of execution times related to the cache memory every test included loading a flow direction file starting time measurement executing a given algorithm and stopping the measurement all operations needed to produce the final result performed after successfully loading the flow direction file into memory were treated as part of the algorithm and included in the time measurements additionally the generated result was verified after each execution all tests were performed on a machine equipped with two intel xeon e5 2670 v3 processors 24 cores in total 128 gb ram and nvidia a100 tensor core gpu in 40 gb memory version the computer was running under almalinux 8 4 the source code was compiled with the nvcc compiler from the cuda toolkit v11 2 code optimization o3 flag and openmp support were enabled for gpu computation the maximum available thread block size 1024 was used it was assumed that the device is initialized and synchronized the code sections executed in parallel on the host had all 24 cores available as part of the performance comparison each tested algorithm was executed ten times on every input dataset the task was to delineate the single largest watershed area in the dataset a single outlet cell was marked 4 results 4 1 performance comparison all results generated in each test passed the automated validation the time measurements clearly show that the flow path reduction algorithm performed by far the best in all cases both implementations of this approach achieved significantly shorter execution times than all the other tested algorithms the single buffer version of the algorithm achieved execution times shorter by over 18 on average compared to the back buffer implementation this result is not surprising considering the memory access patterns and differences in the number of iterations needed as described earlier averaging across all datasets the recursive implementation took approximately 33 times longer to generate the result than the single buffer flow path reduction algorithm taking the sequential recursive approach as a straightforward reference solution this can be considered a significant improvement the flow path reduction algorithm also proved to be significantly more efficient than the gpu based reference solutions in general the measured execution times varied by two orders of magnitude across all datasets the flow path tracing algorithm took on average as much as 119 times longer to execute than the single buffer implementation for the label pulling algorithm the average ratio was even greater reaching 483 times in both cases the difference becomes more significant as the data size increases it is worth emphasizing that the two gpu based reference algorithms turned out to be actually less efficient than simple sequential recursion as mentioned earlier much of the computation performed by parallel versions of these techniques is in fact redundant which may help explain these results the label pulling algorithm achieved by far the longest execution times of all tested implementations although these measurements are not directly comparable to those presented in sit et al 2019 the results appear to be somewhat in line with the fact that the gpu based label pulling technique tested there achieved longer execution times than some sequential implementations table 1 shows the average execution times for selected datasets fig 1 presents a visual comparison of the average execution times for all datasets results of the back buffer version are not present here as they were indistinguishable from the single buffer version at this scale detailed results of all measurements are available in the public repository 4 2 further analysis after the performance comparison additional tests and measurements were performed to further investigate the properties of the flow path reduction algorithm in the single buffer version in particular it was examined which stages of the algorithm require the most computational time a series of tests was performed using the largest dataset to measure the execution time of each stage individually although the main loop of the algorithm repeatedly invoking the gpu kernel is the most time consuming step it turned out to account for only 28 8 of the total execution time on average this may seem surprising but it can be explained by the fact that this key operation is effectively parallelized on the gpu and is performed relatively quickly compared to the other stages data transfers accounted for a large part of the execution time transferring input data to the gpu and retrieving the results back to the host added up to 34 5 of the total time on average also data reshaping operations despite being parallelized on the host side added up to a significant 33 1 table 2 presents the averaged measurements of all individual stages another series of tests was carried out to verify whether the algorithm is able to delineate multiple watersheds simultaneously without the need for additional computational time measurements were performed using the largest dataset up to eight watersheds were delineated simultaneously table 3 shows the average execution times there were no significant deviations from the time needed to delineate a single watershed using the same dataset 5 conclusions as part of this work the watershed delineation algorithms available in the literature were reviewed issues related to their performance were identified indicating a need for improvement a new algorithm was developed and presented proposing an approach that effectively uses the massive parallelism of gpu devices to rapidly delineate watershed areas its performance was measured and compared with reference algorithms based on concepts existing in the literature the results show that the proposed approach allows for a significant reduction in the computational time needed to perform the watershed delineation compared to other tested algorithms as noted in haag et al 2018 the existing techniques are insufficient to delineate the watershed boundaries on the fly the author believes that this work can help fill this gap allowing for a fast delineation of watersheds directly from basic flow direction data even on larger datasets the author decided not to use existing gis software packages as references in performance comparisons instead focusing on parallel algorithms available in the literature although accurate measurements were not carried out some early tests showed that it is not unusual for popular gis platforms to take several orders of magnitude longer to perform similar operations on a machine with the same specification it seems clear that there is still room for significant improvement it is worth emphasizing that the solution presented in this paper makes it possible to delineate multiple watersheds simultaneously as well as use multiple outlet cells for the same watershed with practically zero additional computational costs this can be considered a significant advantage of this method especially in the context of the cited literature in fact delineating a single watershed area could be considered a special case of the more general task discussed here an implementation focusing specifically on this scenario could possibly reduce the computation time of some stages even further while the techniques and ideas presented in this paper were designed for square grid dems they seem to be easily applicable to other types of data as well this may prove important as recent studies focused on hexagonal grids have shown promising results liao et al 2020 in addition although the presented concepts were developed specifically for watershed delineation perhaps they could also be adapted to other similarly structured modeling issues software availability software watershed delineation algorithms description source code of all algorithms developed tested and presented as part of this work including a simple measurement application developer bartłomiej kotyra contact address bartlomiej kotyra mail umcs pl language c 11 cuda openmp libraries required gdal geospatial data abstraction library availability freely available at https github com bkotyra watershed delineation gpu declaration of competing interest the author declares that he has no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper this research did not receive any specific grant from funding agencies in the public commercial or not for profit sectors 
25471,watershed delineation is one of the fundamental tasks in hydrological studies tools for extracting watersheds from digital elevation models and flow direction rasters are commonly implemented in gis software packages however the performance of available techniques and algorithms often turns out to be far from sufficient especially when working with large datasets while modern hardware offers high computing performance through massive parallelism there is still a need for algorithms that can effectively use these capabilities this paper proposes an algorithm for rapid watershed delineation directly from flow direction rasters using the possibilities offered by modern gpu devices performance measurements show a significant reduction in execution time compared to other parallel solutions proposed for this task in the literature moreover this implementation makes it possible to delineate multiple watersheds from the same dataset simultaneously each having one or more outlet cells with virtually no additional computational cost keywords watershed delineation gis parallel algorithms gpu cuda openmp data availability the source code is available in a public repository the data used in performance measurement originate from publicly available resources referenced in the manuscript 1 introduction watershed also referred to as drainage area basin or catchment is considered one of the basic concepts in hydrological studies tesfa et al 2011 it is defined as the area of land whose drainage eventually concentrates in a single location called the watershed outlet chow et al 1988 delineating watersheds and their boundaries is one of the fundamental tasks in this field and is used in many different contexts within and beyond the area of hydrology daniel 2011 singh 2018 while watersheds can be delineated manually using topographic data automatic techniques are widely accepted as being much faster and more precise karimipour et al 2013 procedures using digital elevation models dems as basic input data are well known and widely implemented in gis software barták 2009 in recent decades the availability and precision of this type of data has increased significantly which considerably facilitates accurate calculations and simulations but at the same time creates new challenges related to the processing of large datasets tang and wang 2020 currently the market offers hardware architectures that enable achieving previously unavailable high computing performance wide access to multicore and many core processors and graphics processing units gpus creates new possibilities for parallel processing making it possible to solve complex tasks on large datasets in a much shorter time parallel programming standards like openmp and cuda allow access to these hardware capabilities with relatively little effort chapman et al 2007 cheng et al 2014 still the effective use of the possibilities offered by modern devices requires creating suitable parallel algorithms which remains a challenging task there is a significant time gap between the available technological solutions and their practical applications tang and wang 2020 existing software often turns out to be insufficiently scalable or simply unsuitable for working with modern datasets the motivation behind this research was to investigate and address issues related to the performance of existing watershed delineation algorithms concluding from the available literature this problem has been discussed in multiple studies but there is still room and need for significant improvement the goal of this work was to develop and present a new raster based algorithm for delineating watersheds operating on dems and derivative data allowing this task to be performed more efficiently compared to existing alternatives particular attention was paid to the possibilities offered by modern gpu devices along with parallel processing capabilities on the host side 1 1 existing techniques many different methods and approaches for automatic watershed delineation have been described in the literature they differ significantly in terms of technology and architecture applied the type of input and data structures used as well as the computational complexity of implemented algorithms most of the existing research and practical implementations in this area use square grid dems and their derivatives such as flow direction rasters as the basis for further operations the foundations of this approach are well known o callaghan and mark 1984 jenson and domingue 1988 however it is worth noting that this is not the only way to address the task some research works are focused on delineating watershed areas from triangle based terrain models jones et al 1990 nelson et al 1994 de azeredo freitas et al 2016 or hexagonal grids liao et al 2020 there are also alternative methods that may be useful when accurate dems are not available karimipour et al 2013 the most common well established workflow for delineating watersheds from square grid dems consists of several separate processing stages baker et al 2006 eränen et al 2014 before a proper hydrological analysis can be carried out the elevation data must be corrected by removing spurious sinks and local depressions this is due to the fact that many existing models and algorithms require each dem cell to have a downslope path leading to the edge of the raster wang and liu 2006 depression filling is the most common method used to satisfy this condition tarboton et al 2009 however many recent papers suggest using other approaches as they can produce the desired result with much less modification to the original elevation values lindsay 2016 chen et al 2021 once the depressions are removed the hydrologically corrected dem can be used to calculate the flow direction data lindsay et al 2008 at this stage each cell is assigned a value corresponding to the expected direction of its further downstream flow a variety of approaches and specific algorithms for determining flow directions exist in the literature o callaghan and mark 1984 fairfield and leymarie 1991 freeman 1991 tarboton 1997 seibert and mcglynn 2007 in essence the purpose of this step is to determine how the outflow from each cell is distributed to its immediate neighbors wilson et al 2008 the next step is usually to calculate the flow accumulation data where each cell is assigned the total number of cells that eventually flow to it jenson and domingue 1988 martz and garbrecht 1993 these values can be used to delineate the drainage network as well as to precisely locate the watershed outlet cells lindsay et al 2008 once the flow direction is determined and the outlet cells are located it is possible to perform the delineation of selected watersheds in general this stage is usually carried out using algorithms that identify all upslope cells connected to chosen outlet points by overland flow paths lindsay et al 2008 the remaining sections of this paper mainly deal with the last stage of this workflow assuming that flow direction data and outlet cell locations are already available 1 1 1 recursive algorithms among the algorithms that utilize square grid dems one of the first and best known is the recursive approach the earliest presentation of this concept that the author is aware of comes from marks et al 1984 in this approach calculations start from a designated cell of the watershed outlet the algorithm analyzes the immediate neighbors of this location identifying the upstream cells flowing to a given point and classifying them as part of the watershed area the same procedure is then performed recursively for each cell classified this way the algorithm moves in a bottom up manner starting from the outlet point and following the flow paths upwards opposite to the flow direction the main advantages of the recursive approach are its simplicity and relative efficiency only cells belonging to the watershed area and their direct neighborhood are analyzed other cells are not entered through the procedure however the nature of recursive algorithms in general often leads to memory issues especially on larger datasets and proves difficult to effectively parallelize wallis et al 2009 qin and zhan 2012 1 1 2 iterative algorithms there are multiple references to iterative implementations of watershed delineation algorithms in the literature unfortunately not all papers discuss their approach in detail jenson and domingue 1988 described a procedure that uses a flow direction dataset and a starter raster where outlet points are marked with numerical values the algorithm uses flow direction data to iteratively fill all cells with the start values of the outlet to which they flow martz and garbrecht 1993 mentioned a watershed boundary delineation procedure based on flow vectors determined with the use of the d8 method the algorithm identifies all cells that eventually flow into the user specified outlet cell the details of this stage are not extensively discussed but it can be inferred from the context that the procedure is based on following the steepest descent path starting from each cell individually this method was used there for accumulated drainage area calculations a similar concept was used for tin data in nelson et al 1994 the algorithm starts at the centroid of each triangle and follows the flow path until it hits one of the terminus points another approach is described in choi and engel 2003 here an iterative method using flow direction was used which avoids scanning the entire raster the procedure starts with a single outlet cell selected by the user in each iteration the algorithm considers only the closest neighborhood of cells classified as part of the watershed in the previous step using the flow direction data successive upstream cells are identified this procedure is repeated until no more matching cells can be found 1 1 3 sorting and priority queues another type of approach is based on sorting dem cells and visiting them in a specific order arge et al 2003 described an algorithm that processes cells in a bottom up manner reverse topological order gradually propagating watershed labels from lower to higher cells however it should be noted that computing watersheds is understood here as part of a complex flooding procedure rather than the actual delineation of watershed areas unique labels are assigned to each local sink and then propagated to the remaining cells the idea is to identify areas concentrating their drainage in common sinks and to construct a graph of relationships between these areas it is then used to raise the elevations to ensure that for each cell there is a non ascending path to the edge of the terrain barnes et al 2014 integrated and improved on multiple related works by several authors and presented a unified algorithm based on a priority queue primarily intended for filling depressions in dems it was pointed out that this concept can be adapted and used for labeling watersheds as well in this variant of the algorithm unlabeled cells at the edges of the available data are assumed to be watershed outlets and are assigned unique labels these values are then propagated to the remaining cells by flooding the dem inwards it is worth noting that this group of algorithms processes cells in a relatively straightforward manner but this comes at the expense of additional computational time needed to perform the ordering it is also important to note that this type of approach is based on distinct assumptions placing it outside the typical watershed delineation workflow 1 1 4 other data models yet another approach to the problem can be found in haag et al 2018 2020 the algorithms presented here are intended to significantly reduce the computational time by marching around the watershed boundary without entering or leaving its area however it is necessary to note that these techniques require converting the flow direction to other data models specifically designed for this task which entails additional computational and storage costs performing such an operation directly on the flow direction is not possible castronova and goodall 2014 highlighted the issues related to processing large datasets and demonstrated an alternative approach to watershed delineation however this technique relies on the availability of additional datasets without which the dem based approach still appears to be a valid choice 1 1 5 parallel algorithms parallel algorithms are a separate category in general this approach aims to make better use of hardware capabilities and reduce computational time the earliest application of simd single instruction multiple data computers to watershed delineation that the author is aware of can be found in mower 1994 the data parallel approach was used in two different iterative algorithms developed for thinking machines cm 5 both ideas are based on copying watershed labels from neighboring cells conceptually in the first algorithm each thread pushes its own label to uphill cells and in the second one it pulls the label up from lower neighbors in both cases each iteration of the algorithm propagates the watershed label by only one cell along the flow path but for multiple paths in parallel according to the results presented the label pulling approach turned out to be significantly more effective some of the more recent works use the capabilities of simd architectures available on the graphics processing units one of the steps described in eränen et al 2014 is the watershed delineation algorithm that performs all computations on the gpu in this implementation each thread starts at the assigned cell and follows the entire flow path until a defined stream section or dem boundary is reached cells whose flow path ends in the defined stream section are marked as belonging to the watershed area the work presented in makinen et al 2016 adapts a similar concept but extends its use to a multi gpu environment another work addressing the problem of fast delineation of the watershed area was presented in sit et al 2019 both sequential and parallel approaches were described here the basic iterative sequential algorithm finds cells belonging to the watershed in a bottom up manner starting from the outlet in subsequent iterations the procedure analyzes the flow direction of the nearest neighborhood of cells classified in the previous step the parallel approach takes advantage of the gpu capabilities using webgl shaders the algorithm runs for each cell independently its main idea is to use flow direction to identify the downstream neighbor and repeatedly read its label in each cycle if the downstream cell is identified as belonging to the catchment area the current cell is also marked as such this procedure is repeated iteratively until there are no more updates 1 2 other related studies many published papers focus on developing efficient algorithms related to other aspects of hydrological modeling aside from delineating watershed areas issues such as filling depressions in dems or calculating flow accumulation are often considered planchon and darboux 2002 wang and liu 2006 barnes et al 2014 zhou et al 2016 2019 particularly noteworthy are works considering the parallelization of computations wallis et al 2009 do et al 2011 barnes 2017 zhu et al 2019 including the use of graphics processing units ortega and rueda 2010 qin and zhan 2012 rueda et al 2016 sten et al 2016 wu et al 2019 1 2 1 simd pointer processing although this research focuses on hydrological modeling topics some related work outside of this area should also be mentioned in particular a specific decades old simd technique for processing pointers is relevant to the rest of this work hillis and steele 1986 described a set of data parallel algorithms designed to be performed on machines with a large number of processors one of the concepts presented here was dedicated to locating the last element in a linear linked list it was pointed out that although this task appears to be inherently sequential the work can be organized in a parallel manner allowing it to be completed in less time the core idea starts with expressing the order of elements as a series of pointers each referring to the one immediately behind it these are then repetitively reassigned by acquiring the addresses stored in the pointers to which they currently refer this operation is performed for all elements in parallel quickly leading to a state where all pointers refer to the last element of the list except for the last one which is marked with a special null value hillis and steele 1986 described this technique as surprising and counterintuitive but also pointed out that it had been discovered in other contexts before while coming from different fields some of the problems considered in mathematical morphology digital image processing and computer vision share certain similarities with gis related issues traces of the same simd technique can also be found in these areas one of the best known and most important problems in digital image processing is called segmentation beucher and meyer 1993 it can be broadly defined as the task of separating objects present in the image from their background roerdink and meijster 2000 over the years many techniques and approaches to this task have been developed ranging from the simplest grayscale threshold methods to solutions based on neural networks and deep learning yuheng and hao 2017 dmitruk et al 2021 minaee et al 2022 one of the classic approaches to image segmentation first introduced in digabel and lantuéjoul 1978 is called the watershed transformation both the name and the intuitive idea come from the area of hydrology metaphorically referring to a landscape being flooded by water or immersed in a lake roerdink and meijster 2000 the method operates on grayscale images aiming at segmenting them into regions representing separate objects the key idea is to interpret gray value discontinuity points as object boundaries despite significant advancements in the development of modern techniques the watershed transformation can still be used for some specific issues kornilov et al 2022 many noteworthy publications focus on developing efficient implementations of the watershed transformation often taking advantage of parallel processing and gpu capabilities the amount of research in this area is substantial and has been reviewed and summarized multiple times over the years roerdink and meijster 2000 kornilov and safonov 2018 kornilov et al 2022 some of these works implement variants of the pointer processing technique described previously the key idea here is to treat the pixels of a two dimensional image as pointers to one another and reduce the dependencies between them in a way similar to the one presented in hillis and steele 1986 vitor et al 2010 referred to this concept as path compressing and representative propagation while yeghiazaryan and voiculescu 2018 called it path reduction and label propagation the only adaptation of this technique to a gis related context that the author was able to find comes from mcgough et al 2012 the paper focuses on applying parallel processing to the landscape evolution model in order to reduce the required computational time one of the repetitively performed steps of the model is the sink filling procedure which was implemented here using the general workflow presented in arge et al 2003 described in earlier sections the procedure consists of assigning unique identifiers to each local sink propagating them to all connected cells building a relationship graph between the identified areas and raising the elevation values so that each cell has a non ascending path to the edge of the dem similarly to arge et al 2003 watersheds are understood here as collections of cells flowing into common local sinks in a pre filled dem rather than actual hydrologic units the authors point out that thousands of watersheds can be identified during this procedure the propagation of identifiers was implemented using a variant of the discussed simd technique here two rasters are processed simultaneously one with interrelated pointers and the other containing progressively propagated identifiers the authors refer to this concept as index pointer jumping since the flow direction rasters can be interpreted as matrices of pointers referring to their neighboring cells the discussed technique can be seamlessly adapted to the watershed delineation workflow used in hydrological modeling 2 watershed delineation algorithms 2 1 problem specification considering the numerous references to watershed delineation algorithms appearing in the literature as well as many attempts to develop more efficient methods it can be concluded that there is a need for a highly efficient algorithm allowing this operation to be performed on large datasets in a relatively short time when specifying the problem to be solved by the algorithm it was assumed that the input data consists of a two dimensional flow direction raster and a set of labeled outlet cell locations the possibility of delineating multiple watersheds in a single algorithm run as well as marking more than one cell as the outlet of the same watershed was taken into account many techniques existing in the literature consider delineating only a single watershed at a time and as noted in haag et al 2020 most of them use only a single outlet point which is not always suitable the author of this paper believes that it is worth extending the problem specification to include the possibility of using many such points belonging to the same or different watersheds this will not only address specific practical use cases but also allow for a more detailed analysis of the differences between algorithms especially in the context of parallel data processing there are two main ways to define flow direction in the literature in the single flow approach each raster cell points to only one downstream neighbor thus all drainage is directed to a single neighboring cell in the multiple flow approach drainage can be transferred proportionally to more than one neighbor there are papers comparing the two approaches and pointing to the advantages and specific applications of both barták 2009 lópez vicente et al 2014 the algorithms presented in this work use single flow direction rasters as the main input data as noted in barták 2009 this approach seems more appropriate for watershed delineation as it avoids flow dispersion and catchment overlap specific flow direction algorithms are beyond the scope of this work and are not discussed here in fact any single flow direction algorithm could be used to prepare input rasters which is one of the main reasons why this form of data was chosen as the main input over raw dems the only requirement is that the input dataset is hydrologically correct and does not contain recurring flow paths it is assumed that no additional input datasets other than the flow direction raster and the set of outlet point markers are needed in this work any possible transformation of this basic data or conversion to alternative models is considered as part of the watershed delineation procedure any processing of raw flow direction data is considered an internal part of the algorithm and taken into account when measuring its performance the output of the algorithm will be a raster of numerical values in a correct solution each cell will contain the index of the watershed it belongs to or the none value if its flow path does not end at any of the specified outlet points or goes beyond the dem boundary since it should be possible to delineate multiple watersheds in a single algorithm run a scenario of simultaneously delineating a larger watershed and its smaller sub watersheds must be considered in such cases the cells in the output raster will be labeled as part of the innermost sub watershed to which they belong it is assumed that the algorithm is executed on a single gpu equipped machine and the size of available memory is sufficient for the data to be processed 2 2 implementation details all implementations developed as part of this work were written in c 11 the code dedicated to the gpu was implemented using cuda additionally some parts of the code executed on the host were parallelized using openmp for faster performance only the fundamental constructs introduced in the early versions of these two standards were used the implementations store the watershed labels in 8 bit unsigned char cells this makes it possible to delineate up to 255 watersheds in a single algorithm run one of the 256 possible values is reserved for the none value in case more unique labels were needed implementations could be easily extended by changing the variable type wherever it was necessary to index raster cells with single numbers unsigned 32 bit ints were used to store the indices this imposes a technical constraint on these implementations limiting the maximum number of cells to 2 32 again in case larger data sizes are needed it is possible to increase this limit by changing the variable type 2 3 flow path reduction algorithm the algorithm developed as a key part of this work is designed for gpu devices and takes advantage of the specific properties associated with their massive parallelism in essence each raster cell is processed in parallel by a separate thread the core idea behind the algorithm can be divided into three main stages first an array of cell indices is prepared using the flow direction data initially each index points to its nearest downstream neighbor the flow paths are then reduced so that each cell ultimately contains the index of the outlet cell to which it eventually flows finally these indices are converted to watershed labels 2 3 1 data preparation host side before executing the key parts of the algorithm the host reshapes and adjusts the data to the required form in this work the data structure developed in kotyra et al 2021 was used to store flow direction rasters on the host in essence the data is stored in a two dimensional array with an additional frame of cells containing neutral values a single row or column of neutral cells on each of the four edges of the raster as a result rows and columns containing relevant data are indexed from 1 one based indexing this design was developed due to the large number of raster algorithms using the values of neighboring cells the frame of neutral values often makes it possible to simplify the code and reduce execution time while this feature has no significant application in this paper this structure was chosen to maintain work continuity and consistency in the published code the first step in preparing the data for the gpu is to flatten a two dimensional flow direction raster into a one dimensional array this approach aims to simplify and speed up the transfer and data processing on the device consecutive rows are rewritten without gaps skipping the frame of neutral cells while this operation is trivial it can take a noticeable amount of time for large datasets to accelerate this step the code was parallelized using openmp clauses the next step is to remove the direction from all cells marked as outlets setting their direction values to none due to the design of some parts of the algorithm it is required to ensure that the outlet cells are the endpoints of the flow paths and do not point to subsequent cells as the number of the outlet cells was assumed to be rather small a single cell being the most common scenario this operation did not seem worth parallelizing the prepared one dimensional flow direction array is then transferred to the gpu where it can be further processed 2 3 2 calculation of downstream cell indices the first kernel to run on the device performs a relatively simple transformation of the flow direction data into cell indices a new one dimensional array is prepared where each cell contains the index of its downstream neighbor the cell immediately next in the flow path for this purpose the index of each cell is shifted by an appropriate offset according to the corresponding flow direction the outlet cells marked with the none direction receive their own index and thus point to themselves the cells directing the flow outside the raster boundaries are treated in the same way consequently the endpoint cell of each flow path is marked with its own index the hydrological correctness of the input data is assumed therefore all remaining cells should receive the index of one of their neighbors this stage requires a single kernel run each thread processes a single cell by reading its flow direction value calculating the index and storing it in a new array as each cell can be processed independently parallelizing this operation was straightforward 2 3 3 reduction of flow paths when the indices of all downstream neighbors are calculated the data is ready to be processed by the main loop of the algorithm conceptually it is now possible to traverse a flow path from any cell to its endpoint by reading the indices of successive cells each pointing to the next downstream neighbor until reaching a cell that points to itself the aim of this stage is to reduce these flow paths to just a single step the index array is transformed so that each cell points to the final endpoint of its flow path when this stage is done each cell should either point to itself being an endpoint or directly to another cell pointing to itself after this transformation by reading the index stored in any cell it will be possible to immediately identify the watershed outlet to which it flows the transformation of the array is done by executing the path reduction kernel multiple times this is considered the main loop of the algorithm in a single kernel run the index of each cell is established by reaching its target cell reading the index stored there and updating its own if necessary conceptually the following procedure is performed on each cell read your target index reach the target cell and read its index if the target cell does not point to itself save its target index as your own the kernel is iteratively executed as long as any changes to the array are made it is important to emphasize why this approach is highly efficient the gpu architecture allows this operation to be performed in a massively parallel manner each cell is processed in parallel by a separate thread conceptually the moment a cell is updated with a new target index that target cell is also updated in a similar way although the first iteration of the kernel shifts the target indices only a single step in the flow paths each subsequent execution allows for a jump approximately twice as long using the knowledge accumulated in the previous steps as a result the processing of even large datasets is possible in a relatively small number of iterations it is worth noting that the number of iterations needed is directly dependent on the longest flow path as measured by the number of cells present in the input data it can be shown that the time complexity of this stage of the algorithm is sublinear as a flow path containing twice as many cells requires just one additional iteration conceptually the number of cells identified as flowing into a given endpoint grows exponentially with each step it is also worth emphasizing that the number of outlet cells belonging to the same or different watersheds does not affect the execution time the first implementation of this idea labeled back buffer uses two data buffers in each iteration the current state of the cells is read from one buffer while new values are written into the other the roles of the buffers are swapped before the next step this approach easily eliminates the scenario where some cells might be read by one thread and modified by another in a single iteration this guarantees completely predictable results at every step however the obvious disadvantage of this implementation is the additional memory requirement as data is continuously transferred between two buffers of the same size the second implementation uses a single buffer allowing both reading and writing of the same cells within the same iteration although this makes the result of a single step less predictable it is not obvious whether a given cell will be read before or after being modified by another thread it does not have a negative impact on the final result of the entire procedure in fact this implementation allows not only lower memory consumption but in some scenarios also fewer iterations required and consequently faster execution time a similar approach was discussed in golub and ortega 1993 where bypassing synchronization in iterative methods was used to reduce the overhead the authors referred to this solution as the asynchronous method in assessing this solution some concern may be related to the atomicity of the cell modification while it is not possible to predict whether a cell will be read before or after being modified by another thread it is necessary to guarantee that the value will be correct in both cases being either the value from the previous or the current step the device memory accesses section of the cuda c programming guide nvidia 2022 states that an access to data in global memory is compiled into a single instruction under certain conditions namely when the data size is 1 2 4 8 or 16 bytes and when the data is naturally aligned meaning the memory address is a multiple of that size if these conditions are not met the data access can be compiled into multiple instructions creating potential problems in a multithreaded context as the presented implementation meets the above conditions any additional synchronization of data access would be redundant 2 3 4 assignment of watershed labels the last step performed on the gpu is converting the target indices to watershed labels at this stage each cell points to its flow path endpoint conceptually this operation assigns each cell a label associated with the watershed outlet it points to or none if its flow path does not end at any of the specified outlets in the simplest case assuming a single watershed with only one outlet point the operation would be to compare the target index of each cell with the outlet location cells pointing to it would be labeled as belonging to the watershed area while the remaining ones as being located outside of it however it was decided that the algorithm should be able to delineate multiple watersheds at once each with one or more outlet cells so a more general solution had to be implemented first the algorithm prepares an array where the outlet cells contain the labels of the corresponding watersheds and all other cells are marked as none in order to reduce the memory consumption on the gpu the same array that originally stored the flow direction values is used the array is cleared filled with none values in parallel by a simple kernel then the watershed labels are copied into the outlet cells the conversion is performed in a single kernel execution all cells are processed independently of each other in a parallel manner for each cell a thread reads its target index and then assigns it a label located at that index in the label array consequently cells with flow paths ending at specified outlet points receive valid labels all other cells are marked with the none value the result of this operation is saved in the label array it is worth noting that the outlet cells point to themselves so their labels essentially do not change next the array prepared in this way is transferred back to the host memory it is then unpacked into a two dimensional raster containing a frame of neutral cells similar to the structure used to store the original flow direction data early measurements showed that parallelizing this operation can noticeably reduce its execution time therefore openmp clauses were used for this section of the code as well it is worth noting that the data arrays transferred to the gpu flow direction and from it watershed labels are both of 8 bit values the index array which requires considerably more memory is created and processed solely on the gpu never being transferred this design aims to minimize the time needed for data transfers 2 4 reference algorithms in order to be able to compare the performance of the proposed algorithm to the existing solutions several reference algorithms were implemented these implementations were prepared by the author of this paper but are based on concepts from the existing literature special attention was paid to related studies involving parallel processing and gpu devices the main techniques described there were adapted and used in performance comparisons 2 4 1 sequential recursive algorithm as one of the simplest and most intuitive solutions to the problem the recursive algorithm was implemented in a straightforward single threaded version in the first step the inverse flow direction matrix is prepared with each cell pointing to its inflow neighbors then starting with the outlet cells a recursive procedure is performed the procedure climbs in a bottom up manner recursively invoking itself for each inflow neighbor each cell traversed in this way is marked as part of the watershed area as it eventually flows down to the outlet point where the procedure started since the reference algorithms should be able to work with multiple outlet points as well it was necessary to handle a scenario where one of the outlet cells belongs to another larger watershed area marked with some other outlet point a simple mechanism was implemented to limit recursive bottom up climbing when one of these cells is reached this approach eliminates scenarios where any sub area is traversed more than once it is worth noting that the execution time of this algorithm depends on the total size of the watershed areas marked in the input data the recursive procedure only moves within these regions ignoring the cells outside it is also worth emphasizing that in practical applications this class of algorithms can lead to memory problems such as stack overflows in this paper these issues are not discussed in detail as this implementation is treated purely as a simple benchmark for performance measurements 2 4 2 flow path tracing algorithm gpu this implementation is based on the concept described in eränen et al 2014 in this algorithm each raster cell is assigned its own individual thread on the gpu device the main idea is to follow the flow path of each cell individually until a specified stream section or the dem boundary is reached depending on where the cell s flow path ends a corresponding watershed label is assigned to it the algorithm begins with the preparation and transfer of input data flow direction and outlet cell markers to the gpu memory there a working array is prepared where the outlet cells are initialized with their watershed labels initially all other cells contain the none value the goal of the algorithm is to assign each cell to the correct watershed and update its label the main part of this implementation is the path tracing kernel based on an inner loop the procedure is performed for each cell that initially does not have a watershed label assigned to it each thread starts by reading the flow direction value of its cell and then follows it to its downstream neighbor this step is repeated until a given thread reaches the outlet point marked with no flow direction value or crosses the dem boundary if the flow path ends at one of the outlet points the watershed label is copied from there to the thread s starting cell when this stage is completed the array can be transferred back to the host memory it should be highlighted that this idea involves different threads doing the same work multiple times a single cell in the flow path will be repeatedly traversed by different threads starting from its upstream cells while the parallel nature of gpu devices accelerates processing this way of organizing work is a significant drawback of this algorithm as in the flow path reduction algorithm data reshaping and manipulation steps on the host side both at the first and the last stage were parallelized using openmp 2 4 3 label pulling algorithm gpu another reference implementation is inspired by the concepts presented in mower 1994 and sit et al 2019 again each cell in the raster is assigned its own individual thread on the gpu device the main idea is to use the flow direction of a cell to locate its downstream neighbor and then iteratively reach for the neighbor s watershed label when a valid label becomes available it is copied pulled up to the thread s cell and consequently becomes available to other threads the algorithm starts by adjusting the input data format and transferring it to the gpu again data manipulation on the host side is performed in parallel there the flow direction is used to calculate and store the downstream neighbor index for each cell next a working array is prepared where the outlet cells are initialized with the corresponding labels and all other cells are initially set to none the goal of the algorithm is to propagate the labels from these outlet points gradually updating successive upstream cells the main loop of this algorithm iteratively calls the gpu kernel each thread reads its own cell first if the cell does not have a valid other than none label yet the label of its downstream neighbor is read if this label is found to be valid it is copied into the current cell the kernel is iteratively invoked as long as at least one thread reports that a modification has been made it should be emphasized that although many threads work in parallel here only a small fraction of all operations are meaningful in each iteration a given thread reads the label from its cell and reaches its downstream neighbor if needed of all these cycles only one results in an actual modification of that thread s cell in all other iterations the thread either does not yet have a valid label from its neighbor or has already copied it the number of kernel calls needed to produce a complete result is directly linearly related to the length of the flow path with the highest cell count 3 performance measurements 3 1 data the largest dataset used in this work was a dem with over two billion cells originating from publicly available resources of the head office of geodesy and cartography gugik the data covered a part of south eastern poland one meter resolution pl 1992 coordinate system the dem underwent a depression filling procedure to ensure its hydrological correctness more than 90 of all cells in this dataset belong to a single watershed area this dem was used to prepare flow direction rasters later used as input data for the algorithms the largest raster was generated directly from the entire dem next the dataset was scaled down to prepare smaller test cases with approximately the same internal characteristics covering the same area of land but at different scales a total of 30 input datasets were prepared one with original size and 29 scaled down with the number of cells increasing linearly between approximately 67 5 million and 2 billion this approach was adopted to make it easier to examine the relationship between data size and algorithm performance while minimizing other differences in data characteristics 3 2 testing procedure the direct input for all tested algorithms were flow direction rasters loaded from previously prepared files before starting the actual tests their correctness was verified before the measurements each algorithm passed through multiple series of automated tests to confirm that its implementation was correct various scenarios including specific corner cases different data sizes and multithreading configurations were taken into account the performance of the algorithms was measured by repeatedly running each one on every dataset and recording execution times in each test the measurement application was restarted to reflect a realistic use case and eliminate the potential distortion of execution times related to the cache memory every test included loading a flow direction file starting time measurement executing a given algorithm and stopping the measurement all operations needed to produce the final result performed after successfully loading the flow direction file into memory were treated as part of the algorithm and included in the time measurements additionally the generated result was verified after each execution all tests were performed on a machine equipped with two intel xeon e5 2670 v3 processors 24 cores in total 128 gb ram and nvidia a100 tensor core gpu in 40 gb memory version the computer was running under almalinux 8 4 the source code was compiled with the nvcc compiler from the cuda toolkit v11 2 code optimization o3 flag and openmp support were enabled for gpu computation the maximum available thread block size 1024 was used it was assumed that the device is initialized and synchronized the code sections executed in parallel on the host had all 24 cores available as part of the performance comparison each tested algorithm was executed ten times on every input dataset the task was to delineate the single largest watershed area in the dataset a single outlet cell was marked 4 results 4 1 performance comparison all results generated in each test passed the automated validation the time measurements clearly show that the flow path reduction algorithm performed by far the best in all cases both implementations of this approach achieved significantly shorter execution times than all the other tested algorithms the single buffer version of the algorithm achieved execution times shorter by over 18 on average compared to the back buffer implementation this result is not surprising considering the memory access patterns and differences in the number of iterations needed as described earlier averaging across all datasets the recursive implementation took approximately 33 times longer to generate the result than the single buffer flow path reduction algorithm taking the sequential recursive approach as a straightforward reference solution this can be considered a significant improvement the flow path reduction algorithm also proved to be significantly more efficient than the gpu based reference solutions in general the measured execution times varied by two orders of magnitude across all datasets the flow path tracing algorithm took on average as much as 119 times longer to execute than the single buffer implementation for the label pulling algorithm the average ratio was even greater reaching 483 times in both cases the difference becomes more significant as the data size increases it is worth emphasizing that the two gpu based reference algorithms turned out to be actually less efficient than simple sequential recursion as mentioned earlier much of the computation performed by parallel versions of these techniques is in fact redundant which may help explain these results the label pulling algorithm achieved by far the longest execution times of all tested implementations although these measurements are not directly comparable to those presented in sit et al 2019 the results appear to be somewhat in line with the fact that the gpu based label pulling technique tested there achieved longer execution times than some sequential implementations table 1 shows the average execution times for selected datasets fig 1 presents a visual comparison of the average execution times for all datasets results of the back buffer version are not present here as they were indistinguishable from the single buffer version at this scale detailed results of all measurements are available in the public repository 4 2 further analysis after the performance comparison additional tests and measurements were performed to further investigate the properties of the flow path reduction algorithm in the single buffer version in particular it was examined which stages of the algorithm require the most computational time a series of tests was performed using the largest dataset to measure the execution time of each stage individually although the main loop of the algorithm repeatedly invoking the gpu kernel is the most time consuming step it turned out to account for only 28 8 of the total execution time on average this may seem surprising but it can be explained by the fact that this key operation is effectively parallelized on the gpu and is performed relatively quickly compared to the other stages data transfers accounted for a large part of the execution time transferring input data to the gpu and retrieving the results back to the host added up to 34 5 of the total time on average also data reshaping operations despite being parallelized on the host side added up to a significant 33 1 table 2 presents the averaged measurements of all individual stages another series of tests was carried out to verify whether the algorithm is able to delineate multiple watersheds simultaneously without the need for additional computational time measurements were performed using the largest dataset up to eight watersheds were delineated simultaneously table 3 shows the average execution times there were no significant deviations from the time needed to delineate a single watershed using the same dataset 5 conclusions as part of this work the watershed delineation algorithms available in the literature were reviewed issues related to their performance were identified indicating a need for improvement a new algorithm was developed and presented proposing an approach that effectively uses the massive parallelism of gpu devices to rapidly delineate watershed areas its performance was measured and compared with reference algorithms based on concepts existing in the literature the results show that the proposed approach allows for a significant reduction in the computational time needed to perform the watershed delineation compared to other tested algorithms as noted in haag et al 2018 the existing techniques are insufficient to delineate the watershed boundaries on the fly the author believes that this work can help fill this gap allowing for a fast delineation of watersheds directly from basic flow direction data even on larger datasets the author decided not to use existing gis software packages as references in performance comparisons instead focusing on parallel algorithms available in the literature although accurate measurements were not carried out some early tests showed that it is not unusual for popular gis platforms to take several orders of magnitude longer to perform similar operations on a machine with the same specification it seems clear that there is still room for significant improvement it is worth emphasizing that the solution presented in this paper makes it possible to delineate multiple watersheds simultaneously as well as use multiple outlet cells for the same watershed with practically zero additional computational costs this can be considered a significant advantage of this method especially in the context of the cited literature in fact delineating a single watershed area could be considered a special case of the more general task discussed here an implementation focusing specifically on this scenario could possibly reduce the computation time of some stages even further while the techniques and ideas presented in this paper were designed for square grid dems they seem to be easily applicable to other types of data as well this may prove important as recent studies focused on hexagonal grids have shown promising results liao et al 2020 in addition although the presented concepts were developed specifically for watershed delineation perhaps they could also be adapted to other similarly structured modeling issues software availability software watershed delineation algorithms description source code of all algorithms developed tested and presented as part of this work including a simple measurement application developer bartłomiej kotyra contact address bartlomiej kotyra mail umcs pl language c 11 cuda openmp libraries required gdal geospatial data abstraction library availability freely available at https github com bkotyra watershed delineation gpu declaration of competing interest the author declares that he has no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper this research did not receive any specific grant from funding agencies in the public commercial or not for profit sectors 
25472,topographic data are increasingly important to environmental models as fine scale resolution wide coverage data sets become available scale is an important consideration for predictive model quality recent advances in multiscale terrain analysis led to scaling techniques that allow the scale at which a topographic parameter is represented to vary spatially this research compared predictive soil model performance across feature sets generated with different scaling strategies including multiple heterogeneous strategies common feature selection algorithms applied to homogeneously scaled data and unscaled data model performance was assessed for accuracy and uncertainty the results showed that unscaled data performed worse in all circumstances compared to multiscale feature sets overall heterogeneous and homogeneous feature sets did not differ substantially in accuracy prediction uncertainty or error however one scaling strategy exploited the flexibility of heterogeneous scaling to consistently perform better than other feature sets for most soil properties in terms of accuracy and consistently ranked among the least uncertain and least error prone up to a 0 080 increase in accuracy with a corresponding 0 017 decrease in prediction uncertainty and 0 011 decrease in error relative to the second best method in the case of the proportion of clay modelled at 5 15 cm depth this was achieved by decoupling the definition of process scales from analytical parameterization allowing the optimization to occur within broadly defined process scales this research demonstrates how to exploit heterogeneous scaling of topographic attributes to improve model performance keywords multiscale geomorphometry digital soil mapping machine learning data mining 1 introduction soils are highly integrated into many of earth s systems acting as an interface between the lithosphere atmosphere and biosphere rodrigo comino et al 2020 digital soil mapping dsm is a soil science field that uses numerical modelling and soil observations to predict the spatial distribution of soil properties zhang et al 2017 dsm often uses the scorpan model a generalization of the soil formation model proposed by jenny 1941 to define the relevant environmental factors for spatially distributed soil property predictions mcbratney et al 2003 according to the scorpan model a soil class or attribute at a point is predicted using previously measured soil information climatic properties organisms topography parent material age and spatial position data characterizing these factors are known as environmental covariates some factors such as parent material are relatively challenging to characterize in detail in contrast advances in remote sensing technologies such as light detection and ranging lidar and multispectral imaging makes the collection of spatially dense topographic and spectral information with wide coverage feasible mcbratney et al 2003 as a result topographic and spectral data form the basis of dsm in practice deumlich et al 2010 zhang et al 2017 soils with similar characteristics tend to occupy similar topographic settings daniels and hammer 1992 topography directly and indirectly influences soil properties for example slope orientation influences solar insolation which in turn affects organic activity ironside et al 2018 and snow melt kumar et al 2013 topographic data most commonly in the form of a regular raster digital elevation model dem is analyzed to interrogate the geometric and topological properties of the landscape collectively known as land surface parameters lsp olaya 2009 the discrete nature of the elevation sampling and data structures e g the sample spacing parameter spatial resolution embed scale information in the data by defining the distance at which spatial variation can be resolved goodchild 2011 these scale properties affect subsequent analyses such as the derivation of lsps resulting in well documented scale dependency e g deng et al 2007 grohmann 2015 schmidt and andrew 2005 sørensen and seibert 2007 that is known to strongly affect dsm results e g behrens et al 2010 2014 möller et al 2008 möller and volk 2015 sun et al 2017 behrens et al 2014 and möller and volk 2015 both recognized that the landscape characteristics interact with soil formation processes at specific scales and that dsm results are improved when these data are appropriately matched thus the prevalence of fine resolution topographic data simultaneously allows the generation of dsm products with unprecedented detail and exaggerates errors due to scale process mismatch goodchild 2011 the most suitable scale for a given application is rarely the default scale deng et al 2007 sørensen and seibert 2007 wu et al 2007 this prompted widespread research integrating multiscale terrain analysis into dsm practices e g araújo et al 2021 deumlich et al 2010 sun et al 2017 early work by möller et al 2008 integrated multiscale hierarchically segmented terrain objects with a classification system based on soil genesis and transport meanwhile behrens et al 2010 demonstrated how the integration of feature selection techniques to condense feature space can improve model performance by eliminating redundant information from many highly correlated scales this work has been continued through the introduction of novel scaling methods behrens et al 2014 2018 and further investigations on the impact of multiscale topographic feature selection in dsm miller et al 2015 these developments support the consensus that multiple scales provide quantitative rigour to the identification of suitable scales for analysis and that the quantity of data becomes problematic with many scales especially at finer resolution using an excessive number of input dimensions in modelling is obviously inefficient and moreover is known to negatively affect model performance this is due to the hughes phenomenon or the curse of dimensionality which describes the loss of statistical confidence as training data become sparse and unable to characterize patterns in complex feature spaces belgiu and drăgut 2016 maxwell et al 2018 most lsp scaling methods analyze a single scale across space iteratively modulating a scale parameter each scale is spatially homogeneous i e the same scale is measured everywhere and represents a potential input dimension often resulting large highly autocorrelated and redundant datasets behrens et al 2010 sørensen and seibert 2007 thus scale selection serves two purposes it reduces scale mismatch error and reduces the number and redundancy of input dimensions available to a model this is generally viewed as an optimization problem with two basic strategies wrapper and filter guyon and elisseeff 2003 wrapper strategies such as recursive feature elimination rfe e g jeong et al 2017 search the feature space to maximize a model performance metric on subsets of predictor variables john et al 1994 filter strategies like principal component analysis e g odgers et al 2011 and variance inflation factor vif methods seek to remove redundant information prior to modelling the fundamental difference between wrapper and filter strategies are that filter strategies function independently of the dependent variable in brief the multiscale terrain analysis methods proposed so far in dsm produce a series of scaled representations of each lsp the number of topographic features is the number of scales multiplied by the number of lsps requiring feature selection techniques to effectively manage the rapidly growing feature space spatially heterogeneous scale representation of lsps e g lindsay et al 2015 lindsay et al 2019 newman et al 2018 newman et al in press may represent an alternative these methods act as a spatially variable filter strategy that analyzes a range of scales retaining the characteristic scale at each point in space heterogeneous scale representation allows each grid cell to evaluate at which scale a lsp is most strongly expressed given its unique topographic setting and is therefore a locally adaptive scale optimization this differs from existing methods in that the scale selection occurs independently in the spatial dimensions rather than selection acting on spatially homogeneous raster layers furthermore heterogeneous methods provide the spatial distribution of optimal i e selected scales providing a unique opportunity to explore the utility of this information the purpose of this research is to evaluate the spatially heterogeneous scaling strategy compared to existing methods through the application of dsm specifically this research seeks to answer the following questions 1 how do heterogeneously scaled lsps affect model performance compared to spatially homogeneous scaling 2 how does the addition of optimal scale information affect model performance these questions will be answered by comparing model performance metrics for several soil properties depths and scaling methods 2 methods 2 1 study site and data the study site covers approximately 2800 km2 surrounding the city of ottawa canada fig 1 this area is part of the lake simcoe rideau ecoregion and has a mild and humid climate crins et al 2009 the bedrock is primarily dolomite and limestone overlain by thick glacial deposits expressed as undulating topography crins et al 2009 the dominant soil classes are orthic humic gleysols and orthic melanic brunisols according to the canadian system of soil classification soil classification working group 1998 much of the land surface is either urbanized or agricultural the topography of the study area is complex and expresses fluvial and glacial processes fluvial channels at multiple spatial scales converge on the ottawa river and a drumlin field is in the lower right corner of the site the soil profile data for the ottawa soil survey project were collected over three field seasons from 2016 to 2018 and made available by the ontario ministry of agriculture food and rural affairs omafra conditioned latin hypercube sampling minasny and mcbratney 2006 was used to identify optimal sampling locations some of which were subsequently modified to more opportunistic locations depending on accessibility a total of 1633 soil profiles were described according to pedological horizon soil classification working group 1998 and samples were submitted for full laboratory analysis the dem was generated from two lidar projects covering the ottawa city region the first project collected data in the fall of 2012 airborne imaging 2013 and the second project collected data during the spring of 2014 airborne imaging 2015 the vertical accuracy of both projects did not exceed 0 6 m at 95th percentile across all land cover types the 10 m resolution bare earth dem was generated by omafra from the classified point cloud 2 2 covariate generation nine lsps were derived from the input dem and used to model soil properties the gaussian scale space scaling method described in newman et al 2022 was used to characterize anisotropy of topographic position difference from mean elevation a measure of local topographic position and local derivatives slope eastness northness mean curvature profile curvature and tangential curvature this algorithm outputs a raster of optimal lsp measurements a raster of scale standardized lsp measurement i e lsp measurements standardized according to the population of measurements at a given scale and a raster of the scale at which the optimal lsp measurement was taken the multi scale standard deviation of normals method described in lindsay et al 2019 was used to characterize surface roughness as this algorithm does not output a scale standardized lsp raster a z score transform was applied to the optimal lsp output after the initial computation both algorithms achieve scale optimization internally by maximizing a lsp comparison function across a sequence of scales on a per cell basis and both algorithms are available in the open source software whiteboxtools lindsay 2022 as the purpose of this experiment is to evaluate the differences between topographic scaling approaches non topographic covariates were excluded 2 3 covariate scaling two approaches to scaling were used to generate each lsp using the algorithms referenced in section 2 2 first homogeneously scaled lsps were generated in a sequence of 15 scales starting at 30 m adding 100 m each iteration i e 30 m 130 m 230 m 1430 m a raster containing lsp values was generated for each scale thus representing the lsp at a spatially homogeneous scale second heterogeneously scaled lsps were generated using the multi scale optimization built in to both algorithms the same scale sequence was reproduced for the heterogeneously scaled data however only a single raster is required to represent the entire scale range as a result of the optimization a variation of heterogeneous scaling was implemented to analyze broadly defined process based scale ranges local and broad scales were analyzed heterogeneously using eight scales evenly distributed between 30 m and 430 m and seven scales between 1030 m and 1430 m respectively this maintains 15 total scales while differentiating between the two scale information classes by increasing scale sampling density in the local and broad ranges to the exclusion of those between 430 and 1030 these data were organized into sets of input variables for the purpose of comparison the naive set represents lsps without multiscale consideration using only the smallest scale of each homogeneous lsp i e those generated with a 3 3 window or equivalent sigma value this represents the scenario in which scaling effects are ignored and lsps are derived at the initial 10 m resolution of the dem the 135 homogeneously scaled rasters 9 covariates 15 scales are too numerous for direct modelling and required feature selection two feature selection strategies were used to identify optimal scales for each lsp based on the correlations between either the dependent and independent variables or between independent variables the top one and top two scales with the strongest correlation coefficients with the dependent soil property were selected for each lsp to create the hom t1 and hom t2 sets this wrapper strategy is similar to the method used by dornik et al 2022 to identify optimal scales for multiple lsps alternatively vif is a filter strategy used to measure the severity of multicollinearity between input features i e independent variables and to iteratively remove the most correlated features dormann et al 2013 this vif feature selection method was applied to each lsp to create the hom vif set the application of each feature selection independently to each lsp ensures equal representation rather than the strongest predictors comprising a majority of the input data without the equal representation condition the homogeneous data sets can leverage the larger number of input variables to retain many scales of a single lsp with strong predictive power to the exclusion of other lsps while the homogeneous sets are limited to one or two per lsp the heterogeneous sets used either scale standardized lsps het z or the combination of the scale standardized lsps and optimal scale rasters het zg both of which are output rasters from the algorithms described in section 2 2 finally the scale standardized output rasters representing the conceptually defined local and broad scale ranges for each lsp i e 2 multiscale rasters for each of the 9 lsps composed the het c feature set note that because of the internal scale selection occurring during the generation of the heterogeneous sets condenses multiscale lsp measurements to a single raster with spatially varying scales of measurement no feature selection was required table 1 summarizes these sets of input variables 2 4 soil property modelling scaling strategies were compared using the performance metrics of quantile regression forest model qrf meinshausen 2006 predictions of continuous soil properties qrf differs from other regression forests by retaining predictions across the individual trees in the forest allowing the distribution of within model predictions to be queried three performance metrics were collected to assess the feature sets lin s concordance correlation coefficient ccc was used to measure the performance of the model accuracy and precision hereafter referred to simply as model accuracy ccc differs from pearson s correlation coefficient in that it considers accuracy as well as precision by measuring residuals relative to the identity line i e the 1 1 line instead of the regression line lin 1989 the average of the differences between the 5th and 95th quantiles i e the mean 90 percent prediction intervals mpi90 for the validation data was recorded as a measure of prediction uncertainty kasraei et al 2021 poggio et al 2021 vaysse and lagacherie 2017 using the qrf model finally rmse was used to measure model error the hypotheses were tested by comparing differences in ccc mpi90 and rmse by soil property to demonstrate how model performance was impacted by scales type and scaling strategy modelled soil properties consisted of cation exchange capacity cec ph soil organic carbon soc and soil particle size fractions i e sand silt and clay table 2 see appendix c for summary statistics the horizon data were interpolated to globalsoilmap net standard depth intervals arrouays et al 2014 up to 1 m i e 0 0 0 05 0 05 0 15 0 15 0 30 0 30 0 60 0 60 1 m using an equal area quadratic spline bishop et al 1999 qrf models were generated for each feature set for each soil property for each depth and were validated using repeated k fold cross validation 10 folds 5 repeats particle size fractions were transformed using isometric log ratios to preserve a sum of unity prior to modelling zhang et al 2020 this addresses the possibility that compositional data may not sum to unity when modelled individually amirian chakan et al 2019 since the isometric log ratio transforms data to k 1 dimensions only two models were trained to represent all three texture fractions once trained performance metrics were computed by inversing the validation data from both models back to the original sand silt and clay dimensions because of this mpi90 for sand silt and clay used quantile regression models koenker and d orey 1987 to estimate the 5th and 95th quantiles instead of qrf kasraei et al 2021 the quantile regression models differ from qrf in that the regression models estimate the conditional distribution while qrf retains the conditional distribution of the forest predictions evaluation of model performance between different covariates was not undertaken because the differences in methodology for computing mpi90 all modelling was conducted using r language with relevant packages see table 3 3 results model performance in terms of accuracy ccc prediction uncertainty mpi90 and error rmse were aggregated by scale type see table 1 and summarized overall unscaled models exhibited much lower accuracy than heterogeneous and homogeneous scaling strategies for all soil properties except soc fig 2 c where all strategies performed poorly heterogeneous and homogeneous strategies exhibited similar performance with homogeneous strategies performing slightly better on average fig 2 a f all strategies performed similarly in terms of prediction uncertainty and error with unscaled data being higher in both measures for all soil properties except soc rather than aggregating data to explore the first hypothesis directly the experimental design is better suited to compare feature sets instead of scale type m and s are used to denote the mean and standard deviation of ccc measurements aggregated across all soil properties and depths n 30 het c demonstrated consistently higher model accuracy compared to the other feature sets fig 3 exhibiting the highest average accuracy m 0 299 s 0 160 while naive was consistently lowest model accuracy fig 3 with the lowest average accuracy m 0 125 s 0 08 hom t2 and hom vif demonstrated similar accuracy on average m 0 260 s 0 140 and m 0 0255 s 0 140 respectively while het zg tended to have comparable accuracy to hom t2 and hom vif fig 3 it had lower accuracy on average m 0 237 s 0 140 het z tended to demonstrate lower accuracy than hom t1 and was the least performant multiscale feature set on average m 0 221 s 0 136 and m 0 241 s 0 126 respectively table 4 presents a summary of averaged ranked performance for each feature set for all soil properties and depths tested naive scaling demonstrated the highest prediction uncertainty mpi90 and error rmse for all soil variables except soc het c consistently exhibited lower prediction uncertainty than other feature sets with followed closely by hom t2 and hom vif fig 4 het c also consistently exhibited the lowest error followed by hom vif and hom t2 fig 5 het zg tended to higher prediction uncertainty and error compared to the other n 18 feature sets het z and hom t1 demonstrated similar performance in terms of prediction uncertainty and error note that averages were not taken for prediction uncertainty and error due the different units for each soil property model accuracy error and prediction uncertainty by soil depth for het z and het zg was also used to examine the impact of the addition of scale information on model accuracy het zg consistently demonstrated slightly higher accuracy than het z for all soil properties fig 3 het zg also exhibited slightly lower prediction uncertainty than het z for all soil properties other than ph fig 4 b and slightly lower error for all soil properties fig 5 examples of spatial distributions of predicted cec at the 0 5 cm depth interval and the width of the prediction interval between the 5th and 95th quantiles are presented in figs 5 and 6 respectively the distributions of predicted soil properties are generally similar as they are entirely predicted from topographic information however naive and hom vif fig 6 b and h both exhibited low spatial auto correlation expressed as salt and pepper noise due to the reliance on very short scale lsp representation the additional feature numbers tended to promote spatial contiguity in clearly visible topographic features such as a valley bottom comparing hom t1 and hom t2 fig 6 d and f respectively or het z and het zg fig 6 c and e respectively demonstrate this clearly het c fig 6 g demonstrates the strongest relationship with the topography fig 6 a the spatial distribution of uncertainty is strongly correlated with the predictions and follows the same patterns described above fig 7 additional spatial distributions representing examples of particularly a particularly high accuracy model clay at 60 100 cm soil depth and a relatively low accuracy model soc at 0 5 cm soil depth are found in appendices a and b respectively 4 discussion feature sets generated using different scaling methods and scale optimization treatments were evaluated through the modelling of several soil properties model performance was measured in terms of accuracy using ccc and uncertainty using mpi90 unscaled data performed worse in all respects the severity of the poor performance e g figs 2 5 corroborates previous findings documenting the strong positive impact of multiscale topographic analysis on dsm applications e g behrens et al 2010 möller and volk 2015 in general the heterogeneous and homogeneous methods performed similarly in terms of accuracy prediction uncertainty and error but the rankings varied heavily by soil property though heterogeneous and homogeneous scaling demonstrated comparable performance in aggregate examining feature set performance revealed that het c was consistently the most accurate data set was among the least uncertain and often exhibited the lowest error figs 3 5 respectively because het z and het zg failed to demonstrate notable increases to performance over the homogeneously scaled data sets the conceptually defined scale ranges are likely responsible for performance increases rather than heterogeneous scale representation alone this aspect of heterogeneous scaling was first discussed by lindsay et al 2015 who used three scale ranges to explore multiple optimal topographic representations differentiated by estimated process scales this was elaborated on in later work demonstrating how multiple optimal scales are better represented using multiple scale classes lindsay et al 2019 the relative performance of het c demonstrates how heterogeneous scaling decouples the definition of process scales operating in the landscape from the detailed analytical parametrization rather than focusing on the definition of or search for optimal parameters i e homogeneous solutions optimal representation of an lsp is based on the local topographic context within the bounds of broadly defined process scales this offers more scientific utility to the modelling exercise because the independent variable is self represented rather than chosen for statistical or a performance maximization function there is a circular logic to selecting an independent variable based on the relationship with the dependent variable even if performance is improved model results also become more intuitive to interpret by removing the requirement to interpret the models use of a particular variable or parameter set e g justify higher variable importance for a particular scale these results demonstrate how heterogeneous scaling can offer the flexibility to adapt to complex processes and are able to characterize scale relationships in natural systems translating to stronger models the heterogeneous scaling methods produce scale information during the internal optimization process het z and het zg differ only by the inclusion of scale information features comparing these two methods revealed that het zg provided small but consistent increases in model accuracy and lowered prediction uncertainty and error by a small amount figs 3 5 these minor differences in performance are more likely due to the additional features i e n 9or n 18 increasing the degrees of freedom and variance of the data set rather than describing performance of the scale information the additional performance from the additional features was also demonstrated by the relative performance gains of hom t2 relative to hom t1 most obvious in fig 3 while the results suggest the inclusion of scale information may improve model performance the effect of the added number of features was not accounted for in the experimental design further research is required to properly evaluate the utility of scale information the relationships between topographic covariates and soil properties were isolated by the experimental design the clearest result was the universally poor predictive power of topographic covariates on soc kasraei et al 2021 also used qrf to model soc for the same study area fig 1 using both topographic and non topographic input data reporting ccc values of 0 61 and 0 53 at 0 5 cm and 30 60 cm depths respectively the difference in ccc between kasraei et al 2021 and the results presented above suggest a weak relationship between topographic covariates soc compared to other environmental covariates the soil science literature offers conflicting evidence on the strength of the relationship between soc and topography a review by wiesmeier et al 2019 contrasts the theoretical basis with the mixed empirical results while another review by lamichhane et al 2019 describes a slightly stronger relationship however both reviews highlighted the potential importance of scale wiesmeier et al 2019 concluded that the relevance of topography is highest at the hillslope scale due to the low variance in soil properties and less relevant at larger scales when soc stocks are influenced by many more processes however the consistently poor model performance presented above does not support this conclusion especially due to the fact that hom t1 and hom t2 specifically identify scales within the experimental range local to hillslope scales with strong performance a possible explanation for this disparity is the exclusion of topographic wetness index identified by wiesmeier et al 2019 as a particularly promising factor along with all other non local lsps from this study expanding the experimental framework to include non local lsps along with larger scale study sites may provide a more reliable assessment of the importance of scale on the relationship between lsps and soc at regional scales or larger 5 conclusions this research sought to explore the impact of recent developments in spatially heterogeneous scale representation of topographic covariates using an applied dsm analysis qrf models of soil cec ph soc and texture fractions at five standard depths were generated to evaluate differences in performance between input feature sets differentiated by scaling strategy in general the results determined that heterogeneous scaling strategies did not offer a consistent improvement to model accuracy error or prediction uncertainty however the results reiterated the value of multiscale methods over unscaled topographic covariates due to the consistently poor performance of the unscaled feature set comparisons of individual scaling strategies revealed het c as the dominant strategy for all tested soil properties except soc it was concluded by process of elimination that the favourable performance was driven by the ability to define broad process based scale ranges from which the heterogeneous scale optimization selects significant scales based on the local topographic context while it was not tested explicitly the relative impact of topographic scaling methods is expected to be generalizable to other study sites because the purpose of multiscale topographic analysis is to characterize the variability in topographic structure independently of subsequent modelling methodology and input data so while different training data and geographic setting will likely impact model performance in absolute terms the benefits of spatially heterogeneous multiscale topographic characterization remains relatively the same future work may evaluate how well the stronger performance of heterogeneous scale representation affect environmental model outcomes in standard workflows i e including non topographic covariates and how well the results translate to other study sites and different sample sizes the inclusion of optimal scale information was also examined and was found to slightly improve model accuracy and slightly lower model error and prediction uncertainty however the differences were minor and inconclusive given the increases in model complexity due to the addition of information the remarkably poor performance of soc models contributes to the discussion on topographic drivers of soc where the scale of topographic covariates is discussed as a confounding variable responsible for mixed empirical results however the results presented suggest scale has a negligible impact on soc model performance at least for local lsps funding this research was funded by a grant provided by the natural sciences and engineering research council of canada nserc grant number 400317 author contributions conceptualization d r n and j b l data curation d r n and d d s formal analysis d r n funding acquisition j b l investigation d r n methodology d r n and d d s project administration j b l resources d r n software d r n and j b l supervision j m h c l d and j b l validation d r n visualization d r n writing original draft d r n writing review editing d d s j m h c l d and j b l declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper appendix a spatial distributions of clay predictions upper case and uncertainty lower case at 60 100 cm soil depth elevation a is given for reference heterogeneous feature sets are on the left het z c het zg e and het c g homogeneous feature sets are on the right naive b hom t1 d hom t2 f and hom vif h image 1 appendix b spatial distributions of soc predictions upper case and uncertainty lower case at 0 5 cm soil depth elevation a is given for reference heterogeneous feature sets are on the left het z c het zg e and het c g homogeneous feature sets are on the right naive b hom t1 d hom t2 f and hom vif h image 2 appendix c summary statistics of soil samples interpolated to globalsoilmap net standard depths soil property depth cm n minimum maximum mean standard deviation cec 0 5 1643 0 220 204 144 24 711 23 224 5 15 1636 0 269 199 406 23 237 22 178 15 30 1611 0 220 187 372 20 246 20 101 30 60 1538 0 220 172 251 16 955 17 333 60 100 1436 0 220 187 085 16 979 15 719 ph 0 5 1643 3 434 8 328 6 278 0 851 5 15 1636 3 544 8 321 6 331 0 835 15 30 1611 3 583 8 333 6 542 0 798 30 60 1538 3 730 8 534 6 923 0 752 60 100 1436 4 048 8 551 7 226 0 710 soc 0 5 1643 0 010 48 100 6 538 9 532 5 15 1636 0 010 48 100 4 698 7 204 15 30 1611 0 010 48 100 3 013 6 325 30 60 1538 0 010 47 947 1 541 5 372 60 100 1436 0 010 46 444 0 860 4 287 sand 0 5 1627 0 015 0 799 0 339 0 154 5 15 1597 0 016 0 859 0 331 0 158 15 30 1525 0 005 0 796 0 316 0 163 30 60 1622 0 017 0 795 0 338 0 154 60 100 1424 0 008 0 867 0 319 0 160 silt 0 5 1627 0 011 0 831 0 258 0 160 5 15 1597 0 007 0 831 0 265 0 185 15 30 1525 0 006 0 842 0 294 0 224 30 60 1622 0 011 0 831 0 258 0 164 60 100 1424 0 006 0 870 0 351 0 245 clay 0 5 1627 0 008 0 954 0 403 0 260 5 15 1597 0 005 0 968 0 405 0 282 15 30 1525 0 003 0 973 0 390 0 306 30 60 1622 0 008 0 956 0 404 0 265 60 100 1424 0 003 0 984 0 330 0 308 
25472,topographic data are increasingly important to environmental models as fine scale resolution wide coverage data sets become available scale is an important consideration for predictive model quality recent advances in multiscale terrain analysis led to scaling techniques that allow the scale at which a topographic parameter is represented to vary spatially this research compared predictive soil model performance across feature sets generated with different scaling strategies including multiple heterogeneous strategies common feature selection algorithms applied to homogeneously scaled data and unscaled data model performance was assessed for accuracy and uncertainty the results showed that unscaled data performed worse in all circumstances compared to multiscale feature sets overall heterogeneous and homogeneous feature sets did not differ substantially in accuracy prediction uncertainty or error however one scaling strategy exploited the flexibility of heterogeneous scaling to consistently perform better than other feature sets for most soil properties in terms of accuracy and consistently ranked among the least uncertain and least error prone up to a 0 080 increase in accuracy with a corresponding 0 017 decrease in prediction uncertainty and 0 011 decrease in error relative to the second best method in the case of the proportion of clay modelled at 5 15 cm depth this was achieved by decoupling the definition of process scales from analytical parameterization allowing the optimization to occur within broadly defined process scales this research demonstrates how to exploit heterogeneous scaling of topographic attributes to improve model performance keywords multiscale geomorphometry digital soil mapping machine learning data mining 1 introduction soils are highly integrated into many of earth s systems acting as an interface between the lithosphere atmosphere and biosphere rodrigo comino et al 2020 digital soil mapping dsm is a soil science field that uses numerical modelling and soil observations to predict the spatial distribution of soil properties zhang et al 2017 dsm often uses the scorpan model a generalization of the soil formation model proposed by jenny 1941 to define the relevant environmental factors for spatially distributed soil property predictions mcbratney et al 2003 according to the scorpan model a soil class or attribute at a point is predicted using previously measured soil information climatic properties organisms topography parent material age and spatial position data characterizing these factors are known as environmental covariates some factors such as parent material are relatively challenging to characterize in detail in contrast advances in remote sensing technologies such as light detection and ranging lidar and multispectral imaging makes the collection of spatially dense topographic and spectral information with wide coverage feasible mcbratney et al 2003 as a result topographic and spectral data form the basis of dsm in practice deumlich et al 2010 zhang et al 2017 soils with similar characteristics tend to occupy similar topographic settings daniels and hammer 1992 topography directly and indirectly influences soil properties for example slope orientation influences solar insolation which in turn affects organic activity ironside et al 2018 and snow melt kumar et al 2013 topographic data most commonly in the form of a regular raster digital elevation model dem is analyzed to interrogate the geometric and topological properties of the landscape collectively known as land surface parameters lsp olaya 2009 the discrete nature of the elevation sampling and data structures e g the sample spacing parameter spatial resolution embed scale information in the data by defining the distance at which spatial variation can be resolved goodchild 2011 these scale properties affect subsequent analyses such as the derivation of lsps resulting in well documented scale dependency e g deng et al 2007 grohmann 2015 schmidt and andrew 2005 sørensen and seibert 2007 that is known to strongly affect dsm results e g behrens et al 2010 2014 möller et al 2008 möller and volk 2015 sun et al 2017 behrens et al 2014 and möller and volk 2015 both recognized that the landscape characteristics interact with soil formation processes at specific scales and that dsm results are improved when these data are appropriately matched thus the prevalence of fine resolution topographic data simultaneously allows the generation of dsm products with unprecedented detail and exaggerates errors due to scale process mismatch goodchild 2011 the most suitable scale for a given application is rarely the default scale deng et al 2007 sørensen and seibert 2007 wu et al 2007 this prompted widespread research integrating multiscale terrain analysis into dsm practices e g araújo et al 2021 deumlich et al 2010 sun et al 2017 early work by möller et al 2008 integrated multiscale hierarchically segmented terrain objects with a classification system based on soil genesis and transport meanwhile behrens et al 2010 demonstrated how the integration of feature selection techniques to condense feature space can improve model performance by eliminating redundant information from many highly correlated scales this work has been continued through the introduction of novel scaling methods behrens et al 2014 2018 and further investigations on the impact of multiscale topographic feature selection in dsm miller et al 2015 these developments support the consensus that multiple scales provide quantitative rigour to the identification of suitable scales for analysis and that the quantity of data becomes problematic with many scales especially at finer resolution using an excessive number of input dimensions in modelling is obviously inefficient and moreover is known to negatively affect model performance this is due to the hughes phenomenon or the curse of dimensionality which describes the loss of statistical confidence as training data become sparse and unable to characterize patterns in complex feature spaces belgiu and drăgut 2016 maxwell et al 2018 most lsp scaling methods analyze a single scale across space iteratively modulating a scale parameter each scale is spatially homogeneous i e the same scale is measured everywhere and represents a potential input dimension often resulting large highly autocorrelated and redundant datasets behrens et al 2010 sørensen and seibert 2007 thus scale selection serves two purposes it reduces scale mismatch error and reduces the number and redundancy of input dimensions available to a model this is generally viewed as an optimization problem with two basic strategies wrapper and filter guyon and elisseeff 2003 wrapper strategies such as recursive feature elimination rfe e g jeong et al 2017 search the feature space to maximize a model performance metric on subsets of predictor variables john et al 1994 filter strategies like principal component analysis e g odgers et al 2011 and variance inflation factor vif methods seek to remove redundant information prior to modelling the fundamental difference between wrapper and filter strategies are that filter strategies function independently of the dependent variable in brief the multiscale terrain analysis methods proposed so far in dsm produce a series of scaled representations of each lsp the number of topographic features is the number of scales multiplied by the number of lsps requiring feature selection techniques to effectively manage the rapidly growing feature space spatially heterogeneous scale representation of lsps e g lindsay et al 2015 lindsay et al 2019 newman et al 2018 newman et al in press may represent an alternative these methods act as a spatially variable filter strategy that analyzes a range of scales retaining the characteristic scale at each point in space heterogeneous scale representation allows each grid cell to evaluate at which scale a lsp is most strongly expressed given its unique topographic setting and is therefore a locally adaptive scale optimization this differs from existing methods in that the scale selection occurs independently in the spatial dimensions rather than selection acting on spatially homogeneous raster layers furthermore heterogeneous methods provide the spatial distribution of optimal i e selected scales providing a unique opportunity to explore the utility of this information the purpose of this research is to evaluate the spatially heterogeneous scaling strategy compared to existing methods through the application of dsm specifically this research seeks to answer the following questions 1 how do heterogeneously scaled lsps affect model performance compared to spatially homogeneous scaling 2 how does the addition of optimal scale information affect model performance these questions will be answered by comparing model performance metrics for several soil properties depths and scaling methods 2 methods 2 1 study site and data the study site covers approximately 2800 km2 surrounding the city of ottawa canada fig 1 this area is part of the lake simcoe rideau ecoregion and has a mild and humid climate crins et al 2009 the bedrock is primarily dolomite and limestone overlain by thick glacial deposits expressed as undulating topography crins et al 2009 the dominant soil classes are orthic humic gleysols and orthic melanic brunisols according to the canadian system of soil classification soil classification working group 1998 much of the land surface is either urbanized or agricultural the topography of the study area is complex and expresses fluvial and glacial processes fluvial channels at multiple spatial scales converge on the ottawa river and a drumlin field is in the lower right corner of the site the soil profile data for the ottawa soil survey project were collected over three field seasons from 2016 to 2018 and made available by the ontario ministry of agriculture food and rural affairs omafra conditioned latin hypercube sampling minasny and mcbratney 2006 was used to identify optimal sampling locations some of which were subsequently modified to more opportunistic locations depending on accessibility a total of 1633 soil profiles were described according to pedological horizon soil classification working group 1998 and samples were submitted for full laboratory analysis the dem was generated from two lidar projects covering the ottawa city region the first project collected data in the fall of 2012 airborne imaging 2013 and the second project collected data during the spring of 2014 airborne imaging 2015 the vertical accuracy of both projects did not exceed 0 6 m at 95th percentile across all land cover types the 10 m resolution bare earth dem was generated by omafra from the classified point cloud 2 2 covariate generation nine lsps were derived from the input dem and used to model soil properties the gaussian scale space scaling method described in newman et al 2022 was used to characterize anisotropy of topographic position difference from mean elevation a measure of local topographic position and local derivatives slope eastness northness mean curvature profile curvature and tangential curvature this algorithm outputs a raster of optimal lsp measurements a raster of scale standardized lsp measurement i e lsp measurements standardized according to the population of measurements at a given scale and a raster of the scale at which the optimal lsp measurement was taken the multi scale standard deviation of normals method described in lindsay et al 2019 was used to characterize surface roughness as this algorithm does not output a scale standardized lsp raster a z score transform was applied to the optimal lsp output after the initial computation both algorithms achieve scale optimization internally by maximizing a lsp comparison function across a sequence of scales on a per cell basis and both algorithms are available in the open source software whiteboxtools lindsay 2022 as the purpose of this experiment is to evaluate the differences between topographic scaling approaches non topographic covariates were excluded 2 3 covariate scaling two approaches to scaling were used to generate each lsp using the algorithms referenced in section 2 2 first homogeneously scaled lsps were generated in a sequence of 15 scales starting at 30 m adding 100 m each iteration i e 30 m 130 m 230 m 1430 m a raster containing lsp values was generated for each scale thus representing the lsp at a spatially homogeneous scale second heterogeneously scaled lsps were generated using the multi scale optimization built in to both algorithms the same scale sequence was reproduced for the heterogeneously scaled data however only a single raster is required to represent the entire scale range as a result of the optimization a variation of heterogeneous scaling was implemented to analyze broadly defined process based scale ranges local and broad scales were analyzed heterogeneously using eight scales evenly distributed between 30 m and 430 m and seven scales between 1030 m and 1430 m respectively this maintains 15 total scales while differentiating between the two scale information classes by increasing scale sampling density in the local and broad ranges to the exclusion of those between 430 and 1030 these data were organized into sets of input variables for the purpose of comparison the naive set represents lsps without multiscale consideration using only the smallest scale of each homogeneous lsp i e those generated with a 3 3 window or equivalent sigma value this represents the scenario in which scaling effects are ignored and lsps are derived at the initial 10 m resolution of the dem the 135 homogeneously scaled rasters 9 covariates 15 scales are too numerous for direct modelling and required feature selection two feature selection strategies were used to identify optimal scales for each lsp based on the correlations between either the dependent and independent variables or between independent variables the top one and top two scales with the strongest correlation coefficients with the dependent soil property were selected for each lsp to create the hom t1 and hom t2 sets this wrapper strategy is similar to the method used by dornik et al 2022 to identify optimal scales for multiple lsps alternatively vif is a filter strategy used to measure the severity of multicollinearity between input features i e independent variables and to iteratively remove the most correlated features dormann et al 2013 this vif feature selection method was applied to each lsp to create the hom vif set the application of each feature selection independently to each lsp ensures equal representation rather than the strongest predictors comprising a majority of the input data without the equal representation condition the homogeneous data sets can leverage the larger number of input variables to retain many scales of a single lsp with strong predictive power to the exclusion of other lsps while the homogeneous sets are limited to one or two per lsp the heterogeneous sets used either scale standardized lsps het z or the combination of the scale standardized lsps and optimal scale rasters het zg both of which are output rasters from the algorithms described in section 2 2 finally the scale standardized output rasters representing the conceptually defined local and broad scale ranges for each lsp i e 2 multiscale rasters for each of the 9 lsps composed the het c feature set note that because of the internal scale selection occurring during the generation of the heterogeneous sets condenses multiscale lsp measurements to a single raster with spatially varying scales of measurement no feature selection was required table 1 summarizes these sets of input variables 2 4 soil property modelling scaling strategies were compared using the performance metrics of quantile regression forest model qrf meinshausen 2006 predictions of continuous soil properties qrf differs from other regression forests by retaining predictions across the individual trees in the forest allowing the distribution of within model predictions to be queried three performance metrics were collected to assess the feature sets lin s concordance correlation coefficient ccc was used to measure the performance of the model accuracy and precision hereafter referred to simply as model accuracy ccc differs from pearson s correlation coefficient in that it considers accuracy as well as precision by measuring residuals relative to the identity line i e the 1 1 line instead of the regression line lin 1989 the average of the differences between the 5th and 95th quantiles i e the mean 90 percent prediction intervals mpi90 for the validation data was recorded as a measure of prediction uncertainty kasraei et al 2021 poggio et al 2021 vaysse and lagacherie 2017 using the qrf model finally rmse was used to measure model error the hypotheses were tested by comparing differences in ccc mpi90 and rmse by soil property to demonstrate how model performance was impacted by scales type and scaling strategy modelled soil properties consisted of cation exchange capacity cec ph soil organic carbon soc and soil particle size fractions i e sand silt and clay table 2 see appendix c for summary statistics the horizon data were interpolated to globalsoilmap net standard depth intervals arrouays et al 2014 up to 1 m i e 0 0 0 05 0 05 0 15 0 15 0 30 0 30 0 60 0 60 1 m using an equal area quadratic spline bishop et al 1999 qrf models were generated for each feature set for each soil property for each depth and were validated using repeated k fold cross validation 10 folds 5 repeats particle size fractions were transformed using isometric log ratios to preserve a sum of unity prior to modelling zhang et al 2020 this addresses the possibility that compositional data may not sum to unity when modelled individually amirian chakan et al 2019 since the isometric log ratio transforms data to k 1 dimensions only two models were trained to represent all three texture fractions once trained performance metrics were computed by inversing the validation data from both models back to the original sand silt and clay dimensions because of this mpi90 for sand silt and clay used quantile regression models koenker and d orey 1987 to estimate the 5th and 95th quantiles instead of qrf kasraei et al 2021 the quantile regression models differ from qrf in that the regression models estimate the conditional distribution while qrf retains the conditional distribution of the forest predictions evaluation of model performance between different covariates was not undertaken because the differences in methodology for computing mpi90 all modelling was conducted using r language with relevant packages see table 3 3 results model performance in terms of accuracy ccc prediction uncertainty mpi90 and error rmse were aggregated by scale type see table 1 and summarized overall unscaled models exhibited much lower accuracy than heterogeneous and homogeneous scaling strategies for all soil properties except soc fig 2 c where all strategies performed poorly heterogeneous and homogeneous strategies exhibited similar performance with homogeneous strategies performing slightly better on average fig 2 a f all strategies performed similarly in terms of prediction uncertainty and error with unscaled data being higher in both measures for all soil properties except soc rather than aggregating data to explore the first hypothesis directly the experimental design is better suited to compare feature sets instead of scale type m and s are used to denote the mean and standard deviation of ccc measurements aggregated across all soil properties and depths n 30 het c demonstrated consistently higher model accuracy compared to the other feature sets fig 3 exhibiting the highest average accuracy m 0 299 s 0 160 while naive was consistently lowest model accuracy fig 3 with the lowest average accuracy m 0 125 s 0 08 hom t2 and hom vif demonstrated similar accuracy on average m 0 260 s 0 140 and m 0 0255 s 0 140 respectively while het zg tended to have comparable accuracy to hom t2 and hom vif fig 3 it had lower accuracy on average m 0 237 s 0 140 het z tended to demonstrate lower accuracy than hom t1 and was the least performant multiscale feature set on average m 0 221 s 0 136 and m 0 241 s 0 126 respectively table 4 presents a summary of averaged ranked performance for each feature set for all soil properties and depths tested naive scaling demonstrated the highest prediction uncertainty mpi90 and error rmse for all soil variables except soc het c consistently exhibited lower prediction uncertainty than other feature sets with followed closely by hom t2 and hom vif fig 4 het c also consistently exhibited the lowest error followed by hom vif and hom t2 fig 5 het zg tended to higher prediction uncertainty and error compared to the other n 18 feature sets het z and hom t1 demonstrated similar performance in terms of prediction uncertainty and error note that averages were not taken for prediction uncertainty and error due the different units for each soil property model accuracy error and prediction uncertainty by soil depth for het z and het zg was also used to examine the impact of the addition of scale information on model accuracy het zg consistently demonstrated slightly higher accuracy than het z for all soil properties fig 3 het zg also exhibited slightly lower prediction uncertainty than het z for all soil properties other than ph fig 4 b and slightly lower error for all soil properties fig 5 examples of spatial distributions of predicted cec at the 0 5 cm depth interval and the width of the prediction interval between the 5th and 95th quantiles are presented in figs 5 and 6 respectively the distributions of predicted soil properties are generally similar as they are entirely predicted from topographic information however naive and hom vif fig 6 b and h both exhibited low spatial auto correlation expressed as salt and pepper noise due to the reliance on very short scale lsp representation the additional feature numbers tended to promote spatial contiguity in clearly visible topographic features such as a valley bottom comparing hom t1 and hom t2 fig 6 d and f respectively or het z and het zg fig 6 c and e respectively demonstrate this clearly het c fig 6 g demonstrates the strongest relationship with the topography fig 6 a the spatial distribution of uncertainty is strongly correlated with the predictions and follows the same patterns described above fig 7 additional spatial distributions representing examples of particularly a particularly high accuracy model clay at 60 100 cm soil depth and a relatively low accuracy model soc at 0 5 cm soil depth are found in appendices a and b respectively 4 discussion feature sets generated using different scaling methods and scale optimization treatments were evaluated through the modelling of several soil properties model performance was measured in terms of accuracy using ccc and uncertainty using mpi90 unscaled data performed worse in all respects the severity of the poor performance e g figs 2 5 corroborates previous findings documenting the strong positive impact of multiscale topographic analysis on dsm applications e g behrens et al 2010 möller and volk 2015 in general the heterogeneous and homogeneous methods performed similarly in terms of accuracy prediction uncertainty and error but the rankings varied heavily by soil property though heterogeneous and homogeneous scaling demonstrated comparable performance in aggregate examining feature set performance revealed that het c was consistently the most accurate data set was among the least uncertain and often exhibited the lowest error figs 3 5 respectively because het z and het zg failed to demonstrate notable increases to performance over the homogeneously scaled data sets the conceptually defined scale ranges are likely responsible for performance increases rather than heterogeneous scale representation alone this aspect of heterogeneous scaling was first discussed by lindsay et al 2015 who used three scale ranges to explore multiple optimal topographic representations differentiated by estimated process scales this was elaborated on in later work demonstrating how multiple optimal scales are better represented using multiple scale classes lindsay et al 2019 the relative performance of het c demonstrates how heterogeneous scaling decouples the definition of process scales operating in the landscape from the detailed analytical parametrization rather than focusing on the definition of or search for optimal parameters i e homogeneous solutions optimal representation of an lsp is based on the local topographic context within the bounds of broadly defined process scales this offers more scientific utility to the modelling exercise because the independent variable is self represented rather than chosen for statistical or a performance maximization function there is a circular logic to selecting an independent variable based on the relationship with the dependent variable even if performance is improved model results also become more intuitive to interpret by removing the requirement to interpret the models use of a particular variable or parameter set e g justify higher variable importance for a particular scale these results demonstrate how heterogeneous scaling can offer the flexibility to adapt to complex processes and are able to characterize scale relationships in natural systems translating to stronger models the heterogeneous scaling methods produce scale information during the internal optimization process het z and het zg differ only by the inclusion of scale information features comparing these two methods revealed that het zg provided small but consistent increases in model accuracy and lowered prediction uncertainty and error by a small amount figs 3 5 these minor differences in performance are more likely due to the additional features i e n 9or n 18 increasing the degrees of freedom and variance of the data set rather than describing performance of the scale information the additional performance from the additional features was also demonstrated by the relative performance gains of hom t2 relative to hom t1 most obvious in fig 3 while the results suggest the inclusion of scale information may improve model performance the effect of the added number of features was not accounted for in the experimental design further research is required to properly evaluate the utility of scale information the relationships between topographic covariates and soil properties were isolated by the experimental design the clearest result was the universally poor predictive power of topographic covariates on soc kasraei et al 2021 also used qrf to model soc for the same study area fig 1 using both topographic and non topographic input data reporting ccc values of 0 61 and 0 53 at 0 5 cm and 30 60 cm depths respectively the difference in ccc between kasraei et al 2021 and the results presented above suggest a weak relationship between topographic covariates soc compared to other environmental covariates the soil science literature offers conflicting evidence on the strength of the relationship between soc and topography a review by wiesmeier et al 2019 contrasts the theoretical basis with the mixed empirical results while another review by lamichhane et al 2019 describes a slightly stronger relationship however both reviews highlighted the potential importance of scale wiesmeier et al 2019 concluded that the relevance of topography is highest at the hillslope scale due to the low variance in soil properties and less relevant at larger scales when soc stocks are influenced by many more processes however the consistently poor model performance presented above does not support this conclusion especially due to the fact that hom t1 and hom t2 specifically identify scales within the experimental range local to hillslope scales with strong performance a possible explanation for this disparity is the exclusion of topographic wetness index identified by wiesmeier et al 2019 as a particularly promising factor along with all other non local lsps from this study expanding the experimental framework to include non local lsps along with larger scale study sites may provide a more reliable assessment of the importance of scale on the relationship between lsps and soc at regional scales or larger 5 conclusions this research sought to explore the impact of recent developments in spatially heterogeneous scale representation of topographic covariates using an applied dsm analysis qrf models of soil cec ph soc and texture fractions at five standard depths were generated to evaluate differences in performance between input feature sets differentiated by scaling strategy in general the results determined that heterogeneous scaling strategies did not offer a consistent improvement to model accuracy error or prediction uncertainty however the results reiterated the value of multiscale methods over unscaled topographic covariates due to the consistently poor performance of the unscaled feature set comparisons of individual scaling strategies revealed het c as the dominant strategy for all tested soil properties except soc it was concluded by process of elimination that the favourable performance was driven by the ability to define broad process based scale ranges from which the heterogeneous scale optimization selects significant scales based on the local topographic context while it was not tested explicitly the relative impact of topographic scaling methods is expected to be generalizable to other study sites because the purpose of multiscale topographic analysis is to characterize the variability in topographic structure independently of subsequent modelling methodology and input data so while different training data and geographic setting will likely impact model performance in absolute terms the benefits of spatially heterogeneous multiscale topographic characterization remains relatively the same future work may evaluate how well the stronger performance of heterogeneous scale representation affect environmental model outcomes in standard workflows i e including non topographic covariates and how well the results translate to other study sites and different sample sizes the inclusion of optimal scale information was also examined and was found to slightly improve model accuracy and slightly lower model error and prediction uncertainty however the differences were minor and inconclusive given the increases in model complexity due to the addition of information the remarkably poor performance of soc models contributes to the discussion on topographic drivers of soc where the scale of topographic covariates is discussed as a confounding variable responsible for mixed empirical results however the results presented suggest scale has a negligible impact on soc model performance at least for local lsps funding this research was funded by a grant provided by the natural sciences and engineering research council of canada nserc grant number 400317 author contributions conceptualization d r n and j b l data curation d r n and d d s formal analysis d r n funding acquisition j b l investigation d r n methodology d r n and d d s project administration j b l resources d r n software d r n and j b l supervision j m h c l d and j b l validation d r n visualization d r n writing original draft d r n writing review editing d d s j m h c l d and j b l declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper appendix a spatial distributions of clay predictions upper case and uncertainty lower case at 60 100 cm soil depth elevation a is given for reference heterogeneous feature sets are on the left het z c het zg e and het c g homogeneous feature sets are on the right naive b hom t1 d hom t2 f and hom vif h image 1 appendix b spatial distributions of soc predictions upper case and uncertainty lower case at 0 5 cm soil depth elevation a is given for reference heterogeneous feature sets are on the left het z c het zg e and het c g homogeneous feature sets are on the right naive b hom t1 d hom t2 f and hom vif h image 2 appendix c summary statistics of soil samples interpolated to globalsoilmap net standard depths soil property depth cm n minimum maximum mean standard deviation cec 0 5 1643 0 220 204 144 24 711 23 224 5 15 1636 0 269 199 406 23 237 22 178 15 30 1611 0 220 187 372 20 246 20 101 30 60 1538 0 220 172 251 16 955 17 333 60 100 1436 0 220 187 085 16 979 15 719 ph 0 5 1643 3 434 8 328 6 278 0 851 5 15 1636 3 544 8 321 6 331 0 835 15 30 1611 3 583 8 333 6 542 0 798 30 60 1538 3 730 8 534 6 923 0 752 60 100 1436 4 048 8 551 7 226 0 710 soc 0 5 1643 0 010 48 100 6 538 9 532 5 15 1636 0 010 48 100 4 698 7 204 15 30 1611 0 010 48 100 3 013 6 325 30 60 1538 0 010 47 947 1 541 5 372 60 100 1436 0 010 46 444 0 860 4 287 sand 0 5 1627 0 015 0 799 0 339 0 154 5 15 1597 0 016 0 859 0 331 0 158 15 30 1525 0 005 0 796 0 316 0 163 30 60 1622 0 017 0 795 0 338 0 154 60 100 1424 0 008 0 867 0 319 0 160 silt 0 5 1627 0 011 0 831 0 258 0 160 5 15 1597 0 007 0 831 0 265 0 185 15 30 1525 0 006 0 842 0 294 0 224 30 60 1622 0 011 0 831 0 258 0 164 60 100 1424 0 006 0 870 0 351 0 245 clay 0 5 1627 0 008 0 954 0 403 0 260 5 15 1597 0 005 0 968 0 405 0 282 15 30 1525 0 003 0 973 0 390 0 306 30 60 1622 0 008 0 956 0 404 0 265 60 100 1424 0 003 0 984 0 330 0 308 
25473,fao 56 dual crop coefficient based model under various field conditions pierre laluet a luis olivera guerra a vincent rivalland a vincent simonneaux a jordi inglada a joaquim bellvert b salah er raki c d olivier merlin a a centre d etudes spatiales de la biosphère cesbio université de toulouse cnes cnrs ird ups toulouse france centre d etudes spatiales de la biosphère cesbio université de toulouse cnes cnrs ird ups toulouse france centre d etudes spatiales de la biosphere cesbio universite de toulouse cnes cnrs ird ups toulouse france b efficient use of water in agriculture program institute of agrifood research and technology irta parc científic i tecnològic agroalimentari de gardeny pcital fruitcentre 25003 lleida spain efficient use of water in agriculture program institute of agrifood research and technology irta parc científic i tecnològic agroalimentari de gardeny pcital fruitcentre lleida 25003 spain efficient use of water in agriculture program institute of agrifood research and technology irta parc cientific i tecnologic agroalimentari de gardeny pcital fruitcentre 25003 lleida spain c procede agrobiotech center département de physique appliquée faculté des sciences et techniques université cadi ayyad marrakech morocco procede agrobiotech center département de physique appliquée faculté des sciences et techniques université cadi ayyad marrakech morocco procede agrobiotech center departement de physique appliquee faculte des sciences et techniques universite cadi ayyad marrakech morocco d mohammed vi polytechnic university um6p center for remote sensing applications crsa morocco mohammed vi polytechnic university um6p center for remote sensing applications crsa morocco mohammed vi polytechnic university um6p center for remote sensing applications crsa morocco corresponding author fao 56 dual crop coefficient fao 2kc based model are increasingly applied at large scale for agricultural water monitoring requiring field scale data over the spatial extent of interest given the lack of in situ measurements satellite products can be used to estimate indirectly the parameters through calibration however a lack of knowledge about model sensitivity can lead to suboptimal use of satellite data this study aims to analyze the sensitivity of samir a fao 2kc based model using satellite data the sobol method was applied for evapotranspiration et and deep percolation dp simulations on 37 contrasted agricultural seasons results indicate that samir s sensitivity mainly depends on the modeled water stress we proposed a proxy for the model sensitivity which can determine 84 73 of the et dp among the agricultural seasons an interaction analysis allowed reducing the calibration problem to the adjustment of only two parameters a kcb and zr max accounting for most of the sensitivity keywords fao 56 model sobol sensitivity analysis evapotranspiration deep percolation remote sensing calibration data availability data will be made available on request 1 introduction irrigation is the most water intensive anthropogenic activity in the world the resources available for it are already under pressure in some regions and will be even more so in the future fao 2021 to face these challenges crop water balance models are widely used with the aim of optimizing agricultural water use pereira et al 2020 constantin et al 2015 such models seek to estimate the crop water consumption and irrigation needs by simulating all the terms of the crop water balance including evapotranspiration et corresponding to crop consumption and deep percolation dp being an indicator of water loss for crops the dual crop coefficient version fao 2kc of the fao 56 method allen et al 1998 is based on the estimation of actual et coupled to a soil water balance the fao 2kc method has been widely used in both operational and academic contexts for its efficiency and parsimony more than 30 000 citations of the fao 56 method pereira et al 2021 from the same basic fao 2kc formulation many models have been developed with their own specificities like the representation of additional processes e g runoff capillary rise a more detailed description of specific processes e g dp root development soil evaporation or the use of satellite data helman et al 2019 olivera guerra et al 2018 han et al 2018 bellvert et al 2018 campos et al 2017 yang et al 2012 rosa et al 2012a b raes et al 2009 lollato et al 2016 sheikh et al 2009 in the same vein the samir satellite monitoring for irrigation simonneaux et al 2009 model which is used in this work is a fao 2kc based model integrating remotely sensed normalized difference vegetation index ndvi to constrain the vegetative growth it includes 12 parameters related to soil and crop type characteristics the fao 2kc method can simulate et and dp at the plot or at the pixel scale which when aggregated can provide simulations of both fluxes at integrated spatial scales irrigation district or catchment using mapped input data kharrou et al 2021 garrido rubio et al 2020 bretreger et al 2019 the fao 2kc requires i maps of meteorological forcings ii maps of irrigation forcings iii maps of crop type to derive crop parameters and iv maps of soil texture to derive soil parameters those maps are becoming increasingly available and accurate thanks to the development of i reanalysis meteorological data sets at enhanced resolutions e g era5 hersbach et al 2020 safran vidal et al 2010 ii maps of actual irrigation type volume and timing derived from satellite observations massari et al 2021 iii crop type maps whether they come from field observations or classifications based on satellite observations foerster et al 2012 inglada et al 2015 and iv soil texture maps e g soilgrids hengl et al 2017 globalsoilmap arrouays et al 2017 however such data sets still have significant uncertainties due to intrinsic errors in the mapped data and to additional errors associated with their conversion to directly useable input parameters e g hydrodynamical soil properties are usually derived from soil texture maps poggio et al 2021 folberth et al 2016 loosvelt et al 2012 it is thus often necessary to calibrate the model input parameters using external data such a calibration strategy can be implemented at the field scale using in situ measurements kharrou et al 2021 saadi et al 2015 paredes et al 2014 er raki et al 2007 zhang et al 2013 or over extended areas using remotely sensed soil moisture or et data amazirh et al 2022 ouaadi et al 2021 er raki et al 2008 to reduce uncertainties in spatially distributed model input parameters many works have dealt with the assimilation of remotely sensed soil moisture brocca et al 2014 azimi et al 2020 zaussinger et al 2019 and et wu et al 2015 droogers et al 2010 products by minimizing sequentially and recursively the gap between simulations and observations it is possible to indirectly retrieve optimal values of a set of input parameters or at least to reduce their a priori uncertainty however this approach may face difficulties in terms of practical implementation due to its extensive requirement in terms of computational resources calibrating an agro hydrological model over large areas may indeed require a large number of simulations this is especially true when models have a considerable number of input parameters from a dozen simonneaux et al 2009 to several dozen neitsch et al 2011 and when they are spatialized over several thousands of pixels or fields in addition the calibration of many parameters from limited observations raises the issue of equifinality an equifinality occurs when several parameter sets lead to a result considered as optimal this can be problematic because each of these parameter sets does not necessarily have any likelihood with the physical reality of the parameters physical reality of which we can have prior knowledge beven and freer 2001 2006 a solution to the above concerns is to analyze the sensitivity of the studied model in order to identify and calibrate the parameters having the most influence on the outputs indeed focusing only on the most sensitive parameters may significantly reduce the required computer resources in addition to limiting the compensation issues between parameters even though models based on the fao 2kc method are widely used the study of their sensitivity to input parameters is not or almost not present in the scientific literature olivera guerra et al 2020 analyzed the sensitivity of three soil parameters soil moisture at field capacity at wilting point and the maximum roots depth zr max of samir using a local sensitivity analysis method i e by varying independently each of the parameters and looking at their impact on an objective function however for a comprehensive analysis and this is especially required when the number of analyzed parameters is larger global sensitivity analysis methods are strongly recommended saltelli and annoni 2010 song et al 2015 global sensitivity analysis methods can calculate the influence of input parameters over their entire range of variation and are appropriate for all types of models non linear and non monotonic the sobol variance based method sobol 1993 2001 saltelli et al 2008 2010 is a popular global sensitivity analysis method that has been used in many recent articles because of its robustness and of its ability to analyze interactions between parameters nossent et al 2011 baroni and tarantola 2014 tang et al 2007 zhang et al 2013 the principle of the sobol method is to decompose the total variance of a model s output and to look at how each uncertain parameter contributes to it whether this contribution is caused only by a single parameter or by the interaction of two or more parameters in this context the objective of this paper is to perform a sobol sensitivity analysis of the fao 2kc based samir model for both output fluxes et and dp to ensure the analysis results are representative of the field conditions where the fao 2kc method is generally implemented ten fields and a total of 37 agricultural seasons were selected based on their contrasted meteorological pedological and agricultural characteristics the paper is organized as follows the samir model data sets and the sobol method are first described section 2 then the results of the sobol sensitivity analysis are discussed for both et and dp section 3 with the overall goal of identifying the most influential parameters depending on actual field conditions finally the conclusions and perspectives are presented section 4 2 material and methods the overall methodology to assess the sensitivity of the samir model to its input parameters is presented in the flowchart of fig 1 first samir is presented together with its 12 input parameters section 2 1 then the input data composed of forcing precipitation irrigation ndvi crop type and soil properties are described section 2 2 the next three subsections describe the main steps of the sensitivity analysis approach that are calculating the sobol indices for each of the parameters and each target variable et and dp section 2 3 sampling the samir parameters with normal distributions defined for each agricultural season section 2 4 and running samir on the generated samples and computing performance metrics section 2 5 finally a proxy for the sensitivity of samir et and dp simulations is investigated from the variability of sobol indices section 2 6 2 1 satellite monitoring for irrigation samir model the samir simonneaux et al 2009 model used in this study is based on the fao 2kc method while a detailed description of the fao 2kc method is provided in allen et al 1998 only the main components are briefly reminded below as well as the main differences between this method and the samir model for a more detailed overview of the samir model readers are encouraged to refer to saadi et al 2015 the principle of the fao 2kc method is the calculation of crop water balance components for daily et estimation taking into account the plants and the soil water status it uses i meteorological forcing variables to calculate the reference et called et0 ii precipitation and irrigation amounts resulting in water available for et or for soil reservoir recharge iii crop and soil parameters to compute soil reservoir properties as well as plant and soil resistance to water stress and iv the initial soil water content at the start date of model simulations in addition to these variables and parameters the samir model incorporates remotely sensed ndvi time series to drive the development of the modeled vegetation the daily water balance equation simulated by samir is 1 p i e t d p δ s w with p being the precipitation i the irrigation et the actual evapotranspiration δsw the variation of soil water content from the previous day and dp the deep percolation being the water exceeding the maximum soil storage capacity the et calculation is done by applying crop coefficients to et0 as follow 2 e t e t 0 k c b k s k e where et0 kcb ks is the water transpired by plants t and et0 ke is the soil evaporation e in equation 2 et0 is calculated according to the fao penman monteith equation allen et al 1998 kcb is the basal crop coefficient following a linear relationship with ndvi ks is the water stress coefficient being a reduction factor of t and ke is the soil evaporative coefficient being related to vegetation fraction cover fc surface soil moisture and soil properties similarly to kcb fc follows a linear relationship with ndvi in addition to the use of ndvi time series samir incorporates two additional modifications from the classical fao 2kc method i the kr evaporation reduction coefficient accounting for soil evaporation resistance as a function of surface soil moisture is calculated with the method proposed by merlin et al 2016 this method instead of using an a priori parameterization uses a pedotransfer function based on clay and sand fractions fclay and fsand which was derived and evaluated over a variety of sites and soil textures merlin et al 2016 lehmann et al 2018 amazirh et al 2021 ii the soil moisture values at field capacity and at wilting point are calculated using fclay and fsand from the pedotransfer function proposed by román dobarco et al 2019 samir uses a total of 12 user defined parameters six parameters related to the plants phenological stage and t capabilities that we named the pheno parameters five parameters related to soil reservoir properties and plant resistances to stress that we named the stress parameters and one parameter used to set the soil water content at the start date of model simulations a detailed description of the parameters is provided in table 1 with their definition and the processes in which they are involved 2 2 sites and data description data of 37 agricultural seasons from ten different crop fields around the world were used they were obtained from national and international databases or from specific intensive field campaigns the agricultural seasons cover a wide variety of agro pedo climatic conditions that reflect the contexts of use of samir i e different crop types mainly located in dry areas where water related agricultural issues are important but also and to a lesser extent in temperate areas where et estimation may also be key for a good water management the data set involves 13 crop types including summer and winter cereals vegetables and fruit trees 10 soil textures ranging from clay to silty loam 4 irrigation types flood irrigation sprinkler drip irrigation no irrigation and 2 different climates semi arid and temperate table 2 reports the characteristics of each agricultural season the data used for running samir are composed of i meteorological variables obtained from local weather stations for precipitation and et0 calculation air temperature wind speed solar radiation and relative air humidity ii irrigation dates and amounts obtained from water meters iii crop types iv soil texture fclay and fsand and v ndvi time series obtained from sentinel 2a and 2 b and landsat 7 and 8 the sentinel 2 constellation provides a 10 m resolution pixel with a temporal resolution of 10 days from 2015 to 5 days since 2017 in clear sky conditions landsat 7 provides 30 m resolution pixels with a 16 day time resolution since 1999 landsat 8 has the same characteristics but has been available since 2013 and its overpass is offset by 8 days with respect to landsat 7 it is important to note that for all agricultural seasons the periods studied ranged from the beginning of the vegetation development to the end of the senescence period periods between successive agricultural seasons with bare soil or low evaporative demand were not considered herein because this study only assessed the sensitivity of et and dp when the major part of crop s water consumption occurs 2 3 sobol sensitivity analysis 2 3 1 sobol indices a general overview of the sobol method is presented here while more detailed descriptions can be found in saltelli et al 2008 2010 or khorashadi zadeh et al 2017 consider a model y f x f x1 xk with k parameters where y is the model output e g a performance metric such as the root mean square difference and x x1 xk the parameter set which can be decomposed into 2k terms representing different order of interaction between parameters 3 f x 1 x k f 0 i 1 k f i x i i 1 k j i 1 k f i j x i x j f 1 k x 1 x k the total output variance of the model v y can be decomposed into corresponding partial variances 4 v y i 1 k v i i 1 k 1 j i 1 k v i j v 1 k the first order index si also called main effect is the ratio between vi in equation 4 and the total output variance v y si can be written as follows 5 s i v i v y v x i e x i y x i v y the total index sti represents the sum of xi s main effect with all its higher order interactions up to order k v i is the variance resulting from the contribution of all parameters except xi it can be written as follows 6 s t i 1 v i v y e x i v x i y x i v y sti index is the one we used in this study to evaluate the sensitivity of samir parameters indeed sti unlike si integrates all the influence a parameter has on a model output which is of interest to determine the most sensitive parameters si is used to calculate the interactions a parameter has with the others by calculating the difference between sti and si if the sum of the si is equal to 1 it means that the model is linear and that there is no interaction between the parameters on the contrary if the sum of the si is smaller than 1 the model is non linear the lower the sum of the si the more the parameters of the model interact with each other the safe toolbox pianosi et al 2015 was used in this study in order to perform the sobol indices calculation and generate the samples 2 3 2 convergence analysis for each agricultural season a convergence analysis of si and sti was done in order to verify that the number of simulations is sufficient to ensure their stability results not shown indicated that n n k 2 33 000 with n 3000 and k 9 parameters is sufficient for all the 37 agricultural seasons 2 3 3 bootstrapping in order to optimally take into account the uncertainties related to the distribution of the model outputs in the sti and si values we used a bootstrapping technique was applied efron and tibshirani 1994 this technique consists of randomly resampling the 33 000 model outputs 1000 times and recalculating the sobol indices for each resampled data set then the average of si and sti is derived from the 1000 resamples and can be kept for the sensitivity analysis 2 3 4 dummy parameter a dummy parameter a parameter added to the analysis that we know has no influence on the model output was introduced in this sensitivity analysis for further explanation on the calculation of the dummy parameter readers are encouraged to refer to khorashadi zadeh et al 2017 the sti calculated for the dummy parameters were used herein as thresholds to identify sensitive parameters from insensitive parameters they were also used to ease the readability of the sti values by normalizing them for each agricultural season between the dummy parameter s sti and the sum of all parameters sti it is then called stinorm and is expressed as a percentage 2 4 parameters sampling 2 4 1 latin hypercube sampling despite its popularity the sobol sensitivity analysis method can be challenging to implement due to its high computational cost it requires a total of n n k 2 samples with n a baseline sample size that can vary between 1000 to more than 10 000 and k the number of analyzed parameters to optimize the sampling efficiency the latin hypercube sampling method lhs was used lhs is a monte carlo based method using a stratified sampling approach where the distribution of each parameter is divided into p ranges each with a probability of occurrence of 1 p parameter values are randomly generated so that each range gets sampled only once the same step is repeated for each of the k parameters to generate a matrix of size p k with random sample combinations of the different parameters the lhs method has been used in many studies e g campolongo et al 2011 saltelli and annoni 2010 zhang et al 2013 tang et al 2007 and song et al 2015 2 4 2 parameters mean and standard deviation nine parameters were sampled and analyzed out of the 12 included in samir of these three parameters were fixed kcb max kc max and init hum see table 1 for kcb max and kc max a previous analysis not shown performed on six contrasted agricultural seasons indicated that their sensitivity is negligible the main explanation lies in the way the kcb ndvi relationships are constructed using minimum and maximum ndvi values from satellite observations limiting the number of days when these parameters can influence et regarding init hum we considered it as a forcing data and set it to its median value 0 5 because soil moisture measurements at the beginning of agricultural seasons were not available for all sites the parameters analyzed were sampled according to a normal distribution using mean and standard deviation values to make the link with the lhs method mentioned in section 2 4 1 the normal distributions obtained for each parameter were divided into p intervals of equal probability intervals being then more or less narrow depending on the parameter s distribution the mean values of the nine parameters were defined for each agricultural season from i field analyses for fclay and fsand ii literature references for zr max p and ze and iii satellite observations for ndvi related parameters a fc b fc a kcb and b kcb the standard deviation of crop related parameters zr max p a fc b fc a kcb and b kcb vary according to the crop type it allows us to represent the uncertainty of a given parameter as a function of the mean value of this parameter and therefore according to the crop type for example zr max of broccoli crops having a mean zr max of 500 mm has a smaller uncertainty and therefore a smaller standard deviation than zr max of maize crops having a mean value of 1050 mm since maize has been extensively investigated the standard deviations of its parameters were chosen as a reference therefore the ratios between the standard deviation and the mean obtained for each of the maize parameters was applied to the mean values of the corresponding parameters of the 12 other crop types to derive their standard deviation regarding the soil related parameters the mean and standard deviation of ze were fixed for all the 37 agricultural seasons as well as the standard deviations of fclay and fsand table 3 summarizes the mean and standard deviation values used to generate samples for the nine analyzed parameters 2 5 outputs for sobol indices computation root mean square deviation the root mean square deviation rmsd performance metric was used to calculate the sobol indices for each of the 37 agricultural seasons 33 000 rmsd were calculated for both et and dp corresponding to the 33 000 parameters samples used for samir simulations the rmsd formula is written as follows 7 r m s d i 1 j ŷ i y i 2 j where j is the number of days in the simulated time series i is one day of the time series ŷi is a simulated variable time series and yi is a reference variable time series in this study the reference variable time series yi were obtained for each agricultural season by averaging the 33 000 simulated time series 2 6 deriving a fao 2kc sensitivity proxy spfao 2kc from field conditions once the sobol indices were obtained for the nine parameters a correlation was sought between them and the soil vegetation atmosphere characteristics of the 37 agricultural seasons the idea was to confront the sobol indices with different criteria soil texture crop type cumulative rainfall mean et0 mean ndvi modeled crop water stress level etc until a satisfactory correlation was observed from this correlation a proxy of the sensitivity of the samir parameters spfao 2kc has been proposed 3 results and discussion the results of the sobol sensitivity analysis of samir are presented and discussed in this section sections 3 1 3 3 focus on the model sensitivity for et simulations while section 3 4 is about the dp simulations first the parameter sensitivity obtained for the 37 agricultural seasons are investigated section 3 1 then a fao 2kc sensitivity proxy spfao 2kc is proposed to explain and predict the samir s parameter sensitivity from the on site characteristics solely section 3 2 next the interactions between parameters are studied to better assess their sensitivity and to be able to select a minimum parameter set for calibration section 3 3 finally for dp the similarities and differences with the et case are discussed and the main results are presented section 3 4 3 1 parameters sensitivity for et simulations each group of pheno a fc b fc a kcb b kcb and stress zr max p fclay fsand ze parameters is involved in distinct and clearly identified processes as explained in section 2 1 this leads us to hypothesize that these two groups will have significant differences in terms of stinorm values for each pheno and stress group we summed the stinorm of their parameters and named these sums pheno stinorm and stress stinorm table 4 reports the sti si and stinorm values obtained for three selected agricultural seasons which were found to reflect well the different sensitivity types lam wheat 11 is a stress sensitive agricultural season cat maize2 21 a phenology sensitive one and mex chilli 08 has a balanced sensitivity a stress sensitive phenology sensitive agricultural season is characterized by a stress stinorm pheno stinorm greater than 66 respectively similarly a balanced agricultural season has a pheno stinorm and a stress stinorm between 33 and 66 in table 4 the first column corresponding to lam wheat 11 shows a stress stinorm equal to 81 it means that for this agricultural season the five stress parameters account for 81 of the rmsd variation and thus for 81 of the total sensitivity of the samir s parameters among these parameters zr max p and fclay showed to be the most sensitive having higher stinorm ze which has a sti value equal to the one of the dummy parameter ends up with a stinorm equal to 0 pheno stinorm of cat maize2 21 is equal to 100 indicating that it corresponds to a phenology sensitive site the pheno stinorm of mex chilli 08 is equal to 64 reflecting a balanced sensitivity fig 2 a shows boxplots of the stinorm of the nine parameters analyzed separately and fig 2b shows the boxplots of the stinorm of the parameter groups pheno stinorm and stress stinorm obtained for the 37 agricultural seasons two elements stand out in this figure i a kcb b kcb and zr max have in most cases a larger stinorm than the other parameters with however a large dispersion first quartile q1 is 0 1 for a kcb b kcb and zr max and third quartile q3 is 0 28 for a kcb and b kcb and 0 37 for zr max ii pheno stinorm and stress stinorm fig 2b are nearly identical with a large dispersion for both parameter groups q1 and q3 are 0 23 and 0 78 respectively this large dispersion indicates that from one agricultural season to another and therefore from one agro pedo climatic context to another the parameter sensitivity can vary significantly in fig 3 results are presented grouped by the three types of sensitivity found within the agricultural seasons stress sensitive phenology sensitive and balanced fig 3a shows that for the majority of the 12 phenology sensitive agricultural seasons a kcb and b kcb have the largest stinorm fig 3c shows that for the 15 stress sensitive agricultural seasons zr max has the largest stinorm with a mean value of 36 for these agricultural seasons fclay also shows a certain level of sensitivity mean stinorm is 18 in fig 3e showing the 10 balanced agricultural seasons the three parameters a kcb b kcb and zr max already identified as the most sensitive stand out the fact that very contrasting or even opposite types of sensitivity were identified between the 12 phenology sensitive and the 15 stress sensitive agricultural seasons confirms the relevance of gathering the parameters into pheno and stress groups figs 2 and 3 highlight distinct sensitivity types being well represented among the 37 agricultural seasons they also indicate which parameters are the most sensitive depending on the type of agricultural season a kcb and b kcb for the phenology sensitive ones dominated by the pheno parameter group zr max for the stress sensitive ones dominated by the stress group and a kcb b kcb and zr max for the balanced ones 3 2 searching for spfao 2kc for et simulations as a step further we tried to find a proxy for samir sensitivity spfao 2kc based on the agro pedo climatic characteristics of agricultural seasons different criteria were tested and confronted with the stinorm of the parameter groups pheno stinorm and stress stinorm for all agricultural seasons through trial and error a good correlation emerged r2 0 84 between the modeled stress intensity in the root zone calculated with an average set of parameters and the stinorm of the parameter groups fig 4 formally spfao 2kc is written as follows 8 s p f a o 2 k c i 1 d k s i d i f k s i 1 a n d k c b i k c b max 0 2 with d being the number of days of the simulated time series when the crop is stressed ks lower than 1 and when the crop coefficient kcb is larger than 20 of its maximum value i being a specific day meeting this condition ksi being a daily value of ks and kcbi being a daily value of kcb to simplify the interpretation of spfao 2kc we normalized it between the minimum 0 41 and the maximum 0 93 of the spfao 2kc values obtained among the 37 agricultural seasons spfao 2kc can be understood as follows the more an average set of parameters of an agricultural season generates intense crop water stress levels related to ks values lower than 1 at times when potential t is significant the more the parameters belonging to the stress group will weigh in the model sensitivity to figure out why this indice emerged rather than another we must seek to better understand how samir works and how the different modeled processes influence the rmsd of simulated et fig 5 shows the mean time series of four samir outputs for lam wheat 11 and cat maize2 21 and their associated uncertainties represented with q1 and q3 lam wheat 11 is a non irrigated winter wheat crop with significant precipitation in winter and spring until april may followed by a decrease in rainfall resulting in water stress from april it is a stress sensitive agricultural season stress stinorm is 81 cat maize2 21 is a heavily irrigated summer maize crop with no water stress ks 1 throughout the season it is a phenology sensitive agricultural season with a pheno stinorm equal to 100 simulated time series of lam wheat 11 and cat maize2 21 illustrated in fig 5 and the stinorm values shown in table 4 provide keys to understand the relevance of spfao 2kc equation 8 i in fig 5 we see that there are more uncertainties associated with t than with e this explains why a kcb b kcb and zr max stand out to be more sensitive table 4 as they are related to t process it also explains why the parameters fclay fsand ze and even a fc and b fc are less sensitive as they are partly or entirely associated with e process such differences in the level of uncertainties between t and e can be explained by the fact that i the simulated time series of the 37 agricultural seasons include few bare soil periods fraction cover higher than 0 75 most of the time and ii the formalism for e simulation generates less uncertainty than the formalism related to t simulation ii the uncertainties in t and e related to a kcb and b kcb are practically constant during the whole simulated period an uncertainty associated to kcb is always present on each simulated day but are considerably less important than the uncertainties in t associated to water stress i e when ks is lower than 1 iii during the simulated periods when ks is lower than 1 the uncertainties associated with t can be very large if the water stress lasts long enough while ks is decreasing the stress parameters and thus mostly zr max become more sensitive than the pheno parameters as is the case for lam wheat 11 iv when there is no water stress ks equal to 1 as for cat maize2 21 the uncertainties associated with t are entirely related to the pheno parameters and thus essentially to a kcb and b kcb the above four points help understand why spfao 2kc based on the modeled crop water stress intensity emerged as an efficient proxy for the model sensitivity indeed when there is little water stress ks close to 1 the uncertainties generated by the stress parameters are low this is explained by the fact that ks is a bounded variable whose maximum value is fixed at 1 resulting in limiting the uncertainties caused by the stress parameters when ks is close to 1 by contrast when there are more intense stress episodes the soil moisture content of the root reservoir and thus the ks value takes time to decrease generating a longer period with larger uncertainties associated with t consequently the longer and more intense the stress episode is the more t simulations will be affected by the stress parameters and the more they will weigh in the final sensitivity of the agricultural season note that although the correlation between spfao 2kc and pheno stinorm is close to 1 the relationship shows a significant variability when pheno stinorm is below 30 this is because spfao 2kc is less effective for agricultural seasons dominated by the stress parameters with low pheno stinorm 3 3 interactions between parameters fig 6 shows boxplots with the total interactions of the nine analyzed parameters for the 37 agricultural seasons for et simulations the total interactions of a parameter calculated by subtracting si to sti reflect how this parameter indirectly affects the sensitivity of all the other parameters when its value is changed the knowledge of the total interactions coupled with a certain knowledge of the samir model gives enough information to select the minimum parameter set to be calibrated among the parameters being the most sensitive the knowledge of samir that is of interest here is that the pheno and the stress parameter groups are involved in very different processes and have little relationship with each other as mentioned in section 2 3 1 for a given agricultural season the lower the sum of its si the higher the general level of interaction between its parameters here the 37 agricultural seasons have globally important levels of interactions since the sum of their si is on average relatively low 0 48 with a minimum of 0 17 and a maximum of 0 68 in fig 6 we can see that among the pheno group a kcb and b kcb concentrate most of the total interactions this makes sense since they are part of the same linear relationship linking kcb to ndvi this means that when a kcb is modified for calibration it greatly affects the b kcb sensitivity and conversely therefore calibrating only one of these two parameters is somehow equivalent to calibrating the kcb ndvi relationship in addition to avoiding compensation effects between them herein we chose to calibrate a kcb rather than b kcb because a kcb has a greater influence on the vegetation growth being a time when the t demand is generally high regarding the stress group almost all the total interactions are shared between zr max fclay and fsand these three parameters interact strongly with each other as they are all involved in the size of the root reservoir which is the key element governing the occurrence and intensity of water stress therefore as with a kcb and b kcb calibration of only one of these three parameters can be sufficient to indirectly calibrate the root reservoir we recommend calibrating zr max rather than fclay and fsand because zr max appeared to be the most sensitive and because no validated maximum root depth maps are currently available as opposed to soil texture maps from this parameter interactions analysis it appears that out of the nine parameters only two a kcb and zr max can explain most of the model sensitivity these parameters can be calibrated either together or individually depending on the value of spfao 2kc and the available computer resources 3 4 parameters sensitivity for dp simulations and relationship with agricultural season characteristics this section deals with the sensitivity of the rmsd calculated for dp simulation it goes into less detail than for et simulation and focuses on the similarities and differences between dp and et cases herein only 31 of the 37 agricultural seasons were analyzed because six had no dp event as for the et case the parameters were gathered into stress and pheno groups and the most sensitive ones were a kcb b kcb zr max and to a lesser extent fclay and fsand not shown the sensitivity proxy for dp simulations spfao 2kc dp is similar to the one proposed for et spfao 2kc with the only difference being that the period used to calculate it excludes the days following the last dp event indeed the days following the last dp event and their associated agro climatic conditions do not count in the generation of any dp event fig 7 shows the relationship between spfao 2kc dp and pheno stinorm for dp simulations a determination coefficient of 0 73 was obtained the lower correlation obtained for dp than for et r2 0 84 can be explained by the higher complexity of the parameters sensitivity for dp simulations indeed in samir the dp events are punctual and each of them has its own sensitivity according to what occurred during the period just before the final sensitivity of the parameters can thus be seen as the product of several stinorm obtained for each dp event also especially for agricultural seasons with few dp events the period prior to the last dp events may have involved a significant portion of bare soil which would potentially lead to a higher sensitivity of the evaporation related parameters this lower correlation may also be related to the fact that the 31 agricultural seasons are less homogeneously distributed between phenology sensitive and stress sensitive agricultural seasons with 21 agricultural seasons being phenology sensitive and only four being stress sensitive 3 5 overall results in brief the sobol sensitivity analysis for both et and dp simulations as well as the sensitivity proxy spfao 2kc and the total interaction analysis revealed several guidelines for the calibration of the fao 2kc based model samir i for spfao 2kc greater than 0 66 phenology sensitive agricultural season a kcb should be selected for calibration ii for spfao 2kc lower than 0 33 stress sensitive agricultural season it should be zr max iii for spfao 2kc between 0 33 and 0 66 balanced agricultural season it should be both a kcb and zr max note that the above recommendations only apply in the case where the number of parameters to be calibrated should be minimized at maximum because of computer resources limitation or equifinality issues if there are no such issues it is possible to calibrate both a kcb and zr max in every case also by focusing either on a kcb or zr max only certain aspects of the simulated time series would be considered in the calibration for example if only a kcb is calibrated for a phenology sensitive agricultural season with a spfao 2kc equal to 0 70 thus having some water stress periods the days when et or dp are impacted by crop water stress would not be considered in the calibration because a kcb only affects the t demand in contrast if only zr max is calibrated for a stress sensitive agricultural season having a spfao 2kc equal to 0 30 the focus is only given to the periods when the amount of t is reduced by water stress neglecting the days without water stress in the calibration process in summary depending notably on the computer capacities available we recommend that the user calibrate both zr max and a kcb together or choose between the two based on the spfao 2kc calculated for the studied agricultural season see table 5 moreover by choosing to calibrate only a kcb and or zr max and thus only t related parameters the days with bare soil or very little vegetation are not or poorly taken into account in the calibration we found that under the conditions of the studied agricultural seasons the parameters related to e appear to be much less sensitive than the parameters associated with t as shown and explained in section 3 1 however it is worth mentioning that the parameters related to e may be important to consider on specific agricultural seasons not included in this study with long and wetted bare soil periods 4 conclusion fao 2kc based models are increasingly applied in a spatialized way to simulate the inward and outward water fluxes over extended agricultural areas e g irrigation district it requires knowing the sensitivity of the uncertain input parameters in order to be able to calibrate them spatially and optimally and to face computational and equifinality issues however although these models have been widely used no proper sensitivity analysis has yet been done to fill the gap this paper investigated the sensitivity of the fao 2kc based crop water balance model samir for the simulation of et and dp considering the potential influence of different site characteristics for this purpose we applied the sobol sensitivity analysis method on 10 instrumented sites and a total of 37 agricultural seasons being diverse in terms of climate pedology and agricultural practices sobol sensitivity indices were calculated for nine samir parameters and a correlation between them and the agricultural season s conditions was sought sobol s sensitivity indices indicate that three among the nine parameters stand out weighing on average 63 in the sensitivity of samir et a kcb and b kcb related to phenology and zr max related to crop water stress it also appears that the importance of the pheno and stress parameter groups in the samir sensitivity varies greatly from one agricultural season to another depending on the modeled crop water stress intensity a proxy for the sensitivity of samir spfao 2kc has thus been proposed as the average of the crop stress coefficient ks on the days when there is crop water stress ks lower than 1 and when the crop coefficient kcb is larger than 20 of its maximum value kcb 0 2 kcb max spfao 2kc is able to determine 84 of the variability in samir et sensitivity 73 for the dp case among all agricultural seasons considered the total interactions analysis coupled to our knowledge of the samir model revealed a strong interaction between a kcb and b kcb as well as between zr max fclay and fsand this further highlights the importance of the sensitivity of a kcb b kcb and zr max and led us to retain only a kcb and zr max for calibration if the user has no limitation in terms of computing capacity he can calibrate both a kcb and zr max if he is faced with such constraints and needs to optimize the number of parameters to be calibrated he can use spfao 2kc value computed from a simulation performed with an average parameter set when spfao 2kc is lower than 0 33 we recommend calibrating zr max when it is higher than 0 66 a kcb and when it is between 0 33 and 0 66 both a kcb and zr max these results represent a solid basis for spatializing fao 2kc based models using remotely sensed data through notably the distributed calibration of their input parameters such a calibration strategy could rely on the soil moisture products derived from smos smap e g ojha et al 2019 paolini et al 2022 or sentinel 1 e g el hajj et al 2017 and on et products derived from landsat 8 senay et al 2016 or sentinel 2 and sentinel 3 e g guzinski et al 2020 ongoing missions in addition new satellite missions will be launched in the coming years such as trishna lagouarde et al 2018 and lstm koetz et al 2018 which will provide field scale et estimates at an unprecedented frequency software availability samir is an open source software and is available at the following address https gitlab cesbio omp eu modelisation modspa documentation can be found by visiting this link the code is implemented in python 3 language contact information vincent rivalland cesbio cnes fr declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this study was supported by the idewa project anr 19 p026 003 of the partnership for research and innovation in the mediterranean area prima program and by the horizon 2020 accwa project grant agreement 823965 in the context of marie sklodowska curie research and innovation staff exchange rise program 
25473,fao 56 dual crop coefficient based model under various field conditions pierre laluet a luis olivera guerra a vincent rivalland a vincent simonneaux a jordi inglada a joaquim bellvert b salah er raki c d olivier merlin a a centre d etudes spatiales de la biosphère cesbio université de toulouse cnes cnrs ird ups toulouse france centre d etudes spatiales de la biosphère cesbio université de toulouse cnes cnrs ird ups toulouse france centre d etudes spatiales de la biosphere cesbio universite de toulouse cnes cnrs ird ups toulouse france b efficient use of water in agriculture program institute of agrifood research and technology irta parc científic i tecnològic agroalimentari de gardeny pcital fruitcentre 25003 lleida spain efficient use of water in agriculture program institute of agrifood research and technology irta parc científic i tecnològic agroalimentari de gardeny pcital fruitcentre lleida 25003 spain efficient use of water in agriculture program institute of agrifood research and technology irta parc cientific i tecnologic agroalimentari de gardeny pcital fruitcentre 25003 lleida spain c procede agrobiotech center département de physique appliquée faculté des sciences et techniques université cadi ayyad marrakech morocco procede agrobiotech center département de physique appliquée faculté des sciences et techniques université cadi ayyad marrakech morocco procede agrobiotech center departement de physique appliquee faculte des sciences et techniques universite cadi ayyad marrakech morocco d mohammed vi polytechnic university um6p center for remote sensing applications crsa morocco mohammed vi polytechnic university um6p center for remote sensing applications crsa morocco mohammed vi polytechnic university um6p center for remote sensing applications crsa morocco corresponding author fao 56 dual crop coefficient fao 2kc based model are increasingly applied at large scale for agricultural water monitoring requiring field scale data over the spatial extent of interest given the lack of in situ measurements satellite products can be used to estimate indirectly the parameters through calibration however a lack of knowledge about model sensitivity can lead to suboptimal use of satellite data this study aims to analyze the sensitivity of samir a fao 2kc based model using satellite data the sobol method was applied for evapotranspiration et and deep percolation dp simulations on 37 contrasted agricultural seasons results indicate that samir s sensitivity mainly depends on the modeled water stress we proposed a proxy for the model sensitivity which can determine 84 73 of the et dp among the agricultural seasons an interaction analysis allowed reducing the calibration problem to the adjustment of only two parameters a kcb and zr max accounting for most of the sensitivity keywords fao 56 model sobol sensitivity analysis evapotranspiration deep percolation remote sensing calibration data availability data will be made available on request 1 introduction irrigation is the most water intensive anthropogenic activity in the world the resources available for it are already under pressure in some regions and will be even more so in the future fao 2021 to face these challenges crop water balance models are widely used with the aim of optimizing agricultural water use pereira et al 2020 constantin et al 2015 such models seek to estimate the crop water consumption and irrigation needs by simulating all the terms of the crop water balance including evapotranspiration et corresponding to crop consumption and deep percolation dp being an indicator of water loss for crops the dual crop coefficient version fao 2kc of the fao 56 method allen et al 1998 is based on the estimation of actual et coupled to a soil water balance the fao 2kc method has been widely used in both operational and academic contexts for its efficiency and parsimony more than 30 000 citations of the fao 56 method pereira et al 2021 from the same basic fao 2kc formulation many models have been developed with their own specificities like the representation of additional processes e g runoff capillary rise a more detailed description of specific processes e g dp root development soil evaporation or the use of satellite data helman et al 2019 olivera guerra et al 2018 han et al 2018 bellvert et al 2018 campos et al 2017 yang et al 2012 rosa et al 2012a b raes et al 2009 lollato et al 2016 sheikh et al 2009 in the same vein the samir satellite monitoring for irrigation simonneaux et al 2009 model which is used in this work is a fao 2kc based model integrating remotely sensed normalized difference vegetation index ndvi to constrain the vegetative growth it includes 12 parameters related to soil and crop type characteristics the fao 2kc method can simulate et and dp at the plot or at the pixel scale which when aggregated can provide simulations of both fluxes at integrated spatial scales irrigation district or catchment using mapped input data kharrou et al 2021 garrido rubio et al 2020 bretreger et al 2019 the fao 2kc requires i maps of meteorological forcings ii maps of irrigation forcings iii maps of crop type to derive crop parameters and iv maps of soil texture to derive soil parameters those maps are becoming increasingly available and accurate thanks to the development of i reanalysis meteorological data sets at enhanced resolutions e g era5 hersbach et al 2020 safran vidal et al 2010 ii maps of actual irrigation type volume and timing derived from satellite observations massari et al 2021 iii crop type maps whether they come from field observations or classifications based on satellite observations foerster et al 2012 inglada et al 2015 and iv soil texture maps e g soilgrids hengl et al 2017 globalsoilmap arrouays et al 2017 however such data sets still have significant uncertainties due to intrinsic errors in the mapped data and to additional errors associated with their conversion to directly useable input parameters e g hydrodynamical soil properties are usually derived from soil texture maps poggio et al 2021 folberth et al 2016 loosvelt et al 2012 it is thus often necessary to calibrate the model input parameters using external data such a calibration strategy can be implemented at the field scale using in situ measurements kharrou et al 2021 saadi et al 2015 paredes et al 2014 er raki et al 2007 zhang et al 2013 or over extended areas using remotely sensed soil moisture or et data amazirh et al 2022 ouaadi et al 2021 er raki et al 2008 to reduce uncertainties in spatially distributed model input parameters many works have dealt with the assimilation of remotely sensed soil moisture brocca et al 2014 azimi et al 2020 zaussinger et al 2019 and et wu et al 2015 droogers et al 2010 products by minimizing sequentially and recursively the gap between simulations and observations it is possible to indirectly retrieve optimal values of a set of input parameters or at least to reduce their a priori uncertainty however this approach may face difficulties in terms of practical implementation due to its extensive requirement in terms of computational resources calibrating an agro hydrological model over large areas may indeed require a large number of simulations this is especially true when models have a considerable number of input parameters from a dozen simonneaux et al 2009 to several dozen neitsch et al 2011 and when they are spatialized over several thousands of pixels or fields in addition the calibration of many parameters from limited observations raises the issue of equifinality an equifinality occurs when several parameter sets lead to a result considered as optimal this can be problematic because each of these parameter sets does not necessarily have any likelihood with the physical reality of the parameters physical reality of which we can have prior knowledge beven and freer 2001 2006 a solution to the above concerns is to analyze the sensitivity of the studied model in order to identify and calibrate the parameters having the most influence on the outputs indeed focusing only on the most sensitive parameters may significantly reduce the required computer resources in addition to limiting the compensation issues between parameters even though models based on the fao 2kc method are widely used the study of their sensitivity to input parameters is not or almost not present in the scientific literature olivera guerra et al 2020 analyzed the sensitivity of three soil parameters soil moisture at field capacity at wilting point and the maximum roots depth zr max of samir using a local sensitivity analysis method i e by varying independently each of the parameters and looking at their impact on an objective function however for a comprehensive analysis and this is especially required when the number of analyzed parameters is larger global sensitivity analysis methods are strongly recommended saltelli and annoni 2010 song et al 2015 global sensitivity analysis methods can calculate the influence of input parameters over their entire range of variation and are appropriate for all types of models non linear and non monotonic the sobol variance based method sobol 1993 2001 saltelli et al 2008 2010 is a popular global sensitivity analysis method that has been used in many recent articles because of its robustness and of its ability to analyze interactions between parameters nossent et al 2011 baroni and tarantola 2014 tang et al 2007 zhang et al 2013 the principle of the sobol method is to decompose the total variance of a model s output and to look at how each uncertain parameter contributes to it whether this contribution is caused only by a single parameter or by the interaction of two or more parameters in this context the objective of this paper is to perform a sobol sensitivity analysis of the fao 2kc based samir model for both output fluxes et and dp to ensure the analysis results are representative of the field conditions where the fao 2kc method is generally implemented ten fields and a total of 37 agricultural seasons were selected based on their contrasted meteorological pedological and agricultural characteristics the paper is organized as follows the samir model data sets and the sobol method are first described section 2 then the results of the sobol sensitivity analysis are discussed for both et and dp section 3 with the overall goal of identifying the most influential parameters depending on actual field conditions finally the conclusions and perspectives are presented section 4 2 material and methods the overall methodology to assess the sensitivity of the samir model to its input parameters is presented in the flowchart of fig 1 first samir is presented together with its 12 input parameters section 2 1 then the input data composed of forcing precipitation irrigation ndvi crop type and soil properties are described section 2 2 the next three subsections describe the main steps of the sensitivity analysis approach that are calculating the sobol indices for each of the parameters and each target variable et and dp section 2 3 sampling the samir parameters with normal distributions defined for each agricultural season section 2 4 and running samir on the generated samples and computing performance metrics section 2 5 finally a proxy for the sensitivity of samir et and dp simulations is investigated from the variability of sobol indices section 2 6 2 1 satellite monitoring for irrigation samir model the samir simonneaux et al 2009 model used in this study is based on the fao 2kc method while a detailed description of the fao 2kc method is provided in allen et al 1998 only the main components are briefly reminded below as well as the main differences between this method and the samir model for a more detailed overview of the samir model readers are encouraged to refer to saadi et al 2015 the principle of the fao 2kc method is the calculation of crop water balance components for daily et estimation taking into account the plants and the soil water status it uses i meteorological forcing variables to calculate the reference et called et0 ii precipitation and irrigation amounts resulting in water available for et or for soil reservoir recharge iii crop and soil parameters to compute soil reservoir properties as well as plant and soil resistance to water stress and iv the initial soil water content at the start date of model simulations in addition to these variables and parameters the samir model incorporates remotely sensed ndvi time series to drive the development of the modeled vegetation the daily water balance equation simulated by samir is 1 p i e t d p δ s w with p being the precipitation i the irrigation et the actual evapotranspiration δsw the variation of soil water content from the previous day and dp the deep percolation being the water exceeding the maximum soil storage capacity the et calculation is done by applying crop coefficients to et0 as follow 2 e t e t 0 k c b k s k e where et0 kcb ks is the water transpired by plants t and et0 ke is the soil evaporation e in equation 2 et0 is calculated according to the fao penman monteith equation allen et al 1998 kcb is the basal crop coefficient following a linear relationship with ndvi ks is the water stress coefficient being a reduction factor of t and ke is the soil evaporative coefficient being related to vegetation fraction cover fc surface soil moisture and soil properties similarly to kcb fc follows a linear relationship with ndvi in addition to the use of ndvi time series samir incorporates two additional modifications from the classical fao 2kc method i the kr evaporation reduction coefficient accounting for soil evaporation resistance as a function of surface soil moisture is calculated with the method proposed by merlin et al 2016 this method instead of using an a priori parameterization uses a pedotransfer function based on clay and sand fractions fclay and fsand which was derived and evaluated over a variety of sites and soil textures merlin et al 2016 lehmann et al 2018 amazirh et al 2021 ii the soil moisture values at field capacity and at wilting point are calculated using fclay and fsand from the pedotransfer function proposed by román dobarco et al 2019 samir uses a total of 12 user defined parameters six parameters related to the plants phenological stage and t capabilities that we named the pheno parameters five parameters related to soil reservoir properties and plant resistances to stress that we named the stress parameters and one parameter used to set the soil water content at the start date of model simulations a detailed description of the parameters is provided in table 1 with their definition and the processes in which they are involved 2 2 sites and data description data of 37 agricultural seasons from ten different crop fields around the world were used they were obtained from national and international databases or from specific intensive field campaigns the agricultural seasons cover a wide variety of agro pedo climatic conditions that reflect the contexts of use of samir i e different crop types mainly located in dry areas where water related agricultural issues are important but also and to a lesser extent in temperate areas where et estimation may also be key for a good water management the data set involves 13 crop types including summer and winter cereals vegetables and fruit trees 10 soil textures ranging from clay to silty loam 4 irrigation types flood irrigation sprinkler drip irrigation no irrigation and 2 different climates semi arid and temperate table 2 reports the characteristics of each agricultural season the data used for running samir are composed of i meteorological variables obtained from local weather stations for precipitation and et0 calculation air temperature wind speed solar radiation and relative air humidity ii irrigation dates and amounts obtained from water meters iii crop types iv soil texture fclay and fsand and v ndvi time series obtained from sentinel 2a and 2 b and landsat 7 and 8 the sentinel 2 constellation provides a 10 m resolution pixel with a temporal resolution of 10 days from 2015 to 5 days since 2017 in clear sky conditions landsat 7 provides 30 m resolution pixels with a 16 day time resolution since 1999 landsat 8 has the same characteristics but has been available since 2013 and its overpass is offset by 8 days with respect to landsat 7 it is important to note that for all agricultural seasons the periods studied ranged from the beginning of the vegetation development to the end of the senescence period periods between successive agricultural seasons with bare soil or low evaporative demand were not considered herein because this study only assessed the sensitivity of et and dp when the major part of crop s water consumption occurs 2 3 sobol sensitivity analysis 2 3 1 sobol indices a general overview of the sobol method is presented here while more detailed descriptions can be found in saltelli et al 2008 2010 or khorashadi zadeh et al 2017 consider a model y f x f x1 xk with k parameters where y is the model output e g a performance metric such as the root mean square difference and x x1 xk the parameter set which can be decomposed into 2k terms representing different order of interaction between parameters 3 f x 1 x k f 0 i 1 k f i x i i 1 k j i 1 k f i j x i x j f 1 k x 1 x k the total output variance of the model v y can be decomposed into corresponding partial variances 4 v y i 1 k v i i 1 k 1 j i 1 k v i j v 1 k the first order index si also called main effect is the ratio between vi in equation 4 and the total output variance v y si can be written as follows 5 s i v i v y v x i e x i y x i v y the total index sti represents the sum of xi s main effect with all its higher order interactions up to order k v i is the variance resulting from the contribution of all parameters except xi it can be written as follows 6 s t i 1 v i v y e x i v x i y x i v y sti index is the one we used in this study to evaluate the sensitivity of samir parameters indeed sti unlike si integrates all the influence a parameter has on a model output which is of interest to determine the most sensitive parameters si is used to calculate the interactions a parameter has with the others by calculating the difference between sti and si if the sum of the si is equal to 1 it means that the model is linear and that there is no interaction between the parameters on the contrary if the sum of the si is smaller than 1 the model is non linear the lower the sum of the si the more the parameters of the model interact with each other the safe toolbox pianosi et al 2015 was used in this study in order to perform the sobol indices calculation and generate the samples 2 3 2 convergence analysis for each agricultural season a convergence analysis of si and sti was done in order to verify that the number of simulations is sufficient to ensure their stability results not shown indicated that n n k 2 33 000 with n 3000 and k 9 parameters is sufficient for all the 37 agricultural seasons 2 3 3 bootstrapping in order to optimally take into account the uncertainties related to the distribution of the model outputs in the sti and si values we used a bootstrapping technique was applied efron and tibshirani 1994 this technique consists of randomly resampling the 33 000 model outputs 1000 times and recalculating the sobol indices for each resampled data set then the average of si and sti is derived from the 1000 resamples and can be kept for the sensitivity analysis 2 3 4 dummy parameter a dummy parameter a parameter added to the analysis that we know has no influence on the model output was introduced in this sensitivity analysis for further explanation on the calculation of the dummy parameter readers are encouraged to refer to khorashadi zadeh et al 2017 the sti calculated for the dummy parameters were used herein as thresholds to identify sensitive parameters from insensitive parameters they were also used to ease the readability of the sti values by normalizing them for each agricultural season between the dummy parameter s sti and the sum of all parameters sti it is then called stinorm and is expressed as a percentage 2 4 parameters sampling 2 4 1 latin hypercube sampling despite its popularity the sobol sensitivity analysis method can be challenging to implement due to its high computational cost it requires a total of n n k 2 samples with n a baseline sample size that can vary between 1000 to more than 10 000 and k the number of analyzed parameters to optimize the sampling efficiency the latin hypercube sampling method lhs was used lhs is a monte carlo based method using a stratified sampling approach where the distribution of each parameter is divided into p ranges each with a probability of occurrence of 1 p parameter values are randomly generated so that each range gets sampled only once the same step is repeated for each of the k parameters to generate a matrix of size p k with random sample combinations of the different parameters the lhs method has been used in many studies e g campolongo et al 2011 saltelli and annoni 2010 zhang et al 2013 tang et al 2007 and song et al 2015 2 4 2 parameters mean and standard deviation nine parameters were sampled and analyzed out of the 12 included in samir of these three parameters were fixed kcb max kc max and init hum see table 1 for kcb max and kc max a previous analysis not shown performed on six contrasted agricultural seasons indicated that their sensitivity is negligible the main explanation lies in the way the kcb ndvi relationships are constructed using minimum and maximum ndvi values from satellite observations limiting the number of days when these parameters can influence et regarding init hum we considered it as a forcing data and set it to its median value 0 5 because soil moisture measurements at the beginning of agricultural seasons were not available for all sites the parameters analyzed were sampled according to a normal distribution using mean and standard deviation values to make the link with the lhs method mentioned in section 2 4 1 the normal distributions obtained for each parameter were divided into p intervals of equal probability intervals being then more or less narrow depending on the parameter s distribution the mean values of the nine parameters were defined for each agricultural season from i field analyses for fclay and fsand ii literature references for zr max p and ze and iii satellite observations for ndvi related parameters a fc b fc a kcb and b kcb the standard deviation of crop related parameters zr max p a fc b fc a kcb and b kcb vary according to the crop type it allows us to represent the uncertainty of a given parameter as a function of the mean value of this parameter and therefore according to the crop type for example zr max of broccoli crops having a mean zr max of 500 mm has a smaller uncertainty and therefore a smaller standard deviation than zr max of maize crops having a mean value of 1050 mm since maize has been extensively investigated the standard deviations of its parameters were chosen as a reference therefore the ratios between the standard deviation and the mean obtained for each of the maize parameters was applied to the mean values of the corresponding parameters of the 12 other crop types to derive their standard deviation regarding the soil related parameters the mean and standard deviation of ze were fixed for all the 37 agricultural seasons as well as the standard deviations of fclay and fsand table 3 summarizes the mean and standard deviation values used to generate samples for the nine analyzed parameters 2 5 outputs for sobol indices computation root mean square deviation the root mean square deviation rmsd performance metric was used to calculate the sobol indices for each of the 37 agricultural seasons 33 000 rmsd were calculated for both et and dp corresponding to the 33 000 parameters samples used for samir simulations the rmsd formula is written as follows 7 r m s d i 1 j ŷ i y i 2 j where j is the number of days in the simulated time series i is one day of the time series ŷi is a simulated variable time series and yi is a reference variable time series in this study the reference variable time series yi were obtained for each agricultural season by averaging the 33 000 simulated time series 2 6 deriving a fao 2kc sensitivity proxy spfao 2kc from field conditions once the sobol indices were obtained for the nine parameters a correlation was sought between them and the soil vegetation atmosphere characteristics of the 37 agricultural seasons the idea was to confront the sobol indices with different criteria soil texture crop type cumulative rainfall mean et0 mean ndvi modeled crop water stress level etc until a satisfactory correlation was observed from this correlation a proxy of the sensitivity of the samir parameters spfao 2kc has been proposed 3 results and discussion the results of the sobol sensitivity analysis of samir are presented and discussed in this section sections 3 1 3 3 focus on the model sensitivity for et simulations while section 3 4 is about the dp simulations first the parameter sensitivity obtained for the 37 agricultural seasons are investigated section 3 1 then a fao 2kc sensitivity proxy spfao 2kc is proposed to explain and predict the samir s parameter sensitivity from the on site characteristics solely section 3 2 next the interactions between parameters are studied to better assess their sensitivity and to be able to select a minimum parameter set for calibration section 3 3 finally for dp the similarities and differences with the et case are discussed and the main results are presented section 3 4 3 1 parameters sensitivity for et simulations each group of pheno a fc b fc a kcb b kcb and stress zr max p fclay fsand ze parameters is involved in distinct and clearly identified processes as explained in section 2 1 this leads us to hypothesize that these two groups will have significant differences in terms of stinorm values for each pheno and stress group we summed the stinorm of their parameters and named these sums pheno stinorm and stress stinorm table 4 reports the sti si and stinorm values obtained for three selected agricultural seasons which were found to reflect well the different sensitivity types lam wheat 11 is a stress sensitive agricultural season cat maize2 21 a phenology sensitive one and mex chilli 08 has a balanced sensitivity a stress sensitive phenology sensitive agricultural season is characterized by a stress stinorm pheno stinorm greater than 66 respectively similarly a balanced agricultural season has a pheno stinorm and a stress stinorm between 33 and 66 in table 4 the first column corresponding to lam wheat 11 shows a stress stinorm equal to 81 it means that for this agricultural season the five stress parameters account for 81 of the rmsd variation and thus for 81 of the total sensitivity of the samir s parameters among these parameters zr max p and fclay showed to be the most sensitive having higher stinorm ze which has a sti value equal to the one of the dummy parameter ends up with a stinorm equal to 0 pheno stinorm of cat maize2 21 is equal to 100 indicating that it corresponds to a phenology sensitive site the pheno stinorm of mex chilli 08 is equal to 64 reflecting a balanced sensitivity fig 2 a shows boxplots of the stinorm of the nine parameters analyzed separately and fig 2b shows the boxplots of the stinorm of the parameter groups pheno stinorm and stress stinorm obtained for the 37 agricultural seasons two elements stand out in this figure i a kcb b kcb and zr max have in most cases a larger stinorm than the other parameters with however a large dispersion first quartile q1 is 0 1 for a kcb b kcb and zr max and third quartile q3 is 0 28 for a kcb and b kcb and 0 37 for zr max ii pheno stinorm and stress stinorm fig 2b are nearly identical with a large dispersion for both parameter groups q1 and q3 are 0 23 and 0 78 respectively this large dispersion indicates that from one agricultural season to another and therefore from one agro pedo climatic context to another the parameter sensitivity can vary significantly in fig 3 results are presented grouped by the three types of sensitivity found within the agricultural seasons stress sensitive phenology sensitive and balanced fig 3a shows that for the majority of the 12 phenology sensitive agricultural seasons a kcb and b kcb have the largest stinorm fig 3c shows that for the 15 stress sensitive agricultural seasons zr max has the largest stinorm with a mean value of 36 for these agricultural seasons fclay also shows a certain level of sensitivity mean stinorm is 18 in fig 3e showing the 10 balanced agricultural seasons the three parameters a kcb b kcb and zr max already identified as the most sensitive stand out the fact that very contrasting or even opposite types of sensitivity were identified between the 12 phenology sensitive and the 15 stress sensitive agricultural seasons confirms the relevance of gathering the parameters into pheno and stress groups figs 2 and 3 highlight distinct sensitivity types being well represented among the 37 agricultural seasons they also indicate which parameters are the most sensitive depending on the type of agricultural season a kcb and b kcb for the phenology sensitive ones dominated by the pheno parameter group zr max for the stress sensitive ones dominated by the stress group and a kcb b kcb and zr max for the balanced ones 3 2 searching for spfao 2kc for et simulations as a step further we tried to find a proxy for samir sensitivity spfao 2kc based on the agro pedo climatic characteristics of agricultural seasons different criteria were tested and confronted with the stinorm of the parameter groups pheno stinorm and stress stinorm for all agricultural seasons through trial and error a good correlation emerged r2 0 84 between the modeled stress intensity in the root zone calculated with an average set of parameters and the stinorm of the parameter groups fig 4 formally spfao 2kc is written as follows 8 s p f a o 2 k c i 1 d k s i d i f k s i 1 a n d k c b i k c b max 0 2 with d being the number of days of the simulated time series when the crop is stressed ks lower than 1 and when the crop coefficient kcb is larger than 20 of its maximum value i being a specific day meeting this condition ksi being a daily value of ks and kcbi being a daily value of kcb to simplify the interpretation of spfao 2kc we normalized it between the minimum 0 41 and the maximum 0 93 of the spfao 2kc values obtained among the 37 agricultural seasons spfao 2kc can be understood as follows the more an average set of parameters of an agricultural season generates intense crop water stress levels related to ks values lower than 1 at times when potential t is significant the more the parameters belonging to the stress group will weigh in the model sensitivity to figure out why this indice emerged rather than another we must seek to better understand how samir works and how the different modeled processes influence the rmsd of simulated et fig 5 shows the mean time series of four samir outputs for lam wheat 11 and cat maize2 21 and their associated uncertainties represented with q1 and q3 lam wheat 11 is a non irrigated winter wheat crop with significant precipitation in winter and spring until april may followed by a decrease in rainfall resulting in water stress from april it is a stress sensitive agricultural season stress stinorm is 81 cat maize2 21 is a heavily irrigated summer maize crop with no water stress ks 1 throughout the season it is a phenology sensitive agricultural season with a pheno stinorm equal to 100 simulated time series of lam wheat 11 and cat maize2 21 illustrated in fig 5 and the stinorm values shown in table 4 provide keys to understand the relevance of spfao 2kc equation 8 i in fig 5 we see that there are more uncertainties associated with t than with e this explains why a kcb b kcb and zr max stand out to be more sensitive table 4 as they are related to t process it also explains why the parameters fclay fsand ze and even a fc and b fc are less sensitive as they are partly or entirely associated with e process such differences in the level of uncertainties between t and e can be explained by the fact that i the simulated time series of the 37 agricultural seasons include few bare soil periods fraction cover higher than 0 75 most of the time and ii the formalism for e simulation generates less uncertainty than the formalism related to t simulation ii the uncertainties in t and e related to a kcb and b kcb are practically constant during the whole simulated period an uncertainty associated to kcb is always present on each simulated day but are considerably less important than the uncertainties in t associated to water stress i e when ks is lower than 1 iii during the simulated periods when ks is lower than 1 the uncertainties associated with t can be very large if the water stress lasts long enough while ks is decreasing the stress parameters and thus mostly zr max become more sensitive than the pheno parameters as is the case for lam wheat 11 iv when there is no water stress ks equal to 1 as for cat maize2 21 the uncertainties associated with t are entirely related to the pheno parameters and thus essentially to a kcb and b kcb the above four points help understand why spfao 2kc based on the modeled crop water stress intensity emerged as an efficient proxy for the model sensitivity indeed when there is little water stress ks close to 1 the uncertainties generated by the stress parameters are low this is explained by the fact that ks is a bounded variable whose maximum value is fixed at 1 resulting in limiting the uncertainties caused by the stress parameters when ks is close to 1 by contrast when there are more intense stress episodes the soil moisture content of the root reservoir and thus the ks value takes time to decrease generating a longer period with larger uncertainties associated with t consequently the longer and more intense the stress episode is the more t simulations will be affected by the stress parameters and the more they will weigh in the final sensitivity of the agricultural season note that although the correlation between spfao 2kc and pheno stinorm is close to 1 the relationship shows a significant variability when pheno stinorm is below 30 this is because spfao 2kc is less effective for agricultural seasons dominated by the stress parameters with low pheno stinorm 3 3 interactions between parameters fig 6 shows boxplots with the total interactions of the nine analyzed parameters for the 37 agricultural seasons for et simulations the total interactions of a parameter calculated by subtracting si to sti reflect how this parameter indirectly affects the sensitivity of all the other parameters when its value is changed the knowledge of the total interactions coupled with a certain knowledge of the samir model gives enough information to select the minimum parameter set to be calibrated among the parameters being the most sensitive the knowledge of samir that is of interest here is that the pheno and the stress parameter groups are involved in very different processes and have little relationship with each other as mentioned in section 2 3 1 for a given agricultural season the lower the sum of its si the higher the general level of interaction between its parameters here the 37 agricultural seasons have globally important levels of interactions since the sum of their si is on average relatively low 0 48 with a minimum of 0 17 and a maximum of 0 68 in fig 6 we can see that among the pheno group a kcb and b kcb concentrate most of the total interactions this makes sense since they are part of the same linear relationship linking kcb to ndvi this means that when a kcb is modified for calibration it greatly affects the b kcb sensitivity and conversely therefore calibrating only one of these two parameters is somehow equivalent to calibrating the kcb ndvi relationship in addition to avoiding compensation effects between them herein we chose to calibrate a kcb rather than b kcb because a kcb has a greater influence on the vegetation growth being a time when the t demand is generally high regarding the stress group almost all the total interactions are shared between zr max fclay and fsand these three parameters interact strongly with each other as they are all involved in the size of the root reservoir which is the key element governing the occurrence and intensity of water stress therefore as with a kcb and b kcb calibration of only one of these three parameters can be sufficient to indirectly calibrate the root reservoir we recommend calibrating zr max rather than fclay and fsand because zr max appeared to be the most sensitive and because no validated maximum root depth maps are currently available as opposed to soil texture maps from this parameter interactions analysis it appears that out of the nine parameters only two a kcb and zr max can explain most of the model sensitivity these parameters can be calibrated either together or individually depending on the value of spfao 2kc and the available computer resources 3 4 parameters sensitivity for dp simulations and relationship with agricultural season characteristics this section deals with the sensitivity of the rmsd calculated for dp simulation it goes into less detail than for et simulation and focuses on the similarities and differences between dp and et cases herein only 31 of the 37 agricultural seasons were analyzed because six had no dp event as for the et case the parameters were gathered into stress and pheno groups and the most sensitive ones were a kcb b kcb zr max and to a lesser extent fclay and fsand not shown the sensitivity proxy for dp simulations spfao 2kc dp is similar to the one proposed for et spfao 2kc with the only difference being that the period used to calculate it excludes the days following the last dp event indeed the days following the last dp event and their associated agro climatic conditions do not count in the generation of any dp event fig 7 shows the relationship between spfao 2kc dp and pheno stinorm for dp simulations a determination coefficient of 0 73 was obtained the lower correlation obtained for dp than for et r2 0 84 can be explained by the higher complexity of the parameters sensitivity for dp simulations indeed in samir the dp events are punctual and each of them has its own sensitivity according to what occurred during the period just before the final sensitivity of the parameters can thus be seen as the product of several stinorm obtained for each dp event also especially for agricultural seasons with few dp events the period prior to the last dp events may have involved a significant portion of bare soil which would potentially lead to a higher sensitivity of the evaporation related parameters this lower correlation may also be related to the fact that the 31 agricultural seasons are less homogeneously distributed between phenology sensitive and stress sensitive agricultural seasons with 21 agricultural seasons being phenology sensitive and only four being stress sensitive 3 5 overall results in brief the sobol sensitivity analysis for both et and dp simulations as well as the sensitivity proxy spfao 2kc and the total interaction analysis revealed several guidelines for the calibration of the fao 2kc based model samir i for spfao 2kc greater than 0 66 phenology sensitive agricultural season a kcb should be selected for calibration ii for spfao 2kc lower than 0 33 stress sensitive agricultural season it should be zr max iii for spfao 2kc between 0 33 and 0 66 balanced agricultural season it should be both a kcb and zr max note that the above recommendations only apply in the case where the number of parameters to be calibrated should be minimized at maximum because of computer resources limitation or equifinality issues if there are no such issues it is possible to calibrate both a kcb and zr max in every case also by focusing either on a kcb or zr max only certain aspects of the simulated time series would be considered in the calibration for example if only a kcb is calibrated for a phenology sensitive agricultural season with a spfao 2kc equal to 0 70 thus having some water stress periods the days when et or dp are impacted by crop water stress would not be considered in the calibration because a kcb only affects the t demand in contrast if only zr max is calibrated for a stress sensitive agricultural season having a spfao 2kc equal to 0 30 the focus is only given to the periods when the amount of t is reduced by water stress neglecting the days without water stress in the calibration process in summary depending notably on the computer capacities available we recommend that the user calibrate both zr max and a kcb together or choose between the two based on the spfao 2kc calculated for the studied agricultural season see table 5 moreover by choosing to calibrate only a kcb and or zr max and thus only t related parameters the days with bare soil or very little vegetation are not or poorly taken into account in the calibration we found that under the conditions of the studied agricultural seasons the parameters related to e appear to be much less sensitive than the parameters associated with t as shown and explained in section 3 1 however it is worth mentioning that the parameters related to e may be important to consider on specific agricultural seasons not included in this study with long and wetted bare soil periods 4 conclusion fao 2kc based models are increasingly applied in a spatialized way to simulate the inward and outward water fluxes over extended agricultural areas e g irrigation district it requires knowing the sensitivity of the uncertain input parameters in order to be able to calibrate them spatially and optimally and to face computational and equifinality issues however although these models have been widely used no proper sensitivity analysis has yet been done to fill the gap this paper investigated the sensitivity of the fao 2kc based crop water balance model samir for the simulation of et and dp considering the potential influence of different site characteristics for this purpose we applied the sobol sensitivity analysis method on 10 instrumented sites and a total of 37 agricultural seasons being diverse in terms of climate pedology and agricultural practices sobol sensitivity indices were calculated for nine samir parameters and a correlation between them and the agricultural season s conditions was sought sobol s sensitivity indices indicate that three among the nine parameters stand out weighing on average 63 in the sensitivity of samir et a kcb and b kcb related to phenology and zr max related to crop water stress it also appears that the importance of the pheno and stress parameter groups in the samir sensitivity varies greatly from one agricultural season to another depending on the modeled crop water stress intensity a proxy for the sensitivity of samir spfao 2kc has thus been proposed as the average of the crop stress coefficient ks on the days when there is crop water stress ks lower than 1 and when the crop coefficient kcb is larger than 20 of its maximum value kcb 0 2 kcb max spfao 2kc is able to determine 84 of the variability in samir et sensitivity 73 for the dp case among all agricultural seasons considered the total interactions analysis coupled to our knowledge of the samir model revealed a strong interaction between a kcb and b kcb as well as between zr max fclay and fsand this further highlights the importance of the sensitivity of a kcb b kcb and zr max and led us to retain only a kcb and zr max for calibration if the user has no limitation in terms of computing capacity he can calibrate both a kcb and zr max if he is faced with such constraints and needs to optimize the number of parameters to be calibrated he can use spfao 2kc value computed from a simulation performed with an average parameter set when spfao 2kc is lower than 0 33 we recommend calibrating zr max when it is higher than 0 66 a kcb and when it is between 0 33 and 0 66 both a kcb and zr max these results represent a solid basis for spatializing fao 2kc based models using remotely sensed data through notably the distributed calibration of their input parameters such a calibration strategy could rely on the soil moisture products derived from smos smap e g ojha et al 2019 paolini et al 2022 or sentinel 1 e g el hajj et al 2017 and on et products derived from landsat 8 senay et al 2016 or sentinel 2 and sentinel 3 e g guzinski et al 2020 ongoing missions in addition new satellite missions will be launched in the coming years such as trishna lagouarde et al 2018 and lstm koetz et al 2018 which will provide field scale et estimates at an unprecedented frequency software availability samir is an open source software and is available at the following address https gitlab cesbio omp eu modelisation modspa documentation can be found by visiting this link the code is implemented in python 3 language contact information vincent rivalland cesbio cnes fr declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this study was supported by the idewa project anr 19 p026 003 of the partnership for research and innovation in the mediterranean area prima program and by the horizon 2020 accwa project grant agreement 823965 in the context of marie sklodowska curie research and innovation staff exchange rise program 
25474,this paper proposes a new method for rapid prediction of wildfire spread which employs computational wildfire simulations by farsite and assimilates the simulation results with actual observation data by means of an ensemble kalman filter to expedite data assimilation the wildfire perimeter is represented by a two dimensional polyline simplification algorithm in addition to facilitate the data assimilation a new process is developed to relate the prediction results with the actual observation data the proposed method is tested and demonstrated by an example wildfire spread scenario generated based on actual climate topography and vegetation the results confirm that the polyline simplification algorithm can drastically reduce the computational time required for data assimilation while maintaining the accuracy of predictions the proposed method is expected to serve as a core algorithm for near real time prediction and data driven updating of wildfire spread keywords wildfire farsite ensemble kalman filter data assimilation near real time prediction polyline simplification data availability data will be made available on request 1 introduction recently wildfire has become one of the major catastrophes around the world for example the total insurance cost of the record breaking 2020 wildfire season in the western u s was estimated as 13 billion risk management solutions 2020 the most common natural causes of wildfire ignition include lightning volcanic eruptions and spontaneous combustions from dry climates man made causes of wildfire ignition are arson discarded cigarettes and slash and burn clearing due to these various causes and climate changes wildfires steadily occur whereas their suppression becomes more challenging and costlier national interagency fire center 2020 therefore an accurate short term prediction of wildfire spread is crucial to minimize the damage and cost of wildfire through effective suppression activities however it is exceedingly difficult to identify all the various factors influencing wildfire spreads even if identified it is challenging to incorporate the factors into the prediction process therefore the errors of wildfire predictions generally accumulate as the spread continues to address the accumulation of prediction errors and improve prediction accuracy mandel et al 2004 proposed data driven wildfire modeling that can improve the wildfire spread prediction accuracy based on actual wildfire observation data later an ensemble kalman filter enkf evensen 1994 2009 was adopted to assimilate the model based prediction with actual wildfire observation data mandel et al 2008 this method applied the data assimilation to newly developed hydrodynamic models simulating wildfire spread as a result the prediction accuracy of wildfire spread and related parameters were significantly improved gps drones and satellites are generally used to gathering actual wildfire observation data however it is often difficult to obtain high accuracy real time data due to performance limitations and various errors accordingly wildfire observation data is generally used for data assimilation using bayesian filters enkf is considered an effective option in wildfire prediction among various bayesian filters because it is suitable for handling problems with a large number of variables and is efficient when applied to nonlinear systems however because wildfire prediction models are extremely irregular and nonlinear it is important to improve the efficiency of enkfs using wildfire observation data zhang et al 2019 improved the data assimilation using thermal infrared imaging data of wildfires subramanian et al 2020 improved the accuracy of the approach while reducing the computational time when stationary points occurred due to external factors in addition to enkf extended kalman filter ekf and unscented kalman filter ukf julier and uhlmann 1997 are widely applied to nonlinear systems such as wildfire spread ekf is the most basic nonlinear kalman filter and approximates the state distribution through gaussian random variables after linearizing the nonlinear system through the jacobian matrix the estimate is updated however ekf is not suitable for extremely nonlinear wildfire prediction because it causes a large error in the posterior mean and covariance of each variable due to linearization ukf is a nonlinear kalman filter further developed from ekf a properly selected set of minimal sample points represents the entire state distribution ukf can express the posterior mean and covariance of each variable in the nonlinear system up to the 3rd order without error a minimal number of sample points express these is called sigma points set by using this ukf can make nonlinear predictions with much higher accuracy than ekf however even in ukf nonlinear systems relying on higher than 3rd order cause a large error in posterior mean and covariance prediction and the state distribution of the hydrodynamic model used for simulation may correspond to this in addition since the number of variables involved in the wildfire spread is significantly large the number of sigma points to obtain sufficient accuracy is excessive which is not suitable in terms of computational cost on the other hand various computational models and software henceforth termed simulators have been developed to provide base predictions of wildfire spread examples of wildfire spread simulators using computational fluid dynamics cfd include firetec linn and harlow 1997 and wfds mell et al 2007 these simulators are known as one of the best options to consider the underlying physics of the problem but unsuitable for real time wildfire spread predictions because they generally require exceedingly large computational cost and time examples of regional scale wildfire spread simulators include farsite finney 1998 and prometheus tymstra et al 2010 these consider the underlying physics less than cfd based simulators but suitable for real time wildfire spread prediction because of their efficiency however these regional scale simulators do not support data assimilation which could improve their prediction accuracy without compromising efficiency recently research efforts have been conducted to improve the prediction accuracy by combining regional scale simulators with data assimilation for example srivas et al 2016 proposed an algorithm that applies enkf based data assimilation to wildfire spread prediction using farsite zhou et al 2019 applied ensemble transform kalman filter etkf which was originally developed to ignore perturbed observations required for the updating part in the enkf to predict wildfire spread using farsite subsequently zhou et al 2020a introduced a vertex weight based etkf that assigns different weights based on the accuracy of observations rios et al 2019 developed a data driven wildfire simulator smartqfire based on marker tracking implementation another notable attempt to improve the efficiency of wildfire simulators was to determine the rate of spread ros adjustment factor through data assimilation for instance srivas et al 2017 expressed the perimeter and ros adjustment factors in one vector and updated this vector through data assimilation cardil et al 2019 proposed a method to obtain ros adjustment factors in real time using the least square method zhou et al 2020b optimized ros adjustment factors by applying a radial basis function neural network responses to wildfire spread during the firefighting operation generally require a significant amount of time due to topographical and environmental conditions of mountainous regions therefore for effective suppression of wildfires minimizing damage and losses sufficient preparation time must be given to decision makers in this regard rapid prediction of wildfire spread is just as important as an accurate prediction the previous studies summarized above have significantly improved the accuracy of regional scale simulators by using various methods however the process introduced to improve the accuracy inevitably increases the computational cost therefore this paper proposes a new algorithm to reduce the time required for enkf data assimilation of wildfire spread prediction using farsite each perimeter ensemble obtained from farsite is expressed as vectors consisting of coordinate pairs since the shape of each perimeter ensemble is different the lengths of the vectors are also different therefore a re interpolation based unification of the lengths is required to apply the enkf based data assimilation given that the most important factor determining the computational time for data assimilation is the length of the ensemble vector representing the prediction results the length is reduced by applying a two dimensional polyline simplification algorithm in this process topological relationships between perimeters must be maintained while applying polyline simplification in addition to further facilitate the data assimilation a new method is developed to relate points constituting each ensemble with the actual observation points to this end the new algorithm proposed in this paper has an additional part termed as simplification and re interpolation between the prediction and updating parts of the standard enkf in this additional part the results of the prediction part are processed such that data assimilation can be conducted in the subsequent updating part in the following section we briefly review the regional scale wildfire spread simulator farsite and the two dimensional polyline simplification algorithm preserving the topological relations section 3 proposes a rapid enkf based wildfire spread prediction algorithm section 4 demonstrates the proposed algorithm through its applications to a wildfire spread scenario and verifies its performance in terms of the computational time and error for different levels of polyline simplification especially in order to confirm the efficiency of the algorithm in various situations we compare the difference in prediction results caused by the initial wildfire observation scenario and the ros adjustment factor finally section 5 provides the summary and future research topics 2 theoretical background 2 1 farsite wildfire spread simulator farsite finney 1998 is a two dimensional wildfire spread simulator used by the u s forest service and national park service the simulator can predict wildfire spread and various behaviors for designated time periods under heterogeneous conditions of fuel moistures weather streams ignitions barriers ros and terrains farsite incorporates various fire behaviors such as crown fires wagner 1977 spotting fires albini 1979 dead fuel moistures nelson 2000 into the two dimensional surface fire growth model by rothermel 1972 these models are integrated using a vector propagation technique that can control both spatial and temporal elements using the integrated fire behavior models farsite can provide various outputs related to wildfires such as arrival times flame lengths spread directions wildfire perimeters and wildfire spread vectors farsite requires a set of parameters describing various environmental factors of wildfire spread these parameters can be categorized into three types 1 terrains and fuels 2 weather conditions and 3 ros adjustment factor the parameters in the first category characterize terrains and fuels in terms of topography and vegetation they are invariant temporally but variant spatially a farsite landscape file contains these parameters supported by the landfire project rollins 2009 this file represents raster maps that describe terrains and fuels of each location in terms of elevation slope aspect canopy cover and fuel model each fuel model requires 1 h fuel moisture content 10 h fuel moisture content 100 h fuel moisture content live herbaceous moisture content and live woody moisture content there are two commonly used sets of fuel models first the fire behavior fuel models fbfm by anderson 1981 consist of 13 models describing fuel type loading surface area to volume ratio fuel bed depth and moisture of extinction on the other hand the fbfms by scott 2005 consist of 40 models this approach uses more models in each fuel type in order to make fuel models employing live herbaceous component dynamic the parameters in the second category i e weather conditions are temperature relative humidity hourly precipitation amount wind speed wind direction and cloud cover percentage on an hourly basis these parameters are variant temporally and invariant spatially while actual weather conditions vary slightly over space in particular because wind speed and wind direction vary greatly depending on the location in the mountainous terrain an external program is needed to calibrate the wind speed and direction for each grid based on the mean value a wind calibration program windninja forthofer et al 2009 is supported within farsite since windninja is a mass conserving model it is useful for rapid prediction of farsite but on the other hand the predictions might not be particularly faithful to observations it is also possible to apply the custom wind variables in a gridded format directly as for the third category a constant value is assigned to the ros adjustment factor for each fuel model in farsite based wildfire spread predictions errors may occur between the prediction and the actual wildfire spread even if all input parameters are exact this is because given that the input parameters related to weather conditions are spatiotemporally homogenized in a specific range prediction errors occur and increase over large areas and long spans of time errors arise also from the difficulty in predicting the wildfire spread rate due to the nonlinear relationship between wind speed fire acceleration and fire spread rate richards 1993 the adjustment factor introduced to correct spread rates is defined as the ratio of the actual spread rate to the predicted one for each fuel model these factors can be estimated from empirical observations of previous wildfires or obtained in real time using the enkf srivas et al 2017 least squares method cardil et al 2019 or radial basis function neural network zhou et al 2020b 2 2 two dimensional polyline simplification algorithm preserving topological relations in filter based prediction or updating of a geometric shape e g boundary of a wildfire the complexity of a shape inevitably increases computational cost a polyline simplification or polygon simplification algorithm simplifies polylines or polygons with inordinate complexity while satisfying the given constraint conditions e g the maximum number of points the maximum error between before and after simplification on the other hand efforts to reduce the complexity may lead to losing important details to minimize the error of the simplified shape while satisfying the given conditions several simplification algorithms have been developed for example the douglas peucker algorithm douglas and peucker 1973 is used when the maximum error is predetermined and the visvalingam whyatt algorithm visvalingam and whyatt 1993 is used when the maximum number of points is predetermined however caution is needed in applying a polyline simplification to multiple polygons fig 1 shows an example of an undesirable point removal a single line segment p r is generated by applying polyline simplification to two consecutive line segments p q and q r line segments of other adjacent polygons a b and b c do not intersect with p r however if the polygon includes b instead of b a b and b c generate intersections with p r which changes the topological relationship of multiple polygons if the wildfire spread estimation time interval is short or the wildfire spread rate is slow applying the general polyline simplification algorithm may change the topological relationship between wildfire perimeters as shown above because data assimilation is applied to obtain the final wildfire spread prediction results intersections may occur between the final wildfire spread prediction result of the previous time step and that of the current time step however no section of the wildfire spread prediction results at the current time step before applying data assimilation exists inside those of the previous time step therefore the polyline simplification algorithm should be able to preserve the topological relationship in this case if the two perimeters overlap each other it is regarded that the topological relationship does not change the reason is explained at the end of section 2 2 to this end the polyline simplification algorithm by dyken et al 2009 is employed in this paper the algorithm only removes points that satisfy the following requirements 1 the existing intersections are maintained 2 new intersections are not generated 3 polylines are not degenerated into a single point and 4 polygons are not degenerated into a single line segment to implement this rule the triangulation technique is used fig 2 shows an example of polyline simplification using triangulation before applying the polyline simplification we perform a triangulation that includes all points and line segments of polylines and polygons let us first consider the case of removing b from the left part in the figure select points a and c that are adjacent to b on the polyline containing b afterward generate polygon b i e the grey region in the left part that is the union of all triangles which have b as a corner at this time because the line segment a c is completely inside of b b can be removed without altering the topological relationship next let us consider the case of removing b from the right part in the figure define a c and b in the same way as above in this case because the line segment a c intersects with the boundary of b it is not possible to remove b without changing the topological relationship in addition even if a c is completely outside of b b cannot be removed either depending on the shape of triangulation both situations can occur in the same polyline the polyline simplification algorithm using triangulation is computationally intensive but its use is inevitable to maintain the topological relationship a method that can reduce the time required for polyline simplification while preserving the topological relationship is desirable to reduce the computational cost to apply polyline simplification to the proposed framework this paper uses the algorithm implemented in a computational geometry algorithms library in c fabri et al 2000 this algorithm can naturally handle the case of overlapping perimeters through triangulation as visualized in fig 3 fig 3 a shows an example of triangulation between two perimeters p and q while fig 3 b shows how the triangulation changes when b q approaches the segment b p c p as q moves upward the segments b p b q and b q c p created by triangulation overlap completely with the segment b p c p constituting p this means that simplification satisfying the conditions presented in the last paragraph of section 2 2 and fig 2 are infeasible in the section therefore the simplification of this section is naturally excluded 3 rapid prediction of wildfire spread by the ensemble kalman filter unlike the standard kalman filter which updates the entire state gaussian distribution directly the enkf updates the ensemble of vectors that approximates the state distribution this can be also described as an approximate update of the state distribution using monte carlo simulation it is well known that enkf can provide successful results in data assimilation problems of highly nonlinear climate prediction such as wildfire spread hargreaves et al 2004 since then various enkf based wildfire spread prediction algorithms have been proposed mandel et al 2008 rochoux et al 2014 srivas et al 2016 this section proposes improvements of the enkf based wildfire prediction algorithm to facilitate rapid predictions based on computational simulations by farsite in enkf based wildfire spread predictions the most significant factor on the computational time required for data assimilation is the number of points used to represent the wildfire perimeter representation of the perimeter with a smaller number of points reduces the time required for data assimilation and thus facilitates rapid prediction of wildfire spread to this end the proposed enkf based algorithm incorporates the polyline simplification algorithm described in section 2 2 furthermore for effective applications of the proposed enkf a scheme matching the points constituting each perimeter ensemble reasonably with the actual observation points is desired therefore we develop a new method of relating ensembles with the actual observation dataset and incorporate this to the proposed enkf algorithm the algorithm consists of 1 prediction part using farsite 2 simplification and re interpolation part and 3 updating part for data assimilation based on observations for background knowledge of kalman filter and enkf readers can refer to simon 2006 and evensen 2009 respectively 3 1 prediction forward simulations using farsite in the prediction part a forward simulation model is applied to each ensemble at the current time step to estimate its state and uncertainty at the next time step in this paper the wildfire spread simulator farsite described in section 2 1 is adopted as a forward simulation model the procedure is summarized as follows step 1 set the initial wildfire perimeter x 0 0 and the covariance matrix p 0 0 x x to apply the enkf the wildfire perimeters must be expressed by a vector hence the initial wildfire perimeter is described as 1 x 0 0 u 1 v 1 u 2 v 2 u m 0 0 v m 0 0 t where u j and v j are respectively the x and y coordinates of the j th discretized point of the perimeter x 0 0 j 1 m 0 0 then ensembles x 0 0 1 x 0 0 2 x 0 0 n are sampled from the initial gaussian distribution characterized by x 0 0 and p 0 0 x x 2 x 0 0 i n x 0 0 p 0 0 x x i 1 2 n where x 0 0 i r 2 m 0 0 in this paper x a b i represents the i th ensemble at a th prediction and b th update of the enkf depending on the observation data an additional error of initial wildfire perimeter can be introduced when generating initial sample ensembles in addition the set of temporally invariant parameters g is determined at this step step 2 obtain posterior ensembles at the next time step by applying farsite to the current ensembles x k k 1 x k k 2 x k k n 3 x k 1 k i f a r s i t e x k k i g h k i 1 2 n where h k is the set of temporally variant parameters both sets are required for farsite as described in section 2 1 in the prediction part step 1 is applied only when generating initial sample ensembles at the subsequent time steps instead of generating ensembles the assimilated posterior ensembles x k k 1 x k k 2 x k k n are directly used in step 2 3 2 simplification and re interpolation reconstruction of the data for application of enkf the simplification and re interpolation part aims to achieve two main goals the first is to reduce the number of points representing a perimeter in order to curtail the time required for data assimilation in the subsequent updating part to this end the two dimensional polyline simplification algorithm described in section 2 2 is utilized in this part the second goal is to process the posterior ensembles in order to facilitate data assimilation between each ensemble and the observation data there are two challenges that make the data assimilation between the ensembles and the observation data difficult first the number of points used to represent each predicted posterior perimeter ensemble is inconsistent as a result the lengths of the ensemble vectors are not identical which makes the application of enkf impossible the same problem occurs when other wildfire spread simulators are used second it is difficult to reasonably define the matching relationship between the points constituting each predicted posterior ensemble and the actual observation points this is because the distances between adjoining points expressing the perimeter of predicted posterior ensembles are not constant the areas with complex wildfire spread patterns have relatively short distances between points whereas those with simple patterns have longer distances however as the ensembles show variability regarding complexity it is difficult to pre define matching points reasonably between ensembles and the observation data the use of other wildfire simulators should entail the same problem the characteristics of farsite also make it difficult to define the matching relationship esri shapefiles obtained through farsite set the point with the smallest x coordinate value as the first point since each ensemble has different shapes points with the smallest x coordinate value may not show reasonable matches to achieve these two main goals this part uses wildfire perimeter observation points as follows the procedure and its principle are described in step 3 to step 6 as shown below in addition fig 4 illustrates the proposed process using four ensembles and five estimated observation points i e n 4 and r 5 when this procedure is applied to actual wildfires the centroids from different perimeter ensembles should be almost similar to each other and the position should also have some overlap however since the purpose of fig 4 is to help the understanding of the methodology the deviation between the centroid and position of the perimeter ensemble in figure is exaggerated compared to the reality step 3 generate simplified posterior ensembles x k 1 k 1 s i m x k 1 k 2 s i m x k 1 k n s i m by applying the two dimensional polyline simplification algorithm to the predicted posterior ensembles x k 1 k 1 x k 1 k 2 x k 1 k n step 4 obtain wildfire perimeter observation points set y k 1 k which consists of r similarly spaced points at time k 1 fig 4 a illustrates the procedures of step 3 and step 4 the first and second goals are achieved by step 3 and step 4 respectively for data assimilation in the updating part it is required to observe y k 1 k at equidistant intervals the reason will be explained at the end of the updating part however it is impossible to observe at equidistant intervals without the exact perimeter hence this paper proposes an effective algorithm that works even if a small deviation exists in the interval between observation points step 5 translate x k 1 k 1 s i m x k 1 k 2 s i m x k 1 k n s i m to x k 1 k 1 s i m t r x k 1 k 2 s i m t r x k 1 k n s i m t r so that the sum of the squared distances between x k 1 k 1 s i m t r x k 1 k 2 s i m t r x k 1 k n s i m t r and y k 1 k is minimized choose one point from y k 1 k arbitrarily which is denoted by a k 1 k for each of the x k 1 k 1 s i m t r x k 1 k 2 s i m t r x k 1 k n s i m t r find the closest position to a k 1 k on the ensemble perimeter which are denoted as a k 1 k 1 s i m t r a k 1 k 2 s i m t r a k 1 k n s i m t r find the corresponding positions by inverse translation of a k 1 k 1 s i m t r a k 1 k 2 s i m t r a k 1 k n s i m t r to a k 1 k 1 s i m a k 1 k 2 s i m a k 1 k n s i m fig 4 b describes how to select the starting points when discretizing the ensembles at equidistant intervals i e a k 1 k 1 s i m a k 1 k 4 s i m which should match each other reasonably because the simplified posterior ensemble x k 1 k 1 s i m may show different locations and shapes when compared to the other simplified posterior ensembles this ensemble is first translated to x k 1 k 1 s i m t r so that the sum of the squared distances between the translated perimeter ensemble and observation points is minimized it is expected that translated ensembles exist at similar positions next one point is selected from each of the translated perimeter ensemble such that the selected points a k 1 k 1 s i m t r a k 1 k 2 s i m t r a k 1 k 4 s i m t r take similar locations to this end the proposed algorithm arbitrarily chooses one of the observation points a k 1 k and identifies the point on the translated perimeter ensemble closest to a k 1 k as a k 1 k 1 s i m t r finally inverse translate x k 1 k 1 s i m t r to determine a k 1 k 1 s i m on the simplified posterior ensemble repeat this process for all predicted posterior ensembles to obtain the starting points which should show a reasonable matching relationship with each other there is a possibility that the local feature in each ensemble is not recognized due to the interval size issue however it is almost impossible that the local feature is not recognized in any ensembles as the number of ensembles increases the probability of missing local features decreases step 6 the unified length n k 1 k int of the ensembles after re interpolation is determined as 4 n k 1 k int r max n k 1 k 1 s i m n k 1 k 2 s i m n k 1 k n s i m r where n k 1 k 1 s i m n k 1 k 2 s i m n k 1 k n s i m represents the length of x k 1 k 1 s i m x k 1 k 2 s i m x k 1 k n s i m and a means rounding up of a thereafter re interpolate x k 1 k 1 s i m x k 1 k 2 s i m x k 1 k n s i m to n k 1 k int points at equidistant intervals starting from a k 1 k 1 s i m a k 1 k 2 s i m a k 1 k n s i m finally we obtain re interpolated posterior ensembles x k 1 k 1 int x k 1 k 2 int x k 1 k n int to which the enkf is applicable fig 4 c illustrates how to generate a re interpolated posterior ensemble based on the starting points after determining a unified length properly the simplified posterior ensemble is re interpolated starting from a k 1 k 1 s i m at equidistant intervals to obtain x k 1 k 1 int repeat this process for all simplified posterior ensembles and starting points to obtain re interpolated posterior ensembles as a result all re interpolated posterior ensembles have the same lengths while achieving a reasonable matching relationship between points there are three stop thresholds provided by the simplification algorithm in cgal 5 0 which are deviation error between the polyline sets before and after removal number of remaining points and percentage of remaining points with the latter two thresholds complexity after simplification can vary greatly for each ensemble therefore the deviation error between the polyline before and after removal is used as the stop threshold in this paper however this stop threshold does not unify the number of points remaining in each ensemble fortunately the number of points in each ensemble is automatically unified through re interpolation in step 6 in step 6 when n k 1 k int is defined as eq 4 n k 1 k int becomes the minimum value among multiples of r above max n k 1 k 1 s i m n k 1 k 2 s i m n k 1 k n s i m the reason will be described in section 3 3 depending on the given computational cost n k 1 k int can be appropriately selected among multiples of r a large value of n k 1 k int makes the perimeter before and after re interpolation similar but increases computational cost on the contrary a small value of n k 1 k int decreases computational cost but increases perimeter error due to re interpolation however a large difference between n k 1 k int and n k 1 k i s i m makes the process inefficient because of collision with polyline simplification of step 3 therefore it is appropriate to set n k 1 k int and n k 1 k i s i m similarly as shown in the example of this paper 3 3 updating adjustment of the wildfire perimeter through data assimilation with observations in the updating part data assimilation is applied to re interpolated posterior ensembles x k 1 k 1 int x k 1 k 2 int x k 1 k n int generated by the procedure in section 3 2 the updating process is summarized as follows step 7 assume covariance estimate v k 1 k r 2 r 2 r and generate a set of observed ensembles y k 1 k 1 y k 1 k 2 y k 1 k n using y k 1 k and v k 1 k 5 y k 1 k i n y k 1 k v k 1 k i 1 2 n where y k 1 k r 2 r is the set of wildfire perimeter observation points obtained in step 4 y k 1 k and y k 1 k 1 y k 1 k 2 y k 1 k n are represented in the vector form which is equivalent to x 0 0 i in eq 3 step 8 define the re interpolated posterior ensemble error matrix e k 1 k x int and approximated sample covariance of the state p k 1 k x x as 6a e k 1 k x int x k 1 k 1 int x k 1 k int x k 1 k n int x k 1 k int 6b p k 1 k x x 1 n 1 e k 1 k x int e k 1 k x int t where 7 x k 1 k int 1 n i 1 n x k 1 k i int step 9 generate the kalman gain k k 1 8 k k 1 p k 1 k x x c k 1 k t c k 1 k p k 1 k x x c k 1 k t v k 1 k 1 where c k 1 k is a spatial down sampling matrix strang and nguyen 1996 that defines the correspondence between y k 1 k i and x k 1 k i int assuming that x k 1 k int is the re interpolated true state data of the wildfire perimeter and y k 1 k is the true state data of the observation point the following relation is satisfied 9 y k 1 k c k 1 k x k 1 k int where c k 1 k is a binary matrix whose element is 1 for the following cases 10 c k 1 k 2 q 1 q 1 n k 1 k int r 1 q 1 2 r c k 1 k 2 q q 1 n k 1 k int r 2 q 1 2 r the elements that are not defined in eq 10 are zero step 10 generate assimilated posterior ensembles x k 1 k 1 1 x k 1 k 1 2 x k 1 k 1 n which becomes the next sample ensembles of step 2 in section 3 1 11 x k 1 k 1 i x k 1 k i int k k 1 y k 1 k i c k 1 k x k 1 k i int i 1 2 n afterward iterate step 2 to step 10 during the observable period of the wildfire perimeter using the assimilated posterior ensembles x k 1 k 1 1 x k 1 k 1 2 x k 1 k 1 n to which the enkf is applied during k 1 time steps the wildfire perimeter after k 1 time steps x k 1 e s t can be estimated as 12 x k 1 e s t 1 n i 1 n x k 1 k 1 i the reason for defining n k 1 k int as eq 4 in step 6 of section 3 2 is to define the spatial down sampling matrix c k 1 k in step 9 in step 4 of section 3 2 it is assumed that y k 1 k is observed at equidistant intervals however if n k 1 k int is not a multiple of r it is impossible to define c k 1 k such that c k 1 k x k 1 k i int has an equidistant interval which makes it challenging to define a reasonable correspondence between y k 1 k i and c k 1 k x k 1 k i int fig 5 shows the flowchart of the enkf based wildfire spread prediction algorithm proposed in this paper 4 numerical investigations 4 1 artificially generated wildfire scenario to conduct numerical investigations a scenario of an artificial wildfire spread is generated using farsite based on actual conditions of climate topography and vegetation it is noted that actual wildfire often has multiple spot fires therefore in order to predict the actual wildfire spread the proposed methodology should be applied to each spot fire the ensemble kalman filter creates and uses multiple ensembles therefore in this paper which is primarily aimed at developing the methodology generating ensembles of all spot fires and applying the methodology would cost us high computational cost for checking points which are not directly relevant to the scope moreover further research related to the merging of the perimeters of individually spreading wildfires is needed the authors believe that it is difficult to consider this aspect in this paper and decided to artificially generate a wildfire scenario that does not have spot fires it is assumed that a wildfire occurs in calpine california 39 37 8 n 39 40 8 n and 120 26 8 w 120 30 2 w the size of the study region is 7 5 k m 2 3 km 2 5 km actual climate records were obtained from weather underground 2019 while the wind speed and direction were calibrated using windninja forthofer et al 2009 data regarding actual topography and vegetation were gathered from landfire 2016 the fuel of the wildfire is represented by the 40 scott and burgan fire behavior fuel model scott 2005 the live fuel moisture is described by the l3 scenario among standard scenarios developed in scott 2005 which are 90 of live herbaceous and 120 of live woody the data regarding dead fuel moisture were gathered from the wildland fire assessment system 2019 which is 6 in 1 h 7 in 10 h and 8 in 100 h these values are equivalent to the d2 scenario among standard scenarios developed in scott 2005 the ros adjustment factor is 1 0 the fuel conditioning period for the wildfire is from july 1 2019 to july 7 2019 while the wildfire spread simulation period for observation and data assimilation is july 5 2019 from 7 00 to 23 00 it is assumed that the actual observation data are obtained at 2 h intervals fig 6 is a satellite map of the study region in which an artificial initial wildfire area and simulation results of wildfire spread are also shown the grey lines in the background image are mountain roads such as haskell peak road and us frst service road 52 they act as barriers affecting the spread of wildfires hereafter this wildfire scenario is used as observation data during the updating process and is termed as the artificially generated wildfire scenario to avoid confusion over the terminology 4 2 design of numerical experiments to check the effects of initial observations and assumption of the ros adjustment factor on the performance of the proposed algorithm four cases of numerical experiments are designed as summarized in table 1 the numerical example is designed to consider two situations in which the performance of the proposed methodology needs to be tested first when the initial observation wildfire dataset is far from the actual wildfire it should be able to gradually approach the actual location through data assimilation gps or drones could be often used to determine the shape of the initial wildfire perimeter however similar errors could occur in the overall observation due to gps errors etc since gps errors consistently generate similar errors until a reset the performance of the proposed methodology in this situation should be tested second when the initial observation wildfire dataset has a completely different perimeter even though located near the actual wildfire the initial perimeter should be able to gradually approach the actual perimeter through data assimilation when using satellites it is possible to determine the approximate location of the initial wildfire but it is difficult to determine the exact perimeter shape in fact when real time wildfires are detected using the geostationary orbit satellite the resolution of the satellite data is up to 2 km 2 km this suggests that considering the initial wildfire region size there is a possibility of obtaining an inaccurate perimeter shape therefore the operation of the proposed methodology in this situation should also be tested the other parameters used for simulations are assumed to be the same as those used for generating the wildfire scenario as shown in fig 7 the initial wildfire observation in dataset 1 i e cases 1 and 2 has a shape similar to that of the artificially generated initial wildfire perimeter but is located far from that spot on the contrary the initial wildfire observation in dataset 2 i e cases 3 and 4 is located near the artificially generated scenario s spot but has a completely different shape each initial wildfire observation dataset describes a parameter by 22 discrete points for cases in which the ros adjustment factor is 1 0 the same parameters as the artificially generated wildfire scenario are used in cases 2 and 4 the factor is overestimated as 1 5 which makes the prediction of the wildfire spread more challenging fig 7 also shows the results of wildfire spread simulation by farsite only i e without applying the proposed enkf in the four cases it is confirmed that the overestimates of the ros adjustment causes faster wildfire spreads in each case 32 initial ensembles x 0 0 1 x 0 0 2 x 0 0 32 are generated using the covariance matrix 13 p 0 0 x x d i a g 1000 1000 1000 2 22 e l e m e n t s the diagonal matrix indicates that error terms of all coordinate values in initial ensemble x 0 0 i are uncorrelated to each other if p 0 0 x x is too small the efficiency of enkf can be extremely low because the filter cannot achieve a significant difference between ensembles to prevent this in the numerical example we set p 0 0 x x to a sufficiently large value in addition because the initial wildfire observation in dataset 1 is far from the artificially generated initial wildfire perimeter we assume that the accuracy of location is exceptionally low in the cases using this dataset accordingly a randomly generated error u n i f 0 1 1000 u n i f 0 1 700 is added to all coordinates in each ensemble from the artificially generated wildfire scenario 15 observation points are obtained at equidistant intervals on the perimeter however it is difficult to observe at regular intervals in reality to address this issue variances are applied when generating observation points of the wildfire perimeter y k 1 k to account for the errors caused by the inability to observe at equidistant intervals in detail y k 1 k is generated as follows 14 y k 1 k y k 1 k v k 1 k r e a l v k 1 k r e a l d i a g 10 8 k 10 8 k 2 15 e l e m e n t s 2 k 0 1 7 where y k 1 k represents the true state data of observation points v k 1 k r e a l denotes the actual error of the observation data and k means a time step as in section 3 k 0 at 7am and increases by 1 every 2 hours it is assumed that the observation accuracy increases as time elapses v k 1 k r e a l was used to generate the observation data but it is exceedingly difficult to obtain v k 1 k r e a l when predicting actual wildfire spread therefore it is necessary to assume a covariance estimate v k 1 k which is used instead of v k 1 k r e a l when predicting wildfire spread in the updating part a significantly large value of v k 1 k should prevent the algorithm from overfitting in this example v k 1 k is assumed to consider the fact that the observation accuracy increases as time elapses 15 v k 1 k d i a g 50 8 k 50 8 k 50 8 k 2 15 e l e m e n t s 2 k 0 1 7 as mentioned in section 3 2 the stop threshold of simplification in this paper is the deviation error between the polyline sets before and after removal the deviation error upper limit for each case is 5 10 and 25 for the purpose of comparison the case without simplification is also shown in the following sections the case without simplification is termed no ls and that with the 5 10 and 25 deviation error upper limit are termed ls 1 ls 2 and ls 3 respectively when each deviation error upper limit is applied ls 1 ls 2 and ls 3 are using about 25 30 15 20 and 6 9 of the points respectively compared to no ls different case and time steps have different point reductions which are described in detail in section 4 4 and 4 5 filippi et al 2014 reviewed similarity indices suitable for evaluating the prediction accuracy in the wildfire spread process among those this paper uses the index proposed by sørensen 1948 to quantify the accuracy of the prediction the sørensen index si for regions x and y is defined as 16 s i 2 x y x y where a denotes the area of region a and a b represents the intersection of regions a and b when the two areas do not overlap each other at all si is zero on the other hand the perfect match between the two areas leads to si 1 0 all cases are designed and tested using the same computer with an amd ryzen 5 3600 cpu processor at 3 59 ghz 16 00 gb ram and matlab 2020a 4 3 basic test case simplest wildfire spread scenario before verification using an artificially generated wildfire scenario we first test the prediction capability of the proposed algorithm in the simplest topography vegetation climate and initial wildfire shape the topography vegetation and climate of the test region are uniform zero elevation flat surface fuel model 3 canopy cover 45 stand height 2 5 m canopy base height 1 5 m canopy bulk density 0 1kg m3 temperature 25 c relative humidity 20 zero precipitation wind speed 5 m s wind direction south and cloud cover 0 the initial wildfire has an ellipse shape with a semi major and semi minor axis of 300 m and 200 m respectively with a semi major axis parallel to the y axis the initial observation perimeter is assumed as follows to verify how the proposed algorithm works in scaling rotation and initial position error an ellipse with a semi major and semi minor axis of 600 m and 400 m respectively with a semi major axis parallel to the y x graph and the center of the ellipse is located the 1 000 m east and 1 000 m north from the center of the initial wildfire the deviation error upper limit is assumed to be 10 while the other conditions remain the same as in section 4 2 fig 8 shows the transition of the predicted wildfire perimeters over time steps original is a simulation result starting with the initial wildfire enkf applied and enkf not applied are simulation results starting with the initial observation perimeter with and without the proposed algorithm applied respectively the initial position error is reduced rapidly when enkf is applied in contrast the scaling and rotation errors gradually decrease as the time passes this is also confirmed in fig 9 a graph showing the sørensen index between original and enkf applied the sørensen index between the initial wildfire and the initial observation perimeter is zero after that it immediately increases to about 0 6 at 9 00 meaning that the initial position is reduced quickly afterward the sørensen index gradually and steadily increases over each time step this means that the scaling and rotation errors gradually decrease 4 4 cases 1 and 2 initial observation dataset with similar shape but different location from now on we verify the wildfire spread prediction capability of the proposed algorithm and the effect of polyline simplification on the prediction accuracy fig 10 shows the observed perimeters at the beginning of the simulation and the predicted perimeters at the end of the simulation for five cases regarding the uses of enkf and the levels of polyline simplification in addition the initial and final perimeters of the artificially generated wildfire scenario are displayed for comparison if the enkf is not employed a large difference appears between the predicted perimeter and the artificially generated perimeter on the other hand applying the enkf makes the predicted perimeter significantly closer to the artificially generated one even though it is assumed that the observation data has a large error the differences between the final enkf predictions caused by the polyline simplification level are small when compared to the difference caused by the enkf to check the effects of data assimilation fig 11 shows the predicted perimeters before and after the updating part of the proposed enkf algorithm for case 2 for comparison the artificially generated wildfire perimeters are also shown by the green color the perimeters before the enkf updating blue show the results of prediction simplification and re interpolation while the perimeters after the enkf updating red represents the final prediction perimeters at each time step because initial wildfire observation dataset 1 is located far from the artificially generated initial wildfire perimeter the difference between before and after the enkf updating part is significantly large at the first time step even though v k 1 k is exceedingly large except for this there is little difference between the perimeters before and after the enkf updating part at an early period as the simulation progresses decreases in v k 1 k increase the difference these results are similar regardless of the polyline simplification application level fig 12 shows the transition of the predicted perimeters after the enkf updating i e the red perimeters in fig 11 for each polyline simplification level at the beginning i e 7am the perimeter prediction results are similar regardless of the polyline simplification level however as the simulation progresses differences in the perimeter prediction appear according to the polyline simplification application level nevertheless the simplification does not cause major biases in the prediction fig 13 shows the sørensen indices in eq 16 for the four simplification levels in cases 1 and 2 regardless of the polyline simplification level the sørensen indices increase similarly in both cases and converge to around 0 95 comparing the results in the two cases the sørensen indices in case 1 reach 0 95 at 17 00 and remain similar afterward whereas the sørensen indices in case 2 continuously increase until the end of the simulation period the sørensen indices in case 2 are lower than those in case 1 because of the inaccurate assumption of the ros adjustment factor it is noted that both data assimilation and farsite simulation contribute to the sørensen index increment to check the effect by data assimilation the increments of the index generated solely by the data assimilation i e the sum of the changes by the vertical segments in fig 13 are shown in table 2 in both cases the relative contributions by the data assimilations are dominant it is also noted that the effects of the polyline simplification level on the contributions are insignificant in both cases next the effects of polyline simplification on the number of points representing the perimeter and the computational time are investigated if polyline simplification is not applied step 3 of the proposed algorithm is ignored in addition step 5 is excluded from the computational time because the required time of the transfer process varies greatly depending on the computational method the number of points representing the wildfire perimeter when the proposed algorithm is applied to case 1 is shown in fig 14 a in addition the ratio of the number of points to that of the case without simplification and the reduced number of points are shown in fig 14 b and c respectively it is shown that ls 1 ls 2 and ls 3 are using 25 32 15 18 and only 6 9 of the points respectively the differences in the number of points increase rapidly as the wildfire spreads fig 15 shows similar results for case 2 ls 1 ls 2 and ls 3 are using 26 31 15 18 and only 6 10 of the points used by the no ls option on the other hand figs 16 and 17 compare the computational times required by the line simplification levels for cases 1 and 2 respectively at the beginning of the wildfire spread the computational time is almost the same even if polyline simplification is applied however as wildfire simulation progresses the computational time ratio decreases and the polyline simplification reduces the computational time to only around 70 since the weather conditions vary computational cost over time steps the increasing trends of the number of points and the computational time also keep changing in both cases the computational time ratio is not significantly affected by the level of polyline simplification table 3 shows the ratios of the number of points and the computational time calculated to those without simplification at the last step ls 3 with the highest level of polyline simplification used only 7 of the points and around 70 of the computational time it is also noted that ls 3 using about a quarter of points used by ls 1 still requires 95 of the computational time of ls 1 this is due to some processes within farsite which is greatly affected by the length of the perimeter the more points represent a wildfire perimeter the smaller the ratio of the computational time becomes in fig 16 a the computational time does not increase monotonically and there are dips at 11 and 17 o clock the dips can occur because the proposed methodology uses triangulation the polyline simplification scheme with the topological relationship preservation requires a constrained triangulation because polylines already exist this is affected not only by the number of points but also by the number distribution and shape of existing polylines in addition the farsite simulation results before polyline simplification have a deviation in complexity at each location depending on the fuel model topography and vegetation this means that there is a possibility that the area of the wildfire region and the number of points in the farsite simulation result are not proportional to each other 4 5 cases 3 and 4 initial observation dataset with different shape but close location next we investigate cases 3 and 4 whose wildfire scenarios observations and prediction results are shown in fig 18 fig 19 shows the sørensen indices for the four simplification levels just as in cases 1 and 2 the application of enkf can successfully predict artificially generated wildfire perimeters in addition differences between the final enkf predictions caused by the polyline simplification level are insignificant when compared to that caused by the enkf which is also confirmed by the comparison of sørensen indices the sørensen indices in case 4 increase more slowly than those in case 3 because of the inaccurate assumption of the ros adjustment factor which is similar to the relationship between cases 1 and 2 however unlike cases 1 and 2 where the difference in sørensen indices occurs throughout the simulation period the indices of cases 3 and 4 converge to around 0 95 at the end of the simulation the index of case 2 does not converge until the end but it is expected that the indices of cases 1 and 2 will converge to around 0 95 as the simulation continues table 4 shows the increments of the index generated solely by the data assimilation as the polyline simplification level increases the contribution of assimilation decreases but has little effect on the total index increment fig 20 compares the number of points representing the wildfire perimeter when the proposed algorithm is applied to cases 3 and 4 it is shown that ls 1 ls 2 and ls 3 are using 25 32 14 18 and only 6 9 of the points respectively which are similar to cases 1 and 2 in addition fig 21 compares the computational times required by the line simplification levels because the number of points for cases 3 and 4 are similar to cases 1 and 2 computational times are also similar the computational time is almost the same at the beginning of the simulation while only about 70 of the computational time is required at the end 4 6 cases 3 and 4 with distances between centroids initial observation dataset with both different shape and location the above two subsections successfully verified the operation of the proposed algorithm when the initial observation data and the artificially generated initial perimeter have similar shapes but are located far away or are located similar but have completely different shapes the polyline simplification level has little effect on the prediction accuracy and there are great benefits in terms of the number of points and computational time at last we investigate the cases in which the initial observation data is located far from artificially generated data and has a completely different shape for this purpose we perform a test based on cases 3 and 4 but only ls 2 is used because the polyline simplification level does not affect the prediction accuracy in each case there are three levels depending on the distance between the centroid of the initial observation data and the artificially generated data the case in which the difference in the both horizontal and vertical positions of the two centroids are 0 m 400 m and 800 m are termed distance 0 distance 400 and distance 800 respectively fig 22 shows the initial observations and prediction results for three kinds of distances between centroids explained above the data for artificially generated wildfire scenarios are also displayed similar to the previous cases the application of enkf can successfully predict artificially generated wildfire perimeters the effects of the distance between centroids on the contributions are insignificant in the final prediction accuracy of both cases and case 3 with ros of 1 0 showed higher accuracy than case 4 with ros of 1 5 to further compare the change in the predicted perimeter over time fig 23 shows the transition of predicted perimeters for case 3 it is confirmed that the position of the predicted perimeter approaches the artificially generated perimeter position in just one application of enkf however as the distance increases the shape of the predicted perimeter loses the shape of the original observation perimeter and becomes more circular this is because as the distance increases the ensembles greatly deviate from the existing predetermined shape when enkf is applied if the shape of the initial observation data and the artificially generated initial perimeter are similar the initial distance between centroids should be small in order to maintain the initial observed perimeter shape when enkf is applied this results in a rapid increase in prediction accuracy however if there is a large difference between the two perimeter shapes a large distance causes the initial observed wildfire perimeter shape to collapse quickly when enkf is applied this means that the predicted perimeter can more quickly approach the shape of the artificially generated wildfire perimeter which leads to a rapid increase in prediction accuracy fig 24 shows the sørensen indices for the three levels of the distance between centroids it is confirmed in both cases that the sørensen index increases rapidly as the distance between centroids increases as described in the previous paragraph similar to the results of previous examples the sørensen indices in case 4 increases more slowly than those in case 3 due to the inaccurate assumption of the ros adjustment factor it is confirmed that the sørensen index has already or is converging to 0 95 in both cases 3 and 4 similar to the previous example as a result it is confirmed that the proposed algorithm performs successfully regardless of the distance or shape relationship between the initial observation data and the initial perimeter of the artificially generated wildfire 5 conclusions in this paper a new algorithm was proposed to reduce the computational time required for wildfire spread prediction based on the farsite wildfire simulator and real time observation data to facilitate a rapid prediction two dimensional polyline simplification that can preserve the topological relationship was applied to farsite simulation results of each ensemble the application of polyline simplification adaptively reduces the number of points to represent wildfire perimeters which speeds up data assimilation by the enkf furthermore a new method to determine the matching between prediction ensembles and the observation data was proposed to facilitate data assimilation the newly proposed enkf algorithm features an additional part between the prediction and updating parts to incorporate these methods this algorithm can be used in most regional scale wildfire simulators such as prometheus which represent perimeters in a similar way to farsite numerical investigations of two initial wildfire observations from a wildfire spread scenario created based on actual environmental conditions were provided to demonstrate the effectiveness of the proposed algorithm the algorithm successfully reduced computation time required for data assimilation while maintaining accuracy of near real time wildfire spread prediction despite errors in initial wildfire observation and the ros adjustment factor it was found that the effects of the polyline simplification level on the prediction accuracy were insignificant in addition as the wildfire perimeter length increased the reduction rate of computational time for data assimilation significantly increased there was also a certain limitation in increasing the sørensen index by the proposed method due to the limited number of real time observation data the proposed algorithm excludes spot fire and predicts only the spread of the main wildfire and has a limitation to the prediction of wildfire with spotting to overcome this further research is needed to define each newly created spot fire as a separate ignition and merge all prediction results obtained by applying the algorithm to each ignition similarly for applications of the proposed methodology to wildfires with complex shapes further research is needed to overcome the inevitable increase of the error caused by the observation data interval due to these limitations the proposed algorithm was only verified through artificial wildfire scenarios and not based on actual wildfire cases in order to employ the proposed methodology in any operational context such verification based on real wildfire cases is additionally required for the purpose of improving the accuracy of the proposed rapid prediction algorithm further research is underway to utilize omnidirectional observations using satellites as real time observation data furthermore the following study is currently in progress to update the ros adjustment factor that was not considered in the proposed algorithm the proposed method can be further developed to incorporate observations and data of wildfire suppression which is generally difficult in real situations the proposed method showed robust performance under the wildfire observation data including various errors which will be helpful in building effective response strategies such as wildfire suppression and evacuation declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgment this research was supported by the national research foundation of korea nrf grant funded by the korean government nrf 2021r1a2c2003553 the authors are supported by the institute of construction and environmental engineering at seoul national university these supports are gratefully acknowledged 
25474,this paper proposes a new method for rapid prediction of wildfire spread which employs computational wildfire simulations by farsite and assimilates the simulation results with actual observation data by means of an ensemble kalman filter to expedite data assimilation the wildfire perimeter is represented by a two dimensional polyline simplification algorithm in addition to facilitate the data assimilation a new process is developed to relate the prediction results with the actual observation data the proposed method is tested and demonstrated by an example wildfire spread scenario generated based on actual climate topography and vegetation the results confirm that the polyline simplification algorithm can drastically reduce the computational time required for data assimilation while maintaining the accuracy of predictions the proposed method is expected to serve as a core algorithm for near real time prediction and data driven updating of wildfire spread keywords wildfire farsite ensemble kalman filter data assimilation near real time prediction polyline simplification data availability data will be made available on request 1 introduction recently wildfire has become one of the major catastrophes around the world for example the total insurance cost of the record breaking 2020 wildfire season in the western u s was estimated as 13 billion risk management solutions 2020 the most common natural causes of wildfire ignition include lightning volcanic eruptions and spontaneous combustions from dry climates man made causes of wildfire ignition are arson discarded cigarettes and slash and burn clearing due to these various causes and climate changes wildfires steadily occur whereas their suppression becomes more challenging and costlier national interagency fire center 2020 therefore an accurate short term prediction of wildfire spread is crucial to minimize the damage and cost of wildfire through effective suppression activities however it is exceedingly difficult to identify all the various factors influencing wildfire spreads even if identified it is challenging to incorporate the factors into the prediction process therefore the errors of wildfire predictions generally accumulate as the spread continues to address the accumulation of prediction errors and improve prediction accuracy mandel et al 2004 proposed data driven wildfire modeling that can improve the wildfire spread prediction accuracy based on actual wildfire observation data later an ensemble kalman filter enkf evensen 1994 2009 was adopted to assimilate the model based prediction with actual wildfire observation data mandel et al 2008 this method applied the data assimilation to newly developed hydrodynamic models simulating wildfire spread as a result the prediction accuracy of wildfire spread and related parameters were significantly improved gps drones and satellites are generally used to gathering actual wildfire observation data however it is often difficult to obtain high accuracy real time data due to performance limitations and various errors accordingly wildfire observation data is generally used for data assimilation using bayesian filters enkf is considered an effective option in wildfire prediction among various bayesian filters because it is suitable for handling problems with a large number of variables and is efficient when applied to nonlinear systems however because wildfire prediction models are extremely irregular and nonlinear it is important to improve the efficiency of enkfs using wildfire observation data zhang et al 2019 improved the data assimilation using thermal infrared imaging data of wildfires subramanian et al 2020 improved the accuracy of the approach while reducing the computational time when stationary points occurred due to external factors in addition to enkf extended kalman filter ekf and unscented kalman filter ukf julier and uhlmann 1997 are widely applied to nonlinear systems such as wildfire spread ekf is the most basic nonlinear kalman filter and approximates the state distribution through gaussian random variables after linearizing the nonlinear system through the jacobian matrix the estimate is updated however ekf is not suitable for extremely nonlinear wildfire prediction because it causes a large error in the posterior mean and covariance of each variable due to linearization ukf is a nonlinear kalman filter further developed from ekf a properly selected set of minimal sample points represents the entire state distribution ukf can express the posterior mean and covariance of each variable in the nonlinear system up to the 3rd order without error a minimal number of sample points express these is called sigma points set by using this ukf can make nonlinear predictions with much higher accuracy than ekf however even in ukf nonlinear systems relying on higher than 3rd order cause a large error in posterior mean and covariance prediction and the state distribution of the hydrodynamic model used for simulation may correspond to this in addition since the number of variables involved in the wildfire spread is significantly large the number of sigma points to obtain sufficient accuracy is excessive which is not suitable in terms of computational cost on the other hand various computational models and software henceforth termed simulators have been developed to provide base predictions of wildfire spread examples of wildfire spread simulators using computational fluid dynamics cfd include firetec linn and harlow 1997 and wfds mell et al 2007 these simulators are known as one of the best options to consider the underlying physics of the problem but unsuitable for real time wildfire spread predictions because they generally require exceedingly large computational cost and time examples of regional scale wildfire spread simulators include farsite finney 1998 and prometheus tymstra et al 2010 these consider the underlying physics less than cfd based simulators but suitable for real time wildfire spread prediction because of their efficiency however these regional scale simulators do not support data assimilation which could improve their prediction accuracy without compromising efficiency recently research efforts have been conducted to improve the prediction accuracy by combining regional scale simulators with data assimilation for example srivas et al 2016 proposed an algorithm that applies enkf based data assimilation to wildfire spread prediction using farsite zhou et al 2019 applied ensemble transform kalman filter etkf which was originally developed to ignore perturbed observations required for the updating part in the enkf to predict wildfire spread using farsite subsequently zhou et al 2020a introduced a vertex weight based etkf that assigns different weights based on the accuracy of observations rios et al 2019 developed a data driven wildfire simulator smartqfire based on marker tracking implementation another notable attempt to improve the efficiency of wildfire simulators was to determine the rate of spread ros adjustment factor through data assimilation for instance srivas et al 2017 expressed the perimeter and ros adjustment factors in one vector and updated this vector through data assimilation cardil et al 2019 proposed a method to obtain ros adjustment factors in real time using the least square method zhou et al 2020b optimized ros adjustment factors by applying a radial basis function neural network responses to wildfire spread during the firefighting operation generally require a significant amount of time due to topographical and environmental conditions of mountainous regions therefore for effective suppression of wildfires minimizing damage and losses sufficient preparation time must be given to decision makers in this regard rapid prediction of wildfire spread is just as important as an accurate prediction the previous studies summarized above have significantly improved the accuracy of regional scale simulators by using various methods however the process introduced to improve the accuracy inevitably increases the computational cost therefore this paper proposes a new algorithm to reduce the time required for enkf data assimilation of wildfire spread prediction using farsite each perimeter ensemble obtained from farsite is expressed as vectors consisting of coordinate pairs since the shape of each perimeter ensemble is different the lengths of the vectors are also different therefore a re interpolation based unification of the lengths is required to apply the enkf based data assimilation given that the most important factor determining the computational time for data assimilation is the length of the ensemble vector representing the prediction results the length is reduced by applying a two dimensional polyline simplification algorithm in this process topological relationships between perimeters must be maintained while applying polyline simplification in addition to further facilitate the data assimilation a new method is developed to relate points constituting each ensemble with the actual observation points to this end the new algorithm proposed in this paper has an additional part termed as simplification and re interpolation between the prediction and updating parts of the standard enkf in this additional part the results of the prediction part are processed such that data assimilation can be conducted in the subsequent updating part in the following section we briefly review the regional scale wildfire spread simulator farsite and the two dimensional polyline simplification algorithm preserving the topological relations section 3 proposes a rapid enkf based wildfire spread prediction algorithm section 4 demonstrates the proposed algorithm through its applications to a wildfire spread scenario and verifies its performance in terms of the computational time and error for different levels of polyline simplification especially in order to confirm the efficiency of the algorithm in various situations we compare the difference in prediction results caused by the initial wildfire observation scenario and the ros adjustment factor finally section 5 provides the summary and future research topics 2 theoretical background 2 1 farsite wildfire spread simulator farsite finney 1998 is a two dimensional wildfire spread simulator used by the u s forest service and national park service the simulator can predict wildfire spread and various behaviors for designated time periods under heterogeneous conditions of fuel moistures weather streams ignitions barriers ros and terrains farsite incorporates various fire behaviors such as crown fires wagner 1977 spotting fires albini 1979 dead fuel moistures nelson 2000 into the two dimensional surface fire growth model by rothermel 1972 these models are integrated using a vector propagation technique that can control both spatial and temporal elements using the integrated fire behavior models farsite can provide various outputs related to wildfires such as arrival times flame lengths spread directions wildfire perimeters and wildfire spread vectors farsite requires a set of parameters describing various environmental factors of wildfire spread these parameters can be categorized into three types 1 terrains and fuels 2 weather conditions and 3 ros adjustment factor the parameters in the first category characterize terrains and fuels in terms of topography and vegetation they are invariant temporally but variant spatially a farsite landscape file contains these parameters supported by the landfire project rollins 2009 this file represents raster maps that describe terrains and fuels of each location in terms of elevation slope aspect canopy cover and fuel model each fuel model requires 1 h fuel moisture content 10 h fuel moisture content 100 h fuel moisture content live herbaceous moisture content and live woody moisture content there are two commonly used sets of fuel models first the fire behavior fuel models fbfm by anderson 1981 consist of 13 models describing fuel type loading surface area to volume ratio fuel bed depth and moisture of extinction on the other hand the fbfms by scott 2005 consist of 40 models this approach uses more models in each fuel type in order to make fuel models employing live herbaceous component dynamic the parameters in the second category i e weather conditions are temperature relative humidity hourly precipitation amount wind speed wind direction and cloud cover percentage on an hourly basis these parameters are variant temporally and invariant spatially while actual weather conditions vary slightly over space in particular because wind speed and wind direction vary greatly depending on the location in the mountainous terrain an external program is needed to calibrate the wind speed and direction for each grid based on the mean value a wind calibration program windninja forthofer et al 2009 is supported within farsite since windninja is a mass conserving model it is useful for rapid prediction of farsite but on the other hand the predictions might not be particularly faithful to observations it is also possible to apply the custom wind variables in a gridded format directly as for the third category a constant value is assigned to the ros adjustment factor for each fuel model in farsite based wildfire spread predictions errors may occur between the prediction and the actual wildfire spread even if all input parameters are exact this is because given that the input parameters related to weather conditions are spatiotemporally homogenized in a specific range prediction errors occur and increase over large areas and long spans of time errors arise also from the difficulty in predicting the wildfire spread rate due to the nonlinear relationship between wind speed fire acceleration and fire spread rate richards 1993 the adjustment factor introduced to correct spread rates is defined as the ratio of the actual spread rate to the predicted one for each fuel model these factors can be estimated from empirical observations of previous wildfires or obtained in real time using the enkf srivas et al 2017 least squares method cardil et al 2019 or radial basis function neural network zhou et al 2020b 2 2 two dimensional polyline simplification algorithm preserving topological relations in filter based prediction or updating of a geometric shape e g boundary of a wildfire the complexity of a shape inevitably increases computational cost a polyline simplification or polygon simplification algorithm simplifies polylines or polygons with inordinate complexity while satisfying the given constraint conditions e g the maximum number of points the maximum error between before and after simplification on the other hand efforts to reduce the complexity may lead to losing important details to minimize the error of the simplified shape while satisfying the given conditions several simplification algorithms have been developed for example the douglas peucker algorithm douglas and peucker 1973 is used when the maximum error is predetermined and the visvalingam whyatt algorithm visvalingam and whyatt 1993 is used when the maximum number of points is predetermined however caution is needed in applying a polyline simplification to multiple polygons fig 1 shows an example of an undesirable point removal a single line segment p r is generated by applying polyline simplification to two consecutive line segments p q and q r line segments of other adjacent polygons a b and b c do not intersect with p r however if the polygon includes b instead of b a b and b c generate intersections with p r which changes the topological relationship of multiple polygons if the wildfire spread estimation time interval is short or the wildfire spread rate is slow applying the general polyline simplification algorithm may change the topological relationship between wildfire perimeters as shown above because data assimilation is applied to obtain the final wildfire spread prediction results intersections may occur between the final wildfire spread prediction result of the previous time step and that of the current time step however no section of the wildfire spread prediction results at the current time step before applying data assimilation exists inside those of the previous time step therefore the polyline simplification algorithm should be able to preserve the topological relationship in this case if the two perimeters overlap each other it is regarded that the topological relationship does not change the reason is explained at the end of section 2 2 to this end the polyline simplification algorithm by dyken et al 2009 is employed in this paper the algorithm only removes points that satisfy the following requirements 1 the existing intersections are maintained 2 new intersections are not generated 3 polylines are not degenerated into a single point and 4 polygons are not degenerated into a single line segment to implement this rule the triangulation technique is used fig 2 shows an example of polyline simplification using triangulation before applying the polyline simplification we perform a triangulation that includes all points and line segments of polylines and polygons let us first consider the case of removing b from the left part in the figure select points a and c that are adjacent to b on the polyline containing b afterward generate polygon b i e the grey region in the left part that is the union of all triangles which have b as a corner at this time because the line segment a c is completely inside of b b can be removed without altering the topological relationship next let us consider the case of removing b from the right part in the figure define a c and b in the same way as above in this case because the line segment a c intersects with the boundary of b it is not possible to remove b without changing the topological relationship in addition even if a c is completely outside of b b cannot be removed either depending on the shape of triangulation both situations can occur in the same polyline the polyline simplification algorithm using triangulation is computationally intensive but its use is inevitable to maintain the topological relationship a method that can reduce the time required for polyline simplification while preserving the topological relationship is desirable to reduce the computational cost to apply polyline simplification to the proposed framework this paper uses the algorithm implemented in a computational geometry algorithms library in c fabri et al 2000 this algorithm can naturally handle the case of overlapping perimeters through triangulation as visualized in fig 3 fig 3 a shows an example of triangulation between two perimeters p and q while fig 3 b shows how the triangulation changes when b q approaches the segment b p c p as q moves upward the segments b p b q and b q c p created by triangulation overlap completely with the segment b p c p constituting p this means that simplification satisfying the conditions presented in the last paragraph of section 2 2 and fig 2 are infeasible in the section therefore the simplification of this section is naturally excluded 3 rapid prediction of wildfire spread by the ensemble kalman filter unlike the standard kalman filter which updates the entire state gaussian distribution directly the enkf updates the ensemble of vectors that approximates the state distribution this can be also described as an approximate update of the state distribution using monte carlo simulation it is well known that enkf can provide successful results in data assimilation problems of highly nonlinear climate prediction such as wildfire spread hargreaves et al 2004 since then various enkf based wildfire spread prediction algorithms have been proposed mandel et al 2008 rochoux et al 2014 srivas et al 2016 this section proposes improvements of the enkf based wildfire prediction algorithm to facilitate rapid predictions based on computational simulations by farsite in enkf based wildfire spread predictions the most significant factor on the computational time required for data assimilation is the number of points used to represent the wildfire perimeter representation of the perimeter with a smaller number of points reduces the time required for data assimilation and thus facilitates rapid prediction of wildfire spread to this end the proposed enkf based algorithm incorporates the polyline simplification algorithm described in section 2 2 furthermore for effective applications of the proposed enkf a scheme matching the points constituting each perimeter ensemble reasonably with the actual observation points is desired therefore we develop a new method of relating ensembles with the actual observation dataset and incorporate this to the proposed enkf algorithm the algorithm consists of 1 prediction part using farsite 2 simplification and re interpolation part and 3 updating part for data assimilation based on observations for background knowledge of kalman filter and enkf readers can refer to simon 2006 and evensen 2009 respectively 3 1 prediction forward simulations using farsite in the prediction part a forward simulation model is applied to each ensemble at the current time step to estimate its state and uncertainty at the next time step in this paper the wildfire spread simulator farsite described in section 2 1 is adopted as a forward simulation model the procedure is summarized as follows step 1 set the initial wildfire perimeter x 0 0 and the covariance matrix p 0 0 x x to apply the enkf the wildfire perimeters must be expressed by a vector hence the initial wildfire perimeter is described as 1 x 0 0 u 1 v 1 u 2 v 2 u m 0 0 v m 0 0 t where u j and v j are respectively the x and y coordinates of the j th discretized point of the perimeter x 0 0 j 1 m 0 0 then ensembles x 0 0 1 x 0 0 2 x 0 0 n are sampled from the initial gaussian distribution characterized by x 0 0 and p 0 0 x x 2 x 0 0 i n x 0 0 p 0 0 x x i 1 2 n where x 0 0 i r 2 m 0 0 in this paper x a b i represents the i th ensemble at a th prediction and b th update of the enkf depending on the observation data an additional error of initial wildfire perimeter can be introduced when generating initial sample ensembles in addition the set of temporally invariant parameters g is determined at this step step 2 obtain posterior ensembles at the next time step by applying farsite to the current ensembles x k k 1 x k k 2 x k k n 3 x k 1 k i f a r s i t e x k k i g h k i 1 2 n where h k is the set of temporally variant parameters both sets are required for farsite as described in section 2 1 in the prediction part step 1 is applied only when generating initial sample ensembles at the subsequent time steps instead of generating ensembles the assimilated posterior ensembles x k k 1 x k k 2 x k k n are directly used in step 2 3 2 simplification and re interpolation reconstruction of the data for application of enkf the simplification and re interpolation part aims to achieve two main goals the first is to reduce the number of points representing a perimeter in order to curtail the time required for data assimilation in the subsequent updating part to this end the two dimensional polyline simplification algorithm described in section 2 2 is utilized in this part the second goal is to process the posterior ensembles in order to facilitate data assimilation between each ensemble and the observation data there are two challenges that make the data assimilation between the ensembles and the observation data difficult first the number of points used to represent each predicted posterior perimeter ensemble is inconsistent as a result the lengths of the ensemble vectors are not identical which makes the application of enkf impossible the same problem occurs when other wildfire spread simulators are used second it is difficult to reasonably define the matching relationship between the points constituting each predicted posterior ensemble and the actual observation points this is because the distances between adjoining points expressing the perimeter of predicted posterior ensembles are not constant the areas with complex wildfire spread patterns have relatively short distances between points whereas those with simple patterns have longer distances however as the ensembles show variability regarding complexity it is difficult to pre define matching points reasonably between ensembles and the observation data the use of other wildfire simulators should entail the same problem the characteristics of farsite also make it difficult to define the matching relationship esri shapefiles obtained through farsite set the point with the smallest x coordinate value as the first point since each ensemble has different shapes points with the smallest x coordinate value may not show reasonable matches to achieve these two main goals this part uses wildfire perimeter observation points as follows the procedure and its principle are described in step 3 to step 6 as shown below in addition fig 4 illustrates the proposed process using four ensembles and five estimated observation points i e n 4 and r 5 when this procedure is applied to actual wildfires the centroids from different perimeter ensembles should be almost similar to each other and the position should also have some overlap however since the purpose of fig 4 is to help the understanding of the methodology the deviation between the centroid and position of the perimeter ensemble in figure is exaggerated compared to the reality step 3 generate simplified posterior ensembles x k 1 k 1 s i m x k 1 k 2 s i m x k 1 k n s i m by applying the two dimensional polyline simplification algorithm to the predicted posterior ensembles x k 1 k 1 x k 1 k 2 x k 1 k n step 4 obtain wildfire perimeter observation points set y k 1 k which consists of r similarly spaced points at time k 1 fig 4 a illustrates the procedures of step 3 and step 4 the first and second goals are achieved by step 3 and step 4 respectively for data assimilation in the updating part it is required to observe y k 1 k at equidistant intervals the reason will be explained at the end of the updating part however it is impossible to observe at equidistant intervals without the exact perimeter hence this paper proposes an effective algorithm that works even if a small deviation exists in the interval between observation points step 5 translate x k 1 k 1 s i m x k 1 k 2 s i m x k 1 k n s i m to x k 1 k 1 s i m t r x k 1 k 2 s i m t r x k 1 k n s i m t r so that the sum of the squared distances between x k 1 k 1 s i m t r x k 1 k 2 s i m t r x k 1 k n s i m t r and y k 1 k is minimized choose one point from y k 1 k arbitrarily which is denoted by a k 1 k for each of the x k 1 k 1 s i m t r x k 1 k 2 s i m t r x k 1 k n s i m t r find the closest position to a k 1 k on the ensemble perimeter which are denoted as a k 1 k 1 s i m t r a k 1 k 2 s i m t r a k 1 k n s i m t r find the corresponding positions by inverse translation of a k 1 k 1 s i m t r a k 1 k 2 s i m t r a k 1 k n s i m t r to a k 1 k 1 s i m a k 1 k 2 s i m a k 1 k n s i m fig 4 b describes how to select the starting points when discretizing the ensembles at equidistant intervals i e a k 1 k 1 s i m a k 1 k 4 s i m which should match each other reasonably because the simplified posterior ensemble x k 1 k 1 s i m may show different locations and shapes when compared to the other simplified posterior ensembles this ensemble is first translated to x k 1 k 1 s i m t r so that the sum of the squared distances between the translated perimeter ensemble and observation points is minimized it is expected that translated ensembles exist at similar positions next one point is selected from each of the translated perimeter ensemble such that the selected points a k 1 k 1 s i m t r a k 1 k 2 s i m t r a k 1 k 4 s i m t r take similar locations to this end the proposed algorithm arbitrarily chooses one of the observation points a k 1 k and identifies the point on the translated perimeter ensemble closest to a k 1 k as a k 1 k 1 s i m t r finally inverse translate x k 1 k 1 s i m t r to determine a k 1 k 1 s i m on the simplified posterior ensemble repeat this process for all predicted posterior ensembles to obtain the starting points which should show a reasonable matching relationship with each other there is a possibility that the local feature in each ensemble is not recognized due to the interval size issue however it is almost impossible that the local feature is not recognized in any ensembles as the number of ensembles increases the probability of missing local features decreases step 6 the unified length n k 1 k int of the ensembles after re interpolation is determined as 4 n k 1 k int r max n k 1 k 1 s i m n k 1 k 2 s i m n k 1 k n s i m r where n k 1 k 1 s i m n k 1 k 2 s i m n k 1 k n s i m represents the length of x k 1 k 1 s i m x k 1 k 2 s i m x k 1 k n s i m and a means rounding up of a thereafter re interpolate x k 1 k 1 s i m x k 1 k 2 s i m x k 1 k n s i m to n k 1 k int points at equidistant intervals starting from a k 1 k 1 s i m a k 1 k 2 s i m a k 1 k n s i m finally we obtain re interpolated posterior ensembles x k 1 k 1 int x k 1 k 2 int x k 1 k n int to which the enkf is applicable fig 4 c illustrates how to generate a re interpolated posterior ensemble based on the starting points after determining a unified length properly the simplified posterior ensemble is re interpolated starting from a k 1 k 1 s i m at equidistant intervals to obtain x k 1 k 1 int repeat this process for all simplified posterior ensembles and starting points to obtain re interpolated posterior ensembles as a result all re interpolated posterior ensembles have the same lengths while achieving a reasonable matching relationship between points there are three stop thresholds provided by the simplification algorithm in cgal 5 0 which are deviation error between the polyline sets before and after removal number of remaining points and percentage of remaining points with the latter two thresholds complexity after simplification can vary greatly for each ensemble therefore the deviation error between the polyline before and after removal is used as the stop threshold in this paper however this stop threshold does not unify the number of points remaining in each ensemble fortunately the number of points in each ensemble is automatically unified through re interpolation in step 6 in step 6 when n k 1 k int is defined as eq 4 n k 1 k int becomes the minimum value among multiples of r above max n k 1 k 1 s i m n k 1 k 2 s i m n k 1 k n s i m the reason will be described in section 3 3 depending on the given computational cost n k 1 k int can be appropriately selected among multiples of r a large value of n k 1 k int makes the perimeter before and after re interpolation similar but increases computational cost on the contrary a small value of n k 1 k int decreases computational cost but increases perimeter error due to re interpolation however a large difference between n k 1 k int and n k 1 k i s i m makes the process inefficient because of collision with polyline simplification of step 3 therefore it is appropriate to set n k 1 k int and n k 1 k i s i m similarly as shown in the example of this paper 3 3 updating adjustment of the wildfire perimeter through data assimilation with observations in the updating part data assimilation is applied to re interpolated posterior ensembles x k 1 k 1 int x k 1 k 2 int x k 1 k n int generated by the procedure in section 3 2 the updating process is summarized as follows step 7 assume covariance estimate v k 1 k r 2 r 2 r and generate a set of observed ensembles y k 1 k 1 y k 1 k 2 y k 1 k n using y k 1 k and v k 1 k 5 y k 1 k i n y k 1 k v k 1 k i 1 2 n where y k 1 k r 2 r is the set of wildfire perimeter observation points obtained in step 4 y k 1 k and y k 1 k 1 y k 1 k 2 y k 1 k n are represented in the vector form which is equivalent to x 0 0 i in eq 3 step 8 define the re interpolated posterior ensemble error matrix e k 1 k x int and approximated sample covariance of the state p k 1 k x x as 6a e k 1 k x int x k 1 k 1 int x k 1 k int x k 1 k n int x k 1 k int 6b p k 1 k x x 1 n 1 e k 1 k x int e k 1 k x int t where 7 x k 1 k int 1 n i 1 n x k 1 k i int step 9 generate the kalman gain k k 1 8 k k 1 p k 1 k x x c k 1 k t c k 1 k p k 1 k x x c k 1 k t v k 1 k 1 where c k 1 k is a spatial down sampling matrix strang and nguyen 1996 that defines the correspondence between y k 1 k i and x k 1 k i int assuming that x k 1 k int is the re interpolated true state data of the wildfire perimeter and y k 1 k is the true state data of the observation point the following relation is satisfied 9 y k 1 k c k 1 k x k 1 k int where c k 1 k is a binary matrix whose element is 1 for the following cases 10 c k 1 k 2 q 1 q 1 n k 1 k int r 1 q 1 2 r c k 1 k 2 q q 1 n k 1 k int r 2 q 1 2 r the elements that are not defined in eq 10 are zero step 10 generate assimilated posterior ensembles x k 1 k 1 1 x k 1 k 1 2 x k 1 k 1 n which becomes the next sample ensembles of step 2 in section 3 1 11 x k 1 k 1 i x k 1 k i int k k 1 y k 1 k i c k 1 k x k 1 k i int i 1 2 n afterward iterate step 2 to step 10 during the observable period of the wildfire perimeter using the assimilated posterior ensembles x k 1 k 1 1 x k 1 k 1 2 x k 1 k 1 n to which the enkf is applied during k 1 time steps the wildfire perimeter after k 1 time steps x k 1 e s t can be estimated as 12 x k 1 e s t 1 n i 1 n x k 1 k 1 i the reason for defining n k 1 k int as eq 4 in step 6 of section 3 2 is to define the spatial down sampling matrix c k 1 k in step 9 in step 4 of section 3 2 it is assumed that y k 1 k is observed at equidistant intervals however if n k 1 k int is not a multiple of r it is impossible to define c k 1 k such that c k 1 k x k 1 k i int has an equidistant interval which makes it challenging to define a reasonable correspondence between y k 1 k i and c k 1 k x k 1 k i int fig 5 shows the flowchart of the enkf based wildfire spread prediction algorithm proposed in this paper 4 numerical investigations 4 1 artificially generated wildfire scenario to conduct numerical investigations a scenario of an artificial wildfire spread is generated using farsite based on actual conditions of climate topography and vegetation it is noted that actual wildfire often has multiple spot fires therefore in order to predict the actual wildfire spread the proposed methodology should be applied to each spot fire the ensemble kalman filter creates and uses multiple ensembles therefore in this paper which is primarily aimed at developing the methodology generating ensembles of all spot fires and applying the methodology would cost us high computational cost for checking points which are not directly relevant to the scope moreover further research related to the merging of the perimeters of individually spreading wildfires is needed the authors believe that it is difficult to consider this aspect in this paper and decided to artificially generate a wildfire scenario that does not have spot fires it is assumed that a wildfire occurs in calpine california 39 37 8 n 39 40 8 n and 120 26 8 w 120 30 2 w the size of the study region is 7 5 k m 2 3 km 2 5 km actual climate records were obtained from weather underground 2019 while the wind speed and direction were calibrated using windninja forthofer et al 2009 data regarding actual topography and vegetation were gathered from landfire 2016 the fuel of the wildfire is represented by the 40 scott and burgan fire behavior fuel model scott 2005 the live fuel moisture is described by the l3 scenario among standard scenarios developed in scott 2005 which are 90 of live herbaceous and 120 of live woody the data regarding dead fuel moisture were gathered from the wildland fire assessment system 2019 which is 6 in 1 h 7 in 10 h and 8 in 100 h these values are equivalent to the d2 scenario among standard scenarios developed in scott 2005 the ros adjustment factor is 1 0 the fuel conditioning period for the wildfire is from july 1 2019 to july 7 2019 while the wildfire spread simulation period for observation and data assimilation is july 5 2019 from 7 00 to 23 00 it is assumed that the actual observation data are obtained at 2 h intervals fig 6 is a satellite map of the study region in which an artificial initial wildfire area and simulation results of wildfire spread are also shown the grey lines in the background image are mountain roads such as haskell peak road and us frst service road 52 they act as barriers affecting the spread of wildfires hereafter this wildfire scenario is used as observation data during the updating process and is termed as the artificially generated wildfire scenario to avoid confusion over the terminology 4 2 design of numerical experiments to check the effects of initial observations and assumption of the ros adjustment factor on the performance of the proposed algorithm four cases of numerical experiments are designed as summarized in table 1 the numerical example is designed to consider two situations in which the performance of the proposed methodology needs to be tested first when the initial observation wildfire dataset is far from the actual wildfire it should be able to gradually approach the actual location through data assimilation gps or drones could be often used to determine the shape of the initial wildfire perimeter however similar errors could occur in the overall observation due to gps errors etc since gps errors consistently generate similar errors until a reset the performance of the proposed methodology in this situation should be tested second when the initial observation wildfire dataset has a completely different perimeter even though located near the actual wildfire the initial perimeter should be able to gradually approach the actual perimeter through data assimilation when using satellites it is possible to determine the approximate location of the initial wildfire but it is difficult to determine the exact perimeter shape in fact when real time wildfires are detected using the geostationary orbit satellite the resolution of the satellite data is up to 2 km 2 km this suggests that considering the initial wildfire region size there is a possibility of obtaining an inaccurate perimeter shape therefore the operation of the proposed methodology in this situation should also be tested the other parameters used for simulations are assumed to be the same as those used for generating the wildfire scenario as shown in fig 7 the initial wildfire observation in dataset 1 i e cases 1 and 2 has a shape similar to that of the artificially generated initial wildfire perimeter but is located far from that spot on the contrary the initial wildfire observation in dataset 2 i e cases 3 and 4 is located near the artificially generated scenario s spot but has a completely different shape each initial wildfire observation dataset describes a parameter by 22 discrete points for cases in which the ros adjustment factor is 1 0 the same parameters as the artificially generated wildfire scenario are used in cases 2 and 4 the factor is overestimated as 1 5 which makes the prediction of the wildfire spread more challenging fig 7 also shows the results of wildfire spread simulation by farsite only i e without applying the proposed enkf in the four cases it is confirmed that the overestimates of the ros adjustment causes faster wildfire spreads in each case 32 initial ensembles x 0 0 1 x 0 0 2 x 0 0 32 are generated using the covariance matrix 13 p 0 0 x x d i a g 1000 1000 1000 2 22 e l e m e n t s the diagonal matrix indicates that error terms of all coordinate values in initial ensemble x 0 0 i are uncorrelated to each other if p 0 0 x x is too small the efficiency of enkf can be extremely low because the filter cannot achieve a significant difference between ensembles to prevent this in the numerical example we set p 0 0 x x to a sufficiently large value in addition because the initial wildfire observation in dataset 1 is far from the artificially generated initial wildfire perimeter we assume that the accuracy of location is exceptionally low in the cases using this dataset accordingly a randomly generated error u n i f 0 1 1000 u n i f 0 1 700 is added to all coordinates in each ensemble from the artificially generated wildfire scenario 15 observation points are obtained at equidistant intervals on the perimeter however it is difficult to observe at regular intervals in reality to address this issue variances are applied when generating observation points of the wildfire perimeter y k 1 k to account for the errors caused by the inability to observe at equidistant intervals in detail y k 1 k is generated as follows 14 y k 1 k y k 1 k v k 1 k r e a l v k 1 k r e a l d i a g 10 8 k 10 8 k 2 15 e l e m e n t s 2 k 0 1 7 where y k 1 k represents the true state data of observation points v k 1 k r e a l denotes the actual error of the observation data and k means a time step as in section 3 k 0 at 7am and increases by 1 every 2 hours it is assumed that the observation accuracy increases as time elapses v k 1 k r e a l was used to generate the observation data but it is exceedingly difficult to obtain v k 1 k r e a l when predicting actual wildfire spread therefore it is necessary to assume a covariance estimate v k 1 k which is used instead of v k 1 k r e a l when predicting wildfire spread in the updating part a significantly large value of v k 1 k should prevent the algorithm from overfitting in this example v k 1 k is assumed to consider the fact that the observation accuracy increases as time elapses 15 v k 1 k d i a g 50 8 k 50 8 k 50 8 k 2 15 e l e m e n t s 2 k 0 1 7 as mentioned in section 3 2 the stop threshold of simplification in this paper is the deviation error between the polyline sets before and after removal the deviation error upper limit for each case is 5 10 and 25 for the purpose of comparison the case without simplification is also shown in the following sections the case without simplification is termed no ls and that with the 5 10 and 25 deviation error upper limit are termed ls 1 ls 2 and ls 3 respectively when each deviation error upper limit is applied ls 1 ls 2 and ls 3 are using about 25 30 15 20 and 6 9 of the points respectively compared to no ls different case and time steps have different point reductions which are described in detail in section 4 4 and 4 5 filippi et al 2014 reviewed similarity indices suitable for evaluating the prediction accuracy in the wildfire spread process among those this paper uses the index proposed by sørensen 1948 to quantify the accuracy of the prediction the sørensen index si for regions x and y is defined as 16 s i 2 x y x y where a denotes the area of region a and a b represents the intersection of regions a and b when the two areas do not overlap each other at all si is zero on the other hand the perfect match between the two areas leads to si 1 0 all cases are designed and tested using the same computer with an amd ryzen 5 3600 cpu processor at 3 59 ghz 16 00 gb ram and matlab 2020a 4 3 basic test case simplest wildfire spread scenario before verification using an artificially generated wildfire scenario we first test the prediction capability of the proposed algorithm in the simplest topography vegetation climate and initial wildfire shape the topography vegetation and climate of the test region are uniform zero elevation flat surface fuel model 3 canopy cover 45 stand height 2 5 m canopy base height 1 5 m canopy bulk density 0 1kg m3 temperature 25 c relative humidity 20 zero precipitation wind speed 5 m s wind direction south and cloud cover 0 the initial wildfire has an ellipse shape with a semi major and semi minor axis of 300 m and 200 m respectively with a semi major axis parallel to the y axis the initial observation perimeter is assumed as follows to verify how the proposed algorithm works in scaling rotation and initial position error an ellipse with a semi major and semi minor axis of 600 m and 400 m respectively with a semi major axis parallel to the y x graph and the center of the ellipse is located the 1 000 m east and 1 000 m north from the center of the initial wildfire the deviation error upper limit is assumed to be 10 while the other conditions remain the same as in section 4 2 fig 8 shows the transition of the predicted wildfire perimeters over time steps original is a simulation result starting with the initial wildfire enkf applied and enkf not applied are simulation results starting with the initial observation perimeter with and without the proposed algorithm applied respectively the initial position error is reduced rapidly when enkf is applied in contrast the scaling and rotation errors gradually decrease as the time passes this is also confirmed in fig 9 a graph showing the sørensen index between original and enkf applied the sørensen index between the initial wildfire and the initial observation perimeter is zero after that it immediately increases to about 0 6 at 9 00 meaning that the initial position is reduced quickly afterward the sørensen index gradually and steadily increases over each time step this means that the scaling and rotation errors gradually decrease 4 4 cases 1 and 2 initial observation dataset with similar shape but different location from now on we verify the wildfire spread prediction capability of the proposed algorithm and the effect of polyline simplification on the prediction accuracy fig 10 shows the observed perimeters at the beginning of the simulation and the predicted perimeters at the end of the simulation for five cases regarding the uses of enkf and the levels of polyline simplification in addition the initial and final perimeters of the artificially generated wildfire scenario are displayed for comparison if the enkf is not employed a large difference appears between the predicted perimeter and the artificially generated perimeter on the other hand applying the enkf makes the predicted perimeter significantly closer to the artificially generated one even though it is assumed that the observation data has a large error the differences between the final enkf predictions caused by the polyline simplification level are small when compared to the difference caused by the enkf to check the effects of data assimilation fig 11 shows the predicted perimeters before and after the updating part of the proposed enkf algorithm for case 2 for comparison the artificially generated wildfire perimeters are also shown by the green color the perimeters before the enkf updating blue show the results of prediction simplification and re interpolation while the perimeters after the enkf updating red represents the final prediction perimeters at each time step because initial wildfire observation dataset 1 is located far from the artificially generated initial wildfire perimeter the difference between before and after the enkf updating part is significantly large at the first time step even though v k 1 k is exceedingly large except for this there is little difference between the perimeters before and after the enkf updating part at an early period as the simulation progresses decreases in v k 1 k increase the difference these results are similar regardless of the polyline simplification application level fig 12 shows the transition of the predicted perimeters after the enkf updating i e the red perimeters in fig 11 for each polyline simplification level at the beginning i e 7am the perimeter prediction results are similar regardless of the polyline simplification level however as the simulation progresses differences in the perimeter prediction appear according to the polyline simplification application level nevertheless the simplification does not cause major biases in the prediction fig 13 shows the sørensen indices in eq 16 for the four simplification levels in cases 1 and 2 regardless of the polyline simplification level the sørensen indices increase similarly in both cases and converge to around 0 95 comparing the results in the two cases the sørensen indices in case 1 reach 0 95 at 17 00 and remain similar afterward whereas the sørensen indices in case 2 continuously increase until the end of the simulation period the sørensen indices in case 2 are lower than those in case 1 because of the inaccurate assumption of the ros adjustment factor it is noted that both data assimilation and farsite simulation contribute to the sørensen index increment to check the effect by data assimilation the increments of the index generated solely by the data assimilation i e the sum of the changes by the vertical segments in fig 13 are shown in table 2 in both cases the relative contributions by the data assimilations are dominant it is also noted that the effects of the polyline simplification level on the contributions are insignificant in both cases next the effects of polyline simplification on the number of points representing the perimeter and the computational time are investigated if polyline simplification is not applied step 3 of the proposed algorithm is ignored in addition step 5 is excluded from the computational time because the required time of the transfer process varies greatly depending on the computational method the number of points representing the wildfire perimeter when the proposed algorithm is applied to case 1 is shown in fig 14 a in addition the ratio of the number of points to that of the case without simplification and the reduced number of points are shown in fig 14 b and c respectively it is shown that ls 1 ls 2 and ls 3 are using 25 32 15 18 and only 6 9 of the points respectively the differences in the number of points increase rapidly as the wildfire spreads fig 15 shows similar results for case 2 ls 1 ls 2 and ls 3 are using 26 31 15 18 and only 6 10 of the points used by the no ls option on the other hand figs 16 and 17 compare the computational times required by the line simplification levels for cases 1 and 2 respectively at the beginning of the wildfire spread the computational time is almost the same even if polyline simplification is applied however as wildfire simulation progresses the computational time ratio decreases and the polyline simplification reduces the computational time to only around 70 since the weather conditions vary computational cost over time steps the increasing trends of the number of points and the computational time also keep changing in both cases the computational time ratio is not significantly affected by the level of polyline simplification table 3 shows the ratios of the number of points and the computational time calculated to those without simplification at the last step ls 3 with the highest level of polyline simplification used only 7 of the points and around 70 of the computational time it is also noted that ls 3 using about a quarter of points used by ls 1 still requires 95 of the computational time of ls 1 this is due to some processes within farsite which is greatly affected by the length of the perimeter the more points represent a wildfire perimeter the smaller the ratio of the computational time becomes in fig 16 a the computational time does not increase monotonically and there are dips at 11 and 17 o clock the dips can occur because the proposed methodology uses triangulation the polyline simplification scheme with the topological relationship preservation requires a constrained triangulation because polylines already exist this is affected not only by the number of points but also by the number distribution and shape of existing polylines in addition the farsite simulation results before polyline simplification have a deviation in complexity at each location depending on the fuel model topography and vegetation this means that there is a possibility that the area of the wildfire region and the number of points in the farsite simulation result are not proportional to each other 4 5 cases 3 and 4 initial observation dataset with different shape but close location next we investigate cases 3 and 4 whose wildfire scenarios observations and prediction results are shown in fig 18 fig 19 shows the sørensen indices for the four simplification levels just as in cases 1 and 2 the application of enkf can successfully predict artificially generated wildfire perimeters in addition differences between the final enkf predictions caused by the polyline simplification level are insignificant when compared to that caused by the enkf which is also confirmed by the comparison of sørensen indices the sørensen indices in case 4 increase more slowly than those in case 3 because of the inaccurate assumption of the ros adjustment factor which is similar to the relationship between cases 1 and 2 however unlike cases 1 and 2 where the difference in sørensen indices occurs throughout the simulation period the indices of cases 3 and 4 converge to around 0 95 at the end of the simulation the index of case 2 does not converge until the end but it is expected that the indices of cases 1 and 2 will converge to around 0 95 as the simulation continues table 4 shows the increments of the index generated solely by the data assimilation as the polyline simplification level increases the contribution of assimilation decreases but has little effect on the total index increment fig 20 compares the number of points representing the wildfire perimeter when the proposed algorithm is applied to cases 3 and 4 it is shown that ls 1 ls 2 and ls 3 are using 25 32 14 18 and only 6 9 of the points respectively which are similar to cases 1 and 2 in addition fig 21 compares the computational times required by the line simplification levels because the number of points for cases 3 and 4 are similar to cases 1 and 2 computational times are also similar the computational time is almost the same at the beginning of the simulation while only about 70 of the computational time is required at the end 4 6 cases 3 and 4 with distances between centroids initial observation dataset with both different shape and location the above two subsections successfully verified the operation of the proposed algorithm when the initial observation data and the artificially generated initial perimeter have similar shapes but are located far away or are located similar but have completely different shapes the polyline simplification level has little effect on the prediction accuracy and there are great benefits in terms of the number of points and computational time at last we investigate the cases in which the initial observation data is located far from artificially generated data and has a completely different shape for this purpose we perform a test based on cases 3 and 4 but only ls 2 is used because the polyline simplification level does not affect the prediction accuracy in each case there are three levels depending on the distance between the centroid of the initial observation data and the artificially generated data the case in which the difference in the both horizontal and vertical positions of the two centroids are 0 m 400 m and 800 m are termed distance 0 distance 400 and distance 800 respectively fig 22 shows the initial observations and prediction results for three kinds of distances between centroids explained above the data for artificially generated wildfire scenarios are also displayed similar to the previous cases the application of enkf can successfully predict artificially generated wildfire perimeters the effects of the distance between centroids on the contributions are insignificant in the final prediction accuracy of both cases and case 3 with ros of 1 0 showed higher accuracy than case 4 with ros of 1 5 to further compare the change in the predicted perimeter over time fig 23 shows the transition of predicted perimeters for case 3 it is confirmed that the position of the predicted perimeter approaches the artificially generated perimeter position in just one application of enkf however as the distance increases the shape of the predicted perimeter loses the shape of the original observation perimeter and becomes more circular this is because as the distance increases the ensembles greatly deviate from the existing predetermined shape when enkf is applied if the shape of the initial observation data and the artificially generated initial perimeter are similar the initial distance between centroids should be small in order to maintain the initial observed perimeter shape when enkf is applied this results in a rapid increase in prediction accuracy however if there is a large difference between the two perimeter shapes a large distance causes the initial observed wildfire perimeter shape to collapse quickly when enkf is applied this means that the predicted perimeter can more quickly approach the shape of the artificially generated wildfire perimeter which leads to a rapid increase in prediction accuracy fig 24 shows the sørensen indices for the three levels of the distance between centroids it is confirmed in both cases that the sørensen index increases rapidly as the distance between centroids increases as described in the previous paragraph similar to the results of previous examples the sørensen indices in case 4 increases more slowly than those in case 3 due to the inaccurate assumption of the ros adjustment factor it is confirmed that the sørensen index has already or is converging to 0 95 in both cases 3 and 4 similar to the previous example as a result it is confirmed that the proposed algorithm performs successfully regardless of the distance or shape relationship between the initial observation data and the initial perimeter of the artificially generated wildfire 5 conclusions in this paper a new algorithm was proposed to reduce the computational time required for wildfire spread prediction based on the farsite wildfire simulator and real time observation data to facilitate a rapid prediction two dimensional polyline simplification that can preserve the topological relationship was applied to farsite simulation results of each ensemble the application of polyline simplification adaptively reduces the number of points to represent wildfire perimeters which speeds up data assimilation by the enkf furthermore a new method to determine the matching between prediction ensembles and the observation data was proposed to facilitate data assimilation the newly proposed enkf algorithm features an additional part between the prediction and updating parts to incorporate these methods this algorithm can be used in most regional scale wildfire simulators such as prometheus which represent perimeters in a similar way to farsite numerical investigations of two initial wildfire observations from a wildfire spread scenario created based on actual environmental conditions were provided to demonstrate the effectiveness of the proposed algorithm the algorithm successfully reduced computation time required for data assimilation while maintaining accuracy of near real time wildfire spread prediction despite errors in initial wildfire observation and the ros adjustment factor it was found that the effects of the polyline simplification level on the prediction accuracy were insignificant in addition as the wildfire perimeter length increased the reduction rate of computational time for data assimilation significantly increased there was also a certain limitation in increasing the sørensen index by the proposed method due to the limited number of real time observation data the proposed algorithm excludes spot fire and predicts only the spread of the main wildfire and has a limitation to the prediction of wildfire with spotting to overcome this further research is needed to define each newly created spot fire as a separate ignition and merge all prediction results obtained by applying the algorithm to each ignition similarly for applications of the proposed methodology to wildfires with complex shapes further research is needed to overcome the inevitable increase of the error caused by the observation data interval due to these limitations the proposed algorithm was only verified through artificial wildfire scenarios and not based on actual wildfire cases in order to employ the proposed methodology in any operational context such verification based on real wildfire cases is additionally required for the purpose of improving the accuracy of the proposed rapid prediction algorithm further research is underway to utilize omnidirectional observations using satellites as real time observation data furthermore the following study is currently in progress to update the ros adjustment factor that was not considered in the proposed algorithm the proposed method can be further developed to incorporate observations and data of wildfire suppression which is generally difficult in real situations the proposed method showed robust performance under the wildfire observation data including various errors which will be helpful in building effective response strategies such as wildfire suppression and evacuation declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgment this research was supported by the national research foundation of korea nrf grant funded by the korean government nrf 2021r1a2c2003553 the authors are supported by the institute of construction and environmental engineering at seoul national university these supports are gratefully acknowledged 
