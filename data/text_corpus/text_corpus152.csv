index,text
760,damage resulting from flood events is increasing world wide requiring the implementation of mitigation and adaption measures to facilitate their implementation it is essential to correctly model flood hazard at the large scale yet fine spatial resolution to reduce the computational load of models flexible meshes are an efficient means compared to uniform regular grids yet thus far they have been applied only for bespoke small scale studies requiring a high level of a priori grid preparation to better understand possible advantages as well as shortcomings of their application for large scale riverine inundation simulations three different flexible meshes were derived from height above nearest drainage hand data and compared with regular grids under identical spatially explicit hydrologic forcing by using glofrim a framework for integrated hydrologic hydrodynamic inundation modelling by means of glofrim output from the global hydrologic model pcr globwb was passed to the hydrodynamic model delft3d flexible mesh results show that applying flexible meshes can be beneficial depending on the envisaged purpose for discharge simulations similar model accuracy was obtained between flexible and regular grids with the former generally having shorter run times for inundation extent simulations however the coarser gridding of flexible meshes in upstream areas results in a poorer performance if assessed by contingency maps moreover while the ratio between minimum and maximum spatial resolution of flexible meshes has limited impact on discharge simulations water level estimates may be stronger influenced by the application of larger grid cells as this study presents only a small set of possible realizations additional research needs to unravel how the data and methods used as well as the choices for discretizations influence model performance generally the application and particularly discretization process of flexible meshes involves more options bringing more responsibilities for the user once an a priori decision is made on the model purpose flexible meshes can be a valuable addition to modelling approaches where short run times are essential facilitating large scale flood simulations ensemble modelling or operational flood forecasting 1 introduction in recent years losses due to riverine inundations increased strongly between 1980 and 2013 they exceeded 1 trillion of direct economic losses and more than 220 000 fatalities munich re 2013 this development can be attributed to the growth of both population and asset values in floodplains ceola et al 2014 winsemius et al 2016 as well as changes in river regimes jongman et al 2012 munich re 2010 visser et al 2012 winsemius et al 2016 despite inherent uncertainties several studies indicate that flood risk will enhance in the future hirabayashi et al 2013 jongman et al 2014 winsemius et al 2016 to capture the driving climate flood interactions and processes world wide it is beneficial to apply global hydrologic models ghms to guarantee seamless large scale inundation modelling across basins and borders besides modelling flood hazard at such scale information should be provided at a spatial resolution sufficiently fine to be locally relevant bierkens et al 2015 facilitating stakeholders involvement beven et al 2015 however the finest spatial resolution achieved for ghms is currently 10 km x 10 km at the equator bierkens 2015 one way to improve the applicability of ghms would be by simulating lateral floodplain flow and channel floodplain interactions at a finer scale moving to a finer scale is however not straightforward as the current debate about hyper resolution shows beven et al 2015 bierkens et al 2015 wood et al 2011 in contrast to ghms hydrodynamic models can run at a finer spatial resolution for instance 1 km globally sampson et al 2015 or 30 m for the continental united states wing et al 2017 a downside of hydrodynamic models however is that they often use observed discharge as model forcing or employ synthesized flood waves hence not accounting for all relevant hydrological processes consequently the spatial correlation of large scale flood events as well as the impact of climate change on flood hazard and risk can be simulated only with concessions one way to circumvent the problems associated with coarse spatial resolutions of ghms and data dependency of hydrodynamic models is hydrologic hydrodynamic model coupling on smaller scales this was already achieved felder et al 2018 kim et al 2012 viero et al 2014 and in a more recent study hoch et al 2017a coupled large scale hydrologic and hydrodynamic models besides spatial extent the latter approach distinguishes itself from others such that it employs the basic model interface bmi peckham et al 2013 providing a flexible coupling design avoiding changes to and entanglement of model code by means of this interface output from pcr globwb pcr sutanudjaja et al 2018 forced the hydrodynamic model delft3d flexible mesh dfm deltares 2018a kernkamp et al 2011 while discharge simulations improved the extent to which the chosen flexible mesh impacted results remained unclear calling for additional research on the use of flexible meshes for large scale inundation modelling despite the large number of studies employing flexible meshes for fluvial flooding castro gama et al 2013 kim et al 2014 kumar et al 2009 sanders et al 2010 schubert et al 2008 none explicitly assesses the role of different mesh configurations let alone for large scale applications while for these bespoke studies an efficient mesh was usually created first such fine tuning is too time consuming for large scale inundation modelling which may encompass several larger catchments what is rather needed are fast approaches to generate flexible meshes over large areas covering a grand variety of topological properties to make maximum use of the potential of flexible meshes a resolution sufficiently fine to provide locally relevant results has to be determined a priori how various degrees of mesh refinement impact large scale inundation modelling results is hardly researched until now and thus additional insight is needed in contrast to flexible meshes there is a multitude of studies investigating various aspects of regular grid refinement for instance it was found that spatial resolution impacts the accuracy of inundation estimates savage et al 2016a water depth estimates and floodplain drainage flow savage et al 2016b and channel flow through near channel storage effects horritt and bates 2001a hardy et al 1999 concluded that grid resolution impacts simulated discharge linearly and water depth in a less structured way due to the impact of the geographical surrounding of each observation location comparable results were obtained by fewtrell et al 2008 in a small urban environment in this study we will add considerations for large scale potentially even global scale flood hazard models using a fast set up of flexible meshes we present a first benchmark and sensitivity analysis to advance our understanding how model accuracy scales with flexible mesh discretization in large scale studies eventually we want to better understand a how different configurations of flexible meshes influence model accuracy b how results differ between flexible meshes and regular grids and c what lessons can be learned for future applications the analysis was performed by employing glofrim a globally applicable framework for integrated hydrologic hydrodynamic modelling hoch et al 2017b glofrim is an openly accessible modular and extensible tool facilitating model coupling currently allowing for spatially coupling pcr with dfm or lisflood fp lfp bates et al 2010 what was decisive to apply glofrim was the requirement to guarantee identical spatially varying model forcing for all discretizations as well as the need to include all river reaches and floodplains in the analysis three different 1 d 2 d flexible meshes of the lower elbe basin fig 1 were forced with identical output from pcr at 30 arc minutes spatial resolution all flexible meshes were created based on the height above nearest drainage hand method rennó et al 2008 we decided to use hand as it requires only little input data and is fast in computing topographical gradients with respect to the channel network to benchmark model results of flexibly gridded meshes we also applied glofrim to three regular grids all model results were then validated against observed discharge values as well as benchmarked with respect to their simulated water levels water volume run time and inundation extents 2 models and methods 2 1 pcr globwb the global hydrologic model pcr globwb pcr sutanudjaja et al 2018 distinguishes between two vertically stacked soil layers an underlying groundwater layer and a surface canopy layer water can be exchanged vertically and excess surface water can be routed horizontally along a local drainage direction network employing the kinematic wave approximation the model was forced with climate research unit cru precipitation and temperature data harris et al 2014 and potential evaporation was computed using the penman monteith equation data sets were downscaled to daily fields for the period from 1957 to 2010 using ecmwf european centre for medium weather forecasts re analysis products era40 erai kållberg et al 2005 uppala et al 2005 as outlined in van beek 2008 pcr furthermore takes into consideration irrigation water demand and industrial and domestic water abstraction based on reported water demand fao 2017 to use the best possible hydrologic forcing for the hydrodynamic model we applied a regional optimization scheme to find the parameterization yielding the most accurate discharge estimations at neu darchau fig 2 further explanation regarding the optimization technique can be found in hoch et al 2017a manning s surface roughness coefficients of 0 04 s m 1 3 and 0 07 s m 1 3 were used for river channel and floodplain respectively 2 2 delft3d flexible mesh delft3d flexible mesh dfm kernkamp et al 2011 allows its user to discretize the 2 d model domain with a flexible mesh applying different geometrical shapes at various resolutions to discretize the study area or regular grids using the same spatial resolution over the entire domain the application of a flexible mesh with dfm is both mass and momentum conservative as a the continuity equation is formulated in a conservative way and b requirements of orthogonality must be met for stable model runs for instance triangles must be acute that is none of the internal angles must be larger than 90 for further information on the use as well as technical descriptions of dfm we refer to the user manual and technical reference manuals deltares 2018b 2018a in contrast to pcr dfm solves the full shallow water equations and thus can capture important flood triggering processes such as backwater effects moussa and bocquillon 1996 to maintain comparability dfm also employs manning s surface roughness coefficients of 0 04 s m 1 3 and 0 07 s m 1 3 for 1 d channels and 2 d floodplain flow respectively 2 3 glofrim glofrim is a globally applicable framework for integrated hydrologic hydrodynamic modelling hoch et al 2017b with glofrim it is possible to perform spatially explicit coupling between hydrologic and hydrodynamic models at a time step basis with the current version of glofrim pcr can be coupled to either the dfm model used here or lisflood fp bates et al 2010 applying glofrim has two major advantages first identical model forcing is provided through pcr output guaranteeing reproducibility and comparability and second setting up a coupled hydrologic hydrodynamic model is greatly facilitated due to the pre defined workflow glofrim is built upon the basic model interface bmi peckham et al 2013 in contrast to other model coupling studies employing internal coupling morita and yen 2002 using the non invasive bmi allows continuing separate development of the models and avoids the entanglement of model code by means of the bmi it is possible to retrieve manipulate and place model data during model execution hence spatial coupling can be achieved by overlaying grids from two models and assigning hydrodynamic to hydrologic cells on a grid to grid basis consequently the hydrodynamic model is forced with output from pcr by exchanging runoff and discharge volumes between corresponding pcr and dfm cells for further information regarding glofrim we refer to hoch et al 2017b 2 4 hydrodynamic discretizations six different dfm discretizations of the lower elbe basin were designed fig 2 three with spatially varying grid size and three with uniform grid size for the flexible meshes we designed these set ups f1 used a length of 1600 m for its coarsest resolution whereas f2 used 3200 m both employed 400 m for its smallest cells comparable to f1 f3 also used 1600 m for its coarsest resolution but used only 800 m for its finest cell in order to evaluate the effects of both the largest and finest cell lengths for the regular grids we set up discretizations with 400 m r1 800 m r2 and 1600 m spatial resolution r3 to be compared with the flexible meshes additional descriptive statistics can be found in table 1 none of the hydrodynamic discretizations were calibrated as the impact of model parameters scales with spatial resolution fewtrell et al 2008 to derive the model discretization for dfm we used hydrosheds surface elevation and drainage network data at 15 arc seconds lehner et al 2008 to apply the height above nearest drainage algorithm hand rennó et al 2008 we opted for hand as it provides a tool for fast grid generation in terms of both data requirements and execution time and is thus well suited for large scale applications besides hand was applied for other inundation modelling studies nobre et al 2016 speckhann et al 2018 and has only a user defined upstream area threshold as possible source of uncertainty various levels of grid refinement were achieved by using different initial grid sizes as well as varying values for both minimum grid cell size and maximum model time step as 2 d floodplain elevation values we employed canopy removed surface elevation data baugh et al 2013 o loughlin et al 2016 and hydraulically smoothed it to account for the vertical measurement errors inherent in remotely sensed elevation data yamazaki et al 2017 2012 before assigning it to the 2 d part of the grids we based both the network and river width information of the 1 d channels on the global width database for large rivers gwd lr yamazaki et al 2014 while river depth information was derived by applying the equations of leopold and maddock 1953 bathymetric information was stored at cross sections with a spacing of around 10 km and subsequently interpolated between cross sections along the river network for both flexible meshes and regular grids the 1 d channel discretization remained unaltered to guarantee consistency between model runs we did not account for dikes and other man made structures due to the lack of reliable global data for our large scale applications and implementing them would otherwise introduce additional uncertainty to model results due to the same reason we desisted from using sub grid elevation data or spatially heterogeneous surface roughness values which would typically be done for catchment scale studies 2 5 assessment of model results all test cases were run for the period 01 january 2002 until 31 december 2010 after two years of spin up simulated discharge was validated at neu darchau nd tangermuende tm and torgau tg fig 2 by computing the coefficient of correlation r the root mean square error rmse and the model s skill expressed as the kling gupta efficiency kge gupta et al 2009 that way it is possible to assess the impact of different discretizations under different discharge regimes the required discharge observations were kindly provided by the german waterway and shipping administration wasser und schifffahrtsverwaltung des bundes wsv via the federal institute of hydrology bundesanstalt für gewässerkunde bfg to obtain an impression how simulated water levels differ throughout the basin they were compared qualitatively at six observation stations covering the up mid and downstream part of the basin fig 2 inundation extent was benchmarked for all discretizations similar to the approach and reasoning of fewtrell et al 2008 thereby the hit rate h the false alarm ratio f and the critical success index c were determined for each inundation map with respect to the map with the highest spatial resolution r1 h f and c were computed with the subsequent equations where nr 1 and ncomp indicate the number of inundated cells in of the benchmark map obtained with r1 and the map of the discretization to be compared respectively 1 h n c o m p n r 1 n r 1 2 f n c o m p n r 1 n c o m p n r 1 n c o m p n r 1 3 c n c o m p n r 1 n c o m p n r 1 all parameters can vary between 0 and 1 while h 1 shows that all inundated cells in the benchmark data are also inundated in the comparison data f 1 indicates that the inundated cells in the comparison are entirely false alarms with respect to the benchmark the critical success rate c in turn should be 1 for perfect agreement thereby penalizing for both under and overprediction unfortunately it was not achievable to validate simulated inundation extent against observations due to the lack of embankment height information and the resulting overestimation of simulated inundation extent simulated discharge water levels and inundation extent were put into perspective by assessing simulated water volumes which functions as a proxy for overbank water storage in addition run times are reported to evaluate the computational efficiency of the different grids 3 results and discussion 3 1 simulated discharge three observations can be made across all six discretizations regardless the gridding scheme first computed discharge exceeds observations for regular flow regimes but underpredicts discharge for peak flow conditions fig 3 further investigation revealed that this is not mostly due to the discharge overpredicted by pcr which thus already determines the potential accuracy of the coupled output second the magnitude of exceedance increases downstream as expressed by the increase in rmse table 2 a we postulate that this larger bias is caused at least partly by the absence of hydrological processes in the hydrodynamic model such as groundwater infiltration or evaporation last the absence of dikes influences the shape of all simulated hydrographs without dikes simulated discharge is smoother due to less flow constriction dampening and lagging particularly peak discharge while the different aspects do affect model accuracy all discretizations are however affected equally and hence further benchmarking is not hampered we assess the influence of the gridding technique applied first comparing the kges of the discretizations with 400 m f1 f2 and r1 and those with 800 m finest spatial resolution f3 and r2 reveals that the application of a regular grid improves model s skill insignificantly compared to a flexible mesh discretization if the same finer spatial resolution is applied table 2a additionally results obtained for the regular grid runs indicate that further coarsening of the grid from 800 m to 1600 m impacts discharge results less drastically than from 400 m to 800 m especially with respect to peak discharge computations evaluating the impact of spatial resolution on discharge estimates we find at nd that simulated discharge deviates only slightly between spatial resolutions fig 3a and table 2a for flexible meshes f1 and f2 show near identical discharge results while f3 yields lower estimates similarly f1 and f2 yield comparable discharge results at tm and tg the near identical results of f1 and f2 at all three stations suggest that the choice of the finest spatial resolution within a flexible mesh strongly determines the accuracy of discharge simulations while the coarsest resolution is less influential at these farther upstream stations however the deviation of f3 from f1 and f2 as well as of r3 from r1 and r2 is larger than at nd fig 3c f since discharge at nd differs hardly between discretization the overall discharge volumes passing tm and rt should also be comparable to exclude any water balance errors as this is not the case here we re run all discretization with cross sections covering the entire floodplain width to exclude uncaptured floodplain flow as cause comparing discharge obtained from channel flow fig 3 with the full floodplain discharge fig 4 suggest that with coarser cells a larger fraction of total downstream floodplain flow travels via the 2 d floodplain cells most likely due to the reduced number of 2 d cells available to accommodate floodplain flow since model skill is near identical at each station across set ups table 2a the new results provide insight into flood wave propagation at the most upstream station tr pcr discharge and dfm discharge correlate very strongly r 0 94 but with increasing downstream distance the discrepancy between discharge simulated without and with glofrim increases from r 0 75 at tm to r 0 57 at nd this underpins the above made assumption that not accounting for open water evaporation and groundwater infiltration in hydrodynamic models can lead to a reduction of model accuracy in the subsequent section their potential influence is analysed in more depth since as mentioned above discharge peaks are not well simulated by the hydrodynamic model we performed a peak above threshold analysis to assess performance for peak flows separately table 2b as threshold we used the long term mean discharge per station as reported by the bfg 1 1 hydrologic properties for each bfg station in the elbe basin can be found on the undine webpage http undine bafg de elbe elbegebiet html for nd 705 m3 s 1 for tm 562 m3 s 1 and for tr 340 m3 s 1 results suggest that for peak flow conditions the discretization approach opted for as well as the absence of dikes and other flood wave containing measures impacts model accuracy strongly and poses a limitation to using the here applied discretizations in an operational setting besides results corroborate that capturing floodplain flow for coarser discretizations is even more important for peak flow conditions these findings are however in line with expectations as we used only global data sets and thus applicability for local bespoke studies may be reduced ward et al 2015 3 2 water volume from fig 5 three groups can be distinguished the water volume of r3 which grossly exceeds all other simulated volumes an intermediate group consisting of f1 r2 and f3 and the group of f2 and r1 containing the least water volume in the system overall results show that the water volume stored in the system increases significantly when moving to a coarser discretization the aggregation rate expressed as slope of the linear fit ranges between 11 10 5 m3 d 1 and 25 10 5 m3 d 1 for r1 and r3 respectively such accumulation may potentially lead to overestimation of simulated water levels and discharge table 4a one possible cause for the increase in water volume storage may be the absence of feedback loops between hydrodynamics and hydrology as discussed above another reason for the accumulation may be the absence of small 1 d channels hampering the drainage of floodplains as well as the coarse 2 d elevation information obstructing important floodplain channel flows neal et al 2012 we conducted a first order assessment whether estimates of spatial temporal averages of both potential evaporation and groundwater infiltration rates could absorb the accumulated volumes b by multiplying the average of potential evaporation as used by pcr forcing 0 0019 m d 1 with inundation area table 4b a potential volumetric evaporation between 1 37 1011 m3 for r1 and 2 06 1011 m3 for r3 over the entire model period is obtained both greatly exceeding the total accumulated water volume for any discretization the average infiltration capacity expressed as the ksat value is with 0 15 m d 1 even higher than potential evaporation since both values exceed the actually accumulated water volume we cannot exclude the absence of hydrologic processes as cause for the aggregation a clear answer whether this or hindered dewatering of floodplains as reported by neal et al 2012 was the main driver can however not be unambiguously be provided 3 3 water level results show fig 6 that the chosen spatial resolution impacts simulated water levels at all stations regardless the application of flexible meshes or regular grids even though there are locally marked deviations coarser spatial resolutions result in higher water levels at most of the stations the main trend in higher water levels with coarser resolution is consistent with larger flood volumes during inundation the results can be explained by coarser spatial resolutions reducing connectivity as well as representation of both floodplain flow and floodplain channel processes which may result in locally higher water levels altenau et al 2017 horritt et al 2006 neal et al 2012 besides coarser spatial resolutions reduce dynamics and especially at upstream stations do not capture all inundation events even though there is no linear relation between coarsening of grid size and change in surface elevation at all six measuring points elevation values at observation stations tend to increase with spatial resolution potentially limiting the magnitude of water level fluctuations table 3 this decrease of elevation with spatial coarsening is due to spatial averaging of input elevation values savage et al 2016a also studies report a non linear connection between model results and bulk flow effects at coarser resolution as well as varying feedback loops at different resolution due to surrounding cells fewtrell et al 2008 hardy et al 1999 an unambiguous answer which is the driving factor is unfortunately not possible due to the system s complexity water levels of flexible meshes and regular grid per station generally compare well the closest fit between flexible and regular grids could be found for the upstream stations loc3b and loc3c as well as the most downstream station loc1 where the f1 f2 and r1 as well as f3 and r2 respectively exhibit near identical results 3 4 inundation extent we benchmarked inundation extent at the end of the simulations of all discretizations with r1 as reference map and computed contingency maps for visualization of the hit rate h false alarm ratio f and critical success rate c fig 7 it should be noted that similar as for the discharge results the absence of dikes and other man made structures in our discretizations results in overestimations of inundation extent and thus we desisted from performing an actual validation against observed inundation extent besides other factors potentially affecting inundation extent such as urban areas could not be included due to lacking data also including sub grid elevation data and spatially varying surface roughness values may have positively influenced the inundation extent obtained we find that not only the overall spatial resolution but also the gridding approach greatly impacts the agreement of inundation extent at the finest level table 4 b although f1 has the same finest grid size as r1 they agree only to 74 this suggests that accuracy of flexible meshes is reduced in those areas where a coarser spatial resolution is employed which is mostly in upstream areas this underlines the above made suggestion that the coarsest grid size has a marked impact on simulated inundation extent besides it seems that for a certain range of coarser discretizations it is inconsequential which cell size or gridding technique is opted for as h f and c are within close limits coarser resolution models tend to predict larger inundation extent not only on floodplains but also for areas farther away from the channels this again can be related to a lack of 2 d return flows with coarsened spatial resolution or missing hydrological feedback as shown above and in previous studies besides similar studies for regular grids also report an increase in inundation extent for coarser resolutions hardy et al 1999 which in turn is linked to a reduction of contingency and representativeness altenau et al 2017 horritt and bates 2001a savage et al 2016a altenau et al 2017 also concluded that the critical success index drops for coarser spatial resolutions due to averaging of channel and floodplain properties we nevertheless must acknowledge that when other mesh generation approaches other than hand would be applied as for example gis based approaches kumar et al 2009 results may differ as mesh size properties are to some extent configuration dependent 3 5 run time we find that especially for smaller resolutions the differences increase significantly which is in line with expectations and also found in comparable studies although merely considering regular grids altenau et al 2017 savage et al 2016a besides very similar performance in discharge computations run times are almost identical for both r2 and f3 this is because only 2 d cells adjacent to rivers will be inundated and thus run time does not depend on the overall number of 2 d cells but is mainly governed by the number of 2 d cells inundated even though f2 and f1 have the same finer spatial resolution run times differ markedly with the latter having a factor 1 73 longer run time the longest run time was as expected measured for r1 which is 12 longer than f1 while the difference between regular grids and flexible meshes is as expected results show that major gains can be obtained if doubling the coarsest spatial resolution for instance from 1600 m f1 to 3200 m f2 of the flexible mesh provided potential reduction of upstream model accuracy is acceptable 4 conclusion and recommendations to foster our understanding of differences between flexible meshes and regular grids as well as to better understand both advantages and shortcomings of using flexible meshes for large scale inundation modelling we compared six hydrodynamic discretizations of the lower elbe basin to facilitate the fast generation of meshes we used the height above nearest drainage hand algorithm comparability between runs was ensured by employing the glofrim framework allowing for identical spatially varying and explicit forcing of hydrodynamic models with hydrologic output we conclude that the spatial resolution of the hydrodynamic model discretization influences model skill in simulating discharge local water levels and agreement between inundation maps which complies with previous studies although performed with different models and on other scales altenau et al 2017 fewtrell et al 2008 hardy et al 1999 horritt and bates 2001a savage et al 2016b 2016a even though the findings are configuration dependent and we test only a sample of all possible discretizations those similarities across scales and applications are both confirmation of the robustness of our results and proof that these links are model independent and thus of more universal nature for discharge simulations the finest spatial resolution in the grid determines the accuracy furthermore there are no significant differences between the application of a flexible or a regular mesh if comparable in resolution this means that for discharge simulations less detailed discretizations are acceptable for areas farther away from the river system if the finest spatial resolution is sufficiently fine to capture both channel and floodplain flow processes this is crucial since with coarser spatial resolution a higher fraction of overall flow is conveyed via the 2 d part to some extent adding more 1 d channels could alleviate this but on basis of our results we find the spatial resolution the larger impediment since the here presented study solely employs large scale data sets for a catchment scale analysis a peak over threshold assessment shows that such approaches should be critically examined for detailed flood hazard and risk assessments as not considering structures such as dikes and drivers like spatially varying roughness coefficients may reduce the accuracy of discharge simulations unlike discharge assessing inundation extent exhibited stronger deviations between the gridding techniques generally a uniform and fine spatial resolution outperforms any coarser or flexible grid this is mostly due to the progressive coarsening of mesh size in upstream areas leading to larger simulated inundation extent once bankfull discharge capacity is exceeded to better understand to which extent the application of hand influenced the extent of simulated discharge and the model s skill in simulating peak discharge situations we recommend testing other mesh refinement approaches results suggest that applying a coarser spatial resolution enhances the accumulation and flow of water on the floodplains comparing the accumulated volumes with potential reduction due to evaporation or groundwater infiltration showed that these processes cannot be neglected as most hydrodynamic models do not simulate evaporation or groundwater infiltration accumulated water will remain there except for return flows consequently future work should focus on establishing feedback processes between inundation floodplains and hydrologic processes what can be derived from these findings is that there is a threshold resolution defining the limits of meaningfulness of mesh refinement only if a certain minimum fineness of resolution is given flow and inundation processes can be represented sufficiently well while this was already found to be true for regular grids horritt and bates 2001 this study illustrates similar patterns for flexible meshes as the relation between this resolution and model accuracy will most likely differ depending on basin and river dimensions as well as grid generation technique we recommend further research to establish a relation between basin properties and mesh design such knowledge would be of invaluable use for any large scale hydrodynamic study as essential time savings effects by grid size optimization could be achieved as a guideline for future applications of flexible meshes for large scale inundation studies we define three major aspects to consider if applying a flexible mesh for large scale inundation studies using hand to generate large scale flexible meshes is a fast and low level approach for large scale applications where more detailed topographical features can be neglected results for this test case underline importance of including smaller topographic features for bespoke and detailed catchment scale flood hazard and risk assessments local observations such as river discharge and floodplain water levels are less sensitive to coarse resolution flexible meshes in upstream areas a minimum fine spatial resolution must be met for floodplain areas to reduce volume conveyed via floodplains and facilitate return flows into channel domain wide output such as inundation extent profits from the application of uniform fine resolution regular grids as this study is the first of its kind focussing on comparing flexible meshes with regular grids using methods and data for large scale applications the number of flexible meshes used was limited to further increase our understanding of the confines of their applicability we recommend a study merely focussing on the impact of mesh variations but then with a wider range of discretizations to conclude we see potential in the application of flexible meshes for future hyper resolution large scale inundation studies but it also brings more responsibilities applicants of flexible mesh models need to put additional emphasis on the creation of the hydrodynamic discretization as it is the coarsest spatial resolution that may become the bottleneck of accuracy while we used the hand algorithm for fast and semi automated mesh creations we recommend testing other mesh generation approaches as well generally we think that mesh designs should be based on a number of considerations for instance for discharge simulation larger ratios between largest and smallest cell size are admissible whereas for inundation extent computations this ratio should be minimized also the context of the simulation needs to be considered are detailed estimates required or are short run times essential does one need high accuracy output for the entire domain or just a small part of it once the user has a clear idea of the study objectives the application of a flexible mesh can indeed serve as a time saving alternative to regular grids proving potentially useful for large scale operational or ensemble modelling studies where results need to be computed in brief time acknowledgements this study was financed by the european institute of innovation and technology climate kic programme under project title global high resolution database of current and future river flood hazard to support planning adaption and re insurance furthermore we kindly thank the german waterway and shipping administration wasser und schifffahrtsverwaltung des bundes wsv for providing the discharge measurements used for model validation we also thank edwin sutanudjaja for support with pcr globwb as well as herman kernkamp and arthur van dam for advice on delft3d flexible mesh we also thank two anonymous authors for their critical remarks on a previous version of this article supplementary materials supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2018 09 003 appendix supplementary materials image application 1 
760,damage resulting from flood events is increasing world wide requiring the implementation of mitigation and adaption measures to facilitate their implementation it is essential to correctly model flood hazard at the large scale yet fine spatial resolution to reduce the computational load of models flexible meshes are an efficient means compared to uniform regular grids yet thus far they have been applied only for bespoke small scale studies requiring a high level of a priori grid preparation to better understand possible advantages as well as shortcomings of their application for large scale riverine inundation simulations three different flexible meshes were derived from height above nearest drainage hand data and compared with regular grids under identical spatially explicit hydrologic forcing by using glofrim a framework for integrated hydrologic hydrodynamic inundation modelling by means of glofrim output from the global hydrologic model pcr globwb was passed to the hydrodynamic model delft3d flexible mesh results show that applying flexible meshes can be beneficial depending on the envisaged purpose for discharge simulations similar model accuracy was obtained between flexible and regular grids with the former generally having shorter run times for inundation extent simulations however the coarser gridding of flexible meshes in upstream areas results in a poorer performance if assessed by contingency maps moreover while the ratio between minimum and maximum spatial resolution of flexible meshes has limited impact on discharge simulations water level estimates may be stronger influenced by the application of larger grid cells as this study presents only a small set of possible realizations additional research needs to unravel how the data and methods used as well as the choices for discretizations influence model performance generally the application and particularly discretization process of flexible meshes involves more options bringing more responsibilities for the user once an a priori decision is made on the model purpose flexible meshes can be a valuable addition to modelling approaches where short run times are essential facilitating large scale flood simulations ensemble modelling or operational flood forecasting 1 introduction in recent years losses due to riverine inundations increased strongly between 1980 and 2013 they exceeded 1 trillion of direct economic losses and more than 220 000 fatalities munich re 2013 this development can be attributed to the growth of both population and asset values in floodplains ceola et al 2014 winsemius et al 2016 as well as changes in river regimes jongman et al 2012 munich re 2010 visser et al 2012 winsemius et al 2016 despite inherent uncertainties several studies indicate that flood risk will enhance in the future hirabayashi et al 2013 jongman et al 2014 winsemius et al 2016 to capture the driving climate flood interactions and processes world wide it is beneficial to apply global hydrologic models ghms to guarantee seamless large scale inundation modelling across basins and borders besides modelling flood hazard at such scale information should be provided at a spatial resolution sufficiently fine to be locally relevant bierkens et al 2015 facilitating stakeholders involvement beven et al 2015 however the finest spatial resolution achieved for ghms is currently 10 km x 10 km at the equator bierkens 2015 one way to improve the applicability of ghms would be by simulating lateral floodplain flow and channel floodplain interactions at a finer scale moving to a finer scale is however not straightforward as the current debate about hyper resolution shows beven et al 2015 bierkens et al 2015 wood et al 2011 in contrast to ghms hydrodynamic models can run at a finer spatial resolution for instance 1 km globally sampson et al 2015 or 30 m for the continental united states wing et al 2017 a downside of hydrodynamic models however is that they often use observed discharge as model forcing or employ synthesized flood waves hence not accounting for all relevant hydrological processes consequently the spatial correlation of large scale flood events as well as the impact of climate change on flood hazard and risk can be simulated only with concessions one way to circumvent the problems associated with coarse spatial resolutions of ghms and data dependency of hydrodynamic models is hydrologic hydrodynamic model coupling on smaller scales this was already achieved felder et al 2018 kim et al 2012 viero et al 2014 and in a more recent study hoch et al 2017a coupled large scale hydrologic and hydrodynamic models besides spatial extent the latter approach distinguishes itself from others such that it employs the basic model interface bmi peckham et al 2013 providing a flexible coupling design avoiding changes to and entanglement of model code by means of this interface output from pcr globwb pcr sutanudjaja et al 2018 forced the hydrodynamic model delft3d flexible mesh dfm deltares 2018a kernkamp et al 2011 while discharge simulations improved the extent to which the chosen flexible mesh impacted results remained unclear calling for additional research on the use of flexible meshes for large scale inundation modelling despite the large number of studies employing flexible meshes for fluvial flooding castro gama et al 2013 kim et al 2014 kumar et al 2009 sanders et al 2010 schubert et al 2008 none explicitly assesses the role of different mesh configurations let alone for large scale applications while for these bespoke studies an efficient mesh was usually created first such fine tuning is too time consuming for large scale inundation modelling which may encompass several larger catchments what is rather needed are fast approaches to generate flexible meshes over large areas covering a grand variety of topological properties to make maximum use of the potential of flexible meshes a resolution sufficiently fine to provide locally relevant results has to be determined a priori how various degrees of mesh refinement impact large scale inundation modelling results is hardly researched until now and thus additional insight is needed in contrast to flexible meshes there is a multitude of studies investigating various aspects of regular grid refinement for instance it was found that spatial resolution impacts the accuracy of inundation estimates savage et al 2016a water depth estimates and floodplain drainage flow savage et al 2016b and channel flow through near channel storage effects horritt and bates 2001a hardy et al 1999 concluded that grid resolution impacts simulated discharge linearly and water depth in a less structured way due to the impact of the geographical surrounding of each observation location comparable results were obtained by fewtrell et al 2008 in a small urban environment in this study we will add considerations for large scale potentially even global scale flood hazard models using a fast set up of flexible meshes we present a first benchmark and sensitivity analysis to advance our understanding how model accuracy scales with flexible mesh discretization in large scale studies eventually we want to better understand a how different configurations of flexible meshes influence model accuracy b how results differ between flexible meshes and regular grids and c what lessons can be learned for future applications the analysis was performed by employing glofrim a globally applicable framework for integrated hydrologic hydrodynamic modelling hoch et al 2017b glofrim is an openly accessible modular and extensible tool facilitating model coupling currently allowing for spatially coupling pcr with dfm or lisflood fp lfp bates et al 2010 what was decisive to apply glofrim was the requirement to guarantee identical spatially varying model forcing for all discretizations as well as the need to include all river reaches and floodplains in the analysis three different 1 d 2 d flexible meshes of the lower elbe basin fig 1 were forced with identical output from pcr at 30 arc minutes spatial resolution all flexible meshes were created based on the height above nearest drainage hand method rennó et al 2008 we decided to use hand as it requires only little input data and is fast in computing topographical gradients with respect to the channel network to benchmark model results of flexibly gridded meshes we also applied glofrim to three regular grids all model results were then validated against observed discharge values as well as benchmarked with respect to their simulated water levels water volume run time and inundation extents 2 models and methods 2 1 pcr globwb the global hydrologic model pcr globwb pcr sutanudjaja et al 2018 distinguishes between two vertically stacked soil layers an underlying groundwater layer and a surface canopy layer water can be exchanged vertically and excess surface water can be routed horizontally along a local drainage direction network employing the kinematic wave approximation the model was forced with climate research unit cru precipitation and temperature data harris et al 2014 and potential evaporation was computed using the penman monteith equation data sets were downscaled to daily fields for the period from 1957 to 2010 using ecmwf european centre for medium weather forecasts re analysis products era40 erai kållberg et al 2005 uppala et al 2005 as outlined in van beek 2008 pcr furthermore takes into consideration irrigation water demand and industrial and domestic water abstraction based on reported water demand fao 2017 to use the best possible hydrologic forcing for the hydrodynamic model we applied a regional optimization scheme to find the parameterization yielding the most accurate discharge estimations at neu darchau fig 2 further explanation regarding the optimization technique can be found in hoch et al 2017a manning s surface roughness coefficients of 0 04 s m 1 3 and 0 07 s m 1 3 were used for river channel and floodplain respectively 2 2 delft3d flexible mesh delft3d flexible mesh dfm kernkamp et al 2011 allows its user to discretize the 2 d model domain with a flexible mesh applying different geometrical shapes at various resolutions to discretize the study area or regular grids using the same spatial resolution over the entire domain the application of a flexible mesh with dfm is both mass and momentum conservative as a the continuity equation is formulated in a conservative way and b requirements of orthogonality must be met for stable model runs for instance triangles must be acute that is none of the internal angles must be larger than 90 for further information on the use as well as technical descriptions of dfm we refer to the user manual and technical reference manuals deltares 2018b 2018a in contrast to pcr dfm solves the full shallow water equations and thus can capture important flood triggering processes such as backwater effects moussa and bocquillon 1996 to maintain comparability dfm also employs manning s surface roughness coefficients of 0 04 s m 1 3 and 0 07 s m 1 3 for 1 d channels and 2 d floodplain flow respectively 2 3 glofrim glofrim is a globally applicable framework for integrated hydrologic hydrodynamic modelling hoch et al 2017b with glofrim it is possible to perform spatially explicit coupling between hydrologic and hydrodynamic models at a time step basis with the current version of glofrim pcr can be coupled to either the dfm model used here or lisflood fp bates et al 2010 applying glofrim has two major advantages first identical model forcing is provided through pcr output guaranteeing reproducibility and comparability and second setting up a coupled hydrologic hydrodynamic model is greatly facilitated due to the pre defined workflow glofrim is built upon the basic model interface bmi peckham et al 2013 in contrast to other model coupling studies employing internal coupling morita and yen 2002 using the non invasive bmi allows continuing separate development of the models and avoids the entanglement of model code by means of the bmi it is possible to retrieve manipulate and place model data during model execution hence spatial coupling can be achieved by overlaying grids from two models and assigning hydrodynamic to hydrologic cells on a grid to grid basis consequently the hydrodynamic model is forced with output from pcr by exchanging runoff and discharge volumes between corresponding pcr and dfm cells for further information regarding glofrim we refer to hoch et al 2017b 2 4 hydrodynamic discretizations six different dfm discretizations of the lower elbe basin were designed fig 2 three with spatially varying grid size and three with uniform grid size for the flexible meshes we designed these set ups f1 used a length of 1600 m for its coarsest resolution whereas f2 used 3200 m both employed 400 m for its smallest cells comparable to f1 f3 also used 1600 m for its coarsest resolution but used only 800 m for its finest cell in order to evaluate the effects of both the largest and finest cell lengths for the regular grids we set up discretizations with 400 m r1 800 m r2 and 1600 m spatial resolution r3 to be compared with the flexible meshes additional descriptive statistics can be found in table 1 none of the hydrodynamic discretizations were calibrated as the impact of model parameters scales with spatial resolution fewtrell et al 2008 to derive the model discretization for dfm we used hydrosheds surface elevation and drainage network data at 15 arc seconds lehner et al 2008 to apply the height above nearest drainage algorithm hand rennó et al 2008 we opted for hand as it provides a tool for fast grid generation in terms of both data requirements and execution time and is thus well suited for large scale applications besides hand was applied for other inundation modelling studies nobre et al 2016 speckhann et al 2018 and has only a user defined upstream area threshold as possible source of uncertainty various levels of grid refinement were achieved by using different initial grid sizes as well as varying values for both minimum grid cell size and maximum model time step as 2 d floodplain elevation values we employed canopy removed surface elevation data baugh et al 2013 o loughlin et al 2016 and hydraulically smoothed it to account for the vertical measurement errors inherent in remotely sensed elevation data yamazaki et al 2017 2012 before assigning it to the 2 d part of the grids we based both the network and river width information of the 1 d channels on the global width database for large rivers gwd lr yamazaki et al 2014 while river depth information was derived by applying the equations of leopold and maddock 1953 bathymetric information was stored at cross sections with a spacing of around 10 km and subsequently interpolated between cross sections along the river network for both flexible meshes and regular grids the 1 d channel discretization remained unaltered to guarantee consistency between model runs we did not account for dikes and other man made structures due to the lack of reliable global data for our large scale applications and implementing them would otherwise introduce additional uncertainty to model results due to the same reason we desisted from using sub grid elevation data or spatially heterogeneous surface roughness values which would typically be done for catchment scale studies 2 5 assessment of model results all test cases were run for the period 01 january 2002 until 31 december 2010 after two years of spin up simulated discharge was validated at neu darchau nd tangermuende tm and torgau tg fig 2 by computing the coefficient of correlation r the root mean square error rmse and the model s skill expressed as the kling gupta efficiency kge gupta et al 2009 that way it is possible to assess the impact of different discretizations under different discharge regimes the required discharge observations were kindly provided by the german waterway and shipping administration wasser und schifffahrtsverwaltung des bundes wsv via the federal institute of hydrology bundesanstalt für gewässerkunde bfg to obtain an impression how simulated water levels differ throughout the basin they were compared qualitatively at six observation stations covering the up mid and downstream part of the basin fig 2 inundation extent was benchmarked for all discretizations similar to the approach and reasoning of fewtrell et al 2008 thereby the hit rate h the false alarm ratio f and the critical success index c were determined for each inundation map with respect to the map with the highest spatial resolution r1 h f and c were computed with the subsequent equations where nr 1 and ncomp indicate the number of inundated cells in of the benchmark map obtained with r1 and the map of the discretization to be compared respectively 1 h n c o m p n r 1 n r 1 2 f n c o m p n r 1 n c o m p n r 1 n c o m p n r 1 3 c n c o m p n r 1 n c o m p n r 1 all parameters can vary between 0 and 1 while h 1 shows that all inundated cells in the benchmark data are also inundated in the comparison data f 1 indicates that the inundated cells in the comparison are entirely false alarms with respect to the benchmark the critical success rate c in turn should be 1 for perfect agreement thereby penalizing for both under and overprediction unfortunately it was not achievable to validate simulated inundation extent against observations due to the lack of embankment height information and the resulting overestimation of simulated inundation extent simulated discharge water levels and inundation extent were put into perspective by assessing simulated water volumes which functions as a proxy for overbank water storage in addition run times are reported to evaluate the computational efficiency of the different grids 3 results and discussion 3 1 simulated discharge three observations can be made across all six discretizations regardless the gridding scheme first computed discharge exceeds observations for regular flow regimes but underpredicts discharge for peak flow conditions fig 3 further investigation revealed that this is not mostly due to the discharge overpredicted by pcr which thus already determines the potential accuracy of the coupled output second the magnitude of exceedance increases downstream as expressed by the increase in rmse table 2 a we postulate that this larger bias is caused at least partly by the absence of hydrological processes in the hydrodynamic model such as groundwater infiltration or evaporation last the absence of dikes influences the shape of all simulated hydrographs without dikes simulated discharge is smoother due to less flow constriction dampening and lagging particularly peak discharge while the different aspects do affect model accuracy all discretizations are however affected equally and hence further benchmarking is not hampered we assess the influence of the gridding technique applied first comparing the kges of the discretizations with 400 m f1 f2 and r1 and those with 800 m finest spatial resolution f3 and r2 reveals that the application of a regular grid improves model s skill insignificantly compared to a flexible mesh discretization if the same finer spatial resolution is applied table 2a additionally results obtained for the regular grid runs indicate that further coarsening of the grid from 800 m to 1600 m impacts discharge results less drastically than from 400 m to 800 m especially with respect to peak discharge computations evaluating the impact of spatial resolution on discharge estimates we find at nd that simulated discharge deviates only slightly between spatial resolutions fig 3a and table 2a for flexible meshes f1 and f2 show near identical discharge results while f3 yields lower estimates similarly f1 and f2 yield comparable discharge results at tm and tg the near identical results of f1 and f2 at all three stations suggest that the choice of the finest spatial resolution within a flexible mesh strongly determines the accuracy of discharge simulations while the coarsest resolution is less influential at these farther upstream stations however the deviation of f3 from f1 and f2 as well as of r3 from r1 and r2 is larger than at nd fig 3c f since discharge at nd differs hardly between discretization the overall discharge volumes passing tm and rt should also be comparable to exclude any water balance errors as this is not the case here we re run all discretization with cross sections covering the entire floodplain width to exclude uncaptured floodplain flow as cause comparing discharge obtained from channel flow fig 3 with the full floodplain discharge fig 4 suggest that with coarser cells a larger fraction of total downstream floodplain flow travels via the 2 d floodplain cells most likely due to the reduced number of 2 d cells available to accommodate floodplain flow since model skill is near identical at each station across set ups table 2a the new results provide insight into flood wave propagation at the most upstream station tr pcr discharge and dfm discharge correlate very strongly r 0 94 but with increasing downstream distance the discrepancy between discharge simulated without and with glofrim increases from r 0 75 at tm to r 0 57 at nd this underpins the above made assumption that not accounting for open water evaporation and groundwater infiltration in hydrodynamic models can lead to a reduction of model accuracy in the subsequent section their potential influence is analysed in more depth since as mentioned above discharge peaks are not well simulated by the hydrodynamic model we performed a peak above threshold analysis to assess performance for peak flows separately table 2b as threshold we used the long term mean discharge per station as reported by the bfg 1 1 hydrologic properties for each bfg station in the elbe basin can be found on the undine webpage http undine bafg de elbe elbegebiet html for nd 705 m3 s 1 for tm 562 m3 s 1 and for tr 340 m3 s 1 results suggest that for peak flow conditions the discretization approach opted for as well as the absence of dikes and other flood wave containing measures impacts model accuracy strongly and poses a limitation to using the here applied discretizations in an operational setting besides results corroborate that capturing floodplain flow for coarser discretizations is even more important for peak flow conditions these findings are however in line with expectations as we used only global data sets and thus applicability for local bespoke studies may be reduced ward et al 2015 3 2 water volume from fig 5 three groups can be distinguished the water volume of r3 which grossly exceeds all other simulated volumes an intermediate group consisting of f1 r2 and f3 and the group of f2 and r1 containing the least water volume in the system overall results show that the water volume stored in the system increases significantly when moving to a coarser discretization the aggregation rate expressed as slope of the linear fit ranges between 11 10 5 m3 d 1 and 25 10 5 m3 d 1 for r1 and r3 respectively such accumulation may potentially lead to overestimation of simulated water levels and discharge table 4a one possible cause for the increase in water volume storage may be the absence of feedback loops between hydrodynamics and hydrology as discussed above another reason for the accumulation may be the absence of small 1 d channels hampering the drainage of floodplains as well as the coarse 2 d elevation information obstructing important floodplain channel flows neal et al 2012 we conducted a first order assessment whether estimates of spatial temporal averages of both potential evaporation and groundwater infiltration rates could absorb the accumulated volumes b by multiplying the average of potential evaporation as used by pcr forcing 0 0019 m d 1 with inundation area table 4b a potential volumetric evaporation between 1 37 1011 m3 for r1 and 2 06 1011 m3 for r3 over the entire model period is obtained both greatly exceeding the total accumulated water volume for any discretization the average infiltration capacity expressed as the ksat value is with 0 15 m d 1 even higher than potential evaporation since both values exceed the actually accumulated water volume we cannot exclude the absence of hydrologic processes as cause for the aggregation a clear answer whether this or hindered dewatering of floodplains as reported by neal et al 2012 was the main driver can however not be unambiguously be provided 3 3 water level results show fig 6 that the chosen spatial resolution impacts simulated water levels at all stations regardless the application of flexible meshes or regular grids even though there are locally marked deviations coarser spatial resolutions result in higher water levels at most of the stations the main trend in higher water levels with coarser resolution is consistent with larger flood volumes during inundation the results can be explained by coarser spatial resolutions reducing connectivity as well as representation of both floodplain flow and floodplain channel processes which may result in locally higher water levels altenau et al 2017 horritt et al 2006 neal et al 2012 besides coarser spatial resolutions reduce dynamics and especially at upstream stations do not capture all inundation events even though there is no linear relation between coarsening of grid size and change in surface elevation at all six measuring points elevation values at observation stations tend to increase with spatial resolution potentially limiting the magnitude of water level fluctuations table 3 this decrease of elevation with spatial coarsening is due to spatial averaging of input elevation values savage et al 2016a also studies report a non linear connection between model results and bulk flow effects at coarser resolution as well as varying feedback loops at different resolution due to surrounding cells fewtrell et al 2008 hardy et al 1999 an unambiguous answer which is the driving factor is unfortunately not possible due to the system s complexity water levels of flexible meshes and regular grid per station generally compare well the closest fit between flexible and regular grids could be found for the upstream stations loc3b and loc3c as well as the most downstream station loc1 where the f1 f2 and r1 as well as f3 and r2 respectively exhibit near identical results 3 4 inundation extent we benchmarked inundation extent at the end of the simulations of all discretizations with r1 as reference map and computed contingency maps for visualization of the hit rate h false alarm ratio f and critical success rate c fig 7 it should be noted that similar as for the discharge results the absence of dikes and other man made structures in our discretizations results in overestimations of inundation extent and thus we desisted from performing an actual validation against observed inundation extent besides other factors potentially affecting inundation extent such as urban areas could not be included due to lacking data also including sub grid elevation data and spatially varying surface roughness values may have positively influenced the inundation extent obtained we find that not only the overall spatial resolution but also the gridding approach greatly impacts the agreement of inundation extent at the finest level table 4 b although f1 has the same finest grid size as r1 they agree only to 74 this suggests that accuracy of flexible meshes is reduced in those areas where a coarser spatial resolution is employed which is mostly in upstream areas this underlines the above made suggestion that the coarsest grid size has a marked impact on simulated inundation extent besides it seems that for a certain range of coarser discretizations it is inconsequential which cell size or gridding technique is opted for as h f and c are within close limits coarser resolution models tend to predict larger inundation extent not only on floodplains but also for areas farther away from the channels this again can be related to a lack of 2 d return flows with coarsened spatial resolution or missing hydrological feedback as shown above and in previous studies besides similar studies for regular grids also report an increase in inundation extent for coarser resolutions hardy et al 1999 which in turn is linked to a reduction of contingency and representativeness altenau et al 2017 horritt and bates 2001a savage et al 2016a altenau et al 2017 also concluded that the critical success index drops for coarser spatial resolutions due to averaging of channel and floodplain properties we nevertheless must acknowledge that when other mesh generation approaches other than hand would be applied as for example gis based approaches kumar et al 2009 results may differ as mesh size properties are to some extent configuration dependent 3 5 run time we find that especially for smaller resolutions the differences increase significantly which is in line with expectations and also found in comparable studies although merely considering regular grids altenau et al 2017 savage et al 2016a besides very similar performance in discharge computations run times are almost identical for both r2 and f3 this is because only 2 d cells adjacent to rivers will be inundated and thus run time does not depend on the overall number of 2 d cells but is mainly governed by the number of 2 d cells inundated even though f2 and f1 have the same finer spatial resolution run times differ markedly with the latter having a factor 1 73 longer run time the longest run time was as expected measured for r1 which is 12 longer than f1 while the difference between regular grids and flexible meshes is as expected results show that major gains can be obtained if doubling the coarsest spatial resolution for instance from 1600 m f1 to 3200 m f2 of the flexible mesh provided potential reduction of upstream model accuracy is acceptable 4 conclusion and recommendations to foster our understanding of differences between flexible meshes and regular grids as well as to better understand both advantages and shortcomings of using flexible meshes for large scale inundation modelling we compared six hydrodynamic discretizations of the lower elbe basin to facilitate the fast generation of meshes we used the height above nearest drainage hand algorithm comparability between runs was ensured by employing the glofrim framework allowing for identical spatially varying and explicit forcing of hydrodynamic models with hydrologic output we conclude that the spatial resolution of the hydrodynamic model discretization influences model skill in simulating discharge local water levels and agreement between inundation maps which complies with previous studies although performed with different models and on other scales altenau et al 2017 fewtrell et al 2008 hardy et al 1999 horritt and bates 2001a savage et al 2016b 2016a even though the findings are configuration dependent and we test only a sample of all possible discretizations those similarities across scales and applications are both confirmation of the robustness of our results and proof that these links are model independent and thus of more universal nature for discharge simulations the finest spatial resolution in the grid determines the accuracy furthermore there are no significant differences between the application of a flexible or a regular mesh if comparable in resolution this means that for discharge simulations less detailed discretizations are acceptable for areas farther away from the river system if the finest spatial resolution is sufficiently fine to capture both channel and floodplain flow processes this is crucial since with coarser spatial resolution a higher fraction of overall flow is conveyed via the 2 d part to some extent adding more 1 d channels could alleviate this but on basis of our results we find the spatial resolution the larger impediment since the here presented study solely employs large scale data sets for a catchment scale analysis a peak over threshold assessment shows that such approaches should be critically examined for detailed flood hazard and risk assessments as not considering structures such as dikes and drivers like spatially varying roughness coefficients may reduce the accuracy of discharge simulations unlike discharge assessing inundation extent exhibited stronger deviations between the gridding techniques generally a uniform and fine spatial resolution outperforms any coarser or flexible grid this is mostly due to the progressive coarsening of mesh size in upstream areas leading to larger simulated inundation extent once bankfull discharge capacity is exceeded to better understand to which extent the application of hand influenced the extent of simulated discharge and the model s skill in simulating peak discharge situations we recommend testing other mesh refinement approaches results suggest that applying a coarser spatial resolution enhances the accumulation and flow of water on the floodplains comparing the accumulated volumes with potential reduction due to evaporation or groundwater infiltration showed that these processes cannot be neglected as most hydrodynamic models do not simulate evaporation or groundwater infiltration accumulated water will remain there except for return flows consequently future work should focus on establishing feedback processes between inundation floodplains and hydrologic processes what can be derived from these findings is that there is a threshold resolution defining the limits of meaningfulness of mesh refinement only if a certain minimum fineness of resolution is given flow and inundation processes can be represented sufficiently well while this was already found to be true for regular grids horritt and bates 2001 this study illustrates similar patterns for flexible meshes as the relation between this resolution and model accuracy will most likely differ depending on basin and river dimensions as well as grid generation technique we recommend further research to establish a relation between basin properties and mesh design such knowledge would be of invaluable use for any large scale hydrodynamic study as essential time savings effects by grid size optimization could be achieved as a guideline for future applications of flexible meshes for large scale inundation studies we define three major aspects to consider if applying a flexible mesh for large scale inundation studies using hand to generate large scale flexible meshes is a fast and low level approach for large scale applications where more detailed topographical features can be neglected results for this test case underline importance of including smaller topographic features for bespoke and detailed catchment scale flood hazard and risk assessments local observations such as river discharge and floodplain water levels are less sensitive to coarse resolution flexible meshes in upstream areas a minimum fine spatial resolution must be met for floodplain areas to reduce volume conveyed via floodplains and facilitate return flows into channel domain wide output such as inundation extent profits from the application of uniform fine resolution regular grids as this study is the first of its kind focussing on comparing flexible meshes with regular grids using methods and data for large scale applications the number of flexible meshes used was limited to further increase our understanding of the confines of their applicability we recommend a study merely focussing on the impact of mesh variations but then with a wider range of discretizations to conclude we see potential in the application of flexible meshes for future hyper resolution large scale inundation studies but it also brings more responsibilities applicants of flexible mesh models need to put additional emphasis on the creation of the hydrodynamic discretization as it is the coarsest spatial resolution that may become the bottleneck of accuracy while we used the hand algorithm for fast and semi automated mesh creations we recommend testing other mesh generation approaches as well generally we think that mesh designs should be based on a number of considerations for instance for discharge simulation larger ratios between largest and smallest cell size are admissible whereas for inundation extent computations this ratio should be minimized also the context of the simulation needs to be considered are detailed estimates required or are short run times essential does one need high accuracy output for the entire domain or just a small part of it once the user has a clear idea of the study objectives the application of a flexible mesh can indeed serve as a time saving alternative to regular grids proving potentially useful for large scale operational or ensemble modelling studies where results need to be computed in brief time acknowledgements this study was financed by the european institute of innovation and technology climate kic programme under project title global high resolution database of current and future river flood hazard to support planning adaption and re insurance furthermore we kindly thank the german waterway and shipping administration wasser und schifffahrtsverwaltung des bundes wsv for providing the discharge measurements used for model validation we also thank edwin sutanudjaja for support with pcr globwb as well as herman kernkamp and arthur van dam for advice on delft3d flexible mesh we also thank two anonymous authors for their critical remarks on a previous version of this article supplementary materials supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2018 09 003 appendix supplementary materials image application 1 
761,spatiotemporal variations of the pore water pressure and water table response to small amounts of water exchange were measured in a sand column in the dewatering experiment the pressure head variation can be divided into three stages the initial pressure head declining phase when water table is above the sand surface a sudden drop in accordance with the capillary fringe when water level passes through the sand surface and a slow declining phase with respect to the specific yield of the sand as for the watering experiment of the shallow groundwater with different initial truncation factors tf the addition of a minute amount of water results in a rapid rise of the pressure head initially then a gradually increase to its equilibrium when the upper extent of the fully saturated capillary fringe coincides with the sand surface tf 1 a thin film of water addition produces nearly instantaneous increase in pore water pressure corresponding to a rapid rise of groundwater table to the sand surface pressure head increment increases linearly with the tf value and reaches its maximum when tf value approaches unity considering the vertical distribution of the pressure head its gradient becomes larger right after the water addition then decreases to its initial value of hydrostatic condition in the end in the case of tf 1 there is an unsaturated layer above the groundwater table the pressure head increases after the water addition whose magnitude however decreases as the increase of tf value nevertheless the final position of the groundwater table is still below the sand surface and the gradient of the pressure head vertical distribution dose not vary significantly throughout the entire process in consideration of hydrostatic pressure assumption temporal variation of the water table was also quantitatively obtained based on the measured instantaneous pore water pressure data which is consistent with time varying process of the pressure head during both the dewatering and watering experiments keywords pore water pressure head water table capillary fringe truncation factor dewatering and watering processes sand column experiment 1 introduction the presence of capillary fringe has a significant effect on the exchange of water between the ocean and the coastal beach particularly in the shallow aquifer the influence of capillary fringe on periodic water table dynamics is widely recognized in the literature nielsen and perrochet 2000 werner and lockington 2003 water exchange in shallow aquifer which can induce a significant change of pore water pressure and water table has been associated with various coastal problems e g saltwater intrusion structures stability the disposal of waste water and the mobility of beach sediments turner and nielsen 1997 nielsen et al 1997 horn 2006 many models based on the green and ampt 1911 approximations of the capillary effect on beach groundwater have been investigated parlange and brutsaert 1987 added a simple correction term of capillary effect to the boussinesq equation nielsen et al 1997 dealt with ground water waves in an aquifer with finite depth nielsen and perrochet 2000 proposed a concept of complex effective porosity including capillary effect which enables a simpler and more consistently accurate solution of water table subsequently cartwright et al 2005 extended previous experiments with a wider range of oscillation frequencies and improved the complex effective porosity as an empirical frequency dependent model among the experimental studies mentioned above the water table fluctuations were driven by simple harmonic forcing at the base of a sand column however these models cannot figure out the fundamental features of simple water discharge and recharge experiments and the corresponding groundwater table dynamics are still vague and needs to be further investigated in dewatering drainage experiments nielsen et al 1988 found the pressure head can drop tens of centimeters when small amounts of water are drawn slowly from a cylinder filled with beach sand which is referred to as the wieringermeer effect defined by hooghoudt 1947 this phenomenon was also reported by turner and nielsen 1997 in field tests of the swash zone as the wave falls back a few millimeters the highest drop of pressures head was up to 15 cm cartwright et al 2006 later noted that when the upper extent of the capillary fringe coinciding with the sand surface the pressure head may fluctuate up to the order of a capillary fringe height for the loss or gain of an amount of water of the order less than a grain diameter all the sharp drops mentioned above are because of the capillary effect more specifically the alternating appearance and disappearance of meniscuses in sand surface nielsen 2009 in watering wetting experiments some studies have shown that shallow water tables can often rise disproportionatedly compared to the volume of infiltrated water gillham 1984 observed that 0 3 cm s water addition would make a 30 cm rise of water table in 0 25 min termed the reverse wieringermeer effect he pointed out that values of the specific yield decrease as the water table approaches the ground surface and suggested that the capillary fringe is the cause of the rapid water table response which is consistent with dewatering experiments abdul and gillham 1984 conducted laboratory experiments to investigate the water table changing response to precipitation the experimental results showed clearly that if the capillary fringe extends to or near the ground surface the application of a thin film of water can lead to a rapid and large rise of water table in addition khaled et al 2011 applied the soil column experiment to study the shallow water table behaviors in response to various recharge events the tiny bore of water can relieve the tensions in the capillaries resulting in a rapid and high water table rise to the land surface most previous studies of the rapid water table changes in shallow aquifers focus on the magnitude of the sharply varying pressure head the detailed temporal variation of pore water pressure and the corresponding water table to the dewatering and watering processes has received relatively little attention regarding the vertical distribution of pressure head in capillary saturated zone gillham 1984 showed a different slope between its initial distribution and the distribution 1 0 min after the water was applied turner and nielsen 1997 monitored the temporal variation of pressure head when waves run up in a swash zone though the groundwater table can be indicated by the pressure head below the water table cartwright et al 2009 the specific position of the temporal water table was not presented since most of the previous studies arranged only one pressure sensor in the their tests to have a clear understanding of the entire process this study aims to demonstrate the spatiotemporal distribution of pore water pressure and water table response to the dewatering and watering laboratory experiments although some researchers have studied the water table rapid response to the dewatering and watering processes few investigations of the proximity between groundwater table and the sand surface have been taken into account healy and cook 2002 pointed out that in the case of a shallow groundwater table when capillary fringe is close to the soil surface and the water table depth is not great enough to allow moisture content at land surface to reach the value of residual moisture content the instantaneous water table rise could occur with only a small amount of water infiltration in periodic flow systems of sand column experiments cartwright et al 2004 revealed that the proximity of the surface effects only become apparent when the sand surface was approximately half a capillary fringe height above the maximum water table height which was further discussed in hilberts and torch 2006 in the development of the proximity of the water table to the sand surface cartwright et al 2009 showed a significant reduction in the moisture exchange and the dynamic effective porosity for a given water table fluctuation although there are some understandings of the proximity of the sand surface in periodic flow system the influence of different truncated capillary fringe on water rising experiments in a sand column is still unclear considering the influence of truncated capillary fringe on the water rising sand column experiments the present study adopted the parameter of truncation factor cartwright et al 2004 to reflect the relationship between the capillary fringe and the sand surface and to analyze the response of pore water pressure and water table to different truncated conditions findings from these experiments could provide useful insights into the fundamental shallow groundwater dynamic processes which can be used for better understanding of the relevant natural phenomenon the paper is organized as follows section 2 describes the sand column experiment setup and procedure sections 3 and 4 provide detailed analyses of experimental data of the dewatering and watering experiments respectively focusing on the spatiotemporal distribution of the pore water pressure head and water table finally conclusions are drawn in section 5 2 laboratory experiment 2 1 experimental setup experiments were performed using a sand column as shown in fig 1 the sand column is 0 7 m high with a rectangular cross section of 0 2 m 0 15 m a water outlet installed at the bottom was used to drain water from the sand column a 10 cm thick bottom porous cushion plate was deployed at the base of the column to dissipate the flow irregularity and turbulence generated during the drainage process six pressure taps with a uniform vertical interval of 8 cm along the sidewall of the sand column were connected to the pressure measuring system with six 10 mm diameter pressure tubes the bottom tap was set at 14 cm above the base of the column the water outlet and taps were screened using stainless steel mesh to ensure a high degree of permeability and avoid sand grains entering the outlet and pressure tubes at the top of the sand column a sprinkling system was installed which can make rapid and uniform water addition above the sand surface 2 2 sand parameters well sorted sand was used in the present sand column experiment it has a median diameter d50 0 32 mm and d90 d10 2 262 during the experiment wet sands were gradually added and packed uniformly into the water filled column this ensures that no air was trapped inside and the sand was completely saturated which also minimizes any layering due to different sand settling velocities and conserves the homogeneity of the sand column sand surface was also leveled to keep an initial flat sand surface before the experiment the porosity of sand is 0 48 and the thickness of sand layer inside the column is 48 5 cm 2 3 pore water pressure and water table the pore water pressures were measured using six pressure tubes these tubes are marked as a b c d e f from left to right in fig 1 reflecting the local pressure head hydraulic head of the six pressure taps which stand 44 5 cm 36 5 cm 28 5 cm 20 5 cm 12 5 cm 4 5 cm below the sand surface respectively in following discussions sand surface is set as the reference elevation for pressure head and water table calculation the pressure heads at the measuring points were calculated as the hydraulic head recorded from the pressure tube minus the gravitational head since the seepage velocity inside the sand column is fairly slow and its impact on the pressure field could be neglected the entire experimental process was recorded using a video camera to scrutinize the temporal variation of the pore water pressure and water table the instantaneous water table of the sand column is defined as the elevation where the pore water pressure is equal to the atmospheric pressure zero red dyes were added in the pressure tubes as indicators to facilitate the reading of pressure data 2 4 experiment implementation two kinds of laboratory experiments were carried out to investigate the temporal variation of pore water pressure and water table in the sand column under the dewatering and watering conditions as for the dewatering experiment the entire sand column was fully saturated and the initial water table was set as 1 5 cm above the sand surface when conducting the experiment outlet tap at the bottom of the column was slightly released and small volumes of water were gradually extracted from the sand column until the experimental results keep stable the hydraulic heads at different positions in the sand column were observed and recorded during the experiment until the entire column reached a new equilibrium stage in the case of watering experiment the initial truncated condition was achieved by draining small amounts of water through water filled saturated sand column afterwards the hydraulic heads from the pressure tubes were observed at the time when the heads kept stable and were uniform for all tubes the initial condition for the sand column watering experiment was reached and the initial groundwater table was recorded when conducting experiments a sprinkling of water 150 ml equivalent to a 0 5 cm application was spread over the sand surface as quickly and evenly as possible the hydraulic heads in the sand column were observed and recorded until a new static equilibrium stage was achieved 3 dewatering experiment 3 1 pore water pressure head fig 2 shows the pore water pressure response at six different measuring points in the sand column as small amounts of water gradually and slowly drain from the column following nielsen et al 1988 the abscissa is the equivalent head of water drawn with unit of cm drainage volume divided by the cross sectional area of the column the initial pressure heads of the six curves correspond to the elevation difference between the initial water table and the measuring point position at the beginning of the experiment all pressure heads are greater than zero indicating all points are located below the water table during the dewatering process pressure heads at points c d e f become negative indicating that these points move into the capillary saturation region which is above the instantaneous water table nevertheless point f standing at the highest position of the sand column shifts into the unsaturated region during the later dewatering process water drawn head 1 6 cm in fig 2 with air being trapped in the pressure tap due to the intrusion of air the pressure tube data of point f cannot correctly reflect the change of pressure head after water drawn head is greater than 1 6 cm therefore the corresponding air contaminated data of point f have been removed in fig 2 the pressure head variations of the six measuring points are quite similar with each other and the entire process obviously can be divided into three stages the first stage is when the water level is above the sand surface water drawn head 1 3 cm under which the slope of the curve is approximately unity the second stage is characterized by a sudden pressure head drop about 24 cm around the water drawn head of 1 5 cm which lasts only about 150 s during the dewatering experiment as confirmed through the water table variation in fig 4 in the third stage when the water table is below the sand surface water drawn head 1 8 cm the slopes of these curves represent the inverse of the specific yield of the sand n which is the volume of water released from storage per unit cross sectional area of the aquifer per unit decline in the elevation of the water head gillham 1984 therefore uniform slopes of pressure head distribution indicate the specific yield of the sand n 0 365 at the instant when water level passes through the sand surface the beginning of the second stage of the curve water drawn head is about 1 3 cm meniscus developed among the surface sand particles the rapid drop of pressure head is caused by the discontinuity of water pressure across the meniscus which follows the same principle of capillary height gillham 1984 nielsen et al 1988 this drop can also be treated as the capillary fringe hψ cartwright et al 2006 2009 which is an equivalent saturated height that can be achieved from the first drying curve cartwright et al 2005 assuming cubic packing of uniform spherical particles turner and nielsen 1997 proposed that the height of the capillary fringe can be estimated using 1 h ψ 10 σ ρ g d 50 where σ 0 073 n m is the surface tension of water ρ 1000 kg m3 is the density of water and g is the acceleration due to gravity eq 1 shows that hψ is related to the particle size the larger particle is the fewer decline of the pressure head experiment exhibits with d50 0 32 mm eq 1 yields a value of hψ 23 3 cm which is fairly consistent with the experiment observation i e a dropping head of 24 cm in fig 2 therefore the value of capillary fringe was determined to be hψ 24 cm in this study at the beginning of dewatering experiment when water drawn head is less than 0 5 cm a relatively large decreasing rate of the pressure head can be observed with the slope being greater than unity as shown in fig 2 especially for the lower points such as a and b in experiment seepage flow occurs inside the sand column right after the opening of the column outlet when water drawn head is less than 0 5 cm the decrease of pressure head could be ascribed to two mechanisms i e the amount due to outflow of 0 5 cm water and the amount to drive the seepage flow inside the sand fig 3 shows the relation between the measured pressure head decrement and the seepage length defined as the distance between the sand surface and the corresponding measuring point after extracting 0 5 cm water a linear relationship could be confirmed between these two parameters which follows the estimation from the darcy s law considering that the hydraulic conductivity and seepage velocity are almost the same for different measuring points the larger the seepage length is the more pressure head decrement occurs 3 2 temporal variation of water table inferred from the pore water pressure variation it is apparent that the water table response is rapid when the free water surface drops through the sand surface followed by a somewhat slower rate of decline as described by turner and nielsen 1997 the rapid drop response of water table is caused by the consequent formation of meniscuses among sand particles which has been interpreted in section 3 1 although the water dynamics seen in pressure head are considered to be indicative of what would be present in the water table the temporal variation of accurate water table is still unknown in the dewatering experiments nielsen et al 1988 cartwright et al 2009 fig 4 illustrates the temporal variation of the water table during the dewatering experiment here we assumed that the sand column was saturated and uniform thus the hydraulic gradients are regarded to be linear among measuring points gillham 1984 after knowing the vertical distribution of pressure head at any specified instant the instantaneous water table where the pore water pressure is zero can be identified subsequently the temporal variation of the water table during the entire dewatering process can be obtained when water table is above the sand surface time 350 s it descends gradually with the bottom water drainage at the time when water level passes through the sand surface there is a significant decline of water table about 24 cm between 350 s and 500 s the magnitude of rapid water table drop is the same as the capillary fringe hψ presented in section 3 1 afterwards it keeps a slow decreasing rate with water flowing out in the case that the water level is just above the sand surface decrease of small amounts of water can cause tens of centimeters water table drop which again confirms that capillary effect plays an important role in this process at the end of experiment time 960 s the seepage flow disappears and the water table rises slightly and reaches a static equilibrium state 4 watering experiment 4 1 pore water pressure head in the watering wetting experiments the truncated capillary fringe is important for water dynamics in natural beaches particularly for flat beaches where the groundwater table lies close to the sand surface cartwright et al 2004 investigated the truncation effect on the dispersion of pressure waves and found its influence is deemed to be negligible when the sand surface was approximately half a capillary fringe height above the maximum groundwater table height previous studies mainly concentrate on the shallow water aquifer in which capillary fringe extends above the sand surface understanding of groundwater dynamics response to the water addition in the case where the top of capillary fringe does not extend to the ground surface is still limited and needed more clarification miyazaki et al 2012 to further reveal the influence of truncated capillary fringe on groundwater dynamics after water application watering experiments with different truncation factors tf were constructed by spreading a small quantity of water over the surface of sand column tf is a factor presenting the room for the formation of the capillary fringe cartwright et al 2004 2 t f z s a n d h 0 h ψ where h 0 is the initial elevation of the groundwater table zsand is the elevation of sand surface when tf 0 groundwater table is located at the sand surface and there is no room for the formation of the capillary fringe when tf 1 the capillary fringe is above the sand surface and the region between the sand surface and groundwater table is fully saturated when tf 1 the capillary fringe is below the sand surface and there exists an unsaturated zone between the top of capillary fringe and sand surface after knowing the height of the capillary fringe through the dewatering experiment hψ 24 cm a series of watering experiments with different tf values were conducted the corresponding experiment conditions are summarized in table 1 fig 5 shows the temporal variation of the pressure heads in the case of tf 0 369 and tf 1 175 time zero is set at the instant when water is first applied initially the pressure head is the distance from groundwater table to the measuring point as shown in fig 5 a the initial values of points a b c d and e are greater than zero indicating these points were all located below the groundwater table at the beginning of the water addition point f however was in the capillary saturated zone before water addition because its initial pressure head is negative nevertheless three points a b c were initially below the groundwater table whereas points d e were in the capillary saturated zone for tf 1 175 in general temporal variations of the pressure head are similar for different points in the watering experiment the pressure heads grow fast immediately after water addition then slow down and finally reach an equilibrium phase in about 60 s after the water addition which is in agreement with khaled et al 2011 with the quick application of 150 ml water 0 5 cm water height relative to the sand column it is clear that there exists significant difference of pressure head variation between the two representative cases in fig 5 as for tf 1 175 the pressure head increment caused by water addition is smaller than that of tf 0 369 the pressure head increment for tf 0 369 is the distance from the initial groundwater table to the sand surface since groundwater table extends to the sand surface after water addition when there is an unsaturated top layer in the sand column tf 1 175 small amounts of water addition first reacts to the unsaturated layer to increase its water content then passes down to the saturated capillary layer the increment of pressure head is therefore smaller than the distance from the initial groundwater table to the sand surface further scrutinizing the temporal variation of the measured pressure head some points are always located in the same region during the entire experiments for example points a b c d e in fig 5 a and a b c in fig 5 b all present positive values indicating they stay below the groundwater table nevertheless points d e in fig 5 b with negative values are always in the capillary saturated zone with water addition in addition point f in fig 5 a shows a transition from the capillary saturated zone to region below the groundwater table to clarify the influence of tf values to the pressure head variation point e was selected and whose results are presented in fig 6 after water addition three curves with tf values less than unity reach to a value of 12 5 cm at the end of the test while other three curves terminate at smaller values because the sand is completely saturated above the initial groundwater table in the case of tf 1 with small amounts of water addition the meniscus formed among the surface sand particles disappear and the groundwater table grows up to the sand surface considering that point e is located 12 5 cm below the sand surface this phenomenon has also been reported by gillham 1984 the present laboratory based experimental data show a stable stage finally whereas gillham s field test shows all water disappeared about 1 min after the water application in the case that tf is less than unity it is found that the lower the initial groundwater table is the faster and larger pressure head increases when tf is slightly larger than unity tf 1 029 and 1 042 the time to reach the stable pressure head is shorter than others as for the large tf value tf 1 175 the pressure head increment is limited even smaller than that of tf 0 396 whereas its varying duration becomes longer than that of tf 1 029 and 1 042 when the initial groundwater table h0 is above 12 5 cm tf 0 369 point e is always below the water table showing positive pressure heads in the case of 24 cm h0 12 5 cm tf 0 583 and 0 729 point e undergoes a transition procedure from the capillary saturated zone negative pressure head to below the groundwater table positive pressure head as for 24 6 cm h0 24 cm tf 1 029 transition from capillary zone to below the groundwater table also occurs while the terminal value of pressure head cannot reach 12 5 cm when 25 cm h0 24 6 cm tf 1 042 increment of pressure head still occurs but it cannot reach a positive value since an unsaturated zone exists near the sand surface as for h0 25 cm tf 1 175 point e always stays in the capillary saturated zone although increasing by a certain amount the pressure head always shows negative values the influence of truncated capillary on pressure head increment is further demonstrated in fig 7 experiment results of pressure head increment are much larger than the calculated static pressure head increment demonstrating that the capillary fringe has a great influence on the experimental process considering that the amount of water added is small when tf 1 shallow aquifer a linear relationship passing the origin between tf and pressure head increment is confirmed as shown using a solid line in fig 7 this can also be demonstrated in eq 2 since the pressure head increment for tf 1 is zsand h 0 accordingly the linear coefficient between pressure head increment and tf is the capillary fringe hψ the maximum pressure head increment occurs at tf 1 which is about the capillary fringe hψ 24 cm if tf is greater than unity the pressure head increment reduces a sharp decline is observed when tf is slightly greater than unity extending the previous understanding that there has little influence on water dynamics when tf 1 2 cartwright et al 2004 the present study demonstrates that the influence of tf values especially near unity has a significant effect on the groundwater dynamics referring to the field tests of gillham 1984 temporal variation of vertical head distribution after a minute amount of water application is shown in fig 8 assuming the medium sand was saturated and uniform the hydraulic gradients were thus considered to be linear between measured points in fig 8 the initial hydraulic head h the instantaneous pressure head h and the initial gravitational head z are plotted as functions of depth dashed lines show the pressure head distributions at different instants after water application for tf 0 729 and tf 1 175 initially the groundwater table was situated at a depth of 17 5 cm and 28 2 cm below the sand surface for these two cases respectively and the vertical distributions of pressure head have the same static equilibrium gradient of 1 1 however pressure head gradient shows different changing features during the watering experiment for these two cases as for tf 0 729 and during the water addition the vertical pressure head gradient becomes larger at first then reduces and finally returns to the initial static equilibrium gradient the change of pressure head gradient indicates downward seepage flow condition in the capillary saturated zone which was consistent with gillham 1984 when tf 1 175 pressure head gradient in the saturated capillary zone has no obvious change during the water addition process different from the former case as for tf 0 729 water is directly added to the capillary saturated zone and has a great effect on the pressure head distribution nevertheless for tf 1 175 water is added to the surface unsaturated zone though some could transfer to the bottom capillary saturated zone clear modification of the vertical pressure head gradient cannot be observed after the water addition groundwater table rises significantly to the sand surface in fig 8 a whereas it rises only a small amount 5 2 cm in fig 8 b and the final groundwater table is still far below the sand surface 4 2 temporal variation of water table temporal variations of groundwater table under different tf values are presented in fig 9 same as fig 4 water table was obtained on the basis of the vertical distribution of the pore water pressure in general the groundwater table rises quickly after water addition then slows down and finally reaches an equilibrium stage in about 60 s which is similar to the pressure head variation as shown in fig 6 with an addition of only 150 ml water the groundwater table rises to the sand surface for tf 1 when tf 1 the final groundwater table is still below the sand surface similar to the pressure head if tf value is closed to unity i e tf 1 029 and 1 042 temporal variation of the groundwater table is rather sensitive and it takes less time to reach equilibrium the curve of tf 1 175 is flatter than others which is because an unsaturated sand layer exists near the sand surface and a small water addition could not contribute to the significant increase of groundwater table 5 conclusions a series of dewatering and watering experiments were carried out to investigate the influence of capillary fringe and truncation factors in a homogeneous laboratory sand column detailed spatiotemporal measurements of the pore water pressure and water table response to small amounts of water exchange have been conducted and discussed in dewatering experiments three stages of the pressure head variation were observed i e the gradual decline of pressure head when water table is above the sand surface a rapid sharp drop when water level goes through the sand surface and a decline consistent with the specific yield of the sand when water level is below the sand surface with a detailed vertical resolution of the measured pressure heads temporal variation of the water table was quantitatively obtained showing similar features as the pressure head in watering experiments of shallow aquifer a 0 5 cm water addition can induce disproportionate increment of pressure head which grows fast immediately after water addition then slows down and finally reaches an equilibrium stage different truncation factors tf show a significant influence on the ground water dynamics in the watering experiment as for tf 1 the upper extent of fully saturated capillary fringe coincides with the sand surface and small amounts of water addition produce nearly instantaneous increase in pore water pressure corresponding to a rapid rise of water table to sand surface increment of the pressure head shows a linear relationship with tf values for tf 1 approaching its maximum value when tf is equal to unity considering the vertical distribution of the pressure head its gradient becomes larger right after the water addition and then decreases to its initial value in the end with respect to the hydrostatic condition in the case of tf 1 under which there is an unsaturated layer above the groundwater table the pressure head increases after the water addition whose magnitude however decreases with the increase of tf value especially near the value of unity nevertheless the final position of the groundwater table is still below the sand surface and the gradient of pressure head vertical distribution dose not vary significantly during the entire watering process acknowledgments authors are grateful to prof peter nielsen for inspiration of this study this study was financially supported by the national key r d program of china 2017yfc0405401 and the natural science foundation of china no 11632012 no 51761135015 
761,spatiotemporal variations of the pore water pressure and water table response to small amounts of water exchange were measured in a sand column in the dewatering experiment the pressure head variation can be divided into three stages the initial pressure head declining phase when water table is above the sand surface a sudden drop in accordance with the capillary fringe when water level passes through the sand surface and a slow declining phase with respect to the specific yield of the sand as for the watering experiment of the shallow groundwater with different initial truncation factors tf the addition of a minute amount of water results in a rapid rise of the pressure head initially then a gradually increase to its equilibrium when the upper extent of the fully saturated capillary fringe coincides with the sand surface tf 1 a thin film of water addition produces nearly instantaneous increase in pore water pressure corresponding to a rapid rise of groundwater table to the sand surface pressure head increment increases linearly with the tf value and reaches its maximum when tf value approaches unity considering the vertical distribution of the pressure head its gradient becomes larger right after the water addition then decreases to its initial value of hydrostatic condition in the end in the case of tf 1 there is an unsaturated layer above the groundwater table the pressure head increases after the water addition whose magnitude however decreases as the increase of tf value nevertheless the final position of the groundwater table is still below the sand surface and the gradient of the pressure head vertical distribution dose not vary significantly throughout the entire process in consideration of hydrostatic pressure assumption temporal variation of the water table was also quantitatively obtained based on the measured instantaneous pore water pressure data which is consistent with time varying process of the pressure head during both the dewatering and watering experiments keywords pore water pressure head water table capillary fringe truncation factor dewatering and watering processes sand column experiment 1 introduction the presence of capillary fringe has a significant effect on the exchange of water between the ocean and the coastal beach particularly in the shallow aquifer the influence of capillary fringe on periodic water table dynamics is widely recognized in the literature nielsen and perrochet 2000 werner and lockington 2003 water exchange in shallow aquifer which can induce a significant change of pore water pressure and water table has been associated with various coastal problems e g saltwater intrusion structures stability the disposal of waste water and the mobility of beach sediments turner and nielsen 1997 nielsen et al 1997 horn 2006 many models based on the green and ampt 1911 approximations of the capillary effect on beach groundwater have been investigated parlange and brutsaert 1987 added a simple correction term of capillary effect to the boussinesq equation nielsen et al 1997 dealt with ground water waves in an aquifer with finite depth nielsen and perrochet 2000 proposed a concept of complex effective porosity including capillary effect which enables a simpler and more consistently accurate solution of water table subsequently cartwright et al 2005 extended previous experiments with a wider range of oscillation frequencies and improved the complex effective porosity as an empirical frequency dependent model among the experimental studies mentioned above the water table fluctuations were driven by simple harmonic forcing at the base of a sand column however these models cannot figure out the fundamental features of simple water discharge and recharge experiments and the corresponding groundwater table dynamics are still vague and needs to be further investigated in dewatering drainage experiments nielsen et al 1988 found the pressure head can drop tens of centimeters when small amounts of water are drawn slowly from a cylinder filled with beach sand which is referred to as the wieringermeer effect defined by hooghoudt 1947 this phenomenon was also reported by turner and nielsen 1997 in field tests of the swash zone as the wave falls back a few millimeters the highest drop of pressures head was up to 15 cm cartwright et al 2006 later noted that when the upper extent of the capillary fringe coinciding with the sand surface the pressure head may fluctuate up to the order of a capillary fringe height for the loss or gain of an amount of water of the order less than a grain diameter all the sharp drops mentioned above are because of the capillary effect more specifically the alternating appearance and disappearance of meniscuses in sand surface nielsen 2009 in watering wetting experiments some studies have shown that shallow water tables can often rise disproportionatedly compared to the volume of infiltrated water gillham 1984 observed that 0 3 cm s water addition would make a 30 cm rise of water table in 0 25 min termed the reverse wieringermeer effect he pointed out that values of the specific yield decrease as the water table approaches the ground surface and suggested that the capillary fringe is the cause of the rapid water table response which is consistent with dewatering experiments abdul and gillham 1984 conducted laboratory experiments to investigate the water table changing response to precipitation the experimental results showed clearly that if the capillary fringe extends to or near the ground surface the application of a thin film of water can lead to a rapid and large rise of water table in addition khaled et al 2011 applied the soil column experiment to study the shallow water table behaviors in response to various recharge events the tiny bore of water can relieve the tensions in the capillaries resulting in a rapid and high water table rise to the land surface most previous studies of the rapid water table changes in shallow aquifers focus on the magnitude of the sharply varying pressure head the detailed temporal variation of pore water pressure and the corresponding water table to the dewatering and watering processes has received relatively little attention regarding the vertical distribution of pressure head in capillary saturated zone gillham 1984 showed a different slope between its initial distribution and the distribution 1 0 min after the water was applied turner and nielsen 1997 monitored the temporal variation of pressure head when waves run up in a swash zone though the groundwater table can be indicated by the pressure head below the water table cartwright et al 2009 the specific position of the temporal water table was not presented since most of the previous studies arranged only one pressure sensor in the their tests to have a clear understanding of the entire process this study aims to demonstrate the spatiotemporal distribution of pore water pressure and water table response to the dewatering and watering laboratory experiments although some researchers have studied the water table rapid response to the dewatering and watering processes few investigations of the proximity between groundwater table and the sand surface have been taken into account healy and cook 2002 pointed out that in the case of a shallow groundwater table when capillary fringe is close to the soil surface and the water table depth is not great enough to allow moisture content at land surface to reach the value of residual moisture content the instantaneous water table rise could occur with only a small amount of water infiltration in periodic flow systems of sand column experiments cartwright et al 2004 revealed that the proximity of the surface effects only become apparent when the sand surface was approximately half a capillary fringe height above the maximum water table height which was further discussed in hilberts and torch 2006 in the development of the proximity of the water table to the sand surface cartwright et al 2009 showed a significant reduction in the moisture exchange and the dynamic effective porosity for a given water table fluctuation although there are some understandings of the proximity of the sand surface in periodic flow system the influence of different truncated capillary fringe on water rising experiments in a sand column is still unclear considering the influence of truncated capillary fringe on the water rising sand column experiments the present study adopted the parameter of truncation factor cartwright et al 2004 to reflect the relationship between the capillary fringe and the sand surface and to analyze the response of pore water pressure and water table to different truncated conditions findings from these experiments could provide useful insights into the fundamental shallow groundwater dynamic processes which can be used for better understanding of the relevant natural phenomenon the paper is organized as follows section 2 describes the sand column experiment setup and procedure sections 3 and 4 provide detailed analyses of experimental data of the dewatering and watering experiments respectively focusing on the spatiotemporal distribution of the pore water pressure head and water table finally conclusions are drawn in section 5 2 laboratory experiment 2 1 experimental setup experiments were performed using a sand column as shown in fig 1 the sand column is 0 7 m high with a rectangular cross section of 0 2 m 0 15 m a water outlet installed at the bottom was used to drain water from the sand column a 10 cm thick bottom porous cushion plate was deployed at the base of the column to dissipate the flow irregularity and turbulence generated during the drainage process six pressure taps with a uniform vertical interval of 8 cm along the sidewall of the sand column were connected to the pressure measuring system with six 10 mm diameter pressure tubes the bottom tap was set at 14 cm above the base of the column the water outlet and taps were screened using stainless steel mesh to ensure a high degree of permeability and avoid sand grains entering the outlet and pressure tubes at the top of the sand column a sprinkling system was installed which can make rapid and uniform water addition above the sand surface 2 2 sand parameters well sorted sand was used in the present sand column experiment it has a median diameter d50 0 32 mm and d90 d10 2 262 during the experiment wet sands were gradually added and packed uniformly into the water filled column this ensures that no air was trapped inside and the sand was completely saturated which also minimizes any layering due to different sand settling velocities and conserves the homogeneity of the sand column sand surface was also leveled to keep an initial flat sand surface before the experiment the porosity of sand is 0 48 and the thickness of sand layer inside the column is 48 5 cm 2 3 pore water pressure and water table the pore water pressures were measured using six pressure tubes these tubes are marked as a b c d e f from left to right in fig 1 reflecting the local pressure head hydraulic head of the six pressure taps which stand 44 5 cm 36 5 cm 28 5 cm 20 5 cm 12 5 cm 4 5 cm below the sand surface respectively in following discussions sand surface is set as the reference elevation for pressure head and water table calculation the pressure heads at the measuring points were calculated as the hydraulic head recorded from the pressure tube minus the gravitational head since the seepage velocity inside the sand column is fairly slow and its impact on the pressure field could be neglected the entire experimental process was recorded using a video camera to scrutinize the temporal variation of the pore water pressure and water table the instantaneous water table of the sand column is defined as the elevation where the pore water pressure is equal to the atmospheric pressure zero red dyes were added in the pressure tubes as indicators to facilitate the reading of pressure data 2 4 experiment implementation two kinds of laboratory experiments were carried out to investigate the temporal variation of pore water pressure and water table in the sand column under the dewatering and watering conditions as for the dewatering experiment the entire sand column was fully saturated and the initial water table was set as 1 5 cm above the sand surface when conducting the experiment outlet tap at the bottom of the column was slightly released and small volumes of water were gradually extracted from the sand column until the experimental results keep stable the hydraulic heads at different positions in the sand column were observed and recorded during the experiment until the entire column reached a new equilibrium stage in the case of watering experiment the initial truncated condition was achieved by draining small amounts of water through water filled saturated sand column afterwards the hydraulic heads from the pressure tubes were observed at the time when the heads kept stable and were uniform for all tubes the initial condition for the sand column watering experiment was reached and the initial groundwater table was recorded when conducting experiments a sprinkling of water 150 ml equivalent to a 0 5 cm application was spread over the sand surface as quickly and evenly as possible the hydraulic heads in the sand column were observed and recorded until a new static equilibrium stage was achieved 3 dewatering experiment 3 1 pore water pressure head fig 2 shows the pore water pressure response at six different measuring points in the sand column as small amounts of water gradually and slowly drain from the column following nielsen et al 1988 the abscissa is the equivalent head of water drawn with unit of cm drainage volume divided by the cross sectional area of the column the initial pressure heads of the six curves correspond to the elevation difference between the initial water table and the measuring point position at the beginning of the experiment all pressure heads are greater than zero indicating all points are located below the water table during the dewatering process pressure heads at points c d e f become negative indicating that these points move into the capillary saturation region which is above the instantaneous water table nevertheless point f standing at the highest position of the sand column shifts into the unsaturated region during the later dewatering process water drawn head 1 6 cm in fig 2 with air being trapped in the pressure tap due to the intrusion of air the pressure tube data of point f cannot correctly reflect the change of pressure head after water drawn head is greater than 1 6 cm therefore the corresponding air contaminated data of point f have been removed in fig 2 the pressure head variations of the six measuring points are quite similar with each other and the entire process obviously can be divided into three stages the first stage is when the water level is above the sand surface water drawn head 1 3 cm under which the slope of the curve is approximately unity the second stage is characterized by a sudden pressure head drop about 24 cm around the water drawn head of 1 5 cm which lasts only about 150 s during the dewatering experiment as confirmed through the water table variation in fig 4 in the third stage when the water table is below the sand surface water drawn head 1 8 cm the slopes of these curves represent the inverse of the specific yield of the sand n which is the volume of water released from storage per unit cross sectional area of the aquifer per unit decline in the elevation of the water head gillham 1984 therefore uniform slopes of pressure head distribution indicate the specific yield of the sand n 0 365 at the instant when water level passes through the sand surface the beginning of the second stage of the curve water drawn head is about 1 3 cm meniscus developed among the surface sand particles the rapid drop of pressure head is caused by the discontinuity of water pressure across the meniscus which follows the same principle of capillary height gillham 1984 nielsen et al 1988 this drop can also be treated as the capillary fringe hψ cartwright et al 2006 2009 which is an equivalent saturated height that can be achieved from the first drying curve cartwright et al 2005 assuming cubic packing of uniform spherical particles turner and nielsen 1997 proposed that the height of the capillary fringe can be estimated using 1 h ψ 10 σ ρ g d 50 where σ 0 073 n m is the surface tension of water ρ 1000 kg m3 is the density of water and g is the acceleration due to gravity eq 1 shows that hψ is related to the particle size the larger particle is the fewer decline of the pressure head experiment exhibits with d50 0 32 mm eq 1 yields a value of hψ 23 3 cm which is fairly consistent with the experiment observation i e a dropping head of 24 cm in fig 2 therefore the value of capillary fringe was determined to be hψ 24 cm in this study at the beginning of dewatering experiment when water drawn head is less than 0 5 cm a relatively large decreasing rate of the pressure head can be observed with the slope being greater than unity as shown in fig 2 especially for the lower points such as a and b in experiment seepage flow occurs inside the sand column right after the opening of the column outlet when water drawn head is less than 0 5 cm the decrease of pressure head could be ascribed to two mechanisms i e the amount due to outflow of 0 5 cm water and the amount to drive the seepage flow inside the sand fig 3 shows the relation between the measured pressure head decrement and the seepage length defined as the distance between the sand surface and the corresponding measuring point after extracting 0 5 cm water a linear relationship could be confirmed between these two parameters which follows the estimation from the darcy s law considering that the hydraulic conductivity and seepage velocity are almost the same for different measuring points the larger the seepage length is the more pressure head decrement occurs 3 2 temporal variation of water table inferred from the pore water pressure variation it is apparent that the water table response is rapid when the free water surface drops through the sand surface followed by a somewhat slower rate of decline as described by turner and nielsen 1997 the rapid drop response of water table is caused by the consequent formation of meniscuses among sand particles which has been interpreted in section 3 1 although the water dynamics seen in pressure head are considered to be indicative of what would be present in the water table the temporal variation of accurate water table is still unknown in the dewatering experiments nielsen et al 1988 cartwright et al 2009 fig 4 illustrates the temporal variation of the water table during the dewatering experiment here we assumed that the sand column was saturated and uniform thus the hydraulic gradients are regarded to be linear among measuring points gillham 1984 after knowing the vertical distribution of pressure head at any specified instant the instantaneous water table where the pore water pressure is zero can be identified subsequently the temporal variation of the water table during the entire dewatering process can be obtained when water table is above the sand surface time 350 s it descends gradually with the bottom water drainage at the time when water level passes through the sand surface there is a significant decline of water table about 24 cm between 350 s and 500 s the magnitude of rapid water table drop is the same as the capillary fringe hψ presented in section 3 1 afterwards it keeps a slow decreasing rate with water flowing out in the case that the water level is just above the sand surface decrease of small amounts of water can cause tens of centimeters water table drop which again confirms that capillary effect plays an important role in this process at the end of experiment time 960 s the seepage flow disappears and the water table rises slightly and reaches a static equilibrium state 4 watering experiment 4 1 pore water pressure head in the watering wetting experiments the truncated capillary fringe is important for water dynamics in natural beaches particularly for flat beaches where the groundwater table lies close to the sand surface cartwright et al 2004 investigated the truncation effect on the dispersion of pressure waves and found its influence is deemed to be negligible when the sand surface was approximately half a capillary fringe height above the maximum groundwater table height previous studies mainly concentrate on the shallow water aquifer in which capillary fringe extends above the sand surface understanding of groundwater dynamics response to the water addition in the case where the top of capillary fringe does not extend to the ground surface is still limited and needed more clarification miyazaki et al 2012 to further reveal the influence of truncated capillary fringe on groundwater dynamics after water application watering experiments with different truncation factors tf were constructed by spreading a small quantity of water over the surface of sand column tf is a factor presenting the room for the formation of the capillary fringe cartwright et al 2004 2 t f z s a n d h 0 h ψ where h 0 is the initial elevation of the groundwater table zsand is the elevation of sand surface when tf 0 groundwater table is located at the sand surface and there is no room for the formation of the capillary fringe when tf 1 the capillary fringe is above the sand surface and the region between the sand surface and groundwater table is fully saturated when tf 1 the capillary fringe is below the sand surface and there exists an unsaturated zone between the top of capillary fringe and sand surface after knowing the height of the capillary fringe through the dewatering experiment hψ 24 cm a series of watering experiments with different tf values were conducted the corresponding experiment conditions are summarized in table 1 fig 5 shows the temporal variation of the pressure heads in the case of tf 0 369 and tf 1 175 time zero is set at the instant when water is first applied initially the pressure head is the distance from groundwater table to the measuring point as shown in fig 5 a the initial values of points a b c d and e are greater than zero indicating these points were all located below the groundwater table at the beginning of the water addition point f however was in the capillary saturated zone before water addition because its initial pressure head is negative nevertheless three points a b c were initially below the groundwater table whereas points d e were in the capillary saturated zone for tf 1 175 in general temporal variations of the pressure head are similar for different points in the watering experiment the pressure heads grow fast immediately after water addition then slow down and finally reach an equilibrium phase in about 60 s after the water addition which is in agreement with khaled et al 2011 with the quick application of 150 ml water 0 5 cm water height relative to the sand column it is clear that there exists significant difference of pressure head variation between the two representative cases in fig 5 as for tf 1 175 the pressure head increment caused by water addition is smaller than that of tf 0 369 the pressure head increment for tf 0 369 is the distance from the initial groundwater table to the sand surface since groundwater table extends to the sand surface after water addition when there is an unsaturated top layer in the sand column tf 1 175 small amounts of water addition first reacts to the unsaturated layer to increase its water content then passes down to the saturated capillary layer the increment of pressure head is therefore smaller than the distance from the initial groundwater table to the sand surface further scrutinizing the temporal variation of the measured pressure head some points are always located in the same region during the entire experiments for example points a b c d e in fig 5 a and a b c in fig 5 b all present positive values indicating they stay below the groundwater table nevertheless points d e in fig 5 b with negative values are always in the capillary saturated zone with water addition in addition point f in fig 5 a shows a transition from the capillary saturated zone to region below the groundwater table to clarify the influence of tf values to the pressure head variation point e was selected and whose results are presented in fig 6 after water addition three curves with tf values less than unity reach to a value of 12 5 cm at the end of the test while other three curves terminate at smaller values because the sand is completely saturated above the initial groundwater table in the case of tf 1 with small amounts of water addition the meniscus formed among the surface sand particles disappear and the groundwater table grows up to the sand surface considering that point e is located 12 5 cm below the sand surface this phenomenon has also been reported by gillham 1984 the present laboratory based experimental data show a stable stage finally whereas gillham s field test shows all water disappeared about 1 min after the water application in the case that tf is less than unity it is found that the lower the initial groundwater table is the faster and larger pressure head increases when tf is slightly larger than unity tf 1 029 and 1 042 the time to reach the stable pressure head is shorter than others as for the large tf value tf 1 175 the pressure head increment is limited even smaller than that of tf 0 396 whereas its varying duration becomes longer than that of tf 1 029 and 1 042 when the initial groundwater table h0 is above 12 5 cm tf 0 369 point e is always below the water table showing positive pressure heads in the case of 24 cm h0 12 5 cm tf 0 583 and 0 729 point e undergoes a transition procedure from the capillary saturated zone negative pressure head to below the groundwater table positive pressure head as for 24 6 cm h0 24 cm tf 1 029 transition from capillary zone to below the groundwater table also occurs while the terminal value of pressure head cannot reach 12 5 cm when 25 cm h0 24 6 cm tf 1 042 increment of pressure head still occurs but it cannot reach a positive value since an unsaturated zone exists near the sand surface as for h0 25 cm tf 1 175 point e always stays in the capillary saturated zone although increasing by a certain amount the pressure head always shows negative values the influence of truncated capillary on pressure head increment is further demonstrated in fig 7 experiment results of pressure head increment are much larger than the calculated static pressure head increment demonstrating that the capillary fringe has a great influence on the experimental process considering that the amount of water added is small when tf 1 shallow aquifer a linear relationship passing the origin between tf and pressure head increment is confirmed as shown using a solid line in fig 7 this can also be demonstrated in eq 2 since the pressure head increment for tf 1 is zsand h 0 accordingly the linear coefficient between pressure head increment and tf is the capillary fringe hψ the maximum pressure head increment occurs at tf 1 which is about the capillary fringe hψ 24 cm if tf is greater than unity the pressure head increment reduces a sharp decline is observed when tf is slightly greater than unity extending the previous understanding that there has little influence on water dynamics when tf 1 2 cartwright et al 2004 the present study demonstrates that the influence of tf values especially near unity has a significant effect on the groundwater dynamics referring to the field tests of gillham 1984 temporal variation of vertical head distribution after a minute amount of water application is shown in fig 8 assuming the medium sand was saturated and uniform the hydraulic gradients were thus considered to be linear between measured points in fig 8 the initial hydraulic head h the instantaneous pressure head h and the initial gravitational head z are plotted as functions of depth dashed lines show the pressure head distributions at different instants after water application for tf 0 729 and tf 1 175 initially the groundwater table was situated at a depth of 17 5 cm and 28 2 cm below the sand surface for these two cases respectively and the vertical distributions of pressure head have the same static equilibrium gradient of 1 1 however pressure head gradient shows different changing features during the watering experiment for these two cases as for tf 0 729 and during the water addition the vertical pressure head gradient becomes larger at first then reduces and finally returns to the initial static equilibrium gradient the change of pressure head gradient indicates downward seepage flow condition in the capillary saturated zone which was consistent with gillham 1984 when tf 1 175 pressure head gradient in the saturated capillary zone has no obvious change during the water addition process different from the former case as for tf 0 729 water is directly added to the capillary saturated zone and has a great effect on the pressure head distribution nevertheless for tf 1 175 water is added to the surface unsaturated zone though some could transfer to the bottom capillary saturated zone clear modification of the vertical pressure head gradient cannot be observed after the water addition groundwater table rises significantly to the sand surface in fig 8 a whereas it rises only a small amount 5 2 cm in fig 8 b and the final groundwater table is still far below the sand surface 4 2 temporal variation of water table temporal variations of groundwater table under different tf values are presented in fig 9 same as fig 4 water table was obtained on the basis of the vertical distribution of the pore water pressure in general the groundwater table rises quickly after water addition then slows down and finally reaches an equilibrium stage in about 60 s which is similar to the pressure head variation as shown in fig 6 with an addition of only 150 ml water the groundwater table rises to the sand surface for tf 1 when tf 1 the final groundwater table is still below the sand surface similar to the pressure head if tf value is closed to unity i e tf 1 029 and 1 042 temporal variation of the groundwater table is rather sensitive and it takes less time to reach equilibrium the curve of tf 1 175 is flatter than others which is because an unsaturated sand layer exists near the sand surface and a small water addition could not contribute to the significant increase of groundwater table 5 conclusions a series of dewatering and watering experiments were carried out to investigate the influence of capillary fringe and truncation factors in a homogeneous laboratory sand column detailed spatiotemporal measurements of the pore water pressure and water table response to small amounts of water exchange have been conducted and discussed in dewatering experiments three stages of the pressure head variation were observed i e the gradual decline of pressure head when water table is above the sand surface a rapid sharp drop when water level goes through the sand surface and a decline consistent with the specific yield of the sand when water level is below the sand surface with a detailed vertical resolution of the measured pressure heads temporal variation of the water table was quantitatively obtained showing similar features as the pressure head in watering experiments of shallow aquifer a 0 5 cm water addition can induce disproportionate increment of pressure head which grows fast immediately after water addition then slows down and finally reaches an equilibrium stage different truncation factors tf show a significant influence on the ground water dynamics in the watering experiment as for tf 1 the upper extent of fully saturated capillary fringe coincides with the sand surface and small amounts of water addition produce nearly instantaneous increase in pore water pressure corresponding to a rapid rise of water table to sand surface increment of the pressure head shows a linear relationship with tf values for tf 1 approaching its maximum value when tf is equal to unity considering the vertical distribution of the pressure head its gradient becomes larger right after the water addition and then decreases to its initial value in the end with respect to the hydrostatic condition in the case of tf 1 under which there is an unsaturated layer above the groundwater table the pressure head increases after the water addition whose magnitude however decreases with the increase of tf value especially near the value of unity nevertheless the final position of the groundwater table is still below the sand surface and the gradient of pressure head vertical distribution dose not vary significantly during the entire watering process acknowledgments authors are grateful to prof peter nielsen for inspiration of this study this study was financially supported by the national key r d program of china 2017yfc0405401 and the natural science foundation of china no 11632012 no 51761135015 
762,we use a pore scale dissolution model to simulate the dissolution of calcite by hcl in two different systems and compare with experiment the model couples flow and transport with chemical reactions at the mineral surface and in the fluid bulk firstly we inject hcl through a single channel drilled through a solid calcite core as a simple validation case and as a model system with which to elucidate the chemical mechanisms of the dissolution process the overall dissolution rate is compared to a corresponding experiment close agreement with experimental and simulated dissolution rates is found which also serves to validate the model we also define a new form of effective damkohler number which can be obtained from simulated chemical distributions and show how this gives a more precise measure of the balance of transport and reaction secondly we inject hcl into a ketton carbonate rock core at high flow rate which leads to wormhole formation and compare to experiment the simulation matches the experimental mass dissolution rate extracted from the micro ct images and predicts the resulting morphological changes reasonably well the permeability change though is greater in the experiment than in the simulation and this is shown to be due to more elongated wormhole formation in experiment possible reasons for this are discussed including uncertainties in diffusion coefficients and calcite density variations and micro porosity in the ketton grains the distribution of chemical species from the simulation then permits a detailed understanding of the rate controlling mechanisms at work including the relative importance of the h calcite and h2co3 calcite dissolution pathways 1 introduction the dissolution of carbonate minerals by acidic fluids is a complex process involving the interplay of transport and chemical reaction processes this is an important factor in several large scale operations in carbonate reservoirs such as 1 matrix acidizing mcleod 1984 where the acid is injected directly into the formation to increase permeability and 2 co2 sequestration molins et al 2012 where the acid is formed by dissolution of the injected co2 in the brine already in the reservoir the resulting geometric changes to rock structures at the pore and core scale depends on a number of parameters including the flow rate diffusion rate and local fluid composition at high flow rates and reaction rates this can lead to the formation of high permeability wormholes which drastically enhance the permeability of a porous medium fredd and fogler 1998 predictive simulation of these features is a challenging task since it requires a detailed geochemical model to be coupled to transport phenomena core scale models have provided a way to simulate multicomponent reactive systems in lab scale systems at moderate computational cost wen et al 2016 however at this scale it is not possible to resolve diffusion boundary layers which determine the balance of transport and reaction to truly understand the interplay of these mechanisms pore scale modelling is needed at this scale mineral boundaries and their development during injection can be coupled to the flow and transport properties which avoids the need for effective values of permeability and reaction rate pore scale imaging has developed rapidly and is becoming routine in the oil and gas industry blunt et al 2012 high quality micro ct images of the changes in pore geometry due to reaction can now be obtained under reservoir conditions menke et al 2015 providing both initial geometry and post reaction validation for pore scale models these experimental developments have stimulated a large interest in multi component pore scale reactive flow models kang et al 2014 molins et al 2014 flukiger and bernard 2009 huber et al 2014 hiorth et al 2013 ovaysi and piri 2014 liu and mostaghimi 2018 gao et al 2017 yoon et al 2012 tian and wang 2018 such models have been applied to co2 saturated systems in a few cases in realistic rock geometries flukiger and bernard 2009 computed steady state chemical distributions in a 3d limestone geometry for co2 water ovaysi and piri 2014 simulated the dissolution of a limestone core sample injected with co2 acidified brine including changes in the pore space topography however neither of these results were compared with experiment molins et al 2014 compared overall dissolution rate to experiment in a crushed calcite system using a steady state multicomponent geochemical model for co2 saturated fluids without including changes in pore space topography they found a simulated dissolution rate of around a factor of 1 8 above the experimental result gao et al 2017 recently compared dissolution rates in a calcite cemented core plug in batch reactor conditions finding agreement with experimental results pereira nunes et al 2016 simulated carbonate dissolution at the pore scale using simplifying assumptions about the geochemistry and compared to experimental micro ct images with good agreement the dissolution of calcite with hcl has also been considered soulaine et al 2017 simulated the injection of hcl through 2d calcite microfluidic geometries and varied flow and reaction regimes to evaluate the effect on resulting dissolution profiles including wormholing recent work has also extended modelling efforts to mixed lithography systems chen et al 2014 min et al 2016 and multiphase systems chen et al 2015 and some approaches include semi permeable solid regions which allow chemical transport into grains tian and wang 2018 soulaine et al 2017 in this work we describe a comprehensive multicomponent reactive transport model which takes into account flow convection and diffusion through complex pore geometries as well as chemical reactions in the fluid and at mineral surfaces the geometry is able to change with time and this is coupled back to the flow solver which updates the flow field as the pore structure dissolves additionally we include the effects of ion charge coupling in our diffusion model we use this model to look in detail at the chemical mechanisms at work in both a simple geometry and in more complex carbonate geometry during wormholing the model is applied to the dissolution of calcite with hcl in two experiments firstly we simulate the dissolution of a solid calcite core with a 1 mm hole drilled through it the advantage of this system is that the calcite does not contain any micro pores which can contribute to flow in more complex carbonate rocks the simple geometry also provides a useful way to analyse the chemical and transport processes at work kim and santamarina 2015 then we simulate wormhole formation in ketton carbonate compared directly to a corresponding experiment using micro ct imaging ketton carbonate does contain micropores and we account for this here by adjusting the calcite molar density to a lower value in accordance to mean microporosity for this particular carbonate the macro pore flow structure is of several orders of magnitude higher permeability than the microporous regions we therefore neglect any flow through the grains themselves one further advantage of using hcl instead of co2 saturated solutions is that the dissolution reaction between h and calcite is usually much faster than the transport rate this means that the overall dissolution rate is less sensitive to the details of the surface reaction and should be depleted before it diffuses into microporous regions it also means that the computed surface dissolution rate is less sensitive to the ambiguity of estimating true available surface area from micro ct images as well as any uncertainty in the activity coefficients the h 2 co 3 calcite dissolution pathway which plays more of a role in the co 2 saturated systems has a much lower rate constant than that of the h reaction so it is likely to be more sensitive to these phenomena we also address the difficult problem of comparing simulated wormhole formation in micro ct cores to experiment since wormholing is an unstable process the exact location that dominant paths form can be very sensitive to initial channel sizes this is particularly the case for ketton carbonate which has a very homogeneous pore structure in this work we include some important considerations in both experiment and simulation to do this firstly preserving the full micro ct geometry as an initial condition for the simulation is necessary however this is difficult because steel end pieces pushed up against the core lead to streak artefacts in the scan due to the high density difference between the metal and the calcite in experimental work the field of view of a scan usually does not include the very ends of the core for this reason here we use a peek spacer to enable us to image the full core secondly we perform very careful segmentation to accurately maintain the position of the non reactive sleeve around the core finally in the simulation model during dissolution we use very fine surface depth velocity update conditions which couple the flow field to the geometric changes as closely as the discrete grid allows this contrasts with the more common approach of updating the flow field after a fraction of the mineral mass has been depleted a major aim of this work is to see how well the simulation model can predict the experimental dissolution pattern and rate in the ketton carbonate core given the difficulties described above using the corresponding experimental parameters flow rate temperature etc as far as we are aware this is the first time such an exact morphological comparison has been attempted at the pore scale in a full 3d micro ct core under wormholing conditions 1 1 diffusion of charged species the diffusive fluxes of neutral species in dilute solutions can generally be described by fick s law the diffusion of charged species in electrolytes on the other hand is governed by the classical equations of nernst planck and arrhenius vinograd and mcbain 1941 and can be specified by multi component diffusion models which take into account the effects of charge coupling on ion diffusion rates steefel and maher 2009 such models have been used to explain multi component diffusion effects in bulk solutions maher et al 2006 and porous media appelo and wersin 2007 only a few papers have taken into account this effect in reactive flow models in most work a single diffusion coefficient is used for all species liu and mostaghimi 2018 gao et al 2017 molins et al 2014 yoon et al 2012 which maintains electroneutrality but does not capture electrochemical migration effects in a few cases though multicomponent diffusion effects have been included alongside chemical reactions muniruzzaman and rolle 2016 used a multicomponent ionic transport model coupled to geochemical reaction network to investigate dispersion in 2d systems at the core scale notably in pore scale simulation ovaysi and piri 2014 included a multicomponent diffusion term in their particle based reaction model since the chemical composition of a reactive fluid can change quite significantly along the length of a reacting system for example in calcite dissolution where considerable amounts of high charged calcium ca2 ions are released as products the use of a constant diffusion coefficient is a considerable uncertainty in this work we also include the effects of multicomponent ion diffusion using a finite volume method in the transport scheme used here a flux limiter model is used in this approach the concentration flux is approximated by a weighted combination of high and low order approximations the weighting depends on the local curvature of the solution so that an optimal overall approximation balancing accuracy high order and stability low order is attained however near reacting boundaries where products and reactants have opposite diffusion gradients different species can have locally different solution weightings this gives rise to artificial charge splitting which can cause the model to become unstable at higher grid peclet numbers we develop a correction term here to ensure that stability is maintained in all cases 1 2 damkohler number we consider the damkohler number da as a measure for the ratio of the chemical reaction timescale to the transport phenomena timescale in both experiment and simulation 1 da τ trans τ react in the literature either the convective or diffusive form of the damkohler number da is used depending on which transport mechanism is considered to dominate tansey and balhoff 2016 for example used the diffusive damkohler number to parameterise network models of dissolution in porous media tang et al 2015 also used the diffusive damkohler number to investigate pore scale and continuum scale model for solute diffusion reaction and biofilm development here we will also use a more precise effective damkohler number which can be obtained directly from the reactant distribution available from pore scale simulation specific forms of da can be obtained by identifying the time scales in terms of the relevant flow and chemical properties of the system for example for convective transport τtrans l v where v is the average velocity through the system and l is a characteristic length scale also for a first order surface reaction τreact l k where k is the reaction rate constant in units of m s this leads to the definition 2 d a conv k v likewise for a diffusion process τtrans l 2 d d being the diffusion coefficient in m2 s da is then given by 3 d a diff k l d often the damkohler number is defined as the convective form da daconv and the diffusive damkohler number is called the peclet damkohler number since dadiff peda alternatively daconv and dadiff are sometimes referred to as dai and daii respectively however it can be ambiguous as to which form of the damkohler number is most appropriate to use in a particular system this is mainly because these measures generally do not take into account diffusion boundary layers formed near to reactive surfaces at the pore scale in the case of daconv this usually results in an artificially low value the diffusive form dadiff provides a way to specify a length scale over which diffusion occurs and a characteristic pore size is generally used for porous media ideally this length scale should be specified as the size of a diffusion boundary layer however this is often difficult to approximate since it depends on the complex interplay of convection diffusion and reaction some attempts have been made to relate length scale to local surface area to make this more precise soulaine et al 2017 however these measures are still very approximate since pore scale simulations provide access to the full spatial distribution of chemical species inside the domain it is possible to specify a more precise damkohler number using the reactant distribution gray 2017 since the mineral surface concentration gives information about the balance of transport and reaction in the system we can develop an effective damkohler number based on this measure we consider a 1 dimensional diffusion process in which an inlet concentration cin diffuses down to a reactive surface over a distance l with a diffusion coefficient d fig 1 assuming that the system is at steady state so that the diffusion and reaction processes are at equilibrium then this leads to the expression 4 k c s d c i n c s l where cs is the concentration of reactant at the mineral surface and k is the reaction rate constant rearranging this expression allows us to specify a diffusive damkohler number in terms of the surface and inlet concentrations 5 d a diff k l d c i n c s 1 this damkohler number can be specified entirely from the inlet and surface concentrations without any specific reference to transport and reaction mechanisms therefore the expression can be directly generalised to represent any transport process and indeed any first order reaction process we therefore define an effective damkohler number daeff in terms of the averaged surface concentration cs for a system gray 2017 6 d a eff c i n c s 1 now the transport and reaction time scales in 1 are no longer expressed analytically but are obtained implicitly from simulation note that as expected daeff 0 as cs cin and the reaction is strongly surface rate controlled and daeff as cs 0 so that the reaction is strongly transport controlled this form of damkohler number is specific to pore scale simulations which provide reactant concentration everywhere in the domain cs can therefore be determined by averaging this quantity over the mineral surfaces and sampled both spatially and over time 2 experiment 2 1 dissolution in a calcite channel the first system we considered consists of a 1mm diameter cylindrical channel drilled through a solid calcite cylinder 1 cm in length through which hcl was flowed the experimental system was set up as follows the cylindrical calcite was inserted into a viton sleeve with both ends of the mineral connected to stainless steel end caps as shown in fig 2 this was subsequently placed in a core holder hassler cell and a confining pressure applied to the outside of the sleeve to prevent the reactive fluid bypassing the sample the core holder was wrapped around in heating tapes insulated and fitted with a 1 8 inch type j thermocouple for accurate temperature measurements and control the flow rate during the experiment was controlled by the injection pump while a back pressure of 5 mpa was maintained at the outlet from the cell to ensure that any co2 produced during the reaction remained in solution the system was scanned using micro ct imaging zeiss xradia 500 versa to provide an initial condition for the simulation model and again at intervals over the 14 h injection time to enable overall calcite dissolution rate to be obtained the full parameterisation of the experiment is given in table 1 values of peclet and diffusive damkohler numbers are approximate and based on the diffusion rate of the h cl pair in isolation as the true diffusion rate of the reactant will depend on the local ionic composition the value of k the reaction rate constant used in the definitions of damkohler number was k 1 2 5 10 4 ms 1 the mineral concentration value here is the molar density of solid calcite 2 2 dissolution in ketton carbonate a ketton carbonate core of size 5 mm in diameter and 11 mm in length was scanned at two overlapping positions with a voxel size of 7 97 µm in order to try to match inlet conditions as precisely as possible in the simulation a peek ring was placed between the steel inlet head and the calcite core this minimises the appearance of streak artefacts which otherwise arise from regions of large density difference and make segmentation of the rock pore space problematic this approach meant that the full core image could be used the image was then carefully segmented using 2d histogram segmentation in some regions on a slice by slice basis to accurately match the position of the confining sleeve around the outside of the core shah et al 2015 the resulting initial geometry is shown in fig 9 hcl acid was injected over a period of 5 h before being flushed with deionised water to stop the dissolution and scanned again to obtained the resulting geometry we chose to limit the extent of the reaction to ensure that grains did not move due to the structure becoming unstable this was achieved partly by reducing the concentration of the acid used to 0 005 moldm 3 compared to 0 05 moldm 3 for the solid calcite channel the full parameterisation for the dissolution is given in table 2 the after scan was then segmented in the same way to enable the mass dissolved to be quantified permeability was obtained by performing a single phase flow calculation on the image rather than experimentally the value of calcite mineral concentration used in the simulation and in the calculations of dissolved mass in post reaction micro ct images has been reduced by 10 compared to the solid calcite cylinder case to take into account microporosity within the grains shah et al 2014 3 simulation model corresponding simulations were performed using a reactive transport model comprising the following main components 1 single phase flow computed using the lattice boltzmann lb method 2 chemical transport using a finite volume method and 3 chemical reactions at mineral surfaces and equilibrium reactions in the fluid bulk surfaces dissolve over time and the flow field is recomputed at regular intervals the model has been implemented for gpu cluster using 3 dimensional domain decomposition and all calculations were run using 8 tesla p100 gpus calculations for the ketton carbonate required around 2 days to reach the experimental time scale which included 59 lattice boltzmann flow field calculations as the geometry evolved 3 1 flow model the lattice boltzmann method solves the incompressible navier stokes equations through a discrete form of the boltzmann equation the fluid state is represented by a discrete particle velocity distribution function fi x t which expresses the number of particles at position x and time t with a velocity e i the discrete velocity set e i is that of the d3q19 3 dimensional 19 velocity set scheme and is defined 7 e 0 1 1 0 0 0 0 1 1 1 1 0 0 0 0 1 1 1 1 0 0 0 1 1 0 0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 1 1 1 1 1 1 t the hydrodynamic moments density ρ and velocity u are then obtained from the following expressions 8 ρ i f i ρ u i e i f i ρ g 2 where g is a fluid body force vector the fluid equations are solved by evolving the distribution function through streaming and collision operations in the multiple relaxation time model used here the collision operator takes the following form 9 f x t f x t m 1 sm f e q f i 1 2 s mf the matrix m transforms the distribution function into a new set of moments each of which can be relaxed to an equilibrium state feq individually through the diagonal matrix of relaxation rates s which are related to the fluid viscosity d humières and ginzburg 2009 the equilibrium function feq is defined 10 f i e q ρ u ρ w i 1 3 e i u 9 2 e i u 2 3 2 u 2 with lattice weights w 0 1 3 w 1 7 1 18 w 8 18 1 36 the final term in 9 adds momentum contributions from the forcing function f kuzmin et al 2011 the post collisional distribution is then streamed to neighbouring nodes according to 11 f i x e i d t t d t f i x t where the time step dt 1 in our implementation the distribution function is transformed by negating the zero velocity equilibrium function f i e q 1 0 first to maintain greater computational accuracy which permits accurate velocity field calculations in complex porous media using float precision gray and boek 2016 3 2 transport model chemical transport is performed using a finite volume method velocities at cell faces are interpolated from the lb vector field u x and concentration fluxes are convected through the cell boundaries fluxes are approximated using two different approximations a second order central difference method and a first order upstream approximation in 1 dimension the high order flux at the positive face f i 1 2 h of a cell indexed by i is given by 12 f i 1 2 h u i 1 2 c i c i 1 2 where ci is the concentration of a single chemical component in the cell and u i 1 2 is the interpolated velocity component at the positive face the low order approximation f i 1 2 l is given by 13 f i 1 2 l u i 1 2 c i u i 1 2 c i 1 if u i 1 2 0 if u i 1 2 0 the final flux through the cell face is then obtained by the combination of a low order approximation with a high order correction whose magnitude is controlled by the flux limiter function ψ ri 14 f i 1 2 f i 1 2 l ψ r i f i 1 2 h f i 1 2 l the curvature function r i c i c i 1 c i 1 c i measures the local smoothness of the solution and when ψ ri is specified allows the flux to be approximated to high order in smooth regions and low order in non smooth regions in this case we used the superbee limiter defined by ψ ri max 0 min 2ri 1 min ri 2 this arrangement gives an optimal combination of stability and accuracy in highly convective flows and counteracts numerical diffusion as we have reported in previous work gray et al 2016 to account for the diffusion of charged species an electrochemical migration term must be included in addition to fick s law in the absence of large gradients in activity coefficients this flux is given for each species j by steefel and maher 2009 15 j j d j c j x z j d j c j k z k 2 d k c k k z k d k c k x the indices in 15 refer to the chemical species rather than the cell index the diffusion coefficients di for each species considered here are listed in table 3 obtained experimentally at 298 k and adjusted to 323 k using the stokes einstein relation apart from that of h 2 co 3 which was obtained at experimental conditions of 323 k and 142 bar cadogan et al 2014 this is above the pressure of 50 bar in our experiment but the variation of di from 142 to 318 bar in ref cadogan et al 2014 is only 2 so we will assume it is also quite close to the value at 50 bar one extra consideration needs to be given to the convection model in order to stably compute systems with charge coupled diffusion at high peclet numbers since flux limiter models dynamically vary the concentration flux approximation depending on local solution curvature this can give rise to artificial charge migration if different chemical species have different values of ri at lower flow rates this small charge imbalance is implicitly counteracted by the diffusion term 15 and the model remains stable however at higher grid peclet numbers such as in high velocity regions of a porous medium namely pore throats the model becomes unstable this problem is solved here by introducing a correctional drift velocity δuj for each component in response to any net electrical current produced by imbalanced charge convection dropping the cell index in 12 and 13 so that an initial face velocity is denoted u a corrected face velocity for each species j is given by uj u δuj the expression for δuj is derived in appendix 1 and is given by 16 δ u j z j k z k 2 c k k z k f k where fk is the flux through a cell face for a species k using the original face velocity u defined in 14 the subscript i 1 2 indicating the cell index has again been dropped 3 3 surface reaction model the calcite mineral dissolves according the following pathways 17 c a c o 3 h c a 2 h c o 3 18 c a c o 3 h 2 c o 3 c a 2 2 h c o 3 at far from equilibrium conditions the dissolution rate r of calcite is given by 19 r k 1 α h k 2 α h 2 c o 3 where the reaction rate constants are determined experimentally as k 1 2 5 10 4 ms 1 and k 2 5 5 10 7 ms 1 at t 323 k for the respective reactions peng et al 2015 the saturation behaviour of the calcite dissolution reactions is highly non linear and is described here using a term of the following form 20 r a α h k 1 1 q 1 k 1 p a α h 2 c o 3 k 2 1 q 2 k 2 q here q 1 and q 2 are the solubility products defined 21 q 1 α c a 2 α h c o 3 α h and q 2 α c a 2 α h c o 3 2 α h 2 c o 3 the non linear exponents are obtained by fitting to experiment and are set to p q 7 76 anabaraonye 2017 the equilibrium constants are given by k 1 27 07 and k 2 1 44 10 5 in the numerical model the kinetic dissolution reactions 17 and 18 are computed using a 4th order runge kutta rk4 forward integration time step at each iteration on solid nodes at the fluid boundary the surface area a is obtained on a solid node from the discrete computational grid using a local parabolic interpolation scheme gray et al 2016 this counteracts the staircase effect which otherwise leads to surface area being overestimated on cartesian grids solid mass is depleted from surface boundary nodes according to the dissolution rate 20 and mineral concentration both lb and transport models consider nodes fully solid until all their mass has been depleted this approach is generally acceptable for well resolved images though it limits the resolution to the grid size gray et al 2016 the interval between lb flow updates however is a more important consideration particularly for highly localised dissolution such as wormholing in our model the flow field is recomputed whenever any part of the surface has dissolved more than half a voxel this criterion ensures that the flow is always strongly coupled to geometric changes even when the dissolution is very localised in dissolution at high pe and da the areas of highest dissolution tend to be the pore throats the size of these features strongly influences the permeability and therefore the outcome of unstable wormhole formation but they represent only a small fraction of the surface by using surface depth rather than amount dissolved as a flow update condition we ensure that close competition between flow paths is resolved as finely as possible on the discrete grid 3 4 fluid reaction model the following reaction network is solved in the fluid 22 h 2 c o 3 h h c o 3 23 h c o 3 h c o 3 2 we assume that these reactions will be much faster than the mean transport time scale since the half life for reaction 22 for example is of the order 10 6 s langmuir 1997 this means that they can be solved directly to equilibrium at each node at each time step rather than explicitly integrated forward in time the mass action equations at equilibrium are given by 24 k 3 α h 2 c o 3 α h α h c o 3 0 25 k 4 α h c o 3 α h c o 3 α h 0 where α is the activity of a species and the equilibrium constants at t 323 k are k 3 5 303 10 7 moldm 3 and k 4 6 526 10 11 moldm 3 in addition to two mass conservation constraints eqs 24 and 25 are solved using a newton raphson procedure 3 5 chemical activity model chemical activities are computed using the mixed solvent electrolyte mse model from oli systems wang et al 2002 the activity coefficients for each species are obtained from an excess gibbs free energy computed from three interaction terms a long range electrostatic interaction term lr mid range ionic interactions mr and a short range intermolecular contribution sr parameters relevant to the hcl system may be found in refs springer et al 2012 2015 wang et al 2013 4 results and discussion 4 1 calcite channel the channel injected with hcl is shown in fig 3 with both the initial scanned morphology and the resulting geometry simulated after 14 h of physical time the dissolved mass of calcite over the course of the experiment estimated from the ct scans is also plotted alongside the simulated results in fig 4 the experimental result is offset from the simulation since some initial time is taken clearing dead volumes from the tubing however since the dissolution rate is fairly constant this can be compared in preference to the absolute mass dissolved by this measure the simulation using the charge coupled diffusion model 15 agrees very closely with the experiment also shown is a simulation using a constant diffusion coefficient for all species of d 5 6 10 9 m2s 1 this is the diffusion rate of the h cl ion pair in the absence of other species in this respect both charge coupled and constant diffusion models are equivalent in the non reactive inlet region where only h and cl are present however as products from the reactive surface are introduced into solution along the cylinder the h and cl can diffuse at different rates while maintaining overall charge neutrality using constant diffusion coefficients leads to a dissolution rate around 20 lower than the charge coupled diffusion case it is also interesting that the dissolution rate remains virtually constant in both cases even though the channel geometry is changing this suggests that over time the increasing length scale and thus decreasing diffusion rate is balanced by the decreasing mean flow velocity which increases reactant residence time the rate controlling mechanisms of the reaction process can be understood in terms of the concentration fields of the main chemical components profiles along the centre of the channel are shown for the four components h h 2 co 3 ca 2 and h c o 3 in fig 5 firstly we note that the distribution of h is marked by a large boundary layer over which the concentration drops from the inlet concentration 0 05 moldm 3 down to a much lower value at the mineral surface this is usually the hallmark of a transport controlled reaction because the surface concentration which ultimately controls the dissolution rate through eq 20 depends on the diffusion rate of the reactant through this layer the thickness of the boundary layer in turn depends on the balance of convection and diffusion however in this case the boundary layer is not solely determined by the balance of transport and surface reaction rates this can be understood from the concentration of h 2 co 3 in fig 5 it is clear that a considerable amount of this hydrated co2 is produced comparable to concentration of the h at the inlet this species forms through the reverse reaction of 22 meaning that the h c o 3 given off from the mineral surface during the dissolution reaction 17 is reacting with much of the injected h and acting as one of the main inhibiting mechanisms on the overall dissolution rate this is also clear from inspecting the concentration profile of the bicarbonate species h c o 3 in fig 5 if bicarbonate is produced at the same rate as ca 2 fig 5 then the distributions should be similar however it is clear there is orders of magnitude less h c o 3 present in the system these observations are quantified in fig 6 which shows the concentrations of these four species on the mineral surface sampled along the length of the channel at time t 7 h the concentration of h is around 0 001 moldm 3 on the mineral surface more than an order of magnitude lower than at the inlet due to the inhibitive processes described the concentrations of h c o 3 h and h 2 co 3 are consistent with the weak equilibrium k 3 1 to better quantify the degree to which the system is transport or surface reaction rate controlled the damkohler number can be used with da 1 indicating surface rate controlled and da 1 indicating transport controlled without detailed information about the reactant concentration fields the approximate definitions daconv 0 5 and dadiff 45 table 1 need to be used however these clearly give contradictory results daconv suggesting surface rate control and dadiff suggesting transport control this is because they are based on simplistic assumptions about the nature of the transport processes in the system with daconv ignoring diffusion boundary layers in the system and dadiff being based on the assumption of a diffusion boundary layer equal to the length scale of the system in this case 1 mm table 1 in reality the size of the diffusion boundary layer depends on the peclet number as well as the form of the geometry and changes along the system due to the reaction process since the peclet number indicates that the transport is strongly convection dominated the use of the convective form of the damkohler number suggesting surface rate control would seem to be indicated however the results of the pore scale simulation show that the surface concentration of h is an order of magnitude lower than at the inlet fig 5 which implies transport control the effective damkohler number as defined in 6 is sampled along the channel in fig 7 aside from the small region appearing just after inlet daeff varies from around 30 to 90 along the channel length at t 7 h indicating as expected that the reaction is strongly transport controlled this measure implicitly takes account of both convection and diffusion transport processes since these are solved by the simulation the value of daeff is more consistent with dadiff than daconv this can be understood by inspecting the h concentration field fig 5 a large diffusion boundary layer between the bulk concentration and reactive surface can be seen and the transport time scale over this is the main controlling mechanism of the dissolution rate dadiff was determined using a length scale characteristic of the geometry namely here the channel initial diameter it may be more accurate to use the length scale of the diffusion boundary layer but this is a complex function of position and time which is in general only determinable from the concentration fields from the pore scale simulation daeff on the other hand is an overall measure which takes into account all transport and reaction processes including multicomponent diffusion in the simulation additionally it can be used a time dependent measure as the evolving geometry also affects the size of diffusion boundary layers also included in fig 7 is the value of daeff measured over the length of the channel at early time initial chemical steady state where the channel has not yet dissolved in this case daeff varies from around 15 to 70 along the channel length which shows that the dissolution becomes more transport controlled over time one further possible control on the dissolution rate can be considered which is the saturation behaviour of the dissolution pathway 17 the saturation index q 1 k 1 at the mineral surface is plotted over the length of the channel in fig 8 the system remains under saturated by 6 to 7 orders of magnitude which is not surprising in this very short system as a further point of interest the saturation index is also plotted using activity coefficients γ i 1 for all species as would be the case if an activity model were not used this was evaluated from the same dataset under the assumption that since the system is strongly transport controlled and far from equilibrium the overall dissolution rate has minimal dependence on the chemical activities this was also verified with a separate simulation the value of the saturation index in this case is almost 3x higher than if activities are used the high sensitivity of the saturation index on the activity coefficients suggests that in systems which are close to saturation the use of an accurate activity model will be important for accurately determining reaction rates 4 2 ketton carbonate core the initial geometry of the ketton carbonate core and flow field within it computed using the lb method are shown in fig 9 the distributions of dissolved calcite in experiment and simulation are shown in fig 10 after 5 h of dissolution these were obtained by subtracting the post dissolution image from the initial geometry in the case of the experimental data greyscale images were used in the subtraction followed by a segmentation of the difference image the simulation displays a good qualitative correspondence to the experimental mass dissolution profile in both cases several high permeability paths form one of which dominates as is typical during the onset of wormhole formation the overall calcite mass change can also be compared directly between simulation and experiment this is given in fig 11 in terms of the porosity over time and mean values of dissolution rate and calcite mass change over the course of the injection are also given in table 4 the overall dissolution rate corresponds closely between simulation and experiment the simulated dissolution rate is also virtually constant over the experimental time scale which is likely to be due to the limited extent of the reaction to better highlight the distribution of high permeability paths the reactant distribution can be used a steady state geochemical calculation was performed in the segmented post dissolution micro ct image using the same parameters as in the simulation tables 2 and 3 including reactions on the mineral surfaces however without the geometry changing the distribution of h is shown in both simulated and experimental geometries after 5 h of dissolution in fig 12 interestingly the simulated dominant flow paths form in different channels to the experiment the simulated paths generally begin from around the side of the sample where the confining sleeve doesn t quite meet the rounded end of the core in the experiment the larger paths form from the face of the core it is worth noting that the homogeneity of ketton carbonate means that there are several initial flow paths of similar size and permeability this means that it is difficult to predict precisely where a wormhole might form and the detailed results of a simulation can be very sensitive to the initial segmentation as well as to the velocity update conditions however this did not prevent good prediction of the overall extent of reaction the dissolution profiles can be compared on the basis of resulting petrophysical properties the permeability over time in simulation and experiment is shown in fig 13 these values were computed in the full domain geometry including the extra inlet region the value for the experimental image after 5 h was obtained by adding the same inlet region as in the simulation and computing the permeability using the lb model as experimental pressure measurements during the reactive flow were not available each data point for the simulation represents a velocity field update interestingly the permeability in the experiment is considerably higher after dissolution than in the simulation this suggests differences in the morphology of the wormholes this can be examined by plotting the calcite volume dissolved as a function of position along the core shown in fig 14 a and the volume change in fig 14b as is clear from fig 14a the calcite mass is dissolved mainly in the first half of the core in both simulation and experiment comparing the calcite volume fraction changes in fig 14b reveals some differences in the distributions we note that the simulation appears to dissolve more at the core face than the experiment but less than the experiment beyond 3 mm along the core this suggests that the dominant flow paths in the experiment are more elongated than in the simulation the velocity fields after dissolution are shown in fig 15 for both simulation and experiment it is clear that this is the case the experimental paths widening significantly further along the core than in the simulation there are several possible reasons for this difference firstly as noted above the confining sleeve does not press firmly into side of the core at the inlet in the segmented image fig 9 given the high confining pressure used in the experiment it is likely that this is due to uncertainty in the segmentation of the core sleeve boundary here this is also supported by the fact that the experimental wormhole forms more towards the centre of the sample compared to the simulation in which it forms from around the outside if there is extra space around the side of the core then it would reduce the average flow velocity leading effectively to a lower peclet number in this region which may partly explain the difference in dissolved mass distributions similarly this difference could be indicative of a lower diffusion rate in the experiment reactants convect further along a flow path before diffusing to a reactive surface and being depleted uncertainty in the temperature of the experiment might account for this difference as diffusion coefficients depend quite strongly on temperature as the values in table 3 show this also raises concerns that numerical diffusion effects might be artificially biasing the simulation we show in appendix 2 two further calculations performed with lower diffusion coefficients which demonstrate this is not the case here secondly we assumed that the micro porosity was distributed uniformly throughout the ketton grains and adjusted our value of the calcite density accordingly this is actually not the case as can be seen in high resolution confocal scans shah et al 2014 around the outside of many grains there are layers where the micro porosity is considerably higher this could have a significant effect on the geometric changes of the grains as boundaries recede faster in regions of lower mineral density this would then enhance the permeability of major flow paths at early times leading to the more elongated dominant paths which were observed in experiment thirdly dissolved co2 in the form of aqueous h 2 co 3 is known to diffuse into the micro pores of ketton grains where it has a long residence time for reaction and dissolve inside singh et al 2018 this effect would also be focussed along the major flow paths where more of this species is produced in the equilibrium reaction 22 to expand on the chemical mechanisms behind the dissolution process we consider the distributions of the various chemical species in the pore space obtained from the simulation shown in fig 16 are the distributions of h 2 co 3 and ca 2 and h c o 3 ions once again the large quantities of h 2 co 3 imply that some of the h is reacting on the calcite surface producing h c o 3 which then reacts with more h to produce h 2 co 3 the difference between this case and the solid calcite cylinder is that there is far more surface area for the h 2 co 3 to react with as it disperses as a useful indicator of the balance of the reaction processes we can also compute the dissolution rate as a fraction of the molar injection rate of hcl i e h these values are included in table 4 the simulation depletes 0 66 mol of calcite for every mole of h if we assume that 0 5 mol are accounted for by the proton reaction pathway 17 with the other 0 5 mol of h reacting with the product h c o 3 forming h 2 co 3 then we can ascribe the remainder of the dissolution to the h 2 co 3 pathway 18 therefore 32 of the h 2 co 3 produced by the reaction 17 reacts elsewhere in the core this can be seen in fig 16 as the h 2 co 3 concentration decreasing towards the outlet the contribution of h2co3 is therefore important even in dissolution with hcl in complex geometries despite the surface reaction rate constant being 3 orders of magnitude lower mineral surface concentrations are sampled along the core in fig 17 the mean surface concentration of h is once again well below the inlet concentration showing that there are strong transport or chemical depletion mechanisms namely reaction 22 limiting the overall reaction rate the h 2 co 3 surface concentration decreases much more gradually over the length of the core as it is used up to a lesser degree we note that in the second half of the core only 6 mm from the inlet the surface concentration of h 2 co 3 is at least 3 orders of magnitude greater than that of h so its contribution to the overall reaction rate dominates despite its low reaction rate constant since the secondary reaction pathway 18 produces 2 h c o 3 ions for every h 2 co 3 this leads to a build up of h c o 3 along the core which was not present in the solid cylinder example where 18 was much less influential this has significant implications for predicting the outcome of injecting hcl into a carbonate formation where the depth of penetration is much greater than the length of the core used in the experiments here and the influence of the h 2 co 3 reaction will be correspondingly greater in the case of co2 injection into carbonate formations the concentration of h 2 co 3 in co2 saturated water is high before any reaction with the mineral has taken place and the contribution of the second reaction pathway 18 to initial reaction rate is comparable to the first 17 at reservoir conditions peng et al 2015 as reaction proceeds and the surface concentration of the h depletes as shown here the h 2 co 3 reaction pathway will dominate dissolution from very early stages the saturation behaviour of the primary reaction 17 at mineral surfaces is plotted in fig 18 over the length of the core the system remains under saturated to the end of the core however it is interesting to note that the non linear saturation term causes the effective reaction rate constant k 1 e f f k 1 1 q 1 k 1 p to be reduced to approximately 0 2 k1 by the end of the core despite the saturation index q 1 k 1 being only around 0 18 this is compared to a linear saturation term p 1 estimated from the same dataset for interest finally the effective damkohler number is sampled along the core in terms of the plane averaged surface concentration of h and shown in fig 19 even at the inlet face of the core daeff 10 indicating considerable transport control and by the outlet daeff 104 which indicates that the h has barely any access to these surfaces because of the transport and chemical mechanisms described 5 conclusion we injected two calcite systems with hcl acid and performed corresponding simulations to understand the chemical and transport mechanisms during the dissolution process the first experiment consisted of channel drilled through a solid calcite core and provided a simple flow system with which to compare overall mass dissolution rates against experiment close agreement was found in this case and the simulation was used to determine the relative influence of flow diffusion surface reaction rates and chemical equilibrium changes it was also interesting to note here that using constant diffusion coefficients for all species resulted in a lower simulated dissolution rate in the channel compared to using the charge coupled diffusion model a more detailed study of the effects of coupled diffusion will be reported in future publications but the use of this model here allowed us to avoid the uncertainty in transport rates caused by excluding charge migration effects the comparison here confirms that such an error is quite significant for transport controlled reactions we also introduced the concept of an effective damkohler number in terms of the reactant concentration field as a way to measure the balance of transport and reaction in the system this implicitly takes into account all the transport and reaction processes convection multicomponent diffusion and surface reaction rate as well as the changing geometry all of which are reflected in the reactant concentration at the mineral surface this was shown to be a more precise measurement of damkohler number than the approximate forms dadiff and daconv which in this case gave contradictory indications of the controlling mechanisms we then turned to the case of a ketton carbonate core undergoing wormhole type dissolution at a high flow rate we took great care to preserve the inlet geometry during the micro ct scanning so that the initial conditions were matched in the simulation the simulated dissolution regime corresponded to the experimental regime however the dominant paths formed in different places ketton carbonate is probably one of the most challenging geometries in which to accurately predict wormhole patterning at the pore scale because its homogeneity means that many of the flow paths are of comparable initial permeability we used very fine surface depth change velocity field update conditions in our simulation model to ensure that the flow was strongly coupled to changes in pore throat sizes this could be enhanced in future work by including flow through partially dissolved boundary solid nodes it seems likely though that uncertainty in the segmentation of grain boundaries and the confining sleeve was a more prominent influence than the resolution of the simulation model the differences between simulated and experimental dissolution profiles was also suggestive of the role of micro porosity the formation of dominant paths during dissolution usually depends on relative size and therefore permeability each flow path in the initial geometry larger flow paths tend to widen fastest in the high pe da regime because they transport more reactant however this assumes that surfaces dissolve back uniformly with surface reaction rate for ketton grains which can exhibit considerable variation in micro porosity the recession of a surface will be faster in regions which are locally more micro porous during close wormhole competition where small changes in permeability can decide where a dominant path forms this might tip the balance between two competing paths in principle this can be included in the simulation model by mapping the grey scale of the micro ct image to mineral density in practice though there is often too much noise in the image for this to be reliable both of these investigations allowed us to look in detail at the interplay between transport and chemical reaction mechanisms at work during the dissolution of calcite with hcl of particular note was the ph buffering effect of bicarbonate ions released as products of the surface reactions which acted to slow the overall dissolution rate this effect and its influence on the ph has been noted before in simulation studies in the context of co2 saturated fluids molins et al 2014 huber et al 2014 in the case of hcl these processes worked in tandem with the transport limitations caused by the formation of diffusion boundary layers to reduce the local surface reaction rate for the case of ketton carbonate the h 2 co 3 formed through this reaction diffused throughout the core and contributed to dissolution in stagnant regions contrasting with the highly localised wormhole dissolution processes caused by the h dissolution pathway we hope that this work has highlighted some of the challenges involved with comparing experimental and simulated dissolution profiles particularly at high flow and reaction rates where unstable dissolution patterns result several important areas of parameterisation were considered 1 maintaining the experimental inlet configuration in the initial geometry for the simulation by avoiding micro ct scan artefacts 2 accurately segmenting the micro ct image to correctly represent the size of initial flow paths and 3 using flow field update conditions sensitive to small changes in pore throat sizes additionally the results of the comparison in the ketton core suggest that a more careful incorporation of micro porosity may be needed in the model both in terms of the variation in grain mineral densities and for determining the correct surface area to use in the surface controlled reaction with h 2 co 3 acknowledgements we gratefully acknowledge funding from the qatar carbonates and carbon storage research centre qccsrc provided jointly by qatar petroleum shell and qatar science and technology park we would also like to thank t tambach for his generous assistance while implementing and verifying the chemical activity model and m trusler for useful discussions appendix 1 to derive the convective flux correction term we follow a similar procedure as in steefel and maher 2009 for the diffusion term the convection flux through a cell face for a component j is simply 26 j j c o n v f j here the subscript i referring to the cell index is dropped so that fj refers to the flux of each component through one specific face this flux is given by 14 and will not in general conserve total electrical charge through a cell face since the flux limiter function will use a different order approximation for each species we therefore define a corrective flux contribution which acts in response to an electric potential gradient d ϕ d x this is equated with a corrective flux with velocity δuj 27 j j m i g r z j m j f c j d ϕ d x δ u j c j where mj is the mobility of a species and f is faraday s constant the total current passing through the cell face is 28 i f j z j j j c o n v j j m i g r in expanded form this gives 29 i f j z j f j f 2 d ϕ d x j z j 2 m j c j since we require the total current to be zero we obtain 30 j z j f j f d ϕ d x j z j 2 m j c j letting all species mobilities be equal so that mj m we can write an expression for the electrical potential gradient as 31 d ϕ d x j z j f j f m j z j 2 c j substituting 31 back into 27 leads to an expression for the corrective drift velocity 32 δ u j z j k z k 2 c k k z k f k appendix 2 as a further assurance that numerical diffusion is not influencing the dissolution profile in simulations in ketton carbonate two further calculations were run using diffusion coefficients reduced by a factors of 0 5 and 0 1 compared to the values in table 3 these are shown in terms of the h distributions after 5 h of simulated dissolution in fig 20 the profiles are very distinct and the diffusion boundary layers are clearly much smaller in the case of 0 1di compared to 0 5di this would not be the case if numerical diffusion effects were acting strongly 
762,we use a pore scale dissolution model to simulate the dissolution of calcite by hcl in two different systems and compare with experiment the model couples flow and transport with chemical reactions at the mineral surface and in the fluid bulk firstly we inject hcl through a single channel drilled through a solid calcite core as a simple validation case and as a model system with which to elucidate the chemical mechanisms of the dissolution process the overall dissolution rate is compared to a corresponding experiment close agreement with experimental and simulated dissolution rates is found which also serves to validate the model we also define a new form of effective damkohler number which can be obtained from simulated chemical distributions and show how this gives a more precise measure of the balance of transport and reaction secondly we inject hcl into a ketton carbonate rock core at high flow rate which leads to wormhole formation and compare to experiment the simulation matches the experimental mass dissolution rate extracted from the micro ct images and predicts the resulting morphological changes reasonably well the permeability change though is greater in the experiment than in the simulation and this is shown to be due to more elongated wormhole formation in experiment possible reasons for this are discussed including uncertainties in diffusion coefficients and calcite density variations and micro porosity in the ketton grains the distribution of chemical species from the simulation then permits a detailed understanding of the rate controlling mechanisms at work including the relative importance of the h calcite and h2co3 calcite dissolution pathways 1 introduction the dissolution of carbonate minerals by acidic fluids is a complex process involving the interplay of transport and chemical reaction processes this is an important factor in several large scale operations in carbonate reservoirs such as 1 matrix acidizing mcleod 1984 where the acid is injected directly into the formation to increase permeability and 2 co2 sequestration molins et al 2012 where the acid is formed by dissolution of the injected co2 in the brine already in the reservoir the resulting geometric changes to rock structures at the pore and core scale depends on a number of parameters including the flow rate diffusion rate and local fluid composition at high flow rates and reaction rates this can lead to the formation of high permeability wormholes which drastically enhance the permeability of a porous medium fredd and fogler 1998 predictive simulation of these features is a challenging task since it requires a detailed geochemical model to be coupled to transport phenomena core scale models have provided a way to simulate multicomponent reactive systems in lab scale systems at moderate computational cost wen et al 2016 however at this scale it is not possible to resolve diffusion boundary layers which determine the balance of transport and reaction to truly understand the interplay of these mechanisms pore scale modelling is needed at this scale mineral boundaries and their development during injection can be coupled to the flow and transport properties which avoids the need for effective values of permeability and reaction rate pore scale imaging has developed rapidly and is becoming routine in the oil and gas industry blunt et al 2012 high quality micro ct images of the changes in pore geometry due to reaction can now be obtained under reservoir conditions menke et al 2015 providing both initial geometry and post reaction validation for pore scale models these experimental developments have stimulated a large interest in multi component pore scale reactive flow models kang et al 2014 molins et al 2014 flukiger and bernard 2009 huber et al 2014 hiorth et al 2013 ovaysi and piri 2014 liu and mostaghimi 2018 gao et al 2017 yoon et al 2012 tian and wang 2018 such models have been applied to co2 saturated systems in a few cases in realistic rock geometries flukiger and bernard 2009 computed steady state chemical distributions in a 3d limestone geometry for co2 water ovaysi and piri 2014 simulated the dissolution of a limestone core sample injected with co2 acidified brine including changes in the pore space topography however neither of these results were compared with experiment molins et al 2014 compared overall dissolution rate to experiment in a crushed calcite system using a steady state multicomponent geochemical model for co2 saturated fluids without including changes in pore space topography they found a simulated dissolution rate of around a factor of 1 8 above the experimental result gao et al 2017 recently compared dissolution rates in a calcite cemented core plug in batch reactor conditions finding agreement with experimental results pereira nunes et al 2016 simulated carbonate dissolution at the pore scale using simplifying assumptions about the geochemistry and compared to experimental micro ct images with good agreement the dissolution of calcite with hcl has also been considered soulaine et al 2017 simulated the injection of hcl through 2d calcite microfluidic geometries and varied flow and reaction regimes to evaluate the effect on resulting dissolution profiles including wormholing recent work has also extended modelling efforts to mixed lithography systems chen et al 2014 min et al 2016 and multiphase systems chen et al 2015 and some approaches include semi permeable solid regions which allow chemical transport into grains tian and wang 2018 soulaine et al 2017 in this work we describe a comprehensive multicomponent reactive transport model which takes into account flow convection and diffusion through complex pore geometries as well as chemical reactions in the fluid and at mineral surfaces the geometry is able to change with time and this is coupled back to the flow solver which updates the flow field as the pore structure dissolves additionally we include the effects of ion charge coupling in our diffusion model we use this model to look in detail at the chemical mechanisms at work in both a simple geometry and in more complex carbonate geometry during wormholing the model is applied to the dissolution of calcite with hcl in two experiments firstly we simulate the dissolution of a solid calcite core with a 1 mm hole drilled through it the advantage of this system is that the calcite does not contain any micro pores which can contribute to flow in more complex carbonate rocks the simple geometry also provides a useful way to analyse the chemical and transport processes at work kim and santamarina 2015 then we simulate wormhole formation in ketton carbonate compared directly to a corresponding experiment using micro ct imaging ketton carbonate does contain micropores and we account for this here by adjusting the calcite molar density to a lower value in accordance to mean microporosity for this particular carbonate the macro pore flow structure is of several orders of magnitude higher permeability than the microporous regions we therefore neglect any flow through the grains themselves one further advantage of using hcl instead of co2 saturated solutions is that the dissolution reaction between h and calcite is usually much faster than the transport rate this means that the overall dissolution rate is less sensitive to the details of the surface reaction and should be depleted before it diffuses into microporous regions it also means that the computed surface dissolution rate is less sensitive to the ambiguity of estimating true available surface area from micro ct images as well as any uncertainty in the activity coefficients the h 2 co 3 calcite dissolution pathway which plays more of a role in the co 2 saturated systems has a much lower rate constant than that of the h reaction so it is likely to be more sensitive to these phenomena we also address the difficult problem of comparing simulated wormhole formation in micro ct cores to experiment since wormholing is an unstable process the exact location that dominant paths form can be very sensitive to initial channel sizes this is particularly the case for ketton carbonate which has a very homogeneous pore structure in this work we include some important considerations in both experiment and simulation to do this firstly preserving the full micro ct geometry as an initial condition for the simulation is necessary however this is difficult because steel end pieces pushed up against the core lead to streak artefacts in the scan due to the high density difference between the metal and the calcite in experimental work the field of view of a scan usually does not include the very ends of the core for this reason here we use a peek spacer to enable us to image the full core secondly we perform very careful segmentation to accurately maintain the position of the non reactive sleeve around the core finally in the simulation model during dissolution we use very fine surface depth velocity update conditions which couple the flow field to the geometric changes as closely as the discrete grid allows this contrasts with the more common approach of updating the flow field after a fraction of the mineral mass has been depleted a major aim of this work is to see how well the simulation model can predict the experimental dissolution pattern and rate in the ketton carbonate core given the difficulties described above using the corresponding experimental parameters flow rate temperature etc as far as we are aware this is the first time such an exact morphological comparison has been attempted at the pore scale in a full 3d micro ct core under wormholing conditions 1 1 diffusion of charged species the diffusive fluxes of neutral species in dilute solutions can generally be described by fick s law the diffusion of charged species in electrolytes on the other hand is governed by the classical equations of nernst planck and arrhenius vinograd and mcbain 1941 and can be specified by multi component diffusion models which take into account the effects of charge coupling on ion diffusion rates steefel and maher 2009 such models have been used to explain multi component diffusion effects in bulk solutions maher et al 2006 and porous media appelo and wersin 2007 only a few papers have taken into account this effect in reactive flow models in most work a single diffusion coefficient is used for all species liu and mostaghimi 2018 gao et al 2017 molins et al 2014 yoon et al 2012 which maintains electroneutrality but does not capture electrochemical migration effects in a few cases though multicomponent diffusion effects have been included alongside chemical reactions muniruzzaman and rolle 2016 used a multicomponent ionic transport model coupled to geochemical reaction network to investigate dispersion in 2d systems at the core scale notably in pore scale simulation ovaysi and piri 2014 included a multicomponent diffusion term in their particle based reaction model since the chemical composition of a reactive fluid can change quite significantly along the length of a reacting system for example in calcite dissolution where considerable amounts of high charged calcium ca2 ions are released as products the use of a constant diffusion coefficient is a considerable uncertainty in this work we also include the effects of multicomponent ion diffusion using a finite volume method in the transport scheme used here a flux limiter model is used in this approach the concentration flux is approximated by a weighted combination of high and low order approximations the weighting depends on the local curvature of the solution so that an optimal overall approximation balancing accuracy high order and stability low order is attained however near reacting boundaries where products and reactants have opposite diffusion gradients different species can have locally different solution weightings this gives rise to artificial charge splitting which can cause the model to become unstable at higher grid peclet numbers we develop a correction term here to ensure that stability is maintained in all cases 1 2 damkohler number we consider the damkohler number da as a measure for the ratio of the chemical reaction timescale to the transport phenomena timescale in both experiment and simulation 1 da τ trans τ react in the literature either the convective or diffusive form of the damkohler number da is used depending on which transport mechanism is considered to dominate tansey and balhoff 2016 for example used the diffusive damkohler number to parameterise network models of dissolution in porous media tang et al 2015 also used the diffusive damkohler number to investigate pore scale and continuum scale model for solute diffusion reaction and biofilm development here we will also use a more precise effective damkohler number which can be obtained directly from the reactant distribution available from pore scale simulation specific forms of da can be obtained by identifying the time scales in terms of the relevant flow and chemical properties of the system for example for convective transport τtrans l v where v is the average velocity through the system and l is a characteristic length scale also for a first order surface reaction τreact l k where k is the reaction rate constant in units of m s this leads to the definition 2 d a conv k v likewise for a diffusion process τtrans l 2 d d being the diffusion coefficient in m2 s da is then given by 3 d a diff k l d often the damkohler number is defined as the convective form da daconv and the diffusive damkohler number is called the peclet damkohler number since dadiff peda alternatively daconv and dadiff are sometimes referred to as dai and daii respectively however it can be ambiguous as to which form of the damkohler number is most appropriate to use in a particular system this is mainly because these measures generally do not take into account diffusion boundary layers formed near to reactive surfaces at the pore scale in the case of daconv this usually results in an artificially low value the diffusive form dadiff provides a way to specify a length scale over which diffusion occurs and a characteristic pore size is generally used for porous media ideally this length scale should be specified as the size of a diffusion boundary layer however this is often difficult to approximate since it depends on the complex interplay of convection diffusion and reaction some attempts have been made to relate length scale to local surface area to make this more precise soulaine et al 2017 however these measures are still very approximate since pore scale simulations provide access to the full spatial distribution of chemical species inside the domain it is possible to specify a more precise damkohler number using the reactant distribution gray 2017 since the mineral surface concentration gives information about the balance of transport and reaction in the system we can develop an effective damkohler number based on this measure we consider a 1 dimensional diffusion process in which an inlet concentration cin diffuses down to a reactive surface over a distance l with a diffusion coefficient d fig 1 assuming that the system is at steady state so that the diffusion and reaction processes are at equilibrium then this leads to the expression 4 k c s d c i n c s l where cs is the concentration of reactant at the mineral surface and k is the reaction rate constant rearranging this expression allows us to specify a diffusive damkohler number in terms of the surface and inlet concentrations 5 d a diff k l d c i n c s 1 this damkohler number can be specified entirely from the inlet and surface concentrations without any specific reference to transport and reaction mechanisms therefore the expression can be directly generalised to represent any transport process and indeed any first order reaction process we therefore define an effective damkohler number daeff in terms of the averaged surface concentration cs for a system gray 2017 6 d a eff c i n c s 1 now the transport and reaction time scales in 1 are no longer expressed analytically but are obtained implicitly from simulation note that as expected daeff 0 as cs cin and the reaction is strongly surface rate controlled and daeff as cs 0 so that the reaction is strongly transport controlled this form of damkohler number is specific to pore scale simulations which provide reactant concentration everywhere in the domain cs can therefore be determined by averaging this quantity over the mineral surfaces and sampled both spatially and over time 2 experiment 2 1 dissolution in a calcite channel the first system we considered consists of a 1mm diameter cylindrical channel drilled through a solid calcite cylinder 1 cm in length through which hcl was flowed the experimental system was set up as follows the cylindrical calcite was inserted into a viton sleeve with both ends of the mineral connected to stainless steel end caps as shown in fig 2 this was subsequently placed in a core holder hassler cell and a confining pressure applied to the outside of the sleeve to prevent the reactive fluid bypassing the sample the core holder was wrapped around in heating tapes insulated and fitted with a 1 8 inch type j thermocouple for accurate temperature measurements and control the flow rate during the experiment was controlled by the injection pump while a back pressure of 5 mpa was maintained at the outlet from the cell to ensure that any co2 produced during the reaction remained in solution the system was scanned using micro ct imaging zeiss xradia 500 versa to provide an initial condition for the simulation model and again at intervals over the 14 h injection time to enable overall calcite dissolution rate to be obtained the full parameterisation of the experiment is given in table 1 values of peclet and diffusive damkohler numbers are approximate and based on the diffusion rate of the h cl pair in isolation as the true diffusion rate of the reactant will depend on the local ionic composition the value of k the reaction rate constant used in the definitions of damkohler number was k 1 2 5 10 4 ms 1 the mineral concentration value here is the molar density of solid calcite 2 2 dissolution in ketton carbonate a ketton carbonate core of size 5 mm in diameter and 11 mm in length was scanned at two overlapping positions with a voxel size of 7 97 µm in order to try to match inlet conditions as precisely as possible in the simulation a peek ring was placed between the steel inlet head and the calcite core this minimises the appearance of streak artefacts which otherwise arise from regions of large density difference and make segmentation of the rock pore space problematic this approach meant that the full core image could be used the image was then carefully segmented using 2d histogram segmentation in some regions on a slice by slice basis to accurately match the position of the confining sleeve around the outside of the core shah et al 2015 the resulting initial geometry is shown in fig 9 hcl acid was injected over a period of 5 h before being flushed with deionised water to stop the dissolution and scanned again to obtained the resulting geometry we chose to limit the extent of the reaction to ensure that grains did not move due to the structure becoming unstable this was achieved partly by reducing the concentration of the acid used to 0 005 moldm 3 compared to 0 05 moldm 3 for the solid calcite channel the full parameterisation for the dissolution is given in table 2 the after scan was then segmented in the same way to enable the mass dissolved to be quantified permeability was obtained by performing a single phase flow calculation on the image rather than experimentally the value of calcite mineral concentration used in the simulation and in the calculations of dissolved mass in post reaction micro ct images has been reduced by 10 compared to the solid calcite cylinder case to take into account microporosity within the grains shah et al 2014 3 simulation model corresponding simulations were performed using a reactive transport model comprising the following main components 1 single phase flow computed using the lattice boltzmann lb method 2 chemical transport using a finite volume method and 3 chemical reactions at mineral surfaces and equilibrium reactions in the fluid bulk surfaces dissolve over time and the flow field is recomputed at regular intervals the model has been implemented for gpu cluster using 3 dimensional domain decomposition and all calculations were run using 8 tesla p100 gpus calculations for the ketton carbonate required around 2 days to reach the experimental time scale which included 59 lattice boltzmann flow field calculations as the geometry evolved 3 1 flow model the lattice boltzmann method solves the incompressible navier stokes equations through a discrete form of the boltzmann equation the fluid state is represented by a discrete particle velocity distribution function fi x t which expresses the number of particles at position x and time t with a velocity e i the discrete velocity set e i is that of the d3q19 3 dimensional 19 velocity set scheme and is defined 7 e 0 1 1 0 0 0 0 1 1 1 1 0 0 0 0 1 1 1 1 0 0 0 1 1 0 0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 1 1 1 1 1 1 t the hydrodynamic moments density ρ and velocity u are then obtained from the following expressions 8 ρ i f i ρ u i e i f i ρ g 2 where g is a fluid body force vector the fluid equations are solved by evolving the distribution function through streaming and collision operations in the multiple relaxation time model used here the collision operator takes the following form 9 f x t f x t m 1 sm f e q f i 1 2 s mf the matrix m transforms the distribution function into a new set of moments each of which can be relaxed to an equilibrium state feq individually through the diagonal matrix of relaxation rates s which are related to the fluid viscosity d humières and ginzburg 2009 the equilibrium function feq is defined 10 f i e q ρ u ρ w i 1 3 e i u 9 2 e i u 2 3 2 u 2 with lattice weights w 0 1 3 w 1 7 1 18 w 8 18 1 36 the final term in 9 adds momentum contributions from the forcing function f kuzmin et al 2011 the post collisional distribution is then streamed to neighbouring nodes according to 11 f i x e i d t t d t f i x t where the time step dt 1 in our implementation the distribution function is transformed by negating the zero velocity equilibrium function f i e q 1 0 first to maintain greater computational accuracy which permits accurate velocity field calculations in complex porous media using float precision gray and boek 2016 3 2 transport model chemical transport is performed using a finite volume method velocities at cell faces are interpolated from the lb vector field u x and concentration fluxes are convected through the cell boundaries fluxes are approximated using two different approximations a second order central difference method and a first order upstream approximation in 1 dimension the high order flux at the positive face f i 1 2 h of a cell indexed by i is given by 12 f i 1 2 h u i 1 2 c i c i 1 2 where ci is the concentration of a single chemical component in the cell and u i 1 2 is the interpolated velocity component at the positive face the low order approximation f i 1 2 l is given by 13 f i 1 2 l u i 1 2 c i u i 1 2 c i 1 if u i 1 2 0 if u i 1 2 0 the final flux through the cell face is then obtained by the combination of a low order approximation with a high order correction whose magnitude is controlled by the flux limiter function ψ ri 14 f i 1 2 f i 1 2 l ψ r i f i 1 2 h f i 1 2 l the curvature function r i c i c i 1 c i 1 c i measures the local smoothness of the solution and when ψ ri is specified allows the flux to be approximated to high order in smooth regions and low order in non smooth regions in this case we used the superbee limiter defined by ψ ri max 0 min 2ri 1 min ri 2 this arrangement gives an optimal combination of stability and accuracy in highly convective flows and counteracts numerical diffusion as we have reported in previous work gray et al 2016 to account for the diffusion of charged species an electrochemical migration term must be included in addition to fick s law in the absence of large gradients in activity coefficients this flux is given for each species j by steefel and maher 2009 15 j j d j c j x z j d j c j k z k 2 d k c k k z k d k c k x the indices in 15 refer to the chemical species rather than the cell index the diffusion coefficients di for each species considered here are listed in table 3 obtained experimentally at 298 k and adjusted to 323 k using the stokes einstein relation apart from that of h 2 co 3 which was obtained at experimental conditions of 323 k and 142 bar cadogan et al 2014 this is above the pressure of 50 bar in our experiment but the variation of di from 142 to 318 bar in ref cadogan et al 2014 is only 2 so we will assume it is also quite close to the value at 50 bar one extra consideration needs to be given to the convection model in order to stably compute systems with charge coupled diffusion at high peclet numbers since flux limiter models dynamically vary the concentration flux approximation depending on local solution curvature this can give rise to artificial charge migration if different chemical species have different values of ri at lower flow rates this small charge imbalance is implicitly counteracted by the diffusion term 15 and the model remains stable however at higher grid peclet numbers such as in high velocity regions of a porous medium namely pore throats the model becomes unstable this problem is solved here by introducing a correctional drift velocity δuj for each component in response to any net electrical current produced by imbalanced charge convection dropping the cell index in 12 and 13 so that an initial face velocity is denoted u a corrected face velocity for each species j is given by uj u δuj the expression for δuj is derived in appendix 1 and is given by 16 δ u j z j k z k 2 c k k z k f k where fk is the flux through a cell face for a species k using the original face velocity u defined in 14 the subscript i 1 2 indicating the cell index has again been dropped 3 3 surface reaction model the calcite mineral dissolves according the following pathways 17 c a c o 3 h c a 2 h c o 3 18 c a c o 3 h 2 c o 3 c a 2 2 h c o 3 at far from equilibrium conditions the dissolution rate r of calcite is given by 19 r k 1 α h k 2 α h 2 c o 3 where the reaction rate constants are determined experimentally as k 1 2 5 10 4 ms 1 and k 2 5 5 10 7 ms 1 at t 323 k for the respective reactions peng et al 2015 the saturation behaviour of the calcite dissolution reactions is highly non linear and is described here using a term of the following form 20 r a α h k 1 1 q 1 k 1 p a α h 2 c o 3 k 2 1 q 2 k 2 q here q 1 and q 2 are the solubility products defined 21 q 1 α c a 2 α h c o 3 α h and q 2 α c a 2 α h c o 3 2 α h 2 c o 3 the non linear exponents are obtained by fitting to experiment and are set to p q 7 76 anabaraonye 2017 the equilibrium constants are given by k 1 27 07 and k 2 1 44 10 5 in the numerical model the kinetic dissolution reactions 17 and 18 are computed using a 4th order runge kutta rk4 forward integration time step at each iteration on solid nodes at the fluid boundary the surface area a is obtained on a solid node from the discrete computational grid using a local parabolic interpolation scheme gray et al 2016 this counteracts the staircase effect which otherwise leads to surface area being overestimated on cartesian grids solid mass is depleted from surface boundary nodes according to the dissolution rate 20 and mineral concentration both lb and transport models consider nodes fully solid until all their mass has been depleted this approach is generally acceptable for well resolved images though it limits the resolution to the grid size gray et al 2016 the interval between lb flow updates however is a more important consideration particularly for highly localised dissolution such as wormholing in our model the flow field is recomputed whenever any part of the surface has dissolved more than half a voxel this criterion ensures that the flow is always strongly coupled to geometric changes even when the dissolution is very localised in dissolution at high pe and da the areas of highest dissolution tend to be the pore throats the size of these features strongly influences the permeability and therefore the outcome of unstable wormhole formation but they represent only a small fraction of the surface by using surface depth rather than amount dissolved as a flow update condition we ensure that close competition between flow paths is resolved as finely as possible on the discrete grid 3 4 fluid reaction model the following reaction network is solved in the fluid 22 h 2 c o 3 h h c o 3 23 h c o 3 h c o 3 2 we assume that these reactions will be much faster than the mean transport time scale since the half life for reaction 22 for example is of the order 10 6 s langmuir 1997 this means that they can be solved directly to equilibrium at each node at each time step rather than explicitly integrated forward in time the mass action equations at equilibrium are given by 24 k 3 α h 2 c o 3 α h α h c o 3 0 25 k 4 α h c o 3 α h c o 3 α h 0 where α is the activity of a species and the equilibrium constants at t 323 k are k 3 5 303 10 7 moldm 3 and k 4 6 526 10 11 moldm 3 in addition to two mass conservation constraints eqs 24 and 25 are solved using a newton raphson procedure 3 5 chemical activity model chemical activities are computed using the mixed solvent electrolyte mse model from oli systems wang et al 2002 the activity coefficients for each species are obtained from an excess gibbs free energy computed from three interaction terms a long range electrostatic interaction term lr mid range ionic interactions mr and a short range intermolecular contribution sr parameters relevant to the hcl system may be found in refs springer et al 2012 2015 wang et al 2013 4 results and discussion 4 1 calcite channel the channel injected with hcl is shown in fig 3 with both the initial scanned morphology and the resulting geometry simulated after 14 h of physical time the dissolved mass of calcite over the course of the experiment estimated from the ct scans is also plotted alongside the simulated results in fig 4 the experimental result is offset from the simulation since some initial time is taken clearing dead volumes from the tubing however since the dissolution rate is fairly constant this can be compared in preference to the absolute mass dissolved by this measure the simulation using the charge coupled diffusion model 15 agrees very closely with the experiment also shown is a simulation using a constant diffusion coefficient for all species of d 5 6 10 9 m2s 1 this is the diffusion rate of the h cl ion pair in the absence of other species in this respect both charge coupled and constant diffusion models are equivalent in the non reactive inlet region where only h and cl are present however as products from the reactive surface are introduced into solution along the cylinder the h and cl can diffuse at different rates while maintaining overall charge neutrality using constant diffusion coefficients leads to a dissolution rate around 20 lower than the charge coupled diffusion case it is also interesting that the dissolution rate remains virtually constant in both cases even though the channel geometry is changing this suggests that over time the increasing length scale and thus decreasing diffusion rate is balanced by the decreasing mean flow velocity which increases reactant residence time the rate controlling mechanisms of the reaction process can be understood in terms of the concentration fields of the main chemical components profiles along the centre of the channel are shown for the four components h h 2 co 3 ca 2 and h c o 3 in fig 5 firstly we note that the distribution of h is marked by a large boundary layer over which the concentration drops from the inlet concentration 0 05 moldm 3 down to a much lower value at the mineral surface this is usually the hallmark of a transport controlled reaction because the surface concentration which ultimately controls the dissolution rate through eq 20 depends on the diffusion rate of the reactant through this layer the thickness of the boundary layer in turn depends on the balance of convection and diffusion however in this case the boundary layer is not solely determined by the balance of transport and surface reaction rates this can be understood from the concentration of h 2 co 3 in fig 5 it is clear that a considerable amount of this hydrated co2 is produced comparable to concentration of the h at the inlet this species forms through the reverse reaction of 22 meaning that the h c o 3 given off from the mineral surface during the dissolution reaction 17 is reacting with much of the injected h and acting as one of the main inhibiting mechanisms on the overall dissolution rate this is also clear from inspecting the concentration profile of the bicarbonate species h c o 3 in fig 5 if bicarbonate is produced at the same rate as ca 2 fig 5 then the distributions should be similar however it is clear there is orders of magnitude less h c o 3 present in the system these observations are quantified in fig 6 which shows the concentrations of these four species on the mineral surface sampled along the length of the channel at time t 7 h the concentration of h is around 0 001 moldm 3 on the mineral surface more than an order of magnitude lower than at the inlet due to the inhibitive processes described the concentrations of h c o 3 h and h 2 co 3 are consistent with the weak equilibrium k 3 1 to better quantify the degree to which the system is transport or surface reaction rate controlled the damkohler number can be used with da 1 indicating surface rate controlled and da 1 indicating transport controlled without detailed information about the reactant concentration fields the approximate definitions daconv 0 5 and dadiff 45 table 1 need to be used however these clearly give contradictory results daconv suggesting surface rate control and dadiff suggesting transport control this is because they are based on simplistic assumptions about the nature of the transport processes in the system with daconv ignoring diffusion boundary layers in the system and dadiff being based on the assumption of a diffusion boundary layer equal to the length scale of the system in this case 1 mm table 1 in reality the size of the diffusion boundary layer depends on the peclet number as well as the form of the geometry and changes along the system due to the reaction process since the peclet number indicates that the transport is strongly convection dominated the use of the convective form of the damkohler number suggesting surface rate control would seem to be indicated however the results of the pore scale simulation show that the surface concentration of h is an order of magnitude lower than at the inlet fig 5 which implies transport control the effective damkohler number as defined in 6 is sampled along the channel in fig 7 aside from the small region appearing just after inlet daeff varies from around 30 to 90 along the channel length at t 7 h indicating as expected that the reaction is strongly transport controlled this measure implicitly takes account of both convection and diffusion transport processes since these are solved by the simulation the value of daeff is more consistent with dadiff than daconv this can be understood by inspecting the h concentration field fig 5 a large diffusion boundary layer between the bulk concentration and reactive surface can be seen and the transport time scale over this is the main controlling mechanism of the dissolution rate dadiff was determined using a length scale characteristic of the geometry namely here the channel initial diameter it may be more accurate to use the length scale of the diffusion boundary layer but this is a complex function of position and time which is in general only determinable from the concentration fields from the pore scale simulation daeff on the other hand is an overall measure which takes into account all transport and reaction processes including multicomponent diffusion in the simulation additionally it can be used a time dependent measure as the evolving geometry also affects the size of diffusion boundary layers also included in fig 7 is the value of daeff measured over the length of the channel at early time initial chemical steady state where the channel has not yet dissolved in this case daeff varies from around 15 to 70 along the channel length which shows that the dissolution becomes more transport controlled over time one further possible control on the dissolution rate can be considered which is the saturation behaviour of the dissolution pathway 17 the saturation index q 1 k 1 at the mineral surface is plotted over the length of the channel in fig 8 the system remains under saturated by 6 to 7 orders of magnitude which is not surprising in this very short system as a further point of interest the saturation index is also plotted using activity coefficients γ i 1 for all species as would be the case if an activity model were not used this was evaluated from the same dataset under the assumption that since the system is strongly transport controlled and far from equilibrium the overall dissolution rate has minimal dependence on the chemical activities this was also verified with a separate simulation the value of the saturation index in this case is almost 3x higher than if activities are used the high sensitivity of the saturation index on the activity coefficients suggests that in systems which are close to saturation the use of an accurate activity model will be important for accurately determining reaction rates 4 2 ketton carbonate core the initial geometry of the ketton carbonate core and flow field within it computed using the lb method are shown in fig 9 the distributions of dissolved calcite in experiment and simulation are shown in fig 10 after 5 h of dissolution these were obtained by subtracting the post dissolution image from the initial geometry in the case of the experimental data greyscale images were used in the subtraction followed by a segmentation of the difference image the simulation displays a good qualitative correspondence to the experimental mass dissolution profile in both cases several high permeability paths form one of which dominates as is typical during the onset of wormhole formation the overall calcite mass change can also be compared directly between simulation and experiment this is given in fig 11 in terms of the porosity over time and mean values of dissolution rate and calcite mass change over the course of the injection are also given in table 4 the overall dissolution rate corresponds closely between simulation and experiment the simulated dissolution rate is also virtually constant over the experimental time scale which is likely to be due to the limited extent of the reaction to better highlight the distribution of high permeability paths the reactant distribution can be used a steady state geochemical calculation was performed in the segmented post dissolution micro ct image using the same parameters as in the simulation tables 2 and 3 including reactions on the mineral surfaces however without the geometry changing the distribution of h is shown in both simulated and experimental geometries after 5 h of dissolution in fig 12 interestingly the simulated dominant flow paths form in different channels to the experiment the simulated paths generally begin from around the side of the sample where the confining sleeve doesn t quite meet the rounded end of the core in the experiment the larger paths form from the face of the core it is worth noting that the homogeneity of ketton carbonate means that there are several initial flow paths of similar size and permeability this means that it is difficult to predict precisely where a wormhole might form and the detailed results of a simulation can be very sensitive to the initial segmentation as well as to the velocity update conditions however this did not prevent good prediction of the overall extent of reaction the dissolution profiles can be compared on the basis of resulting petrophysical properties the permeability over time in simulation and experiment is shown in fig 13 these values were computed in the full domain geometry including the extra inlet region the value for the experimental image after 5 h was obtained by adding the same inlet region as in the simulation and computing the permeability using the lb model as experimental pressure measurements during the reactive flow were not available each data point for the simulation represents a velocity field update interestingly the permeability in the experiment is considerably higher after dissolution than in the simulation this suggests differences in the morphology of the wormholes this can be examined by plotting the calcite volume dissolved as a function of position along the core shown in fig 14 a and the volume change in fig 14b as is clear from fig 14a the calcite mass is dissolved mainly in the first half of the core in both simulation and experiment comparing the calcite volume fraction changes in fig 14b reveals some differences in the distributions we note that the simulation appears to dissolve more at the core face than the experiment but less than the experiment beyond 3 mm along the core this suggests that the dominant flow paths in the experiment are more elongated than in the simulation the velocity fields after dissolution are shown in fig 15 for both simulation and experiment it is clear that this is the case the experimental paths widening significantly further along the core than in the simulation there are several possible reasons for this difference firstly as noted above the confining sleeve does not press firmly into side of the core at the inlet in the segmented image fig 9 given the high confining pressure used in the experiment it is likely that this is due to uncertainty in the segmentation of the core sleeve boundary here this is also supported by the fact that the experimental wormhole forms more towards the centre of the sample compared to the simulation in which it forms from around the outside if there is extra space around the side of the core then it would reduce the average flow velocity leading effectively to a lower peclet number in this region which may partly explain the difference in dissolved mass distributions similarly this difference could be indicative of a lower diffusion rate in the experiment reactants convect further along a flow path before diffusing to a reactive surface and being depleted uncertainty in the temperature of the experiment might account for this difference as diffusion coefficients depend quite strongly on temperature as the values in table 3 show this also raises concerns that numerical diffusion effects might be artificially biasing the simulation we show in appendix 2 two further calculations performed with lower diffusion coefficients which demonstrate this is not the case here secondly we assumed that the micro porosity was distributed uniformly throughout the ketton grains and adjusted our value of the calcite density accordingly this is actually not the case as can be seen in high resolution confocal scans shah et al 2014 around the outside of many grains there are layers where the micro porosity is considerably higher this could have a significant effect on the geometric changes of the grains as boundaries recede faster in regions of lower mineral density this would then enhance the permeability of major flow paths at early times leading to the more elongated dominant paths which were observed in experiment thirdly dissolved co2 in the form of aqueous h 2 co 3 is known to diffuse into the micro pores of ketton grains where it has a long residence time for reaction and dissolve inside singh et al 2018 this effect would also be focussed along the major flow paths where more of this species is produced in the equilibrium reaction 22 to expand on the chemical mechanisms behind the dissolution process we consider the distributions of the various chemical species in the pore space obtained from the simulation shown in fig 16 are the distributions of h 2 co 3 and ca 2 and h c o 3 ions once again the large quantities of h 2 co 3 imply that some of the h is reacting on the calcite surface producing h c o 3 which then reacts with more h to produce h 2 co 3 the difference between this case and the solid calcite cylinder is that there is far more surface area for the h 2 co 3 to react with as it disperses as a useful indicator of the balance of the reaction processes we can also compute the dissolution rate as a fraction of the molar injection rate of hcl i e h these values are included in table 4 the simulation depletes 0 66 mol of calcite for every mole of h if we assume that 0 5 mol are accounted for by the proton reaction pathway 17 with the other 0 5 mol of h reacting with the product h c o 3 forming h 2 co 3 then we can ascribe the remainder of the dissolution to the h 2 co 3 pathway 18 therefore 32 of the h 2 co 3 produced by the reaction 17 reacts elsewhere in the core this can be seen in fig 16 as the h 2 co 3 concentration decreasing towards the outlet the contribution of h2co3 is therefore important even in dissolution with hcl in complex geometries despite the surface reaction rate constant being 3 orders of magnitude lower mineral surface concentrations are sampled along the core in fig 17 the mean surface concentration of h is once again well below the inlet concentration showing that there are strong transport or chemical depletion mechanisms namely reaction 22 limiting the overall reaction rate the h 2 co 3 surface concentration decreases much more gradually over the length of the core as it is used up to a lesser degree we note that in the second half of the core only 6 mm from the inlet the surface concentration of h 2 co 3 is at least 3 orders of magnitude greater than that of h so its contribution to the overall reaction rate dominates despite its low reaction rate constant since the secondary reaction pathway 18 produces 2 h c o 3 ions for every h 2 co 3 this leads to a build up of h c o 3 along the core which was not present in the solid cylinder example where 18 was much less influential this has significant implications for predicting the outcome of injecting hcl into a carbonate formation where the depth of penetration is much greater than the length of the core used in the experiments here and the influence of the h 2 co 3 reaction will be correspondingly greater in the case of co2 injection into carbonate formations the concentration of h 2 co 3 in co2 saturated water is high before any reaction with the mineral has taken place and the contribution of the second reaction pathway 18 to initial reaction rate is comparable to the first 17 at reservoir conditions peng et al 2015 as reaction proceeds and the surface concentration of the h depletes as shown here the h 2 co 3 reaction pathway will dominate dissolution from very early stages the saturation behaviour of the primary reaction 17 at mineral surfaces is plotted in fig 18 over the length of the core the system remains under saturated to the end of the core however it is interesting to note that the non linear saturation term causes the effective reaction rate constant k 1 e f f k 1 1 q 1 k 1 p to be reduced to approximately 0 2 k1 by the end of the core despite the saturation index q 1 k 1 being only around 0 18 this is compared to a linear saturation term p 1 estimated from the same dataset for interest finally the effective damkohler number is sampled along the core in terms of the plane averaged surface concentration of h and shown in fig 19 even at the inlet face of the core daeff 10 indicating considerable transport control and by the outlet daeff 104 which indicates that the h has barely any access to these surfaces because of the transport and chemical mechanisms described 5 conclusion we injected two calcite systems with hcl acid and performed corresponding simulations to understand the chemical and transport mechanisms during the dissolution process the first experiment consisted of channel drilled through a solid calcite core and provided a simple flow system with which to compare overall mass dissolution rates against experiment close agreement was found in this case and the simulation was used to determine the relative influence of flow diffusion surface reaction rates and chemical equilibrium changes it was also interesting to note here that using constant diffusion coefficients for all species resulted in a lower simulated dissolution rate in the channel compared to using the charge coupled diffusion model a more detailed study of the effects of coupled diffusion will be reported in future publications but the use of this model here allowed us to avoid the uncertainty in transport rates caused by excluding charge migration effects the comparison here confirms that such an error is quite significant for transport controlled reactions we also introduced the concept of an effective damkohler number in terms of the reactant concentration field as a way to measure the balance of transport and reaction in the system this implicitly takes into account all the transport and reaction processes convection multicomponent diffusion and surface reaction rate as well as the changing geometry all of which are reflected in the reactant concentration at the mineral surface this was shown to be a more precise measurement of damkohler number than the approximate forms dadiff and daconv which in this case gave contradictory indications of the controlling mechanisms we then turned to the case of a ketton carbonate core undergoing wormhole type dissolution at a high flow rate we took great care to preserve the inlet geometry during the micro ct scanning so that the initial conditions were matched in the simulation the simulated dissolution regime corresponded to the experimental regime however the dominant paths formed in different places ketton carbonate is probably one of the most challenging geometries in which to accurately predict wormhole patterning at the pore scale because its homogeneity means that many of the flow paths are of comparable initial permeability we used very fine surface depth change velocity field update conditions in our simulation model to ensure that the flow was strongly coupled to changes in pore throat sizes this could be enhanced in future work by including flow through partially dissolved boundary solid nodes it seems likely though that uncertainty in the segmentation of grain boundaries and the confining sleeve was a more prominent influence than the resolution of the simulation model the differences between simulated and experimental dissolution profiles was also suggestive of the role of micro porosity the formation of dominant paths during dissolution usually depends on relative size and therefore permeability each flow path in the initial geometry larger flow paths tend to widen fastest in the high pe da regime because they transport more reactant however this assumes that surfaces dissolve back uniformly with surface reaction rate for ketton grains which can exhibit considerable variation in micro porosity the recession of a surface will be faster in regions which are locally more micro porous during close wormhole competition where small changes in permeability can decide where a dominant path forms this might tip the balance between two competing paths in principle this can be included in the simulation model by mapping the grey scale of the micro ct image to mineral density in practice though there is often too much noise in the image for this to be reliable both of these investigations allowed us to look in detail at the interplay between transport and chemical reaction mechanisms at work during the dissolution of calcite with hcl of particular note was the ph buffering effect of bicarbonate ions released as products of the surface reactions which acted to slow the overall dissolution rate this effect and its influence on the ph has been noted before in simulation studies in the context of co2 saturated fluids molins et al 2014 huber et al 2014 in the case of hcl these processes worked in tandem with the transport limitations caused by the formation of diffusion boundary layers to reduce the local surface reaction rate for the case of ketton carbonate the h 2 co 3 formed through this reaction diffused throughout the core and contributed to dissolution in stagnant regions contrasting with the highly localised wormhole dissolution processes caused by the h dissolution pathway we hope that this work has highlighted some of the challenges involved with comparing experimental and simulated dissolution profiles particularly at high flow and reaction rates where unstable dissolution patterns result several important areas of parameterisation were considered 1 maintaining the experimental inlet configuration in the initial geometry for the simulation by avoiding micro ct scan artefacts 2 accurately segmenting the micro ct image to correctly represent the size of initial flow paths and 3 using flow field update conditions sensitive to small changes in pore throat sizes additionally the results of the comparison in the ketton core suggest that a more careful incorporation of micro porosity may be needed in the model both in terms of the variation in grain mineral densities and for determining the correct surface area to use in the surface controlled reaction with h 2 co 3 acknowledgements we gratefully acknowledge funding from the qatar carbonates and carbon storage research centre qccsrc provided jointly by qatar petroleum shell and qatar science and technology park we would also like to thank t tambach for his generous assistance while implementing and verifying the chemical activity model and m trusler for useful discussions appendix 1 to derive the convective flux correction term we follow a similar procedure as in steefel and maher 2009 for the diffusion term the convection flux through a cell face for a component j is simply 26 j j c o n v f j here the subscript i referring to the cell index is dropped so that fj refers to the flux of each component through one specific face this flux is given by 14 and will not in general conserve total electrical charge through a cell face since the flux limiter function will use a different order approximation for each species we therefore define a corrective flux contribution which acts in response to an electric potential gradient d ϕ d x this is equated with a corrective flux with velocity δuj 27 j j m i g r z j m j f c j d ϕ d x δ u j c j where mj is the mobility of a species and f is faraday s constant the total current passing through the cell face is 28 i f j z j j j c o n v j j m i g r in expanded form this gives 29 i f j z j f j f 2 d ϕ d x j z j 2 m j c j since we require the total current to be zero we obtain 30 j z j f j f d ϕ d x j z j 2 m j c j letting all species mobilities be equal so that mj m we can write an expression for the electrical potential gradient as 31 d ϕ d x j z j f j f m j z j 2 c j substituting 31 back into 27 leads to an expression for the corrective drift velocity 32 δ u j z j k z k 2 c k k z k f k appendix 2 as a further assurance that numerical diffusion is not influencing the dissolution profile in simulations in ketton carbonate two further calculations were run using diffusion coefficients reduced by a factors of 0 5 and 0 1 compared to the values in table 3 these are shown in terms of the h distributions after 5 h of simulated dissolution in fig 20 the profiles are very distinct and the diffusion boundary layers are clearly much smaller in the case of 0 1di compared to 0 5di this would not be the case if numerical diffusion effects were acting strongly 
763,calibration of heterogeneous subsurface flow models usually leads to ill posed nonlinear inverse problems where too many unknown parameters are estimated from limited response measurements when the underlying parameters form complex non gaussian structured spatial connectivity patterns classical variogram based geostatistical techniques cannot describe the underlying distributions modern pattern based geostatistical methods that incorporate higher order spatial statistics are more suitable for describing such complex spatial patterns moreover when the unknown parameters are discrete e g geologic facies distribution conventional model calibration techniques that are designed for continuous parameters cannot be applied directly in this paper we introduce a novel pattern based model calibration method to reconstruct spatially complex facies distributions from dynamic flow response data to reproduce complex connectivity patterns during model calibration we impose a geologic feasibility constraint that ensures the solution honors the expected higher order spatial statistics for model calibration we adopt a regularized least squares formulation involving i data mismatch ii pattern connectivity and iii feasibility constraint terms using an alternating directions optimization algorithm the regularized objective function is divided into a parameterized model calibration sub problem which is solved via gradient based optimization the resulting parameterized solution is then mapped onto the feasible set using the k nearest neighbors k nn as a supervised machine learning approach to honor the expected spatial statistics the two steps of the model calibration formulation are repeated until the convergence criterion is met several numerical examples are used to evaluate the performance of the developed method keywords subsurface flow model calibration discrete facies pattern based simulation k nearest neighbor 1 introduction subsurface flow model calibration often involves estimating the spatial distribution of important properties of geologic formations from limited available measurements with the goal of improving future model predictions for improved optimization and resource management purposes carrera et al 2005 delhomme and lavenue 2000 gavalas et al 1976 hill and tiedeman 2006 kitanidis and vomvoris 1983 mclaughlin and townley 1996 oliver and chen 2010 oliver et al 2008 zhou et al 2014 reproducing the measured data and preserving the expected characteristics of the geologic formation are two general requirements in these problems problem ill posedness computational complexity and multiple sources of error and uncertainty complicate the solution of model calibration problems carrera et al 2005 franssen et al 2009 hill and tiedeman 2006 mclaughlin and townley 1996 oliver and chen 2010 oliver et al 2008 sambridge and mosegaard 2002 tarantola 2005 zhou et al 2014 in many cases the resulting solutions are non unique as limited data are used to constrain high resolution model parameters a rich body of literature on parameterization and regularization techniques exists that discuss various approaches to deal with problem ill posedness and solution non uniqueness in general these techniques either reduce the dimension of model parameters or require additional attributes from the solution properties in both cases additional information is used to explicitly or implicitly restrict the feasible set of the solution oliver and chen 2010 zhou et al 2014 tikhonov 1963 zhou et al 2014 while general regularization and parameterization methods are helpful for properties that have simple spatial distributions e g gaussian smooth or piecewise smooth fields bhark et al 2011 jafarpour and mclaughlin 2009 jafarpour et al 2010 lee and kitanidis 2013 reconstruction of models that have more complex spatial patterns in their distribution requires specific prior knowledge and more sophisticated formulations arpat and caers 2007 gavalas et al 1976 khaninezhad et al 2012 in particular geostatistical methods have been used to constrain the solution of subsurface inverse problems doherty 2003 kitanidis 1995 kitanidis and vomvoris 1983 lavenue et al 1995 zimmerman et al 1998 while the classical variogram based or two point geostatistical methods chiles and delfiner 2009 are appropriate for simulating gaussian processes they are not sufficient for representing more complex connectivity patterns object based simulation techniques deutsch and wang 1996 koltermann such as marked point processes deutsch and wang 1996 are used for modeling complex facies patterns while geologically intuitive and appealing these methods are quite cumbersome for data conditioning primarily due to the lack of flexibility in morphing existing objects modern geostatistical modeling techniques that incorporate higher order statistics a k a multiple point statistics mps have been developed to generate complex geological patterns from a given training image strebelle 2002 the training image represents a conceptual model of spatial connectivity which is generated by combining various sources of information core data well logs and outcrop and in some cases process based geo modeling hu and chugunova 2008 koltermann mariethoz and caers 2014 michael et al 2010 in sequential simulation using a user specified template possible connectivity patterns and their frequencies in the training image are used to compute local conditional probabilities and store them in a large search tree for simulation at each unsampled grid cell the local data patterns in the neighboring cells are used to locate the corresponding conditional probability in the search tree to sample from although conditioning mps simulation on hard data at well locations and soft data e g seismic measurements is straightforward arpat and caers 2007 mariethoz and caers 2014 strebelle 2002 incorporating the training image patterns in model calibration is nontrivial the probability perturbation method ppm caers and hoffman 2006 and probability conditioning method pcm jafarpour and khodabakhshi 2011 are two direct simulation methods that attempt to generate flow conditioned facies realizations from training images in indirect conditioning methods unconditional facies models are generated and updated using a model calibration approach indirect methods encounter two difficulties in estimating complex geologic facies patterns i preserving the complex connectivity patterns during model updating and ii honoring solution discreetness fulfilling each of these requirements by itself is mathematically challenging and cannot be achieved using simple parameterization and regularization techniques several studies have focused on preserving higher order statistics in the subsurface flow inverse problems li et al 2013 sebacher et al 2016 vo and durlofsky 2014 zhou et al 2014 zhou et al 2012 variants of the pca jolliffe 2002 including kernel pca sarma et al 2008 schölkopf et al 1997 and o pca zhou et al 2014 are examples in which the goal is to honor more complex connectivity patterns during model calibration in recent years sparse geologic dictionaries aharon et al 2006 khaninezhad et al 2012 were introduced for reconstruction of complex geological patterns using sparse reconstruction methods candès and wakin 2008 jafarpour et al 2010 li and jafarpour 2010 golmohammadi and jafarpour 2016 another set of techniques that have been used to solve the problem is discrete to continuous transformations including the level set method cardiff and kitanidis 2009 and distance transform hakim elahi and jafarpour 2017 while these methods can produce discrete solutions they do not have a mechanism to preserve complex mps patterns in the training image recently we developed a discrete regularization approach for reconstruction of facies models khaninezhad and jafarpour 2017 in that approach discreteness is promoted to ensure that the solutions obtained based on continuous parameterization retain the categorical nature of facies while discrete regularization methods batenburg and sijbers 2011 khaninezhad and jafarpour 2017 lukić 2011 schüle et al 2005 prove to be effective for encouraging categorical solutions when complex multiple point statistical patterns are involved the use of discrete regularization does not guarantee accurate reconstruction of higher order spatial patters in such cases one should resort to a pattern based model calibration that not only provides discrete solutions but also is able to preserve the expected mps patterns in the solution furthermore promoting solution discreteness is only appropriate when within facies variability is insignificant and can be ignored a more flexible approach is to ensure that the exact form and statistics of spatial patterns can be incorporated in the solution regardless of whether such patterns are discrete or continuous in this paper we introduce a machine learning algorithm to incorporate complex facies connectivity patterns in calibration of subsurface flow models this is achieved by splitting the model calibration problem into two sub problems that are solved iteratively in the first step of our approach a parameterized approximation of the solution is obtained by solving a regularized least squares inversion the parameterization is used to approximately capture the expected connectivity of the geologic patterns while remaining close to the current feasible solution from previous iteration in the second step the parameterized solution from the first step is mapped onto the geologic feasible using a machine learning approach the learning process uses the k nearest neighbor k nn algorithm larose 2005 to construct local pattern feature vectors and compare them with the feature vectors in the training dataset in this step a spatial template is used to scan the continuous solution and for each template the k nn algorithm is used to identify the k most similar feature vectors in the learning dataset the corresponding label vectors i e multivariate feasible patterns are selected and stored in memory once all local patterns are scanned and processed using a defined template size an aggregation step is applied on the overlapping templates to collectively incorporate the mps patterns in assigning final values to each grid block the mapping step ensures that the solution inherits the same statistics as in the prior models the details of the proposed framework along with numerical simulation and additional discussion are presented in the remainder of the paper 2 methodology 2 1 notation we denote the n dimensional vector of model parameters as u r n where u n 1 represents a set of 2d or 3d images of spatial formation property distributions e g permeability or hydraulic conductivity the observations are demonstrated by the m dimensional vector d obs r m where a nonlinear mapping i e g relates them to model parameters i e g u d obs in presence of observation noise this relationship is stated as d obs g u n where n r m stands for the noise vector and is typically assumed to be gaussian and independent of the model parameters u a linear expansion approximation for parameters u in a low dimensional basis s n i e φ ϕ 1 ϕ 2 ϕ s is expressed as 1 u u i 1 s ϕ i v i φ v where v v 1 v 2 vs are the expansion coefficients that represent the weights given to each basis function in approximating u the q norm of a vector e g u u 1 u 2 un t is denoted by u q q 0 where 2 u q i 1 n u i q 1 q in which the operator returns the absolute value of its argument we denote the set of feasible models all valid samples corresponding to a prior geological scenario e g a given training image by ω p when closed form probabilistic models are available to describe the distribution of formation properties they can be used to completely characterize the feasible set ω p however except for very special cases realistic scenarios are far more complex and do not lend themselves to descriptions with closed form models in that case the feasible set can be defined either by a conceptual model that represents the expected spatial statistics e g a training image or by a very large set of model realizations that approximately represent the statistical patterns in such model the reference model in fig 1 shows a meandering fluvial channel a and a proposed training image b to represent the conceptual model with similar patterns along with sample realizations drawn from the training image c fig 1 d represents similar samples to those in fig 1 c except that within facies variability is accounted for through multi gaussian distributions in this paper the feasible set ω p refers to a large set of model realizations similar to those in fig 1 c or d that represent the expected spatial patterns in the solution in practice only a finite set of representative samples are available to represent ω p and its associated statistical patterns however it is important to note that the solution is not assumed to be one of the samples in ω p in our implementation a small template is used to scan the samples in ω p to generate a large set of local spatial patterns that are used in the mapping step of the proposed model calibration 2 2 problem statement and formulation a simple model calibration problem statement can be presented as a constrained weighted least squares formulation 3 min u j u 1 2 c n 1 2 d obs g u 2 2 1 2 d obs g u t c n 1 d obs g u s t u ω p where u ω p restricts the least squares solution to the predefined feasible set here d obs denotes the observed data and c n is typically a diagonal weight matrix covariance of the noise vector n that accounts for the effect of data noise in this paper we assign larger noise variance for the data type s with larger expected magnitude the regularization term can also be expressed using an indicator function of the form 4 i u ω p 0 i f u ω p i f u ω p which can be used to rewrite eq 3 as 5 min u j u 1 2 c n 1 2 d obs g u 2 2 s t i u ω p 0 for special cases such as a multi gaussian distribution of parameters it may be possible to express the constraint i u ω p 0 analytically for example to honor the variogram or covariance function of the underlying distribution however in more complex problems where an analytical expression cannot be used to describe the constraint i u ω p 0 it is not trivial to express this constraint or enforce it during model calibration furthermore exhaustive search of the feasible set to find the minimum of the objective function is computationally impractical in this work we develop an indirect method for solving the constrained minimization problem by defining a two step alternating directions method the first step uses a standard gradient based method to find an approximate parameterized solution to the problem the second step maps the parameterized solution onto the feasible set while ensuring that the updated solution remains close to the parameterized solution from the first step these two steps are repeated until no further improvement in the objective function is obtained in this paper we use the k nearest neighbor algorithm to implement the mapping in the second step section 2 3 to develop the two step solution approach we first expand the parameters into a set of feasible values u and the corresponding parameterized approximation using the linear expansion in eq 1 i e u φ v since the transformation matrix φ is known we choose the expansion coefficients v to represent u hence we can minimize the following objective function to find u and v 6 min v u j v u 1 2 c n 1 2 d obs g φ v 2 2 s t φ v u 2 2 ϵ 2 i u ω p 0 the constraint φ v u 2 2 ϵ 2 is used to ensure that the solution in the linear expansion space does not deviate significantly from the feasible solution in addition g u is replaced by g φv in the data mismatch term to i reduce problem dimensionality and ii separate g u and i u ω p in the alternating steps eqs 8 and 9 an alternative form of the objective function in eq 6 can be written as 7 min v u j v u 1 2 c n 1 2 d obs g φ v 2 2 i u ω p λ 2 2 φ v u 2 2 here λ2 is a regularization parameter that controls the linear expansion approximation error for linear inverse problems i e when g is a linear operator cross validation golub et al 1979 and l curve hansen 1992 methods are developed to properly set the value of λ however these methods do not extend to nonlinear problems for nonlinear problems practical approaches such as trial and error and sensitivity analysis are commonly used in some cases the solution may not show sensitivity to λ within wide range and a simple sensitivity analysis can be used to identify this range khaninezhad et al 2012 to minimize the objective function in eq 7 we use the following two step alternating directions algorithm where the first step updates the parameterized solution through v while the second step maps this solution onto the closest feasible solution u in ω p 8 i v k argmi n v 1 2 c n 1 2 d obs g φ v 2 2 i u k 1 ω p λ 2 2 φ v u k 1 2 2 ii u k argmi n u 1 2 c n 1 2 d obs g φ v k 2 2 i u ω p λ 2 2 φ v k u 2 2 these steps can be further simplified by dropping the fixed terms in the objective functions to arrive at 9 i v k argmi n v 1 2 c n 1 2 d obs g φ v 2 2 λ 2 2 φ v u k 1 2 2 ii u k argmi n u i u ω p λ 2 2 φ v k u 2 2 the minimization problem in step i of eq 9 can be carried out using standard gradient based optimization techniques boyd and vandenberghe 2004 the update in step ii is implemented by mapping the parameterized solution u k φ v k onto the closest feasible model using the proposed method is section 2 3 simple convergence criteria such as a threshold on u k u k 1 2 2 and v k v k 1 2 2 or the objective function value can be used to stop the iterations in eq 9 in the two step alternative direction approach of eq 9 step i updates v by minimizing the data mismatch while ensuring that the approximation error λ 2 2 φ v u k 1 2 2 i e the difference between parametrized and feasible solutions remains small this regularization term defines a closed neighborhood surrounding u k 1 which is the estimation result obtained from the feasible set in the most recent iteration consequently the updated parameter in step i i e u k φ v k remains close to u k 1 while searching for a new solution to improve the data match step ii uses the new parameterized solution from step i i e v k to update u via mapping onto the feasible set since φv k is forced to remain close to u k 1 the new mapping result u k is updated gradually in this way the regularization term λ 2 2 φ v u 2 2 ensures a gradual update and alleviates problem ill posedness 2 3 enforcing feasibility constraint through pattern learning to enforce the feasibility constraint in the second step of the alternating direction algorithm we use ideas from machine learning literature bishop 2006 kotsiantis et al 2007 machine learning techniques are generally classified into supervised unsupervised and semi supervised or reinforcement learning sutton and barto 1998 in supervised learning a set of input output i e feature label samples are observed or constructed and incorporated to learn an approximation of the mapping function between the inputs and outputs in unsupervised learning no output samples labels are available and the input samples are utilized to learn the hidden patterns in the data classification and clustering jain and dubes 1988 are famous examples of supervised and unsupervised learning problems respectively in semi supervised learning a small portion of the samples are labeled and the primary goal is similar to that of unsupervised learning chapelle et al 2009 reinforcement learning sutton and barto 1998 couples the system s functionality with the dynamic environment to reach to a certain goal which is usually carried out by maximizing a reward function in this paper we use a classification algorithm as a supervised learning approach to implement the mapping in step ii of the solution approach several algorithms such as k nearest neighbor k nn linear discriminant analysis support vector machines decision trees logistic regression and neural networks bishop 2006 are proposed for classification problems each with specific applications the k nearest neighbor k nn larose 2005 which is implemented in this paper is a simple classification technique where the decision about the label of a feature vector is made by investigating the behavior of the most similar feature vectors in the learning dataset using an appropriate distance measure the algorithm selects the k most similar feature vectors from a training dataset and assigns the most frequent label corresponding to these feature vectors as the output label mathematically given a feature vector x and a learning dataset d x i y i i 1 n the classifier searches over x 1 x 2 x n and identifies k feature vectors with smallest distance from x the method then assigns the corresponding k output vectors from the training set i e y 1 y 2 y n to the feature vector x depending on the specifics of the problem the output labels are used to extract the decision criteria typically in supervised learning problems y i takes a single discrete value that represents the label of a specific class however in our formulation the label vectors are high dimensional and carry local feasible connectivity patterns discussed next despite its simplicity the k nn algorithm enjoys strong theoretical background as a classification technique ripley 2007 new we present the implementation details of the k nn algorithm that we use in this paper fig 2 depicts a simple schematic of mapping a sample parameterized solution u k φ v k onto ω p for this example we have used the basis φ to be the truncated pca basis which consists of the eigenvectors of the sample covariance matrix associated with feasible set as it is demonstrated in fig 2 the mapping uses the parametrized solution as input and provides a corresponding solution from the feasible set we denote r ω p as the function that maps a solution outside the feasible set onto the set ω p additionally a set of initial learning samples i e r i u i i 1 n are provided where u i is the mapping of r i onto the set ω p the goal is to use d r i u i i 1 n to learn the mapping operator which can be used to find the feasible solution u based on the parameterized approximation u fig 3 demonstrates the schematic of a supervised learning approach used for the mapping in our application r i s can be calculated as r i φ φ t u i if the basis functions are orthogonal e g pca this way of generating the learning dataset ensures that the input learning samples r i s and the parametrized solution during inversion i e φ v k are defined using the same basis fig 4 depicts how the parameterized learning samples are constructed to correspond to the same level and type of parameterization as in step i of the model calibration approach a major criterion in learning the mapping from parameterized input samples i e r i s to their corresponding feasible output samples i e u i s is to preserve the statistical consistency of the patterns a simple mapping approach is to search the feasible set to find the closest solution to the full parameterized map however this strategy is unlikely to work in automatic model calibration for two reasons first arbitrary updates are applied to the solution at each iteration of model calibration which can result in significant deviation from the global connectivity patterns in the training models second it requires an extremely large set of training models to cover all possible global pattern configurations which is neither necessary nor practical a more flexible alternative is to use the training data to extract statistical information local patterns and use those patterns to construct a feasible model that corresponds to the parameterized solution a similar approach is used in multiple point geostatistical simulation techniques such as snesim strebelle 2002 and simpat arpat and caers 2007 we implement the feasibility mapping in two steps in the first stage using a specified template size we scan the given parameterized image φ v k i e current iterate and compute the corresponding feature vectors local patterns inside the defined template for each instance of the scanning template we use the k nn classification algorithm to identify the k closets feature vectors in the parameterized dataset which are generated from r i i 1 n the corresponding k label vectors in the dataset are then stored not pasted for the cells covered by the scanned area in the second step an aggregation approach is used to combine all the stored feasible instances in each model cell to approximately represent the conditional distribution for the cells each cell is then assigned the mode value with highest frequency of the conditional probability density the resulting map constitutes the feasible solution obtained in step ii of eq 9 and is passed to step i to continue the iterations we note that the feature vectors do not need to be the spatial descriptions of the patterns and could be defined based on various factors including computation for instance one class of feature vectors can be the projected coefficients of the patterns onto a user defined low dimensional subspace e g leading pca elements of each template the feature vector could also be defined by considering a subset of grid cell within the template either randomly or deterministically for computational gain regardless of how the feature vectors are defined the corresponding label vectors are local templates that are generated from the feasible set fig 5 left demonstrates the learning stage where a circular template is used to scan the prior image realizations i e pairs of r i u i to generate the corresponding segments of the parameterized and feasible input images in this case the feature vectors denoted as x i are defined as the exact grid block values that are located inside the templates the corresponding feasible labels are denoted as y i as shown in fig 5 in this paper we adopt as feature vectors the exact grid block values inside the scanning templates the final dataset of features and labels d x i y i i 1 n is then used to implement the mapping in step ii of eq 9 fig 6 demonstrates the schematic of the first stage of the segmentation procedure the k nn classifier explores the feature space in the training dataset to find the k feature vectors with smallest distance to each scanned portion of φ v k and stores the corresponding feasible label vectors for the scanned regions as shown in fig 6 the same template that is used to construct the learning dataset is applied to extract the feature vectors in φ v k we note that the first stage results in significant overlap between the scanned regions which provides many samples for each cell in the model after the initial classification and storing of the label vectors an aggregation or voting step is used to approximate the conditional probability of the labels for each cell based on all the assigned values to each cell the resulting distribution is then used to decide about the feasible values for each grid block fig 7 depicts the aggregation approach where for any given cell marked with a dot the labels from scanning different regions are combined to approximate the corresponding conditional probabilities in this paper we assign the model of this distribution to each grid block to construct the feasible solution we note that for non discrete feasible values e g channels with within facies heterogeneity a histogram of the values associated with each cell is constructed and the value with the highest frequency is adopted as the solution for each cell we note that the aggregation step ensures that facies assignment to each grid block accounts for the spatial statistics and connectivity patterns from an extended neighborhood approximately twice the size of the template in each direction moreover aggregation leads to far more samples than k for each grid cell which substantially increases the accuracy and spatial consistency of the method fig 8 illustrates the importance of aggregation with a simple example the parameterized map φ v k from fig 2 is shown on the top left along with a discrete version that is obtained by thresholding clearly thresholding does not incorporate spatial statistics and results in a poor reconstruction of the parameterized map the middle and the bottom rows of fig 8 show the results of the proposed mapping approach without and with using the aggregation step respectively the results are shown for different template sizes i e r 5 and r 25 and different k values in k nn classifier k 1 and k 5 in both cases large values of the template size and k lead to improved reconstruction in the case without aggregation the results are not as accurate and random artifacts emerge in the feasible solution aggregation improves the connectivity by exploiting additional spatial information by extending the local neighborhood furthermore when aggregation is used less sensitivity to k is observed as aggregation provides many more samples than can be obtained by increasing k many overlapping templates cover the same cell in the model learning the mapping operator for step ii of eq 9 can be computationally demanding for large template sizes during the k nn classification step the feature vector in the solution i e x is compared with the n feature vectors in d x i y i i 1 n where the comparison is performed by applying a normed measure such as l 2 norm therefore the computational cost of mapping φ v k onto ω p is proportional to the number of grid blocks the value of n and the complexity of calculating x x i for an arbitrary index i the computational complexity of calculating x x i is directly related to the type of norm used and the dimension of the feature vector x increasing the dimension of the feature vector i e size of x i s linearly increases the computational complexity in calculating x x i thus larger neighborhood templates require more demanding calculations to construct the feature vectors a template and a reasonable value for n are assigned the feature vectors are then generated by scanning the data samples in d r i u i i 1 n for practical implementation the scanning does not need to be exhaustive which would involve redundancy and can be performed by considering a random subset of regions within each sample which also reduces the memory demand the value of n i e number of feature vectors in the learning dataset linearly increases the computational complexity of the k nn classifier therefore a reasonable value for n must be considered e g 1000 a major contributor to computational complexity is the number of k nn classifications before the aggregation step one way to reduce this computation is to use a small subset of the overlapping templates fig 9 a and b depict the schematic of the uniform i e using all the grid blocks and random k nn classifications respectively the reconstructed discrete solutions for different subsampling ratios are shown in fig 9 c even using as low as 10 of the cells for classification does not deteriorate the reconstruction results significantly this is primarily because the aggregation step increases the number of samples for each grid block the computational complexity is however significantly reduced by introducing subsampling additionally the k nn classification for each cell is independent of the each other allowing for parallel implementation using multiple processors in performing the first step of mapping i e classification step an additional computational consideration is in solving the optimization problem in step i of eq 9 which requires several numerical flow simulation runs in practice finding the exact minimum of step i in eq 9 is not necessary and only a few iterations are sufficient to move to step ii of the algorithm several justifications can be brought for this approximation in general the first few updates provide the main changes in the solution moreover the solution is initialized by the solution obtained from the previous step and the term φ v u 2 2 in the objective function discourages significant departure from the solution obtained in the previous step thereby requiring modest updates furthermore the solution is bound to be updated in the next several iterations even when a computationally more expensive exact solution is obtained in our experience the first few less than 5 iterations provided the main updates to the solution in summary the computational complexity of a single mapping is proportional to number of cells in the random path for classification the size of the feature vectors and the number of features vectors in d x i y i i 1 n usually n does not change significantly by increasing the dimension of the model in addition minimization of eq 8 require several forward simulations and adjoint computational which can be computationally very significant for field scale problems an important point to discuss before presenting the results is that while other methods may be applied to perform the mapping in step ii one needs to consider the properties and effectiveness of the implementation for instance one may consider using a scaled version of the parametrized solution as facies probability map soft data to generate a conditional discrete facies realization as in pcm jafarpour and khodabakhshi 2011 however the pcm was used in ensemble based data assimilation with enkf where the updated ensemble mean after data assimilation was used as soft data to regenerate an updated ensemble i e conditional facies realizations this approach ensured that the correct statistics were inherited from the training image before implementing the next forecast and update steps however in pcm using the scaled version of the ensemble mean as soft data implies that departure from the local connectivity patterns in the probability map is allowed this is acceptable in pcm because the ensemble realizations did not follow the same exact connectivity that was in the ensemble mean as a result in the pcm only the mean of the generated ensemble is consistent with the facies probability map due to the stochastic nature of the snesim simulation in the current formulation the goal is to obtain a feasible map that is closest to parametrized solution that is obtained in step i therefore the method should not allow departure from the continuous solution there are also other important differences between these methods the method in the current paper is based on pattern learning and can be applied to continuous and discrete patterns whereas pcm was developed for discrete facies additionally the proposed method does not rely on the snesim or any other algorithm for facies simulation the use of a prior training set allows the method to be applicable to training datasets that are obtained from other geostatistical simulation techniques this property generalizes the method and makes it independent of the geostatistical method used to generate the prior training set 3 numerical results we present three numerical experiments to examine the performance of our developed approach the first example is a straight ray travel time tomographic inversion bregman et al 1989 which leads to a linear inverse problem the second experiment involves a ground water pumping test in which the facies represent the hydraulic conductivity of the field in this case steady state pressure head data are used to reconstruct the spatial distribution of lithofacies that represent the hydraulic conductivity maps franssen et al 2009 in the last experiment we consider a two phase flow problem where a 3 dimensional model of facies distribution is estimated from transient pressure and flowrate data in these examples the k nn classification step is based on a random subsampling approach by scanning only 20 of the grid blocks for the feature label data pairs i e d x i y i i 1 n we use a total of n 2000 samples 3 1 example 1 linear tomographic inversion in tomographic inversion measurements of travel arrival times of transmitted acoustic waves are used to estimate the slowness 1 velocity of the medium between the source and receiver pairs subsurface environment the configuration of the travel time tomographic inversion is shown in fig 10 a where the sources left transmit the acoustic waves to the receivers right in this experiment an array of 6 sources and 6 receivers with equal interval are used resulting in 36 arrival time measurements the governing equation that relates the observation and parameter spaces is 10 t u x d x 0 u x 1 v x where t denotes the travel time of each wave and u x is the rock slowness 1 velocity at the location specified by x coordinates a discretized version of eq 10 can be used to form a linear system of equations between the observations ray arrival times and parameters rock slowness for each cell in the discretized i e d gu case 1 example with discrete facies the reference slowness map contains a complex meandering structure as depicted in fig 10 b in this case the values of the parameter are set to log 10 mm s and log 600 mm s for outside and inside of the channel respectively the model domain has a dimension of 1000 1000 m2 which is discretized uniformly into 100 100 cells the prior training data consists of n 500 model realizations which are drawn from a training image shown in fig 11 a consisting of discrete meandering connectivity patterns these samples i e previously defined u i s in d r i u i i 1 n are used to approximately represent the patterns in the feasible set ω p we note that for small values of n e g n 50 the existing patterns in u i i 1 n may not be representative of the entire local connectivity features of the feasible set in addition a small n may result in an inaccurate pca parameterization on the other hand large values of n e g n 20 000 may overestimate the complexity of the patterns in the feasible set and result in redundant patterns and drastic computational overhead therefore a reasonable value needs to be assigned for n fig 11 b displays four sample realizations from this set the corresponding parameterized models i e r i s in r i u i i 1 n are obtained using truncated pca parameterization of u i i 1 n the corresponding pca basis functions φ of the parameterization subspace are shown in fig 11 c fig 10 c depicts the projection of the reference model on the truncated pca basis for inversion the initial map which is the mean of the samples in ω p is shown in fig 10 d fig 12 a shows 10 iterations of the inversion results i e φv top and u bottom the feasibility mapping step uses a 50 50 square template and the value of k in the k nn algorithm is set to 5 we initially start with 20 leading pca bases functions for parameterization and gradually increase the dimension of parametrization to 50 throughout 10 consequent iterations the objective is to initially capture the large scale connectivity using fewer global connectivity patterns and gradually increase the parameter size to improve the resolution of the reconstructed model as shown in fig 12 a the inversion process can detect the meandering connectivity structures with good accuracy within the first 10 iterations during the initial iterations the algorithm primarily identifies the global connectivity structures while at later iterations it enhances the identified global pattern with small scale adjustments case 2 including within facies variability in this case the reference map which is demonstrated in fig 13 a consists of non discrete heterogeneity inside and outside the meandering channel structure the heterogeneity within the fluvial channels is modeled independently of the heterogeneity within the background facies the heterogeneity inside and outside the channel in the reference map is modeled by gaussian processes using two different variogram models the direction of maximum continuity for the heterogenicity i e the azimuth is θ 45 fig 13 d depicts the histogram of the parameter values in the reference model which indicates a bimodal distribution we note that to the contrast in the facies values the variability within channel facies is not as visible as that in the background facies fig 13 e shows four out of n 500 of model realizations that were used as training data to represent the feasible set these prior models were also used to construct the pca basis φ to form the parameterization subspace in this case we use 100 basis functions to represent the solution in the parameterization space however we start the inversion with 20 basis functions and uniformly increase the number of basis functions to 100 throughout 15 iterations until convergence the best achievable solution and the initial map are shown in fig 13 b and c respectively all other parameters are the same as case 1 the results of parameter reconstruction for φv and u are summarized in fig 14 showing the evolution of the parameterized and feasible solutions throughout the iterations the results show that the method can reconstruct models that include facies and within facies viability interestingly the algorithm can detect the channel and non channel facies and identify the general trend in the background heterogeneity we note also that if only the continuous solution is used to reconstruct the model the resulting solution provides a lumped image making it hard to separate the facies distribution from the background heterogeneity whereas the feasibility mapping step identifies the heterogeneity in addition to the facies configuration 3 2 example 2 2d ground water pumping test in the second example we consider integration of data from a groundwater pumping text fig 15 a shows the schematic of the test which includes a pumping well in the middle of a 1500 1500 20 m3 domain which is divided into 100 100 1 cells the well is extracting water with a rate of 60 m 3 h the top and bottom boundaries of the model have no flow boundary conditions while the boundary conditions on the left and the right sides of the domain are constant pressures of 20m and 10m respectively a uniformly placed network of 25 monitoring wells is used to measure the hydraulic conductivity values and the steady state pressure heads the governing equations for the pumping test in this example involve single phase flow equations in a saturated porous medium which are derived by combining mass conservation and darcy s law and can be expressed as 11 ϕ ρ t ρ v q conservation of mass v 1 μ k p ρ g z darcy s law here ϕ ρ v p and q represent porosity density velocity pressure and pumping rates respectively μ represents the fluid viscosity in addition k denote the intrinsic formation permeability which we assume is the same in x y and z directions the hydraulic conductivity defined as k h k ρ g μ as well as the observed pressure head values at the monitoring well are the observations d the solution of eq 11 provide the operator g that maps the parameter space to data space i e d g u k h case 1 example with two facies types for this case the reference and initial log conductivity maps are shown in fig 15 b and d respectively while their corresponding pressure distributions are depicted in fig 15 e and f respectively the logarithm of the hydraulic conductivity in the reference model fig 15 b for the two facies types are 2 3 and 0 5 log m s respectively fig 15 c shows the best parameterized approximation of the reference map based on a rank 40 pca approximation the training image in fig 16 a which consists of intersecting channel features is used to generate n 500 facies realizations to be used as prior training data i e u i s in r i u i i 1 n fig 16 b shows four samples from the resulting prior training data the prior samples are used to develop the pca basis functions φ fig 16 c for low rank parameterization of the facies models the model calibration process starts by including 20 leading pca basis functions and gradually increasing the number of pca basis function to 40 by adding 2 basis elements per iteration to examine the significance of mapping the parameterized solution onto the correct feasible set we compare the results from our k nn machine learning approach with an alternative method in which thresholding of the continuous solutions φ v k is used in step ii of eq 9 to obtain discrete facies distributions figs 17 and 18 summarize the solution iterations for our method and the simple thresholding approach respectively in this example for implementing the k nn method the value of k is set to 3 and the neighborhood template consists of a circle with radius 25 cells comparison between figs 17 and 18 shows that mapping onto the feasible set provides superior results to a simple thresholding the main difference between the two method stems from the absence of correct spatial statistics patterns in the grid based thresholding approach from fig 17 it is evident that mapping the continuous solutions onto the correct feasible set provides patterns and spatial statistics that are consistent with the prior model whereas a discrete thresholding does not improve the spatial connectivity and only replaces continuous values with their closest discrete values it is particularly noteworthy that mapping onto the feasible set tends to correct errors in channel connectivity and eliminate inconsistent discontinuities case 2 example with three facies types in this example the reference log hydraulic conductivity map depicted in fig 19 a consists of 3 discrete facies types with values 2 3 blue 0 7 green and 0 6 red log m s the reference pressure head map as well as the location of 25 monitoring and pumping wells are shown in fig 19 b the monitoring wells are located uniformly within the domain and the steady state pressure and hydraulic conductivity values measured at the location of these wells are used as data for inversion for this example 1000 prior model realizations are generated from a training image with 3 facies values four sample realizations from this set are depicted in fig 19 c we apply the pca with the prior model realizations to generate the parameterization basis functions 49 leading pca basis images are demonstrated in fig 19 d the inversion is started with 25 basis functions and after each iteration 3 additional pca basis elements are added to improve the solution all other parameters are the same as case 1 fig 20 depicts the solutions at iterations 0 start 1 2 7 and 10 as can be seen the general connectivity pattern in the reference model is captured correctly at the boundary of the facies the solution does not exhibit the irregularity that is observed in the reference model the smoothness at the boundaries is primarily due to that fact in the mapping step of the algorithm the mode of the local conditional distribution of facies is assigned to each cell 3 3 example 3 two phase flow in 3d formation for our last example we consider two phase flow in a 3d formation to integrate transient and highly nonlinear observations of pressure and flow rate the model configuration and the reference map for this example are shown in fig 21 a and b the model consists of a 1000 1000 160 m3 domain which is divided into a uniform grid system with 100 100 8 cells of size 100 100 20 m3 the reference permeability map with 10md and 600md values for low and high permeability regions includes two distinct spatial patterns each extending four vertical layers of the model fig 21 b bottom there are four fluid injection and five fluid extraction wells in the domain constant injection rate and extraction pressure are used to control the wells while the pressure at the injection wells and phase fluid flow rates at the extraction wells are observed with one month interval and used for model calibration in the first 4 years model calibration stage 0 8 pore volume of the wetting phase e g water is uniformly injected into the formation resulting in 48 observation time intervals another 0 8 pore volume is injection during the next four years the forecast period the main uncertainty in this example is related to the spatial distribution of rock facies types which is constrained by assimilating the observed dynamic responses at the well locations fig 21 c and d shows the dynamic evolution of the non wetting phase i e hydrocarbon saturation and fluid pressure in the formation in this example a two phase incompressible and immiscible fluid flow system is considered for which the governing pde equations can be expressed as 12 λ w b w k p w γ w z t ϕ s w b w q w λ n b n k p n γ n z t ϕ s n b n q n in these equations phase mobility phase density formation volume factor intrinsic formation permeability formation porosity gravity potential phase saturation and flux are represented by λ γ b k ϕ z s and q respectively and w and n represent the wetting and non wetting phases the governing equations in eq 12 involve four unknown dynamic state variables p n s n p w and s w two additional equations are needed to close the pde system these two equations are the constitutive equations that relate phase pressures and saturations and are expressed as 13 p n p w p c s w s w s n 1 0 s w s n 1 the first equation describes the capillary pressure difference between non wetting and wetting phase pressures as a function of the wetting phase saturation while the second equation imposes a physical constraint on the saturation of two phases in a fully saturated medium the capillary pressure is assumed to be zero in this example i e p n p w 0 the spatial distribution of permeability k in eq 12 is the unknown parameter of interest and is estimated from the observed data in this example we assume no flow condition on the boundaries of the domain for model calibration n 1000 prior realizations of facies are generated using the same training image in fig 16 a the spatial patterns used for the two zones layers 1 4 and layers 5 8 are assumed to belong to the same training image fig 22 a shows the four samples from the prior training data the leading 12 out of 50 pca basis function for parameterization are shown in fig 23 b while the best rank 50 pca approximations for the two zones are depicted in fig 22 c fig 23 summarizes the inversion results which are shown separately for layers 1 4 and 5 8 for the first step of the algorithm i e continuous solution the pca parameterization is initialized using 30 leading basis functions and two new basis functions are added in the 10 subsequent iterations to arrive at a rank 50 approximation for the second stage of the inversion mapping the continuous solution φ v k onto the feasible set layers 1 4 and 5 8 are projected onto ω p independently the neighborhood template for the k nn algorithm has a size of 40 40 and k is 3 the projection results for layers 1 4 and 5 8 are depicted in the second and fourth columns of fig 23 4 conclusion an important aspect of applying inverse modeling to physical systems is imposing feasibility constraint in subsurface flow model calibration ensuring the model calibration solutions are geologically plausible can be a difficult constraint to enforce when the expected connectivity patterns are complex and hard to represent much less preserve during model updating for instance updating meandering fluvial channels to match the observed data while preserving their shape and connectivity pattern is very challenging and difficult to achieve using classical model calibration techniques in this paper we employed machine learning techniques to develop a framework for automatic calibration of complex facies models while maintaining their complex connectivity patterns specifically we defined a feasible set for model parameters that must be observed during model calibration using examples from fluvial systems we described the feasible set to with many prior model realizations that summarize the expected spatial statistics of the solution to implement the feasibility constraint we formulated a regularized least square formulation in which the regularization imposes a complex feasibility constraint using alternative directions method of optimization we split the objective function into two sequential optimization sub problems where in the first problem a parameterized model calibration is solved to use dynamic flow data to infer the approximate connectivity patterns without honoring the feasibility constraint in the second step this solution is mapped onto the feasible set by employing the k nn algorithm as a supervised machine learning technique the k nn algorithm is used to learn the mapping operator using multi dimensional feature and label vectors initially the realizations of the feasible set i e discrete fluvial channel maps are projected onto a pca parameterization space to obtain the corresponding continuous maps a local neighborhood template is then used to scan and store the discrete and corresponding parameterized maps in the training data this obtained learning dataset is then used during the second stage of the inversion solution to implementing the mapping onto the feasible set given the parameterized solution from the first step of model calibration the k nn algorithm is used to scan the continuous solution with a predefined template size to identify the k best representative features in the training dataset the corresponding feasible vectors for those k maps are stored as instances or samples belonging to the scanned locations after scanning is completed the stored maps are processed using an aggregation method where all the relevant samples for a given cell including those obtained by overlapping templates are used to characterize the conditional probability for every cell in the domain using the resulting conditional probability a maximum likelihood approach is used to assign final feasible values to each cell of the model the aggregation step is very important in increasing the consistency of the results and in reducing the computational complexity of the method using a series of increasingly complex numerical experiments we examined the performance of the proposed approach to highlight the importance of honoring complex prior models as feasibility constraint and demonstrating the performance of the machine learning approach in implementing this constraint several aspects of the introduced method can be extended for instance instead of a supervised learning which was the core of the introduced method in this paper other supervised and unsupervised approaches with different properties may also be applicable we used a simple definition of the feature vector which was the group of cells that were covered by the template or a subset of those cells to reveal the underlying spatial pattern however a more sophisticated and efficient description of the feature vector may be considered for example by including kernel functions furthermore an important assumption that is made in this work is that the training data is reliable in practice the prior model that is available for training may be subject to uncertainty for instance multiple geologic scenarios may be considered plausible for a given formation therefore an extension of the current formulation is application to problems in which several geologic scenarios and hence feasible sets larose 2005 in addition to uncertainty in the feasible set an important aspect of model calibration is uncertainty quantification in this paper we used the proposed approach within a deterministic inverse modeling context ignoring various sources of uncertainty to quantify the uncertainty in the solution and future prediction a bayesian inversion formulation may be developed however theoretically rigorous probabilistic treatment of model calibration problems with complex connectivity patterns is not trivial as the underlying spatial patterns are difficult to represent using simple probabilistic models e g gaussian it is important to note that the heterogeneity in subsurface formation can take different forms and one must verify whether the assumptions used in adopting a method are applicable to any specific site for instance if the dominant heterogeneity in the field exhibit smooth transition in crossing extreme values estimating facies architecture may not be important or relevant i e one can simply estimate rock property distributions as continuous random field as in traditional approaches in other cases the facies distribution and boundaries may be so dominant in describing the flow and transport behavior that one could ignore the role of within facies variability and perform facies calibration as the observed data may not exhibit sensitivity to within facies changes there are also cases in which one must include both facies distribution and within facies variability to adequately represent the correct subsurface heterogeneity during model calibration another important issue is the resolution and information content of data which may allow for estimation of facies distribution but may not support the inference of within facies variability in conclusion machine learning techniques can offer effective tools for solving difficult model calibration problems and lead to novel data oriented formulation for solving inverse problems in particular for calibration of flow models with complex spatial connectivity patterns where traditional methods may not have the sophistication required to represent and preserve such patterns in the solution learning techniques may provide a viable and powerful alternative to perform the task for large scale problems the learning process may involve considerable computational overhead however in many cases the learning step can be performed off line and may be conveniently parallelized making it a feasible option for practical applications acknowledgement the work in this paper is supported in part by energi simulation all data and digital content in this manuscript can be accessed by sending an email to behnam jafarpour usc edu 
763,calibration of heterogeneous subsurface flow models usually leads to ill posed nonlinear inverse problems where too many unknown parameters are estimated from limited response measurements when the underlying parameters form complex non gaussian structured spatial connectivity patterns classical variogram based geostatistical techniques cannot describe the underlying distributions modern pattern based geostatistical methods that incorporate higher order spatial statistics are more suitable for describing such complex spatial patterns moreover when the unknown parameters are discrete e g geologic facies distribution conventional model calibration techniques that are designed for continuous parameters cannot be applied directly in this paper we introduce a novel pattern based model calibration method to reconstruct spatially complex facies distributions from dynamic flow response data to reproduce complex connectivity patterns during model calibration we impose a geologic feasibility constraint that ensures the solution honors the expected higher order spatial statistics for model calibration we adopt a regularized least squares formulation involving i data mismatch ii pattern connectivity and iii feasibility constraint terms using an alternating directions optimization algorithm the regularized objective function is divided into a parameterized model calibration sub problem which is solved via gradient based optimization the resulting parameterized solution is then mapped onto the feasible set using the k nearest neighbors k nn as a supervised machine learning approach to honor the expected spatial statistics the two steps of the model calibration formulation are repeated until the convergence criterion is met several numerical examples are used to evaluate the performance of the developed method keywords subsurface flow model calibration discrete facies pattern based simulation k nearest neighbor 1 introduction subsurface flow model calibration often involves estimating the spatial distribution of important properties of geologic formations from limited available measurements with the goal of improving future model predictions for improved optimization and resource management purposes carrera et al 2005 delhomme and lavenue 2000 gavalas et al 1976 hill and tiedeman 2006 kitanidis and vomvoris 1983 mclaughlin and townley 1996 oliver and chen 2010 oliver et al 2008 zhou et al 2014 reproducing the measured data and preserving the expected characteristics of the geologic formation are two general requirements in these problems problem ill posedness computational complexity and multiple sources of error and uncertainty complicate the solution of model calibration problems carrera et al 2005 franssen et al 2009 hill and tiedeman 2006 mclaughlin and townley 1996 oliver and chen 2010 oliver et al 2008 sambridge and mosegaard 2002 tarantola 2005 zhou et al 2014 in many cases the resulting solutions are non unique as limited data are used to constrain high resolution model parameters a rich body of literature on parameterization and regularization techniques exists that discuss various approaches to deal with problem ill posedness and solution non uniqueness in general these techniques either reduce the dimension of model parameters or require additional attributes from the solution properties in both cases additional information is used to explicitly or implicitly restrict the feasible set of the solution oliver and chen 2010 zhou et al 2014 tikhonov 1963 zhou et al 2014 while general regularization and parameterization methods are helpful for properties that have simple spatial distributions e g gaussian smooth or piecewise smooth fields bhark et al 2011 jafarpour and mclaughlin 2009 jafarpour et al 2010 lee and kitanidis 2013 reconstruction of models that have more complex spatial patterns in their distribution requires specific prior knowledge and more sophisticated formulations arpat and caers 2007 gavalas et al 1976 khaninezhad et al 2012 in particular geostatistical methods have been used to constrain the solution of subsurface inverse problems doherty 2003 kitanidis 1995 kitanidis and vomvoris 1983 lavenue et al 1995 zimmerman et al 1998 while the classical variogram based or two point geostatistical methods chiles and delfiner 2009 are appropriate for simulating gaussian processes they are not sufficient for representing more complex connectivity patterns object based simulation techniques deutsch and wang 1996 koltermann such as marked point processes deutsch and wang 1996 are used for modeling complex facies patterns while geologically intuitive and appealing these methods are quite cumbersome for data conditioning primarily due to the lack of flexibility in morphing existing objects modern geostatistical modeling techniques that incorporate higher order statistics a k a multiple point statistics mps have been developed to generate complex geological patterns from a given training image strebelle 2002 the training image represents a conceptual model of spatial connectivity which is generated by combining various sources of information core data well logs and outcrop and in some cases process based geo modeling hu and chugunova 2008 koltermann mariethoz and caers 2014 michael et al 2010 in sequential simulation using a user specified template possible connectivity patterns and their frequencies in the training image are used to compute local conditional probabilities and store them in a large search tree for simulation at each unsampled grid cell the local data patterns in the neighboring cells are used to locate the corresponding conditional probability in the search tree to sample from although conditioning mps simulation on hard data at well locations and soft data e g seismic measurements is straightforward arpat and caers 2007 mariethoz and caers 2014 strebelle 2002 incorporating the training image patterns in model calibration is nontrivial the probability perturbation method ppm caers and hoffman 2006 and probability conditioning method pcm jafarpour and khodabakhshi 2011 are two direct simulation methods that attempt to generate flow conditioned facies realizations from training images in indirect conditioning methods unconditional facies models are generated and updated using a model calibration approach indirect methods encounter two difficulties in estimating complex geologic facies patterns i preserving the complex connectivity patterns during model updating and ii honoring solution discreetness fulfilling each of these requirements by itself is mathematically challenging and cannot be achieved using simple parameterization and regularization techniques several studies have focused on preserving higher order statistics in the subsurface flow inverse problems li et al 2013 sebacher et al 2016 vo and durlofsky 2014 zhou et al 2014 zhou et al 2012 variants of the pca jolliffe 2002 including kernel pca sarma et al 2008 schölkopf et al 1997 and o pca zhou et al 2014 are examples in which the goal is to honor more complex connectivity patterns during model calibration in recent years sparse geologic dictionaries aharon et al 2006 khaninezhad et al 2012 were introduced for reconstruction of complex geological patterns using sparse reconstruction methods candès and wakin 2008 jafarpour et al 2010 li and jafarpour 2010 golmohammadi and jafarpour 2016 another set of techniques that have been used to solve the problem is discrete to continuous transformations including the level set method cardiff and kitanidis 2009 and distance transform hakim elahi and jafarpour 2017 while these methods can produce discrete solutions they do not have a mechanism to preserve complex mps patterns in the training image recently we developed a discrete regularization approach for reconstruction of facies models khaninezhad and jafarpour 2017 in that approach discreteness is promoted to ensure that the solutions obtained based on continuous parameterization retain the categorical nature of facies while discrete regularization methods batenburg and sijbers 2011 khaninezhad and jafarpour 2017 lukić 2011 schüle et al 2005 prove to be effective for encouraging categorical solutions when complex multiple point statistical patterns are involved the use of discrete regularization does not guarantee accurate reconstruction of higher order spatial patters in such cases one should resort to a pattern based model calibration that not only provides discrete solutions but also is able to preserve the expected mps patterns in the solution furthermore promoting solution discreteness is only appropriate when within facies variability is insignificant and can be ignored a more flexible approach is to ensure that the exact form and statistics of spatial patterns can be incorporated in the solution regardless of whether such patterns are discrete or continuous in this paper we introduce a machine learning algorithm to incorporate complex facies connectivity patterns in calibration of subsurface flow models this is achieved by splitting the model calibration problem into two sub problems that are solved iteratively in the first step of our approach a parameterized approximation of the solution is obtained by solving a regularized least squares inversion the parameterization is used to approximately capture the expected connectivity of the geologic patterns while remaining close to the current feasible solution from previous iteration in the second step the parameterized solution from the first step is mapped onto the geologic feasible using a machine learning approach the learning process uses the k nearest neighbor k nn algorithm larose 2005 to construct local pattern feature vectors and compare them with the feature vectors in the training dataset in this step a spatial template is used to scan the continuous solution and for each template the k nn algorithm is used to identify the k most similar feature vectors in the learning dataset the corresponding label vectors i e multivariate feasible patterns are selected and stored in memory once all local patterns are scanned and processed using a defined template size an aggregation step is applied on the overlapping templates to collectively incorporate the mps patterns in assigning final values to each grid block the mapping step ensures that the solution inherits the same statistics as in the prior models the details of the proposed framework along with numerical simulation and additional discussion are presented in the remainder of the paper 2 methodology 2 1 notation we denote the n dimensional vector of model parameters as u r n where u n 1 represents a set of 2d or 3d images of spatial formation property distributions e g permeability or hydraulic conductivity the observations are demonstrated by the m dimensional vector d obs r m where a nonlinear mapping i e g relates them to model parameters i e g u d obs in presence of observation noise this relationship is stated as d obs g u n where n r m stands for the noise vector and is typically assumed to be gaussian and independent of the model parameters u a linear expansion approximation for parameters u in a low dimensional basis s n i e φ ϕ 1 ϕ 2 ϕ s is expressed as 1 u u i 1 s ϕ i v i φ v where v v 1 v 2 vs are the expansion coefficients that represent the weights given to each basis function in approximating u the q norm of a vector e g u u 1 u 2 un t is denoted by u q q 0 where 2 u q i 1 n u i q 1 q in which the operator returns the absolute value of its argument we denote the set of feasible models all valid samples corresponding to a prior geological scenario e g a given training image by ω p when closed form probabilistic models are available to describe the distribution of formation properties they can be used to completely characterize the feasible set ω p however except for very special cases realistic scenarios are far more complex and do not lend themselves to descriptions with closed form models in that case the feasible set can be defined either by a conceptual model that represents the expected spatial statistics e g a training image or by a very large set of model realizations that approximately represent the statistical patterns in such model the reference model in fig 1 shows a meandering fluvial channel a and a proposed training image b to represent the conceptual model with similar patterns along with sample realizations drawn from the training image c fig 1 d represents similar samples to those in fig 1 c except that within facies variability is accounted for through multi gaussian distributions in this paper the feasible set ω p refers to a large set of model realizations similar to those in fig 1 c or d that represent the expected spatial patterns in the solution in practice only a finite set of representative samples are available to represent ω p and its associated statistical patterns however it is important to note that the solution is not assumed to be one of the samples in ω p in our implementation a small template is used to scan the samples in ω p to generate a large set of local spatial patterns that are used in the mapping step of the proposed model calibration 2 2 problem statement and formulation a simple model calibration problem statement can be presented as a constrained weighted least squares formulation 3 min u j u 1 2 c n 1 2 d obs g u 2 2 1 2 d obs g u t c n 1 d obs g u s t u ω p where u ω p restricts the least squares solution to the predefined feasible set here d obs denotes the observed data and c n is typically a diagonal weight matrix covariance of the noise vector n that accounts for the effect of data noise in this paper we assign larger noise variance for the data type s with larger expected magnitude the regularization term can also be expressed using an indicator function of the form 4 i u ω p 0 i f u ω p i f u ω p which can be used to rewrite eq 3 as 5 min u j u 1 2 c n 1 2 d obs g u 2 2 s t i u ω p 0 for special cases such as a multi gaussian distribution of parameters it may be possible to express the constraint i u ω p 0 analytically for example to honor the variogram or covariance function of the underlying distribution however in more complex problems where an analytical expression cannot be used to describe the constraint i u ω p 0 it is not trivial to express this constraint or enforce it during model calibration furthermore exhaustive search of the feasible set to find the minimum of the objective function is computationally impractical in this work we develop an indirect method for solving the constrained minimization problem by defining a two step alternating directions method the first step uses a standard gradient based method to find an approximate parameterized solution to the problem the second step maps the parameterized solution onto the feasible set while ensuring that the updated solution remains close to the parameterized solution from the first step these two steps are repeated until no further improvement in the objective function is obtained in this paper we use the k nearest neighbor algorithm to implement the mapping in the second step section 2 3 to develop the two step solution approach we first expand the parameters into a set of feasible values u and the corresponding parameterized approximation using the linear expansion in eq 1 i e u φ v since the transformation matrix φ is known we choose the expansion coefficients v to represent u hence we can minimize the following objective function to find u and v 6 min v u j v u 1 2 c n 1 2 d obs g φ v 2 2 s t φ v u 2 2 ϵ 2 i u ω p 0 the constraint φ v u 2 2 ϵ 2 is used to ensure that the solution in the linear expansion space does not deviate significantly from the feasible solution in addition g u is replaced by g φv in the data mismatch term to i reduce problem dimensionality and ii separate g u and i u ω p in the alternating steps eqs 8 and 9 an alternative form of the objective function in eq 6 can be written as 7 min v u j v u 1 2 c n 1 2 d obs g φ v 2 2 i u ω p λ 2 2 φ v u 2 2 here λ2 is a regularization parameter that controls the linear expansion approximation error for linear inverse problems i e when g is a linear operator cross validation golub et al 1979 and l curve hansen 1992 methods are developed to properly set the value of λ however these methods do not extend to nonlinear problems for nonlinear problems practical approaches such as trial and error and sensitivity analysis are commonly used in some cases the solution may not show sensitivity to λ within wide range and a simple sensitivity analysis can be used to identify this range khaninezhad et al 2012 to minimize the objective function in eq 7 we use the following two step alternating directions algorithm where the first step updates the parameterized solution through v while the second step maps this solution onto the closest feasible solution u in ω p 8 i v k argmi n v 1 2 c n 1 2 d obs g φ v 2 2 i u k 1 ω p λ 2 2 φ v u k 1 2 2 ii u k argmi n u 1 2 c n 1 2 d obs g φ v k 2 2 i u ω p λ 2 2 φ v k u 2 2 these steps can be further simplified by dropping the fixed terms in the objective functions to arrive at 9 i v k argmi n v 1 2 c n 1 2 d obs g φ v 2 2 λ 2 2 φ v u k 1 2 2 ii u k argmi n u i u ω p λ 2 2 φ v k u 2 2 the minimization problem in step i of eq 9 can be carried out using standard gradient based optimization techniques boyd and vandenberghe 2004 the update in step ii is implemented by mapping the parameterized solution u k φ v k onto the closest feasible model using the proposed method is section 2 3 simple convergence criteria such as a threshold on u k u k 1 2 2 and v k v k 1 2 2 or the objective function value can be used to stop the iterations in eq 9 in the two step alternative direction approach of eq 9 step i updates v by minimizing the data mismatch while ensuring that the approximation error λ 2 2 φ v u k 1 2 2 i e the difference between parametrized and feasible solutions remains small this regularization term defines a closed neighborhood surrounding u k 1 which is the estimation result obtained from the feasible set in the most recent iteration consequently the updated parameter in step i i e u k φ v k remains close to u k 1 while searching for a new solution to improve the data match step ii uses the new parameterized solution from step i i e v k to update u via mapping onto the feasible set since φv k is forced to remain close to u k 1 the new mapping result u k is updated gradually in this way the regularization term λ 2 2 φ v u 2 2 ensures a gradual update and alleviates problem ill posedness 2 3 enforcing feasibility constraint through pattern learning to enforce the feasibility constraint in the second step of the alternating direction algorithm we use ideas from machine learning literature bishop 2006 kotsiantis et al 2007 machine learning techniques are generally classified into supervised unsupervised and semi supervised or reinforcement learning sutton and barto 1998 in supervised learning a set of input output i e feature label samples are observed or constructed and incorporated to learn an approximation of the mapping function between the inputs and outputs in unsupervised learning no output samples labels are available and the input samples are utilized to learn the hidden patterns in the data classification and clustering jain and dubes 1988 are famous examples of supervised and unsupervised learning problems respectively in semi supervised learning a small portion of the samples are labeled and the primary goal is similar to that of unsupervised learning chapelle et al 2009 reinforcement learning sutton and barto 1998 couples the system s functionality with the dynamic environment to reach to a certain goal which is usually carried out by maximizing a reward function in this paper we use a classification algorithm as a supervised learning approach to implement the mapping in step ii of the solution approach several algorithms such as k nearest neighbor k nn linear discriminant analysis support vector machines decision trees logistic regression and neural networks bishop 2006 are proposed for classification problems each with specific applications the k nearest neighbor k nn larose 2005 which is implemented in this paper is a simple classification technique where the decision about the label of a feature vector is made by investigating the behavior of the most similar feature vectors in the learning dataset using an appropriate distance measure the algorithm selects the k most similar feature vectors from a training dataset and assigns the most frequent label corresponding to these feature vectors as the output label mathematically given a feature vector x and a learning dataset d x i y i i 1 n the classifier searches over x 1 x 2 x n and identifies k feature vectors with smallest distance from x the method then assigns the corresponding k output vectors from the training set i e y 1 y 2 y n to the feature vector x depending on the specifics of the problem the output labels are used to extract the decision criteria typically in supervised learning problems y i takes a single discrete value that represents the label of a specific class however in our formulation the label vectors are high dimensional and carry local feasible connectivity patterns discussed next despite its simplicity the k nn algorithm enjoys strong theoretical background as a classification technique ripley 2007 new we present the implementation details of the k nn algorithm that we use in this paper fig 2 depicts a simple schematic of mapping a sample parameterized solution u k φ v k onto ω p for this example we have used the basis φ to be the truncated pca basis which consists of the eigenvectors of the sample covariance matrix associated with feasible set as it is demonstrated in fig 2 the mapping uses the parametrized solution as input and provides a corresponding solution from the feasible set we denote r ω p as the function that maps a solution outside the feasible set onto the set ω p additionally a set of initial learning samples i e r i u i i 1 n are provided where u i is the mapping of r i onto the set ω p the goal is to use d r i u i i 1 n to learn the mapping operator which can be used to find the feasible solution u based on the parameterized approximation u fig 3 demonstrates the schematic of a supervised learning approach used for the mapping in our application r i s can be calculated as r i φ φ t u i if the basis functions are orthogonal e g pca this way of generating the learning dataset ensures that the input learning samples r i s and the parametrized solution during inversion i e φ v k are defined using the same basis fig 4 depicts how the parameterized learning samples are constructed to correspond to the same level and type of parameterization as in step i of the model calibration approach a major criterion in learning the mapping from parameterized input samples i e r i s to their corresponding feasible output samples i e u i s is to preserve the statistical consistency of the patterns a simple mapping approach is to search the feasible set to find the closest solution to the full parameterized map however this strategy is unlikely to work in automatic model calibration for two reasons first arbitrary updates are applied to the solution at each iteration of model calibration which can result in significant deviation from the global connectivity patterns in the training models second it requires an extremely large set of training models to cover all possible global pattern configurations which is neither necessary nor practical a more flexible alternative is to use the training data to extract statistical information local patterns and use those patterns to construct a feasible model that corresponds to the parameterized solution a similar approach is used in multiple point geostatistical simulation techniques such as snesim strebelle 2002 and simpat arpat and caers 2007 we implement the feasibility mapping in two steps in the first stage using a specified template size we scan the given parameterized image φ v k i e current iterate and compute the corresponding feature vectors local patterns inside the defined template for each instance of the scanning template we use the k nn classification algorithm to identify the k closets feature vectors in the parameterized dataset which are generated from r i i 1 n the corresponding k label vectors in the dataset are then stored not pasted for the cells covered by the scanned area in the second step an aggregation approach is used to combine all the stored feasible instances in each model cell to approximately represent the conditional distribution for the cells each cell is then assigned the mode value with highest frequency of the conditional probability density the resulting map constitutes the feasible solution obtained in step ii of eq 9 and is passed to step i to continue the iterations we note that the feature vectors do not need to be the spatial descriptions of the patterns and could be defined based on various factors including computation for instance one class of feature vectors can be the projected coefficients of the patterns onto a user defined low dimensional subspace e g leading pca elements of each template the feature vector could also be defined by considering a subset of grid cell within the template either randomly or deterministically for computational gain regardless of how the feature vectors are defined the corresponding label vectors are local templates that are generated from the feasible set fig 5 left demonstrates the learning stage where a circular template is used to scan the prior image realizations i e pairs of r i u i to generate the corresponding segments of the parameterized and feasible input images in this case the feature vectors denoted as x i are defined as the exact grid block values that are located inside the templates the corresponding feasible labels are denoted as y i as shown in fig 5 in this paper we adopt as feature vectors the exact grid block values inside the scanning templates the final dataset of features and labels d x i y i i 1 n is then used to implement the mapping in step ii of eq 9 fig 6 demonstrates the schematic of the first stage of the segmentation procedure the k nn classifier explores the feature space in the training dataset to find the k feature vectors with smallest distance to each scanned portion of φ v k and stores the corresponding feasible label vectors for the scanned regions as shown in fig 6 the same template that is used to construct the learning dataset is applied to extract the feature vectors in φ v k we note that the first stage results in significant overlap between the scanned regions which provides many samples for each cell in the model after the initial classification and storing of the label vectors an aggregation or voting step is used to approximate the conditional probability of the labels for each cell based on all the assigned values to each cell the resulting distribution is then used to decide about the feasible values for each grid block fig 7 depicts the aggregation approach where for any given cell marked with a dot the labels from scanning different regions are combined to approximate the corresponding conditional probabilities in this paper we assign the model of this distribution to each grid block to construct the feasible solution we note that for non discrete feasible values e g channels with within facies heterogeneity a histogram of the values associated with each cell is constructed and the value with the highest frequency is adopted as the solution for each cell we note that the aggregation step ensures that facies assignment to each grid block accounts for the spatial statistics and connectivity patterns from an extended neighborhood approximately twice the size of the template in each direction moreover aggregation leads to far more samples than k for each grid cell which substantially increases the accuracy and spatial consistency of the method fig 8 illustrates the importance of aggregation with a simple example the parameterized map φ v k from fig 2 is shown on the top left along with a discrete version that is obtained by thresholding clearly thresholding does not incorporate spatial statistics and results in a poor reconstruction of the parameterized map the middle and the bottom rows of fig 8 show the results of the proposed mapping approach without and with using the aggregation step respectively the results are shown for different template sizes i e r 5 and r 25 and different k values in k nn classifier k 1 and k 5 in both cases large values of the template size and k lead to improved reconstruction in the case without aggregation the results are not as accurate and random artifacts emerge in the feasible solution aggregation improves the connectivity by exploiting additional spatial information by extending the local neighborhood furthermore when aggregation is used less sensitivity to k is observed as aggregation provides many more samples than can be obtained by increasing k many overlapping templates cover the same cell in the model learning the mapping operator for step ii of eq 9 can be computationally demanding for large template sizes during the k nn classification step the feature vector in the solution i e x is compared with the n feature vectors in d x i y i i 1 n where the comparison is performed by applying a normed measure such as l 2 norm therefore the computational cost of mapping φ v k onto ω p is proportional to the number of grid blocks the value of n and the complexity of calculating x x i for an arbitrary index i the computational complexity of calculating x x i is directly related to the type of norm used and the dimension of the feature vector x increasing the dimension of the feature vector i e size of x i s linearly increases the computational complexity in calculating x x i thus larger neighborhood templates require more demanding calculations to construct the feature vectors a template and a reasonable value for n are assigned the feature vectors are then generated by scanning the data samples in d r i u i i 1 n for practical implementation the scanning does not need to be exhaustive which would involve redundancy and can be performed by considering a random subset of regions within each sample which also reduces the memory demand the value of n i e number of feature vectors in the learning dataset linearly increases the computational complexity of the k nn classifier therefore a reasonable value for n must be considered e g 1000 a major contributor to computational complexity is the number of k nn classifications before the aggregation step one way to reduce this computation is to use a small subset of the overlapping templates fig 9 a and b depict the schematic of the uniform i e using all the grid blocks and random k nn classifications respectively the reconstructed discrete solutions for different subsampling ratios are shown in fig 9 c even using as low as 10 of the cells for classification does not deteriorate the reconstruction results significantly this is primarily because the aggregation step increases the number of samples for each grid block the computational complexity is however significantly reduced by introducing subsampling additionally the k nn classification for each cell is independent of the each other allowing for parallel implementation using multiple processors in performing the first step of mapping i e classification step an additional computational consideration is in solving the optimization problem in step i of eq 9 which requires several numerical flow simulation runs in practice finding the exact minimum of step i in eq 9 is not necessary and only a few iterations are sufficient to move to step ii of the algorithm several justifications can be brought for this approximation in general the first few updates provide the main changes in the solution moreover the solution is initialized by the solution obtained from the previous step and the term φ v u 2 2 in the objective function discourages significant departure from the solution obtained in the previous step thereby requiring modest updates furthermore the solution is bound to be updated in the next several iterations even when a computationally more expensive exact solution is obtained in our experience the first few less than 5 iterations provided the main updates to the solution in summary the computational complexity of a single mapping is proportional to number of cells in the random path for classification the size of the feature vectors and the number of features vectors in d x i y i i 1 n usually n does not change significantly by increasing the dimension of the model in addition minimization of eq 8 require several forward simulations and adjoint computational which can be computationally very significant for field scale problems an important point to discuss before presenting the results is that while other methods may be applied to perform the mapping in step ii one needs to consider the properties and effectiveness of the implementation for instance one may consider using a scaled version of the parametrized solution as facies probability map soft data to generate a conditional discrete facies realization as in pcm jafarpour and khodabakhshi 2011 however the pcm was used in ensemble based data assimilation with enkf where the updated ensemble mean after data assimilation was used as soft data to regenerate an updated ensemble i e conditional facies realizations this approach ensured that the correct statistics were inherited from the training image before implementing the next forecast and update steps however in pcm using the scaled version of the ensemble mean as soft data implies that departure from the local connectivity patterns in the probability map is allowed this is acceptable in pcm because the ensemble realizations did not follow the same exact connectivity that was in the ensemble mean as a result in the pcm only the mean of the generated ensemble is consistent with the facies probability map due to the stochastic nature of the snesim simulation in the current formulation the goal is to obtain a feasible map that is closest to parametrized solution that is obtained in step i therefore the method should not allow departure from the continuous solution there are also other important differences between these methods the method in the current paper is based on pattern learning and can be applied to continuous and discrete patterns whereas pcm was developed for discrete facies additionally the proposed method does not rely on the snesim or any other algorithm for facies simulation the use of a prior training set allows the method to be applicable to training datasets that are obtained from other geostatistical simulation techniques this property generalizes the method and makes it independent of the geostatistical method used to generate the prior training set 3 numerical results we present three numerical experiments to examine the performance of our developed approach the first example is a straight ray travel time tomographic inversion bregman et al 1989 which leads to a linear inverse problem the second experiment involves a ground water pumping test in which the facies represent the hydraulic conductivity of the field in this case steady state pressure head data are used to reconstruct the spatial distribution of lithofacies that represent the hydraulic conductivity maps franssen et al 2009 in the last experiment we consider a two phase flow problem where a 3 dimensional model of facies distribution is estimated from transient pressure and flowrate data in these examples the k nn classification step is based on a random subsampling approach by scanning only 20 of the grid blocks for the feature label data pairs i e d x i y i i 1 n we use a total of n 2000 samples 3 1 example 1 linear tomographic inversion in tomographic inversion measurements of travel arrival times of transmitted acoustic waves are used to estimate the slowness 1 velocity of the medium between the source and receiver pairs subsurface environment the configuration of the travel time tomographic inversion is shown in fig 10 a where the sources left transmit the acoustic waves to the receivers right in this experiment an array of 6 sources and 6 receivers with equal interval are used resulting in 36 arrival time measurements the governing equation that relates the observation and parameter spaces is 10 t u x d x 0 u x 1 v x where t denotes the travel time of each wave and u x is the rock slowness 1 velocity at the location specified by x coordinates a discretized version of eq 10 can be used to form a linear system of equations between the observations ray arrival times and parameters rock slowness for each cell in the discretized i e d gu case 1 example with discrete facies the reference slowness map contains a complex meandering structure as depicted in fig 10 b in this case the values of the parameter are set to log 10 mm s and log 600 mm s for outside and inside of the channel respectively the model domain has a dimension of 1000 1000 m2 which is discretized uniformly into 100 100 cells the prior training data consists of n 500 model realizations which are drawn from a training image shown in fig 11 a consisting of discrete meandering connectivity patterns these samples i e previously defined u i s in d r i u i i 1 n are used to approximately represent the patterns in the feasible set ω p we note that for small values of n e g n 50 the existing patterns in u i i 1 n may not be representative of the entire local connectivity features of the feasible set in addition a small n may result in an inaccurate pca parameterization on the other hand large values of n e g n 20 000 may overestimate the complexity of the patterns in the feasible set and result in redundant patterns and drastic computational overhead therefore a reasonable value needs to be assigned for n fig 11 b displays four sample realizations from this set the corresponding parameterized models i e r i s in r i u i i 1 n are obtained using truncated pca parameterization of u i i 1 n the corresponding pca basis functions φ of the parameterization subspace are shown in fig 11 c fig 10 c depicts the projection of the reference model on the truncated pca basis for inversion the initial map which is the mean of the samples in ω p is shown in fig 10 d fig 12 a shows 10 iterations of the inversion results i e φv top and u bottom the feasibility mapping step uses a 50 50 square template and the value of k in the k nn algorithm is set to 5 we initially start with 20 leading pca bases functions for parameterization and gradually increase the dimension of parametrization to 50 throughout 10 consequent iterations the objective is to initially capture the large scale connectivity using fewer global connectivity patterns and gradually increase the parameter size to improve the resolution of the reconstructed model as shown in fig 12 a the inversion process can detect the meandering connectivity structures with good accuracy within the first 10 iterations during the initial iterations the algorithm primarily identifies the global connectivity structures while at later iterations it enhances the identified global pattern with small scale adjustments case 2 including within facies variability in this case the reference map which is demonstrated in fig 13 a consists of non discrete heterogeneity inside and outside the meandering channel structure the heterogeneity within the fluvial channels is modeled independently of the heterogeneity within the background facies the heterogeneity inside and outside the channel in the reference map is modeled by gaussian processes using two different variogram models the direction of maximum continuity for the heterogenicity i e the azimuth is θ 45 fig 13 d depicts the histogram of the parameter values in the reference model which indicates a bimodal distribution we note that to the contrast in the facies values the variability within channel facies is not as visible as that in the background facies fig 13 e shows four out of n 500 of model realizations that were used as training data to represent the feasible set these prior models were also used to construct the pca basis φ to form the parameterization subspace in this case we use 100 basis functions to represent the solution in the parameterization space however we start the inversion with 20 basis functions and uniformly increase the number of basis functions to 100 throughout 15 iterations until convergence the best achievable solution and the initial map are shown in fig 13 b and c respectively all other parameters are the same as case 1 the results of parameter reconstruction for φv and u are summarized in fig 14 showing the evolution of the parameterized and feasible solutions throughout the iterations the results show that the method can reconstruct models that include facies and within facies viability interestingly the algorithm can detect the channel and non channel facies and identify the general trend in the background heterogeneity we note also that if only the continuous solution is used to reconstruct the model the resulting solution provides a lumped image making it hard to separate the facies distribution from the background heterogeneity whereas the feasibility mapping step identifies the heterogeneity in addition to the facies configuration 3 2 example 2 2d ground water pumping test in the second example we consider integration of data from a groundwater pumping text fig 15 a shows the schematic of the test which includes a pumping well in the middle of a 1500 1500 20 m3 domain which is divided into 100 100 1 cells the well is extracting water with a rate of 60 m 3 h the top and bottom boundaries of the model have no flow boundary conditions while the boundary conditions on the left and the right sides of the domain are constant pressures of 20m and 10m respectively a uniformly placed network of 25 monitoring wells is used to measure the hydraulic conductivity values and the steady state pressure heads the governing equations for the pumping test in this example involve single phase flow equations in a saturated porous medium which are derived by combining mass conservation and darcy s law and can be expressed as 11 ϕ ρ t ρ v q conservation of mass v 1 μ k p ρ g z darcy s law here ϕ ρ v p and q represent porosity density velocity pressure and pumping rates respectively μ represents the fluid viscosity in addition k denote the intrinsic formation permeability which we assume is the same in x y and z directions the hydraulic conductivity defined as k h k ρ g μ as well as the observed pressure head values at the monitoring well are the observations d the solution of eq 11 provide the operator g that maps the parameter space to data space i e d g u k h case 1 example with two facies types for this case the reference and initial log conductivity maps are shown in fig 15 b and d respectively while their corresponding pressure distributions are depicted in fig 15 e and f respectively the logarithm of the hydraulic conductivity in the reference model fig 15 b for the two facies types are 2 3 and 0 5 log m s respectively fig 15 c shows the best parameterized approximation of the reference map based on a rank 40 pca approximation the training image in fig 16 a which consists of intersecting channel features is used to generate n 500 facies realizations to be used as prior training data i e u i s in r i u i i 1 n fig 16 b shows four samples from the resulting prior training data the prior samples are used to develop the pca basis functions φ fig 16 c for low rank parameterization of the facies models the model calibration process starts by including 20 leading pca basis functions and gradually increasing the number of pca basis function to 40 by adding 2 basis elements per iteration to examine the significance of mapping the parameterized solution onto the correct feasible set we compare the results from our k nn machine learning approach with an alternative method in which thresholding of the continuous solutions φ v k is used in step ii of eq 9 to obtain discrete facies distributions figs 17 and 18 summarize the solution iterations for our method and the simple thresholding approach respectively in this example for implementing the k nn method the value of k is set to 3 and the neighborhood template consists of a circle with radius 25 cells comparison between figs 17 and 18 shows that mapping onto the feasible set provides superior results to a simple thresholding the main difference between the two method stems from the absence of correct spatial statistics patterns in the grid based thresholding approach from fig 17 it is evident that mapping the continuous solutions onto the correct feasible set provides patterns and spatial statistics that are consistent with the prior model whereas a discrete thresholding does not improve the spatial connectivity and only replaces continuous values with their closest discrete values it is particularly noteworthy that mapping onto the feasible set tends to correct errors in channel connectivity and eliminate inconsistent discontinuities case 2 example with three facies types in this example the reference log hydraulic conductivity map depicted in fig 19 a consists of 3 discrete facies types with values 2 3 blue 0 7 green and 0 6 red log m s the reference pressure head map as well as the location of 25 monitoring and pumping wells are shown in fig 19 b the monitoring wells are located uniformly within the domain and the steady state pressure and hydraulic conductivity values measured at the location of these wells are used as data for inversion for this example 1000 prior model realizations are generated from a training image with 3 facies values four sample realizations from this set are depicted in fig 19 c we apply the pca with the prior model realizations to generate the parameterization basis functions 49 leading pca basis images are demonstrated in fig 19 d the inversion is started with 25 basis functions and after each iteration 3 additional pca basis elements are added to improve the solution all other parameters are the same as case 1 fig 20 depicts the solutions at iterations 0 start 1 2 7 and 10 as can be seen the general connectivity pattern in the reference model is captured correctly at the boundary of the facies the solution does not exhibit the irregularity that is observed in the reference model the smoothness at the boundaries is primarily due to that fact in the mapping step of the algorithm the mode of the local conditional distribution of facies is assigned to each cell 3 3 example 3 two phase flow in 3d formation for our last example we consider two phase flow in a 3d formation to integrate transient and highly nonlinear observations of pressure and flow rate the model configuration and the reference map for this example are shown in fig 21 a and b the model consists of a 1000 1000 160 m3 domain which is divided into a uniform grid system with 100 100 8 cells of size 100 100 20 m3 the reference permeability map with 10md and 600md values for low and high permeability regions includes two distinct spatial patterns each extending four vertical layers of the model fig 21 b bottom there are four fluid injection and five fluid extraction wells in the domain constant injection rate and extraction pressure are used to control the wells while the pressure at the injection wells and phase fluid flow rates at the extraction wells are observed with one month interval and used for model calibration in the first 4 years model calibration stage 0 8 pore volume of the wetting phase e g water is uniformly injected into the formation resulting in 48 observation time intervals another 0 8 pore volume is injection during the next four years the forecast period the main uncertainty in this example is related to the spatial distribution of rock facies types which is constrained by assimilating the observed dynamic responses at the well locations fig 21 c and d shows the dynamic evolution of the non wetting phase i e hydrocarbon saturation and fluid pressure in the formation in this example a two phase incompressible and immiscible fluid flow system is considered for which the governing pde equations can be expressed as 12 λ w b w k p w γ w z t ϕ s w b w q w λ n b n k p n γ n z t ϕ s n b n q n in these equations phase mobility phase density formation volume factor intrinsic formation permeability formation porosity gravity potential phase saturation and flux are represented by λ γ b k ϕ z s and q respectively and w and n represent the wetting and non wetting phases the governing equations in eq 12 involve four unknown dynamic state variables p n s n p w and s w two additional equations are needed to close the pde system these two equations are the constitutive equations that relate phase pressures and saturations and are expressed as 13 p n p w p c s w s w s n 1 0 s w s n 1 the first equation describes the capillary pressure difference between non wetting and wetting phase pressures as a function of the wetting phase saturation while the second equation imposes a physical constraint on the saturation of two phases in a fully saturated medium the capillary pressure is assumed to be zero in this example i e p n p w 0 the spatial distribution of permeability k in eq 12 is the unknown parameter of interest and is estimated from the observed data in this example we assume no flow condition on the boundaries of the domain for model calibration n 1000 prior realizations of facies are generated using the same training image in fig 16 a the spatial patterns used for the two zones layers 1 4 and layers 5 8 are assumed to belong to the same training image fig 22 a shows the four samples from the prior training data the leading 12 out of 50 pca basis function for parameterization are shown in fig 23 b while the best rank 50 pca approximations for the two zones are depicted in fig 22 c fig 23 summarizes the inversion results which are shown separately for layers 1 4 and 5 8 for the first step of the algorithm i e continuous solution the pca parameterization is initialized using 30 leading basis functions and two new basis functions are added in the 10 subsequent iterations to arrive at a rank 50 approximation for the second stage of the inversion mapping the continuous solution φ v k onto the feasible set layers 1 4 and 5 8 are projected onto ω p independently the neighborhood template for the k nn algorithm has a size of 40 40 and k is 3 the projection results for layers 1 4 and 5 8 are depicted in the second and fourth columns of fig 23 4 conclusion an important aspect of applying inverse modeling to physical systems is imposing feasibility constraint in subsurface flow model calibration ensuring the model calibration solutions are geologically plausible can be a difficult constraint to enforce when the expected connectivity patterns are complex and hard to represent much less preserve during model updating for instance updating meandering fluvial channels to match the observed data while preserving their shape and connectivity pattern is very challenging and difficult to achieve using classical model calibration techniques in this paper we employed machine learning techniques to develop a framework for automatic calibration of complex facies models while maintaining their complex connectivity patterns specifically we defined a feasible set for model parameters that must be observed during model calibration using examples from fluvial systems we described the feasible set to with many prior model realizations that summarize the expected spatial statistics of the solution to implement the feasibility constraint we formulated a regularized least square formulation in which the regularization imposes a complex feasibility constraint using alternative directions method of optimization we split the objective function into two sequential optimization sub problems where in the first problem a parameterized model calibration is solved to use dynamic flow data to infer the approximate connectivity patterns without honoring the feasibility constraint in the second step this solution is mapped onto the feasible set by employing the k nn algorithm as a supervised machine learning technique the k nn algorithm is used to learn the mapping operator using multi dimensional feature and label vectors initially the realizations of the feasible set i e discrete fluvial channel maps are projected onto a pca parameterization space to obtain the corresponding continuous maps a local neighborhood template is then used to scan and store the discrete and corresponding parameterized maps in the training data this obtained learning dataset is then used during the second stage of the inversion solution to implementing the mapping onto the feasible set given the parameterized solution from the first step of model calibration the k nn algorithm is used to scan the continuous solution with a predefined template size to identify the k best representative features in the training dataset the corresponding feasible vectors for those k maps are stored as instances or samples belonging to the scanned locations after scanning is completed the stored maps are processed using an aggregation method where all the relevant samples for a given cell including those obtained by overlapping templates are used to characterize the conditional probability for every cell in the domain using the resulting conditional probability a maximum likelihood approach is used to assign final feasible values to each cell of the model the aggregation step is very important in increasing the consistency of the results and in reducing the computational complexity of the method using a series of increasingly complex numerical experiments we examined the performance of the proposed approach to highlight the importance of honoring complex prior models as feasibility constraint and demonstrating the performance of the machine learning approach in implementing this constraint several aspects of the introduced method can be extended for instance instead of a supervised learning which was the core of the introduced method in this paper other supervised and unsupervised approaches with different properties may also be applicable we used a simple definition of the feature vector which was the group of cells that were covered by the template or a subset of those cells to reveal the underlying spatial pattern however a more sophisticated and efficient description of the feature vector may be considered for example by including kernel functions furthermore an important assumption that is made in this work is that the training data is reliable in practice the prior model that is available for training may be subject to uncertainty for instance multiple geologic scenarios may be considered plausible for a given formation therefore an extension of the current formulation is application to problems in which several geologic scenarios and hence feasible sets larose 2005 in addition to uncertainty in the feasible set an important aspect of model calibration is uncertainty quantification in this paper we used the proposed approach within a deterministic inverse modeling context ignoring various sources of uncertainty to quantify the uncertainty in the solution and future prediction a bayesian inversion formulation may be developed however theoretically rigorous probabilistic treatment of model calibration problems with complex connectivity patterns is not trivial as the underlying spatial patterns are difficult to represent using simple probabilistic models e g gaussian it is important to note that the heterogeneity in subsurface formation can take different forms and one must verify whether the assumptions used in adopting a method are applicable to any specific site for instance if the dominant heterogeneity in the field exhibit smooth transition in crossing extreme values estimating facies architecture may not be important or relevant i e one can simply estimate rock property distributions as continuous random field as in traditional approaches in other cases the facies distribution and boundaries may be so dominant in describing the flow and transport behavior that one could ignore the role of within facies variability and perform facies calibration as the observed data may not exhibit sensitivity to within facies changes there are also cases in which one must include both facies distribution and within facies variability to adequately represent the correct subsurface heterogeneity during model calibration another important issue is the resolution and information content of data which may allow for estimation of facies distribution but may not support the inference of within facies variability in conclusion machine learning techniques can offer effective tools for solving difficult model calibration problems and lead to novel data oriented formulation for solving inverse problems in particular for calibration of flow models with complex spatial connectivity patterns where traditional methods may not have the sophistication required to represent and preserve such patterns in the solution learning techniques may provide a viable and powerful alternative to perform the task for large scale problems the learning process may involve considerable computational overhead however in many cases the learning step can be performed off line and may be conveniently parallelized making it a feasible option for practical applications acknowledgement the work in this paper is supported in part by energi simulation all data and digital content in this manuscript can be accessed by sending an email to behnam jafarpour usc edu 
764,this study developed a computational model for large wood deposition patterns in shallow flows considering the effect of a root wad based on laboratory experiments we used the nays2dh depth averaged two dimensional model of iric to simulate shallow flows a newly developed large wood simulation model was combined with the shallow flow model the laboratory tests were performed by changing several hydraulic parameters in shallow water with a depth similar to the diameter of large wood the root wad decreased the draft for wood motion the depth at which large wood contacts the river bed by lifting the head of large wood the experimental results showed that the large wood tends to move toward the side walls and deposit on the bed after passing an obstacle computational results reasonably showed that the proposed coupling model reproduced the fundamental and physical aspects of the phenomena 1 introduction in rivers large wood is transported along with water and sediment deposited large wood affects river morphology by causing local scour and deposition of bed materials such large wood may initiate the formation of an island as a midchannel bar keller and swanson 1979 nakamura and swanson 1993 abbe and montgomery 1996 gurnell and petts 2002 swanson 2003 van der nat et al 2003 brooks et al 2006 sawyer and cardenas 2012 large wood and deposition patterns can change in response to input processes channel morphology and hydrological parameters including flood events montgomery and piégay 2003 previous research showed that the relative influence of these factors changes along the river system keller and swanson 1979 gurnell et al 2000 2002 collins et al 2012 gurnell 2013 resulting in distinct downstream trends in large wood accumulation style keller and swanson 1979 gurnell et al 1995 marcus et al 2002 comiti et al 2006 the ratio of wood length to channel width is a key parameter that affects large wood deposition patterns braudrick et al 1997 in addition the patterns are influenced by diverse parameters such as drag force water level bed friction and obstacles numerous researchers have studied large wood dynamics braudrick et al 1997 braudrick and grant 2000 2001 provided the basic framework to approach large wood mobility and entrainment in rivers based on these frameworks several researchers investigated large wood transport dynamics e g gurnell et al 2002 haga et al 2002 bocchiola et al 2002 2006 welber et al 2013 bertoldi et al 2014 these studies successfully predicted a wood motion relationship between hydrodynamic and resistance forces and a few of them examined transport systems in studying large wood transport the large wood shape and density for motion are considered first comiti et al 2006 gurnell and sweet 1998 a piece of large wood may consist of a stem branch and or root wad among these the stem and root wad are the major components and they actively affect the motion of large wood large wood deposition is particularly sensitive to the characteristics of the root wad because this part increases the friction with the bed in addition to experiments and observations computational models for large wood dynamics have been developed such as the iber wood two dimensional 2d hydrodynamic model ruiz villanueva et al 2014a b ruiz villanueva et al 2016 and the nayscube kimura 2012 three dimensional 3d reynolds averaged navier stokes model kimura and kitazono 2017 kitazono et al 2016 both models address large wood dynamics following a lagrangian method coupling water flow with wood transport the iber wood model considers the wood shape to be a simple cylinder in contrast the nayscube large wood model applies a particle method to consider the impinging motion of large wood using a discrete element method these studies showed the mechanism of large wood dynamics in deep water flows where water depth is larger than wood diameter to demonstrate floating motion only and they accurately reproduced large wood motion however the analysis of large wood dynamics through the integration of river bed and large wood such as the root wad effect and anisotropic bed friction is neglected in these models even though large wood dynamics show floating motion sliding motion and deposition in a natural river system clarifying such phenomena by coupling motions floating sliding and settling motions is crucial in understanding large wood behavior in rivers and the role of large wood floating advection thus it is required to study sliding motion and deposition with the root wad effect and anisotropic bed friction as the key parameters of large wood dynamics for improving computational methods the main aim of this work is to develop a numerical method for simulating the transport of large wood along with hydrodynamics in shallow flows considering the root effect under various hydrologic parameters flow discharge and channel slope this study presents a newly developed numerical model that can simulate various behaviors of large wood we use a 2d depth averaged flow model nays2dh shimizu et al 2014 of international river interface cooperative iric 2017 which is an eulerian model for calculating water flow and bed morphology a lagrange type large wood model is newly developed and combined with nays2dh the applicability of the present model is discussed through a comparison with experimental results 2 experimental setup 2 1 scale of the experiment among several studies related to large wood and river morphology gurnell et al 2002 first presented the classification of river scales small medium and large kramer and wohl 2017 recently systematically categorized four river scales in relation to large wood table 1 we conducted laboratory tests on a medium scale river which was arbitrarily selected for a simplified experiment to examine the reproducibility of the large wood model as obstacle was used to clearly describe the process of wood deposition with change in channel width in addition we could observe the transition of large wood motions such as floating sliding settling and re moving in shallow flows the root scale diameter approximately 0 01 m was taken from bertoldi et al 2014 2 2 experimental model for large wood and flume the flume channel measured 0 3 m 2 m fig 1 a and the flume bed was created using extremely smooth wood panels fig 1 d obstacle size was 10 cm 10 cm 10 cm and two obstacles were attached to the side wall fig 1 a the large wood models were created using wood cylinders with diameters of 1 cm and lengths of 10 cm the density of the large wood models was 650 kg m3 in wet conditions i e when floating on water the root model was developed by attaching two thin wood cylinders 0 18 cm in diameter and 2 cm in length as a cross shape at 90 to the end of the large wood model welber et al 2013 bertoldi et al 2014 the shape of the stem was cylindrical hence the resistance to the motion in the lateral direction was considerably smaller than that to the motion in the stem wise direction because of rolling motion hence small pieces of wood 0 1 cm 0 2 cm were attached at the middle of the stem to increase the resistance to rolling motion fig 1 b table 2 shows the experimental cases this study considered large wood deposition in relation to channel slope flow discharge and the root effect in the experiments we used two different flume slopes i e 0 0045 and 0 007 m m to consider the change in flow velocity under the same flow discharge the following two cases of discharge were selected based on the minimal discharge smaller case for the motion of the wood piece with a root wad and approximately twice the minimal discharge larger discharge a smaller case 0 00065 m3 s and 0 00060 m3 s and a larger case 0 0010 m3 s and 0 0011 m3 s in addition the root effect was examined for two cases i e with and without roots therefore eight cases were investigated in the experiments and computations each case was repeated thrice to verify reproducibility then a typical case was selected wood pieces were supplied one by one at intervals of 4 s after the flow reached an equilibrium state no wave with the back water effect ten wood pieces were supplied in each case the initial input direction of the wood pieces was random the range of the initial positions of the wood pieces was also random but limited within a circle with a radius of 5 cm starting zone fig 1 a because the initial angle and position were difficult to control these settings were identically applied to the computational conditions 3 simulation method we used a 2d plane numerical solver developed based on nays2dh shimizu et al 2014 of international river interface cooperative iric 2017 to simulate the flows a third order total variation diminishing monotonic upstream centered scheme for conservation laws method was used for the advection term and a zero equation model was used for the turbulence model the coupled model was a two way model that considered the drag force exerted by the large wood on the flow we estimated the manning roughness coefficient as 0 006 because the bottom of the flume was flat and smooth grid size was set as 1cm 1cm the number of grids was 200 and 30 in the x direction and y direction respectively to resolve the flows around the large wood with a diameter of 1 cm the initial time step δt was 0 0035 s and the total simulation time was 70 s large wood pieces were supplied one by one every 4 s after the flow reached a stable condition no back water effect close to the obstacle in the same manner in which 10 pieces were supplied in the experimental cases we reduced the simulation time using the parallelization method openmp 3 1 particle method for modeling large wood dynamics 3 1 1 governing equation for the large wood motion the large wood dynamics equation kimura 2012 kimura and kitazono 2017 kitazono et al 2016 in cartesian coordinates is described as follows 1 σ a 3 d 3 d u p d t f d f w a f a m f b e d f d f w a f a m f b e d 0 f d f w a f a m f b e d where 2 f d 1 2 c d ρ λ a s u b a 2 d 2 u u p u u p 3 f wa ρ λ v s u b a 3 d 3 d u d t 4 f am c m ρ λ v s u b a 3 d 3 d u d t d u p d t 5 f bed 0 h h c g μ a 3 d 3 σ λ v s u b ρ u p u p h h c the bed friction coefficient μ in eq 5 which is an anisotropic friction coefficient is changed by the stemwise angle of the wood piece fig 2 this parameter is expressed by eqs 6 and 7 6 μ x n μ s o r k cos 2 θ sin 2 θ 7 μ y n μ s o r k sin 2 θ cos 2 θ where ρ is the water density σ is the density of large wood cm is an additional mass coefficient cm 0 5 was used in this work cd is the drag coefficient a 2 is the 2d shape coefficient π 4 in this work a 3 is the 3d shape coefficient π 6 in this work d is the diameter of the particle sphere u is the fluid velocity around the sphere up is the velocity of the sphere ν is the kinematic viscosity coefficient t is time g is gravitational acceleration μ x and μ y are the x and y components of the friction coefficient respectively θ is the stemwise angle θ 0 indicates the angle matching the x axis direction n is the number of particles for one large wood piece μ s or k is the sliding friction coefficient s static k kinematic fbed is the bed friction force λ a sub is the submerged area rate λ v sub is the submerged volume rate fd is the drag force fwa is the effect of the acceleration of surrounding water fam is the effect of the added mass of water hc is the critical draft for wood motion when calculating sliding friction eqs 6 and 7 the sliding friction coefficient μ s or k should be multiplied by the total number of spherical particles n in the large wood because the sliding frictions of all particles are in the same direction and move as one material the following drag force terms caused by the large wood are added for the eulerian momentum equation for water flows 8 f d r 1 2 ρ c d 1 h a c e l l n 1 n c e l l a n u i u p i u i u p i where an is the projected area of a particle toward the flow direction acell is the area of a grid cell and i is the number of particles in the large wood piece the present study neglects the effect of collisions among large wood pieces to focus on the large wood advection and deposition patterns therefore each large wood dynamic is independent however this numerical analysis includes motion that is indirectly changed by the water alterations due to adjacent large wood because such a large wood piece is modeled as causing drag force and changing water depth 3 1 2 anisotropic friction coefficient the static friction coefficient between the wood used to construct the flume bed and the large wood is generally approximately 0 4 0 6 ambrose and tripeny 2011 the friction coefficient under submerged conditions is uncertain however it may be expected to decrease therefore we conducted trials to compare the experiment and simulation and adjusted the friction coefficients accordingly table 3 the static μ s and dynamic μ k friction coefficients refer to the cases when large wood is deposited and sliding respectively these coefficients are valid in the stemwise direction of the wood piece if large wood slides in the direction perpendicular to the stemwise direction the large wood rolls and rolling friction becomes dominant table 3 shows the static kinematic and rolling friction coefficients 3 1 3 buoyancy and critical draft for wood motion one important factor that affects large wood deposition is its buoyancy this study considers the sphere by sphere transition of the submerged volume based on the change in flow depth and the presence of a root wad in addition the inclination of the settled large wood based on the root wad shape is considered to evaluate buoyancy the large wood is deposited if buoyancy is less than the weight of the large wood the large wood floats if buoyancy is larger than gravitational force if the density of the large wood is less than the water density the draft of the large wood is determined by the balance between buoyancy and weight the water depth that is the same as the draft is referred to as the critical draft for wood motion cdm in other words the cdm is the minimum depth at which the large wood does not touch the bed the cdm hc is the same as the maximum draft in the floating condition which can be obtained as follows the maximum unsubmerged volume of the large wood is calculated using eq 9 9 v e b w w d ρ w w d 4 3 π d 3 ρ d b w 4 3 π d 3 ρ w where r is the diameter of a large wood particle wd is the weight of a large wood particle under the totally submerged condition bw is the buoyancy under the totally submerged condition ve is the unsubmerged volume of a particle under the totally floating condition fig 3 we can obtain the maximum unsubmerged volume ve from eq 9 and the submerged volume vs using eq 10 10 v s r a c f y 2 π d y f y r 2 y 2 π r a c r 2 y 2 d y π r 3 y 1 3 y 3 r a c π r 2 a c 1 3 a c 3 r 3 1 3 r 3 the unsubmerged volume ve can also be calculated by eq 11 from eqs 9 and 10 11 v e v t v s v t 4 3 π r 3 where vs is the submerged volume of the large wood particle vt is the volume of the large wood particle ac is the water level from the center of the particle fig 3 we can obtain the cdm by iteratively executing these equations we assume that the diameter of a root wad particle is twice the diameter of a stem particle to model the different drag force and buoyancy of the root the drag force of a root particle is larger than that of a stem particle because of the larger projection area of the root whereas the weight of the root is large because of its increased volume at the same density 3 1 4 change in water depth by root effect when a root wad of floating large wood fig 4 a becomes deposited on the bed at a lower water depth than the cdm the root wad effect is activated thereby decreasing the draft of the wood piece fig 4 b the root wad lifts one edge of the stem up from the river bed thus the submerged volume of the large wood decreases then the influence of buoyancy is decreases and the normal force to the bed increases compared to the case of a large wood piece with no root wad in the computation we first determine the cdm of each particle in the large wood piece then the change in water depth caused by the root wad effect is applied as follows if the average cdm of the large wood particles is lower than the present water depth fig 4 c 12 h s n i p h s h r l a i p i p n h r h p l d l a l a h p 2 l d 2 where hs is the present water depth under the root particle hp is half the difference between the stem diameter and root diameter ld is the stem length la is the projection length of the stem into the river bed hr is the increase in height caused by the root effect h s n i p is the calculated water depth caused by the root effect for each particle ipn is the total number of particles in the large wood ip is the index of each particle 3 2 interaction between water flow and large wood in the computation in this solver the calculation schemes for water flow and large wood are different in the present large wood dynamics model one large wood piece is expressed as connected particles which are driven following the lagrangian method therefore a method for modeling the interaction between grid based water flow and particle based large wood motion is required accordingly we use linear interpolation fig 5 to obtain the flow velocity and water depth at target points on the particles that compose the large wood if each particle is located at an arbitrary position the relative length la lb from a standard grid point f 1 to the center of the particle can be calculated to determine the inverse distance weight here all grid lengths between all grid points are 1 on the generalized curvilinear coordinate system then the value at the particle center fip is calculated with reference to the values at four surrounding grid points using eqs 13 and 14 13 f i p k 1 4 f k a k k 1 4 1 a k 14 a 1 l a l b a 2 l a 1 l b a 3 1 l a 1 l b a 4 1 l a l b where fip is the interpolated value at the particle center f 1 f 4 are the grid nodes in the ξ and η directions ξ and η are generalized curvilinear coordinates a 1 a 4 are the weighting areas considering the inverse distance from the particle fip la and lb are the relative lengths from the target point based on the standard grid point f 1 the values of la and lb are 0 if the positions of f 1 and f ip are the same k is the index for the grid point through this interpolation we can obtain the interpolated flow velocity and water depth values at each particle location from a grid node point in the generalized curvilinear coordinate system fig 6 shows the process of calculating the interaction between water flow and large wood motion the specific calculation procedure is as follows 1 water flow is calculated using the 2d shallow flow model based on nays2dh 2 the submerged volume of the large wood is calculated considering buoyancy and the cdm if the large wood contains a root component the cdm and submerged volume are recalculated 3 the bed friction acting on the large wood is determined considering the cdm no bed friction is applied to the large wood motion if the large wood is located at a water depth higher than the cdm the bed friction coefficient models static friction with rolling friction if the large wood is stopped otherwise the bed friction coefficient models kinematic friction with rolling friction 4 large wood particle velocity is calculated based on drag force with reference to water flow velocity bed friction is considered if the draft of the large wood particle is less than the cdm 5 from the particle velocity calculation the advection of each particle is calculated separately without connection for one time step δt then the new location of the gravitational center of the large wood is determined by averaging the new particle locations in each particle the rotation angle of the large wood is calculated by averaging the rotation angle of each particle in relation to the center of gravity of the large wood piece fig 6 b 6 the position of each particle is rearranged into the shape of the large wood considering the averaged rotation angle and center of gravity fig 6 b 7 from the rearranged large wood drag force is calculated and applied to the water flow calculation in the next time step 3 3 validation of simulation results for flow depth fig 7 illustrates the final pattern of the flow velocity vector and water depth in the computational domain the constriction caused by obstacle forces can cause a transition to subcritical flow upstream of the obstacle because the energy of the flow is lower than the minimum energy required to pass through the narrow section water depth rapidly decreases close to downstream obstacles because supercritical flow is dominant owing to the smooth bed the effect of localized narrowing depends on the froude number of the unaffected flow upstream of obstacle and on the constriction ratio width of narrow section width of wide section in other words water flow is supercritical with higher velocity and lower depth with respect to the nonuniform flow conditions in addition it tends asymptotically to uniform flow similarly in the experiment fig 8 a hydraulic jump was observed because of the transition of water flow from supercritical flow fig 8 compares the water depth along the centerline between the simulations at the final computation step and experiments the red box in these figures marks an obstacle section even though the simulation result overestimates the flow depth upstream of the obstacle this inconsistency can be neglected because we focused on the area downstream of the obstacle in the downstream section the difference between the simulation and experimental results is small therefore the present flow model can be considered to capture flow depth well 4 results of the experiments and simulations 4 1 large wood pieces with no root wad cases 1 4 as shown in table 2 we conducted eight cases of experiments and simulations this section describes the results of cases 1 4 which considered the wood piece without a root wad note that no figure for the simulation results is presented in this paper only experiment results are shown in fig 9 because none of the wood pieces without a root wad were deposited in the simulation results the experimental results show that the wood piece without a root wad can become deposited close to the side wall fig 9 this is in contrast to the simulation results which indicated that no deposition of wood pieces occurred in the flume channel rolling friction was dominant in the experiments hence when the wood piece was touched the bed it was immediately flowed by floating and rolling motion we considered that the applicability of the depth averaged 2d model was relatively poor close to the side wall because the strong secondary current at the corner between the bed and sidewall was not considered in the present 2d flow model and this secondary current increased flow resistance in addition the simulations only considered the motion of the wood piece in the horizontal direction and neglected the advection in the vertical direction thus the downward motion of the wood piece induced by secondary flow was neglected 4 2 large wood pieces with root wad cases 5 8 in the simulations the wood pieces without a root wad showed no deposition however the wood pieces with a root wad clearly exhibited deposition fig 10 illustrates the change in the proportion of the deposited wood with time the range of the final proportion of the deposited wood pieces in the simulation results 30 100 reproduced the experimental results 20 80 well in particular the deepening on the discharge increased and the decrease in the proportion of the deposited wood pieces with time was similar between the simulation and experimental results on the contrary the responses of the changes in channel slope were unclear because channel slope changed only flow dispersion fig 7 which may be insufficient to affect the motion of the wood pieces we measured the final averaged position considering the center of gravity of a wood piece in a normalized coordinate system using flume width and length to compare the final deposition patterns obtained through the experiments and simulations fig 10 fig 11 a shows the final averaged deposition in the streamwise direction while fig 11 b presents the distribution of the wood deposition in the lateral direction mean deviation value in fig 11 a values of 1 and 0 indicate that all wood pieces are deposited downstream and upstream respectively fig 11 b indicates the wood deposition in the lateral direction where values of 0 and 1 indicate that the wood pieces are deposited along the channel centerline and close to a wall respectively according to fig 11 a if flow discharge is larger cases 6 and 8 the value of the average position increases in the streamwise direction because the larger flow discharge increases water depth and flow velocity thus drag force increases and the influence of buoyancy becomes strong the average values in fig 11 b show an unclear relation between wood piece deposition and two parameters i e discharge and channel slope because the wood piece deposition in the lateral direction depends on several parameters such as the strength of secondary flow the angle of stem direction and the initial position in the lateral direction the initial conditions must be controlled more strictly e g the input stemwise angle of the wood piece and input position should be set uniformly to analyze this relation fig 12 shows the experimental and simulation results for the wood pieces with a root wad in the simulation results the yellow items represent the wood piece where the large head is a root and the small head is the opposite side two types of contours are indicated i e water depth 0 0 025 m and flow velocity 0 1 m s in this work we focus on distances up to 80 cm from the obstacles once a wood piece is deposited on the center of the flume bed other wood pieces can easily become deposited around the first piece because the settled wood piece exerts drag force on flow decelerating flow velocity and increasing water depth thus when the large wood settles on the bed it acts like vegetation this implies that future work should carefully consider the bed morphology around settled large wood in actual rivers vegetation is known to increase the flow resistance and water level around it such that the vegetation area captures sediment even though the immediately adjacent area is eroded by increased flow velocity e g ikeda and izumi 1990 nanson and knighton 1996 murray and paola 2003 similar bed shape changes can be expected around settled large wood the wood pieces with a root wad face upstream because the root lifts the wood piece from the bed when the wood piece flows into a zone with lower water depth this lifting decreases the draft for the root wad section the deposited wood piece tends to maintain the deposited state by reducing the projection area of the wood piece e g persson 1999 hence the stem section is immediately rotated to be parallel to the flow direction we found that the angle of the deposited wood piece tends to depend on the flow discharge fig 12 as shown in table 4 when the flow discharge is larger cases 6 and 8 the angle between the streamwise and stemwise directions of the wood piece decreases as compared to the angle in the cases with smaller discharge cases 5 and 7 this result indicates that the change in the deposition angle is associated with the projection area affected by the flow discharge wilcox and wohl 2006 however the strength of the relationship between the discharge and the deposition angle due to the projection area is unclear and should be further analyzed 5 discussion the results of the simulations and experiments showed that two deposition patterns are dominant in the presence and absence of root wads i e at the center of the flume and close to the side walls fig 13 the water depth upstream of the obstacle is larger 1 2 cm than that of the cdm root 1 2 cm stem 0 6 cm hence all the wood pieces flow downstream after passing the obstacles the water depth becomes smaller than that in the cdm and the wood pieces touch the bed around the center of the flume in this region if the drag force is sufficient to drive a wood piece sliding motion of the wood piece occurs and the piece may gradually flow downstream in addition the wood piece rotates in its stemwise direction along with the flow direction because the increased flow velocity in the narrowing section enhances the drag force around the obstacle fig 13 a and b the wood piece with a root wad can easily be deposited after passing the obstacle because of the decrease in draft for wood motion at the root wad part even though the diameter of the root wad is larger than that of the stem the wood piece with the root wad is easily subjected to a larger drag force because the root wad can lift the head of the wood piece from the flume bed this is because when the root first touches the flume bed the buoyancy of the wood piece decreases owing to the decrease in the draft caused by the root wad merten et al 2010 shields and alonso 2012 after the wood piece comes into contact with the flume bed the water depth around the wood piece and the velocity head increase because the wood piece affects the flow of water this increases the buoyancy and drag force and thus the wood piece shifts toward the flume walls by rolling even though the friction force in the stemwise direction remains larger fig 13 c if the root wad of the wood piece sticks to the flume walls the wood piece settles in a stable manner and decelerates the water flow around it then the wood piece can be considered to be completely deposited near the walls wilcox and wohl 2006 braudrick et al 1997 demonstrated that wood pieces are frequently deposited on the lateral margins wall or bank of a channel through a flume experiment and braudrick and grant 2000 presented three equations to analyze the flotation threshold for wood motion for diverse densities however these studies neglected the bed friction for wood motion in the presented equations on the contrary the present study shows that rolling motion is a factor for deposition on lateral margins moreover the flow velocity downstream of the obstacle has a component directed toward the wall as steering flow caused by the channel geometry close to the obstacle fig 7 this rolling occurs because the sliding friction static μ 0 4 and kinematic μ 0 05 along the stemwise direction is considerably larger than that along the lateral direction in which the friction for wood motion is governed by rolling friction μ 0 001 to a higher extent compared to sliding friction therefore the wood piece cannot move in the stemwise direction however it can move in the lateral direction thus it moves toward a flume wall the wood piece flows downstream if drag force and the stemwise angle are larger fig 13 d the wood piece without a root wad flows more easily downstream because its draft is larger than that of the wood piece with a root wad such a deposition pattern which is parallel to the flow direction was observed in the experiments welber et al 2013 bertoldi et al 2014 and field observations e g piegay and gurnell 1997 in the drôme river france and by gurnell et al 2000 and bertoldi et al 2013 in the tagliamento river italy however rolling motion can be limited to occur within the artificial hydraulic structure which contains a rigid and flat channel bed this is because in the natural system most channel beds are not flat in shape owing to sediment e g sand gravel and boulder the results of the experiments and simulations showed that the presence of a root wad and the cdm had the most important roles in controlling deposition braudrick et al 1997 experimentally showed that large wood was mainly deposited at the most shallow water depth davidson et al 2015 demonstrated that root wads significantly decreased travel distance braudrick and grant 2000 merten et al 2010 wilcox and wohl 2006 and schenk et al 2014 showed that the presence of root wads is the most important determinant of mobilization and travel distance in this study the root wad particles were regarded as having a diameter larger than that of the trunk particles by a factor of 2 thus the root wad particles were more easily subjected to larger drag forces because of the larger projection area compared to the trunk particles however the root wad particles weighed more than the trunk particles which had the same density and thus they had a higher cdm compared to the trunk particles moreover we considered the change in water depth caused by the root wad effect fig 3 therefore the deposition of a larger number of wood pieces with a root wad was observed in the simulation and experimental results and the proportion of the deposited wood pieces with a root wad was clearly in good agreement with the experimental results figs 10 and 12 thus we have shown that the root wad effect is reasonably reproduced by the proposed numerical method and that wood motions can be accurately simulated using more diverse root wads size and density however the root wad effect should be carefully refined using more detailed measurements the present simulation model employed an empirical bed friction coefficient and the bed friction coefficients of the root wad particles and trunk particles were tuned using a trial and error approach through a comparison with the experimental results in addition the deposition close to the wall was not reproduced for the wood pieces without a root wad thus the motion of such wood pieces should also be improved considering several parameters such as the secondary flow and projection area of a wood piece this simulation considered a random stemwise angle of the wood piece at the starting zone because the stemwise angle cannot be controlled prior to an obstacle therefore the positions of the wood pieces and the number of wood pieces may show slightly different patterns between the simulations and experiments in the experiments we observed floating wood pieces close to the obstacle in the upstream direction these pieces did not flow downstream this may be because the variable stemwise input angle caused different projection areas for wood motion if we make the stemwise angle of the wood pieces constant before they pass the obstacle in the experiment and simulation both sets of results may show more regular patterns of wood piece deposition and advection the responses of the wood motion to the change in the channel slope exhibited an unclear pattern the debris flow on the steep channel exhibited the active motion of large wood because the steep channel can cause gravity flow through higher potential energy e g schenk et al 2014 carter 1975 pierson 1981 costa 1984 takahashi et al 1992 shieh et al 1996 imran et al 2000 shrestha et al 2012 in this steep channel the large wood can move downward more easily compared to in a mild channel such as an alluvial river because of the gravity flow such gravity flow was neglected herein because we used a mild channel slope in the simulations and experiments therefore the change in channel slope should be large to consider the gravity flow due to a steep channel this will be performed in subsequent studies in contrast the present study showed that the motion of the wood pieces is considerably affected by flow discharge wilcox and wohl 2006 conducted an experiment with a laboratory flume for investigating the flow resistance dynamics in step pool channels considering diverse wood profiles such as the effect of root wad length of wood piece and density their experiment showed that the wood pieces on the step pool channels increase the water depth because these wood pieces increase the flow resistance moreover they indicated that the position of deposited wood pieces is an important parameter for the flow resistance the wood pieces in the present study exhibited two deposition positions near walls and at the center in the flume indicating the occurrence of sliding and rolling motions due to flow discharge in addition the deposition angles of the wood pieces were clearly dependent on the flow discharge table 4 thus our study also demonstrated that the flow discharge is an important factor for large wood dynamics interestingly the experiments in this study showed simple jam formation by the deposited wood pieces fig 12 this jam formation was clearly observed when flow discharge was smaller cases 5 and 7 the jam formation could be accelerated by the first wood piece which could interrupt the motion of subsequent wood pieces several researchers e g braudrick et al 1997 welber et al 2013 bertoldi et al 2014 showed the characteristics of jam formation based on wood supply bed elevation and flow discharge through experiments however these studies only conducted a flume experiment and several researchers are attempting to investigate this using a computational model for jam formation the computational model is challenging to apply as a large wood dynamics model because multiple parameters should be considered such as wood collision large wood shape profile of the channel bed and water flow a large cpu time with a high performance workstation is required to reproduce a number of wood motions similar to the experiments for jam formation therefore our study would provide informative data for the development of the computational model considering anisotropic bed friction change in draft for wood motion and the root wad effect the present study focused on the flume scale experiment and the computational model was validated under such flume scale conditions however this model is required to be expanded into large scale cases for practical purposes thus in a future study the large wood dynamics model should be considered for the natural system associated with the large scale problem for instance the large wood dynamics model can simulate larger scale experiments welber et al 2013 bertoldi et al 2014 and river scale observations gurnell et al 2000 bertoldi et al 2013 through comparisons among the simulation results the results of the large scale experiments and the observations the applicability of the present computational model to large scale cases should be examined in a future study 6 conclusions this study developed a numerical model for simulating large wood dynamics with floating sliding and settling motions in shallow flows by coupling a depth averaged 2d flow model and a lagrange type large wood dynamics model in the present numerical model we considered the change in draft for wood motion anisotropic bed friction and the root wad effect the reproducibility of the proposed computational method is generally good in addition the method produces a reasonable simulation of the different deposition patterns in relation to the changes in flow discharge and the root wad effect based on the experiment the detailed conclusions are as follows 1 responses of the wood motion to flow discharge and channel slope a wood piece tends to move toward the side walls after touching down on a lower water depth zone because of small rolling friction such motion becomes more dominant when flow discharge decreases in addition drag force and water depth increase with flow discharge moreover the wood piece easily flows in the streamwise direction however the responses of wood motion to the change in channel slope show an unclear pattern because the employed values of channel slope are insufficient to affect wood piece motion 1 root wad effect in the experiments a wood piece can become deposited more easily if it contains a root wad because the root wad decreases the draft for the wood motion in the wood piece by lifting its head even though the weight and volume in part of the root wad are larger than the stem of the wood piece the simulation results clearly reproduce the root wad effect well however in cases of wood pieces without a root wad the experimental results only show deposited wood pieces close to the wall through these results we discovered a relationship between the deposition angle of the large wood and flow discharge the stemwise angle of the deposited wood piece becomes smaller when flow discharge is larger consequently this implies that the deposited wood angle is associated with the projection area between the flow discharge and wood piece thus further study should consider additional parameters such as the projection area of the wood piece considering the angle between the stemwise and streamwise directions 1 limitations of the developed model in the experimental and simulation results the wood piece with a root wad tends to move toward the side walls because of the smaller rolling friction coefficient we considered the shape of the wood piece stem to be a cylinder in the experiment in the simulation we considered the wood piece as a spherical particle therefore the rolling friction coefficient is relatively smaller than that of natural large wood which contains branches rough surfaces and irregularly shaped cross sections acknowledgment we are thankful to the ministry of science technology grant name mext scholarship research students and the river foundation grant no 2018 5211 042 for providing funding for this research 
764,this study developed a computational model for large wood deposition patterns in shallow flows considering the effect of a root wad based on laboratory experiments we used the nays2dh depth averaged two dimensional model of iric to simulate shallow flows a newly developed large wood simulation model was combined with the shallow flow model the laboratory tests were performed by changing several hydraulic parameters in shallow water with a depth similar to the diameter of large wood the root wad decreased the draft for wood motion the depth at which large wood contacts the river bed by lifting the head of large wood the experimental results showed that the large wood tends to move toward the side walls and deposit on the bed after passing an obstacle computational results reasonably showed that the proposed coupling model reproduced the fundamental and physical aspects of the phenomena 1 introduction in rivers large wood is transported along with water and sediment deposited large wood affects river morphology by causing local scour and deposition of bed materials such large wood may initiate the formation of an island as a midchannel bar keller and swanson 1979 nakamura and swanson 1993 abbe and montgomery 1996 gurnell and petts 2002 swanson 2003 van der nat et al 2003 brooks et al 2006 sawyer and cardenas 2012 large wood and deposition patterns can change in response to input processes channel morphology and hydrological parameters including flood events montgomery and piégay 2003 previous research showed that the relative influence of these factors changes along the river system keller and swanson 1979 gurnell et al 2000 2002 collins et al 2012 gurnell 2013 resulting in distinct downstream trends in large wood accumulation style keller and swanson 1979 gurnell et al 1995 marcus et al 2002 comiti et al 2006 the ratio of wood length to channel width is a key parameter that affects large wood deposition patterns braudrick et al 1997 in addition the patterns are influenced by diverse parameters such as drag force water level bed friction and obstacles numerous researchers have studied large wood dynamics braudrick et al 1997 braudrick and grant 2000 2001 provided the basic framework to approach large wood mobility and entrainment in rivers based on these frameworks several researchers investigated large wood transport dynamics e g gurnell et al 2002 haga et al 2002 bocchiola et al 2002 2006 welber et al 2013 bertoldi et al 2014 these studies successfully predicted a wood motion relationship between hydrodynamic and resistance forces and a few of them examined transport systems in studying large wood transport the large wood shape and density for motion are considered first comiti et al 2006 gurnell and sweet 1998 a piece of large wood may consist of a stem branch and or root wad among these the stem and root wad are the major components and they actively affect the motion of large wood large wood deposition is particularly sensitive to the characteristics of the root wad because this part increases the friction with the bed in addition to experiments and observations computational models for large wood dynamics have been developed such as the iber wood two dimensional 2d hydrodynamic model ruiz villanueva et al 2014a b ruiz villanueva et al 2016 and the nayscube kimura 2012 three dimensional 3d reynolds averaged navier stokes model kimura and kitazono 2017 kitazono et al 2016 both models address large wood dynamics following a lagrangian method coupling water flow with wood transport the iber wood model considers the wood shape to be a simple cylinder in contrast the nayscube large wood model applies a particle method to consider the impinging motion of large wood using a discrete element method these studies showed the mechanism of large wood dynamics in deep water flows where water depth is larger than wood diameter to demonstrate floating motion only and they accurately reproduced large wood motion however the analysis of large wood dynamics through the integration of river bed and large wood such as the root wad effect and anisotropic bed friction is neglected in these models even though large wood dynamics show floating motion sliding motion and deposition in a natural river system clarifying such phenomena by coupling motions floating sliding and settling motions is crucial in understanding large wood behavior in rivers and the role of large wood floating advection thus it is required to study sliding motion and deposition with the root wad effect and anisotropic bed friction as the key parameters of large wood dynamics for improving computational methods the main aim of this work is to develop a numerical method for simulating the transport of large wood along with hydrodynamics in shallow flows considering the root effect under various hydrologic parameters flow discharge and channel slope this study presents a newly developed numerical model that can simulate various behaviors of large wood we use a 2d depth averaged flow model nays2dh shimizu et al 2014 of international river interface cooperative iric 2017 which is an eulerian model for calculating water flow and bed morphology a lagrange type large wood model is newly developed and combined with nays2dh the applicability of the present model is discussed through a comparison with experimental results 2 experimental setup 2 1 scale of the experiment among several studies related to large wood and river morphology gurnell et al 2002 first presented the classification of river scales small medium and large kramer and wohl 2017 recently systematically categorized four river scales in relation to large wood table 1 we conducted laboratory tests on a medium scale river which was arbitrarily selected for a simplified experiment to examine the reproducibility of the large wood model as obstacle was used to clearly describe the process of wood deposition with change in channel width in addition we could observe the transition of large wood motions such as floating sliding settling and re moving in shallow flows the root scale diameter approximately 0 01 m was taken from bertoldi et al 2014 2 2 experimental model for large wood and flume the flume channel measured 0 3 m 2 m fig 1 a and the flume bed was created using extremely smooth wood panels fig 1 d obstacle size was 10 cm 10 cm 10 cm and two obstacles were attached to the side wall fig 1 a the large wood models were created using wood cylinders with diameters of 1 cm and lengths of 10 cm the density of the large wood models was 650 kg m3 in wet conditions i e when floating on water the root model was developed by attaching two thin wood cylinders 0 18 cm in diameter and 2 cm in length as a cross shape at 90 to the end of the large wood model welber et al 2013 bertoldi et al 2014 the shape of the stem was cylindrical hence the resistance to the motion in the lateral direction was considerably smaller than that to the motion in the stem wise direction because of rolling motion hence small pieces of wood 0 1 cm 0 2 cm were attached at the middle of the stem to increase the resistance to rolling motion fig 1 b table 2 shows the experimental cases this study considered large wood deposition in relation to channel slope flow discharge and the root effect in the experiments we used two different flume slopes i e 0 0045 and 0 007 m m to consider the change in flow velocity under the same flow discharge the following two cases of discharge were selected based on the minimal discharge smaller case for the motion of the wood piece with a root wad and approximately twice the minimal discharge larger discharge a smaller case 0 00065 m3 s and 0 00060 m3 s and a larger case 0 0010 m3 s and 0 0011 m3 s in addition the root effect was examined for two cases i e with and without roots therefore eight cases were investigated in the experiments and computations each case was repeated thrice to verify reproducibility then a typical case was selected wood pieces were supplied one by one at intervals of 4 s after the flow reached an equilibrium state no wave with the back water effect ten wood pieces were supplied in each case the initial input direction of the wood pieces was random the range of the initial positions of the wood pieces was also random but limited within a circle with a radius of 5 cm starting zone fig 1 a because the initial angle and position were difficult to control these settings were identically applied to the computational conditions 3 simulation method we used a 2d plane numerical solver developed based on nays2dh shimizu et al 2014 of international river interface cooperative iric 2017 to simulate the flows a third order total variation diminishing monotonic upstream centered scheme for conservation laws method was used for the advection term and a zero equation model was used for the turbulence model the coupled model was a two way model that considered the drag force exerted by the large wood on the flow we estimated the manning roughness coefficient as 0 006 because the bottom of the flume was flat and smooth grid size was set as 1cm 1cm the number of grids was 200 and 30 in the x direction and y direction respectively to resolve the flows around the large wood with a diameter of 1 cm the initial time step δt was 0 0035 s and the total simulation time was 70 s large wood pieces were supplied one by one every 4 s after the flow reached a stable condition no back water effect close to the obstacle in the same manner in which 10 pieces were supplied in the experimental cases we reduced the simulation time using the parallelization method openmp 3 1 particle method for modeling large wood dynamics 3 1 1 governing equation for the large wood motion the large wood dynamics equation kimura 2012 kimura and kitazono 2017 kitazono et al 2016 in cartesian coordinates is described as follows 1 σ a 3 d 3 d u p d t f d f w a f a m f b e d f d f w a f a m f b e d 0 f d f w a f a m f b e d where 2 f d 1 2 c d ρ λ a s u b a 2 d 2 u u p u u p 3 f wa ρ λ v s u b a 3 d 3 d u d t 4 f am c m ρ λ v s u b a 3 d 3 d u d t d u p d t 5 f bed 0 h h c g μ a 3 d 3 σ λ v s u b ρ u p u p h h c the bed friction coefficient μ in eq 5 which is an anisotropic friction coefficient is changed by the stemwise angle of the wood piece fig 2 this parameter is expressed by eqs 6 and 7 6 μ x n μ s o r k cos 2 θ sin 2 θ 7 μ y n μ s o r k sin 2 θ cos 2 θ where ρ is the water density σ is the density of large wood cm is an additional mass coefficient cm 0 5 was used in this work cd is the drag coefficient a 2 is the 2d shape coefficient π 4 in this work a 3 is the 3d shape coefficient π 6 in this work d is the diameter of the particle sphere u is the fluid velocity around the sphere up is the velocity of the sphere ν is the kinematic viscosity coefficient t is time g is gravitational acceleration μ x and μ y are the x and y components of the friction coefficient respectively θ is the stemwise angle θ 0 indicates the angle matching the x axis direction n is the number of particles for one large wood piece μ s or k is the sliding friction coefficient s static k kinematic fbed is the bed friction force λ a sub is the submerged area rate λ v sub is the submerged volume rate fd is the drag force fwa is the effect of the acceleration of surrounding water fam is the effect of the added mass of water hc is the critical draft for wood motion when calculating sliding friction eqs 6 and 7 the sliding friction coefficient μ s or k should be multiplied by the total number of spherical particles n in the large wood because the sliding frictions of all particles are in the same direction and move as one material the following drag force terms caused by the large wood are added for the eulerian momentum equation for water flows 8 f d r 1 2 ρ c d 1 h a c e l l n 1 n c e l l a n u i u p i u i u p i where an is the projected area of a particle toward the flow direction acell is the area of a grid cell and i is the number of particles in the large wood piece the present study neglects the effect of collisions among large wood pieces to focus on the large wood advection and deposition patterns therefore each large wood dynamic is independent however this numerical analysis includes motion that is indirectly changed by the water alterations due to adjacent large wood because such a large wood piece is modeled as causing drag force and changing water depth 3 1 2 anisotropic friction coefficient the static friction coefficient between the wood used to construct the flume bed and the large wood is generally approximately 0 4 0 6 ambrose and tripeny 2011 the friction coefficient under submerged conditions is uncertain however it may be expected to decrease therefore we conducted trials to compare the experiment and simulation and adjusted the friction coefficients accordingly table 3 the static μ s and dynamic μ k friction coefficients refer to the cases when large wood is deposited and sliding respectively these coefficients are valid in the stemwise direction of the wood piece if large wood slides in the direction perpendicular to the stemwise direction the large wood rolls and rolling friction becomes dominant table 3 shows the static kinematic and rolling friction coefficients 3 1 3 buoyancy and critical draft for wood motion one important factor that affects large wood deposition is its buoyancy this study considers the sphere by sphere transition of the submerged volume based on the change in flow depth and the presence of a root wad in addition the inclination of the settled large wood based on the root wad shape is considered to evaluate buoyancy the large wood is deposited if buoyancy is less than the weight of the large wood the large wood floats if buoyancy is larger than gravitational force if the density of the large wood is less than the water density the draft of the large wood is determined by the balance between buoyancy and weight the water depth that is the same as the draft is referred to as the critical draft for wood motion cdm in other words the cdm is the minimum depth at which the large wood does not touch the bed the cdm hc is the same as the maximum draft in the floating condition which can be obtained as follows the maximum unsubmerged volume of the large wood is calculated using eq 9 9 v e b w w d ρ w w d 4 3 π d 3 ρ d b w 4 3 π d 3 ρ w where r is the diameter of a large wood particle wd is the weight of a large wood particle under the totally submerged condition bw is the buoyancy under the totally submerged condition ve is the unsubmerged volume of a particle under the totally floating condition fig 3 we can obtain the maximum unsubmerged volume ve from eq 9 and the submerged volume vs using eq 10 10 v s r a c f y 2 π d y f y r 2 y 2 π r a c r 2 y 2 d y π r 3 y 1 3 y 3 r a c π r 2 a c 1 3 a c 3 r 3 1 3 r 3 the unsubmerged volume ve can also be calculated by eq 11 from eqs 9 and 10 11 v e v t v s v t 4 3 π r 3 where vs is the submerged volume of the large wood particle vt is the volume of the large wood particle ac is the water level from the center of the particle fig 3 we can obtain the cdm by iteratively executing these equations we assume that the diameter of a root wad particle is twice the diameter of a stem particle to model the different drag force and buoyancy of the root the drag force of a root particle is larger than that of a stem particle because of the larger projection area of the root whereas the weight of the root is large because of its increased volume at the same density 3 1 4 change in water depth by root effect when a root wad of floating large wood fig 4 a becomes deposited on the bed at a lower water depth than the cdm the root wad effect is activated thereby decreasing the draft of the wood piece fig 4 b the root wad lifts one edge of the stem up from the river bed thus the submerged volume of the large wood decreases then the influence of buoyancy is decreases and the normal force to the bed increases compared to the case of a large wood piece with no root wad in the computation we first determine the cdm of each particle in the large wood piece then the change in water depth caused by the root wad effect is applied as follows if the average cdm of the large wood particles is lower than the present water depth fig 4 c 12 h s n i p h s h r l a i p i p n h r h p l d l a l a h p 2 l d 2 where hs is the present water depth under the root particle hp is half the difference between the stem diameter and root diameter ld is the stem length la is the projection length of the stem into the river bed hr is the increase in height caused by the root effect h s n i p is the calculated water depth caused by the root effect for each particle ipn is the total number of particles in the large wood ip is the index of each particle 3 2 interaction between water flow and large wood in the computation in this solver the calculation schemes for water flow and large wood are different in the present large wood dynamics model one large wood piece is expressed as connected particles which are driven following the lagrangian method therefore a method for modeling the interaction between grid based water flow and particle based large wood motion is required accordingly we use linear interpolation fig 5 to obtain the flow velocity and water depth at target points on the particles that compose the large wood if each particle is located at an arbitrary position the relative length la lb from a standard grid point f 1 to the center of the particle can be calculated to determine the inverse distance weight here all grid lengths between all grid points are 1 on the generalized curvilinear coordinate system then the value at the particle center fip is calculated with reference to the values at four surrounding grid points using eqs 13 and 14 13 f i p k 1 4 f k a k k 1 4 1 a k 14 a 1 l a l b a 2 l a 1 l b a 3 1 l a 1 l b a 4 1 l a l b where fip is the interpolated value at the particle center f 1 f 4 are the grid nodes in the ξ and η directions ξ and η are generalized curvilinear coordinates a 1 a 4 are the weighting areas considering the inverse distance from the particle fip la and lb are the relative lengths from the target point based on the standard grid point f 1 the values of la and lb are 0 if the positions of f 1 and f ip are the same k is the index for the grid point through this interpolation we can obtain the interpolated flow velocity and water depth values at each particle location from a grid node point in the generalized curvilinear coordinate system fig 6 shows the process of calculating the interaction between water flow and large wood motion the specific calculation procedure is as follows 1 water flow is calculated using the 2d shallow flow model based on nays2dh 2 the submerged volume of the large wood is calculated considering buoyancy and the cdm if the large wood contains a root component the cdm and submerged volume are recalculated 3 the bed friction acting on the large wood is determined considering the cdm no bed friction is applied to the large wood motion if the large wood is located at a water depth higher than the cdm the bed friction coefficient models static friction with rolling friction if the large wood is stopped otherwise the bed friction coefficient models kinematic friction with rolling friction 4 large wood particle velocity is calculated based on drag force with reference to water flow velocity bed friction is considered if the draft of the large wood particle is less than the cdm 5 from the particle velocity calculation the advection of each particle is calculated separately without connection for one time step δt then the new location of the gravitational center of the large wood is determined by averaging the new particle locations in each particle the rotation angle of the large wood is calculated by averaging the rotation angle of each particle in relation to the center of gravity of the large wood piece fig 6 b 6 the position of each particle is rearranged into the shape of the large wood considering the averaged rotation angle and center of gravity fig 6 b 7 from the rearranged large wood drag force is calculated and applied to the water flow calculation in the next time step 3 3 validation of simulation results for flow depth fig 7 illustrates the final pattern of the flow velocity vector and water depth in the computational domain the constriction caused by obstacle forces can cause a transition to subcritical flow upstream of the obstacle because the energy of the flow is lower than the minimum energy required to pass through the narrow section water depth rapidly decreases close to downstream obstacles because supercritical flow is dominant owing to the smooth bed the effect of localized narrowing depends on the froude number of the unaffected flow upstream of obstacle and on the constriction ratio width of narrow section width of wide section in other words water flow is supercritical with higher velocity and lower depth with respect to the nonuniform flow conditions in addition it tends asymptotically to uniform flow similarly in the experiment fig 8 a hydraulic jump was observed because of the transition of water flow from supercritical flow fig 8 compares the water depth along the centerline between the simulations at the final computation step and experiments the red box in these figures marks an obstacle section even though the simulation result overestimates the flow depth upstream of the obstacle this inconsistency can be neglected because we focused on the area downstream of the obstacle in the downstream section the difference between the simulation and experimental results is small therefore the present flow model can be considered to capture flow depth well 4 results of the experiments and simulations 4 1 large wood pieces with no root wad cases 1 4 as shown in table 2 we conducted eight cases of experiments and simulations this section describes the results of cases 1 4 which considered the wood piece without a root wad note that no figure for the simulation results is presented in this paper only experiment results are shown in fig 9 because none of the wood pieces without a root wad were deposited in the simulation results the experimental results show that the wood piece without a root wad can become deposited close to the side wall fig 9 this is in contrast to the simulation results which indicated that no deposition of wood pieces occurred in the flume channel rolling friction was dominant in the experiments hence when the wood piece was touched the bed it was immediately flowed by floating and rolling motion we considered that the applicability of the depth averaged 2d model was relatively poor close to the side wall because the strong secondary current at the corner between the bed and sidewall was not considered in the present 2d flow model and this secondary current increased flow resistance in addition the simulations only considered the motion of the wood piece in the horizontal direction and neglected the advection in the vertical direction thus the downward motion of the wood piece induced by secondary flow was neglected 4 2 large wood pieces with root wad cases 5 8 in the simulations the wood pieces without a root wad showed no deposition however the wood pieces with a root wad clearly exhibited deposition fig 10 illustrates the change in the proportion of the deposited wood with time the range of the final proportion of the deposited wood pieces in the simulation results 30 100 reproduced the experimental results 20 80 well in particular the deepening on the discharge increased and the decrease in the proportion of the deposited wood pieces with time was similar between the simulation and experimental results on the contrary the responses of the changes in channel slope were unclear because channel slope changed only flow dispersion fig 7 which may be insufficient to affect the motion of the wood pieces we measured the final averaged position considering the center of gravity of a wood piece in a normalized coordinate system using flume width and length to compare the final deposition patterns obtained through the experiments and simulations fig 10 fig 11 a shows the final averaged deposition in the streamwise direction while fig 11 b presents the distribution of the wood deposition in the lateral direction mean deviation value in fig 11 a values of 1 and 0 indicate that all wood pieces are deposited downstream and upstream respectively fig 11 b indicates the wood deposition in the lateral direction where values of 0 and 1 indicate that the wood pieces are deposited along the channel centerline and close to a wall respectively according to fig 11 a if flow discharge is larger cases 6 and 8 the value of the average position increases in the streamwise direction because the larger flow discharge increases water depth and flow velocity thus drag force increases and the influence of buoyancy becomes strong the average values in fig 11 b show an unclear relation between wood piece deposition and two parameters i e discharge and channel slope because the wood piece deposition in the lateral direction depends on several parameters such as the strength of secondary flow the angle of stem direction and the initial position in the lateral direction the initial conditions must be controlled more strictly e g the input stemwise angle of the wood piece and input position should be set uniformly to analyze this relation fig 12 shows the experimental and simulation results for the wood pieces with a root wad in the simulation results the yellow items represent the wood piece where the large head is a root and the small head is the opposite side two types of contours are indicated i e water depth 0 0 025 m and flow velocity 0 1 m s in this work we focus on distances up to 80 cm from the obstacles once a wood piece is deposited on the center of the flume bed other wood pieces can easily become deposited around the first piece because the settled wood piece exerts drag force on flow decelerating flow velocity and increasing water depth thus when the large wood settles on the bed it acts like vegetation this implies that future work should carefully consider the bed morphology around settled large wood in actual rivers vegetation is known to increase the flow resistance and water level around it such that the vegetation area captures sediment even though the immediately adjacent area is eroded by increased flow velocity e g ikeda and izumi 1990 nanson and knighton 1996 murray and paola 2003 similar bed shape changes can be expected around settled large wood the wood pieces with a root wad face upstream because the root lifts the wood piece from the bed when the wood piece flows into a zone with lower water depth this lifting decreases the draft for the root wad section the deposited wood piece tends to maintain the deposited state by reducing the projection area of the wood piece e g persson 1999 hence the stem section is immediately rotated to be parallel to the flow direction we found that the angle of the deposited wood piece tends to depend on the flow discharge fig 12 as shown in table 4 when the flow discharge is larger cases 6 and 8 the angle between the streamwise and stemwise directions of the wood piece decreases as compared to the angle in the cases with smaller discharge cases 5 and 7 this result indicates that the change in the deposition angle is associated with the projection area affected by the flow discharge wilcox and wohl 2006 however the strength of the relationship between the discharge and the deposition angle due to the projection area is unclear and should be further analyzed 5 discussion the results of the simulations and experiments showed that two deposition patterns are dominant in the presence and absence of root wads i e at the center of the flume and close to the side walls fig 13 the water depth upstream of the obstacle is larger 1 2 cm than that of the cdm root 1 2 cm stem 0 6 cm hence all the wood pieces flow downstream after passing the obstacles the water depth becomes smaller than that in the cdm and the wood pieces touch the bed around the center of the flume in this region if the drag force is sufficient to drive a wood piece sliding motion of the wood piece occurs and the piece may gradually flow downstream in addition the wood piece rotates in its stemwise direction along with the flow direction because the increased flow velocity in the narrowing section enhances the drag force around the obstacle fig 13 a and b the wood piece with a root wad can easily be deposited after passing the obstacle because of the decrease in draft for wood motion at the root wad part even though the diameter of the root wad is larger than that of the stem the wood piece with the root wad is easily subjected to a larger drag force because the root wad can lift the head of the wood piece from the flume bed this is because when the root first touches the flume bed the buoyancy of the wood piece decreases owing to the decrease in the draft caused by the root wad merten et al 2010 shields and alonso 2012 after the wood piece comes into contact with the flume bed the water depth around the wood piece and the velocity head increase because the wood piece affects the flow of water this increases the buoyancy and drag force and thus the wood piece shifts toward the flume walls by rolling even though the friction force in the stemwise direction remains larger fig 13 c if the root wad of the wood piece sticks to the flume walls the wood piece settles in a stable manner and decelerates the water flow around it then the wood piece can be considered to be completely deposited near the walls wilcox and wohl 2006 braudrick et al 1997 demonstrated that wood pieces are frequently deposited on the lateral margins wall or bank of a channel through a flume experiment and braudrick and grant 2000 presented three equations to analyze the flotation threshold for wood motion for diverse densities however these studies neglected the bed friction for wood motion in the presented equations on the contrary the present study shows that rolling motion is a factor for deposition on lateral margins moreover the flow velocity downstream of the obstacle has a component directed toward the wall as steering flow caused by the channel geometry close to the obstacle fig 7 this rolling occurs because the sliding friction static μ 0 4 and kinematic μ 0 05 along the stemwise direction is considerably larger than that along the lateral direction in which the friction for wood motion is governed by rolling friction μ 0 001 to a higher extent compared to sliding friction therefore the wood piece cannot move in the stemwise direction however it can move in the lateral direction thus it moves toward a flume wall the wood piece flows downstream if drag force and the stemwise angle are larger fig 13 d the wood piece without a root wad flows more easily downstream because its draft is larger than that of the wood piece with a root wad such a deposition pattern which is parallel to the flow direction was observed in the experiments welber et al 2013 bertoldi et al 2014 and field observations e g piegay and gurnell 1997 in the drôme river france and by gurnell et al 2000 and bertoldi et al 2013 in the tagliamento river italy however rolling motion can be limited to occur within the artificial hydraulic structure which contains a rigid and flat channel bed this is because in the natural system most channel beds are not flat in shape owing to sediment e g sand gravel and boulder the results of the experiments and simulations showed that the presence of a root wad and the cdm had the most important roles in controlling deposition braudrick et al 1997 experimentally showed that large wood was mainly deposited at the most shallow water depth davidson et al 2015 demonstrated that root wads significantly decreased travel distance braudrick and grant 2000 merten et al 2010 wilcox and wohl 2006 and schenk et al 2014 showed that the presence of root wads is the most important determinant of mobilization and travel distance in this study the root wad particles were regarded as having a diameter larger than that of the trunk particles by a factor of 2 thus the root wad particles were more easily subjected to larger drag forces because of the larger projection area compared to the trunk particles however the root wad particles weighed more than the trunk particles which had the same density and thus they had a higher cdm compared to the trunk particles moreover we considered the change in water depth caused by the root wad effect fig 3 therefore the deposition of a larger number of wood pieces with a root wad was observed in the simulation and experimental results and the proportion of the deposited wood pieces with a root wad was clearly in good agreement with the experimental results figs 10 and 12 thus we have shown that the root wad effect is reasonably reproduced by the proposed numerical method and that wood motions can be accurately simulated using more diverse root wads size and density however the root wad effect should be carefully refined using more detailed measurements the present simulation model employed an empirical bed friction coefficient and the bed friction coefficients of the root wad particles and trunk particles were tuned using a trial and error approach through a comparison with the experimental results in addition the deposition close to the wall was not reproduced for the wood pieces without a root wad thus the motion of such wood pieces should also be improved considering several parameters such as the secondary flow and projection area of a wood piece this simulation considered a random stemwise angle of the wood piece at the starting zone because the stemwise angle cannot be controlled prior to an obstacle therefore the positions of the wood pieces and the number of wood pieces may show slightly different patterns between the simulations and experiments in the experiments we observed floating wood pieces close to the obstacle in the upstream direction these pieces did not flow downstream this may be because the variable stemwise input angle caused different projection areas for wood motion if we make the stemwise angle of the wood pieces constant before they pass the obstacle in the experiment and simulation both sets of results may show more regular patterns of wood piece deposition and advection the responses of the wood motion to the change in the channel slope exhibited an unclear pattern the debris flow on the steep channel exhibited the active motion of large wood because the steep channel can cause gravity flow through higher potential energy e g schenk et al 2014 carter 1975 pierson 1981 costa 1984 takahashi et al 1992 shieh et al 1996 imran et al 2000 shrestha et al 2012 in this steep channel the large wood can move downward more easily compared to in a mild channel such as an alluvial river because of the gravity flow such gravity flow was neglected herein because we used a mild channel slope in the simulations and experiments therefore the change in channel slope should be large to consider the gravity flow due to a steep channel this will be performed in subsequent studies in contrast the present study showed that the motion of the wood pieces is considerably affected by flow discharge wilcox and wohl 2006 conducted an experiment with a laboratory flume for investigating the flow resistance dynamics in step pool channels considering diverse wood profiles such as the effect of root wad length of wood piece and density their experiment showed that the wood pieces on the step pool channels increase the water depth because these wood pieces increase the flow resistance moreover they indicated that the position of deposited wood pieces is an important parameter for the flow resistance the wood pieces in the present study exhibited two deposition positions near walls and at the center in the flume indicating the occurrence of sliding and rolling motions due to flow discharge in addition the deposition angles of the wood pieces were clearly dependent on the flow discharge table 4 thus our study also demonstrated that the flow discharge is an important factor for large wood dynamics interestingly the experiments in this study showed simple jam formation by the deposited wood pieces fig 12 this jam formation was clearly observed when flow discharge was smaller cases 5 and 7 the jam formation could be accelerated by the first wood piece which could interrupt the motion of subsequent wood pieces several researchers e g braudrick et al 1997 welber et al 2013 bertoldi et al 2014 showed the characteristics of jam formation based on wood supply bed elevation and flow discharge through experiments however these studies only conducted a flume experiment and several researchers are attempting to investigate this using a computational model for jam formation the computational model is challenging to apply as a large wood dynamics model because multiple parameters should be considered such as wood collision large wood shape profile of the channel bed and water flow a large cpu time with a high performance workstation is required to reproduce a number of wood motions similar to the experiments for jam formation therefore our study would provide informative data for the development of the computational model considering anisotropic bed friction change in draft for wood motion and the root wad effect the present study focused on the flume scale experiment and the computational model was validated under such flume scale conditions however this model is required to be expanded into large scale cases for practical purposes thus in a future study the large wood dynamics model should be considered for the natural system associated with the large scale problem for instance the large wood dynamics model can simulate larger scale experiments welber et al 2013 bertoldi et al 2014 and river scale observations gurnell et al 2000 bertoldi et al 2013 through comparisons among the simulation results the results of the large scale experiments and the observations the applicability of the present computational model to large scale cases should be examined in a future study 6 conclusions this study developed a numerical model for simulating large wood dynamics with floating sliding and settling motions in shallow flows by coupling a depth averaged 2d flow model and a lagrange type large wood dynamics model in the present numerical model we considered the change in draft for wood motion anisotropic bed friction and the root wad effect the reproducibility of the proposed computational method is generally good in addition the method produces a reasonable simulation of the different deposition patterns in relation to the changes in flow discharge and the root wad effect based on the experiment the detailed conclusions are as follows 1 responses of the wood motion to flow discharge and channel slope a wood piece tends to move toward the side walls after touching down on a lower water depth zone because of small rolling friction such motion becomes more dominant when flow discharge decreases in addition drag force and water depth increase with flow discharge moreover the wood piece easily flows in the streamwise direction however the responses of wood motion to the change in channel slope show an unclear pattern because the employed values of channel slope are insufficient to affect wood piece motion 1 root wad effect in the experiments a wood piece can become deposited more easily if it contains a root wad because the root wad decreases the draft for the wood motion in the wood piece by lifting its head even though the weight and volume in part of the root wad are larger than the stem of the wood piece the simulation results clearly reproduce the root wad effect well however in cases of wood pieces without a root wad the experimental results only show deposited wood pieces close to the wall through these results we discovered a relationship between the deposition angle of the large wood and flow discharge the stemwise angle of the deposited wood piece becomes smaller when flow discharge is larger consequently this implies that the deposited wood angle is associated with the projection area between the flow discharge and wood piece thus further study should consider additional parameters such as the projection area of the wood piece considering the angle between the stemwise and streamwise directions 1 limitations of the developed model in the experimental and simulation results the wood piece with a root wad tends to move toward the side walls because of the smaller rolling friction coefficient we considered the shape of the wood piece stem to be a cylinder in the experiment in the simulation we considered the wood piece as a spherical particle therefore the rolling friction coefficient is relatively smaller than that of natural large wood which contains branches rough surfaces and irregularly shaped cross sections acknowledgment we are thankful to the ministry of science technology grant name mext scholarship research students and the river foundation grant no 2018 5211 042 for providing funding for this research 
