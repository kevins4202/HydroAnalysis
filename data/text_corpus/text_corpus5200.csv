index,text
26000,there has been increasing interest in using automated bioacoustics analysis to monitor the environment this involves using computational approaches to identify animals and other environmental phenomena from the sounds that they generate the volume of data being recorded for bioacoustics analyses is increasing as the scale of environmental surveys is increasing this presents significant computational demands to perform analyses these large scale analyses cannot be performed at feasible speeds using traditional computing approaches this research proposes acousticloud a system framework which represents bioacoustics processes as workflows and executes these across a cloud based system it enables fast and efficient bioacoustics analysis for a variety of scenarios the proposed system considers characteristics specific to bioacoustics processes resulting in fast execution times and high scalability an implemented prototype is found to execute a bioacoustics workflow with 10 min of audio over 10 times faster than pegasus a widely used workflow management system keywords bioacoustics system architecture distributed computing cloud computing workflow management big data 1 introduction environmental health monitoring and assessment is an important task performed by both governmental and industrial organisations while on the ground surveys have been the standard approach for monitoring species distribution and density these are costly labour intensive and cannot cover all areas of interest with equal frequency consequently there has been increasing interest in reducing the cost of environmental monitoring through the use of audio recorders for capturing sounds of different animal species this is a more effective and scalable way of assessing the health of the environment without significantly disturbing animal behaviours the area of study of animal sounds is called bioacoustics recently there has been increasing interest in using computational approaches to reduce alonso et al 2017 dema et al 2017 or even fully automate bailey et al 2017 salamon et al 2016 shamir et al 2014 the work required to analyse recordings as such automated bioacoustics tools are becoming increasingly used by researchers in environmental monitoring surveys ganchev 2015 researchers have now acquired huge amounts of data to analyse for example as of 2015 the queensland university of technology qut has acquired 20 years worth of recordings or 35 tb towsey et al 2015 and this has likely increased since this volume of data cannot be processed in reasonable times using desktop computers high performance computing approaches are needed however moving to high performance computing introduces several challenges in particular strong parallelisation effective data storage and overhead minimisation become important while high performance approaches have been utilised by the group and others dugan et al 2015 these are for specific processes and cannot be applied in many scenarios there is still a need to perform bioacoustics processes in a way that suits many different scenarios generic systems exist for parallel and distributed processing but these are not optimised for bioacoustics and often fail to natively support data parallelisation or process parallelisation can be overly overhead heavy and fail to utilise common bioacoustics processing structures to simplify process creation to address the challenges of high computational demands and the different approaches required to perform bioacoustics processing to suit many scenarios a cloud based workflow system architecture is proposed called acousticloud which breaks up bioacoustics processes into smaller processing tasks which can be arranged by users to create workflows to perform automatic analyses of large scale unsupervised bioacoustics recordings the key innovations of this architecture are representation of bioacoustics processes as workflows made up of smaller sub tasks allowing flexibility and modifiability in creating bioacoustics processes for many scenarios bioacoustics processing deployment over distributed architectures specifically tuned for bioacoustics processing allowing for fast cost effective processing and the potential to use cloud processing for further improved efficiency automatic data provenance and reuse to avoid redundant processing and examine previous outputs including outputs from intermediate processes this architecture is designed with cloud computing in mind with the capability of enabling resources to be allocated on demand to achieve high speed and low cost bioacoustics processing this will make large scale environmental surveys more accessible and cheaper for researchers even those without significant computer skills this architecture enables cost effective large scale environmental monitoring which is required to make effective conservation decisions which has been identified as an important research need a review of current biodiversity monitoring processes by proença et al 2017 differentiates between intensive studies where a small area is studied thoroughly and extensive studies where a large area is examined in much lower detail and typically targeting common species the implication is that traditional analysis methods need to compromise to survey large areas the level of compromise could be reduced using large scale bioaocustics analysis which acousticloud could provide furthermore the review also identifies that prohibitive costs result in bias towards analyses in wealthy countries and data gaps in less developed countries acousticloud could enable large scale monitoring surveys in areas that have been previously difficult to monitor an idea proposed by sugai and llusia 2019 is to use bioacoustics time capsules which could track how biodiversity and soundscape characteristics have changed over time acousticloud could be used to analyse changes in recordings which is made easier by its use of flexible workflow structures with interchangeable tasks and scalability through use of cloud resources it can perform a large variety of analysis processes on a large set of recording data for a more in depth use case consider the work of taylor et al 2018 who monitored regent honeyeaters a critically endangered southeastern australian bird species to do this they attached transmitters to 13 pairs of captive birds released these into the wild and monitored 10 nesting locations using video while this study did obtain valuable data in terms of identifying predators that cause failed nesting attempts a more expansive broader bioacoustics analysis might be able to give a more precise picture as to how to preserve the species through identifying behaviour outside of nesting and targeting an area with more individuals this requires efficient processing infrastructure and potentially unique processing tasks and workflows to be processed efficiently all of which is enabled through acousticloud this article will first examine characteristics of automated bioacoustics processing which present opportunities and challenges for executing at large scales it then describes the system architecture in detail including its goals and the mechanisms used to achieve these goals finally a prototype implementation is described and tested against pegasus an existing scientific workflow management system swfms before testing the prototype s scalability 2 requirements of processing bioacoustics workflows there are several characteristics of bioacoustics processing which in combination with each other make it distinctive from other forms of scientific processing this results in different requirements and challenges compared to other scientific analyses and these are specifically considered in the proposed architecture bioacoustics processes often follow similar workflow structures as in fig 1 but the way in which the steps of the pipeline are approached can vary greatly this is in part because the applications of bioacoustics processing are wide and the nature of recording data vary widely for example some recordings might be noisier than others and sounds of interest might have different characteristics such as frequency range duration etc this means that in any bioacoustics processing some assumptions can often be made as to how processing can be carried out to which can inform processing structures at the same time however the system still needs to be adaptable in order to work in many different scenarios the nature of the data also offers opportunities for example because audio data is continuous it can be easily split without loss of information provided that audio phenomena are not caught in between two splits this means that bioacoustics processes are easy to parallelise machines can be dedicated to processing different parts of the audio simultaneously additionally a large number of bioacoustics recordings will not contain phenomena of interest and can be discarded meaning that only a fraction of a recording needs to be processed in its entirety this means that less processing needs to be done overall but there are challenges in making this as efficient as possible for instance it cannot be known ahead of time how much audio can be discarded this results in unpredictable workloads and requires workflows to be somewhat dynamic in that tasks need to be applied to unknown quantities of bioacoustic data which can be made up of an arbitrary number of files each of arbitrary size the sizes of files can also be very small phenomena of interest such as bird calls could have a very short duration often 1 s furthermore there can be a lot of phenomena isolated over a short amount of time each of which can be considered as separate files that need to be processed independently of each other each of these files can take a very short amount of time to process but the workflow overall will need to work with large numbers of processing tasks and data will need to be queried from a large set of files this means minimising overhead and structuring data efficiently is of particular importance the continuous nature of bioacoustic recordings also means that it is possible to process it as a stream this can enable real time environmental monitoring which has wide reaching applications van parijs et al 2009 as such real time processing is an important area of bioacoustics analysis this presents several requirements and challenges in itself such as ensuring that recordings are being processed at appropriate speeds finally the same recording data can be used for multiple different analyses or improved versions of previous analyses and these could be made up of some processes that have already been computed this can even include workflow structures that are changed while being executed as such intermediate data can be stored to avoid redundant processing greatly improving efficiency assuming data are reused the system needs to track changes in all data including machine learning models so that they can be reused in subsequent executions 3 related work existing bioacoustics software platforms as well as generic systems aiming to process workflows at large scales are now examined in particular this examination considers whether these existing systems fully address the challenge of achieving scalable efficient bioacoustics processing across a large number of scenarios 3 1 existing bioacoustics platforms several platforms already exist for bioacoustic and ecoacoustic analysis these often aim to make existing methods more readily available to users and some focus on addressing the big data problem these approaches are either designed for users who already have technical knowledge in bioacoustics analysis requiring significant input or are rigidly designed to solve a limited set of problems using specific approaches existing tools do not offer the functionality to input a set of bioacoustics processes and have them all execute to give meaningful output without requiring significant effort and programming ability to serve anything more than a fraction of possible bioacoustics use cases different approaches are required to accommodate different real scenarios towsey et al 2012 therefore to enable automated bioacoustic analysis to be used by non technical users to its fullest potential a more flexible general yet nonetheless easy to use design is needed in addition research into scaling bioacoustics processing is in early stages with kaleidoscope pro wildlife acoustics 0000 ecosounds towsey et al 2012 2014b raven and raven x krein et al 2009 dugan et al 2016 and bioacoustica baker et al 2015 only recently introducing high performance computing approaches to bioacoustics processing significant research still needs to be done into how to perform bioacoustics processes as quickly and efficiently as possible 3 2 scientific workflow management systems a scientific workflow management system swfms is a system that manages the execution of scientific workflows they aim to make the construction and execution of large scale easier for researchers existing swfmss include pegasus deelman et al 2015 kepler ludäscher et al 2006 swift wilde et al 2011 galaxy giardine et al 2005 specifically for bioinformatics and taverna wolstencroft et al 2013 a summary of these swfmss is given in table 1 existing swfmss aim to make the construction and deployment of generic or domain specific workflows easier and can operate over cluster grid and sometimes cloud based systems some particularly pegasus and swift feature mechanisms to improve performance such as data reuse task clustering and coasters hategan et al 2011 notably these two systems utilise a text based interface either via the use of an api in the case of pegasus or via the use of an entirely novel language in the case of swift others utilise a graphical user interface to visually represent workflows this makes workflow construction easier up until a point though more complex workflow structures might end up being easier to construct using text based representations while current work is well developed existing systems do not appear to be optimised for bioacoustics processing furthermore some work would be needed to manage processing tasks with unpredictable outputs which could influence subsequent steps of a bioacoustics workflow additionally it is unclear if any workflows can handle processing short tasks on a very large number of short files most of them do not natively support data parallelism bux and leser 2013 which can be effectively used in bioacoustics analysis for example by having multiple machines process tasks on different chunks of audio data simultaneously there is currently little documentation about use cases similar to those found in bioacoustics scenarios while it is likely that these existing swfmss could be adapted for use in bioacoustics processing their generic nature means that any development would need to work around unneeded functionalities and background processes without fine grained control because of this a new architecture is proposed that is built specifically for bioacoustics workflows considering the opportunities and challenges that arise from bioacoustics processing without needing to understand the intricacies of systems built to solve similar but different problems 3 3 processing frameworks processing frameworks are lower level than swfmss they aim to provide a means to easily perform distributed processing of data on large scales while not complete solutions for managing workflow execution their features need to be examined for their suitability to automated bioacoustics analysis popular frameworks include mapreduce dean and ghemawat 2008 and the apache projects spark zaharia et al 2016 flink carbone et al 2015 samza noghabi et al 2017 storm zaharia et al 2012 and apex pathak et al 2016 while these frameworks and engines have features that would be useful for bioacoustics processing at scale they are far from complete solutions firstly these usually use static processing pipelines and significant work would be needed to enable more dynamic structures both for enabling users to easily construct and modify workflows and for the system itself in response to the unpredictable results of processing tasks such as activity detection which can output any number of files and possibly noise reduction processes which could completely discard files additionally data provenance is not natively supported and while there is a project to incorporate provenance into spark in particular interlandi et al 2015 this does not include mechanisms to reuse previously computed data and is more of a logging and debugging tool furthermore many of these frameworks are specialised for streaming data and do not keep permanent records this functionality is extremely useful for tracing data through a workflow for analysis and debugging purposes furthermore systems failing to support data provenance fail to take advantage of an opportunity to reuse intermediate data after initial processing which means computation power is potentially wasted repeating processing being generic tools these engines inevitably need to make efficiency compromises to support a huge variety of scenarios and anything built on top of them will be suboptimal this means that a system developed specifically for bioacoustics processing should be more efficient than adapting existing work made for general scenarios additionally because they abstract a significant amount of processing it is more difficult to evaluate and optimise different approaches when using these systems 4 system design overview this section describes acousticloud s system architecture at first the design goals for the system are specified the core components and ideas behind the system which help achieve the design goals are first outlined at a high level before the execution of a typical workflow and the components that make up the system are described in detail this architecture provides a foundation from which several tools and mechanisms could be developed to further contribute to achieving the core aims of the acousticloud system 4 1 design goals the proposed system acousticloud is designed to enable fast and efficient yet easily modifiable bioacoustics workflow processing to achieve this the following goals are identified for the system 1 simple workflow management users should be able to create and edit workflows to suit their analysis scenario importantly the process by which workflows are created and edited should require minimal effort from users they should only need to decide which tasks to use and in which order with the system handling dependencies and inputs and outputs of each processing tasks 2 cloud based execution the high volumes of data being collected for bioacoustic analysis cannot be analysed quickly using traditional computing approaches furthermore the potential of real time bioacoustics analysis for applications such as threat detection could result in changing computational demands as such cloud computing should be utilised by the system to provision resources on demand to process data efficiently and quickly 3 efficient task scheduling given that cloud computing is used to perform processing quickly consideration needs to be made as to how to schedule tasks for maximum efficiency 4 efficient resource provisioning within the cloud based system resources need to be allocated to appropriately meet the computational demands of any workflows being processed this involves minimising idle resources while ensuring all processing is being performed at necessary speeds 5 efficient data management it is important for the system to ensure that the large volume of data processed is easy to store retrieve and manipulate by both users and processes this includes storing intermediate data when it is efficient to do so meaning the cost trade offs between storing the data and recomputing it need to be considered alongside how it has been processed in this way if a workflow is changed e g users want to add a new acoustic feature then any stored intermediate data does not need to undergo any processing it has already done 6 machine learning model management machine learning models are likely to be improved over time as more labelled data enters the system as such models need to be retrained or for online models improved as new data becomes available at the users request or if a workflow using the model is still being processed classified data from existing workflows should be reclassified using the new model 4 2 system layers fig 2 describes the layers of the system and their primary responsibilities all of these are critical to the system s design in order to achieve efficiency the user layer handles user interactions including workflow construction and editing as well as data querying users will be able to construct and edit workflows using a pre existing catalogue of processing tasks the workflow management layer keeps track of when dependencies are completed such that tasks are ready for execution the common structure of automated bioacoustics workflows can be used to make these tasks easier in particular minimising the overhead of monitoring workflow execution is of utmost importance given how small some processing tasks are within workflows hence how quickly workflows need to be updated this layer is split into two components firstly workflow generation which interprets user input and generates workflows from including modelling workflow structure and dataflow this includes optimisation in the form of clustering tasks together or applying the same task to multiple files at once iterating through each rather than representing each task file pair as a separate task the analytical task management layer is in charge of managing workflow execution on a per task level this includes how resources communicate with each other including managing parallelisation and sending and receiving of data it also involves task scheduling which determines which tasks to send when to send them and to what machine firstly workflows are separated into individual tasks these tasks are then scheduled and given to resources which execute them data management is also a core component of the system it is critical to ensure that data can be queried and reused efficiently in the system this includes storing retrieving and editing huge datasets a key emphasis for this system is how data is structured in order to achieve efficiency and minimise overhead in processing tasks another core system component is cloud resource provisioning management this involves monitoring the status of resources and how well they are being utilised which affects task scheduling it also affects resource provisioning which is considered a responsibility of this component which determines whether or not computational resources need to be added or removed in response to current workloads and which configuration these resources should be in finally the cloud infrastructure represents the physical resources used in executing the workflows this system does not target any one platform and the prototype uses lower level constructs such as socket programming later it could be deployed on commercial platforms such as amazon elastic compute cloud and google cloud 4 3 core functionality this section describes how the system operates including the general flow of execution and how data is stored and reused this serves as a basis for scalable bioacoustics cloud processing but does not comprehensively cover all required functionality for true cloud processing this is designed to be expanded upon with future research in particular this section will discuss four of the core processes used to achieve the overarching design goal of delivering efficient and modifiable bioacoustics workflow processing 1 workflow creation including user input and dependency management 2 task queueing processing and output handling 3 distributed computation and data communication 4 data provenance and discovery 4 3 1 workflow creation workflow processing begins with users inputting the workflow in many swfmss users need to specify what data gets input and output from any given process in acousticloud however some assumptions are made about data flow such that users only need to specify the workflow s initial inputs followed by which tasks they want to run and in what order all tasks within acousticloud are defined in a directory these contain specifications about the type of processing they do the type of data input the type of data output and information relating to parameters from a user s perspective tasks can be viewed by category in order to help in workflow creation users simply select the tasks they want to perform in the order they wish to perform them in some circumstances tasks might be able to be performed simultaneously if they have the same data inputs and the system can account for this finer grained control functionality over data flow will also be integrated in general tasks can output one file many files an associated file such as a spectrogram a set of values e g acoustic features or a classification it is assumed that the outputs from each task are used in subsequent tasks where applicable for example after denoising the denoised file will be used in the next steps if files are split into many all of them are taken as inputs of separate executions of subsequent tasks in general task inputs are assumed to be the most recent output of the same type as the input required by the task in cases where the input is a file associated with a chunk of audio that has not been calculated yet the tasks needed to generate the associated file are implicitly added to the workflow for example if a file with an associated spectrogram is denoised the spectrogram of the newly denoised file will be calculated before being taken as input for a new task rather than the old file s spectrogram these rules while too rigid for a generic swfms make sense for the majority of bioacoustics scenarios for example most users are not going to want to split files into segments based on activity then calculate acoustic features for classification on the original files because of this the system can be used to generate workflows with less work from users than generic tools alongside tasks workflow processes will have associated audio files these might have ground truth labels which can help in building classification models they can also have combinations of labelled and unlabelled data where the labelled data are used to build a model and the unlabelled data are put through this model 4 3 2 task processing life cycle workflow progress is tracked via a database which contains information about every file including intermediate files every task every acoustic feature and every workflow every used by the workflow system the general life cycle of processing tasks is summarised in fig 3 textually it can be described as follows 1 a task specification is defined when workflow processing begins this includes the task s name any associated parameters and pre requisite tasks this is not tied to any specific file at the same time for each file the tasks that have been previously applied to it or one of its parent files and the tasks still left to be applied to it are recorded 2 a task instance is created for one file once it is ready for processing i e when all pre requisite tasks are done this is added to a pool 3 a task instance is selected from the pool and allocated to a resource i e process this is done once resources complete tasks resources will be allocated a new task once their previous task is completed 4 all data required for the processing task is sent to the resource if it does not already have it 5 the resource processes the task 6 processing output is sent to an output queue for handling this output usually does not contain the actual files output from the processing task but instead contains associated metadata such as file names and paths 7 a dedicated master process works through the output queue updating workflow progress and calculating new ready tasks this life cycle is motivated by several characteristics of the task domain firstly unlike many other scientific workflows it is not known ahead of time how many outputs there will be for some tasks particularly segmentation and activity detection tasks and the same task is likely to be processed many times for different files as such task specifications and files are separated until tasks are ready for processing secondly this life cycle has separate processes that can operate in parallel to each other thereby improving efficiency compared to something more sequential task selection task processing and output handling are considered separate processes meaning tasks can be selected for different resources at the same time as others are being processed at the same time as previous outputs are being handled thirdly this mechanism allows processing tasks from several workflows to be handled simultaneously while file metadata and task specifications are tied to workflows there is no reason why tasks from different workflows cannot be in the same queue simultaneously this presents the potential for bioacoustics processing as a service where different users could access cloud resources at the same time to do the work that they need this mechanism also enables the possibility of task scheduling this could either be done pre emptively calculating which tasks to process prior to a new task being requested or once a new task is requested this architecture is flexible to different approaches which will be explored in the future this model resembles the existing swift parallel scripting language wilde et al 2011 in that parallelism is handled automatically based on what processes users input into the system in swift many instances of a task can be parallelised for different data inputs using for each loops which is similar to how each file within a set progresses through each task in the acousticloud system acousticloud does however make more underlying assumptions about the flow of data meaning dependency management and data input and output are simpler from a user perspective 4 3 3 distributed computation and communication a key requirement for the system is to process over distributed systems this is necessary for cloud computing which has important advantages over other high performance architectures and discussed in section 4 1 acousticloud uses a master slave model but with decentralised storage to reduce communication time slaves only perform processing and store data while masters are responsible for communicating with slaves and users selecting tasks preparing and sending data and handling outputs a master machine can contain processes that are essentially slaves performing processing tasks except without the need of sending task information over a network the steps involving communication between masters and slaves are indicated in fig 3 slave machines have two types of processes one type handles the actual processing component receiving files and processing information performing tasks and sending output information back the other type simply listens for file requests and sends files to the required machine there are typically multiple instances of the former processing type and only one of the latter each process type first creates a separate connection with the master for processing dedicated to computing tasks the master immediately selects a task if there are any tasks in the queue sends the required files and sends the task information task name and parameters the slave process then performs the processing and sends the output to the master which is sent to its own queue to be processed in an independent thread this output only contains metadata when large outputs are created and not the actual files which are kept on the slave system this way communication overhead can be reduced as subsequent tasks using the newly created file can be scheduled on the slave process thereby never needing to be sent anywhere else additionally the master can process one instance of process a per slave meaning it does not need to wait for slave machines to finish before allocating different tasks to other slaves so it can be constantly communicating without delay the other type of process simply listens to the master until it gets a message to send a file to another machine and then the process does exactly that it can also be used to receive other messages such as telling a slave to transfer all files and terminate if the system is being scaled down in a cloud computing scenario in other words file transferring is an independent process to the other steps a request can come at any time to transfer files from one slave to the master and then to any other slave to then receive this mechanism resembles the successful actor director mechanism used in the kepler scientific workflow management system ludäscher et al 2006 where there are multiple processing components whose execution is dictated by a director data communication is slightly different however as using analogies from the kepler model the actors are not necessarily connected to each other for direct communication and communication is instead handled by intermediate processes this separation of communication processes from workflow tasks simplifies the architecture distributed storage has also been implemented in successful systems before such as in apache spark s resilient distributed datasets rdds zaharia et al 2016 where data can be stored in many locations and can even be stored for repeated computations 4 3 4 data provenance and discovery data provenance is implicitly handled by the nature of how processing works metadata relating to each file are tracked including which tasks have been applied to a given file what the outputs of these tasks were and what workflows they were used in intermediate data are never deleted during processing nonetheless in order to simplify the process of discovering previous intermediate data separate data tables explicitly store information about all tasks that have been performed on a parent file or its children and what files were output from the tasks this way if the same task is applied to the same file in multiple workflows the intermediate outputs can be easily retrieved and used in subsequent processing steps skipping repeating any tasks notably tasks which do the same processing i e they call the same script or function but have different parameters and or dependencies are considered different tasks thereby ensuring this provenance system is consistent upon any newly input workflow the system checks to see if any files have been used before and if they have if any tasks in the workflow have been applied to previous workflows before if they have any workflow task completed previously is considered complete in the new workflow and the old tasks outputs are used for the new workflow 5 experimental set up to evaluate the potential of acousticloud a prototype was developed and evaluated both on its own under different system architectures across different workflows and with an implementation of a bioacoustics workflow using pegasus a popular swfms this prototype is evaluated to determine how well it achieves the following objectives minimise total execution time achieve linear scalability horizontal and vertical execution time reduces by half when double the resources are utilised minimise communication between machines minimise time spent reading and writing files and the database avoid redundant processing of tasks 5 1 prototype the prototype developed for testing implements most of the core functionality outlined in section 4 this includes being able to accept a workflow structure and compute this workflow across arbitrary datasets across multiple multithreaded virtual machines vms it does not contain any functionality relating to the cloud resource provisioning management layer and a very limited input interface only intended to aid in research because of the demand for the system to work on large scales non relational databases which were conceived with large scale big data processing in mind are likely a superior option compared to traditional relational databases such as sql for this scenario chodorow 2013 as such mongodb is the basis for data representation in the prototype data interactions such as strong provenance information fetching the directory of tasks calculating ready tasks and adding and selecting from the task queue are all handled through a mongodb database the prototype utilises a scheduling approach where priority is given to tasks using data that are already on the resource being scheduled to a request is made for a task to be selected for a target vm if there is a task available that does not require any data to be transferred to the vm in question then it is selected if there are no such tasks an arbitrary task is selected and data are transferred accordingly this simple task selection mechanism could be improved in future all processes including those to handle the workflow management and bioacoustics processing itself are written in python several external libraries are used in these process implementations including librosa mcfee et al 2015 scikit learn pedregosa et al 2011 and pymongo 5 2 experimental methodology evaluation involves processing several bioacoustics workflows in order on a distributed system prototype these workflows were applied on a 10 min recording generated using the scaper program developed by salamon et al 2017 this tool automatically generates soundscapes with annotations which can be used as training data to build classification like workflows the soundscapes were generated using the sounds of several birds were taken from recordings on the xeno canto website which is an open source repository where users upload recordings of bird sounds these sounds were randomly inserted and manipulated by scaper pink noise was inserted to simulated the background noise of the soundscapes 5 2 1 workflows used for evaluation a set of workflows is computed during each test each of which should help gain some insight into how well the system works processing of each workflow is mutually exclusive i e they do not explicitly interact with each other although some processes from earlier workflows are identical to later workflows meaning intermediate data can be reused workflow 1 simple call detector and classifier split to 5 s downsample to 22 05 khz convert to mono hilbert follower activity detector potamitis et al 2014 32 mel frequency cepstral coefficients mfccs random forest classifier breiman 2001 workflow 2 is the same as workflow 1 except audio is split into 10 s chunks at the start functionally acting in the same way workflow 3 complex call detector and classifier split to 5 s downsample to 22 05 khz convert to mono 500 hz high pass filter mmse stsa denoising ephraim and malah 1984 brown et al 2017 hilbert follower activity detector 32 mfccs spectral cover towsey et al 2014a acoustic complexity index aci pieretti et al 2011 spectrum 10 frequency bins uniformly distributed between 0 10 khz spectral entropy sueur et al 2008 spectrum 7 frequency bins uniformly distributed between 1 8 khz k nearest neighbour classifier knn peterson 2009 k 5 workflow 4 is the same as workflow 3 but with median clipping another form of denoising inserted after mmse stsa denoising this has the same objectives but with extra denoising to better handle very noisy recordings workflow 5 has the same pre processing as workflow 4 but has no activity detection 5 second chunks are classified from there the rest of the workflow includes all features from workflow 3 4 except spectral cover spectral cover spectrum 19 frequency bins uniformly distributed between 500 10000 hz segment length root mean square rms of the amplitude knn k 10 this aims to not isolate individual calls but determine the presence and classify calls of any animals within a 5 s duration these workflows are chosen to have the same basic structure as typical classification workflows while using processes used in previous bioacoustics analyses briggs et al 2012 brown et al 2017 2019 lasseck 2013 pieretti et al 2011 potamitis et al 2014 sueur et al 2008 towsey et al 2014a these are likely not the most effective workflow choices to perform effective classification but are selected to give different levels of data reuse different numbers of chunks to process and different numbers of tasks each of which have different computational intensities they nonetheless resemble real bioacoustics processing pipelines used in previous research with each processing task being used in previous bioacoustics analyses and have a typical bioacoustics workflow structure all classifiers are multi class meaning that they can output more than one class for each segment which is useful particularly if calls overlap 5 2 2 cloud architecture used in evaluations the primary motivation of this testing is to evaluate the viability of the system particularly in terms of the data model and workflow processing mechanisms the following questions are addressed in testing how does the system compare to an existing leading workflow system pegasus is the workflow system vertically and horizontally scalable does data reuse improve execution times and if so by how much how much data is sent between vms is the scalability of the workflow system dependent on the structure of the workflows initially tests are performed on both pegasus and acousticloud for just workflow 5 for a single vm 8 core architecture to compare them this is executing using default settings on pegasus except for task clustering where tasks are grouped together such that there are 8 clusters of tasks created for each process where applicable i e 8 resample tasks 8 mmse stsa tasks etc as opposed to 120 tasks or one per file execution time is measured in each case existing bioacoustics processing suites do not enable the customisation enabled by acousticloud and cannot easily be directly compared this output of this comparison will inform if more tests are needed to determine where acousticloud and pegasus are stronger if more tests are not needed testing will evaluate general scalability and efficiency in order to address these questions the following architectures are tested one 8 vcpu vm 8 one 4 vcpu vm 4 one 2 vcpu vm 2 two 2 vcpu vms 2 2 one 4 vcpu vm and two 2 vcpu vms 4 2 2 one 8 vcpu one 4 vcpu vms and two 2 vcpu vms 8 4 2 2 these machines are created using the nectar cloud their specifications are 8 vcpu 16 gb ram 4 vcpu 8 gb ram and 2 vcpu 4 gb ram all machines run on ubuntu 18 04 1 lts in reality processing these workflows using the system is cpu bound and most of the ram is not utilised although this could change in the future with optimisation to keep data in ram to prevent reads and writes from storage typically approximately 600 700 mb of ram is in use at any time with approximately 2 gb being cached at any given point when processing test workflows on one machine each vm has 30 gb of storage which is easily sufficient for the testing performed in all cases where there is more than one vm the vm with the highest number of vcpus is selected as the master each vm starts as many slave processes as there are vcpus on the machine including the master and the master starts up half as many processes as there are vcpus i e on the master machine itself for handling result outputs this appears to work best after some small tests for each architecture the six workflows are computed 10 times in sequence the database and data are cleared whenever a new repetition of the six workflows starts tests measure the execution time of each workflow for each repetition to determine the system s scalability in addition after each workflow step the size of data stored on all machines is summed for each workflow this shows how much data has been sent between machines since data is never deleted additional tests are performed with the 8 and 4 architectures where data is deleted after every workflow computed effectively disabling data reuse in order to determine how much data reuse improves computation the architecture is specifically made with reuse in mind so this will not take into account any extra overhead due to adding reuse as a requirement but the amount of computation saved gives an idea as to whether reuse is worthwhile 6 experimental results and analysis 6 1 comparison with pegasus wms fig 4 shows a performance comparison between acousticloud and pegasus for an 8 core vm this was repeated 10 times in each case pegasus overhead is simply too high for processing bioacoustics workflows similar to those being tested given that no bioacoustics workflow tested executed nearly as slow even on 2 cores compared to pegasus with 8 this test can be considered conclusive at least without significant reconfiguration and restructuring of pegasus workflows hence no further tests are conducted using pegasus the slow performance is in part due to the implementation s reliance on importing python libraries to perform many bioacoustics tasks acousticloud only needs to import the libraries once whereas pegasus needs to import required libraries once per execution and there are over 1000 task executions in the workflow rewriting the tasks in a way that avoids library imports and maybe even python altogether would likely improve execution time significantly in pegasus but this demonstrates a limitation of the system furthermore workflow creation in pegasus is more difficult than with acousticloud most significantly because inputs and outputs need to be explicitly stated for each task which would make tasks with unpredictable outputs such as activity detection more difficult to work into the system 6 2 scalability of the proposed system scalability refers to the ability of a system to maintain processing efficiency regardless of scale in an ideal scenario the system should process workflows twice as quickly with twice as many resources it is important to get as close to this ideal as possible as this means that the system is capable of efficiently processing workflows at any scale two types of scalability are evaluated vertical scalability processing capacity is increased by using a more powerful machine and horizontal scalability where processing capacity is increased by adding more comparatively less powerful machines 6 2 1 vertical scalability fig 5 shows how the system scales with the number of virtual cores because of the greatly different execution times of each workflow times are shown in terms of their improvement rate compared to the average execution time for two core execution these mean times are shown in table 2 these improvement rates are calculated by dividing the mean 2 core execution times for a given workflow by the execution time of any given evaluation in a theoretically ideal scenario the 4 core architecture would have an improvement rate of 2 and the 8 core architecture would have an improvement rate of 4 scalability appears excellent for moving from 2 cores to 4 cores but is not quite as good for moving from 4 cores to 8 although this does depend on the workflow notably workflows 3 and 4 show mediocre scalability whereas the others scale reasonably well a reason for this might be extra data overhead arising from very small segments being extracted during activity detection and very small acoustic feature calculation tasks workflow 5 s relatively strong scalability in comparison to workflows 3 and 4 support this particularly given workflow 5 uses fixed 5 second long chunks which results in fewer but longer processes overall compared to splitting based on activity which can result in many very short chunks overall it appears the system prototype does benefit from using more powerful machines but the benefit is somewhat workflow dependent and vertical scalability is often non linear 6 2 2 horizontal scalability fig 6 shows how the system scales as more vms are added the 4 core architecture is made up of two 2 core vms the 8 core architecture adds a 4 core machine to this and the 16 core architecture adds an 8 core vm on top of the 8 core architecture the improvement rates are once again relative to the single 2 core architecture it might be expected that because of the time required to communicate between different vms that the system s horizontal scalability would be lower than its vertical scalability however the opposite appears to be true with even the 16 core architecture improving linearly for some workflows compared to the 8 core architecture a glance back to fig 5 shows that architectures with multiple machines outperform architectures with single machines but the same number of virtual cores there are at least two reasons why this might be the case firstly in single machine architectures the same vm is doing all data management and processing tasks which could be overloading the system resulting in inefficiencies due to thread switching whereas in architectures with multiple vms slave machines are dedicated entirely to processing which might be more efficient secondly this workflow system relies heavily on file reads and writes and having multiple machines might reduce i o bottlenecks in general however this shows that the system can scale effectively up to 16 cores although the degree of scalability does depend on the workflow compared to the vertical scalability test workflows 4 and 5 scale noticeably better while workflow 3 is again the least scalable 6 3 communication overhead minimisation another aspect worth considering is communication overhead these tests were ran on vms that all operate from the same location so the communication overhead is very low in this testing but a scenario where this system would be deployed in a cloud environment with machines in different locations would incur higher overhead as such minimising the amount of communication is important in order to test communication overhead the total storage in the system was measured both for single machine architectures and multiple machine architectures because the system stores everything the difference between the total storage sizes of the multiple machine and single machine architectures shows the total communication overhead assuming the system is functioning correctly and not sending the same data multiple times this does not include messages sent between masters and slaves concerning what tasks to do and what outputs there are from tasks these are assumed to be very small given these are mostly in the form of lists with five or less values in the form of numbers or small strings almost certainly making up less than 10 mb of data overall the total data storage size after each workflow for any single machine architecture is shown in table 3 subsequent analysis will look at the percentage of duplicated data which is determined based on this table this determines the communication overhead communication overhead output is shown in fig 7 if all data was to be sent back and forth between slaves and masters for every task one would expect the overhead to be around 50 for each case given the master itself processes approximately 50 of the data in each instance remembering that the 4 vcpu case has a 2 core master the 8 vcpu has a 4 core master and the 16 vcpu case has an 8 core master in all workflows particularly the later workflows which have more intermediate files avoiding transferring files reduces the communication overhead to around 20 for all multi machine architectures that is of all data generated by the workflow processing only 20 is ever sent between machines in real terms this means that over 300 mb of communication is saved by keeping data on slaves and avoiding sending it to masters until it is needed this still means that around 300 mb of data is sent between machines by the end of workflow 5 this is equivalent to about 1 h of audio in wav format with mono 22 05 khz sample rate and 32 bit floating point samples which is used for most of the processing in the tests 6 4 redundant processing optimisation another test determines if reusing data saved computation time of course the amount of reuse possible will be dependent on the application a workflow effectiveness comparison where multiple workflows are to determine which one works best could benefit greatly from data reuse but a user with one specific workflow in mind will get no benefit and might even see a performance loss because of extra overhead involved with storing data in the database so that it can be reused fig 8 shows the effect of data reuse on execution times this shows that the benefit of reuse greatly depends on the workflow being used and even the number of cores workflows 1 and 2 cannot reuse any data which is reflected in the fact that execution times are the same with and without reuse workflow 3 can reuse split and resampled data which does improve execution times slightly with 4 vcpus but not with 8 the data overhead trade off is felt in this case for workflow 4 mmse stsa denoising can also be skipped which results in a more significant improvement even with 8 cores the observed difference in execution time is statistically significant t test p 0 01 for reuse vs no reuse for 8 cores or workflow 4 workflow 5 sees a huge difference between reusing and not reusing data with the 8 core case taking only slightly less time than the 2 core case when data was reused improvement rate over 2 cores is close to 1 in other words reusing data saved almost 6 cores worth of processing power in that case workflow 5 can take intermediate output from denoising tasks in workflow 4 which are the most computationally intensive tasks in the workflow overall this analysis shows that database reuse does improve execution times given the right processing scenario the value of reusing data is scenario dependent however and there is some overhead cost which is likely slightly underrepresented in this analysis 6 5 key experimental findings the findings in relation to the acousticloud system prototype can be summarised as follows the prototype demonstrates strong horizontal scalability and scales better horizontally than vertically scalability is dependent on workflows with improvement rates on a 4 vm 16 vcpu architecture varying between 3 6 times over a 1 vm 2 vcpu architecture depending on the workflow scalability is best when workflow tasks are computationally intensive and data chunks are kept somewhat large bottlenecks are more significant if the time taken to perform given processes is small communication overhead is reduced effectively from a theoretical 50 of data sent between machines if all data was sent back and forth to around 20 30 regardless of the number of vcpus vms although this could change if the power ratio between the master and slave cpus were different by keeping data on vms until they are needed by other vms reusing data effectively reduces computation time in some scenarios but not in others in particular it helps significantly if it can be used to avoid computationally intensive tasks its impact is more felt on less powerful architectures these findings all suggest that the acousticloud architecture presents a promising approach to perform flexible and efficient large scale bioacoustics processing with the addition of cloud capabilities in the future this could be used to process large numbers of diverse bioacoustics workflows simultaneously at low cost 7 conclusion this work discussed a new architecture for performing efficient yet flexible large scale bioacoustics processing using distributed computing which can later be adapted to work with cloud infrastructures this system has several features such as fast data provenance reuse mechanism reduced communication overhead over multiple machines and relatively small processing overhead particularly when compared to more generic workflow management systems such as pegasus this architecture takes advantage of characteristics of bioacoustics processing such as easy data parallelisation while mitigating the effects of some disadvantages such as occasional very short processing times for a large number of tasks testing of a prototype implementation of the architecture showed that the system is horizontally scalable and can effectively reuse data to perform equal processing on smaller numbers of vcpus in some scenarios it is also found to perform significantly better than pegasus a commonly used workflow management system with execution times approximately 8 times lower using its default configuration while also requiring less work to adapt for particularly in terms of managing inputs and outputs in the future dynamic resource provisioning approaches could be examined to determine to correct processing capacity required for handling bioacoustic workflows particularly when data is being streamed in real time task scheduling optimisation could also be examined as the prototype s approach is very basic additionally as far as data provenance is concerned examining what intermediate data are worth storing and when it is more efficient to simply recompute it as well as whether data compression could help storage and communication efficiency research could also consider which workflows are best suited for different scenarios both in terms of efficiency and accuracy of results in particular automatic evaluation of bioacoustics workflows could be done in the future using the system given it can swap tasks easily and reuse data effectively once this additional research has been done acousticloud could be deployed in many real world scenarios in particular it could be deployed in large scale biodiversity monitoring applications both in real time and in batches for example after the 2019 2020 australian bushfire season it is of great interest to monitor animal populations that have been severely affected a system with acousticloud s characteristics would be ideal for monitoring vocalising species due of its flexibility in workflow creation allowing specific workflows to be created to specialise for different species or audio phenomena and its ability to efficiently process large scale data software availability research code used in prototype evaluation is available at https sourceforge net projects acousticlod research code due to licencing issues the generated audio used for evaluation cannot be shared but similar datasets can be generated using the methods described in the paper credit authorship contribution statement alexander brown conceptualization methodology software formal analysis investigation visualization writing original draft writing review editing saurabh garg conceptualization supervision writing review editing james montgomery conceptualization supervision writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements special thanks to mutaz barika for providing valuable feedback to improve the manuscript as well as helping in describing the structure of the system architecture recording data used for sample recordings was generated based on recordings taken from the xeno canto website www xeno canto org by recordists greg mclachlan nick talbot marc anderson frank lambert oswaldo cortes pradnyavant man mandar bhagat fernand deroussen and sander lagerveld the lead author is supported by the australian government s research training program rtp 
26000,there has been increasing interest in using automated bioacoustics analysis to monitor the environment this involves using computational approaches to identify animals and other environmental phenomena from the sounds that they generate the volume of data being recorded for bioacoustics analyses is increasing as the scale of environmental surveys is increasing this presents significant computational demands to perform analyses these large scale analyses cannot be performed at feasible speeds using traditional computing approaches this research proposes acousticloud a system framework which represents bioacoustics processes as workflows and executes these across a cloud based system it enables fast and efficient bioacoustics analysis for a variety of scenarios the proposed system considers characteristics specific to bioacoustics processes resulting in fast execution times and high scalability an implemented prototype is found to execute a bioacoustics workflow with 10 min of audio over 10 times faster than pegasus a widely used workflow management system keywords bioacoustics system architecture distributed computing cloud computing workflow management big data 1 introduction environmental health monitoring and assessment is an important task performed by both governmental and industrial organisations while on the ground surveys have been the standard approach for monitoring species distribution and density these are costly labour intensive and cannot cover all areas of interest with equal frequency consequently there has been increasing interest in reducing the cost of environmental monitoring through the use of audio recorders for capturing sounds of different animal species this is a more effective and scalable way of assessing the health of the environment without significantly disturbing animal behaviours the area of study of animal sounds is called bioacoustics recently there has been increasing interest in using computational approaches to reduce alonso et al 2017 dema et al 2017 or even fully automate bailey et al 2017 salamon et al 2016 shamir et al 2014 the work required to analyse recordings as such automated bioacoustics tools are becoming increasingly used by researchers in environmental monitoring surveys ganchev 2015 researchers have now acquired huge amounts of data to analyse for example as of 2015 the queensland university of technology qut has acquired 20 years worth of recordings or 35 tb towsey et al 2015 and this has likely increased since this volume of data cannot be processed in reasonable times using desktop computers high performance computing approaches are needed however moving to high performance computing introduces several challenges in particular strong parallelisation effective data storage and overhead minimisation become important while high performance approaches have been utilised by the group and others dugan et al 2015 these are for specific processes and cannot be applied in many scenarios there is still a need to perform bioacoustics processes in a way that suits many different scenarios generic systems exist for parallel and distributed processing but these are not optimised for bioacoustics and often fail to natively support data parallelisation or process parallelisation can be overly overhead heavy and fail to utilise common bioacoustics processing structures to simplify process creation to address the challenges of high computational demands and the different approaches required to perform bioacoustics processing to suit many scenarios a cloud based workflow system architecture is proposed called acousticloud which breaks up bioacoustics processes into smaller processing tasks which can be arranged by users to create workflows to perform automatic analyses of large scale unsupervised bioacoustics recordings the key innovations of this architecture are representation of bioacoustics processes as workflows made up of smaller sub tasks allowing flexibility and modifiability in creating bioacoustics processes for many scenarios bioacoustics processing deployment over distributed architectures specifically tuned for bioacoustics processing allowing for fast cost effective processing and the potential to use cloud processing for further improved efficiency automatic data provenance and reuse to avoid redundant processing and examine previous outputs including outputs from intermediate processes this architecture is designed with cloud computing in mind with the capability of enabling resources to be allocated on demand to achieve high speed and low cost bioacoustics processing this will make large scale environmental surveys more accessible and cheaper for researchers even those without significant computer skills this architecture enables cost effective large scale environmental monitoring which is required to make effective conservation decisions which has been identified as an important research need a review of current biodiversity monitoring processes by proença et al 2017 differentiates between intensive studies where a small area is studied thoroughly and extensive studies where a large area is examined in much lower detail and typically targeting common species the implication is that traditional analysis methods need to compromise to survey large areas the level of compromise could be reduced using large scale bioaocustics analysis which acousticloud could provide furthermore the review also identifies that prohibitive costs result in bias towards analyses in wealthy countries and data gaps in less developed countries acousticloud could enable large scale monitoring surveys in areas that have been previously difficult to monitor an idea proposed by sugai and llusia 2019 is to use bioacoustics time capsules which could track how biodiversity and soundscape characteristics have changed over time acousticloud could be used to analyse changes in recordings which is made easier by its use of flexible workflow structures with interchangeable tasks and scalability through use of cloud resources it can perform a large variety of analysis processes on a large set of recording data for a more in depth use case consider the work of taylor et al 2018 who monitored regent honeyeaters a critically endangered southeastern australian bird species to do this they attached transmitters to 13 pairs of captive birds released these into the wild and monitored 10 nesting locations using video while this study did obtain valuable data in terms of identifying predators that cause failed nesting attempts a more expansive broader bioacoustics analysis might be able to give a more precise picture as to how to preserve the species through identifying behaviour outside of nesting and targeting an area with more individuals this requires efficient processing infrastructure and potentially unique processing tasks and workflows to be processed efficiently all of which is enabled through acousticloud this article will first examine characteristics of automated bioacoustics processing which present opportunities and challenges for executing at large scales it then describes the system architecture in detail including its goals and the mechanisms used to achieve these goals finally a prototype implementation is described and tested against pegasus an existing scientific workflow management system swfms before testing the prototype s scalability 2 requirements of processing bioacoustics workflows there are several characteristics of bioacoustics processing which in combination with each other make it distinctive from other forms of scientific processing this results in different requirements and challenges compared to other scientific analyses and these are specifically considered in the proposed architecture bioacoustics processes often follow similar workflow structures as in fig 1 but the way in which the steps of the pipeline are approached can vary greatly this is in part because the applications of bioacoustics processing are wide and the nature of recording data vary widely for example some recordings might be noisier than others and sounds of interest might have different characteristics such as frequency range duration etc this means that in any bioacoustics processing some assumptions can often be made as to how processing can be carried out to which can inform processing structures at the same time however the system still needs to be adaptable in order to work in many different scenarios the nature of the data also offers opportunities for example because audio data is continuous it can be easily split without loss of information provided that audio phenomena are not caught in between two splits this means that bioacoustics processes are easy to parallelise machines can be dedicated to processing different parts of the audio simultaneously additionally a large number of bioacoustics recordings will not contain phenomena of interest and can be discarded meaning that only a fraction of a recording needs to be processed in its entirety this means that less processing needs to be done overall but there are challenges in making this as efficient as possible for instance it cannot be known ahead of time how much audio can be discarded this results in unpredictable workloads and requires workflows to be somewhat dynamic in that tasks need to be applied to unknown quantities of bioacoustic data which can be made up of an arbitrary number of files each of arbitrary size the sizes of files can also be very small phenomena of interest such as bird calls could have a very short duration often 1 s furthermore there can be a lot of phenomena isolated over a short amount of time each of which can be considered as separate files that need to be processed independently of each other each of these files can take a very short amount of time to process but the workflow overall will need to work with large numbers of processing tasks and data will need to be queried from a large set of files this means minimising overhead and structuring data efficiently is of particular importance the continuous nature of bioacoustic recordings also means that it is possible to process it as a stream this can enable real time environmental monitoring which has wide reaching applications van parijs et al 2009 as such real time processing is an important area of bioacoustics analysis this presents several requirements and challenges in itself such as ensuring that recordings are being processed at appropriate speeds finally the same recording data can be used for multiple different analyses or improved versions of previous analyses and these could be made up of some processes that have already been computed this can even include workflow structures that are changed while being executed as such intermediate data can be stored to avoid redundant processing greatly improving efficiency assuming data are reused the system needs to track changes in all data including machine learning models so that they can be reused in subsequent executions 3 related work existing bioacoustics software platforms as well as generic systems aiming to process workflows at large scales are now examined in particular this examination considers whether these existing systems fully address the challenge of achieving scalable efficient bioacoustics processing across a large number of scenarios 3 1 existing bioacoustics platforms several platforms already exist for bioacoustic and ecoacoustic analysis these often aim to make existing methods more readily available to users and some focus on addressing the big data problem these approaches are either designed for users who already have technical knowledge in bioacoustics analysis requiring significant input or are rigidly designed to solve a limited set of problems using specific approaches existing tools do not offer the functionality to input a set of bioacoustics processes and have them all execute to give meaningful output without requiring significant effort and programming ability to serve anything more than a fraction of possible bioacoustics use cases different approaches are required to accommodate different real scenarios towsey et al 2012 therefore to enable automated bioacoustic analysis to be used by non technical users to its fullest potential a more flexible general yet nonetheless easy to use design is needed in addition research into scaling bioacoustics processing is in early stages with kaleidoscope pro wildlife acoustics 0000 ecosounds towsey et al 2012 2014b raven and raven x krein et al 2009 dugan et al 2016 and bioacoustica baker et al 2015 only recently introducing high performance computing approaches to bioacoustics processing significant research still needs to be done into how to perform bioacoustics processes as quickly and efficiently as possible 3 2 scientific workflow management systems a scientific workflow management system swfms is a system that manages the execution of scientific workflows they aim to make the construction and execution of large scale easier for researchers existing swfmss include pegasus deelman et al 2015 kepler ludäscher et al 2006 swift wilde et al 2011 galaxy giardine et al 2005 specifically for bioinformatics and taverna wolstencroft et al 2013 a summary of these swfmss is given in table 1 existing swfmss aim to make the construction and deployment of generic or domain specific workflows easier and can operate over cluster grid and sometimes cloud based systems some particularly pegasus and swift feature mechanisms to improve performance such as data reuse task clustering and coasters hategan et al 2011 notably these two systems utilise a text based interface either via the use of an api in the case of pegasus or via the use of an entirely novel language in the case of swift others utilise a graphical user interface to visually represent workflows this makes workflow construction easier up until a point though more complex workflow structures might end up being easier to construct using text based representations while current work is well developed existing systems do not appear to be optimised for bioacoustics processing furthermore some work would be needed to manage processing tasks with unpredictable outputs which could influence subsequent steps of a bioacoustics workflow additionally it is unclear if any workflows can handle processing short tasks on a very large number of short files most of them do not natively support data parallelism bux and leser 2013 which can be effectively used in bioacoustics analysis for example by having multiple machines process tasks on different chunks of audio data simultaneously there is currently little documentation about use cases similar to those found in bioacoustics scenarios while it is likely that these existing swfmss could be adapted for use in bioacoustics processing their generic nature means that any development would need to work around unneeded functionalities and background processes without fine grained control because of this a new architecture is proposed that is built specifically for bioacoustics workflows considering the opportunities and challenges that arise from bioacoustics processing without needing to understand the intricacies of systems built to solve similar but different problems 3 3 processing frameworks processing frameworks are lower level than swfmss they aim to provide a means to easily perform distributed processing of data on large scales while not complete solutions for managing workflow execution their features need to be examined for their suitability to automated bioacoustics analysis popular frameworks include mapreduce dean and ghemawat 2008 and the apache projects spark zaharia et al 2016 flink carbone et al 2015 samza noghabi et al 2017 storm zaharia et al 2012 and apex pathak et al 2016 while these frameworks and engines have features that would be useful for bioacoustics processing at scale they are far from complete solutions firstly these usually use static processing pipelines and significant work would be needed to enable more dynamic structures both for enabling users to easily construct and modify workflows and for the system itself in response to the unpredictable results of processing tasks such as activity detection which can output any number of files and possibly noise reduction processes which could completely discard files additionally data provenance is not natively supported and while there is a project to incorporate provenance into spark in particular interlandi et al 2015 this does not include mechanisms to reuse previously computed data and is more of a logging and debugging tool furthermore many of these frameworks are specialised for streaming data and do not keep permanent records this functionality is extremely useful for tracing data through a workflow for analysis and debugging purposes furthermore systems failing to support data provenance fail to take advantage of an opportunity to reuse intermediate data after initial processing which means computation power is potentially wasted repeating processing being generic tools these engines inevitably need to make efficiency compromises to support a huge variety of scenarios and anything built on top of them will be suboptimal this means that a system developed specifically for bioacoustics processing should be more efficient than adapting existing work made for general scenarios additionally because they abstract a significant amount of processing it is more difficult to evaluate and optimise different approaches when using these systems 4 system design overview this section describes acousticloud s system architecture at first the design goals for the system are specified the core components and ideas behind the system which help achieve the design goals are first outlined at a high level before the execution of a typical workflow and the components that make up the system are described in detail this architecture provides a foundation from which several tools and mechanisms could be developed to further contribute to achieving the core aims of the acousticloud system 4 1 design goals the proposed system acousticloud is designed to enable fast and efficient yet easily modifiable bioacoustics workflow processing to achieve this the following goals are identified for the system 1 simple workflow management users should be able to create and edit workflows to suit their analysis scenario importantly the process by which workflows are created and edited should require minimal effort from users they should only need to decide which tasks to use and in which order with the system handling dependencies and inputs and outputs of each processing tasks 2 cloud based execution the high volumes of data being collected for bioacoustic analysis cannot be analysed quickly using traditional computing approaches furthermore the potential of real time bioacoustics analysis for applications such as threat detection could result in changing computational demands as such cloud computing should be utilised by the system to provision resources on demand to process data efficiently and quickly 3 efficient task scheduling given that cloud computing is used to perform processing quickly consideration needs to be made as to how to schedule tasks for maximum efficiency 4 efficient resource provisioning within the cloud based system resources need to be allocated to appropriately meet the computational demands of any workflows being processed this involves minimising idle resources while ensuring all processing is being performed at necessary speeds 5 efficient data management it is important for the system to ensure that the large volume of data processed is easy to store retrieve and manipulate by both users and processes this includes storing intermediate data when it is efficient to do so meaning the cost trade offs between storing the data and recomputing it need to be considered alongside how it has been processed in this way if a workflow is changed e g users want to add a new acoustic feature then any stored intermediate data does not need to undergo any processing it has already done 6 machine learning model management machine learning models are likely to be improved over time as more labelled data enters the system as such models need to be retrained or for online models improved as new data becomes available at the users request or if a workflow using the model is still being processed classified data from existing workflows should be reclassified using the new model 4 2 system layers fig 2 describes the layers of the system and their primary responsibilities all of these are critical to the system s design in order to achieve efficiency the user layer handles user interactions including workflow construction and editing as well as data querying users will be able to construct and edit workflows using a pre existing catalogue of processing tasks the workflow management layer keeps track of when dependencies are completed such that tasks are ready for execution the common structure of automated bioacoustics workflows can be used to make these tasks easier in particular minimising the overhead of monitoring workflow execution is of utmost importance given how small some processing tasks are within workflows hence how quickly workflows need to be updated this layer is split into two components firstly workflow generation which interprets user input and generates workflows from including modelling workflow structure and dataflow this includes optimisation in the form of clustering tasks together or applying the same task to multiple files at once iterating through each rather than representing each task file pair as a separate task the analytical task management layer is in charge of managing workflow execution on a per task level this includes how resources communicate with each other including managing parallelisation and sending and receiving of data it also involves task scheduling which determines which tasks to send when to send them and to what machine firstly workflows are separated into individual tasks these tasks are then scheduled and given to resources which execute them data management is also a core component of the system it is critical to ensure that data can be queried and reused efficiently in the system this includes storing retrieving and editing huge datasets a key emphasis for this system is how data is structured in order to achieve efficiency and minimise overhead in processing tasks another core system component is cloud resource provisioning management this involves monitoring the status of resources and how well they are being utilised which affects task scheduling it also affects resource provisioning which is considered a responsibility of this component which determines whether or not computational resources need to be added or removed in response to current workloads and which configuration these resources should be in finally the cloud infrastructure represents the physical resources used in executing the workflows this system does not target any one platform and the prototype uses lower level constructs such as socket programming later it could be deployed on commercial platforms such as amazon elastic compute cloud and google cloud 4 3 core functionality this section describes how the system operates including the general flow of execution and how data is stored and reused this serves as a basis for scalable bioacoustics cloud processing but does not comprehensively cover all required functionality for true cloud processing this is designed to be expanded upon with future research in particular this section will discuss four of the core processes used to achieve the overarching design goal of delivering efficient and modifiable bioacoustics workflow processing 1 workflow creation including user input and dependency management 2 task queueing processing and output handling 3 distributed computation and data communication 4 data provenance and discovery 4 3 1 workflow creation workflow processing begins with users inputting the workflow in many swfmss users need to specify what data gets input and output from any given process in acousticloud however some assumptions are made about data flow such that users only need to specify the workflow s initial inputs followed by which tasks they want to run and in what order all tasks within acousticloud are defined in a directory these contain specifications about the type of processing they do the type of data input the type of data output and information relating to parameters from a user s perspective tasks can be viewed by category in order to help in workflow creation users simply select the tasks they want to perform in the order they wish to perform them in some circumstances tasks might be able to be performed simultaneously if they have the same data inputs and the system can account for this finer grained control functionality over data flow will also be integrated in general tasks can output one file many files an associated file such as a spectrogram a set of values e g acoustic features or a classification it is assumed that the outputs from each task are used in subsequent tasks where applicable for example after denoising the denoised file will be used in the next steps if files are split into many all of them are taken as inputs of separate executions of subsequent tasks in general task inputs are assumed to be the most recent output of the same type as the input required by the task in cases where the input is a file associated with a chunk of audio that has not been calculated yet the tasks needed to generate the associated file are implicitly added to the workflow for example if a file with an associated spectrogram is denoised the spectrogram of the newly denoised file will be calculated before being taken as input for a new task rather than the old file s spectrogram these rules while too rigid for a generic swfms make sense for the majority of bioacoustics scenarios for example most users are not going to want to split files into segments based on activity then calculate acoustic features for classification on the original files because of this the system can be used to generate workflows with less work from users than generic tools alongside tasks workflow processes will have associated audio files these might have ground truth labels which can help in building classification models they can also have combinations of labelled and unlabelled data where the labelled data are used to build a model and the unlabelled data are put through this model 4 3 2 task processing life cycle workflow progress is tracked via a database which contains information about every file including intermediate files every task every acoustic feature and every workflow every used by the workflow system the general life cycle of processing tasks is summarised in fig 3 textually it can be described as follows 1 a task specification is defined when workflow processing begins this includes the task s name any associated parameters and pre requisite tasks this is not tied to any specific file at the same time for each file the tasks that have been previously applied to it or one of its parent files and the tasks still left to be applied to it are recorded 2 a task instance is created for one file once it is ready for processing i e when all pre requisite tasks are done this is added to a pool 3 a task instance is selected from the pool and allocated to a resource i e process this is done once resources complete tasks resources will be allocated a new task once their previous task is completed 4 all data required for the processing task is sent to the resource if it does not already have it 5 the resource processes the task 6 processing output is sent to an output queue for handling this output usually does not contain the actual files output from the processing task but instead contains associated metadata such as file names and paths 7 a dedicated master process works through the output queue updating workflow progress and calculating new ready tasks this life cycle is motivated by several characteristics of the task domain firstly unlike many other scientific workflows it is not known ahead of time how many outputs there will be for some tasks particularly segmentation and activity detection tasks and the same task is likely to be processed many times for different files as such task specifications and files are separated until tasks are ready for processing secondly this life cycle has separate processes that can operate in parallel to each other thereby improving efficiency compared to something more sequential task selection task processing and output handling are considered separate processes meaning tasks can be selected for different resources at the same time as others are being processed at the same time as previous outputs are being handled thirdly this mechanism allows processing tasks from several workflows to be handled simultaneously while file metadata and task specifications are tied to workflows there is no reason why tasks from different workflows cannot be in the same queue simultaneously this presents the potential for bioacoustics processing as a service where different users could access cloud resources at the same time to do the work that they need this mechanism also enables the possibility of task scheduling this could either be done pre emptively calculating which tasks to process prior to a new task being requested or once a new task is requested this architecture is flexible to different approaches which will be explored in the future this model resembles the existing swift parallel scripting language wilde et al 2011 in that parallelism is handled automatically based on what processes users input into the system in swift many instances of a task can be parallelised for different data inputs using for each loops which is similar to how each file within a set progresses through each task in the acousticloud system acousticloud does however make more underlying assumptions about the flow of data meaning dependency management and data input and output are simpler from a user perspective 4 3 3 distributed computation and communication a key requirement for the system is to process over distributed systems this is necessary for cloud computing which has important advantages over other high performance architectures and discussed in section 4 1 acousticloud uses a master slave model but with decentralised storage to reduce communication time slaves only perform processing and store data while masters are responsible for communicating with slaves and users selecting tasks preparing and sending data and handling outputs a master machine can contain processes that are essentially slaves performing processing tasks except without the need of sending task information over a network the steps involving communication between masters and slaves are indicated in fig 3 slave machines have two types of processes one type handles the actual processing component receiving files and processing information performing tasks and sending output information back the other type simply listens for file requests and sends files to the required machine there are typically multiple instances of the former processing type and only one of the latter each process type first creates a separate connection with the master for processing dedicated to computing tasks the master immediately selects a task if there are any tasks in the queue sends the required files and sends the task information task name and parameters the slave process then performs the processing and sends the output to the master which is sent to its own queue to be processed in an independent thread this output only contains metadata when large outputs are created and not the actual files which are kept on the slave system this way communication overhead can be reduced as subsequent tasks using the newly created file can be scheduled on the slave process thereby never needing to be sent anywhere else additionally the master can process one instance of process a per slave meaning it does not need to wait for slave machines to finish before allocating different tasks to other slaves so it can be constantly communicating without delay the other type of process simply listens to the master until it gets a message to send a file to another machine and then the process does exactly that it can also be used to receive other messages such as telling a slave to transfer all files and terminate if the system is being scaled down in a cloud computing scenario in other words file transferring is an independent process to the other steps a request can come at any time to transfer files from one slave to the master and then to any other slave to then receive this mechanism resembles the successful actor director mechanism used in the kepler scientific workflow management system ludäscher et al 2006 where there are multiple processing components whose execution is dictated by a director data communication is slightly different however as using analogies from the kepler model the actors are not necessarily connected to each other for direct communication and communication is instead handled by intermediate processes this separation of communication processes from workflow tasks simplifies the architecture distributed storage has also been implemented in successful systems before such as in apache spark s resilient distributed datasets rdds zaharia et al 2016 where data can be stored in many locations and can even be stored for repeated computations 4 3 4 data provenance and discovery data provenance is implicitly handled by the nature of how processing works metadata relating to each file are tracked including which tasks have been applied to a given file what the outputs of these tasks were and what workflows they were used in intermediate data are never deleted during processing nonetheless in order to simplify the process of discovering previous intermediate data separate data tables explicitly store information about all tasks that have been performed on a parent file or its children and what files were output from the tasks this way if the same task is applied to the same file in multiple workflows the intermediate outputs can be easily retrieved and used in subsequent processing steps skipping repeating any tasks notably tasks which do the same processing i e they call the same script or function but have different parameters and or dependencies are considered different tasks thereby ensuring this provenance system is consistent upon any newly input workflow the system checks to see if any files have been used before and if they have if any tasks in the workflow have been applied to previous workflows before if they have any workflow task completed previously is considered complete in the new workflow and the old tasks outputs are used for the new workflow 5 experimental set up to evaluate the potential of acousticloud a prototype was developed and evaluated both on its own under different system architectures across different workflows and with an implementation of a bioacoustics workflow using pegasus a popular swfms this prototype is evaluated to determine how well it achieves the following objectives minimise total execution time achieve linear scalability horizontal and vertical execution time reduces by half when double the resources are utilised minimise communication between machines minimise time spent reading and writing files and the database avoid redundant processing of tasks 5 1 prototype the prototype developed for testing implements most of the core functionality outlined in section 4 this includes being able to accept a workflow structure and compute this workflow across arbitrary datasets across multiple multithreaded virtual machines vms it does not contain any functionality relating to the cloud resource provisioning management layer and a very limited input interface only intended to aid in research because of the demand for the system to work on large scales non relational databases which were conceived with large scale big data processing in mind are likely a superior option compared to traditional relational databases such as sql for this scenario chodorow 2013 as such mongodb is the basis for data representation in the prototype data interactions such as strong provenance information fetching the directory of tasks calculating ready tasks and adding and selecting from the task queue are all handled through a mongodb database the prototype utilises a scheduling approach where priority is given to tasks using data that are already on the resource being scheduled to a request is made for a task to be selected for a target vm if there is a task available that does not require any data to be transferred to the vm in question then it is selected if there are no such tasks an arbitrary task is selected and data are transferred accordingly this simple task selection mechanism could be improved in future all processes including those to handle the workflow management and bioacoustics processing itself are written in python several external libraries are used in these process implementations including librosa mcfee et al 2015 scikit learn pedregosa et al 2011 and pymongo 5 2 experimental methodology evaluation involves processing several bioacoustics workflows in order on a distributed system prototype these workflows were applied on a 10 min recording generated using the scaper program developed by salamon et al 2017 this tool automatically generates soundscapes with annotations which can be used as training data to build classification like workflows the soundscapes were generated using the sounds of several birds were taken from recordings on the xeno canto website which is an open source repository where users upload recordings of bird sounds these sounds were randomly inserted and manipulated by scaper pink noise was inserted to simulated the background noise of the soundscapes 5 2 1 workflows used for evaluation a set of workflows is computed during each test each of which should help gain some insight into how well the system works processing of each workflow is mutually exclusive i e they do not explicitly interact with each other although some processes from earlier workflows are identical to later workflows meaning intermediate data can be reused workflow 1 simple call detector and classifier split to 5 s downsample to 22 05 khz convert to mono hilbert follower activity detector potamitis et al 2014 32 mel frequency cepstral coefficients mfccs random forest classifier breiman 2001 workflow 2 is the same as workflow 1 except audio is split into 10 s chunks at the start functionally acting in the same way workflow 3 complex call detector and classifier split to 5 s downsample to 22 05 khz convert to mono 500 hz high pass filter mmse stsa denoising ephraim and malah 1984 brown et al 2017 hilbert follower activity detector 32 mfccs spectral cover towsey et al 2014a acoustic complexity index aci pieretti et al 2011 spectrum 10 frequency bins uniformly distributed between 0 10 khz spectral entropy sueur et al 2008 spectrum 7 frequency bins uniformly distributed between 1 8 khz k nearest neighbour classifier knn peterson 2009 k 5 workflow 4 is the same as workflow 3 but with median clipping another form of denoising inserted after mmse stsa denoising this has the same objectives but with extra denoising to better handle very noisy recordings workflow 5 has the same pre processing as workflow 4 but has no activity detection 5 second chunks are classified from there the rest of the workflow includes all features from workflow 3 4 except spectral cover spectral cover spectrum 19 frequency bins uniformly distributed between 500 10000 hz segment length root mean square rms of the amplitude knn k 10 this aims to not isolate individual calls but determine the presence and classify calls of any animals within a 5 s duration these workflows are chosen to have the same basic structure as typical classification workflows while using processes used in previous bioacoustics analyses briggs et al 2012 brown et al 2017 2019 lasseck 2013 pieretti et al 2011 potamitis et al 2014 sueur et al 2008 towsey et al 2014a these are likely not the most effective workflow choices to perform effective classification but are selected to give different levels of data reuse different numbers of chunks to process and different numbers of tasks each of which have different computational intensities they nonetheless resemble real bioacoustics processing pipelines used in previous research with each processing task being used in previous bioacoustics analyses and have a typical bioacoustics workflow structure all classifiers are multi class meaning that they can output more than one class for each segment which is useful particularly if calls overlap 5 2 2 cloud architecture used in evaluations the primary motivation of this testing is to evaluate the viability of the system particularly in terms of the data model and workflow processing mechanisms the following questions are addressed in testing how does the system compare to an existing leading workflow system pegasus is the workflow system vertically and horizontally scalable does data reuse improve execution times and if so by how much how much data is sent between vms is the scalability of the workflow system dependent on the structure of the workflows initially tests are performed on both pegasus and acousticloud for just workflow 5 for a single vm 8 core architecture to compare them this is executing using default settings on pegasus except for task clustering where tasks are grouped together such that there are 8 clusters of tasks created for each process where applicable i e 8 resample tasks 8 mmse stsa tasks etc as opposed to 120 tasks or one per file execution time is measured in each case existing bioacoustics processing suites do not enable the customisation enabled by acousticloud and cannot easily be directly compared this output of this comparison will inform if more tests are needed to determine where acousticloud and pegasus are stronger if more tests are not needed testing will evaluate general scalability and efficiency in order to address these questions the following architectures are tested one 8 vcpu vm 8 one 4 vcpu vm 4 one 2 vcpu vm 2 two 2 vcpu vms 2 2 one 4 vcpu vm and two 2 vcpu vms 4 2 2 one 8 vcpu one 4 vcpu vms and two 2 vcpu vms 8 4 2 2 these machines are created using the nectar cloud their specifications are 8 vcpu 16 gb ram 4 vcpu 8 gb ram and 2 vcpu 4 gb ram all machines run on ubuntu 18 04 1 lts in reality processing these workflows using the system is cpu bound and most of the ram is not utilised although this could change in the future with optimisation to keep data in ram to prevent reads and writes from storage typically approximately 600 700 mb of ram is in use at any time with approximately 2 gb being cached at any given point when processing test workflows on one machine each vm has 30 gb of storage which is easily sufficient for the testing performed in all cases where there is more than one vm the vm with the highest number of vcpus is selected as the master each vm starts as many slave processes as there are vcpus on the machine including the master and the master starts up half as many processes as there are vcpus i e on the master machine itself for handling result outputs this appears to work best after some small tests for each architecture the six workflows are computed 10 times in sequence the database and data are cleared whenever a new repetition of the six workflows starts tests measure the execution time of each workflow for each repetition to determine the system s scalability in addition after each workflow step the size of data stored on all machines is summed for each workflow this shows how much data has been sent between machines since data is never deleted additional tests are performed with the 8 and 4 architectures where data is deleted after every workflow computed effectively disabling data reuse in order to determine how much data reuse improves computation the architecture is specifically made with reuse in mind so this will not take into account any extra overhead due to adding reuse as a requirement but the amount of computation saved gives an idea as to whether reuse is worthwhile 6 experimental results and analysis 6 1 comparison with pegasus wms fig 4 shows a performance comparison between acousticloud and pegasus for an 8 core vm this was repeated 10 times in each case pegasus overhead is simply too high for processing bioacoustics workflows similar to those being tested given that no bioacoustics workflow tested executed nearly as slow even on 2 cores compared to pegasus with 8 this test can be considered conclusive at least without significant reconfiguration and restructuring of pegasus workflows hence no further tests are conducted using pegasus the slow performance is in part due to the implementation s reliance on importing python libraries to perform many bioacoustics tasks acousticloud only needs to import the libraries once whereas pegasus needs to import required libraries once per execution and there are over 1000 task executions in the workflow rewriting the tasks in a way that avoids library imports and maybe even python altogether would likely improve execution time significantly in pegasus but this demonstrates a limitation of the system furthermore workflow creation in pegasus is more difficult than with acousticloud most significantly because inputs and outputs need to be explicitly stated for each task which would make tasks with unpredictable outputs such as activity detection more difficult to work into the system 6 2 scalability of the proposed system scalability refers to the ability of a system to maintain processing efficiency regardless of scale in an ideal scenario the system should process workflows twice as quickly with twice as many resources it is important to get as close to this ideal as possible as this means that the system is capable of efficiently processing workflows at any scale two types of scalability are evaluated vertical scalability processing capacity is increased by using a more powerful machine and horizontal scalability where processing capacity is increased by adding more comparatively less powerful machines 6 2 1 vertical scalability fig 5 shows how the system scales with the number of virtual cores because of the greatly different execution times of each workflow times are shown in terms of their improvement rate compared to the average execution time for two core execution these mean times are shown in table 2 these improvement rates are calculated by dividing the mean 2 core execution times for a given workflow by the execution time of any given evaluation in a theoretically ideal scenario the 4 core architecture would have an improvement rate of 2 and the 8 core architecture would have an improvement rate of 4 scalability appears excellent for moving from 2 cores to 4 cores but is not quite as good for moving from 4 cores to 8 although this does depend on the workflow notably workflows 3 and 4 show mediocre scalability whereas the others scale reasonably well a reason for this might be extra data overhead arising from very small segments being extracted during activity detection and very small acoustic feature calculation tasks workflow 5 s relatively strong scalability in comparison to workflows 3 and 4 support this particularly given workflow 5 uses fixed 5 second long chunks which results in fewer but longer processes overall compared to splitting based on activity which can result in many very short chunks overall it appears the system prototype does benefit from using more powerful machines but the benefit is somewhat workflow dependent and vertical scalability is often non linear 6 2 2 horizontal scalability fig 6 shows how the system scales as more vms are added the 4 core architecture is made up of two 2 core vms the 8 core architecture adds a 4 core machine to this and the 16 core architecture adds an 8 core vm on top of the 8 core architecture the improvement rates are once again relative to the single 2 core architecture it might be expected that because of the time required to communicate between different vms that the system s horizontal scalability would be lower than its vertical scalability however the opposite appears to be true with even the 16 core architecture improving linearly for some workflows compared to the 8 core architecture a glance back to fig 5 shows that architectures with multiple machines outperform architectures with single machines but the same number of virtual cores there are at least two reasons why this might be the case firstly in single machine architectures the same vm is doing all data management and processing tasks which could be overloading the system resulting in inefficiencies due to thread switching whereas in architectures with multiple vms slave machines are dedicated entirely to processing which might be more efficient secondly this workflow system relies heavily on file reads and writes and having multiple machines might reduce i o bottlenecks in general however this shows that the system can scale effectively up to 16 cores although the degree of scalability does depend on the workflow compared to the vertical scalability test workflows 4 and 5 scale noticeably better while workflow 3 is again the least scalable 6 3 communication overhead minimisation another aspect worth considering is communication overhead these tests were ran on vms that all operate from the same location so the communication overhead is very low in this testing but a scenario where this system would be deployed in a cloud environment with machines in different locations would incur higher overhead as such minimising the amount of communication is important in order to test communication overhead the total storage in the system was measured both for single machine architectures and multiple machine architectures because the system stores everything the difference between the total storage sizes of the multiple machine and single machine architectures shows the total communication overhead assuming the system is functioning correctly and not sending the same data multiple times this does not include messages sent between masters and slaves concerning what tasks to do and what outputs there are from tasks these are assumed to be very small given these are mostly in the form of lists with five or less values in the form of numbers or small strings almost certainly making up less than 10 mb of data overall the total data storage size after each workflow for any single machine architecture is shown in table 3 subsequent analysis will look at the percentage of duplicated data which is determined based on this table this determines the communication overhead communication overhead output is shown in fig 7 if all data was to be sent back and forth between slaves and masters for every task one would expect the overhead to be around 50 for each case given the master itself processes approximately 50 of the data in each instance remembering that the 4 vcpu case has a 2 core master the 8 vcpu has a 4 core master and the 16 vcpu case has an 8 core master in all workflows particularly the later workflows which have more intermediate files avoiding transferring files reduces the communication overhead to around 20 for all multi machine architectures that is of all data generated by the workflow processing only 20 is ever sent between machines in real terms this means that over 300 mb of communication is saved by keeping data on slaves and avoiding sending it to masters until it is needed this still means that around 300 mb of data is sent between machines by the end of workflow 5 this is equivalent to about 1 h of audio in wav format with mono 22 05 khz sample rate and 32 bit floating point samples which is used for most of the processing in the tests 6 4 redundant processing optimisation another test determines if reusing data saved computation time of course the amount of reuse possible will be dependent on the application a workflow effectiveness comparison where multiple workflows are to determine which one works best could benefit greatly from data reuse but a user with one specific workflow in mind will get no benefit and might even see a performance loss because of extra overhead involved with storing data in the database so that it can be reused fig 8 shows the effect of data reuse on execution times this shows that the benefit of reuse greatly depends on the workflow being used and even the number of cores workflows 1 and 2 cannot reuse any data which is reflected in the fact that execution times are the same with and without reuse workflow 3 can reuse split and resampled data which does improve execution times slightly with 4 vcpus but not with 8 the data overhead trade off is felt in this case for workflow 4 mmse stsa denoising can also be skipped which results in a more significant improvement even with 8 cores the observed difference in execution time is statistically significant t test p 0 01 for reuse vs no reuse for 8 cores or workflow 4 workflow 5 sees a huge difference between reusing and not reusing data with the 8 core case taking only slightly less time than the 2 core case when data was reused improvement rate over 2 cores is close to 1 in other words reusing data saved almost 6 cores worth of processing power in that case workflow 5 can take intermediate output from denoising tasks in workflow 4 which are the most computationally intensive tasks in the workflow overall this analysis shows that database reuse does improve execution times given the right processing scenario the value of reusing data is scenario dependent however and there is some overhead cost which is likely slightly underrepresented in this analysis 6 5 key experimental findings the findings in relation to the acousticloud system prototype can be summarised as follows the prototype demonstrates strong horizontal scalability and scales better horizontally than vertically scalability is dependent on workflows with improvement rates on a 4 vm 16 vcpu architecture varying between 3 6 times over a 1 vm 2 vcpu architecture depending on the workflow scalability is best when workflow tasks are computationally intensive and data chunks are kept somewhat large bottlenecks are more significant if the time taken to perform given processes is small communication overhead is reduced effectively from a theoretical 50 of data sent between machines if all data was sent back and forth to around 20 30 regardless of the number of vcpus vms although this could change if the power ratio between the master and slave cpus were different by keeping data on vms until they are needed by other vms reusing data effectively reduces computation time in some scenarios but not in others in particular it helps significantly if it can be used to avoid computationally intensive tasks its impact is more felt on less powerful architectures these findings all suggest that the acousticloud architecture presents a promising approach to perform flexible and efficient large scale bioacoustics processing with the addition of cloud capabilities in the future this could be used to process large numbers of diverse bioacoustics workflows simultaneously at low cost 7 conclusion this work discussed a new architecture for performing efficient yet flexible large scale bioacoustics processing using distributed computing which can later be adapted to work with cloud infrastructures this system has several features such as fast data provenance reuse mechanism reduced communication overhead over multiple machines and relatively small processing overhead particularly when compared to more generic workflow management systems such as pegasus this architecture takes advantage of characteristics of bioacoustics processing such as easy data parallelisation while mitigating the effects of some disadvantages such as occasional very short processing times for a large number of tasks testing of a prototype implementation of the architecture showed that the system is horizontally scalable and can effectively reuse data to perform equal processing on smaller numbers of vcpus in some scenarios it is also found to perform significantly better than pegasus a commonly used workflow management system with execution times approximately 8 times lower using its default configuration while also requiring less work to adapt for particularly in terms of managing inputs and outputs in the future dynamic resource provisioning approaches could be examined to determine to correct processing capacity required for handling bioacoustic workflows particularly when data is being streamed in real time task scheduling optimisation could also be examined as the prototype s approach is very basic additionally as far as data provenance is concerned examining what intermediate data are worth storing and when it is more efficient to simply recompute it as well as whether data compression could help storage and communication efficiency research could also consider which workflows are best suited for different scenarios both in terms of efficiency and accuracy of results in particular automatic evaluation of bioacoustics workflows could be done in the future using the system given it can swap tasks easily and reuse data effectively once this additional research has been done acousticloud could be deployed in many real world scenarios in particular it could be deployed in large scale biodiversity monitoring applications both in real time and in batches for example after the 2019 2020 australian bushfire season it is of great interest to monitor animal populations that have been severely affected a system with acousticloud s characteristics would be ideal for monitoring vocalising species due of its flexibility in workflow creation allowing specific workflows to be created to specialise for different species or audio phenomena and its ability to efficiently process large scale data software availability research code used in prototype evaluation is available at https sourceforge net projects acousticlod research code due to licencing issues the generated audio used for evaluation cannot be shared but similar datasets can be generated using the methods described in the paper credit authorship contribution statement alexander brown conceptualization methodology software formal analysis investigation visualization writing original draft writing review editing saurabh garg conceptualization supervision writing review editing james montgomery conceptualization supervision writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements special thanks to mutaz barika for providing valuable feedback to improve the manuscript as well as helping in describing the structure of the system architecture recording data used for sample recordings was generated based on recordings taken from the xeno canto website www xeno canto org by recordists greg mclachlan nick talbot marc anderson frank lambert oswaldo cortes pradnyavant man mandar bhagat fernand deroussen and sander lagerveld the lead author is supported by the australian government s research training program rtp 
26001,the increasing availability of remotely sensed soil moisture data offers new opportunities for data driven modelling approaches as alternatives for process based modelling this study presents the applicability of transfer function noise tfn modelling for predicting unsaturated zone conditions the tfn models are calibrated using smap l3 enhanced surface soil moisture data we found that soil moisture conditions are accurately represented by tfn models when exponential functions are used to define impulse response functions a sensitivity analysis showed the importance of using a calibrated period which is representative of the hydrological conditions for which the tfn model will be applied the ir function parameters provide valuable information on water system characteristics such as the total response and the response times of soil moisture to precipitation and evapotranspiration finally we encourage exploring the possibilities of tfn soil moisture modelling as predicting soil moisture conditions is promising for operational settings keywords data driven modelling pastas impulse response function smap unsaturated zone 1 introduction soil moisture is a key component of the hydrological cycle linking surface and subsurface hydrological processes entekhabi and rodriguez iturbe 1994 vereecken et al 2016 among others soil moisture governs the partitioning of precipitation into infiltration and runoff affecting streamflow and groundwater recharge brocca et al 2010 2017 up to date information on soil moisture helps effective decision making in operational water management e g for drought assessments grillakis 2019 mishra et al 2017 moravec et al 2019 sehgal and sridhar 2019 flood predictions brocca et al 2017 tramblay et al 2010 and irrigation management brocca et al 2018 rai et al 2018 in general three methods exist for estimating soil moisture at various spatiotemporal scales in situ dobriyal et al 2012 susha lekshmi et al 2014 remote sensing fang and lakshmi 2014 petropoulos et al 2015 zhuo and han 2016 and hydrological modelling vischel et al 2008 zhuo and han 2016 in situ soil moisture sensors provide accurate information at local scales since soil specific calibration procedures can be performed however in situ sensors typically have limited spatial coverage susha lekshmi et al 2014 remote sensing and hydrological modelling are alternative sources for providing spatially distributed soil moisture information at larger scales remotely sensed soil moisture information is often retrieved using active and passive microwave sensors petropoulos et al 2015 the temporal coverage of remote sensing is limited in comparison with in situ methods as satellite imagery is only available during satellite overpasses in addition only surface soil moisture can be retrieved by remote sensing due to sensor capabilities zhuo and han 2016 furthermore vegetation dynamics surface roughness and satellite sensor uncertainty significantly affect remote sensing retrievals petropoulos et al 2015 benninga et al 2019 hydrological modelling provides a means to estimate soil moisture at various spatiotemporal scales vereecken et al 2016 brocca et al 2017 the complexity of unsaturated zone models ranges from simple conceptual lumped models to complex integrated physically based distributed models water managers often regard model accuracy as a limiting factor for the application of hydrological modelling for decision making in operational water management pezij et al 2019a the accuracy of hydrological models is partly based on which dataset is used for calibration purposes data assimilation schemes can be applied to update model simulations with available observations although such schemes often require significant computational power liu et al 2012 weerts et al 2014 pezij et al 2019b furthermore physically based unsaturated zone models are often based on the richards equation which is highly non linear and therefore poses challenges for numerical solutions šimůnek et al 2003 vereecken et al 2016 in addition richards based models e g swap or hydrus are generally developed for local applications van dam et al 2008 šimůnek and van genuchten 2008 significant computational power is required to apply such models at regional scales van walsum and groenendijk 2008 therefore the application of richards type soil water flow models is not trivial in operational water management at catchment scales data driven modelling methods are suitable alternatives for process based modelling todini 2007 solomatine and ostfeld 2008 especially when large amounts of data are available such as in the netherlands data driven methods for the prediction of soil moisture conditions are promising kolassa et al 2018 cai et al 2019 the availability of new high resolution remotely sensed soil moisture data offers new opportunities for data driven modelling methods petropoulos et al 2015 however data driven methods such as neural networks and random forests are often considered black box models it can be challenging to interpret and apply the results of data driven methods padarian et al 2020 therefore although suitable for predictions such data driven methods are limitedly suitable to increase our understanding of hydrological processes and characteristics in this study we present a data driven framework based on transfer function noise tfn modelling for describing soil moisture dynamics and its controls tfn modelling is a data driven method to model an observed time series by applying a linear transformation of one or more deterministic input series known as stress series von asmuth et al 2002 the stress series are transformed using one or more impulse response ir functions the strengths of the framework are 1 tfn modelling is a fast and easy to construct data driven alternative for complex process based models 2 the ir functions are set up using only observational data tfn modelling does not need prior assumptions on system characteristics which is an interesting property since no model structure is expected to work best everywhere peterson and western 2014 3 the ir functions contain information on the response of a water system to input stresses such as precipitation and therefore increase our understanding of hydrological characteristics and processes 4 due to the stochastic nature of the included noise model tfn models can model system dynamics which are not well explained by physical laws von asmuth et al 2002 the applicability of tfn modelling for groundwater studies has been shown extensively yi and lee 2004 bakker et al 2007 manzione et al 2010 fabbri et al 2011 obergfell et al 2013 sutanudjaja et al 2013 peterson and western 2014 zaadnoordijk et al 2018 bakker and schaars 2019 these studies show that tfn modelling can be used to describe groundwater dynamics in addition the ir functions contain valuable information on groundwater system characteristics such as response times a tfn model is suitable for short term soil moisture predictions using short term meteorological predictions also depending on the time period covered by the meteorological input data one could use a tfn model to construct historical surface soil moisture time series commonly applied tfn modelling tools for groundwater modelling in the netherlands are menyanthes von asmuth et al 2012 and pastas collenteur et al 2019a we are interested in the applicability of tfn modelling as an innovative data driven method for explicitly calculating soil moisture dynamics which has not been studied before our main focus period is the year 2018 the soil moisture drought caused by the 2018 european heat wave significantly impacted water management and agricultural practices vogel et al 2019 which shows the importance of retrieving up to date soil moisture information we want to show that tfn modelling is a fast and relatively simple method for soil moisture modelling also as data driven models are often black boxes we want to show that tfn models can help in identifying controls on soil moisture dynamics we address the following research question in this study to what extent can tfn modelling describe and predict soil moisture dynamics using remotely sensed soil moisture information this paper is organised as follows section 2 describes the research methodology section 3 presents the results which are discussed in section 4 finally conclusions are drawn in section 5 2 methodology 2 1 transfer function noise modelling the time series modelling approach applied in this study originates from tfn autoregressive moving average arma modelling tfn models are often applied in hydrological applications since they are fast and provide accurate predictions yi and lee 2004 von asmuth et al 2008 manzione et al 2010 fabbri et al 2011 peterson and western 2014 the main concept is to interpret the output of an observed system as a combination of analytically defined impulse responses of a linear system which can be described by transfer functions kennaugh and moffatt 1965 box and jenkins 1970 among others marco 1993 and remesan and mathew 2015 discuss the applicability of transfer functions in hydrology such as in the unit hydrograph approach applied for estimating river discharge rates sherman 1932 the use of transfer functions for modelling subsurface dynamics has been studied by for example besbes and de marsily 1984 gehrels et al 1994 and von asmuth et al 2002 ramirez beltran et al 2008 showed the applicability of transfer functions for soil moisture modelling using in situ soil moisture measurements as validation data a tfn model has a deterministic component for the linear transformation of observations to a change of the output variable and an exponential noise model comparable to an auto regressive lag 1 ar1 model peterson and western 2014 remesan and mathew 2015 the linear transformation is performed using transfer functions the transfer functions or ir functions are not known a priori and have to be estimated the ir functions can be estimated using the predefined impulse response function in continuous time pirfict method von asmuth et al 2002 von asmuth and bierkens 2005 the pirfict method assumes that ir functions have known analytical expressions von asmuth et al 2002 show that the pirfict method overcomes the following limitations of estimating ir functions in regular tfn arma models 1 pirfict allows the use of data with an irregular time interval and 2 the iterative box jenkins style model identification procedure is generally applied to define the number of parameters in arma models box and jenkins 1970 the box jenkins model identification procedure can be knowledge and labour intensive since the analytical expression of the ir functions has to be defined a priori the order of the ir functions does not have to be defined using a box jenkins model identification procedure von asmuth et al 2012 the pirfict method allows selecting a typical ir function for a specific type of series von asmuth et al 2008 the pirfict method is applied in this study a general form of a continuous time tfn model in this case formulated for soil moisture dynamics is von asmuth et al 2008 1 h t j i 1 n s t r e s s h i t j d n r e s t j where h t j is the observed soil moisture state at time step t j m 3 m 3 n s t r e s s is the number of stress series which influence the soil moisture state h i t j is the change in the soil moisture state due to stress series i at time t j m 3 m 3 d is the baseline soil moisture state m 3 m 3 and n r e s t j is a residual time series m 3 m 3 the subscript j indicates the day number a tfn model can have multiple stress series as input the contribution of stress series i to the soil moisture state h i t j is determined by solving a convolution integral in continuous time using an ir function which describes the variation of the soil moisture state due to an individual stress series h i t j is defined as 2 h i t j t j r i τ θ i t j τ d τ where r i is the value of a stress series i m m at the times up to time step t j and θ i t j is the ir function of the corresponding stress series i to solve equation 2 θ i t j should be known the type and shape of the functions depend on the type of stress and water system characteristics we assume that the main drivers of soil moisture dynamics are precipitation entekhabi and rodriguez iturbe 1994 vereecken et al 2016 and evapotranspiration syed et al 2004 therefore we use time series of precipitation and evapotranspiration as stress series for the tfn model von asmuth et al 2012 state that independently of system properties analytical expressions such as the scaled gamma function fit the behaviour of many hydrogeological systems the scaled gamma step response function is a commonly applied ir function for precipitation and evapotranspiration stress series in groundwater tfn modelling besbes and de marsily 1984 von asmuth et al 2012 the scaled gamma ir function is defined as 3 θ t j a g a m t j n 1 a g a m n γ n e x p t j a g a m where a g a m corresponds to the unit response of the soil moisture state due to the stress at time t j 0 m 3 m 3 a g a m is a shape parameter d a y n is a shape parameter and γ n is the gamma function of the form n 1 typical behaviour of the gamma block response function is visualized in fig 1 in addition to the gamma function an exponential function is used particularly we studied whether an exponential function is a better representation of the unit response of soil moisture to precipitation and evapotranspiration as we expect that soil moisture shows a fast response to precipitation and evapotranspiration the exponential ir function is defined as 4 θ t j a a e x p t j a where a corresponds to the unit response of the soil moisture state due to the stress at time t j 0 m 3 m 3 and a is a shape parameter d a y fig 1 also shows typical behaviour of the exponential block response function the parameters of the ir functions are unique for every location which is analysed bakker et al 2008 the output of a deterministic model will never match observations perfectly residuals of tfn models often show large autocorrelation collenteur et al 2019a modelling the residuals helps to provide more accurate predictions of the soil moisture state at unobserved time steps therefore the residuals n r e s t j defined in equation 1 are modelled using a noise model to satisfy a white noise requirement von asmuth and bierkens 2005 the residuals are assumed white noise if all important system stresses are considered the noise model with exponential decay of the residuals is formulated as 5 n r e s t j υ t j e x p δ t j α n r e s t j 1 where υ t j is white noise resulting from a random process for time step t j m 3 m 3 α is a decay parameter d a y n r e s t j 1 is the residual value at the previous time step m 3 m 3 and δ t j is the length of the time step d a y which is one day in this study in addition the noise model allows the application of a least squares objective function von asmuth and bierkens 2005 peterson and western 2014 the optimal parameter sets for the ir functions used in equations 1 and 2 are determined by minimizing the objective function the a g a m a g a m and n parameters are estimated when the gamma function is used as ir function the a and a parameters are estimated when the exponential function is used as ir function more information on the parameter estimation procedure and the noise model can be found in von asmuth and bierkens 2005 2 2 tfn modelling library pastas to set up the tfn models we use the open source library pastas collenteur et al 2019a b which is a python 3 implementation of the tfn pirfict modelling approach described in section 2 1 more information on the pastas library can be found at https pastas readthedocs io 2 3 study area and data we assessed the applicability of tfn modelling for soil moisture predictions in the twente region in the eastern part of the netherlands see fig 2 the twente area has been part of soil moisture studies e g dente et al 2012 benninga et al 2019 chen et al 2019 pezij et al 2019a van der velde et al 2019 we will compare the results of the tfn modelling approach with the smap soil moisture active passive soil moisture retrieval studies of colliander et al 2017 chan et al 2018 and kolassa et al 2018 who included the twente area in their assessment the study area is situated in a temperate marine climate zone hendriks et al 2014 has an elevation ranging between 3 and 85 m a s l and has an extent of approximately 40 k m by 50 k m the main soil types are sandy and loamy sandy wösten et al 2013 the primary land use is agriculture annual precipitation varies between 800 and 850 mm kaandorp et al 2018 table 1 provides an overview of the data products used in this study we use two types of stress series precipitation and makkink reference crop evapotranspiration makkink reference crop evapotranspiration in the following evapotranspiration describes the potential evapotranspiration from a reference surface covered with grass makkink 1957 the makkink method requires less input variables than the penman monteith method de bruin and lablans 1998 show that the reference crop evapotranspiration estimates obtained using the makkink method correspond well to the reference crop evapotranspiration estimates obtained using the penman monteith method for the netherlands open source precipitation and evapotranspiration data from the royal netherlands meteorological institute knmi are used knmi 2018a b the precipitation data are based on radar data which are corrected using knmi station data by applying ordinary kriging the precipitation data have a spatial resolution of 1 k m by 1 k m the evapotranspiration data are based on extrapolating knmi station data using thin plate spline tps interpolation the station data are calculated by knmi using incoming shortwave radiation and mean daily temperature measurements at the knmi stations hiemstra and sluiter 2011 suggest that at least 15 stations should be used to interpolate daily evapotranspiration at a national scale in total 32 stations are used for the interpolation in the netherlands of these 32 stations three stations are located in or near the twente study area twenthe heino and hupsel the evapotranspiration data are gridded at a spatial resolution of 10 k m by 10 k m we use the smap l3 enhanced radiometer only daily gridded soil moisture product to calibrate the parameters of the tfn models entekhabi et al 2010 chan et al 2018 o neill et al 2018 also we use the smap l3 enhanced product to validate the tfn models the smap enhanced products are developed by applying the backus gilbert optimal interpolation technique to the smap brightness temperature data t a chan et al 2018 the latter is posted on a grid with a spatial resolution of 36 k m by 36 k m chan et al 2016 after various correction and calibration procedures of the regridded brightness temperatures the currently operational smap baseline soil moisture algorithm is used to produce the smap l3 enhanced surface soil moisture estimates the smap l3 enhanced product is gridded with a spatial resolution of 9 k m by 9 k m in general smap soil moisture products perform well in the twente region colliander et al 2017 chan et al 2018 assessed the smap l3 enhanced product using in situ soil moisture measurements they found an unbiased root mean square error urmse of 0 056 m 3 m 3 using the sca v retrieval algorithm for the twente region the smap data products were designed to meet a soil moisture retrieval accuracy of 0 040 m 3 m 3 urmse entekhabi et al 2010 the smap observations are available for the study area approximately every 2 3 days the footprint of the smap l3 enhanced product in the twente region is visualized in fig 2 we have analysed the morning smap retrievals for the period january 1 2016 january 1 2019 the smap soil moisture retrievals are affected by for example sensor noise and uncertainties in surface roughness and vegetation changes by applying the noise model with exponential decay of the residuals equation 5 we partly include these uncertainties in the tfn modelling approach additionally we use in situ soil moisture measurements from a monitoring network to assess whether the tfn models can describe soil moisture field conditions the monitoring network operating since 2009 is maintained by the itc faculty of the university of twente dente et al 2012 van der velde 2018 van der velde et al 2019 both volumetric moisture content and soil temperature are measured at 20 locations in the twente region stations 1 2 3 4 5 11 12 13 14 15 16 17 18 and 19 cover agricultural grass fields stations 6 7 8 9 and 10 cover maize fields station 20 is installed in a forest area the station locations are shown in fig 2 the monitoring network consists of decagon 5tm probes at 5 10 20 40 and 80 cm soil depths and provide a reading every 15 min we use daily averaged measurements at 5 cm soil depth since remotely sensed soil moisture data are limited to surface soil moisture petropoulos et al 2015 benninga et al 2019 fig 2 provides an overview of the smap l3 enhanced footprint relative to the in situ locations in the study area 2 4 general workflow we set up a tfn model including corresponding ir functions for each location of the in situ soil moisture monitoring network in the study area fig 3 shows the general research workflow to set up a tfn model the workflow focuses on three main parts indicated by the yellow boxes in the figure first the tfn models are set up and calibrated using smap data smap calibration next these models are validated using smap data for a different period smap validation finally we assess the applicability of the tfn models for estimating soil moisture at field scales using in situ measurements field validation the numbers 1 to 16 in the following sections refer to the steps shown in fig 3 2 4 1 smap calibration first 1 the input datasets which are described in section 2 3 are selected the datasets are split in a 2 calibration set and a 3 validation set the calibration set is used to determine the parameter sets of the ir functions for the implementation of the tfn models the validation set is used to assess the tfn model results we use a calibration period which covers at least the response time of the hydrological system that we observe we refer to section 3 2 for the determination of the response time especially we are interested in the predictive capabilities of the tfn model for the dry summer period of 2018 therefore the calibration set covers the period january 1 2016 january 1 2018 while the validation set covers the period january 1 2018 january 1 2019 since more smap observations are becoming available the calibration period can be continuously extended in an operational setting we assessed the influence of the calibration period length by performing a sensitivity analysis in which both the length and period of calibration set are varied the calibration periods used for the sensitivity analysis are a summer period april october 2016 a winter period october 2016 april 2017 the full year 2016 and the full year 2017 in addition we assessed the tfn model capabilities by switching the calibration and validation period january 1 2017 january 1 2019 for the calibration set and january 1 2016 january 1 2017 for the validation set section 4 2 discusses the results of the sensitivity analysis and the implications for tfn modelling section 4 3 discusses the tfn model results for the 2016 validation period first 4 the smap series of the calibration set are used as the observational calibration dataset for the tfn model then 5 the stress series are defined for the calibration period subsequently 6 a ir function is defined for each stress series both the observations and the stress series are added to a 7 pastas model object then pastas applies 8 a least squares optimization approach to find 9 optimal parameter sets for the precipitation and evapotranspiration ir functions for each in situ location by solving equation 1 these sets lead to the best fit of the smap soil moisture observations for the calibration period finally the sets are used to estimate soil moisture dynamics in the validation period we assessed the applicability of two functions to define the ir function of each stress series a gamma and an exponential function table 2 lists the combinations the explained variance percentage evp is calculated to assess the applicability of each combination the evp is defined as 6 e v p 100 σ h 2 σ n 2 σ h 2 where σ h 2 is the variance of the smap soil moisture observations m 3 m 3 2 and σ n 2 is the variance of the tfn model residuals as defined in equation 1 m 3 m 3 2 von asmuth et al 2002 an evp of 100 indicates a perfect simulation of the observations since no residuals exist in that case as a rule of thumb one generally accepts the results of a tfn model if the evp 70 van engelenburg et al 2020 additionally the noise series should not be significantly autocorrelated autocorrelation would indicate that the white noise assumption does not hold von asmuth et al 2002 we use the ljung box test to determine whether the noise series shows significant autocorrelation ljung and box 1978 2 4 2 smap validation we use smap observations from the year 2018 to validate the tfn models we define 10 the precipitation and evapotranspiration stress series for the period january 1 2018 january 1 2019 these series are used to 11 set up a pastas model for the validation period the 9 optimized parameter sets from the calibration set are applied to define the ir functions again pastas solves equation 1 using the defined ir functions the optimized parameter sets and the stress series resulting in 12 predictions of soil moisture for the validation period we use 13 the smap validation set to 14 assess the tfn model results using the unbiased root mean square error urmse bias and pearson correlation coefficient see appendix a 2 4 3 field validation furthermore we are interested in the applicability of the tfn models at field scales compared to the regional scales represented by the smap observations therefore we use 15 in situ soil moisture measurements from the soil moisture monitoring network to 16 evaluate the tfn model results at field scales using the urmse bias and pearson correlation coefficient error metrics the evaluation is performed for all in situ location for which measurement data are available for the validation period the following eleven stations provide data for the 2018 validation period 2 4 7 9 10 11 13 14 15 16 and 17 3 results 3 1 selection of ir functions first we evaluated which combination of ir functions leads to the best fit of the tfn models in terms of evp fig 4 shows the spatially averaged evp for the four function combinations as defined in table 2 the gg and ge combinations lead to tfn models which cannot sufficiently explain soil moisture dynamics both the gg and ge combinations show spatially averaged evp values lower than 50 the ee and eg combinations show the best model behaviour even though the eg combination leads to a rejection of the tfn model for one location i e station 16 67 the evp of the ee combination is consistently larger than 70 for all stations exceeding the model acceptance criterion since the ee combination consistently shows good accuracy we use the exponential function for both the precipitation and evapotranspiration ir functions in the remainder of the study because both the gg and ge combinations are rejected the tfn models will be rejected when a gamma function is used for the precipitation stress series so although von asmuth et al 2002 show that the gamma function is suitable to model the response of groundwater head to recharge using precipitation stress series the gamma function is not the best choice for precipitation stress series when modelling surface soil moisture dynamics the difference is less distinct for the evapotranspiration stress series either a gamma or exponential function leads to approximately similar results in terms of evp 3 2 calibrated ir functions the ir functions contain valuable information on the soil moisture response to the stress series fig 5 shows the calibrated ir functions of the precipitation and evapotranspiration stresses for location 2 as expected the precipitation stress series has a positive impact on the soil moisture state while the evapotranspiration stress series decreases the soil moisture state also the time scale of the precipitation stress series is smaller than the time scale of the evapotranspiration stress series furthermore the initial response of soil moisture at day one is much larger for the precipitation stress series than for the evapotranspiration stress series similar observations hold for all individual locations these findings are physically reasonable since precipitation causes immediate spikes in soil moisture while drydown due to evapotranspiration takes place on longer time scales in addition fig 5 shows that the length of the precipitation ir function for location 2 is approximately 75 days while the evapotranspiration ir function has a length of approximately 150 days the length of the ir functions can be interpreted as the system response to that specific stress series the calibration period of two year covers this length multiple times therefore we can conclude that the calibration period is of sufficient length to estimate soil moisture dynamics in the twente region 3 3 assessment of soil moisture modelling we will show the tfn model results for the first in situ location for which a full field validation dataset is available which is location 2 fig 6 shows the tfn model results smap observations and in situ measurements for location 2 during the period january 1 2016 january 1 2019 in both the calibration and validation period the tfn model can correctly simulate the summer winter cycle of drying and wetting large deviations between the smap observations and the tfn model results can be observed in winter periods such as december 2016 and february 2018 the soil can freeze in winter periods which significantly affects smap satellite as well as in situ sensor readings van der velde et al 2019 the use of ir functions prevents the presence of these outliers in the tfn model result in addition as temperature is not part of the input data the tfn model does not detect such periods on the other hand the tfn model underestimates soil moisture during july august 2018 finally the smap observations overestimate soil moisture in the transition from summer to fall in 2018 van der velde et al 2019 evaluated smap surface soil moisture data in the twente region and stated that in the summer fall of 2018 dry spells were ended by a sequence of substantial rain events that exposed the disparity in sampling depth between smap and the in situ sensors shellito et al 2016 and benninga et al 2018 found similar results we quantify the accuracy of the tfn models by calculating the urmse bias and correlation coefficient error metrics with respect to the smap observations for each in situ location in the 2018 validation period the figure also shows the results for the 2016 validation period which are discussed in section 4 2 the dots in fig 7 smap validation 2018 visualize the error metrics of the tfn models results with respect to the smap observations for the year 2018 the urmse varies between 0 059 and 0 070 m 3 m 3 the bias varies between 0 0040 and 0 019 m 3 m 3 and the correlation coefficient varies between 0 79 and 0 82 for all locations recognizing that the tfn models do not consider the over and underestimation of smap in frozen and dry conditions the tfn models perform well in predicting smap surface soil moisture an implication is that the calibrated tfn model can be used to estimate surface soil moisture and extend smap data if precipitation and evapotranspiration data are available in addition fig 7 shows the urmse bias and correlation coefficient of the tfn model results with respect to the in situ soil moisture measurements for eleven locations in the 2018 validation period field validation 2018 although a fundamental difference exists in spatial scales represented by the smap satellite footprint and the in situ measurements the tfn models accurately predict field scale soil moisture for seven out of eleven locations in terms of urmse for four out of eleven locations in terms of bias and for all locations in terms of correlation coefficient 4 discussion 4 1 verification of tfn modelling approach the results show that tfn modelling using the pirfict method can be applied to predict surface soil moisture conditions in the twente region using smap surface soil moisture remote sensing data as calibration set as part of the tfn model verification we assessed whether the noise series show autocorrelation using the ljung box statistical test ljung and box 1978 the autocorrelation is assessed considering a significance level of 0 05 the ljung box test shows that no significant autocorrelation is observed for all stations therefore the white noise assumption holds for all stations it is generally known that the validity of data driven models is often limited to the hydrological conditions for which the model is calibrated e g abrahart et al 2010 kornelsen and coulibaly 2014 mount et al 2016 therefore because of the data based nature of tfn models there is the risk to extrapolate results to situations for which no references are available in the calibration set von asmuth et al 2012 for example the tfn model of location 2 does not capture the extremely dry summer period of 2018 well as seen in fig 6 according to the tfn model the volumetric moisture content drops to almost 0 m 3 m 3 in that period however both the smap observations and the in situ measurements show that soil moisture has a physical lower limit of approximately 0 1 m 3 m 3 the tfn models do not identify the lower limit this limitation might be explained by evapotranspiration reduction which is a non linear mechanism which reduces actual evapotranspiration when only low amounts of moisture are available the linear nature of tfn models indeed decreases their accuracy during long dry periods according to peterson and western 2014 moreover the makkink method describes the potential evapotranspiration which is the maximum evapotranspiration occurring when water is not a limiting factor actual evapotranspiration observations could be considered to improve the tfn model results these findings stress the need for a representative calibration dataset in addition the length of the calibration period may affect the tfn model results the influence of the calibration period will be assessed in the next section 4 2 sensitivity of calibration period we performed a sensitivity analysis on both the calibration set period and the length of the period based on the rmse error metric with respect to the in situ measurements in the 2018 validation period fig 8 shows the results of the sensitivity analysis soil moisture dynamics at some stations cannot be properly explained using the 2016 2017 winter period as calibration period the difference between the summer of 2016 the year 2016 2017 and 2016 2017 calibration periods is not significant the 2016 2017 winter period shows the largest rmse values whereas the 2016 summer calibration set shows the smallest rmse values fig 6 shows that the 2016 2017 calibration set leads to a large underestimation of the 2018 dry summer period the same finding holds for the 2016 and 2017 calibration sets fig 9 shows the tfn model results for location 2 when only the 2016 summer period is used as calibration set using the 2016 summer calibration set the tfn models can simulate the dry period of 2018 correctly thus the tfn models can represent the drought period of 2018 when a representative calibration period is selected we want to stress that the representativeness of the calibration period is defined by the characteristics of the soil moisture dynamics occurring in that period rather than the calendar date one would have to assess the mean variance and other statistics to assess whether the calibration set is representative for the period of interest 4 3 results for 2016 validation period the dry period in the summer of 2018 is an extreme event probably the sensitivity analysis results are case specific and thus not generalizable to test the applicability of the tfn models in more general situations we switched the calibration and validation set periods the tfn models are calibrated for the period january 1 2017 january 1 2019 and validated for the year 2016 the triangles in fig 7 smap validation 2016 show the urmse bias and correlation coefficient of the tfn models with respect to the smap observations when the year 2016 is used as validation set the 2016 results have consistently higher accuracies than the smap 2018 validation results the urmse varies between 0 042 and 0 052 m 3 m 3 the bias varies between 0 0061 and 0 023 m 3 m 3 for the locations and the correlation coefficient varies between 0 82 and 0 88 for the locations furthermore the stars in fig 7 field validation 2016 show the error metrics of the tfn models with respect to the in situ measurements when the year 2016 is used as validation set the following stations have a full dataset for the 2016 validation period 1 2 3 7 9 16 and 18 no apparent change in accuracy is found for the 2016 validation results at field scales which are represented by the in situ measurements 4 4 accuracy of tfn modelling estimates this section discusses the accuracy of the tfn modelling estimates as shown in fig 7 table 3 shows the performance of the soil moisture tfn modelling approach in comparison with other smap validation studies in the twente study area colliander et al 2017 and chan et al 2018 validated the smap and smap enhanced soil moisture products respectively kolassa et al 2018 used the smap microwave observations and a neural network to retrieve surface soil moisture estimates since these studies considered spatially averaged results for the twente study area the tfn model results are also spatially averaged especially the tfn model validated for the year 2016 performs well as shown by the urmse bias rmse and correlation coefficient error metrics less extreme dry and wet events occurred in 2016 in comparison with 2018 the underestimation of the dry 2018 period has a significant impact on the model accuracy which explains the increased accuracy of the tfn models in the 2016 validation period the comparison with the data driven approach by kolassa et al 2018 shows that smap tfn modelling for the year 2016 has a comparable accuracy these results indicate that a calibration period including the extreme dry summer of 2018 will lead to more accurate tfn models for drought predictions especially at spatial scales similar to the smap satellite footprint the smap satellite retrievals have a design accuracy of 0 04 m 3 m 3 in terms of urmse entekhabi et al 2010 which is represented by the red line in fig 7 we want to stress that the accuracy of the original smap l3 enhanced retrievals do not meet this design accuracy in the twente study area 0 056 m 3 m 3 chan et al 2018 the upper panel of fig 7 shows the accuracy of the tfn model estimates with respect to the design accuracy although not meeting the design accuracy smap validation 2018 and smap validation 2016 approach the accuracies found by chan et al 2018 in terms of the urmse bias and correlation coefficient error metrics also the field validation 2016 estimates correspond well with the field observations although local scale issues cause discrepancies for some stations large discrepancies can be found for the field validation 2018 which are mainly caused by large biases as shown in panel b of fig 7 4 5 spatial and temporal resolution of tfn soil moisture estimates the spatial resolution of the tfn soil moisture estimates depends on the spatial resolution of the input data only considering the spatial resolution the reference crop evapotranspiration data form the limiting factor for the tfn soil moisture estimates these data have a resolution of 10 k m by 10 k m see table 1 however evapotranspiration does not vary as much as precipitation over short distances dalezios et al 2002 hess et al 2016 therefore we assume that the tfn soil moisture estimates are bound by the spatial resolution of the smap input data which is 9 k m by 9 k m the results shown in table 3 and fig 7 reflect this statement table 3 shows that at a regional scale the accuracy of the tfn soil moisture estimates approximates the accuracy of other soil moisture products we define a regional scale as the extent of our study area which approaches the spatial resolution of the smap passive soil moisture product 36 k m by 36 k m at local scales the accuracy of the tfn soil moisture estimates mainly depends on whether the smap input data reflect local conditions as shown in fig 7 the in situ soil moisture monitoring stations represent these local scales although the correlation shows good corresponding between the tfn soil moisture estimates and in situ data the urmse and bias error metrics show large discrepancies for approximately half of the locations therefore the spatial resolution of the input data is an important aspect which should be considered in tfn modelling in addition the smap soil moisture retrievals are limited to shallow soil depths petropoulos et al 2015 as the ir functions are calibrated using the smap data the tfn soil moisture estimates are also limited to shallow soil depths other methods such as data assimilation using process based models liu et al 2012 pezij et al 2019b are necessary for the translation to soil moisture in deeper layers in contrast the temporal resolution of the tfn soil moisture estimates only partially depends on the temporal resolution of the input data the ir functions enable prediction of soil moisture if precipitation and reference crop evapotranspiration data are available as both the precipitation and reference crop evapotranspiration are available on a daily basis the temporal resolution of the tfn soil moisture estimates is also considered to be on a daily basis as the smap satellite does not provide daily soil moisture observations the tfn models can be used to fill the data gaps occurring on days without a satellite overpass as discussed in section 3 3 the smap input data have an availability of 2 3 days this study shows that this temporal resolution is sufficient to develop ir functions for accurately estimating soil moisture however the soil moisture response due to some individual precipitation events might not be included in the calibration procedure if the smap satellite does not overpass the area of interest therefore the accuracy of the tfn soil moisture estimates might be affected when heavy precipitation events are missed 4 6 water system characteristics generally ir functions can provide information on characteristics of the system which is observed among others these functions describe the unit step response and response time of groundwater dynamics when a groundwater system is observed bakker et al 2008 zaadnoordijk et al 2018 the exponential function applied in this work has two parameters a and a see equation 4 the parameter a is related to the total change in soil moisture due to a unit stress a large a indicates a large total change the shape parameter a is related to the time scale on which a unit stress affects soil moisture a large a indicates a slow response fig 10 shows the calibrated parameters for the precipitation and evapotranspiration ir functions for all locations based on the 2016 2017 calibration set we have assessed the relationship of the parameters with the following variables longitude latitude soil elevation vegetation type and soil type the elevation data are obtained from actueel hoogtebestand nederland 2019 the vegetation characteristics are obtained from van der velde et al 2019 the soil type characteristics are obtained from the bofek2012 dataset wösten et al 2013 the next sections will elaborate on the relationships found for parameters a related to the total change and a related to the response time 4 6 1 total change the change in total soil moisture volume due to precipitation is strongly correlated to longitude the change is larger for stations in the western part of the study area longitude is strongly correlated with distance to the coast in the netherlands daniels et al 2014 found that on average the precipitation amount is higher near the coast similarly the change in total soil moisture due to evaporation is correlated to latitude the change is larger for stations in the north of the study area in addition the total change in soil moisture might be related to soil physical characteristics and vegetation type however the elevation soil type and vegetation type do not show clear patterns with respect to the total change in soil moisture these characteristics are quite homogeneous for the locations in the study area with mainly sandy soils and grass vegetation a valuable addition can be to study whether different ir functions and corresponding parameters are found in areas with other soil characteristics such as peaty or clayey areas or areas with different vegetation types 4 6 2 response time a linear trend is seen in the spatial distribution of parameter a in relation with longitude in general if a large soil moisture response time scale due to precipitation is found at a specific location the corresponding soil moisture response time scale due to evapotranspiration is relatively small and vice versa the time scale of the precipitation ir function is larger for locations in the western part of the study area also the subsurface in the western part of the study area contains thick sand layers precipitation infiltrates relatively easy in sandy layers which causes a fast response of shallow soil moisture moraines of clay are found in the eastern part of the study area clay layers have low infiltration rates which results in a slow increase of soil moisture content however the soil type characteristics do not show a clear relation with respect to the a parameter the latitude elevation and vegetation type characteristics do not show clear patterns with respect to the soil moisture response time more research is needed to generalize these findings one should be careful in relating these parameters to physical processes as the selection of a representative ir function is an assumption von asmuth et al 2012 for example fig 5 shows that the time scale of the precipitation ir function is approximately 75 days to our knowledge this time scale cannot be directly connected to physical phenomena more research on the ir function parameters is needed to increase the understanding of their physical meaning in terms of soil moisture 5 conclusions we studied the applicability of transfer function noise modelling tfn for describing and predicting soil moisture dynamics tfn modelling is a fast alternative for process based models taking only seconds to simulate a full year of daily soil moisture conditions tfn modelling is based on the assumption that soil moisture dynamics can be explained by linearly transforming precipitation and evapotranspiration stress series using impulse response ir functions the smap l3 enhanced surface soil moisture product is used to calibrate the tfn models we found that exponential functions describe the ir functions of both the precipitation and evapotranspiration stress series better than gamma functions tfn models describe soil moisture conditions well when comparing the tfn model results with the smap observations in addition the tfn model results were compared with in situ soil moisture measurements to assess the field scale applicability of tfn modelling the accuracy of the tfn models mainly depends on the representation of the smap satellite product for that specific spatial scale a practical application for operational water management is that the tfn modelling approach can be used to estimate soil moisture dynamics using predictions of precipitation and evapotranspiration the application is promising if sufficient calibration data are available although one should be careful when interpreting results in extreme situations since the tfn models do not consider the physical lower and upper limits of soil moisture however a sensitivity analysis and variation of the calibration and validation periods showed that selecting a suitable calibration period can significantly increase the tfn model capabilities in both regular and extreme situations in addition the ir function parameters potentially provide valuable information on water system characteristics such as the total response and the response times of soil moisture to precipitation and evapotranspiration thus the tfn models have value in explaining hydrological processes and characteristics in contrast to other data driven tools such as neural networks which are black boxes however more research on the physical meaning of these parameters is needed to understand their applicability concluding we consider the applicability of tfn modelling for explaining soil moisture dynamics promising and propose to explore the possibilities of tfn modelling for predicting soil moisture in operational settings declaration of competing interest the authors declare that they have no know competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work is part of the owas1s research programme optimizing water availability with sentinel 1 satellites with project number 13871 which is partly financed by the netherlands organisation for scientific research nwo we want to thank all owas1s programme partners for their contribution the research data are freely available at the 4tu researchdata repository https doi org 10 4121 uuid ba33fc56 e07b 4547 9630 9b1565d18040 the authors want to thank the water resources department of the itc faculty of the university of twente in particular rogier van der velde and harm jan benninga for sharing the in situ soil moisture measurements from their monitoring network the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper appendix a error measures the root mean square error rmse is defined as a 1 r m s e j 1 n θ j o b s θ j p r e d 2 n in which θ j o b s are the smap soil moisture measurements for each day j m 3 m 3 θ j p r e d are the tfn model results for each day j m 3 m 3 and n is the number of observations the unbiased root mean square error urmse is defined as a 2 u r m s e j 1 n θ j o b s θ o b s θ j p r e d θ p r e d 2 n in which θ o b s is the arithmetic mean of the smap soil moisture measurements m 3 m 3 and θ p r e d is arithmetic mean of the tfn model results m 3 m 3 the bias is defined as a 3 b i a s θ o b s θ p r e d the pearson correlation coefficient r is defined as a 4 r j 1 n θ j o b s θ o b s θ j p r e d θ p r e d j 1 n θ j o b s θ o b s 2 j 1 n θ j p r e d θ p r e d 2 appendix b supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix b supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104756 
26001,the increasing availability of remotely sensed soil moisture data offers new opportunities for data driven modelling approaches as alternatives for process based modelling this study presents the applicability of transfer function noise tfn modelling for predicting unsaturated zone conditions the tfn models are calibrated using smap l3 enhanced surface soil moisture data we found that soil moisture conditions are accurately represented by tfn models when exponential functions are used to define impulse response functions a sensitivity analysis showed the importance of using a calibrated period which is representative of the hydrological conditions for which the tfn model will be applied the ir function parameters provide valuable information on water system characteristics such as the total response and the response times of soil moisture to precipitation and evapotranspiration finally we encourage exploring the possibilities of tfn soil moisture modelling as predicting soil moisture conditions is promising for operational settings keywords data driven modelling pastas impulse response function smap unsaturated zone 1 introduction soil moisture is a key component of the hydrological cycle linking surface and subsurface hydrological processes entekhabi and rodriguez iturbe 1994 vereecken et al 2016 among others soil moisture governs the partitioning of precipitation into infiltration and runoff affecting streamflow and groundwater recharge brocca et al 2010 2017 up to date information on soil moisture helps effective decision making in operational water management e g for drought assessments grillakis 2019 mishra et al 2017 moravec et al 2019 sehgal and sridhar 2019 flood predictions brocca et al 2017 tramblay et al 2010 and irrigation management brocca et al 2018 rai et al 2018 in general three methods exist for estimating soil moisture at various spatiotemporal scales in situ dobriyal et al 2012 susha lekshmi et al 2014 remote sensing fang and lakshmi 2014 petropoulos et al 2015 zhuo and han 2016 and hydrological modelling vischel et al 2008 zhuo and han 2016 in situ soil moisture sensors provide accurate information at local scales since soil specific calibration procedures can be performed however in situ sensors typically have limited spatial coverage susha lekshmi et al 2014 remote sensing and hydrological modelling are alternative sources for providing spatially distributed soil moisture information at larger scales remotely sensed soil moisture information is often retrieved using active and passive microwave sensors petropoulos et al 2015 the temporal coverage of remote sensing is limited in comparison with in situ methods as satellite imagery is only available during satellite overpasses in addition only surface soil moisture can be retrieved by remote sensing due to sensor capabilities zhuo and han 2016 furthermore vegetation dynamics surface roughness and satellite sensor uncertainty significantly affect remote sensing retrievals petropoulos et al 2015 benninga et al 2019 hydrological modelling provides a means to estimate soil moisture at various spatiotemporal scales vereecken et al 2016 brocca et al 2017 the complexity of unsaturated zone models ranges from simple conceptual lumped models to complex integrated physically based distributed models water managers often regard model accuracy as a limiting factor for the application of hydrological modelling for decision making in operational water management pezij et al 2019a the accuracy of hydrological models is partly based on which dataset is used for calibration purposes data assimilation schemes can be applied to update model simulations with available observations although such schemes often require significant computational power liu et al 2012 weerts et al 2014 pezij et al 2019b furthermore physically based unsaturated zone models are often based on the richards equation which is highly non linear and therefore poses challenges for numerical solutions šimůnek et al 2003 vereecken et al 2016 in addition richards based models e g swap or hydrus are generally developed for local applications van dam et al 2008 šimůnek and van genuchten 2008 significant computational power is required to apply such models at regional scales van walsum and groenendijk 2008 therefore the application of richards type soil water flow models is not trivial in operational water management at catchment scales data driven modelling methods are suitable alternatives for process based modelling todini 2007 solomatine and ostfeld 2008 especially when large amounts of data are available such as in the netherlands data driven methods for the prediction of soil moisture conditions are promising kolassa et al 2018 cai et al 2019 the availability of new high resolution remotely sensed soil moisture data offers new opportunities for data driven modelling methods petropoulos et al 2015 however data driven methods such as neural networks and random forests are often considered black box models it can be challenging to interpret and apply the results of data driven methods padarian et al 2020 therefore although suitable for predictions such data driven methods are limitedly suitable to increase our understanding of hydrological processes and characteristics in this study we present a data driven framework based on transfer function noise tfn modelling for describing soil moisture dynamics and its controls tfn modelling is a data driven method to model an observed time series by applying a linear transformation of one or more deterministic input series known as stress series von asmuth et al 2002 the stress series are transformed using one or more impulse response ir functions the strengths of the framework are 1 tfn modelling is a fast and easy to construct data driven alternative for complex process based models 2 the ir functions are set up using only observational data tfn modelling does not need prior assumptions on system characteristics which is an interesting property since no model structure is expected to work best everywhere peterson and western 2014 3 the ir functions contain information on the response of a water system to input stresses such as precipitation and therefore increase our understanding of hydrological characteristics and processes 4 due to the stochastic nature of the included noise model tfn models can model system dynamics which are not well explained by physical laws von asmuth et al 2002 the applicability of tfn modelling for groundwater studies has been shown extensively yi and lee 2004 bakker et al 2007 manzione et al 2010 fabbri et al 2011 obergfell et al 2013 sutanudjaja et al 2013 peterson and western 2014 zaadnoordijk et al 2018 bakker and schaars 2019 these studies show that tfn modelling can be used to describe groundwater dynamics in addition the ir functions contain valuable information on groundwater system characteristics such as response times a tfn model is suitable for short term soil moisture predictions using short term meteorological predictions also depending on the time period covered by the meteorological input data one could use a tfn model to construct historical surface soil moisture time series commonly applied tfn modelling tools for groundwater modelling in the netherlands are menyanthes von asmuth et al 2012 and pastas collenteur et al 2019a we are interested in the applicability of tfn modelling as an innovative data driven method for explicitly calculating soil moisture dynamics which has not been studied before our main focus period is the year 2018 the soil moisture drought caused by the 2018 european heat wave significantly impacted water management and agricultural practices vogel et al 2019 which shows the importance of retrieving up to date soil moisture information we want to show that tfn modelling is a fast and relatively simple method for soil moisture modelling also as data driven models are often black boxes we want to show that tfn models can help in identifying controls on soil moisture dynamics we address the following research question in this study to what extent can tfn modelling describe and predict soil moisture dynamics using remotely sensed soil moisture information this paper is organised as follows section 2 describes the research methodology section 3 presents the results which are discussed in section 4 finally conclusions are drawn in section 5 2 methodology 2 1 transfer function noise modelling the time series modelling approach applied in this study originates from tfn autoregressive moving average arma modelling tfn models are often applied in hydrological applications since they are fast and provide accurate predictions yi and lee 2004 von asmuth et al 2008 manzione et al 2010 fabbri et al 2011 peterson and western 2014 the main concept is to interpret the output of an observed system as a combination of analytically defined impulse responses of a linear system which can be described by transfer functions kennaugh and moffatt 1965 box and jenkins 1970 among others marco 1993 and remesan and mathew 2015 discuss the applicability of transfer functions in hydrology such as in the unit hydrograph approach applied for estimating river discharge rates sherman 1932 the use of transfer functions for modelling subsurface dynamics has been studied by for example besbes and de marsily 1984 gehrels et al 1994 and von asmuth et al 2002 ramirez beltran et al 2008 showed the applicability of transfer functions for soil moisture modelling using in situ soil moisture measurements as validation data a tfn model has a deterministic component for the linear transformation of observations to a change of the output variable and an exponential noise model comparable to an auto regressive lag 1 ar1 model peterson and western 2014 remesan and mathew 2015 the linear transformation is performed using transfer functions the transfer functions or ir functions are not known a priori and have to be estimated the ir functions can be estimated using the predefined impulse response function in continuous time pirfict method von asmuth et al 2002 von asmuth and bierkens 2005 the pirfict method assumes that ir functions have known analytical expressions von asmuth et al 2002 show that the pirfict method overcomes the following limitations of estimating ir functions in regular tfn arma models 1 pirfict allows the use of data with an irregular time interval and 2 the iterative box jenkins style model identification procedure is generally applied to define the number of parameters in arma models box and jenkins 1970 the box jenkins model identification procedure can be knowledge and labour intensive since the analytical expression of the ir functions has to be defined a priori the order of the ir functions does not have to be defined using a box jenkins model identification procedure von asmuth et al 2012 the pirfict method allows selecting a typical ir function for a specific type of series von asmuth et al 2008 the pirfict method is applied in this study a general form of a continuous time tfn model in this case formulated for soil moisture dynamics is von asmuth et al 2008 1 h t j i 1 n s t r e s s h i t j d n r e s t j where h t j is the observed soil moisture state at time step t j m 3 m 3 n s t r e s s is the number of stress series which influence the soil moisture state h i t j is the change in the soil moisture state due to stress series i at time t j m 3 m 3 d is the baseline soil moisture state m 3 m 3 and n r e s t j is a residual time series m 3 m 3 the subscript j indicates the day number a tfn model can have multiple stress series as input the contribution of stress series i to the soil moisture state h i t j is determined by solving a convolution integral in continuous time using an ir function which describes the variation of the soil moisture state due to an individual stress series h i t j is defined as 2 h i t j t j r i τ θ i t j τ d τ where r i is the value of a stress series i m m at the times up to time step t j and θ i t j is the ir function of the corresponding stress series i to solve equation 2 θ i t j should be known the type and shape of the functions depend on the type of stress and water system characteristics we assume that the main drivers of soil moisture dynamics are precipitation entekhabi and rodriguez iturbe 1994 vereecken et al 2016 and evapotranspiration syed et al 2004 therefore we use time series of precipitation and evapotranspiration as stress series for the tfn model von asmuth et al 2012 state that independently of system properties analytical expressions such as the scaled gamma function fit the behaviour of many hydrogeological systems the scaled gamma step response function is a commonly applied ir function for precipitation and evapotranspiration stress series in groundwater tfn modelling besbes and de marsily 1984 von asmuth et al 2012 the scaled gamma ir function is defined as 3 θ t j a g a m t j n 1 a g a m n γ n e x p t j a g a m where a g a m corresponds to the unit response of the soil moisture state due to the stress at time t j 0 m 3 m 3 a g a m is a shape parameter d a y n is a shape parameter and γ n is the gamma function of the form n 1 typical behaviour of the gamma block response function is visualized in fig 1 in addition to the gamma function an exponential function is used particularly we studied whether an exponential function is a better representation of the unit response of soil moisture to precipitation and evapotranspiration as we expect that soil moisture shows a fast response to precipitation and evapotranspiration the exponential ir function is defined as 4 θ t j a a e x p t j a where a corresponds to the unit response of the soil moisture state due to the stress at time t j 0 m 3 m 3 and a is a shape parameter d a y fig 1 also shows typical behaviour of the exponential block response function the parameters of the ir functions are unique for every location which is analysed bakker et al 2008 the output of a deterministic model will never match observations perfectly residuals of tfn models often show large autocorrelation collenteur et al 2019a modelling the residuals helps to provide more accurate predictions of the soil moisture state at unobserved time steps therefore the residuals n r e s t j defined in equation 1 are modelled using a noise model to satisfy a white noise requirement von asmuth and bierkens 2005 the residuals are assumed white noise if all important system stresses are considered the noise model with exponential decay of the residuals is formulated as 5 n r e s t j υ t j e x p δ t j α n r e s t j 1 where υ t j is white noise resulting from a random process for time step t j m 3 m 3 α is a decay parameter d a y n r e s t j 1 is the residual value at the previous time step m 3 m 3 and δ t j is the length of the time step d a y which is one day in this study in addition the noise model allows the application of a least squares objective function von asmuth and bierkens 2005 peterson and western 2014 the optimal parameter sets for the ir functions used in equations 1 and 2 are determined by minimizing the objective function the a g a m a g a m and n parameters are estimated when the gamma function is used as ir function the a and a parameters are estimated when the exponential function is used as ir function more information on the parameter estimation procedure and the noise model can be found in von asmuth and bierkens 2005 2 2 tfn modelling library pastas to set up the tfn models we use the open source library pastas collenteur et al 2019a b which is a python 3 implementation of the tfn pirfict modelling approach described in section 2 1 more information on the pastas library can be found at https pastas readthedocs io 2 3 study area and data we assessed the applicability of tfn modelling for soil moisture predictions in the twente region in the eastern part of the netherlands see fig 2 the twente area has been part of soil moisture studies e g dente et al 2012 benninga et al 2019 chen et al 2019 pezij et al 2019a van der velde et al 2019 we will compare the results of the tfn modelling approach with the smap soil moisture active passive soil moisture retrieval studies of colliander et al 2017 chan et al 2018 and kolassa et al 2018 who included the twente area in their assessment the study area is situated in a temperate marine climate zone hendriks et al 2014 has an elevation ranging between 3 and 85 m a s l and has an extent of approximately 40 k m by 50 k m the main soil types are sandy and loamy sandy wösten et al 2013 the primary land use is agriculture annual precipitation varies between 800 and 850 mm kaandorp et al 2018 table 1 provides an overview of the data products used in this study we use two types of stress series precipitation and makkink reference crop evapotranspiration makkink reference crop evapotranspiration in the following evapotranspiration describes the potential evapotranspiration from a reference surface covered with grass makkink 1957 the makkink method requires less input variables than the penman monteith method de bruin and lablans 1998 show that the reference crop evapotranspiration estimates obtained using the makkink method correspond well to the reference crop evapotranspiration estimates obtained using the penman monteith method for the netherlands open source precipitation and evapotranspiration data from the royal netherlands meteorological institute knmi are used knmi 2018a b the precipitation data are based on radar data which are corrected using knmi station data by applying ordinary kriging the precipitation data have a spatial resolution of 1 k m by 1 k m the evapotranspiration data are based on extrapolating knmi station data using thin plate spline tps interpolation the station data are calculated by knmi using incoming shortwave radiation and mean daily temperature measurements at the knmi stations hiemstra and sluiter 2011 suggest that at least 15 stations should be used to interpolate daily evapotranspiration at a national scale in total 32 stations are used for the interpolation in the netherlands of these 32 stations three stations are located in or near the twente study area twenthe heino and hupsel the evapotranspiration data are gridded at a spatial resolution of 10 k m by 10 k m we use the smap l3 enhanced radiometer only daily gridded soil moisture product to calibrate the parameters of the tfn models entekhabi et al 2010 chan et al 2018 o neill et al 2018 also we use the smap l3 enhanced product to validate the tfn models the smap enhanced products are developed by applying the backus gilbert optimal interpolation technique to the smap brightness temperature data t a chan et al 2018 the latter is posted on a grid with a spatial resolution of 36 k m by 36 k m chan et al 2016 after various correction and calibration procedures of the regridded brightness temperatures the currently operational smap baseline soil moisture algorithm is used to produce the smap l3 enhanced surface soil moisture estimates the smap l3 enhanced product is gridded with a spatial resolution of 9 k m by 9 k m in general smap soil moisture products perform well in the twente region colliander et al 2017 chan et al 2018 assessed the smap l3 enhanced product using in situ soil moisture measurements they found an unbiased root mean square error urmse of 0 056 m 3 m 3 using the sca v retrieval algorithm for the twente region the smap data products were designed to meet a soil moisture retrieval accuracy of 0 040 m 3 m 3 urmse entekhabi et al 2010 the smap observations are available for the study area approximately every 2 3 days the footprint of the smap l3 enhanced product in the twente region is visualized in fig 2 we have analysed the morning smap retrievals for the period january 1 2016 january 1 2019 the smap soil moisture retrievals are affected by for example sensor noise and uncertainties in surface roughness and vegetation changes by applying the noise model with exponential decay of the residuals equation 5 we partly include these uncertainties in the tfn modelling approach additionally we use in situ soil moisture measurements from a monitoring network to assess whether the tfn models can describe soil moisture field conditions the monitoring network operating since 2009 is maintained by the itc faculty of the university of twente dente et al 2012 van der velde 2018 van der velde et al 2019 both volumetric moisture content and soil temperature are measured at 20 locations in the twente region stations 1 2 3 4 5 11 12 13 14 15 16 17 18 and 19 cover agricultural grass fields stations 6 7 8 9 and 10 cover maize fields station 20 is installed in a forest area the station locations are shown in fig 2 the monitoring network consists of decagon 5tm probes at 5 10 20 40 and 80 cm soil depths and provide a reading every 15 min we use daily averaged measurements at 5 cm soil depth since remotely sensed soil moisture data are limited to surface soil moisture petropoulos et al 2015 benninga et al 2019 fig 2 provides an overview of the smap l3 enhanced footprint relative to the in situ locations in the study area 2 4 general workflow we set up a tfn model including corresponding ir functions for each location of the in situ soil moisture monitoring network in the study area fig 3 shows the general research workflow to set up a tfn model the workflow focuses on three main parts indicated by the yellow boxes in the figure first the tfn models are set up and calibrated using smap data smap calibration next these models are validated using smap data for a different period smap validation finally we assess the applicability of the tfn models for estimating soil moisture at field scales using in situ measurements field validation the numbers 1 to 16 in the following sections refer to the steps shown in fig 3 2 4 1 smap calibration first 1 the input datasets which are described in section 2 3 are selected the datasets are split in a 2 calibration set and a 3 validation set the calibration set is used to determine the parameter sets of the ir functions for the implementation of the tfn models the validation set is used to assess the tfn model results we use a calibration period which covers at least the response time of the hydrological system that we observe we refer to section 3 2 for the determination of the response time especially we are interested in the predictive capabilities of the tfn model for the dry summer period of 2018 therefore the calibration set covers the period january 1 2016 january 1 2018 while the validation set covers the period january 1 2018 january 1 2019 since more smap observations are becoming available the calibration period can be continuously extended in an operational setting we assessed the influence of the calibration period length by performing a sensitivity analysis in which both the length and period of calibration set are varied the calibration periods used for the sensitivity analysis are a summer period april october 2016 a winter period october 2016 april 2017 the full year 2016 and the full year 2017 in addition we assessed the tfn model capabilities by switching the calibration and validation period january 1 2017 january 1 2019 for the calibration set and january 1 2016 january 1 2017 for the validation set section 4 2 discusses the results of the sensitivity analysis and the implications for tfn modelling section 4 3 discusses the tfn model results for the 2016 validation period first 4 the smap series of the calibration set are used as the observational calibration dataset for the tfn model then 5 the stress series are defined for the calibration period subsequently 6 a ir function is defined for each stress series both the observations and the stress series are added to a 7 pastas model object then pastas applies 8 a least squares optimization approach to find 9 optimal parameter sets for the precipitation and evapotranspiration ir functions for each in situ location by solving equation 1 these sets lead to the best fit of the smap soil moisture observations for the calibration period finally the sets are used to estimate soil moisture dynamics in the validation period we assessed the applicability of two functions to define the ir function of each stress series a gamma and an exponential function table 2 lists the combinations the explained variance percentage evp is calculated to assess the applicability of each combination the evp is defined as 6 e v p 100 σ h 2 σ n 2 σ h 2 where σ h 2 is the variance of the smap soil moisture observations m 3 m 3 2 and σ n 2 is the variance of the tfn model residuals as defined in equation 1 m 3 m 3 2 von asmuth et al 2002 an evp of 100 indicates a perfect simulation of the observations since no residuals exist in that case as a rule of thumb one generally accepts the results of a tfn model if the evp 70 van engelenburg et al 2020 additionally the noise series should not be significantly autocorrelated autocorrelation would indicate that the white noise assumption does not hold von asmuth et al 2002 we use the ljung box test to determine whether the noise series shows significant autocorrelation ljung and box 1978 2 4 2 smap validation we use smap observations from the year 2018 to validate the tfn models we define 10 the precipitation and evapotranspiration stress series for the period january 1 2018 january 1 2019 these series are used to 11 set up a pastas model for the validation period the 9 optimized parameter sets from the calibration set are applied to define the ir functions again pastas solves equation 1 using the defined ir functions the optimized parameter sets and the stress series resulting in 12 predictions of soil moisture for the validation period we use 13 the smap validation set to 14 assess the tfn model results using the unbiased root mean square error urmse bias and pearson correlation coefficient see appendix a 2 4 3 field validation furthermore we are interested in the applicability of the tfn models at field scales compared to the regional scales represented by the smap observations therefore we use 15 in situ soil moisture measurements from the soil moisture monitoring network to 16 evaluate the tfn model results at field scales using the urmse bias and pearson correlation coefficient error metrics the evaluation is performed for all in situ location for which measurement data are available for the validation period the following eleven stations provide data for the 2018 validation period 2 4 7 9 10 11 13 14 15 16 and 17 3 results 3 1 selection of ir functions first we evaluated which combination of ir functions leads to the best fit of the tfn models in terms of evp fig 4 shows the spatially averaged evp for the four function combinations as defined in table 2 the gg and ge combinations lead to tfn models which cannot sufficiently explain soil moisture dynamics both the gg and ge combinations show spatially averaged evp values lower than 50 the ee and eg combinations show the best model behaviour even though the eg combination leads to a rejection of the tfn model for one location i e station 16 67 the evp of the ee combination is consistently larger than 70 for all stations exceeding the model acceptance criterion since the ee combination consistently shows good accuracy we use the exponential function for both the precipitation and evapotranspiration ir functions in the remainder of the study because both the gg and ge combinations are rejected the tfn models will be rejected when a gamma function is used for the precipitation stress series so although von asmuth et al 2002 show that the gamma function is suitable to model the response of groundwater head to recharge using precipitation stress series the gamma function is not the best choice for precipitation stress series when modelling surface soil moisture dynamics the difference is less distinct for the evapotranspiration stress series either a gamma or exponential function leads to approximately similar results in terms of evp 3 2 calibrated ir functions the ir functions contain valuable information on the soil moisture response to the stress series fig 5 shows the calibrated ir functions of the precipitation and evapotranspiration stresses for location 2 as expected the precipitation stress series has a positive impact on the soil moisture state while the evapotranspiration stress series decreases the soil moisture state also the time scale of the precipitation stress series is smaller than the time scale of the evapotranspiration stress series furthermore the initial response of soil moisture at day one is much larger for the precipitation stress series than for the evapotranspiration stress series similar observations hold for all individual locations these findings are physically reasonable since precipitation causes immediate spikes in soil moisture while drydown due to evapotranspiration takes place on longer time scales in addition fig 5 shows that the length of the precipitation ir function for location 2 is approximately 75 days while the evapotranspiration ir function has a length of approximately 150 days the length of the ir functions can be interpreted as the system response to that specific stress series the calibration period of two year covers this length multiple times therefore we can conclude that the calibration period is of sufficient length to estimate soil moisture dynamics in the twente region 3 3 assessment of soil moisture modelling we will show the tfn model results for the first in situ location for which a full field validation dataset is available which is location 2 fig 6 shows the tfn model results smap observations and in situ measurements for location 2 during the period january 1 2016 january 1 2019 in both the calibration and validation period the tfn model can correctly simulate the summer winter cycle of drying and wetting large deviations between the smap observations and the tfn model results can be observed in winter periods such as december 2016 and february 2018 the soil can freeze in winter periods which significantly affects smap satellite as well as in situ sensor readings van der velde et al 2019 the use of ir functions prevents the presence of these outliers in the tfn model result in addition as temperature is not part of the input data the tfn model does not detect such periods on the other hand the tfn model underestimates soil moisture during july august 2018 finally the smap observations overestimate soil moisture in the transition from summer to fall in 2018 van der velde et al 2019 evaluated smap surface soil moisture data in the twente region and stated that in the summer fall of 2018 dry spells were ended by a sequence of substantial rain events that exposed the disparity in sampling depth between smap and the in situ sensors shellito et al 2016 and benninga et al 2018 found similar results we quantify the accuracy of the tfn models by calculating the urmse bias and correlation coefficient error metrics with respect to the smap observations for each in situ location in the 2018 validation period the figure also shows the results for the 2016 validation period which are discussed in section 4 2 the dots in fig 7 smap validation 2018 visualize the error metrics of the tfn models results with respect to the smap observations for the year 2018 the urmse varies between 0 059 and 0 070 m 3 m 3 the bias varies between 0 0040 and 0 019 m 3 m 3 and the correlation coefficient varies between 0 79 and 0 82 for all locations recognizing that the tfn models do not consider the over and underestimation of smap in frozen and dry conditions the tfn models perform well in predicting smap surface soil moisture an implication is that the calibrated tfn model can be used to estimate surface soil moisture and extend smap data if precipitation and evapotranspiration data are available in addition fig 7 shows the urmse bias and correlation coefficient of the tfn model results with respect to the in situ soil moisture measurements for eleven locations in the 2018 validation period field validation 2018 although a fundamental difference exists in spatial scales represented by the smap satellite footprint and the in situ measurements the tfn models accurately predict field scale soil moisture for seven out of eleven locations in terms of urmse for four out of eleven locations in terms of bias and for all locations in terms of correlation coefficient 4 discussion 4 1 verification of tfn modelling approach the results show that tfn modelling using the pirfict method can be applied to predict surface soil moisture conditions in the twente region using smap surface soil moisture remote sensing data as calibration set as part of the tfn model verification we assessed whether the noise series show autocorrelation using the ljung box statistical test ljung and box 1978 the autocorrelation is assessed considering a significance level of 0 05 the ljung box test shows that no significant autocorrelation is observed for all stations therefore the white noise assumption holds for all stations it is generally known that the validity of data driven models is often limited to the hydrological conditions for which the model is calibrated e g abrahart et al 2010 kornelsen and coulibaly 2014 mount et al 2016 therefore because of the data based nature of tfn models there is the risk to extrapolate results to situations for which no references are available in the calibration set von asmuth et al 2012 for example the tfn model of location 2 does not capture the extremely dry summer period of 2018 well as seen in fig 6 according to the tfn model the volumetric moisture content drops to almost 0 m 3 m 3 in that period however both the smap observations and the in situ measurements show that soil moisture has a physical lower limit of approximately 0 1 m 3 m 3 the tfn models do not identify the lower limit this limitation might be explained by evapotranspiration reduction which is a non linear mechanism which reduces actual evapotranspiration when only low amounts of moisture are available the linear nature of tfn models indeed decreases their accuracy during long dry periods according to peterson and western 2014 moreover the makkink method describes the potential evapotranspiration which is the maximum evapotranspiration occurring when water is not a limiting factor actual evapotranspiration observations could be considered to improve the tfn model results these findings stress the need for a representative calibration dataset in addition the length of the calibration period may affect the tfn model results the influence of the calibration period will be assessed in the next section 4 2 sensitivity of calibration period we performed a sensitivity analysis on both the calibration set period and the length of the period based on the rmse error metric with respect to the in situ measurements in the 2018 validation period fig 8 shows the results of the sensitivity analysis soil moisture dynamics at some stations cannot be properly explained using the 2016 2017 winter period as calibration period the difference between the summer of 2016 the year 2016 2017 and 2016 2017 calibration periods is not significant the 2016 2017 winter period shows the largest rmse values whereas the 2016 summer calibration set shows the smallest rmse values fig 6 shows that the 2016 2017 calibration set leads to a large underestimation of the 2018 dry summer period the same finding holds for the 2016 and 2017 calibration sets fig 9 shows the tfn model results for location 2 when only the 2016 summer period is used as calibration set using the 2016 summer calibration set the tfn models can simulate the dry period of 2018 correctly thus the tfn models can represent the drought period of 2018 when a representative calibration period is selected we want to stress that the representativeness of the calibration period is defined by the characteristics of the soil moisture dynamics occurring in that period rather than the calendar date one would have to assess the mean variance and other statistics to assess whether the calibration set is representative for the period of interest 4 3 results for 2016 validation period the dry period in the summer of 2018 is an extreme event probably the sensitivity analysis results are case specific and thus not generalizable to test the applicability of the tfn models in more general situations we switched the calibration and validation set periods the tfn models are calibrated for the period january 1 2017 january 1 2019 and validated for the year 2016 the triangles in fig 7 smap validation 2016 show the urmse bias and correlation coefficient of the tfn models with respect to the smap observations when the year 2016 is used as validation set the 2016 results have consistently higher accuracies than the smap 2018 validation results the urmse varies between 0 042 and 0 052 m 3 m 3 the bias varies between 0 0061 and 0 023 m 3 m 3 for the locations and the correlation coefficient varies between 0 82 and 0 88 for the locations furthermore the stars in fig 7 field validation 2016 show the error metrics of the tfn models with respect to the in situ measurements when the year 2016 is used as validation set the following stations have a full dataset for the 2016 validation period 1 2 3 7 9 16 and 18 no apparent change in accuracy is found for the 2016 validation results at field scales which are represented by the in situ measurements 4 4 accuracy of tfn modelling estimates this section discusses the accuracy of the tfn modelling estimates as shown in fig 7 table 3 shows the performance of the soil moisture tfn modelling approach in comparison with other smap validation studies in the twente study area colliander et al 2017 and chan et al 2018 validated the smap and smap enhanced soil moisture products respectively kolassa et al 2018 used the smap microwave observations and a neural network to retrieve surface soil moisture estimates since these studies considered spatially averaged results for the twente study area the tfn model results are also spatially averaged especially the tfn model validated for the year 2016 performs well as shown by the urmse bias rmse and correlation coefficient error metrics less extreme dry and wet events occurred in 2016 in comparison with 2018 the underestimation of the dry 2018 period has a significant impact on the model accuracy which explains the increased accuracy of the tfn models in the 2016 validation period the comparison with the data driven approach by kolassa et al 2018 shows that smap tfn modelling for the year 2016 has a comparable accuracy these results indicate that a calibration period including the extreme dry summer of 2018 will lead to more accurate tfn models for drought predictions especially at spatial scales similar to the smap satellite footprint the smap satellite retrievals have a design accuracy of 0 04 m 3 m 3 in terms of urmse entekhabi et al 2010 which is represented by the red line in fig 7 we want to stress that the accuracy of the original smap l3 enhanced retrievals do not meet this design accuracy in the twente study area 0 056 m 3 m 3 chan et al 2018 the upper panel of fig 7 shows the accuracy of the tfn model estimates with respect to the design accuracy although not meeting the design accuracy smap validation 2018 and smap validation 2016 approach the accuracies found by chan et al 2018 in terms of the urmse bias and correlation coefficient error metrics also the field validation 2016 estimates correspond well with the field observations although local scale issues cause discrepancies for some stations large discrepancies can be found for the field validation 2018 which are mainly caused by large biases as shown in panel b of fig 7 4 5 spatial and temporal resolution of tfn soil moisture estimates the spatial resolution of the tfn soil moisture estimates depends on the spatial resolution of the input data only considering the spatial resolution the reference crop evapotranspiration data form the limiting factor for the tfn soil moisture estimates these data have a resolution of 10 k m by 10 k m see table 1 however evapotranspiration does not vary as much as precipitation over short distances dalezios et al 2002 hess et al 2016 therefore we assume that the tfn soil moisture estimates are bound by the spatial resolution of the smap input data which is 9 k m by 9 k m the results shown in table 3 and fig 7 reflect this statement table 3 shows that at a regional scale the accuracy of the tfn soil moisture estimates approximates the accuracy of other soil moisture products we define a regional scale as the extent of our study area which approaches the spatial resolution of the smap passive soil moisture product 36 k m by 36 k m at local scales the accuracy of the tfn soil moisture estimates mainly depends on whether the smap input data reflect local conditions as shown in fig 7 the in situ soil moisture monitoring stations represent these local scales although the correlation shows good corresponding between the tfn soil moisture estimates and in situ data the urmse and bias error metrics show large discrepancies for approximately half of the locations therefore the spatial resolution of the input data is an important aspect which should be considered in tfn modelling in addition the smap soil moisture retrievals are limited to shallow soil depths petropoulos et al 2015 as the ir functions are calibrated using the smap data the tfn soil moisture estimates are also limited to shallow soil depths other methods such as data assimilation using process based models liu et al 2012 pezij et al 2019b are necessary for the translation to soil moisture in deeper layers in contrast the temporal resolution of the tfn soil moisture estimates only partially depends on the temporal resolution of the input data the ir functions enable prediction of soil moisture if precipitation and reference crop evapotranspiration data are available as both the precipitation and reference crop evapotranspiration are available on a daily basis the temporal resolution of the tfn soil moisture estimates is also considered to be on a daily basis as the smap satellite does not provide daily soil moisture observations the tfn models can be used to fill the data gaps occurring on days without a satellite overpass as discussed in section 3 3 the smap input data have an availability of 2 3 days this study shows that this temporal resolution is sufficient to develop ir functions for accurately estimating soil moisture however the soil moisture response due to some individual precipitation events might not be included in the calibration procedure if the smap satellite does not overpass the area of interest therefore the accuracy of the tfn soil moisture estimates might be affected when heavy precipitation events are missed 4 6 water system characteristics generally ir functions can provide information on characteristics of the system which is observed among others these functions describe the unit step response and response time of groundwater dynamics when a groundwater system is observed bakker et al 2008 zaadnoordijk et al 2018 the exponential function applied in this work has two parameters a and a see equation 4 the parameter a is related to the total change in soil moisture due to a unit stress a large a indicates a large total change the shape parameter a is related to the time scale on which a unit stress affects soil moisture a large a indicates a slow response fig 10 shows the calibrated parameters for the precipitation and evapotranspiration ir functions for all locations based on the 2016 2017 calibration set we have assessed the relationship of the parameters with the following variables longitude latitude soil elevation vegetation type and soil type the elevation data are obtained from actueel hoogtebestand nederland 2019 the vegetation characteristics are obtained from van der velde et al 2019 the soil type characteristics are obtained from the bofek2012 dataset wösten et al 2013 the next sections will elaborate on the relationships found for parameters a related to the total change and a related to the response time 4 6 1 total change the change in total soil moisture volume due to precipitation is strongly correlated to longitude the change is larger for stations in the western part of the study area longitude is strongly correlated with distance to the coast in the netherlands daniels et al 2014 found that on average the precipitation amount is higher near the coast similarly the change in total soil moisture due to evaporation is correlated to latitude the change is larger for stations in the north of the study area in addition the total change in soil moisture might be related to soil physical characteristics and vegetation type however the elevation soil type and vegetation type do not show clear patterns with respect to the total change in soil moisture these characteristics are quite homogeneous for the locations in the study area with mainly sandy soils and grass vegetation a valuable addition can be to study whether different ir functions and corresponding parameters are found in areas with other soil characteristics such as peaty or clayey areas or areas with different vegetation types 4 6 2 response time a linear trend is seen in the spatial distribution of parameter a in relation with longitude in general if a large soil moisture response time scale due to precipitation is found at a specific location the corresponding soil moisture response time scale due to evapotranspiration is relatively small and vice versa the time scale of the precipitation ir function is larger for locations in the western part of the study area also the subsurface in the western part of the study area contains thick sand layers precipitation infiltrates relatively easy in sandy layers which causes a fast response of shallow soil moisture moraines of clay are found in the eastern part of the study area clay layers have low infiltration rates which results in a slow increase of soil moisture content however the soil type characteristics do not show a clear relation with respect to the a parameter the latitude elevation and vegetation type characteristics do not show clear patterns with respect to the soil moisture response time more research is needed to generalize these findings one should be careful in relating these parameters to physical processes as the selection of a representative ir function is an assumption von asmuth et al 2012 for example fig 5 shows that the time scale of the precipitation ir function is approximately 75 days to our knowledge this time scale cannot be directly connected to physical phenomena more research on the ir function parameters is needed to increase the understanding of their physical meaning in terms of soil moisture 5 conclusions we studied the applicability of transfer function noise modelling tfn for describing and predicting soil moisture dynamics tfn modelling is a fast alternative for process based models taking only seconds to simulate a full year of daily soil moisture conditions tfn modelling is based on the assumption that soil moisture dynamics can be explained by linearly transforming precipitation and evapotranspiration stress series using impulse response ir functions the smap l3 enhanced surface soil moisture product is used to calibrate the tfn models we found that exponential functions describe the ir functions of both the precipitation and evapotranspiration stress series better than gamma functions tfn models describe soil moisture conditions well when comparing the tfn model results with the smap observations in addition the tfn model results were compared with in situ soil moisture measurements to assess the field scale applicability of tfn modelling the accuracy of the tfn models mainly depends on the representation of the smap satellite product for that specific spatial scale a practical application for operational water management is that the tfn modelling approach can be used to estimate soil moisture dynamics using predictions of precipitation and evapotranspiration the application is promising if sufficient calibration data are available although one should be careful when interpreting results in extreme situations since the tfn models do not consider the physical lower and upper limits of soil moisture however a sensitivity analysis and variation of the calibration and validation periods showed that selecting a suitable calibration period can significantly increase the tfn model capabilities in both regular and extreme situations in addition the ir function parameters potentially provide valuable information on water system characteristics such as the total response and the response times of soil moisture to precipitation and evapotranspiration thus the tfn models have value in explaining hydrological processes and characteristics in contrast to other data driven tools such as neural networks which are black boxes however more research on the physical meaning of these parameters is needed to understand their applicability concluding we consider the applicability of tfn modelling for explaining soil moisture dynamics promising and propose to explore the possibilities of tfn modelling for predicting soil moisture in operational settings declaration of competing interest the authors declare that they have no know competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work is part of the owas1s research programme optimizing water availability with sentinel 1 satellites with project number 13871 which is partly financed by the netherlands organisation for scientific research nwo we want to thank all owas1s programme partners for their contribution the research data are freely available at the 4tu researchdata repository https doi org 10 4121 uuid ba33fc56 e07b 4547 9630 9b1565d18040 the authors want to thank the water resources department of the itc faculty of the university of twente in particular rogier van der velde and harm jan benninga for sharing the in situ soil moisture measurements from their monitoring network the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper appendix a error measures the root mean square error rmse is defined as a 1 r m s e j 1 n θ j o b s θ j p r e d 2 n in which θ j o b s are the smap soil moisture measurements for each day j m 3 m 3 θ j p r e d are the tfn model results for each day j m 3 m 3 and n is the number of observations the unbiased root mean square error urmse is defined as a 2 u r m s e j 1 n θ j o b s θ o b s θ j p r e d θ p r e d 2 n in which θ o b s is the arithmetic mean of the smap soil moisture measurements m 3 m 3 and θ p r e d is arithmetic mean of the tfn model results m 3 m 3 the bias is defined as a 3 b i a s θ o b s θ p r e d the pearson correlation coefficient r is defined as a 4 r j 1 n θ j o b s θ o b s θ j p r e d θ p r e d j 1 n θ j o b s θ o b s 2 j 1 n θ j p r e d θ p r e d 2 appendix b supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix b supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104756 
26002,adapting water resources systems to climate change requires identifying hydroclimatic signals that reliably indicate long term transitions to vulnerable system states while recent studies have classified the conditions under which vulnerability occurs i e scenario discovery there remains an opportunity to extend such methods into a dynamic planning context to design and assess early warning signals this study contributes a machine learning approach to classifying the occurrence of long term water supply vulnerability over lead times ranging from 0 to 20 years using a case study of the northern california reservoir system results indicate that this approach predicts the occurrence of future vulnerabilities in validation significantly better than a random classifier given a balanced set of training data accuracy decreases at longer lead times and the most influential predictors include long term monthly averages of reservoir storage dynamic early warning signals can be used to inform monitoring and detection of vulnerabilities under a changing climate 1 introduction climate change requires water supply planners to navigate significant uncertainty in future precipitation projections which in many regions disagree on the magnitude and direction of change hallegatte 2009 much of this uncertainty centers on extreme drought and flood events which are expected to become more frequent and severe in the coming decades trenberth et al 2014 polade et al 2017 in general this uncertainty prevents the optimal planning of adaptations and requires innovative approaches such as bottom up vulnerability assessment a key feature of robust planning frameworks lempert and collins 2007 wilby and dessai 2010 herman et al 2015 bottom up methods focus on identifying the conditions under which vulnerability occurs using either a wide range of plausible scenarios e g bryant and lempert 2010 or scenario narratives driven by local decision contexts rounsevell and metzger 2010 carlsen et al 2013 this classification problem known as scenario discovery lempert et al 2008 has benefitted from the application of a variety of statistical methods hadka et al 2015 kwakkel 2015 quinn et al 2018 the main outcome of scenario discovery methods is a trained classifier capable of mapping uncertain scenario properties to a binary outcome vulnerable or not for each scenario aggregated over time however a related question remains less explored under climate uncertainty can system vulnerabilities be detected in advance this is highly relevant to dynamic planning approaches where adaptations are taken over time in response to observed and projected information haasnoot et al 2013 zeff et al 2016 hui et al 2018 fletcher et al 2019 it also relates to the challenge of anticipating tipping points in environmental systems scheffer et al 2009 2012 in the case that robust planning proves too costly e g borgomeo et al 2018 dynamic planning may increase the effectiveness and appropriateness of adaptations both preventing over investments in unnecessary infrastructure and lessening the severity of vulnerabilities if detection methods are sufficiently accurate dynamic planning approaches have in common the need to design a policy mapping information to actions which could include either observations or predictions of vulnerable states herman et al 2020 while the use of observations to trigger adaptations has been explored in detail by bottom up methods such as scenario discovery and dynamic adaptive policy pathways haasnoot et al 2013 there remains significant opportunity to study the second case by predicting the occurrence of vulnerable states dynamically in other words by designing statistical early warning signals for adaptation the process of designing early warning signals first requires the selection of informative feature predictor variables due to the dynamic aspect of the problem this includes both the type of variable as well as the timescale aggregation window and statistical transformation for example the 30 year moving average of annual reservoir inflow raso et al 2019a feature selection can affect both efficiency and accuracy and choosing informative features can leverage both human expertise and statistical techniques dietterich 2002 this concept is analogous to input variable selection in the water resources field guyon and elisseeff 2003 galelli et al 2014 which has been widely used to support reservoir policy search e g giuliani et al 2015 however the longer timescales involved in the climate adaptation problem create additional challenges it may be more difficult to separate signal from noise hegerl and zwiers 2011 hawkins and sutton 2012 and the decisions that the features are meant to inform are often irreversible raso et al 2019b this challenge is closely related to the choice of signposts for adaptation policies which are evaluated based on their relevance credibility and legitimacy haasnoot et al 2018 relevance refers to the predictive skill of a monitoring system in observing long term trends amid short term variability analysis of predictive skill has focused on both type i errors representing over investment as well as type ii errors representing under investment rosner et al 2014 stephens et al 2018 raso et al 2019 the goals of credibility and legitimacy reflect the fact that monitoring systems must inform human decisions within a broader context of objectives actions and spatiotemporal scales among multiple actors and institutions hermans et al 2013 2017 the interpretability of early warning signals is therefore critical and may be supported by the parsimony of signpost variables as well as the completeness of the sources of uncertainty considered raso et al 2019a in total these goals for a monitoring system may reflect a tradeoff between predictive skill and interpretability that is widely recognized in statistical modeling for example while several studies have considered adaptations triggered by linear threshold values hallegatte et al 2012 walker et al 2013 robinson and herman 2019 such signals could also be represented by more complex functions potentially incorporating multiple variables on different timescales herman and giuliani 2018 nayak et al 2018 this study considers the potential for nonlinear multivariate classifiers to address this problem recognizing that improvements in predictive skill will likely be met with a decrease in interpretability to address this challenge this study frames the design and testing of early warning signals under climate change as a machine learning classification problem using observations of human and hydrologic variables to predict the binary occurrence of future water supply vulnerability in a systems model the goal is to detect vulnerability without knowledge of future forcing which would be required in a forward simulation of the system specifically this paper addresses the following research questions to investigate the utility of the proposed method 1 can machine learning classification techniques predict long term vulnerability of a water resources system in advance and are these predictions significantly better than random 2 how is the accuracy of the prediction affected by the lead time and vulnerability threshold 3 can the interpretability of the classifiers be assessed and improved using feature importance i e can the feature set be simplified while retaining accuracy to support real world applications the proposed methodology is intended as a tool to support adaptive planning under a changing climate specifically by using these early warning signals to inform monitoring and detection of vulnerabilities 2 methods a flowchart of models and datasets used in this experiment is shown in fig 1 the experiment is demonstrated using a case study of the northern california reservoir system a large network of storage and conveyance infrastructure designed primarily to move winter precipitation from north to south to support summer irrigation the largest consumptive use is represented by the 7 9 million acres of irrigated farmland generating 100 billion annually in agricultural production followed by urban use by california s 39 7 million residents johnson and cody 2015 this system complexity yields many options for adaptation but is complicated by the fact that it is managed by hundreds of distinct agencies utilities and districts making coordination difficult hanak and lund 2012 while the effects of climate change on average precipitation in california remain uncertain projections agree on increases in the frequency of both wet and dry extremes as well as the increased likelihood of these extremes occurring sequentially swain et al 2018 placing more stress on the long term planning and operation of water supply storage 2 1 data sources precipitation and temperature data are taken from an ensemble of downscaled cmip5 projections publicly available from the u s bureau of reclamation brekke et al 2014 this daily timestep dataset contains hydrologic projections streamflow and snowpack for multiple point and gridded locations created by routing precipitation and temperature through the variable infiltration capacity vic hydrologic model liang et al 1994 ensemble projections are available for 31 global climate models gcms and four rcps as shown in supplemental table s1 fig 2 shows the locations of streamflow 11 snowpack 4 and precipitation and temperature 3 gages used 2 2 simulation model the operation of reservoirs in california orca model cohen et al accepted is used to simulate the operation of the northern california reservoirs under all 97 cmip5 climate scenarios on a daily timestep over the period 2000 2100 given the input data shown in fig 2 the model simulates the operations of shasta oroville and folsom reservoirs which are located respectively on the sacramento feather and american rivers in addition to reservoir management the model also simulates the operations of south of delta exports via pumping from the sacramento san joaquin delta to meet urban and agricultural demands via the central valley project cvp and state water project swp while also meeting environmental flow and salinity requirements for the delta for the purposes of this study the supply reliability of delta exports is the key model output that determines system vulnerability the model has been found to adequately reproduce historical operations of the system on a daily timestep with nash sutcliffe efficiency above 0 9 for reservoir storage model code and documentation can be found at https github com jscohen4 orca more details about the origin use and locations of each type of data described above can be found in supplemental table s2 2 3 feature and target data the classification problem is to predict system vulnerability at a certain lead time given a set of feature variables vulnerability is determined based on the 30 year moving average of the supply reliability of water exports from the delta a metric that reflects a focus on identifying long term trends rather than a single drought period this metric is based on meeting the target demand for a certain fraction of monthly timesteps vulnerability occurs when the 30 year average supply reliability falls below a chosen threshold resulting in a single binary target metric for classification while this approach does not distinguish between different magnitudes of vulnerability we adopt this binary classification following the standard for scenario discovery methods in the water resources field this study tests a range of possible threshold values to understand the impact on classifier performance recognizing that vulnerability definitions in practice are determined by decision makers the threshold value plays a key role in determining the balance of positive and negative classifications in the training set the feature variables used to classify future vulnerabilities each year include time series of the hydrologic variables shown in fig 2 as well as several internal states of the simulation model these variables are aggregated to annual and monthly values using either the mean maximum or sum they are then translated into moving averages and standard deviations using timescales of 10 20 30 40 and 50 year rolling windows this differentiation by timescale and statistic is conducted to improve the ability of the machine learning methods to detect important trends in the data ahmed et al 2010 given that many of the moving windows share overlapping data several of the features are expected to be correlated a summary of feature variables is shown in table 1 2 4 machine learning methods several classification methods are selected table 2 to learn from the annual and monthly data to classify water supply reliability as vulnerable below a chosen threshold or not vulnerable above a chosen threshold at lead times of 0 1 5 10 and 20 years these methods were chosen from the many classification methods available in the open source scikit learn library pedregosa et al 2011 scikit learn 2019 based on their widespread use and demonstrated effectiveness for nonlinear problems by examining the performance of each classifier against the others and a random classifier which serves as a baseline application specific insights can be drawn about the similarity of their performance and the likelihood of misclassifications the random classifier guesses proportional to the ratio of possible outcomes with an accuracy equal to the square of the ratio of possible outcomes all methods in table 2 are implemented using default parameter settings from the scikit learn library it is recognized that these parameter choices can significantly impact the performance and that a meta level analysis would be needed to determine the optimal parameter settings each of the classification methods follows the same prediction structure f x t v u l n e r a b l e p t l e a d t h r e s h o l d n o t v u l n e r a b l e p t l e a d t h r e s h o l d where f is the fitted function embedded in each of the classification methods using the features x t to evaluate the function and make predictions if the prediction p at the specified lead time is less than the threshold the instance is classified as vulnerable otherwise the instance is classified as not vulnerable the classification methods in table 2 differ primarily in the family of functions used to represent f more recently developed algorithms are denoted with citations in table 2 while the other fundamental methods can be referenced in hastie et al 2009 importantly this classification approach is not meant to emulate the reservoir system model itself it is predicting based on a combination of hydrologic and system observations whether it is likely to be on a trajectory toward a long term vulnerable state 2 5 experimental design next each method is fit to the training data lagged predictors and binary targets and re evaluated against held out validation data using a repeated leave one out approach from the ensemble of cmip5 scenarios one scenario is held out for testing while the classifier is trained on the remaining 96 scenarios the process is then repeated for all scenarios this design provides several benefits the evaluation scenario is hidden from the classifier during training the temporal structure of the data is preserved which prevents biased training or testing with either too many values from the beginning or the end of the century and it represents the realistic case in which the future hydrologic forcing is unknown e g to evaluate in a systems model but where recent observations can be used to make a statistical prediction in the next section all results will be reported in the validation stage using the ensemble of leave one out experiments to estimate confidence intervals for the prediction accuracy in summary this experiment tests seven classification methods five lead times ranging from 0 to 20 years vulnerability thresholds ranging from 0 60 to 0 86 the full range in which both positive and negative classifications are possible and three numbers of features 5 10 and 500 as described in the following paragraph in each of these cases the machine learning method classifies each prediction as either a true positive t p i t false positive f p i t false negative f n i t or true negative t n i t the primary metrics used to analyze classifier accuracy are the true positive and negative ratios which are the fraction of possible positive negative outcomes that are correctly predicted the number of possible outcomes in each class is determined by counting the occurrences in the test set the results from each of the combinations are evaluated against the baseline to test the null hypothesis that the accuracy of the machine learning classifiers is no better than random if the null hypothesis is rejected p 0 05 then the accuracy of the machine learning classifiers is significantly better than random we consider this the absolute minimum standard to evaluate the practical utility of the approach finally the original set of feature variables is too large for some of the classifiers to converge the set is reduced using feature importance scores determined based on the frequency of occurrence of each feature in a random forest of 10 000 trees a default method from the scikit learn library with more occurrences corresponding to a higher importance the 500 features with the highest importance scores are used in the training step before training all feature and target data are scaled to unit variance additional cases are considered in which the feature set is reduced to 5 and 10 features again based on the importance scores from the random forest method fig 3 shows conceptually how a classifier attempts to predict at every time step whether the long term water supply reliability of the system will be vulnerable below the threshold or not vulnerable above the threshold at a given lead time which in this example is 10 years time t l is the year the scenario becomes vulnerable and only the five most informative features are shown the gray dotted lines show information that is not available to the classification methods when they are making a prediction at time t years the long term vulnerability is always based on the 30 year trailing average water supply reliability regardless of the lead time at which the prediction is made 3 results 3 1 classifier accuracy as a function of threshold value the ability to detect early warning signals of water supply vulnerability depends in part on how frequently these events occur in the training data which is controlled by the threshold value as well as the strength of the climate change signal relative to noise in each of the scenarios fig 4 compares true positive and true negative ratios for vulnerability thresholds between 0 6 and 0 86 for a fixed lead time of five years each classification method contains the median 10th and 90th percentiles across the ensemble of leave one out experiments the performance of a random classifier is included as a benchmark black line which always guesses proportional to the ratio of possible outcomes with an accuracy of the ratio squared while all methods perform similarly the naïve bayes classifier shows a slight advantage with only one median value below 0 8 for both true positives and true negatives depending on the ratio of possible positives and negatives dotted line imbalance in the training data may lead to low rates of true classifications with the best performance near the center of the range of possible thresholds 0 4 0 6 this is reflected with the benchmark random classifier showing that at unbalanced ratios thresholds below 0 66 or greater than 0 84 some methods fail to perform better than random understanding the effects of the training set imbalance can inform whether true positive or true negative classifications are more likely to be accurate a well known challenge for machine learning methods in water resources applications the choice of the vulnerability threshold is left to the decision maker and cannot be selected arbitrarily to ensure an accurate classifier however a decision maker may choose a different classifier threshold to balance the tradeoff between false positives and false negatives to increase the effectiveness of the early warning signal in a real world institutional context this analysis underscores that if vulnerabilities are rare among the set of climate projections tested then by definition it will be difficult to train a machine learning model to predict them and that the estimates of prediction accuracy should be accompanied by confidence intervals to improve their interpretability for stakeholders in addition while the median true positive and true negative ratios suggest substantial improvement over the random classifier benchmark the 10th percentiles indicate the lower range of performance across the validation ensemble and in particular sometimes fail to outperform the random classifier the 90th percentile markers generally show an accuracy of 1 0 excepting some classifiers for thresholds below 0 70 for true positive ratios and above 0 84 for true negative ratios we return to the question of statistical significance in section 3 3 fig 4 only shows the true positive and true negative ratios for a single lead time 5 years similar figures for other lead times can be found in supplemental fig s1 3 2 classifier accuracy as a function of lead time accuracy was also evaluated across lead times for a fixed threshold of 0 76 fig 5 in general the median accuracy ratios decrease with lead time with the exception of the true positive ratio for the adaboost classifier the confidence intervals triangles suggest that all classifiers generally outperform the random classifier except for the true negative ratios at a 20 year lead time suggesting a lack of skill at the lower end of the validation ensemble the spread of the confidence intervals increases with lead time though this observation is most apparent for the true negative ratios the 90th percentiles of all distributions fall at or near an accuracy of 1 0 across all lead times finally the true positive ratio of the random classifier black increases with longer lead times due to the increase in possible positive outcomes further ahead in the century the practical implication of this is to raise the standard of performance needed for the other algorithms to outperform the random benchmark at longer lead times 3 3 statistical significance to determine whether to reject the null hypothesis that the accuracy of a machine learning classifier is equal to that of a random classifier the p values for all combinations of lead times and thresholds must be examined by rejecting the null hypothesis the alternative hypothesis the accuracy of the machine learning classifier is better than a random classifier will be accepted the null hypothesis will be rejected for a particular combination of lead time and threshold value with p 0 05 fig 6 shows a heatmap of p values for true positives and true negatives as a function of both lead time and threshold value for the random forest classifier similar heat maps showing results for the other classifiers can be found in supplemental figs s2 through s7 the p values are determined by the percentile of the leave one out distribution that falls below the accuracy of the random classifier indicating the likelihood of the classifier performing worse than random for many combinations of threshold and lead time the classifier performs significantly better than random p 0 05 fig 6 also suggests that threshold values have a larger impact on accuracy than lead times likely driven by the ratio of possible positive to possible negative classifications only a narrow range of threshold values 0 70 0 76 exists in which both the true positive and true negative classifications are significantly better than random for all lead times 3 4 feature importance to reduce the number of features used to classify vulnerability the importance of each feature must be determined the top five features for each lead time are shown below table 3 ranked in the order of importance the most important feature for all lead times is the 30 year moving average of oroville reservoir storage which appears as either the first or second ranked feature for each of the lead times the 30 year average of shasta storage is the second most common feature in table 3 however for a 20 year lead time the most important feature is the annual maximum air temperature at folsom dam which likely reflects the longer term temperature trends associated with climate change the most common months in the important features are july august and june which are the drier months in california and have the potential to carry important signals in a water system in which intra annual water storage is vital these influential features are also highly correlated with each other see supplemental fig s8 the finding that summer reservoir storage dominates the ability to detect early warning signals is not surprising but also is not obvious the target prediction is not water supply vulnerability in a given year but rather a long term trend in the 30 year average water supply reliability this result suggests that the role of reservoir storage in integrating different aspects of the hydrologic cycle also make it a good indicator for future change provided that the system operations remain the same as assumed in this study this can also be interpreted in light of the storage to inflow ratios of each reservoir which are approximately 0 80 0 93 and 0 43 for shasta oroville and folsom reservoirs respectively fig 7 shows the effect of reducing the number of features used with the machine learning methods given a threshold of 0 76 and a 5 year lead time where the number of features are prioritized according to their relative importance using the random forest method table 3 across all methods the true negative ratios are higher above 0 95 from 5 to 10 features and above 0 9 for 500 features than the true positive ratios ranging between 0 8 and 0 95 for all features in general most ratios have a slight increase in performance from 5 to 10 features performance does not show significant changes between 10 and 500 features except for the logistic regression true negative ratio which falls from about 1 0 to 0 9 the 10th percentiles range from 0 57 to 0 76 for the true positive ratios and range from 0 45 to 0 6 for the true negative ratios both outperforming the random classifier overall these results imply that the number of features can be reduced from 500 to 5 with only small reductions if any in the true positive and true negative ratios due to the high correlation among features with overlapping rolling windows in general feature importance can be linked to the signal to noise ratio of each feature variables that change more slowly such as the storage of large reservoirs or the annual temperature likely provide more reliable signals than observations with a more variable response to climate forcing the reduced complexity of this problem will improve opportunities for practical application feature reduction figures for lead times of 0 1 10 and 20 years can be found in the supplemental material fig s9 3 5 accuracy over time the previous results consider the true positive and true negative ratios for different parameters aggregated over the entire century fig 8 shows the true positive and true negative ratios as they change over the century for classifiers trained on the full time period using a threshold of 0 76 and a 5 year lead time see supplemental fig s10 for 0 1 10 and 20 year lead times the random benchmark classifier for each year is also shown in general the true positive ratios become more accurate throughout the century while the true negative ratios become less accurate importantly the classifier is not being retrained over time only applied to new data which explains the decreasing true negative rate a higher true positive ratio later in the century means that it is easier to correctly make a vulnerable positive classification later in the century when many of the features show stronger climate change signals most of the methods perform better than the random classification benchmark except for some true positive ratios before 2040 and some true negative ratios before 2020 in summary results suggest that the machine learning classifiers outperform the random classifier benchmark for most lead times under thresholds with a balanced ratio of training outcomes for a reduced set of features and for most years throughout the century the classifications generally do not show statistically significant differences in performance figs 4 5 and 7 though this finding may not generalize to other applications additionally only a few influential features mostly reservoir storage variables and their transformations are responsible for the predictions this may indicate that the reservoir storage values are able to uniquely aggregate input information given that the other features are either influencing or influenced by reservoir storage 4 discussion these experiments have analyzed the predictive skill of machine learning classifiers trained to detect future water supply vulnerabilities under climate change the analysis has therefore considered several of the goals of a monitoring system haasnoot et al 2018 by exploring how the reliability and observability of early warning signals change at different levels of timeliness and vulnerability however the remaining goals of credibility and legitimacy have not been quantified here and it is recognized that the use of machine learning classifiers rather than linear thresholds will hinder the interpretability of this approach for stakeholders the attempt to improve interpretability in this study depends on feature importance table 2 to prune the set of input variables which may support the parsimony of a monitoring system raso et al 2019b additionally the logic of the dominant features is demonstrated within the context of the system reservoir storage integrates hydrologic and demand dynamics over time and therefore provides the most reliable signal of vulnerability interpretability may be further improved by monitoring a continuous variable from the classifier such as the class probability rather than the binary prediction which might provide a more reliable signal of change similar to the p value detection method proposed by haasnoot et al 2018 in general explainability is a rapidly advancing area of machine learning doshi velez and kim 2017 xie et al 2020 that will likely yield developments to support environmental systems analysis in the coming years classifier skill strongly depends on the extent to which the training data reflects the range of possible future scenarios this is true of any machine learning problem and arises in two key aspects of this study first unbalanced training data cause difficulty in classifying positive and negative outcomes this may be amplified by relatively small sample sizes in the training set with significant implications for water resources planning under climate extremes the second challenge more specific to this problem is that of deep uncertainty in the climate scenarios it is entirely possible that the future hydrology will depart significantly from the training data due to a combination of model uncertainty emissions scenarios and natural variability this study employs a leave one out training and validation strategy to partially account for potential bias by testing whether the classifier can generalize to 1 other realizations of a similar uncertainty characterization and 2 other gcm and rcp combinations with different uncertainty characterizations however good out of sample performance is perhaps less reassuring here than in typical machine learning problems relying on large datasets with well characterized uncertainty a more complex validation approach could consider alternate ensembles generated with different climate models or expert judgment of bias in the training data as in any study of deeply uncertain futures the findings are contingent on the inherently subjective design of the training and validation experiments this study is only partially linked to a specific decision context it aims to analyze the range of timescales and vulnerability thresholds over which reliable prediction of water supply vulnerability might be possible a real world decision context would also include the adaptations to be selected when detection occurs a subject of ongoing work as well as the necessary timescales for each for example water conservation and regulation may benefit from information on annual or sub annual lead times while infrastructure may require a decade or more the findings are also specific to the range of uncertainty demonstrated in the water supply projections for this system which are quite large nearly 50 change in mean annual flows by the end of the century arising from a combination of gcm and emissions uncertainty even so we do not achieve a complete representation of all sources of uncertainty in the early warning system raso et al 2019a particularly the endogenous uncertainties arising from changes to system operations or water demand additionally we do not attempt to evaluate how stakeholders learn from monitoring information in their decision making process hermans et al 2013 or the extent to which stakeholders with different problem framings hermans et al 2017 quinn et al 2018 may find the early warning signals convincing within their system of organizational decision making haasnoot et al 2018 much interesting work remains at this intersection of statistical modeling and policymaking for wicked problems that by definition do not lend themselves to straightforward prediction rittel and webber 1973 kwakkel et al 2016 5 conclusions this paper contributes a methodology for detecting early warning signals of water supply vulnerabilities under climate change using machine learning demonstrated on a case study of the northern california reservoir system among the many goals of a monitoring system relevance credibility and legitimacy proposed by haasnoot et al 2018 this study has primarily focused on relevance represented by the predictive skill of detecting future change results indicate that the classification methods generally outperform a benchmark random classifier though the factor most strongly influencing this result is the balance of the training data determined by the vulnerability threshold in addition the overall classification accuracy decreases with larger lead times to improve the interpretability and parsimony of the resulting classifiers raso et al 2019b the feature set can be reduced with minimal impact on accuracy due to high correlation between features at short lead times the features most strongly influencing the predictions are long term averages of summer reservoir storage which demonstrates predictive power in the ability of storage to integrate different aspects of the hydrologic cycle with further work to analyze the credibility and legitimacy of this approach in a real world decision context with significant human and institutional uncertainties hermans et al 2013 2017 this approach could be implemented as a tool to support water resources planning under climate uncertainty an additional limitation is the assumption that the system infrastructure and operations remain unchanged throughout the century the trained classifiers are expected to become less accurate over time as a result of endogenous adaptation a topic of ongoing work however even when ensemble climate projections suggest substantial uncertainty in future hydrology this approach can help to identify what signals should be monitored to inform adaptation while this study has developed vulnerability classification methods in line with previous work on scenario discovery future work will consider regression methods to identify the magnitude of failure as well additional research will focus on integrating these dynamic vulnerability classifications with an adaptive infrastructure planning problem where early warning signals can be used directly to trigger decisions this analysis will provide insights into the benefits of predicting vulnerabilities along with the consequences of inaccurate classifications including the costs of unnecessary adaptations and the regrets of foregoing beneficial ones declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements and software availability this work was partially supported by the u s national science foundation grants cnh 1716130 and cbet 1803589 any opinions findings and conclusions are those of the authors and do not necessarily reflect the views or policies of the nsf all code and data used in this study is available on github https github com brobinson3 early warning signals ml orca we further acknowledge the world climate research program s working group on coupled modeling and the climate modeling groups listed in the supplement of this paper for producing and making available their model output appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104781 
26002,adapting water resources systems to climate change requires identifying hydroclimatic signals that reliably indicate long term transitions to vulnerable system states while recent studies have classified the conditions under which vulnerability occurs i e scenario discovery there remains an opportunity to extend such methods into a dynamic planning context to design and assess early warning signals this study contributes a machine learning approach to classifying the occurrence of long term water supply vulnerability over lead times ranging from 0 to 20 years using a case study of the northern california reservoir system results indicate that this approach predicts the occurrence of future vulnerabilities in validation significantly better than a random classifier given a balanced set of training data accuracy decreases at longer lead times and the most influential predictors include long term monthly averages of reservoir storage dynamic early warning signals can be used to inform monitoring and detection of vulnerabilities under a changing climate 1 introduction climate change requires water supply planners to navigate significant uncertainty in future precipitation projections which in many regions disagree on the magnitude and direction of change hallegatte 2009 much of this uncertainty centers on extreme drought and flood events which are expected to become more frequent and severe in the coming decades trenberth et al 2014 polade et al 2017 in general this uncertainty prevents the optimal planning of adaptations and requires innovative approaches such as bottom up vulnerability assessment a key feature of robust planning frameworks lempert and collins 2007 wilby and dessai 2010 herman et al 2015 bottom up methods focus on identifying the conditions under which vulnerability occurs using either a wide range of plausible scenarios e g bryant and lempert 2010 or scenario narratives driven by local decision contexts rounsevell and metzger 2010 carlsen et al 2013 this classification problem known as scenario discovery lempert et al 2008 has benefitted from the application of a variety of statistical methods hadka et al 2015 kwakkel 2015 quinn et al 2018 the main outcome of scenario discovery methods is a trained classifier capable of mapping uncertain scenario properties to a binary outcome vulnerable or not for each scenario aggregated over time however a related question remains less explored under climate uncertainty can system vulnerabilities be detected in advance this is highly relevant to dynamic planning approaches where adaptations are taken over time in response to observed and projected information haasnoot et al 2013 zeff et al 2016 hui et al 2018 fletcher et al 2019 it also relates to the challenge of anticipating tipping points in environmental systems scheffer et al 2009 2012 in the case that robust planning proves too costly e g borgomeo et al 2018 dynamic planning may increase the effectiveness and appropriateness of adaptations both preventing over investments in unnecessary infrastructure and lessening the severity of vulnerabilities if detection methods are sufficiently accurate dynamic planning approaches have in common the need to design a policy mapping information to actions which could include either observations or predictions of vulnerable states herman et al 2020 while the use of observations to trigger adaptations has been explored in detail by bottom up methods such as scenario discovery and dynamic adaptive policy pathways haasnoot et al 2013 there remains significant opportunity to study the second case by predicting the occurrence of vulnerable states dynamically in other words by designing statistical early warning signals for adaptation the process of designing early warning signals first requires the selection of informative feature predictor variables due to the dynamic aspect of the problem this includes both the type of variable as well as the timescale aggregation window and statistical transformation for example the 30 year moving average of annual reservoir inflow raso et al 2019a feature selection can affect both efficiency and accuracy and choosing informative features can leverage both human expertise and statistical techniques dietterich 2002 this concept is analogous to input variable selection in the water resources field guyon and elisseeff 2003 galelli et al 2014 which has been widely used to support reservoir policy search e g giuliani et al 2015 however the longer timescales involved in the climate adaptation problem create additional challenges it may be more difficult to separate signal from noise hegerl and zwiers 2011 hawkins and sutton 2012 and the decisions that the features are meant to inform are often irreversible raso et al 2019b this challenge is closely related to the choice of signposts for adaptation policies which are evaluated based on their relevance credibility and legitimacy haasnoot et al 2018 relevance refers to the predictive skill of a monitoring system in observing long term trends amid short term variability analysis of predictive skill has focused on both type i errors representing over investment as well as type ii errors representing under investment rosner et al 2014 stephens et al 2018 raso et al 2019 the goals of credibility and legitimacy reflect the fact that monitoring systems must inform human decisions within a broader context of objectives actions and spatiotemporal scales among multiple actors and institutions hermans et al 2013 2017 the interpretability of early warning signals is therefore critical and may be supported by the parsimony of signpost variables as well as the completeness of the sources of uncertainty considered raso et al 2019a in total these goals for a monitoring system may reflect a tradeoff between predictive skill and interpretability that is widely recognized in statistical modeling for example while several studies have considered adaptations triggered by linear threshold values hallegatte et al 2012 walker et al 2013 robinson and herman 2019 such signals could also be represented by more complex functions potentially incorporating multiple variables on different timescales herman and giuliani 2018 nayak et al 2018 this study considers the potential for nonlinear multivariate classifiers to address this problem recognizing that improvements in predictive skill will likely be met with a decrease in interpretability to address this challenge this study frames the design and testing of early warning signals under climate change as a machine learning classification problem using observations of human and hydrologic variables to predict the binary occurrence of future water supply vulnerability in a systems model the goal is to detect vulnerability without knowledge of future forcing which would be required in a forward simulation of the system specifically this paper addresses the following research questions to investigate the utility of the proposed method 1 can machine learning classification techniques predict long term vulnerability of a water resources system in advance and are these predictions significantly better than random 2 how is the accuracy of the prediction affected by the lead time and vulnerability threshold 3 can the interpretability of the classifiers be assessed and improved using feature importance i e can the feature set be simplified while retaining accuracy to support real world applications the proposed methodology is intended as a tool to support adaptive planning under a changing climate specifically by using these early warning signals to inform monitoring and detection of vulnerabilities 2 methods a flowchart of models and datasets used in this experiment is shown in fig 1 the experiment is demonstrated using a case study of the northern california reservoir system a large network of storage and conveyance infrastructure designed primarily to move winter precipitation from north to south to support summer irrigation the largest consumptive use is represented by the 7 9 million acres of irrigated farmland generating 100 billion annually in agricultural production followed by urban use by california s 39 7 million residents johnson and cody 2015 this system complexity yields many options for adaptation but is complicated by the fact that it is managed by hundreds of distinct agencies utilities and districts making coordination difficult hanak and lund 2012 while the effects of climate change on average precipitation in california remain uncertain projections agree on increases in the frequency of both wet and dry extremes as well as the increased likelihood of these extremes occurring sequentially swain et al 2018 placing more stress on the long term planning and operation of water supply storage 2 1 data sources precipitation and temperature data are taken from an ensemble of downscaled cmip5 projections publicly available from the u s bureau of reclamation brekke et al 2014 this daily timestep dataset contains hydrologic projections streamflow and snowpack for multiple point and gridded locations created by routing precipitation and temperature through the variable infiltration capacity vic hydrologic model liang et al 1994 ensemble projections are available for 31 global climate models gcms and four rcps as shown in supplemental table s1 fig 2 shows the locations of streamflow 11 snowpack 4 and precipitation and temperature 3 gages used 2 2 simulation model the operation of reservoirs in california orca model cohen et al accepted is used to simulate the operation of the northern california reservoirs under all 97 cmip5 climate scenarios on a daily timestep over the period 2000 2100 given the input data shown in fig 2 the model simulates the operations of shasta oroville and folsom reservoirs which are located respectively on the sacramento feather and american rivers in addition to reservoir management the model also simulates the operations of south of delta exports via pumping from the sacramento san joaquin delta to meet urban and agricultural demands via the central valley project cvp and state water project swp while also meeting environmental flow and salinity requirements for the delta for the purposes of this study the supply reliability of delta exports is the key model output that determines system vulnerability the model has been found to adequately reproduce historical operations of the system on a daily timestep with nash sutcliffe efficiency above 0 9 for reservoir storage model code and documentation can be found at https github com jscohen4 orca more details about the origin use and locations of each type of data described above can be found in supplemental table s2 2 3 feature and target data the classification problem is to predict system vulnerability at a certain lead time given a set of feature variables vulnerability is determined based on the 30 year moving average of the supply reliability of water exports from the delta a metric that reflects a focus on identifying long term trends rather than a single drought period this metric is based on meeting the target demand for a certain fraction of monthly timesteps vulnerability occurs when the 30 year average supply reliability falls below a chosen threshold resulting in a single binary target metric for classification while this approach does not distinguish between different magnitudes of vulnerability we adopt this binary classification following the standard for scenario discovery methods in the water resources field this study tests a range of possible threshold values to understand the impact on classifier performance recognizing that vulnerability definitions in practice are determined by decision makers the threshold value plays a key role in determining the balance of positive and negative classifications in the training set the feature variables used to classify future vulnerabilities each year include time series of the hydrologic variables shown in fig 2 as well as several internal states of the simulation model these variables are aggregated to annual and monthly values using either the mean maximum or sum they are then translated into moving averages and standard deviations using timescales of 10 20 30 40 and 50 year rolling windows this differentiation by timescale and statistic is conducted to improve the ability of the machine learning methods to detect important trends in the data ahmed et al 2010 given that many of the moving windows share overlapping data several of the features are expected to be correlated a summary of feature variables is shown in table 1 2 4 machine learning methods several classification methods are selected table 2 to learn from the annual and monthly data to classify water supply reliability as vulnerable below a chosen threshold or not vulnerable above a chosen threshold at lead times of 0 1 5 10 and 20 years these methods were chosen from the many classification methods available in the open source scikit learn library pedregosa et al 2011 scikit learn 2019 based on their widespread use and demonstrated effectiveness for nonlinear problems by examining the performance of each classifier against the others and a random classifier which serves as a baseline application specific insights can be drawn about the similarity of their performance and the likelihood of misclassifications the random classifier guesses proportional to the ratio of possible outcomes with an accuracy equal to the square of the ratio of possible outcomes all methods in table 2 are implemented using default parameter settings from the scikit learn library it is recognized that these parameter choices can significantly impact the performance and that a meta level analysis would be needed to determine the optimal parameter settings each of the classification methods follows the same prediction structure f x t v u l n e r a b l e p t l e a d t h r e s h o l d n o t v u l n e r a b l e p t l e a d t h r e s h o l d where f is the fitted function embedded in each of the classification methods using the features x t to evaluate the function and make predictions if the prediction p at the specified lead time is less than the threshold the instance is classified as vulnerable otherwise the instance is classified as not vulnerable the classification methods in table 2 differ primarily in the family of functions used to represent f more recently developed algorithms are denoted with citations in table 2 while the other fundamental methods can be referenced in hastie et al 2009 importantly this classification approach is not meant to emulate the reservoir system model itself it is predicting based on a combination of hydrologic and system observations whether it is likely to be on a trajectory toward a long term vulnerable state 2 5 experimental design next each method is fit to the training data lagged predictors and binary targets and re evaluated against held out validation data using a repeated leave one out approach from the ensemble of cmip5 scenarios one scenario is held out for testing while the classifier is trained on the remaining 96 scenarios the process is then repeated for all scenarios this design provides several benefits the evaluation scenario is hidden from the classifier during training the temporal structure of the data is preserved which prevents biased training or testing with either too many values from the beginning or the end of the century and it represents the realistic case in which the future hydrologic forcing is unknown e g to evaluate in a systems model but where recent observations can be used to make a statistical prediction in the next section all results will be reported in the validation stage using the ensemble of leave one out experiments to estimate confidence intervals for the prediction accuracy in summary this experiment tests seven classification methods five lead times ranging from 0 to 20 years vulnerability thresholds ranging from 0 60 to 0 86 the full range in which both positive and negative classifications are possible and three numbers of features 5 10 and 500 as described in the following paragraph in each of these cases the machine learning method classifies each prediction as either a true positive t p i t false positive f p i t false negative f n i t or true negative t n i t the primary metrics used to analyze classifier accuracy are the true positive and negative ratios which are the fraction of possible positive negative outcomes that are correctly predicted the number of possible outcomes in each class is determined by counting the occurrences in the test set the results from each of the combinations are evaluated against the baseline to test the null hypothesis that the accuracy of the machine learning classifiers is no better than random if the null hypothesis is rejected p 0 05 then the accuracy of the machine learning classifiers is significantly better than random we consider this the absolute minimum standard to evaluate the practical utility of the approach finally the original set of feature variables is too large for some of the classifiers to converge the set is reduced using feature importance scores determined based on the frequency of occurrence of each feature in a random forest of 10 000 trees a default method from the scikit learn library with more occurrences corresponding to a higher importance the 500 features with the highest importance scores are used in the training step before training all feature and target data are scaled to unit variance additional cases are considered in which the feature set is reduced to 5 and 10 features again based on the importance scores from the random forest method fig 3 shows conceptually how a classifier attempts to predict at every time step whether the long term water supply reliability of the system will be vulnerable below the threshold or not vulnerable above the threshold at a given lead time which in this example is 10 years time t l is the year the scenario becomes vulnerable and only the five most informative features are shown the gray dotted lines show information that is not available to the classification methods when they are making a prediction at time t years the long term vulnerability is always based on the 30 year trailing average water supply reliability regardless of the lead time at which the prediction is made 3 results 3 1 classifier accuracy as a function of threshold value the ability to detect early warning signals of water supply vulnerability depends in part on how frequently these events occur in the training data which is controlled by the threshold value as well as the strength of the climate change signal relative to noise in each of the scenarios fig 4 compares true positive and true negative ratios for vulnerability thresholds between 0 6 and 0 86 for a fixed lead time of five years each classification method contains the median 10th and 90th percentiles across the ensemble of leave one out experiments the performance of a random classifier is included as a benchmark black line which always guesses proportional to the ratio of possible outcomes with an accuracy of the ratio squared while all methods perform similarly the naïve bayes classifier shows a slight advantage with only one median value below 0 8 for both true positives and true negatives depending on the ratio of possible positives and negatives dotted line imbalance in the training data may lead to low rates of true classifications with the best performance near the center of the range of possible thresholds 0 4 0 6 this is reflected with the benchmark random classifier showing that at unbalanced ratios thresholds below 0 66 or greater than 0 84 some methods fail to perform better than random understanding the effects of the training set imbalance can inform whether true positive or true negative classifications are more likely to be accurate a well known challenge for machine learning methods in water resources applications the choice of the vulnerability threshold is left to the decision maker and cannot be selected arbitrarily to ensure an accurate classifier however a decision maker may choose a different classifier threshold to balance the tradeoff between false positives and false negatives to increase the effectiveness of the early warning signal in a real world institutional context this analysis underscores that if vulnerabilities are rare among the set of climate projections tested then by definition it will be difficult to train a machine learning model to predict them and that the estimates of prediction accuracy should be accompanied by confidence intervals to improve their interpretability for stakeholders in addition while the median true positive and true negative ratios suggest substantial improvement over the random classifier benchmark the 10th percentiles indicate the lower range of performance across the validation ensemble and in particular sometimes fail to outperform the random classifier the 90th percentile markers generally show an accuracy of 1 0 excepting some classifiers for thresholds below 0 70 for true positive ratios and above 0 84 for true negative ratios we return to the question of statistical significance in section 3 3 fig 4 only shows the true positive and true negative ratios for a single lead time 5 years similar figures for other lead times can be found in supplemental fig s1 3 2 classifier accuracy as a function of lead time accuracy was also evaluated across lead times for a fixed threshold of 0 76 fig 5 in general the median accuracy ratios decrease with lead time with the exception of the true positive ratio for the adaboost classifier the confidence intervals triangles suggest that all classifiers generally outperform the random classifier except for the true negative ratios at a 20 year lead time suggesting a lack of skill at the lower end of the validation ensemble the spread of the confidence intervals increases with lead time though this observation is most apparent for the true negative ratios the 90th percentiles of all distributions fall at or near an accuracy of 1 0 across all lead times finally the true positive ratio of the random classifier black increases with longer lead times due to the increase in possible positive outcomes further ahead in the century the practical implication of this is to raise the standard of performance needed for the other algorithms to outperform the random benchmark at longer lead times 3 3 statistical significance to determine whether to reject the null hypothesis that the accuracy of a machine learning classifier is equal to that of a random classifier the p values for all combinations of lead times and thresholds must be examined by rejecting the null hypothesis the alternative hypothesis the accuracy of the machine learning classifier is better than a random classifier will be accepted the null hypothesis will be rejected for a particular combination of lead time and threshold value with p 0 05 fig 6 shows a heatmap of p values for true positives and true negatives as a function of both lead time and threshold value for the random forest classifier similar heat maps showing results for the other classifiers can be found in supplemental figs s2 through s7 the p values are determined by the percentile of the leave one out distribution that falls below the accuracy of the random classifier indicating the likelihood of the classifier performing worse than random for many combinations of threshold and lead time the classifier performs significantly better than random p 0 05 fig 6 also suggests that threshold values have a larger impact on accuracy than lead times likely driven by the ratio of possible positive to possible negative classifications only a narrow range of threshold values 0 70 0 76 exists in which both the true positive and true negative classifications are significantly better than random for all lead times 3 4 feature importance to reduce the number of features used to classify vulnerability the importance of each feature must be determined the top five features for each lead time are shown below table 3 ranked in the order of importance the most important feature for all lead times is the 30 year moving average of oroville reservoir storage which appears as either the first or second ranked feature for each of the lead times the 30 year average of shasta storage is the second most common feature in table 3 however for a 20 year lead time the most important feature is the annual maximum air temperature at folsom dam which likely reflects the longer term temperature trends associated with climate change the most common months in the important features are july august and june which are the drier months in california and have the potential to carry important signals in a water system in which intra annual water storage is vital these influential features are also highly correlated with each other see supplemental fig s8 the finding that summer reservoir storage dominates the ability to detect early warning signals is not surprising but also is not obvious the target prediction is not water supply vulnerability in a given year but rather a long term trend in the 30 year average water supply reliability this result suggests that the role of reservoir storage in integrating different aspects of the hydrologic cycle also make it a good indicator for future change provided that the system operations remain the same as assumed in this study this can also be interpreted in light of the storage to inflow ratios of each reservoir which are approximately 0 80 0 93 and 0 43 for shasta oroville and folsom reservoirs respectively fig 7 shows the effect of reducing the number of features used with the machine learning methods given a threshold of 0 76 and a 5 year lead time where the number of features are prioritized according to their relative importance using the random forest method table 3 across all methods the true negative ratios are higher above 0 95 from 5 to 10 features and above 0 9 for 500 features than the true positive ratios ranging between 0 8 and 0 95 for all features in general most ratios have a slight increase in performance from 5 to 10 features performance does not show significant changes between 10 and 500 features except for the logistic regression true negative ratio which falls from about 1 0 to 0 9 the 10th percentiles range from 0 57 to 0 76 for the true positive ratios and range from 0 45 to 0 6 for the true negative ratios both outperforming the random classifier overall these results imply that the number of features can be reduced from 500 to 5 with only small reductions if any in the true positive and true negative ratios due to the high correlation among features with overlapping rolling windows in general feature importance can be linked to the signal to noise ratio of each feature variables that change more slowly such as the storage of large reservoirs or the annual temperature likely provide more reliable signals than observations with a more variable response to climate forcing the reduced complexity of this problem will improve opportunities for practical application feature reduction figures for lead times of 0 1 10 and 20 years can be found in the supplemental material fig s9 3 5 accuracy over time the previous results consider the true positive and true negative ratios for different parameters aggregated over the entire century fig 8 shows the true positive and true negative ratios as they change over the century for classifiers trained on the full time period using a threshold of 0 76 and a 5 year lead time see supplemental fig s10 for 0 1 10 and 20 year lead times the random benchmark classifier for each year is also shown in general the true positive ratios become more accurate throughout the century while the true negative ratios become less accurate importantly the classifier is not being retrained over time only applied to new data which explains the decreasing true negative rate a higher true positive ratio later in the century means that it is easier to correctly make a vulnerable positive classification later in the century when many of the features show stronger climate change signals most of the methods perform better than the random classification benchmark except for some true positive ratios before 2040 and some true negative ratios before 2020 in summary results suggest that the machine learning classifiers outperform the random classifier benchmark for most lead times under thresholds with a balanced ratio of training outcomes for a reduced set of features and for most years throughout the century the classifications generally do not show statistically significant differences in performance figs 4 5 and 7 though this finding may not generalize to other applications additionally only a few influential features mostly reservoir storage variables and their transformations are responsible for the predictions this may indicate that the reservoir storage values are able to uniquely aggregate input information given that the other features are either influencing or influenced by reservoir storage 4 discussion these experiments have analyzed the predictive skill of machine learning classifiers trained to detect future water supply vulnerabilities under climate change the analysis has therefore considered several of the goals of a monitoring system haasnoot et al 2018 by exploring how the reliability and observability of early warning signals change at different levels of timeliness and vulnerability however the remaining goals of credibility and legitimacy have not been quantified here and it is recognized that the use of machine learning classifiers rather than linear thresholds will hinder the interpretability of this approach for stakeholders the attempt to improve interpretability in this study depends on feature importance table 2 to prune the set of input variables which may support the parsimony of a monitoring system raso et al 2019b additionally the logic of the dominant features is demonstrated within the context of the system reservoir storage integrates hydrologic and demand dynamics over time and therefore provides the most reliable signal of vulnerability interpretability may be further improved by monitoring a continuous variable from the classifier such as the class probability rather than the binary prediction which might provide a more reliable signal of change similar to the p value detection method proposed by haasnoot et al 2018 in general explainability is a rapidly advancing area of machine learning doshi velez and kim 2017 xie et al 2020 that will likely yield developments to support environmental systems analysis in the coming years classifier skill strongly depends on the extent to which the training data reflects the range of possible future scenarios this is true of any machine learning problem and arises in two key aspects of this study first unbalanced training data cause difficulty in classifying positive and negative outcomes this may be amplified by relatively small sample sizes in the training set with significant implications for water resources planning under climate extremes the second challenge more specific to this problem is that of deep uncertainty in the climate scenarios it is entirely possible that the future hydrology will depart significantly from the training data due to a combination of model uncertainty emissions scenarios and natural variability this study employs a leave one out training and validation strategy to partially account for potential bias by testing whether the classifier can generalize to 1 other realizations of a similar uncertainty characterization and 2 other gcm and rcp combinations with different uncertainty characterizations however good out of sample performance is perhaps less reassuring here than in typical machine learning problems relying on large datasets with well characterized uncertainty a more complex validation approach could consider alternate ensembles generated with different climate models or expert judgment of bias in the training data as in any study of deeply uncertain futures the findings are contingent on the inherently subjective design of the training and validation experiments this study is only partially linked to a specific decision context it aims to analyze the range of timescales and vulnerability thresholds over which reliable prediction of water supply vulnerability might be possible a real world decision context would also include the adaptations to be selected when detection occurs a subject of ongoing work as well as the necessary timescales for each for example water conservation and regulation may benefit from information on annual or sub annual lead times while infrastructure may require a decade or more the findings are also specific to the range of uncertainty demonstrated in the water supply projections for this system which are quite large nearly 50 change in mean annual flows by the end of the century arising from a combination of gcm and emissions uncertainty even so we do not achieve a complete representation of all sources of uncertainty in the early warning system raso et al 2019a particularly the endogenous uncertainties arising from changes to system operations or water demand additionally we do not attempt to evaluate how stakeholders learn from monitoring information in their decision making process hermans et al 2013 or the extent to which stakeholders with different problem framings hermans et al 2017 quinn et al 2018 may find the early warning signals convincing within their system of organizational decision making haasnoot et al 2018 much interesting work remains at this intersection of statistical modeling and policymaking for wicked problems that by definition do not lend themselves to straightforward prediction rittel and webber 1973 kwakkel et al 2016 5 conclusions this paper contributes a methodology for detecting early warning signals of water supply vulnerabilities under climate change using machine learning demonstrated on a case study of the northern california reservoir system among the many goals of a monitoring system relevance credibility and legitimacy proposed by haasnoot et al 2018 this study has primarily focused on relevance represented by the predictive skill of detecting future change results indicate that the classification methods generally outperform a benchmark random classifier though the factor most strongly influencing this result is the balance of the training data determined by the vulnerability threshold in addition the overall classification accuracy decreases with larger lead times to improve the interpretability and parsimony of the resulting classifiers raso et al 2019b the feature set can be reduced with minimal impact on accuracy due to high correlation between features at short lead times the features most strongly influencing the predictions are long term averages of summer reservoir storage which demonstrates predictive power in the ability of storage to integrate different aspects of the hydrologic cycle with further work to analyze the credibility and legitimacy of this approach in a real world decision context with significant human and institutional uncertainties hermans et al 2013 2017 this approach could be implemented as a tool to support water resources planning under climate uncertainty an additional limitation is the assumption that the system infrastructure and operations remain unchanged throughout the century the trained classifiers are expected to become less accurate over time as a result of endogenous adaptation a topic of ongoing work however even when ensemble climate projections suggest substantial uncertainty in future hydrology this approach can help to identify what signals should be monitored to inform adaptation while this study has developed vulnerability classification methods in line with previous work on scenario discovery future work will consider regression methods to identify the magnitude of failure as well additional research will focus on integrating these dynamic vulnerability classifications with an adaptive infrastructure planning problem where early warning signals can be used directly to trigger decisions this analysis will provide insights into the benefits of predicting vulnerabilities along with the consequences of inaccurate classifications including the costs of unnecessary adaptations and the regrets of foregoing beneficial ones declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements and software availability this work was partially supported by the u s national science foundation grants cnh 1716130 and cbet 1803589 any opinions findings and conclusions are those of the authors and do not necessarily reflect the views or policies of the nsf all code and data used in this study is available on github https github com brobinson3 early warning signals ml orca we further acknowledge the world climate research program s working group on coupled modeling and the climate modeling groups listed in the supplement of this paper for producing and making available their model output appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104781 
26003,interbasin water transfer regimes a h essenfelder a c giupponi b a euro mediterranean center on climate change and ca foscari university of venice edificio porta dell innovazione 2nd floor via delle libertà 12 30175 venice ve italy euro mediterranean center on climate change and ca foscari university of venice edificio porta dell innovazione 2nd floor via delle libertà venice ve 12 30175 italy euromediterranean center on climate change and ca foscari university of venice venice italy edificio porta dell innovazione 2nd floor via delle libert 12 30175 venice ve italy b ca foscari university of venice department of economics italy ca foscari university of venice department of economics italy ca foscari university of venice department of economics italy corresponding author interbasin water transfer iwt is often a complex decision making process that depends on factors ranging from hydro meteorological conditions to socio economic pressures hydrologic modelling is particularly challenging under these circumstances requiring accurate quantitative information which may not always be available this study proposes a methodological framework to simulate iwt flow contributions in the absence of observational data by introducing a coupled machine learning hydrologic modelling approach the proposed methodology employs a hydrologic model to simulate the rainfall runoff process of a watershed while a machine learning algorithm is used to simulate the decision making process of iwts methods are illustrated by simulating the hydrologic balance of the dese zero river basin dzrb a highly artificially modified catchment located in north east italy results suggest the proposed methodological framework can successfully simulate the complex water flow dynamics of the studied watershed and be a useful instrument to support complex scenario analysis under iwts data scarce conditions keywords complex water dynamics interbasin water transfer swat model machine learning artificial neural networks 1 introduction a watershed can be understood as the land area which drains water to a specific point in space by a stream network system it can also be seen as a control system where the water stored in the river basin is the result of the interaction between inputs e g precipitation incoming groundwater etc and outputs e g surface runoff evapotranspiration outgoing groundwater etc as a consequence a watershed is spatially defined according to its natural hydrology representing the most logical basis for the management of water resources the overall water movement in a watershed can however be rather complex for instance boundaries may not necessarily be hard borders and water may move between watersheds by means of processes such as interbasin groundwater flow igf and interbasin water transfers iwt hydrological modelling under these circumstances is particularly challenging requiring not only detailed quantified information about the external hydraulic loadings entering or leaving the watershed nyeko 2014 but also a holistic perspective of the issues involved giupponi et al 2012 iwts are a subject growing relevance both in science and for economic reasons due to expanding infrastructure base that allows for iwts trading gomez et al 2015 marston and cai 2016 and water reallocation potential pérez blanco et al 2020 rey et al 2019 even though an expanding literature on iwts and water reallocation exists the rationale behind the decision making process that results in the complex management of iwts hydraulic devices is a topic that requires further research pande and sivapalan 2017 particularly in the case of lack of information indeed as the connections in human water systems become increasingly stronger the endogenisation of human agency becomes fundamental for describing the complex water movement of highly modified watersheds sivapalan et al 2014 an example of a watershed that is subject to complex water flow dynamics resulting from complex artificial hydraulic management mechanisms e g iwt is the dese zero river basin dzrb in italy to the best of our knowledge no study has previously attempted to simulate the decision making process of managing iwts by using hydrologic modelling and machine learning techniques and under conditions of data scarcity this is the case of the dzrb where no time series data is available that specifies the quantities and timing of how iwts are managed however factors that drive this processes are known such as the accumulated precipitation during the past days streamflow conditions downstream to iwts and baseflow contributions bixio et al 2009c essenfelder et al 2016 essenfelder 2017 this paper proposes an innovative methodological framework to simulate the decision making process behind the management of iwts in the dzrb under a situation of data scarcity to do so this paper identifies the main actors driving this decision making process and makes use of a coupled machine learning hydrologic modelling approach to quantify the estimated iwt flows under the proposed methodological framework the hydrologic model takes the role of simulating the rainfall runoff process while the machine learning model accounts for the estimation of the iwt loadings entering the dzrb the proposed methodological framework is designed to be flexible enough so to be used for different case study areas and under different time periods the hydrologic model selected for this study is the soil water assessment tool swat model arnold et al 1998 an eco hydrologic model that has already been used to simulate the hydrologic balance in the dzrb in previous studies salvetti et al 2007 2008 azzellino et al 2013 essenfelder et al 2016 pesce et al 2017 hence providing a reliable starting point and solid foundation for comparison basis the machine learning technique selected for this study is a non linear artificial neural networks ann model given its robustness in simulating hydrologic and decision making processes matsuda 2005 demuth 2006 çevirgen et al 2015 noori and kalin 2016 essenfelder et al 2018 the capabilities of the coupled swat ann model are illustrated with an application to the dese zero river basin dzrb in north eastern italy 2 methodology 2 1 the study area 2 1 1 the dese zero river basin the dzrb is a highly artificially modified catchment that is part of the venice lagoon watershed vlw in its most northern north western region the dzrb receives significant amounts of spring waters coming mainly from outside the watershed s land area this igf contribution originates in an open aquifer located in the venetian high plains playing an important role in the overall water budget and nutrient balance of the dzrb regioneveneto 2000 salvetti et al 2007 2008 azzellino et al 2013 being highly variable throughout a year boscolo and mion 2008 the influence of the igf on the dzrb is one of the reasons why this watershed hosts such remarkably modified environment the dzrb and its main features are shown in fig 1 throughout the past centuries several hydraulic works e g water pumping systems and iwts have been implemented in the vlw adbve and adbadige 2010 these interventions have been implemented mainly for land reclamations and or to regulate the amount of water flowing into the lagoon of venice bixio et al 2009b currently the general hydraulic management of the dzrb and of the vlw in general can be divided in two distinct phases under low flow conditions and under high flow conditions the low flow phase is understood as the ordinary streamflow condition of the vlw while high flow conditions are the situations when streamflow level are abnormally high bixio et al 2009a this complex artificial management scheme of the dzrb depends mainly on two factors i the amount of water flowing downstream of the hydraulic nodes and ii the amount of water entering the watershed from external sources boscolo and mion 2008 the contrast between the two hydraulic management phases in the vlw is significant to the point that during the high flow phase the totality of some sub basins streamflow is diverted to neighbour river basins which do not discharge in the lagoon of venice bixio et al 2009b as a consequence the dzrb is a dynamic watershed in extension ranging in area from a minimum of 250 6 km2 to a maximum of 394 7 km2 depending on specific hydraulic management conditions bixio et al 2009b fig 1 two main iwts are under operation in the upper basin of the dzrb namely castelfranco veneto hydraulic node and albaredo di vedelago hydraulic node hydrologically the castelfranco veneto hydraulic node is the most important one adbve and adbadige 2010 this hydraulic node is responsible for managing a iwt from the avenale sub basin to either the vlw or to the muson dei sassi river the latter not discharging into the venice lagoon when water is derived from the avenale sub basin to the vlw it may either flow to the dese river or to the marzenego river the latter not part of the dzrb but part of the vlw the complex avenale s iwt is operated at the hydraulic junction of castelfranco veneto as shown in fig 1 similarly the zero river receives hydraulic contributions from the brenton del maglio sub basin by means of a hydraulic junction located near the city of albaredo di vedelago bixio et al 2009b water that is deviated from the brenton del maglio sub basin may either flow to the zero or to the sile rivers the latter not discharging into the lagoon of venice the artificially controlled water dynamics observed in the dzrb is fundamental for maintaining an optimal streamflow regioneveneto 2000 this is especially true during the spring and summer seasons when the amount of water withdrawals increases mainly due to increased evapotranspiration as a result of the increased irrigation water demand bixio et al 2009b as a consequence an irrigation schedule scheme is currently in place in the dzrb bixio et al 2009c this irrigation schedule is set up considering water availability and irrigation water demand which in turn depends on soil characteristics and water availability as a consequence water may be imported from external sources when water demand for irrigation is greater than the amount of water available piave 2011 similarly when water supply is greater than the demand this same intricate hydraulic control system enables the administration of flood related risks as the controlled hydraulic devices can be operated as a flood control system bixio et al 2009b 2 2 the swat model swat arnold et al 1998 is a conceptual eco hydrologic river basin scale model which allows a number of different physical processes to be simulated in a watershed while also enabling the evaluation of the impacts of different land management practices on the surface runoff water quality sediment transport and agricultural chemical yields processes swat offers the capability of assessing different land and water management processes such as irrigation scheduling and water transfer between sub basins while at the same time providing the means to support the assessment of their impacts on a river basin scale neitsch et al 2011 for hydrologic modelling purposes swat considers a river basin as a mosaic of smaller spatially defined units known as sub basins which are in turn further subdivided into smaller land units called hydrological response units hrus neitsch et al 2011 winchell et al 2013 hrus are defined as lumped land areas within a sub basin that are comprised of unique land cover soil slope and management combinations arnold et al 2012b the consideration of spatially distributed elements i e sub basins and its subdivision into hrus enables the model to reflect consequences on eco hydrologic processes for a variety of land use and soil classification both temporally and spatially with regards to the simulation of external complex water flow contributions swat allows water to be transferred and applied on an hru from any water source within e g between reservoirs reaches and sub basins or outside e g iwt a watershed neitsch et al 2011 swat accounts for these processes by reading input information pertaining to the type of water source the location of the source the type of water body receiving the transfer the location of the receiving water body and the amount of water transferred for every day of the simulation neitsch et al 2011 as a consequence detailed quantified information is required to accurately reproduce water transfers in a watershed something that can become a serious issue in cases of lack or insufficient availability of data the most basic information required by swat in order to simulate the hydrologic balance of a watershed are i digital elevation model dem ii soil spatial distribution and characteristics iii land use spatial distribution and respective land management operations and iv hydro meteorological data arnold et al 2012a in this study a dem of resolution 5 5 m is utilised regioneveneto 2014 the soil and land use vector maps regioneveneto 2014 were converted to raster format at a 5 5 m resolution grid by using the resample majority method so to maintain spatial integrity with the dem the original soil map classes were combined into 8 representative classes according to their soil textures soil depth soil layer profiles and total coverage area missing soil parameters required by swat e g saturated hydraulic conductivity were derived from pedotransfer functions sun et al 2016 the original land use classes were combined and converted to swat compatible land use classes according to their representativeness in terms of total coverage area resulting in a final number of 13 land use classes as a result the dzrb was sub divided into 14 sub basins for a final number of 476 hrus groundwater flow contributions coming from the shared aquifer system depicted in fig 1 were considered as additional inlet elements azzellino et al 2013 essenfelder et al 2016 in order to define an upper boundary for the external hydraulic loadings iwt from the dynamic areas of the dzrb the avenale and brenton del maglio sub basins were set up as single swat projects hence the avenale sub basin was sub divided into 79 hrus while the brenton del maglio sub basin has been partitioned into 16 hrus meteorological data available for the case study area has been processed using the swat weather database tool essenfelder 2016 and temporally ranges from jan 1993 to dec 2014 on a daily basis for the meteorological stations shown in fig 1 streamflow data was available as measured by two distinct stream gauges with data ranging from 1997 to 2005 for calibration purposes observed streamflow data was arranged into the calibration and validation datasets aiming at a validation calibration ratio of around 1 4 table 1 shows more details about the data used for the calibration and validation of the swat model 2 3 the ann model artificial neural networks anns are mathematical constructs that can be configured as linear or non linear models being defined as massively parallel distributed processors constituted of single process units having the ability to store experiential knowledge and make it available for later use haykin 2001 anns have been applied with relatively success in several fields of research such as hydrologic modelling image classification remote sensing human behaviour analysis and decision making simulation wu et al 2005 giacinto and roli 2001 hsieh 2009 strnad et al 2015 neural networks were born taking inspiration from the attempt to translate the knowledge on how the human brain works into a quantitative model being capable of acquiring and storing knowledge from the surrounding environment ultimately being capable of learning through training from this interaction wilamowski and irwin 2011 together with the notion of learning comes the idea of adaptability as anns have the capability of adapting their synaptic weights according to their perception of the surrounding environment these capabilities of ann models make this kind of machine learning technique an interesting option to simulate intellectual activity processes matsuda 2005 such as the decision making processes behind the management of iwts the ann model used in this study has been developed by the authors essenfelder 2017 and is structured as a multilayer perceptron mlp neural network using back propagation as the supervised training algorithm and levenberg marquardt as the optimisation algorithm hagan and menhaj 1994 yu and wilamowski 2011 the model is capable of running in a multi core configuration either as a simple input output model as a time delayed input output model or as either non linear autoregressive nar or non linear autoregressive with exogenous inputs narx variants for this study a narx ann model variant is used consisting of two hidden layers and using a swish activation function ramachandran et al 2017 between hidden layer nodes the activation function at the output layer is a rectified linear activation unit relu function hence allowing simulated values to be greater than the maximum observed valued in the target dataset regarding the training process the ann model evaluates the evolution of the mean squared error mse of the simulated results with regards to the targets i e observations as a metric system to assess whether generalisation has been achieved and to avoid the overfitting of the resulting trained model maier and dandy 2000 hsieh and tang 1998 the target variable considered in this study is the daily amount of water that is transferred in iwts systems as described in section 2 2 the basins that contribute to the two simulated iwts are modelled separately i e avenale and brenton del maglio sub basins hence the target variable describes the amount of water that is transferred from each of the individual sub basins the dzrb by means of iwts similarly the difference between the total amount of streamflow as simulated by the individual swat projects for each subbasin and the amount of water that is transferred to the dzrb represents the water flow that is not discharging in the vlw the latter being an important aspect for managing flood risk in the dzrb which is however out of the scope of analysis of this paper the amount of water transferred in iwts systems is usually variable in time depending among other factor on water availability at the source and water demand at the destination in this context the way the iwts of the dzrb are managed corresponds to a decision making process in which decision makers determine the amount of water to be transferred as a function of hydro meterological hydraulic and water requirement e g irrigation conditions since there is no available data series of observed flow transferred in the iwts systems the daily amount of water that is transferred in iwts systems is estimated by means of hydrologic modelling as described in section 2 4 during the hydrological simulations of the coupled swat ann model the neural networks module receives on line information from and is called iteratively by the swat model for every day of simulation the iwt results obtained from the ann model are then transmitted to the swat model and incorporated into the hydrological simulations being finally passed to the subsequent routing phase calculation steps the maximum iwt estimated values should not exceed neither the daily simulated streamflow that is reaching an hydraulic node i e upstream basin streamflow in table 2 neither the technical limitations of the hydraulic node i e maximum iwts flow in table 2 the maximum technical flow of the iwts in the case study area were obtained from consiglioveneto 2009 and bixio et al 2009b in order to account for past hydro meteorological information the variables precipitation upstream streamflow and downstream streamflow are considered in a temporal window of 7 days since two iwts are being considered in this study an ann model is set up individually for each hydraulic node table 2 shows the list of input variables to the ann models the spatial scale in which they are considered their units and their symbols in the case study area expert local knowledge on irrigation land reclamation and the hydraulic management of the watershed must be integrated with the understanding of hydro meteorological parameters for the proper management of the vlw s sub basins bixio et al 2009c hence the information shown in table 2 is selected having in mind the complex decision making process where the selected input variables are presented to the ann model in order to transmit the knowledge of when water availability and water demand might need artificial interventions 2 4 the methodological framework a schematic representation of the proposed methodological framework to couple the swat and ann models is depicted in fig 2 the methodological framework presented here relies on two assumptions i the hydrologic model utilised in the first step is capable of reproducing physically consistent results regarding the rainfall runoff process for the case study area and ii the difference between observed and simulated streamflow values in any point downstream of iwts contribution points is a good proxy for the estimation of the quantification of the absolute amount of water transferred by iwts in the case study area the second assumption is necessary in the dzrb case study as no quantitative information is available regarding the iwt process in river basins where the amount of water transferred by iwts is known this assumption can be relaxed and this step can be by passed the first step of the proposed methodological framework i e swat model reference calibration in fig 2 consists in reference calibrating the swat model based on specific information already published in the literature salvetti et al 2007 2008 azzellino et al 2013 essenfelder et al 2016 information from other sources is used as well in order to get specific information pertaining the hydrologic balance of the case study area such as technical details about the groundwater dynamics and irrigation schedule schemes bixio et al 2009a b c boscolo and mion 2008 regioneveneto 2000 adbve and adbadige 2010 all this information is manually transferred to the parametrisation of the swat model the second step i e validation pbias in fig 2 the first inside the swat ann box consists in evaluating the accuracy in which the swat model is capable of reproducing the hydrologic balance of the case study watershed the pbias model efficiency metric is selected as it is capable of expressing the tendency of simulated data to over or underestimate the reference data moriasi et al 2007 in case the results are considered non satisfactory something that is expected for the dzrb due to the operation of iwts and assuming that the first methodological step is consistent it is possible to estimate the difference between the observed and simulated values this information then is considered as a proxy for the quantification of the absolute amount of water transferred by iwts in the case study area the third methodological step i e ann model training validation inside swat ann box in fig 2 consists in two different phases first the ann model is trained under varying structures i e number of neurons per layer so to obtain an optimal ann model configuration second the model is validated by means of k fold cross validation for training and validation of the ann model the inputs target dataset is randomly split into calibration 70 validation 15 and test 15 datasets the test dataset is used during the training phase of the ann model in order to avoid the over fitting of the model not being used however for the calibration of the ann model itself the validation dataset instead is used only after the completion of the ann training to evaluate its performance thereby not being presented to the model during the training phase the training process of the ann model is repeated 1000 times where each training attempt is initialised by restarting the initial weight connection values adjusting the neural network structure and k folding the data samples the 10 best ann models i e the ann models with the 10 lowest mse are stored as an ensemble of valid models considered to be the best candidates for fitted models demuth 2006 the simulated iwts values are estimated as the median of the 10 models step three is done separately for each iwt i e one for the castelfranco veneto and another for the albaredo di vedelago hydraulic nodes the fourth step i e swat ann model swat cup calibration inside the swat ann box in fig 2 consists in coupling the ann model obtained at step three with the swat model obtained at step one the two models are coupled and run simultaneously during the swat ann calibration cycle exchanging information at a daily level this is done through a modification in the source code of the swat model to call the ann model during every simulated day the coupled swat ann model utilises the swat cup software and the sufi2 procedure abbaspour 2015 for calibration purposes a first initial calibration cycle is performed by running 900 calibration iterations followed by two sub sequent fine tuning calibration cycles of 500 iterations each at the end of every calibration cycle results with lowest pbias are assumed to be the most fitted model for the calibration cycle while the model configuration with the lowest pbias value at the end of the third calibration cycle is assumed to be the overall most fitted swat ann model in case a pbias value lower than 25 and greater than 25 is obtained the model is assumed as valid and the calibration process of the model proceeds successfully to the next methodological step in case pbias results indicate non satisfactory results the resulting swat ann model is considered as a non valid model and the overall calibration process is terminated i e an impossible estimation of iwt flows is assumed as shown in fig 2 in case of a positive pbias validation check the fifth methodological step is reached and a new calibration of the coupled swat ann model is performed i e swat ann model swat cup calibration outside the swat ann box in fig 2 by using the swat cup software and in a similar fashion to the calibration procedure described in the fourth methodological step this time however two efficiency criteria are simultaneously evaluated namely the nash sutcliffe model efficiency coefficient nse and again the pbias the nse is employed in this step as it allows for the identification of not only how well the simulated values of the swat ann model is being reproduced in time but also how well these results fit with the observations as the calculation of the nse criterion involves the computation of the squared difference between the observed and predicted values krause and boyle 2005 the metrics to evaluate the performance of the coupled swat ann are obtained from the literature moriasi et al 2007 in case the resulting swat ann model is capable of producing satisfactory results with respect to observations the model is assumed to be validated 3 results and discussion the results and discussion section are presented following the five methodological steps presented in section 2 4 and summarised in fig 2 3 1 steps 1 and 2 swat model reference calibration following the methodological framework described in section 2 4 and summarised in fig 2 the manually calibrated swat model is followed by a calibration step using the swat cup and the sufi2 procedure to further refinement regarding the water balance calibration this procedure consists in modifying a total of 16 distinct swat parameters focusing on three main hydrological related processes i e surface runoff baseflow and soil hydraulic properties the list of modified parameters and their final calibrated values for this step is shown in table 3 in accordance with the proposed methodology the performance of the pre calibrated swat model is evaluated by means of the r2 coefficient of determination the results regarding the performance evaluation of the pre calibrated swat model are shown in table 4 the results shown in table 4 indicate an acceptable behaviour if r2 is considered as the sole model efficiency criterion both under the daily and monthly model configurations for streamflow with the monthly basis configuration showing slightly better results however the results for the nse and pbias are both non acceptable these results indicate that the hydrologic simulations are mispredicting the observed streamflow this behaviour is confirmed by a considerable large underestimation bias for streamflow in the order of 51 for the dzrb the results shown in table 4 are somehow expected due to the fact that the dzrb is characterised by receiving significant amounts of external water coming from the iwts especially during the crop growing season in general under the assumed configuration and due to the complexity of the studied watershed the swat model alone is not capable of reproducing the overall water balance of the dzrb however it is interesting to notice that even if the iwts components of the system are missing the proposed pre calibrated swat model is capable of producing simulations somewhat linearly correlated to the observed data as evidenced by the coefficient of determination results in fact on a daily basis the linear correlation between the observed streamflow and the simulated streamflow is as high as 0 736 for the dzrb in summary the results for the pre calibrated swat model suggest that although the proposed model configuration is not capable of accurately simulating the hydrology of the studied watersheds it is capable of linearly reproducing its behaviour in time even if it systematically underestimate the streamflow observed values 3 2 step 3 ann model training validation recognising the fact that iwts are missing to fully describe the hydrologic system of the case study area see fig 2 the next proposed methodological step consists in estimating the hydrologic influences resulting from the complex management of the iwts hydraulic nodes as described in the section 2 4 an ann model is used for such a purpose moreover for modelling purposes the dzrb is split into dese and zero subbasins allowing for the training of specific ann models for each system the modelling results of this operation are summarised in table 5 while fig 3 depicts the information regarding the training progress and model performance the results shown in table 5 and fig 3 indicate that the ann model is capable of satisfactory reproducing the estimated total iwts water flows to the studied watershed in fact both the nse and pbias results shown in table 5 suggest a very good skill of the ann model to reproduce the desired hydraulic influences particularly when aggregated at the monthly scale spatially the skill of the ann model is slightly lower when attempting to simulate the estimated total hydraulic influences from iwts to the dese subbasin even so the results are considered to be satisfactory as shown by a value of approximately 0 73 for nse and of approximately 1 0 for pbias both for the test dataset the positive pbias results indicate however a slightly tendency of the ann model to underestimate the target values the strong modelling skill of the ann model is graphically presented by the scatter plot of the target vs simulated values as shown at the second row of fig 3 the third row of fig 3 depicts some useful statistics regarding the simulation results of the ensemble of ann models such as the range of the simulated values and their respective first and third quartiles for each month together with the median value of the target values for comparison purposes in general these results indicate that the ann model has a very good skill in reproducing the monthly median iwts flow contributions interestingly it is possible to draw an interesting parallel with the water demand for irrigation during the crop growing season in the dzrb as can be observed during the increased water inflow during the months between march and september in summary the results shown here indicate that the employed ann model is capable of computing both at the daily and monthly scales the estimated hydraulic influences not taken into account when running the swat model alone 3 3 steps 4 and 5 swat ann model swat cup calibration validation following the calibration and validation of the neural networks model the next proposed methodological step consists on coupling the swat and the ann models on re running the calibration process for the new model and on the re evaluation of the new simulation results the outcomes of this operation are summarised in tables 6 and 7 fig 4 instead depicts a comparison between the results of the swat and swat ann models when performing the simulation of streamflow extreme values on a daily basis confronting the results shown in tables 3 and 7 it is possible to verify that the mean calibrated values for the majority of the calibrated parameters have not change significantly from the pre calibrated swat model configuration indicating a satisfactory swat pre calibration procedure in fact considering only the parameters pertinent to the hydraulic balance of the studied watersheds only one parameter show variations above an absolute threshold value of 50 with respect to the results of the pre calibrated swat model when considering all three watersheds namely gw delay the increase in the absolute value of the parameter gw delay indicates that the period of time that water takes to leave the soil profile and recharge the shallow aquifer is greater in the coupled swat ann model signalling that the pre calibrated swat model was possibly over predicting the magnitude of the return flow in order to cope with the absence of the already mentioned iwts hydraulic influences the variation in the gwqmn parameter corroborates this idea the coupled swat ann model calibration points to the direction that a deeper threshold of water in the shallow aquifer necessary for the return flow to occur with regards to the results of the pre calibrated swat model another parameter that shows interesting results it the surlag the actual increase in the surlag parameter indicates that the pre calibrated swat model was over predicting the amount of surface runoff discharging into the streams for every day of simulation in any case the gw delay gwqmn and surlag values obtained after the swat model pre calibration procedure indicate a configuration which tried to emulate the observed hydrological behaviour of a system without however considering any of the iwts hydraulic influences to the studied watershed consequently the automatic calibration of the swat model adjusted the intrinsic watershed parametrisation in order to account for such missing information ultimately this has lead to an overestimation of both groundwater and surface runoff contributions to the main channel flow when the relevant external water sources to the studied watersheds are not considered regarding the modelling capabilities of the coupled swat ann model it can be verified a significant improvement of the model s performance as shown in table 7 and fig 4 particularly at the monthly scale an interesting result attributed to the difference in the composition of the calibration and validation datasets can be verified when analysing the results shown in table 7 while the coupled swat ann model exhibits a tendency to overestimation the streamflow for the calibration dataset it shows the opposite behaviour when applied to the validation dataset this discrepancy can be attributed to particularities of both calibration and validation datasets such as the presence of wetter years in the calibration dataset anyhow the results of the coupled swat ann model for case study area can be considered to be satisfactory and significantly improved from the results of pre calibrated swat model the results shown in table 7 and fig 4 confirms the importance of iwts flows for the water balance of the dzrb by applying the methodology described in section 2 it is possible to quantify these influences even under a situation of data scarcity for the case study area the mean percentage of the total streamflow of the dzrb that can be attributed to sources external to the watershed varies from 19 during wet weather conditions to 59 during dry weather conditions during the simulation run from 1996 to 2014 4 conclusions the dzrb is characterised for being a very complex catchment several modifications in its superficial watercourses throughout the centuries have resulted in a very unique environment requiring specialised hydraulic management practices from a number of external hydraulic contributions capable of affecting the water dynamics of the vlw two stand out namely i groundwater entering the vlw from the big unconfined aquifer to the north north west of the vlw and ii artificially controlled superficial waters deviated from to bordering watersheds that do not discharge into the lagoon of venice this study proposed a framework to estimate the total external hydraulic contributions to the dzrb a sub basin of the vlw the proposed methodological framework is built upon the use of a coupled mechanistic empirical modelling technique based on the hypothesis that the mechanistic model i e swat is capable of simulating the hydrological processes and water movement occurring inside boundaries of the studied watersheds while the empirical model is capable of simulating the total external hydraulic contributions the mechanistic model used in this research is the swat model while the empirical counterpart is an artificial neural networks ann model the results obtained from the implementation of the proposed methodological framework suggest that the coupled swat ann model is not only capable of satisfactory reproducing the water balance of the studied watershed but also to increase the hydrological modelling capability of the swat model when performing under an intricate and complex environment such as the dzrb the results obtained from the application of the proposed methodology also confirm the findings of previous studies in the same area indicating that under ordinary flow conditions and in dry periods the water balance of the dzrb is in general highly affected by water flowing into the watershed from bordering watersheds particularly during the spring and summer seasons finally as some recommendations for further developments in this field of research it is proposed the consideration of other hydro meteorological variables such as snow coverage area snow depth and potential snow melt flux in the pre alpine region affecting the recharge processes of the aquifer system and the water table depth of the aquifer system in the vicinities of the vlw if at a compatible time scale moreover the use of more robust modelling tools such as the swat modflow model could also lead to better simulation results particularly in what pertains to the groundwater flow dynamics declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors are thankful and acknowledge the support regarding the possibility of performing some calculations on the scscf computer cluster a multiprocessor system owned by ca foscari university of venice and running under a gnu linux operating system all the data and source codes utilised during the development of this research can be found here https doi org 10 5281 zenodo 3699658 the authors would also like to thank all the people who has directly or indirectly contributed to the development of this research in particular arpav area tecnico scientifica regione veneto ambiente e territorio and the consorzio di bonifica acque risorgiva finally the authors are thankful for the invaluable support given by dr arianna azzellino and dr silvio giove particularly in providing specific data and technical information without which would make this research not possible the authors gratefully acknowledge the support by the venice centre in economic and risk analytics for public policies vera 
26003,interbasin water transfer regimes a h essenfelder a c giupponi b a euro mediterranean center on climate change and ca foscari university of venice edificio porta dell innovazione 2nd floor via delle libertà 12 30175 venice ve italy euro mediterranean center on climate change and ca foscari university of venice edificio porta dell innovazione 2nd floor via delle libertà venice ve 12 30175 italy euromediterranean center on climate change and ca foscari university of venice venice italy edificio porta dell innovazione 2nd floor via delle libert 12 30175 venice ve italy b ca foscari university of venice department of economics italy ca foscari university of venice department of economics italy ca foscari university of venice department of economics italy corresponding author interbasin water transfer iwt is often a complex decision making process that depends on factors ranging from hydro meteorological conditions to socio economic pressures hydrologic modelling is particularly challenging under these circumstances requiring accurate quantitative information which may not always be available this study proposes a methodological framework to simulate iwt flow contributions in the absence of observational data by introducing a coupled machine learning hydrologic modelling approach the proposed methodology employs a hydrologic model to simulate the rainfall runoff process of a watershed while a machine learning algorithm is used to simulate the decision making process of iwts methods are illustrated by simulating the hydrologic balance of the dese zero river basin dzrb a highly artificially modified catchment located in north east italy results suggest the proposed methodological framework can successfully simulate the complex water flow dynamics of the studied watershed and be a useful instrument to support complex scenario analysis under iwts data scarce conditions keywords complex water dynamics interbasin water transfer swat model machine learning artificial neural networks 1 introduction a watershed can be understood as the land area which drains water to a specific point in space by a stream network system it can also be seen as a control system where the water stored in the river basin is the result of the interaction between inputs e g precipitation incoming groundwater etc and outputs e g surface runoff evapotranspiration outgoing groundwater etc as a consequence a watershed is spatially defined according to its natural hydrology representing the most logical basis for the management of water resources the overall water movement in a watershed can however be rather complex for instance boundaries may not necessarily be hard borders and water may move between watersheds by means of processes such as interbasin groundwater flow igf and interbasin water transfers iwt hydrological modelling under these circumstances is particularly challenging requiring not only detailed quantified information about the external hydraulic loadings entering or leaving the watershed nyeko 2014 but also a holistic perspective of the issues involved giupponi et al 2012 iwts are a subject growing relevance both in science and for economic reasons due to expanding infrastructure base that allows for iwts trading gomez et al 2015 marston and cai 2016 and water reallocation potential pérez blanco et al 2020 rey et al 2019 even though an expanding literature on iwts and water reallocation exists the rationale behind the decision making process that results in the complex management of iwts hydraulic devices is a topic that requires further research pande and sivapalan 2017 particularly in the case of lack of information indeed as the connections in human water systems become increasingly stronger the endogenisation of human agency becomes fundamental for describing the complex water movement of highly modified watersheds sivapalan et al 2014 an example of a watershed that is subject to complex water flow dynamics resulting from complex artificial hydraulic management mechanisms e g iwt is the dese zero river basin dzrb in italy to the best of our knowledge no study has previously attempted to simulate the decision making process of managing iwts by using hydrologic modelling and machine learning techniques and under conditions of data scarcity this is the case of the dzrb where no time series data is available that specifies the quantities and timing of how iwts are managed however factors that drive this processes are known such as the accumulated precipitation during the past days streamflow conditions downstream to iwts and baseflow contributions bixio et al 2009c essenfelder et al 2016 essenfelder 2017 this paper proposes an innovative methodological framework to simulate the decision making process behind the management of iwts in the dzrb under a situation of data scarcity to do so this paper identifies the main actors driving this decision making process and makes use of a coupled machine learning hydrologic modelling approach to quantify the estimated iwt flows under the proposed methodological framework the hydrologic model takes the role of simulating the rainfall runoff process while the machine learning model accounts for the estimation of the iwt loadings entering the dzrb the proposed methodological framework is designed to be flexible enough so to be used for different case study areas and under different time periods the hydrologic model selected for this study is the soil water assessment tool swat model arnold et al 1998 an eco hydrologic model that has already been used to simulate the hydrologic balance in the dzrb in previous studies salvetti et al 2007 2008 azzellino et al 2013 essenfelder et al 2016 pesce et al 2017 hence providing a reliable starting point and solid foundation for comparison basis the machine learning technique selected for this study is a non linear artificial neural networks ann model given its robustness in simulating hydrologic and decision making processes matsuda 2005 demuth 2006 çevirgen et al 2015 noori and kalin 2016 essenfelder et al 2018 the capabilities of the coupled swat ann model are illustrated with an application to the dese zero river basin dzrb in north eastern italy 2 methodology 2 1 the study area 2 1 1 the dese zero river basin the dzrb is a highly artificially modified catchment that is part of the venice lagoon watershed vlw in its most northern north western region the dzrb receives significant amounts of spring waters coming mainly from outside the watershed s land area this igf contribution originates in an open aquifer located in the venetian high plains playing an important role in the overall water budget and nutrient balance of the dzrb regioneveneto 2000 salvetti et al 2007 2008 azzellino et al 2013 being highly variable throughout a year boscolo and mion 2008 the influence of the igf on the dzrb is one of the reasons why this watershed hosts such remarkably modified environment the dzrb and its main features are shown in fig 1 throughout the past centuries several hydraulic works e g water pumping systems and iwts have been implemented in the vlw adbve and adbadige 2010 these interventions have been implemented mainly for land reclamations and or to regulate the amount of water flowing into the lagoon of venice bixio et al 2009b currently the general hydraulic management of the dzrb and of the vlw in general can be divided in two distinct phases under low flow conditions and under high flow conditions the low flow phase is understood as the ordinary streamflow condition of the vlw while high flow conditions are the situations when streamflow level are abnormally high bixio et al 2009a this complex artificial management scheme of the dzrb depends mainly on two factors i the amount of water flowing downstream of the hydraulic nodes and ii the amount of water entering the watershed from external sources boscolo and mion 2008 the contrast between the two hydraulic management phases in the vlw is significant to the point that during the high flow phase the totality of some sub basins streamflow is diverted to neighbour river basins which do not discharge in the lagoon of venice bixio et al 2009b as a consequence the dzrb is a dynamic watershed in extension ranging in area from a minimum of 250 6 km2 to a maximum of 394 7 km2 depending on specific hydraulic management conditions bixio et al 2009b fig 1 two main iwts are under operation in the upper basin of the dzrb namely castelfranco veneto hydraulic node and albaredo di vedelago hydraulic node hydrologically the castelfranco veneto hydraulic node is the most important one adbve and adbadige 2010 this hydraulic node is responsible for managing a iwt from the avenale sub basin to either the vlw or to the muson dei sassi river the latter not discharging into the venice lagoon when water is derived from the avenale sub basin to the vlw it may either flow to the dese river or to the marzenego river the latter not part of the dzrb but part of the vlw the complex avenale s iwt is operated at the hydraulic junction of castelfranco veneto as shown in fig 1 similarly the zero river receives hydraulic contributions from the brenton del maglio sub basin by means of a hydraulic junction located near the city of albaredo di vedelago bixio et al 2009b water that is deviated from the brenton del maglio sub basin may either flow to the zero or to the sile rivers the latter not discharging into the lagoon of venice the artificially controlled water dynamics observed in the dzrb is fundamental for maintaining an optimal streamflow regioneveneto 2000 this is especially true during the spring and summer seasons when the amount of water withdrawals increases mainly due to increased evapotranspiration as a result of the increased irrigation water demand bixio et al 2009b as a consequence an irrigation schedule scheme is currently in place in the dzrb bixio et al 2009c this irrigation schedule is set up considering water availability and irrigation water demand which in turn depends on soil characteristics and water availability as a consequence water may be imported from external sources when water demand for irrigation is greater than the amount of water available piave 2011 similarly when water supply is greater than the demand this same intricate hydraulic control system enables the administration of flood related risks as the controlled hydraulic devices can be operated as a flood control system bixio et al 2009b 2 2 the swat model swat arnold et al 1998 is a conceptual eco hydrologic river basin scale model which allows a number of different physical processes to be simulated in a watershed while also enabling the evaluation of the impacts of different land management practices on the surface runoff water quality sediment transport and agricultural chemical yields processes swat offers the capability of assessing different land and water management processes such as irrigation scheduling and water transfer between sub basins while at the same time providing the means to support the assessment of their impacts on a river basin scale neitsch et al 2011 for hydrologic modelling purposes swat considers a river basin as a mosaic of smaller spatially defined units known as sub basins which are in turn further subdivided into smaller land units called hydrological response units hrus neitsch et al 2011 winchell et al 2013 hrus are defined as lumped land areas within a sub basin that are comprised of unique land cover soil slope and management combinations arnold et al 2012b the consideration of spatially distributed elements i e sub basins and its subdivision into hrus enables the model to reflect consequences on eco hydrologic processes for a variety of land use and soil classification both temporally and spatially with regards to the simulation of external complex water flow contributions swat allows water to be transferred and applied on an hru from any water source within e g between reservoirs reaches and sub basins or outside e g iwt a watershed neitsch et al 2011 swat accounts for these processes by reading input information pertaining to the type of water source the location of the source the type of water body receiving the transfer the location of the receiving water body and the amount of water transferred for every day of the simulation neitsch et al 2011 as a consequence detailed quantified information is required to accurately reproduce water transfers in a watershed something that can become a serious issue in cases of lack or insufficient availability of data the most basic information required by swat in order to simulate the hydrologic balance of a watershed are i digital elevation model dem ii soil spatial distribution and characteristics iii land use spatial distribution and respective land management operations and iv hydro meteorological data arnold et al 2012a in this study a dem of resolution 5 5 m is utilised regioneveneto 2014 the soil and land use vector maps regioneveneto 2014 were converted to raster format at a 5 5 m resolution grid by using the resample majority method so to maintain spatial integrity with the dem the original soil map classes were combined into 8 representative classes according to their soil textures soil depth soil layer profiles and total coverage area missing soil parameters required by swat e g saturated hydraulic conductivity were derived from pedotransfer functions sun et al 2016 the original land use classes were combined and converted to swat compatible land use classes according to their representativeness in terms of total coverage area resulting in a final number of 13 land use classes as a result the dzrb was sub divided into 14 sub basins for a final number of 476 hrus groundwater flow contributions coming from the shared aquifer system depicted in fig 1 were considered as additional inlet elements azzellino et al 2013 essenfelder et al 2016 in order to define an upper boundary for the external hydraulic loadings iwt from the dynamic areas of the dzrb the avenale and brenton del maglio sub basins were set up as single swat projects hence the avenale sub basin was sub divided into 79 hrus while the brenton del maglio sub basin has been partitioned into 16 hrus meteorological data available for the case study area has been processed using the swat weather database tool essenfelder 2016 and temporally ranges from jan 1993 to dec 2014 on a daily basis for the meteorological stations shown in fig 1 streamflow data was available as measured by two distinct stream gauges with data ranging from 1997 to 2005 for calibration purposes observed streamflow data was arranged into the calibration and validation datasets aiming at a validation calibration ratio of around 1 4 table 1 shows more details about the data used for the calibration and validation of the swat model 2 3 the ann model artificial neural networks anns are mathematical constructs that can be configured as linear or non linear models being defined as massively parallel distributed processors constituted of single process units having the ability to store experiential knowledge and make it available for later use haykin 2001 anns have been applied with relatively success in several fields of research such as hydrologic modelling image classification remote sensing human behaviour analysis and decision making simulation wu et al 2005 giacinto and roli 2001 hsieh 2009 strnad et al 2015 neural networks were born taking inspiration from the attempt to translate the knowledge on how the human brain works into a quantitative model being capable of acquiring and storing knowledge from the surrounding environment ultimately being capable of learning through training from this interaction wilamowski and irwin 2011 together with the notion of learning comes the idea of adaptability as anns have the capability of adapting their synaptic weights according to their perception of the surrounding environment these capabilities of ann models make this kind of machine learning technique an interesting option to simulate intellectual activity processes matsuda 2005 such as the decision making processes behind the management of iwts the ann model used in this study has been developed by the authors essenfelder 2017 and is structured as a multilayer perceptron mlp neural network using back propagation as the supervised training algorithm and levenberg marquardt as the optimisation algorithm hagan and menhaj 1994 yu and wilamowski 2011 the model is capable of running in a multi core configuration either as a simple input output model as a time delayed input output model or as either non linear autoregressive nar or non linear autoregressive with exogenous inputs narx variants for this study a narx ann model variant is used consisting of two hidden layers and using a swish activation function ramachandran et al 2017 between hidden layer nodes the activation function at the output layer is a rectified linear activation unit relu function hence allowing simulated values to be greater than the maximum observed valued in the target dataset regarding the training process the ann model evaluates the evolution of the mean squared error mse of the simulated results with regards to the targets i e observations as a metric system to assess whether generalisation has been achieved and to avoid the overfitting of the resulting trained model maier and dandy 2000 hsieh and tang 1998 the target variable considered in this study is the daily amount of water that is transferred in iwts systems as described in section 2 2 the basins that contribute to the two simulated iwts are modelled separately i e avenale and brenton del maglio sub basins hence the target variable describes the amount of water that is transferred from each of the individual sub basins the dzrb by means of iwts similarly the difference between the total amount of streamflow as simulated by the individual swat projects for each subbasin and the amount of water that is transferred to the dzrb represents the water flow that is not discharging in the vlw the latter being an important aspect for managing flood risk in the dzrb which is however out of the scope of analysis of this paper the amount of water transferred in iwts systems is usually variable in time depending among other factor on water availability at the source and water demand at the destination in this context the way the iwts of the dzrb are managed corresponds to a decision making process in which decision makers determine the amount of water to be transferred as a function of hydro meterological hydraulic and water requirement e g irrigation conditions since there is no available data series of observed flow transferred in the iwts systems the daily amount of water that is transferred in iwts systems is estimated by means of hydrologic modelling as described in section 2 4 during the hydrological simulations of the coupled swat ann model the neural networks module receives on line information from and is called iteratively by the swat model for every day of simulation the iwt results obtained from the ann model are then transmitted to the swat model and incorporated into the hydrological simulations being finally passed to the subsequent routing phase calculation steps the maximum iwt estimated values should not exceed neither the daily simulated streamflow that is reaching an hydraulic node i e upstream basin streamflow in table 2 neither the technical limitations of the hydraulic node i e maximum iwts flow in table 2 the maximum technical flow of the iwts in the case study area were obtained from consiglioveneto 2009 and bixio et al 2009b in order to account for past hydro meteorological information the variables precipitation upstream streamflow and downstream streamflow are considered in a temporal window of 7 days since two iwts are being considered in this study an ann model is set up individually for each hydraulic node table 2 shows the list of input variables to the ann models the spatial scale in which they are considered their units and their symbols in the case study area expert local knowledge on irrigation land reclamation and the hydraulic management of the watershed must be integrated with the understanding of hydro meteorological parameters for the proper management of the vlw s sub basins bixio et al 2009c hence the information shown in table 2 is selected having in mind the complex decision making process where the selected input variables are presented to the ann model in order to transmit the knowledge of when water availability and water demand might need artificial interventions 2 4 the methodological framework a schematic representation of the proposed methodological framework to couple the swat and ann models is depicted in fig 2 the methodological framework presented here relies on two assumptions i the hydrologic model utilised in the first step is capable of reproducing physically consistent results regarding the rainfall runoff process for the case study area and ii the difference between observed and simulated streamflow values in any point downstream of iwts contribution points is a good proxy for the estimation of the quantification of the absolute amount of water transferred by iwts in the case study area the second assumption is necessary in the dzrb case study as no quantitative information is available regarding the iwt process in river basins where the amount of water transferred by iwts is known this assumption can be relaxed and this step can be by passed the first step of the proposed methodological framework i e swat model reference calibration in fig 2 consists in reference calibrating the swat model based on specific information already published in the literature salvetti et al 2007 2008 azzellino et al 2013 essenfelder et al 2016 information from other sources is used as well in order to get specific information pertaining the hydrologic balance of the case study area such as technical details about the groundwater dynamics and irrigation schedule schemes bixio et al 2009a b c boscolo and mion 2008 regioneveneto 2000 adbve and adbadige 2010 all this information is manually transferred to the parametrisation of the swat model the second step i e validation pbias in fig 2 the first inside the swat ann box consists in evaluating the accuracy in which the swat model is capable of reproducing the hydrologic balance of the case study watershed the pbias model efficiency metric is selected as it is capable of expressing the tendency of simulated data to over or underestimate the reference data moriasi et al 2007 in case the results are considered non satisfactory something that is expected for the dzrb due to the operation of iwts and assuming that the first methodological step is consistent it is possible to estimate the difference between the observed and simulated values this information then is considered as a proxy for the quantification of the absolute amount of water transferred by iwts in the case study area the third methodological step i e ann model training validation inside swat ann box in fig 2 consists in two different phases first the ann model is trained under varying structures i e number of neurons per layer so to obtain an optimal ann model configuration second the model is validated by means of k fold cross validation for training and validation of the ann model the inputs target dataset is randomly split into calibration 70 validation 15 and test 15 datasets the test dataset is used during the training phase of the ann model in order to avoid the over fitting of the model not being used however for the calibration of the ann model itself the validation dataset instead is used only after the completion of the ann training to evaluate its performance thereby not being presented to the model during the training phase the training process of the ann model is repeated 1000 times where each training attempt is initialised by restarting the initial weight connection values adjusting the neural network structure and k folding the data samples the 10 best ann models i e the ann models with the 10 lowest mse are stored as an ensemble of valid models considered to be the best candidates for fitted models demuth 2006 the simulated iwts values are estimated as the median of the 10 models step three is done separately for each iwt i e one for the castelfranco veneto and another for the albaredo di vedelago hydraulic nodes the fourth step i e swat ann model swat cup calibration inside the swat ann box in fig 2 consists in coupling the ann model obtained at step three with the swat model obtained at step one the two models are coupled and run simultaneously during the swat ann calibration cycle exchanging information at a daily level this is done through a modification in the source code of the swat model to call the ann model during every simulated day the coupled swat ann model utilises the swat cup software and the sufi2 procedure abbaspour 2015 for calibration purposes a first initial calibration cycle is performed by running 900 calibration iterations followed by two sub sequent fine tuning calibration cycles of 500 iterations each at the end of every calibration cycle results with lowest pbias are assumed to be the most fitted model for the calibration cycle while the model configuration with the lowest pbias value at the end of the third calibration cycle is assumed to be the overall most fitted swat ann model in case a pbias value lower than 25 and greater than 25 is obtained the model is assumed as valid and the calibration process of the model proceeds successfully to the next methodological step in case pbias results indicate non satisfactory results the resulting swat ann model is considered as a non valid model and the overall calibration process is terminated i e an impossible estimation of iwt flows is assumed as shown in fig 2 in case of a positive pbias validation check the fifth methodological step is reached and a new calibration of the coupled swat ann model is performed i e swat ann model swat cup calibration outside the swat ann box in fig 2 by using the swat cup software and in a similar fashion to the calibration procedure described in the fourth methodological step this time however two efficiency criteria are simultaneously evaluated namely the nash sutcliffe model efficiency coefficient nse and again the pbias the nse is employed in this step as it allows for the identification of not only how well the simulated values of the swat ann model is being reproduced in time but also how well these results fit with the observations as the calculation of the nse criterion involves the computation of the squared difference between the observed and predicted values krause and boyle 2005 the metrics to evaluate the performance of the coupled swat ann are obtained from the literature moriasi et al 2007 in case the resulting swat ann model is capable of producing satisfactory results with respect to observations the model is assumed to be validated 3 results and discussion the results and discussion section are presented following the five methodological steps presented in section 2 4 and summarised in fig 2 3 1 steps 1 and 2 swat model reference calibration following the methodological framework described in section 2 4 and summarised in fig 2 the manually calibrated swat model is followed by a calibration step using the swat cup and the sufi2 procedure to further refinement regarding the water balance calibration this procedure consists in modifying a total of 16 distinct swat parameters focusing on three main hydrological related processes i e surface runoff baseflow and soil hydraulic properties the list of modified parameters and their final calibrated values for this step is shown in table 3 in accordance with the proposed methodology the performance of the pre calibrated swat model is evaluated by means of the r2 coefficient of determination the results regarding the performance evaluation of the pre calibrated swat model are shown in table 4 the results shown in table 4 indicate an acceptable behaviour if r2 is considered as the sole model efficiency criterion both under the daily and monthly model configurations for streamflow with the monthly basis configuration showing slightly better results however the results for the nse and pbias are both non acceptable these results indicate that the hydrologic simulations are mispredicting the observed streamflow this behaviour is confirmed by a considerable large underestimation bias for streamflow in the order of 51 for the dzrb the results shown in table 4 are somehow expected due to the fact that the dzrb is characterised by receiving significant amounts of external water coming from the iwts especially during the crop growing season in general under the assumed configuration and due to the complexity of the studied watershed the swat model alone is not capable of reproducing the overall water balance of the dzrb however it is interesting to notice that even if the iwts components of the system are missing the proposed pre calibrated swat model is capable of producing simulations somewhat linearly correlated to the observed data as evidenced by the coefficient of determination results in fact on a daily basis the linear correlation between the observed streamflow and the simulated streamflow is as high as 0 736 for the dzrb in summary the results for the pre calibrated swat model suggest that although the proposed model configuration is not capable of accurately simulating the hydrology of the studied watersheds it is capable of linearly reproducing its behaviour in time even if it systematically underestimate the streamflow observed values 3 2 step 3 ann model training validation recognising the fact that iwts are missing to fully describe the hydrologic system of the case study area see fig 2 the next proposed methodological step consists in estimating the hydrologic influences resulting from the complex management of the iwts hydraulic nodes as described in the section 2 4 an ann model is used for such a purpose moreover for modelling purposes the dzrb is split into dese and zero subbasins allowing for the training of specific ann models for each system the modelling results of this operation are summarised in table 5 while fig 3 depicts the information regarding the training progress and model performance the results shown in table 5 and fig 3 indicate that the ann model is capable of satisfactory reproducing the estimated total iwts water flows to the studied watershed in fact both the nse and pbias results shown in table 5 suggest a very good skill of the ann model to reproduce the desired hydraulic influences particularly when aggregated at the monthly scale spatially the skill of the ann model is slightly lower when attempting to simulate the estimated total hydraulic influences from iwts to the dese subbasin even so the results are considered to be satisfactory as shown by a value of approximately 0 73 for nse and of approximately 1 0 for pbias both for the test dataset the positive pbias results indicate however a slightly tendency of the ann model to underestimate the target values the strong modelling skill of the ann model is graphically presented by the scatter plot of the target vs simulated values as shown at the second row of fig 3 the third row of fig 3 depicts some useful statistics regarding the simulation results of the ensemble of ann models such as the range of the simulated values and their respective first and third quartiles for each month together with the median value of the target values for comparison purposes in general these results indicate that the ann model has a very good skill in reproducing the monthly median iwts flow contributions interestingly it is possible to draw an interesting parallel with the water demand for irrigation during the crop growing season in the dzrb as can be observed during the increased water inflow during the months between march and september in summary the results shown here indicate that the employed ann model is capable of computing both at the daily and monthly scales the estimated hydraulic influences not taken into account when running the swat model alone 3 3 steps 4 and 5 swat ann model swat cup calibration validation following the calibration and validation of the neural networks model the next proposed methodological step consists on coupling the swat and the ann models on re running the calibration process for the new model and on the re evaluation of the new simulation results the outcomes of this operation are summarised in tables 6 and 7 fig 4 instead depicts a comparison between the results of the swat and swat ann models when performing the simulation of streamflow extreme values on a daily basis confronting the results shown in tables 3 and 7 it is possible to verify that the mean calibrated values for the majority of the calibrated parameters have not change significantly from the pre calibrated swat model configuration indicating a satisfactory swat pre calibration procedure in fact considering only the parameters pertinent to the hydraulic balance of the studied watersheds only one parameter show variations above an absolute threshold value of 50 with respect to the results of the pre calibrated swat model when considering all three watersheds namely gw delay the increase in the absolute value of the parameter gw delay indicates that the period of time that water takes to leave the soil profile and recharge the shallow aquifer is greater in the coupled swat ann model signalling that the pre calibrated swat model was possibly over predicting the magnitude of the return flow in order to cope with the absence of the already mentioned iwts hydraulic influences the variation in the gwqmn parameter corroborates this idea the coupled swat ann model calibration points to the direction that a deeper threshold of water in the shallow aquifer necessary for the return flow to occur with regards to the results of the pre calibrated swat model another parameter that shows interesting results it the surlag the actual increase in the surlag parameter indicates that the pre calibrated swat model was over predicting the amount of surface runoff discharging into the streams for every day of simulation in any case the gw delay gwqmn and surlag values obtained after the swat model pre calibration procedure indicate a configuration which tried to emulate the observed hydrological behaviour of a system without however considering any of the iwts hydraulic influences to the studied watershed consequently the automatic calibration of the swat model adjusted the intrinsic watershed parametrisation in order to account for such missing information ultimately this has lead to an overestimation of both groundwater and surface runoff contributions to the main channel flow when the relevant external water sources to the studied watersheds are not considered regarding the modelling capabilities of the coupled swat ann model it can be verified a significant improvement of the model s performance as shown in table 7 and fig 4 particularly at the monthly scale an interesting result attributed to the difference in the composition of the calibration and validation datasets can be verified when analysing the results shown in table 7 while the coupled swat ann model exhibits a tendency to overestimation the streamflow for the calibration dataset it shows the opposite behaviour when applied to the validation dataset this discrepancy can be attributed to particularities of both calibration and validation datasets such as the presence of wetter years in the calibration dataset anyhow the results of the coupled swat ann model for case study area can be considered to be satisfactory and significantly improved from the results of pre calibrated swat model the results shown in table 7 and fig 4 confirms the importance of iwts flows for the water balance of the dzrb by applying the methodology described in section 2 it is possible to quantify these influences even under a situation of data scarcity for the case study area the mean percentage of the total streamflow of the dzrb that can be attributed to sources external to the watershed varies from 19 during wet weather conditions to 59 during dry weather conditions during the simulation run from 1996 to 2014 4 conclusions the dzrb is characterised for being a very complex catchment several modifications in its superficial watercourses throughout the centuries have resulted in a very unique environment requiring specialised hydraulic management practices from a number of external hydraulic contributions capable of affecting the water dynamics of the vlw two stand out namely i groundwater entering the vlw from the big unconfined aquifer to the north north west of the vlw and ii artificially controlled superficial waters deviated from to bordering watersheds that do not discharge into the lagoon of venice this study proposed a framework to estimate the total external hydraulic contributions to the dzrb a sub basin of the vlw the proposed methodological framework is built upon the use of a coupled mechanistic empirical modelling technique based on the hypothesis that the mechanistic model i e swat is capable of simulating the hydrological processes and water movement occurring inside boundaries of the studied watersheds while the empirical model is capable of simulating the total external hydraulic contributions the mechanistic model used in this research is the swat model while the empirical counterpart is an artificial neural networks ann model the results obtained from the implementation of the proposed methodological framework suggest that the coupled swat ann model is not only capable of satisfactory reproducing the water balance of the studied watershed but also to increase the hydrological modelling capability of the swat model when performing under an intricate and complex environment such as the dzrb the results obtained from the application of the proposed methodology also confirm the findings of previous studies in the same area indicating that under ordinary flow conditions and in dry periods the water balance of the dzrb is in general highly affected by water flowing into the watershed from bordering watersheds particularly during the spring and summer seasons finally as some recommendations for further developments in this field of research it is proposed the consideration of other hydro meteorological variables such as snow coverage area snow depth and potential snow melt flux in the pre alpine region affecting the recharge processes of the aquifer system and the water table depth of the aquifer system in the vicinities of the vlw if at a compatible time scale moreover the use of more robust modelling tools such as the swat modflow model could also lead to better simulation results particularly in what pertains to the groundwater flow dynamics declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors are thankful and acknowledge the support regarding the possibility of performing some calculations on the scscf computer cluster a multiprocessor system owned by ca foscari university of venice and running under a gnu linux operating system all the data and source codes utilised during the development of this research can be found here https doi org 10 5281 zenodo 3699658 the authors would also like to thank all the people who has directly or indirectly contributed to the development of this research in particular arpav area tecnico scientifica regione veneto ambiente e territorio and the consorzio di bonifica acque risorgiva finally the authors are thankful for the invaluable support given by dr arianna azzellino and dr silvio giove particularly in providing specific data and technical information without which would make this research not possible the authors gratefully acknowledge the support by the venice centre in economic and risk analytics for public policies vera 
26004,modelling climate is complex due to multi scale interactions and strong nonlinearities however climate signals are typically quasi periodical and are likely to depend on exogenous variables motivated by this insight we propose a strategy to circumvent modelling complexity based on the following ideas 1 the observed signals can be decomposed into non stationary trends and quasi periodicities through dynamic harmonic regressions dhr 2 the main frequencies and decomposed signals can be used for constructing a harmonic model with varying parameters depending on exogenous variables 3 the state dependent parameter sdp technique allows for the dynamical estimation of these parameters the resulting dhr sdp combined approach is applied to rainfall monthly modelling using global climate signals as exogenous variables as a result 1 the model yields better predictions than standard alternative techniques 2 the model is robust regarding data limitations and useful for several steps ahead forecasting 3 interesting relations between global climate states and the local rainfall s seasonality are obtained from the sdp estimated functions keywords dynamic harmonic regressions state dependent parameters monthly rainfall trends quasi periodicities 1 introduction in general deterministic models describe natural phenomena through partial or ordinary differential equations which inherently involve several modes and interactions through different scales beven 2010 often lower scales imply higher complexity and are sometimes parameterized under mass energy and momentum restrictions so that they can be coupled into the general differential equations of higher scales however these techniques frequently entail a large number of parameters and variables which are difficult to measure estimate or optimize neelin et al 2010 prominent cases of what we have stated above are the global climate models gcm s and the dynamical downscaling techniques those techniques link the climate simulations at different spatial scales assuming some particular parameterizations according to the regional case hewitson and crane 1996 however in some regions the natural high complexity of climate often entails an inability to adequately parameterize the climate conditions producing serious inaccuracies and uncertainties for regional climate simulations moncrieff et al 2007 an example is the case of climate modeling in tropical andean areas and its intense convective activity lin et al 2006 buytaert et al 2009 ochoa et al 2016 therefore reduced order dynamical models or other simplified options incorporating some information of the main modes of the phenomena are needed for practical purposes a possible answer to conceive simpler and reliable alternatives to complex deterministic models may come from specific evidence arguing that environmental phenomena are often dominated by a reduced number of modes raper 1996 young and ratto 2011 geoffroy et al 2013 young 2018 this is reminiscent of reduced order models that describe complex phenomena by considering the most essential information however the question of how to determine what information is crucial and how to systematize it into a meaningful model is a scientifical challenge for certain climate signals the previous issues can be tackled by identifying typical climate features and patterns through different scales for instance global climate indicators synthesize a considerable part of the worldwide climate variability and are commonly used for unveiling climate interactions at different scales teleconnections glantz et al 1991 also typical local climate signals obey non stationary trends and quasi periodicities which could be hypothesized as linked and controlled by global indicators thus it seems necessary to identify these signals components first after that the global climate information must be related to those components following an appropriate mathematical framework the dynamic harmonic regression dhr is a flexible tool for identifying the non stationary trends and quasi periodical components of a climate signal mindham and tych 2019 khelifi et al 2018 young 1998 2018 trapero et al 2015 young et al 1991 into the standard dhr model the temporal variability of the parameters is determined by generalized random walk grw processes which includes several random walk types of which the most important are the random walk rw integrated random walk irw and smoothed random walk srw young et al 1999 young 2012 this suggests the idea of including the global information into the dhr s structure by replacing these grw processes with functions dependent on global climate states playing the role of exogenous variables employing the state dependent parameter sdp technique then for constructing a climate model the present work proposes a methodology that combines the dhr and the sdp techniques on the one hand dhr will allow the identification of the fundamental harmonics and trend here also named components for a climate signal on the other hand the sdp technique will determine non parametric functions of global climate signals explaining the parameters variation used for modelling such components in other words global states play the role of exogenous variables handling both the harmonics and the trend through their corresponding variable coefficients the first step of the modelling process assumes some prior knowledge about the leading global signals interacting within a specific region however for complex climate regions climate interaction with global phenomena could be diverse and complex therefore pursuing parsimony an identification of the most appropriate global signals explaining the variation of the model s parameters is suggested such identification can be reached through statistical selection from the combinations of the exogenous variables conforming to sdp nonetheless to avoid the explosion of combinations the harmonic and trend signals which are products of dhr s decomposition are modelled separately in the proposed approach testing the spd s combinations of global variables once the global variables are identified for each component the sdp technique is applied over the final ensemble model three monthly rainfall signals belonging to an andean tropical climate region are modelled for testing the model s capabilities this andean tropical region is characterized by strong climate variability which is reflected by the presence of very diverse rainfall regimes these complex climate conditions make this region a challenging area for rainfall climate modeling therefore it can be regarded as a natural laboratory to test the proposed technique a rigorous specific out of sample procedure will be applied to evaluate the model s performance additionally the model is calibrated under different amounts of global and rainfall information to show the model s ability to deal with data scarcity finally modeling tests including diverse lagged global information will provide evidence about the forecasting skills of the model in order to explore some salient features that come from the integration of techniques the standard univariate dhr and an alternative sdp model are used for comparison purposes in summary this work proposes a novel model strategy resulting from the combination of the dhr and the sdp techniques dhr determines the model s structure with its fundamental harmonics and trend components sdp identifies their functional dependence on global climate signals the model s performance is assessed through an out of sample procedure and comparison with the univariate standard dhr and an alternative sdp model furthermore the non parametric functions given by the sdp non parametric functional forms is analyzed searching for possible climate mechanisms 2 materials 2 1 study area and rainfall description the paute river basin fig 1 is located in the tropical south ecuadorian andean depression coltorti and ollier 2000 the basin has an area around 6481 km2 with elevations ranging between 900 and 4200 m a s l the basin supports several ecosystems that are regarded as water providers due to its hydrological features such as the neotropical alpine wetland ecosystem célleri and feyen 2009 due to its climate characteristics the basin supplies a significant part of hydropower generation in the ecuadorian region nearly 1100 mwh of its energy furthermore its tropical mountain location provides to the area with interesting rainfall variability schneider et al 2014 cobb et al 2003 for instance three main rainfall regimes have been characterized according to a rainfall distribution analysis inside the paute basin the first of them a unimodal um rainfall regime concentrates the rainfall amount mainly in the months of june july august jja the second a bimodal bm rainfall regime correspond respectively to the seasonal periods of march april may mam and september october november son finally a trimodal tm rainfall regime is identified during the seasons of mam jja son celleri et al 2007 campozano et al 2016 in turn the rainfall regimes in the paute basin have been linked with global climate conditions for example mora and willems 2012 conclude that pacific and atlantic ocean patterns have a strong influence at decadal time scales over such regimes on the one hand enso mainly exerts a negative influence over the um areas on the other hand tropical south atlantic tsa patterns have a positive influence over the um eastern regions which coincides with buytaert et al 2006 for the region and by vuille et al 2000 in a larger scale inside the andean eastern cordillera significant findings are also reported by campozano et al 2018 in a mesoscale analysis focused on the paute basin in that study essential effects and interactions between enso trans niño index tni and the tropical south atlantic tsa are revealed these interactions are also supported by the findings reported in mendoza et al 2018 through a teleconnection analysis over decomposed rainfall signals therefore since the climate conditions of the paute basin are complex by nature it makes it an interesting natural laboratory and a strategic area for testing new and more advanced mathematical approximations for climate modelling 2 2 data three monthly data sets fig 1 provided by the ecuadorian national institute of meteorology inamhi were considered for the modelling purposes set herein due to data limitations the period considered corresponds to 30 years january 1980 december 2010 nonetheless the chosen period ensures the maximum continuity of data in the three localities gaps in data represent less than 7 of the whole series for each data set linear regressions with other neighbouring stations in the studied region fig 1 were applied for data filling villazón and willems 2010 in that respect the regressions with the highest correlations coefficients were used to estimate the gaps furthermore the regressions were applied for each month i e data for a specific month was estimated with data from other locations corresponding to the same month the identification and other features of the three chosen data sets are given in table 1 monthly global weather patterns were obtained from the national oceanic and atmospheric administration noaa for this study we include the next data sets with the same period of rainfall data extent pacific decadal oscillations pdo north atlantic oscillation nao bivariate enso time series best multivariate enso index mei enso 3 4 enso 3 enso 4 enso 1 2 trans niño index tni quasi biennial oscillation qbo southern oscillation index soi tropical northern atlantic index tna tropical southern atlantic index tsa atlantic multidecadal oscillation amo caribbean index car and madden julian oscillation mjo these signals synthesize a large part of climate variability of the atlantic and pacific oceans which surround the studied region 3 methods two main steps integrate the precipitation modelling approach based on global information presented herein first the precipitation signal is decomposed using the dynamic harmonic regressions framework then the functional relationship between the variable coefficients of the dynamic harmonic components and the global climate signals is identified using the state dependent parameter procedure these steps are described below however in order to have the necessary context to better understand this description it is important to start by reviewing the dhr and sdp frameworks 3 1 dynamic harmonic regressions dhr this stochastic framework is built upon the recursive time series analysis theory introducing the concept of time variable parameters tvp s such parameter variation time evolution is determined from data by kalman filtering techniques in linear models obeying gauss markov random walk processes the details of these techniques are better explained in the specialized literature young et al 1999 young 2000 2012 bryson 2018 and implemented in the matlab captain toolbox available at http wp lancs ac uk captaintoolbox taylor et al 2007 nonetheless since the main interest here is to understand the role of this framework for the proposed methodology we will treat it briefly starting with the following general additive model 1 y k t k s k e k e k n 0 σ 2 where y k represents the output variable observation while t k s k and e k are respectively the trend seasonal and error terms in the k th time step furthermore we can state that 2 t k s k i 0 r s a i k cos w i k b i k sin w i k a 0 k t k w h e n w 0 0 equation 2 is just a specification of equation 1 in which the trend t component is interpreted spectrally as a zero frequency term the symbols a i k and b i k represent tvps for the i th seasonality each associated to a frequency w i the fundamental and harmonic frequencies w i are estimated from the fourier transform of an autoregressive ar model s signal also known as observed spectrum of order n which is determined according to the well known akaike coefficient aic akaike 1974 1977 beamish and priestley 1981 on the other hand the state vector x i k l i k d i k t defines the evolution of the tvp a i b i parameters through a generalized random walk grw process 3 3 x i k f i x i k 1 g i η i k 1 f i α i β i 0 γ i g i δ i 0 0 ε i the vector η i k η1 i k η2 i k t is also a two dimensional zero mean vector with noise characterized by a diagonal covariance matrix q η i each α β γ δ ε as well as the elements of q η i or usually the nvr matrix q nvr q η i σ2 are the so called hyperparameters which are time invariant and are estimated from data through optimization such optimization fits the pseudo spectrum of dhr fourier transform of equation 1 to the mentioned observed spectrum once the hyper parameters are obtained the well known kalman filter and the fixed interval smoothing kf fis technique is applied over the complete tvp regression model 4a y k h k t x k o b s e r v a t i o n e q u a t i o n 4b x k f x k 1 g η k 1 s t a t e e q u a t i o n where x k of dimension p 4rc 2 is the vector arrangement of all x i k vectors while h k is the so called observation vector with a value of one for the first element while the other components correspond to the i th cos w i k and sin w i k values for the corresponding k th time step in fact alternating zeros at the even positions in h k are included in order to attain the needed dimension for the x k vector notice that equation 4a is just a compact restatement of equation 1 in addition f and g are the total block matrices of dimension p x p with blocks defined by the sub matrices f i and g i young et al 1999 peter c young 2012 3 2 state dependent parameter sdp one step further towards developing the methodology proposed here is to consider the possibility of treating the parameters a i and b i in equation 2 as functions of some state variables in that case the state dependent parameter sdp algorithm is able to provide a framework to identify the corresponding functions using kf fis techniques briefly the sdp algorithm is based on redefining the expression for the output variable y in equation 4a making it depend on a state variable χ of the system this entails changing from time space to state space and results in a smooth evolution for y in terms of χ which facilitates the application of the kf fis and grw techniques given that changes are now less abrupt as a consequence the grw evolution for the parameters in the model is characterized by non parametric functions of the state variables χ rather than being directly linked to time k furthermore the sdp algorithm can be adapted to define the evolution of the a i and b i parameters represented conveniently by x i into the following mathematical model 5a y k h k t ρ k e k 5b ρ k x 1 χ k x 2 χ k x 3 χ k x p χ k t 5c x i χ k s 1 m i ϑ s i χ s k for i 1 2 3 p vector χ represents the collection of state variables on which the parameters x i are dependent the additive function expressions in equation 5c are linearly combined into the observation equation 5a through the components of vector h k facilitating the application of the sdp algorithm in matlab captain toolbox available at http wp lancs ac uk captaintoolbox taylor et al 2007 it is worth mentioning that considering the specific harmonic form in equation 2 the h k vector components include for the matrix operation in equation 5a to make sense the repetition of m 1 times the value of one 1 and m i times the cos w i k and sin w i k values regarding the identification of the non parametric functions in equation 5c it is attained through the well known back fitting algorithm and the so called modified dependent variables mdv finally the non parametric functions are parameterized through a proper curve fitting technique expressing the functions in a concise form for the sake of brevity the details of the complete sdp algorithm and its mdvs are omitted here the reader is referred to the specialized literature for further information young 2000 2012 young et al 2001 3 3 modelling process combining the dhr and sdp frameworks now after laying out the necessary context we proceed to explain the details of our methodology from and overall perspective it is defined by the following two steps 1 frequency identification and signal decomposition using dhr 2 model construction and sdp application 3 3 1 frequency identification and signal decomposition using dhr to begin with dhr is applied over the time series of interest two outputs of interest are obtained after this process first the frequencies obtained from the observed spectrum second the non stationary decomposed components of the signal on the one hand the frequencies w i will be used for constructing the observation vector h k at each k th time step on the other hand the decomposed signals will help to tackle the technical problem of identifying proper state variables on which the model s parameters are dependent 3 3 2 model construction and sdp application first we assume a harmonic model see equation 6a below with parameters determined by state variables additive functions see equation 6b below as said before in the model each w i frequency comes from the empirical spectrum estimated by the application of dhr notice that the model meets the required linear coupled form in equations 5a and 5c which is convenient for applying the sdp technique 6a y k t χ k i 1 r s a i χ k cos w i k b i χ k sin w i k e k 6b t χ k j 1 n f j χ j k l j a i χ k s 1 m i ϑ s i χ s k l s i b i χ k s 1 m i ϕ s i χ s k l s i the inclusion of a lag l into equation 6b makes the sdp parameters estimates for the k th time step depended on the k l th time step information of the state variables thus the k th time step estimation of y made by the model 6a are calculated using the specific k l th state variables values we refer the latter property as an indicative of the predictive model s ability herein which can be used for both interpolations and future estimations of the observed variable in the model the trend component t is an additive function of n state variables while each pair of parameters a i b i is composed by the same m i state variables this condition is not mandatory although it seems convenient since each pair of ϑ s i and ϕ s i functions could be regarded as individual signals assembling the i th harmonic component therefore each i th harmonic component can be fragmented into m i signals with its non stationary amplitudes a and phases φ as functions of specific state variables 7a a s i k ϑ s i χ s k l s i 2 ϕ s i χ s k l s i 2 1 2 7b φ s i k tan 1 ϑ s i χ s k l s i ϕ s i χ s k l s i for s 1 2 3 m i these individual amplitudes and phases could entail some advantages in the analysis of how each state variable determines its influence over a specific i th seasonal component however these amplitudes and phases cannot be considered additive with respect to the overall amplitude and phase a i and ϕi of a specific i th harmonic component because they do not have a linear form the modelling process is almost complete when the state variables are known for each specific signal to be modelled the next step is to estimate the non parametric functions f ϑ ϕ through the sdp technique finally for the sake of simplicity each of these functions is fitted to some parametric form or some other rational alternative as the one considered here for the application nonetheless in complex processes involving several state variables without necessarily playing significant roles in the context of this particular modelling technique the question of an efficient and parsimonious model becomes a technical problem the latter could be tackled through the statistical searching of the best model among all possible combinations of state variables included in it in an attempt for avoiding the explosion of combinations in the complete model see eq 6 we suggest an sdp modelling for the trend and each i th harmonic in equation 6b separately using the dhr s decomposed signals as surrogate observations for fitting these models after we select the best combination of state variables to be included in each component and we ensembled the final model with the form of equation 6a 3 4 the application in the context of rainfall modelling and evaluation in previous research by some of the authors of this paper global climate variables were linked to the trend and amplitudes of decomposed rainfall signals through a methodology proposed by mendoza et al 2018 summarized in annex 1 we use this process as a starting point that enables us to have an educated guess for the state variables global climate signals that should be considered for the functions in equation 6b since global variables were smoothed in mendoza et al 2018 for reasons exposed therein these are the state variables used herein a lag of one l 1 is given to all the functions of state variables into the model as the required minimum for forecasting tasks for the selection process the coefficient of determination r2 is used to assess the selection of the best individual trend and harmonic models including r 1 2 3 n mi state variables on the other hand the akaike coefficient aic is used to measure parsimony choosing the best model for trend as well as for each seasonal component the complete model is then constructed by assembling the selected state variables identified from each of the components and finally applying the sdp algorithm to evaluate performance the k fold cross validation process a particular out of sample evaluation technique is applied elsner and schmertmann 1994 this technique partitions the data set into k equal sized subsamples avoiding overlapping thus one subsample is retained for evaluation out of sample predictions while the rest of the data is available for training the model from here we get k different trained data sets and the same amount of out of sample predicted data sets the trained and predicted data sets are statistically analyzed using the kling gupta efficiency kge parameter which synthesizes the correlation coefficient r the bias ratio β and the variability ratio γ in one single metric kling et al 2012 using a single lag l 1 into the model implies that each set used for out of sample prediction in the cross validation process delivers a one step ahead prediction however it is worth mentioning that out of sample predictions are instruments used for evaluation of the model into the k fold cross validation process and should not be considered as a forecasting representation ability of the model notice that the previous predictions are analogous to the dhr s one step ahead forecasting estimations so that it is reasonable to compare them in order to contrast the overall predictive capacity of the proposed model additionally we use an additive sdp technique for contrasting purposes the model follows equations 5 a and b considering each parameter x i as a function of one state variable and h k is a vector of ones for each time step k all the climate signals described in the data section are used as state variables and we give a minimum lag of one l 1 for all the sdp functions the same k fold cross process with a k 12 will generate the training and the out of sample predictions sets for comparison of the general predictivity finally to evaluate the model s tolerance to data limitations and the possible degradation of forecasting effect against a progressively lagging increase we replicate the cross validation process for different values of k along with different lag values l thus if we consider k 12 6 4 and 3 and a lag sequence set from l 1 to l 12 we obtain four experiments for each lag parameter i e 48 experiments in total for the sake of simplicity the statistics of all these tests will be graphically summarized and analyzed the confidence bounds of the models corresponding to one standard deviation attained through the kf fis algorithm are analyzed for the models young et al 2001 young 2012 the error s assumptions on dhr are statistically evaluated using the shapiro wilk breusch pagan and durbin watson breusch and pagan 1979 farebrother 1980 royston 1982 for the proposed model a cross correlation diagnostic between errors and input variables is likely to furnish greater confidence in the overall results and therefore we perform it here 3 5 important considerations and comments about the rainfall application the model does not include explicitly any local or regional climate information it could be incorporated for example through a stochastic component driven by regional climate variables this is an exciting possible extension of our modelling approach for the future for simplicity linear interpolations between two consecutive points of the non parametric functions are used as surrogates for parameterizations in the proposed and the additive models when extrapolations are necessary the models assume the functions values corresponding to either the maximum or the minimum of the state variables domains this circumvents the need for any assumptions beyond what was inferred during training the reason why we refrain from hypothesizing a parametrical form is that as opposed to other applications we do not have any a priori knowledge of a functional form having some conceptual or physical meaning in fact we believe that one of the contributions of this work is to open the possibility to explore the physical meaning of the non parametrical state functions that were obtained smoothing global signals into the model s functions provide the model with an attenuation effect against extreme anomalies of the global climate thus the model reacts under persistent global climate variations rather than fast fluctuations this consideration could entail some meaning in the sense that high frequencies in climate are indeed attenuated through distance the smoothing effect also attenuates the noise in the measured global climate variables thus mitigating the bias effect of sdp non parametric estimations to simplify the model form and its sate variable identification we use the same lag parameter for all the sdp functions nonetheless the authors are aware that better and more interesting models could be found with other lag combinations unobserved component models as reference techniques for comparisons could be used however to the authors opinion this task is beyond the scope of this work mainly because of the following reasons o previous research identified climate connections between the global and rainfall decomposed signals specifically for trends and harmonics amplitudes these connections were statistically established and conceptually validated which supports the application here mendoza et al 2018 an alternative uc model could require the same process otherwise it could suffer from identifiability problems o the proposed model here is purely dependent on exogenous variables which allows the analysis of possible incidences that global climate states could have on trends amplitudes and phases for a local signal no information from the history of the local variable is used as an input for the model 4 results and discussion 4 1 dhr signal decomposition besides the trend component three main frequencies mostly describe the rainfall s observed spectrum as shown in table 2 these frequencies represent the seasonal regimes in the studied area celleri et al 2007 for instance in fig 2 a the seasonality of peñas m217 shows a dominant frequency on its power spectrum 12 month period which is reasonable since the rainfall signal belongs the um region inside the paute basin similarly the stations of jacarin m197 and labrado m141 show two and three dominant frequencies which agreed with the bm and tm regimes respectively fig 2 b c for the three rainfall signals modelled by the dhr the smoothed random walk process srw 0 α 1 β γ ε 1 δ 0 and the first order autoregressive random walk process ar 1 0 α 1 β γ ε 0 δ 1 determine the evolution for trends and all seasonal parameters a i b i of equation 2 these srw and ar 1 processes are special grw s cases see equation 3 and provide the best matching between the observed spectrum and the pseudo spectrum as shown in fig 2 table 2 reports the hyper parameters optimization α n v r as well as their scores and standard errors the dhr performance is statistically summarized in table 3 notice that since our main objective is to measure the dynamical performance of dhr in replicating the rainfall signals the well known non parametric kendall tau test is used for measuring the ordinal association between observed and simulated values abdi 2007 kendall 1938 the peñas m217 station reached a minimum of 0 77 which means an acceptable performance in the simulation recursive algorithms for dhr modelling require normal distributed homoscedastic and white noise residuals which are evaluated by the sh bp and dw tests respectively normality is not fulfilled for the labrado m141 station according to the sh test all three cases show non homoscedastic and uncorrelated residuals according to bp and dw tests these results have implications on statistical inference nonetheless since dhr modelling is used for signal decomposition the results are accepted given the purposes set herein 4 2 the monthly rainfall modelling and evaluation the dhr decomposed signals were used for training the sdp models in equation 6b after that we selected a model for each of the state variables in it i e the best model considering j 1 2 and s 1 2 in equation 6b the selection is based on the coefficient of determination r 2 on the one hand an increasing pattern is shown when the r2 values are plotted according to the number of state variables on the other hand the aic parameter shows an optimum a minimum for a specific number of state variables see fig 3 the above results indicate that each time we choose a component model with a specific set of r state variables in it i e r 1 2 3 n mi the next set containing r 1 state variables provides the model with a better representation of its variability nonetheless there is an adequate number of state variables with respect to the criterion of parsimony annex 1 summarizes the global climate variables included in the model s components which are used for assembling the complete model structure i e the model in equation 6a with respect to the evaluation of the models table 4 summarizes the statistical performance obtained from the k fold cross validation using k 12 for data partitioning we analyze the kge average and its standard deviation from the twelve tests for describing the model s predictive ability during the training process labrado m141 shows the best kge average value among the three models 0 81 however for the out of sample predictions the kge average falls to 0 59 the reasons for such a loss rely mainly on the deficient ability of the model to capture the signal s variability as shown by the r coefficient fig 4 h shows accurate predictions for the last out of sample evaluated subset however kge s standard deviation indicates difficulties regarding generalization as confirmed by the ensembled out of sample prediction subsets in annex 2 c these limitations are probably related to the complex rainfall regime to which it belongs celleri et al 2007 campozano et al 2016 regarding the jacarin m197 signal the model does not perform high in kge terms during the training process and fig 4 e shows deficient out of sample predictions for the last evaluated subset however the predictivity is generally acceptable for the remaining data subsets being assessed which is statistically shown in table 4 the model is able to capture the signal s dynamic r coefficient but is deficient in representing the variability γ coefficient the latter is supported by the ensemble results shown in annex 2 b in which the seasonality is well modelled but not the high extremes the jacarin m197 signal belongs to the bm rainfall regime which is characterized by a marked seasonality and complex convective mechanisms related with extreme events celleri et al 2007 campozano et al 2016 2018 then the performance of the model could be linked to the described climate features interestingly the convective activity is significatively influenced by tni and tna campozano et al 2018 which are part of the global signals driving the rainfall model here see annexe 1 finally the rainfall modelling for peñas m217 is relatively the one with the lowest kge performance during the training sets as well as for the out of sample predictions table 4 the model shows limitations for capturing the dynamic and the variability of the signal i e lower r and γ values however the out of sample predictions have a stable performance through the differently evaluated data subsets as indicated by its narrower kge s standard deviation in table 4 the latter is supported by fig 4 b and annex 2 a peñas m217 belongs to the um regime of the paute basin where its seasonality concentrates the rain mainly in one period inside the year celleri et al 2007 further the rainfall has its origins from the amazon s water vapour which is transported by the easterly winds that finally triggers advective processes when forced to rise through the mountains campozano et al 2016 in turn these easterly winds are influenced by the atlantic climate states tsa and tna which are the same global climate signals included in the rainfall trends and its 12 monthly periodical model s component annex 3 contains the results of cross correlation tests between errors and exogenous input variables for the models in general no significant traces indicating under modelling are observed these tests increase our confidence in the overall modelling results 4 3 contrasting the new model with related alternatives in the case of dhr s modelling the fitted and one step ahead predictions for each rainfall signal were obtained from a single kf fis process employing the complete set of observations although no cross evaluation method was applied for the dhr modelling the results were statistically systematized separately for each of the data subsets used for cross evaluation of the model proposed herein from this comparisons between these two models were performed and the results are detailed in table 4 according to the kge coefficient table 4 the fitted values reached by the dhr model during the training phase outperform the fitted rainfall values attained by our model for all the modelled signals specifically the correlation coefficient r suggests that dhr s fitting represents the history of rainfall observations better this supports the idea of using dhr s frequencies and decomposed signals to construct our model because they reproduce the rainfall signals accurately however when one step ahead predictions are considered our model renders results that outperform those attained by the dhr model as indicated by the kge coefficient in table 4 within the context of this work one step ahead predictions are used as a measure of the predictive ability of the models which is meaningful in a very practical sense with the proper global climate information available at a given month a potential user of the dhr sdp technique would be able to obtain an improved prediction of the subsequent monthly rainfall value with respect to that estimated by the dhr model alone regarding the alternative model for comparison as shown in table 4 for all the modelled signals our proposed model performs statistically better than the additive model during the training process as well as for the out of sample predictions given that an alternative sdp model does not have the same predictive effectiveness as the proposed model it can be said that feeding harmonic inputs to sdp functions is an adequate strategy for modelling monthly rainfall signals definitely one can hypothesize that the phenomenon is mainly defined by a few quasi periodical modes whose time evolution is determined by global climate states the new model also estimates narrower standard error bounds compared with the estimations made by the dhr and the additive model fig 4 with regard to the comparison with the dhr technique the better predictive ability and reduced error bounds in our model when considered together are reminiscent of causality in the sense of granger 2001 because the output variable is predicted both more accurately and more reliably by external states than by historical rainfall values 4 4 predictive ability under data limitations and longer time lag effects as explained before four k values k 3 4 6 12 along with twelve sequential lags l 1 to 12 were experimented for training the models the resulting forty eight k fold cross validation experiments in each rainfall signal were statistically evaluated and their results are summarized in fig 5 a discussion is given in the following paragraphs to start with fig 5 a b and c show the kge statistics during the training and the out of sample predictions for different partitions of data k using a lag of one l 1 on its sdp functions these figures suggest that our model can deal with data scarcity because there is no significant change in the statistics during the training and the out of sample predictions nonetheless since the model is data based larger data sets encoding more information are likely to increase the reliability of the model it is also interesting to study the performance of our model when different lagged values for global climate variables are included fig 5 d e and f for instance although for all the cases the degradation of predictive ability with respect to lagged information is observed these tendencies are not monotonic on the one hand the experiments made for peñas m217 and labrado m141 signals show an increasing trending for the first three lagged values see fig 5 d and f on the other hand for the jacarin m197 the worst predictive performance is observed when we give a lag of two months fig 5 e this reveals the possibility of improving the model s predictivity by the combination of different lags for the state variables however this is an open task to be tackled in future research 4 5 non parametric sdp functions unveiling interactions non parametric sdp functions can be further analyzed in order to explore the possibility to unveil climate interactions between global and local scales for instance fig 6 shows some of the non parametric sdp estimations for the labrado m141 trend model see equation 6b in that figure the global signals of tna car and soi have different patterns such behaviours indeed mean that global climate states exert different effects over the local rainfall trend to clarify this idea we now discuss some particular cases found for the mentioned rainfall signal on the one hand the monotonic direct relations exhibited by the function in fig 6 a implies that the higher the tna state the greater the local rainfall trend on the other hand the extreme values of the concave and convex functions exhibited by car and tni entail ranges over which the influence of those climate variables cause the values of rainfall trends to switch from increasing to decreasing stages and vice versa thus the analysis of these functions makes it possible to determine particular global climate states that are useful to anticipate certain rainfall trend conditions additionally we can also analyze the sensitivity of the influence that each global climate signal has on the local rainfall trend for example car does not exert the same level of stimulus over rainfall trend as that provided by tna and soi because its range variability is the narrowest i e from 13 mm to 16 mm approximately the standard error bounds for the non parametric functions shown in fig 6 a b and c are inherently estimated by the sdp technique young 2000 2012 the band widths are similarly thin for the functions of tna car and soi climate signals which supports the significant role played by these climate variables into the model nonetheless it is worth mentioning significative wider bounds would imply identifiability problems which might have implications regarding the significance of the state variable and its role in the model thus care must be taken in relation with the non parametric seasonal functions fig 7 a and c show the sdp estimations for the twelve month seasonal parameters a 1 and b 1 in equation 6b related with the tna climate variable after that the corresponding tna s twelve month amplitude and phase were calculated following equation 7 and the results are shown in fig 7 b and d a discussion about these figures is presented in the following paragraphs on the one hand the amplitude function fig 7 b shows a minimum somewhere in the domain between 0 and 0 25 since the amplitude is related to the energy in a wave that minimum could entail a minimum state of energy in the rainfall process connected with tna states on the other hand the phase function fig 7 d shows a severe change around the interval previously mentioned which implies a delay or advance of the seasonality depending on the tna state these interesting interactions involving amplitude and energy as well as phase and seasonality show the potentialities of the technique for unveiling relations with possible physical properties and conceptual effects furthermore the phase and amplitude could be used as an alternative way to the more conventional suggestions for analyzing the seasonal replicability as the one suggested by walsh and lawler 1981 for example fig 7 a and c also show the standard error bounds for the non parametric functions the limits in fig 7 b and d correspond to the upper and lower variations which were estimated using the higher and lower bounds of the corresponding non parametric functions as expected the bounds are wider where the observations of state variables are less frequent therefore the nonlinear information estimated by sdp must be carefully treated when analyzing the extremes due to the increment of standard error bounds it is worth mentioning that while the confidence bounds in fig 7 a and c are rigorously estimated through application of the kf fis algorithm the bounds of fig 7 b and d are proxy s containing information of the variations of amplitude and phase variables in summary the nonlinear features of sdp functions might encode climate information which is potentially useful for improving our understanding of the effects that global climate states have on the local rainfall trend and seasonality furthermore the analysis of the sdp functions could help as a complementary technique for enhancing other climate studies in the area involving similar descriptions between different spatial scales for instance it has been argued that a significative portion of rain in the studied region is traced from the caribbean zone esquivel hernández et al 2019 in turn the atlantic warm pool awp has an influence over mass transportation from the caribbean wang and lee 2007 interestingly tna tsa and car located in the same awp areas are the global signals handling the rainfall trends in this region wang et al 2007 furthermore amo nao and enso signals complement the set of rainfall drivers annex 1 which are the same climate indices that strongly interact with the awp area in previous literature i e interact with tna tsa and car sorí et al 2015 campozano et al 2018 these are some examples of the potential interactions that our research could have with other climate studies 5 conclusions a novel modelling procedure combining the well known dynamic harmonic regressions dhr and the state dependent parameter sdp techniques is proposed in this work in the first stage the rainfall signals are decomposed into their trends and quasi periodicities by dhr the fundamental and main frequencies as well as the decomposed signals are useful for the construction and identification of a harmonic model with seasonal and trend parameters defined by state variables functions in the second stage the non parametric functions are estimated through the state dependent parameter sdp algorithm as a result an improved and efficient model based on exogenous variables as the only inputs is built the proposed model was tested on local monthly rainfall signals belonging to the andean complex climate areas the exogenous state variables constitute typical climate signals from the atlantic and the pacific regions tsa tna tni car enso and other climate signals were included as state variables the non parametric functions estimated through sdp encode particular features which could lead to understanding global climate states with significant incidence in the local climate this could be useful for unveiling combinations of global climate states driving local climate anomalies regarding the model s predictivity it reveals itself as a useful tool for forecasting interpolation purposes which performs better than its related dhr model and another additive sdp model when one step ahead predictions are compared furthermore the model is able to provide larger future predictions beyond just one month ahead and is robust with respect to data limitations nonetheless possible improvements for the model might be attained by exploring the following ideas 1 including local or regional climate variables into the model as well as other complementary stochastic techniques 2 considering different lags of global information driving the rainfall components the proposed modelling technique could be beneficial for agriculture engineering and environmental issues in the rainfall and climate context it opens new opportunities for further research such as the possibility of coupling it with gcm s for interpolating large global climate fluctuations to local climate effects this could be useful for complex climate regions where limitations to represent climate processes exist as for the case of the andean mountain system and its convective activity funding this work is supported by the research department of the university of cuenca diuc through its research project named description and modelling of rain through global climate information declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this manuscript is an outcome of the doctoral program in water resources offered by universidad de cuenca escuela politécnica nacional and universidad técnica particular de loja the authors acknowledge the valuable comments of dr lenin campozano also the authors are grateful to dr daniela ballari as well as to the department of water resources and environmental sciences of university of cuenca for including this work within scientific discussion spaces appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104786 annexe 1 final gcs variables included in the model specified by components peñas m217 jacarin m197 labrado m141 trend 12 m 6 m 4 m trend 12 m 6 m 4 m trend 12 m 6 m 4 m pdo nao best mei enso 3 4 enso 1 2 enso 3 enso 4 tni qbo soi tna tsa amo car mjo the indicates the global climate signals included in the final model for each component as indicated in equation 6 a and b annexe 2 ensembled out of sample one step ahead predictions from the k fold cross validation process for subsets of 31 one months k 12 image 1 annexe 3 cross correlation of errors and climate global variables inputs in the model image 2 
26004,modelling climate is complex due to multi scale interactions and strong nonlinearities however climate signals are typically quasi periodical and are likely to depend on exogenous variables motivated by this insight we propose a strategy to circumvent modelling complexity based on the following ideas 1 the observed signals can be decomposed into non stationary trends and quasi periodicities through dynamic harmonic regressions dhr 2 the main frequencies and decomposed signals can be used for constructing a harmonic model with varying parameters depending on exogenous variables 3 the state dependent parameter sdp technique allows for the dynamical estimation of these parameters the resulting dhr sdp combined approach is applied to rainfall monthly modelling using global climate signals as exogenous variables as a result 1 the model yields better predictions than standard alternative techniques 2 the model is robust regarding data limitations and useful for several steps ahead forecasting 3 interesting relations between global climate states and the local rainfall s seasonality are obtained from the sdp estimated functions keywords dynamic harmonic regressions state dependent parameters monthly rainfall trends quasi periodicities 1 introduction in general deterministic models describe natural phenomena through partial or ordinary differential equations which inherently involve several modes and interactions through different scales beven 2010 often lower scales imply higher complexity and are sometimes parameterized under mass energy and momentum restrictions so that they can be coupled into the general differential equations of higher scales however these techniques frequently entail a large number of parameters and variables which are difficult to measure estimate or optimize neelin et al 2010 prominent cases of what we have stated above are the global climate models gcm s and the dynamical downscaling techniques those techniques link the climate simulations at different spatial scales assuming some particular parameterizations according to the regional case hewitson and crane 1996 however in some regions the natural high complexity of climate often entails an inability to adequately parameterize the climate conditions producing serious inaccuracies and uncertainties for regional climate simulations moncrieff et al 2007 an example is the case of climate modeling in tropical andean areas and its intense convective activity lin et al 2006 buytaert et al 2009 ochoa et al 2016 therefore reduced order dynamical models or other simplified options incorporating some information of the main modes of the phenomena are needed for practical purposes a possible answer to conceive simpler and reliable alternatives to complex deterministic models may come from specific evidence arguing that environmental phenomena are often dominated by a reduced number of modes raper 1996 young and ratto 2011 geoffroy et al 2013 young 2018 this is reminiscent of reduced order models that describe complex phenomena by considering the most essential information however the question of how to determine what information is crucial and how to systematize it into a meaningful model is a scientifical challenge for certain climate signals the previous issues can be tackled by identifying typical climate features and patterns through different scales for instance global climate indicators synthesize a considerable part of the worldwide climate variability and are commonly used for unveiling climate interactions at different scales teleconnections glantz et al 1991 also typical local climate signals obey non stationary trends and quasi periodicities which could be hypothesized as linked and controlled by global indicators thus it seems necessary to identify these signals components first after that the global climate information must be related to those components following an appropriate mathematical framework the dynamic harmonic regression dhr is a flexible tool for identifying the non stationary trends and quasi periodical components of a climate signal mindham and tych 2019 khelifi et al 2018 young 1998 2018 trapero et al 2015 young et al 1991 into the standard dhr model the temporal variability of the parameters is determined by generalized random walk grw processes which includes several random walk types of which the most important are the random walk rw integrated random walk irw and smoothed random walk srw young et al 1999 young 2012 this suggests the idea of including the global information into the dhr s structure by replacing these grw processes with functions dependent on global climate states playing the role of exogenous variables employing the state dependent parameter sdp technique then for constructing a climate model the present work proposes a methodology that combines the dhr and the sdp techniques on the one hand dhr will allow the identification of the fundamental harmonics and trend here also named components for a climate signal on the other hand the sdp technique will determine non parametric functions of global climate signals explaining the parameters variation used for modelling such components in other words global states play the role of exogenous variables handling both the harmonics and the trend through their corresponding variable coefficients the first step of the modelling process assumes some prior knowledge about the leading global signals interacting within a specific region however for complex climate regions climate interaction with global phenomena could be diverse and complex therefore pursuing parsimony an identification of the most appropriate global signals explaining the variation of the model s parameters is suggested such identification can be reached through statistical selection from the combinations of the exogenous variables conforming to sdp nonetheless to avoid the explosion of combinations the harmonic and trend signals which are products of dhr s decomposition are modelled separately in the proposed approach testing the spd s combinations of global variables once the global variables are identified for each component the sdp technique is applied over the final ensemble model three monthly rainfall signals belonging to an andean tropical climate region are modelled for testing the model s capabilities this andean tropical region is characterized by strong climate variability which is reflected by the presence of very diverse rainfall regimes these complex climate conditions make this region a challenging area for rainfall climate modeling therefore it can be regarded as a natural laboratory to test the proposed technique a rigorous specific out of sample procedure will be applied to evaluate the model s performance additionally the model is calibrated under different amounts of global and rainfall information to show the model s ability to deal with data scarcity finally modeling tests including diverse lagged global information will provide evidence about the forecasting skills of the model in order to explore some salient features that come from the integration of techniques the standard univariate dhr and an alternative sdp model are used for comparison purposes in summary this work proposes a novel model strategy resulting from the combination of the dhr and the sdp techniques dhr determines the model s structure with its fundamental harmonics and trend components sdp identifies their functional dependence on global climate signals the model s performance is assessed through an out of sample procedure and comparison with the univariate standard dhr and an alternative sdp model furthermore the non parametric functions given by the sdp non parametric functional forms is analyzed searching for possible climate mechanisms 2 materials 2 1 study area and rainfall description the paute river basin fig 1 is located in the tropical south ecuadorian andean depression coltorti and ollier 2000 the basin has an area around 6481 km2 with elevations ranging between 900 and 4200 m a s l the basin supports several ecosystems that are regarded as water providers due to its hydrological features such as the neotropical alpine wetland ecosystem célleri and feyen 2009 due to its climate characteristics the basin supplies a significant part of hydropower generation in the ecuadorian region nearly 1100 mwh of its energy furthermore its tropical mountain location provides to the area with interesting rainfall variability schneider et al 2014 cobb et al 2003 for instance three main rainfall regimes have been characterized according to a rainfall distribution analysis inside the paute basin the first of them a unimodal um rainfall regime concentrates the rainfall amount mainly in the months of june july august jja the second a bimodal bm rainfall regime correspond respectively to the seasonal periods of march april may mam and september october november son finally a trimodal tm rainfall regime is identified during the seasons of mam jja son celleri et al 2007 campozano et al 2016 in turn the rainfall regimes in the paute basin have been linked with global climate conditions for example mora and willems 2012 conclude that pacific and atlantic ocean patterns have a strong influence at decadal time scales over such regimes on the one hand enso mainly exerts a negative influence over the um areas on the other hand tropical south atlantic tsa patterns have a positive influence over the um eastern regions which coincides with buytaert et al 2006 for the region and by vuille et al 2000 in a larger scale inside the andean eastern cordillera significant findings are also reported by campozano et al 2018 in a mesoscale analysis focused on the paute basin in that study essential effects and interactions between enso trans niño index tni and the tropical south atlantic tsa are revealed these interactions are also supported by the findings reported in mendoza et al 2018 through a teleconnection analysis over decomposed rainfall signals therefore since the climate conditions of the paute basin are complex by nature it makes it an interesting natural laboratory and a strategic area for testing new and more advanced mathematical approximations for climate modelling 2 2 data three monthly data sets fig 1 provided by the ecuadorian national institute of meteorology inamhi were considered for the modelling purposes set herein due to data limitations the period considered corresponds to 30 years january 1980 december 2010 nonetheless the chosen period ensures the maximum continuity of data in the three localities gaps in data represent less than 7 of the whole series for each data set linear regressions with other neighbouring stations in the studied region fig 1 were applied for data filling villazón and willems 2010 in that respect the regressions with the highest correlations coefficients were used to estimate the gaps furthermore the regressions were applied for each month i e data for a specific month was estimated with data from other locations corresponding to the same month the identification and other features of the three chosen data sets are given in table 1 monthly global weather patterns were obtained from the national oceanic and atmospheric administration noaa for this study we include the next data sets with the same period of rainfall data extent pacific decadal oscillations pdo north atlantic oscillation nao bivariate enso time series best multivariate enso index mei enso 3 4 enso 3 enso 4 enso 1 2 trans niño index tni quasi biennial oscillation qbo southern oscillation index soi tropical northern atlantic index tna tropical southern atlantic index tsa atlantic multidecadal oscillation amo caribbean index car and madden julian oscillation mjo these signals synthesize a large part of climate variability of the atlantic and pacific oceans which surround the studied region 3 methods two main steps integrate the precipitation modelling approach based on global information presented herein first the precipitation signal is decomposed using the dynamic harmonic regressions framework then the functional relationship between the variable coefficients of the dynamic harmonic components and the global climate signals is identified using the state dependent parameter procedure these steps are described below however in order to have the necessary context to better understand this description it is important to start by reviewing the dhr and sdp frameworks 3 1 dynamic harmonic regressions dhr this stochastic framework is built upon the recursive time series analysis theory introducing the concept of time variable parameters tvp s such parameter variation time evolution is determined from data by kalman filtering techniques in linear models obeying gauss markov random walk processes the details of these techniques are better explained in the specialized literature young et al 1999 young 2000 2012 bryson 2018 and implemented in the matlab captain toolbox available at http wp lancs ac uk captaintoolbox taylor et al 2007 nonetheless since the main interest here is to understand the role of this framework for the proposed methodology we will treat it briefly starting with the following general additive model 1 y k t k s k e k e k n 0 σ 2 where y k represents the output variable observation while t k s k and e k are respectively the trend seasonal and error terms in the k th time step furthermore we can state that 2 t k s k i 0 r s a i k cos w i k b i k sin w i k a 0 k t k w h e n w 0 0 equation 2 is just a specification of equation 1 in which the trend t component is interpreted spectrally as a zero frequency term the symbols a i k and b i k represent tvps for the i th seasonality each associated to a frequency w i the fundamental and harmonic frequencies w i are estimated from the fourier transform of an autoregressive ar model s signal also known as observed spectrum of order n which is determined according to the well known akaike coefficient aic akaike 1974 1977 beamish and priestley 1981 on the other hand the state vector x i k l i k d i k t defines the evolution of the tvp a i b i parameters through a generalized random walk grw process 3 3 x i k f i x i k 1 g i η i k 1 f i α i β i 0 γ i g i δ i 0 0 ε i the vector η i k η1 i k η2 i k t is also a two dimensional zero mean vector with noise characterized by a diagonal covariance matrix q η i each α β γ δ ε as well as the elements of q η i or usually the nvr matrix q nvr q η i σ2 are the so called hyperparameters which are time invariant and are estimated from data through optimization such optimization fits the pseudo spectrum of dhr fourier transform of equation 1 to the mentioned observed spectrum once the hyper parameters are obtained the well known kalman filter and the fixed interval smoothing kf fis technique is applied over the complete tvp regression model 4a y k h k t x k o b s e r v a t i o n e q u a t i o n 4b x k f x k 1 g η k 1 s t a t e e q u a t i o n where x k of dimension p 4rc 2 is the vector arrangement of all x i k vectors while h k is the so called observation vector with a value of one for the first element while the other components correspond to the i th cos w i k and sin w i k values for the corresponding k th time step in fact alternating zeros at the even positions in h k are included in order to attain the needed dimension for the x k vector notice that equation 4a is just a compact restatement of equation 1 in addition f and g are the total block matrices of dimension p x p with blocks defined by the sub matrices f i and g i young et al 1999 peter c young 2012 3 2 state dependent parameter sdp one step further towards developing the methodology proposed here is to consider the possibility of treating the parameters a i and b i in equation 2 as functions of some state variables in that case the state dependent parameter sdp algorithm is able to provide a framework to identify the corresponding functions using kf fis techniques briefly the sdp algorithm is based on redefining the expression for the output variable y in equation 4a making it depend on a state variable χ of the system this entails changing from time space to state space and results in a smooth evolution for y in terms of χ which facilitates the application of the kf fis and grw techniques given that changes are now less abrupt as a consequence the grw evolution for the parameters in the model is characterized by non parametric functions of the state variables χ rather than being directly linked to time k furthermore the sdp algorithm can be adapted to define the evolution of the a i and b i parameters represented conveniently by x i into the following mathematical model 5a y k h k t ρ k e k 5b ρ k x 1 χ k x 2 χ k x 3 χ k x p χ k t 5c x i χ k s 1 m i ϑ s i χ s k for i 1 2 3 p vector χ represents the collection of state variables on which the parameters x i are dependent the additive function expressions in equation 5c are linearly combined into the observation equation 5a through the components of vector h k facilitating the application of the sdp algorithm in matlab captain toolbox available at http wp lancs ac uk captaintoolbox taylor et al 2007 it is worth mentioning that considering the specific harmonic form in equation 2 the h k vector components include for the matrix operation in equation 5a to make sense the repetition of m 1 times the value of one 1 and m i times the cos w i k and sin w i k values regarding the identification of the non parametric functions in equation 5c it is attained through the well known back fitting algorithm and the so called modified dependent variables mdv finally the non parametric functions are parameterized through a proper curve fitting technique expressing the functions in a concise form for the sake of brevity the details of the complete sdp algorithm and its mdvs are omitted here the reader is referred to the specialized literature for further information young 2000 2012 young et al 2001 3 3 modelling process combining the dhr and sdp frameworks now after laying out the necessary context we proceed to explain the details of our methodology from and overall perspective it is defined by the following two steps 1 frequency identification and signal decomposition using dhr 2 model construction and sdp application 3 3 1 frequency identification and signal decomposition using dhr to begin with dhr is applied over the time series of interest two outputs of interest are obtained after this process first the frequencies obtained from the observed spectrum second the non stationary decomposed components of the signal on the one hand the frequencies w i will be used for constructing the observation vector h k at each k th time step on the other hand the decomposed signals will help to tackle the technical problem of identifying proper state variables on which the model s parameters are dependent 3 3 2 model construction and sdp application first we assume a harmonic model see equation 6a below with parameters determined by state variables additive functions see equation 6b below as said before in the model each w i frequency comes from the empirical spectrum estimated by the application of dhr notice that the model meets the required linear coupled form in equations 5a and 5c which is convenient for applying the sdp technique 6a y k t χ k i 1 r s a i χ k cos w i k b i χ k sin w i k e k 6b t χ k j 1 n f j χ j k l j a i χ k s 1 m i ϑ s i χ s k l s i b i χ k s 1 m i ϕ s i χ s k l s i the inclusion of a lag l into equation 6b makes the sdp parameters estimates for the k th time step depended on the k l th time step information of the state variables thus the k th time step estimation of y made by the model 6a are calculated using the specific k l th state variables values we refer the latter property as an indicative of the predictive model s ability herein which can be used for both interpolations and future estimations of the observed variable in the model the trend component t is an additive function of n state variables while each pair of parameters a i b i is composed by the same m i state variables this condition is not mandatory although it seems convenient since each pair of ϑ s i and ϕ s i functions could be regarded as individual signals assembling the i th harmonic component therefore each i th harmonic component can be fragmented into m i signals with its non stationary amplitudes a and phases φ as functions of specific state variables 7a a s i k ϑ s i χ s k l s i 2 ϕ s i χ s k l s i 2 1 2 7b φ s i k tan 1 ϑ s i χ s k l s i ϕ s i χ s k l s i for s 1 2 3 m i these individual amplitudes and phases could entail some advantages in the analysis of how each state variable determines its influence over a specific i th seasonal component however these amplitudes and phases cannot be considered additive with respect to the overall amplitude and phase a i and ϕi of a specific i th harmonic component because they do not have a linear form the modelling process is almost complete when the state variables are known for each specific signal to be modelled the next step is to estimate the non parametric functions f ϑ ϕ through the sdp technique finally for the sake of simplicity each of these functions is fitted to some parametric form or some other rational alternative as the one considered here for the application nonetheless in complex processes involving several state variables without necessarily playing significant roles in the context of this particular modelling technique the question of an efficient and parsimonious model becomes a technical problem the latter could be tackled through the statistical searching of the best model among all possible combinations of state variables included in it in an attempt for avoiding the explosion of combinations in the complete model see eq 6 we suggest an sdp modelling for the trend and each i th harmonic in equation 6b separately using the dhr s decomposed signals as surrogate observations for fitting these models after we select the best combination of state variables to be included in each component and we ensembled the final model with the form of equation 6a 3 4 the application in the context of rainfall modelling and evaluation in previous research by some of the authors of this paper global climate variables were linked to the trend and amplitudes of decomposed rainfall signals through a methodology proposed by mendoza et al 2018 summarized in annex 1 we use this process as a starting point that enables us to have an educated guess for the state variables global climate signals that should be considered for the functions in equation 6b since global variables were smoothed in mendoza et al 2018 for reasons exposed therein these are the state variables used herein a lag of one l 1 is given to all the functions of state variables into the model as the required minimum for forecasting tasks for the selection process the coefficient of determination r2 is used to assess the selection of the best individual trend and harmonic models including r 1 2 3 n mi state variables on the other hand the akaike coefficient aic is used to measure parsimony choosing the best model for trend as well as for each seasonal component the complete model is then constructed by assembling the selected state variables identified from each of the components and finally applying the sdp algorithm to evaluate performance the k fold cross validation process a particular out of sample evaluation technique is applied elsner and schmertmann 1994 this technique partitions the data set into k equal sized subsamples avoiding overlapping thus one subsample is retained for evaluation out of sample predictions while the rest of the data is available for training the model from here we get k different trained data sets and the same amount of out of sample predicted data sets the trained and predicted data sets are statistically analyzed using the kling gupta efficiency kge parameter which synthesizes the correlation coefficient r the bias ratio β and the variability ratio γ in one single metric kling et al 2012 using a single lag l 1 into the model implies that each set used for out of sample prediction in the cross validation process delivers a one step ahead prediction however it is worth mentioning that out of sample predictions are instruments used for evaluation of the model into the k fold cross validation process and should not be considered as a forecasting representation ability of the model notice that the previous predictions are analogous to the dhr s one step ahead forecasting estimations so that it is reasonable to compare them in order to contrast the overall predictive capacity of the proposed model additionally we use an additive sdp technique for contrasting purposes the model follows equations 5 a and b considering each parameter x i as a function of one state variable and h k is a vector of ones for each time step k all the climate signals described in the data section are used as state variables and we give a minimum lag of one l 1 for all the sdp functions the same k fold cross process with a k 12 will generate the training and the out of sample predictions sets for comparison of the general predictivity finally to evaluate the model s tolerance to data limitations and the possible degradation of forecasting effect against a progressively lagging increase we replicate the cross validation process for different values of k along with different lag values l thus if we consider k 12 6 4 and 3 and a lag sequence set from l 1 to l 12 we obtain four experiments for each lag parameter i e 48 experiments in total for the sake of simplicity the statistics of all these tests will be graphically summarized and analyzed the confidence bounds of the models corresponding to one standard deviation attained through the kf fis algorithm are analyzed for the models young et al 2001 young 2012 the error s assumptions on dhr are statistically evaluated using the shapiro wilk breusch pagan and durbin watson breusch and pagan 1979 farebrother 1980 royston 1982 for the proposed model a cross correlation diagnostic between errors and input variables is likely to furnish greater confidence in the overall results and therefore we perform it here 3 5 important considerations and comments about the rainfall application the model does not include explicitly any local or regional climate information it could be incorporated for example through a stochastic component driven by regional climate variables this is an exciting possible extension of our modelling approach for the future for simplicity linear interpolations between two consecutive points of the non parametric functions are used as surrogates for parameterizations in the proposed and the additive models when extrapolations are necessary the models assume the functions values corresponding to either the maximum or the minimum of the state variables domains this circumvents the need for any assumptions beyond what was inferred during training the reason why we refrain from hypothesizing a parametrical form is that as opposed to other applications we do not have any a priori knowledge of a functional form having some conceptual or physical meaning in fact we believe that one of the contributions of this work is to open the possibility to explore the physical meaning of the non parametrical state functions that were obtained smoothing global signals into the model s functions provide the model with an attenuation effect against extreme anomalies of the global climate thus the model reacts under persistent global climate variations rather than fast fluctuations this consideration could entail some meaning in the sense that high frequencies in climate are indeed attenuated through distance the smoothing effect also attenuates the noise in the measured global climate variables thus mitigating the bias effect of sdp non parametric estimations to simplify the model form and its sate variable identification we use the same lag parameter for all the sdp functions nonetheless the authors are aware that better and more interesting models could be found with other lag combinations unobserved component models as reference techniques for comparisons could be used however to the authors opinion this task is beyond the scope of this work mainly because of the following reasons o previous research identified climate connections between the global and rainfall decomposed signals specifically for trends and harmonics amplitudes these connections were statistically established and conceptually validated which supports the application here mendoza et al 2018 an alternative uc model could require the same process otherwise it could suffer from identifiability problems o the proposed model here is purely dependent on exogenous variables which allows the analysis of possible incidences that global climate states could have on trends amplitudes and phases for a local signal no information from the history of the local variable is used as an input for the model 4 results and discussion 4 1 dhr signal decomposition besides the trend component three main frequencies mostly describe the rainfall s observed spectrum as shown in table 2 these frequencies represent the seasonal regimes in the studied area celleri et al 2007 for instance in fig 2 a the seasonality of peñas m217 shows a dominant frequency on its power spectrum 12 month period which is reasonable since the rainfall signal belongs the um region inside the paute basin similarly the stations of jacarin m197 and labrado m141 show two and three dominant frequencies which agreed with the bm and tm regimes respectively fig 2 b c for the three rainfall signals modelled by the dhr the smoothed random walk process srw 0 α 1 β γ ε 1 δ 0 and the first order autoregressive random walk process ar 1 0 α 1 β γ ε 0 δ 1 determine the evolution for trends and all seasonal parameters a i b i of equation 2 these srw and ar 1 processes are special grw s cases see equation 3 and provide the best matching between the observed spectrum and the pseudo spectrum as shown in fig 2 table 2 reports the hyper parameters optimization α n v r as well as their scores and standard errors the dhr performance is statistically summarized in table 3 notice that since our main objective is to measure the dynamical performance of dhr in replicating the rainfall signals the well known non parametric kendall tau test is used for measuring the ordinal association between observed and simulated values abdi 2007 kendall 1938 the peñas m217 station reached a minimum of 0 77 which means an acceptable performance in the simulation recursive algorithms for dhr modelling require normal distributed homoscedastic and white noise residuals which are evaluated by the sh bp and dw tests respectively normality is not fulfilled for the labrado m141 station according to the sh test all three cases show non homoscedastic and uncorrelated residuals according to bp and dw tests these results have implications on statistical inference nonetheless since dhr modelling is used for signal decomposition the results are accepted given the purposes set herein 4 2 the monthly rainfall modelling and evaluation the dhr decomposed signals were used for training the sdp models in equation 6b after that we selected a model for each of the state variables in it i e the best model considering j 1 2 and s 1 2 in equation 6b the selection is based on the coefficient of determination r 2 on the one hand an increasing pattern is shown when the r2 values are plotted according to the number of state variables on the other hand the aic parameter shows an optimum a minimum for a specific number of state variables see fig 3 the above results indicate that each time we choose a component model with a specific set of r state variables in it i e r 1 2 3 n mi the next set containing r 1 state variables provides the model with a better representation of its variability nonetheless there is an adequate number of state variables with respect to the criterion of parsimony annex 1 summarizes the global climate variables included in the model s components which are used for assembling the complete model structure i e the model in equation 6a with respect to the evaluation of the models table 4 summarizes the statistical performance obtained from the k fold cross validation using k 12 for data partitioning we analyze the kge average and its standard deviation from the twelve tests for describing the model s predictive ability during the training process labrado m141 shows the best kge average value among the three models 0 81 however for the out of sample predictions the kge average falls to 0 59 the reasons for such a loss rely mainly on the deficient ability of the model to capture the signal s variability as shown by the r coefficient fig 4 h shows accurate predictions for the last out of sample evaluated subset however kge s standard deviation indicates difficulties regarding generalization as confirmed by the ensembled out of sample prediction subsets in annex 2 c these limitations are probably related to the complex rainfall regime to which it belongs celleri et al 2007 campozano et al 2016 regarding the jacarin m197 signal the model does not perform high in kge terms during the training process and fig 4 e shows deficient out of sample predictions for the last evaluated subset however the predictivity is generally acceptable for the remaining data subsets being assessed which is statistically shown in table 4 the model is able to capture the signal s dynamic r coefficient but is deficient in representing the variability γ coefficient the latter is supported by the ensemble results shown in annex 2 b in which the seasonality is well modelled but not the high extremes the jacarin m197 signal belongs to the bm rainfall regime which is characterized by a marked seasonality and complex convective mechanisms related with extreme events celleri et al 2007 campozano et al 2016 2018 then the performance of the model could be linked to the described climate features interestingly the convective activity is significatively influenced by tni and tna campozano et al 2018 which are part of the global signals driving the rainfall model here see annexe 1 finally the rainfall modelling for peñas m217 is relatively the one with the lowest kge performance during the training sets as well as for the out of sample predictions table 4 the model shows limitations for capturing the dynamic and the variability of the signal i e lower r and γ values however the out of sample predictions have a stable performance through the differently evaluated data subsets as indicated by its narrower kge s standard deviation in table 4 the latter is supported by fig 4 b and annex 2 a peñas m217 belongs to the um regime of the paute basin where its seasonality concentrates the rain mainly in one period inside the year celleri et al 2007 further the rainfall has its origins from the amazon s water vapour which is transported by the easterly winds that finally triggers advective processes when forced to rise through the mountains campozano et al 2016 in turn these easterly winds are influenced by the atlantic climate states tsa and tna which are the same global climate signals included in the rainfall trends and its 12 monthly periodical model s component annex 3 contains the results of cross correlation tests between errors and exogenous input variables for the models in general no significant traces indicating under modelling are observed these tests increase our confidence in the overall modelling results 4 3 contrasting the new model with related alternatives in the case of dhr s modelling the fitted and one step ahead predictions for each rainfall signal were obtained from a single kf fis process employing the complete set of observations although no cross evaluation method was applied for the dhr modelling the results were statistically systematized separately for each of the data subsets used for cross evaluation of the model proposed herein from this comparisons between these two models were performed and the results are detailed in table 4 according to the kge coefficient table 4 the fitted values reached by the dhr model during the training phase outperform the fitted rainfall values attained by our model for all the modelled signals specifically the correlation coefficient r suggests that dhr s fitting represents the history of rainfall observations better this supports the idea of using dhr s frequencies and decomposed signals to construct our model because they reproduce the rainfall signals accurately however when one step ahead predictions are considered our model renders results that outperform those attained by the dhr model as indicated by the kge coefficient in table 4 within the context of this work one step ahead predictions are used as a measure of the predictive ability of the models which is meaningful in a very practical sense with the proper global climate information available at a given month a potential user of the dhr sdp technique would be able to obtain an improved prediction of the subsequent monthly rainfall value with respect to that estimated by the dhr model alone regarding the alternative model for comparison as shown in table 4 for all the modelled signals our proposed model performs statistically better than the additive model during the training process as well as for the out of sample predictions given that an alternative sdp model does not have the same predictive effectiveness as the proposed model it can be said that feeding harmonic inputs to sdp functions is an adequate strategy for modelling monthly rainfall signals definitely one can hypothesize that the phenomenon is mainly defined by a few quasi periodical modes whose time evolution is determined by global climate states the new model also estimates narrower standard error bounds compared with the estimations made by the dhr and the additive model fig 4 with regard to the comparison with the dhr technique the better predictive ability and reduced error bounds in our model when considered together are reminiscent of causality in the sense of granger 2001 because the output variable is predicted both more accurately and more reliably by external states than by historical rainfall values 4 4 predictive ability under data limitations and longer time lag effects as explained before four k values k 3 4 6 12 along with twelve sequential lags l 1 to 12 were experimented for training the models the resulting forty eight k fold cross validation experiments in each rainfall signal were statistically evaluated and their results are summarized in fig 5 a discussion is given in the following paragraphs to start with fig 5 a b and c show the kge statistics during the training and the out of sample predictions for different partitions of data k using a lag of one l 1 on its sdp functions these figures suggest that our model can deal with data scarcity because there is no significant change in the statistics during the training and the out of sample predictions nonetheless since the model is data based larger data sets encoding more information are likely to increase the reliability of the model it is also interesting to study the performance of our model when different lagged values for global climate variables are included fig 5 d e and f for instance although for all the cases the degradation of predictive ability with respect to lagged information is observed these tendencies are not monotonic on the one hand the experiments made for peñas m217 and labrado m141 signals show an increasing trending for the first three lagged values see fig 5 d and f on the other hand for the jacarin m197 the worst predictive performance is observed when we give a lag of two months fig 5 e this reveals the possibility of improving the model s predictivity by the combination of different lags for the state variables however this is an open task to be tackled in future research 4 5 non parametric sdp functions unveiling interactions non parametric sdp functions can be further analyzed in order to explore the possibility to unveil climate interactions between global and local scales for instance fig 6 shows some of the non parametric sdp estimations for the labrado m141 trend model see equation 6b in that figure the global signals of tna car and soi have different patterns such behaviours indeed mean that global climate states exert different effects over the local rainfall trend to clarify this idea we now discuss some particular cases found for the mentioned rainfall signal on the one hand the monotonic direct relations exhibited by the function in fig 6 a implies that the higher the tna state the greater the local rainfall trend on the other hand the extreme values of the concave and convex functions exhibited by car and tni entail ranges over which the influence of those climate variables cause the values of rainfall trends to switch from increasing to decreasing stages and vice versa thus the analysis of these functions makes it possible to determine particular global climate states that are useful to anticipate certain rainfall trend conditions additionally we can also analyze the sensitivity of the influence that each global climate signal has on the local rainfall trend for example car does not exert the same level of stimulus over rainfall trend as that provided by tna and soi because its range variability is the narrowest i e from 13 mm to 16 mm approximately the standard error bounds for the non parametric functions shown in fig 6 a b and c are inherently estimated by the sdp technique young 2000 2012 the band widths are similarly thin for the functions of tna car and soi climate signals which supports the significant role played by these climate variables into the model nonetheless it is worth mentioning significative wider bounds would imply identifiability problems which might have implications regarding the significance of the state variable and its role in the model thus care must be taken in relation with the non parametric seasonal functions fig 7 a and c show the sdp estimations for the twelve month seasonal parameters a 1 and b 1 in equation 6b related with the tna climate variable after that the corresponding tna s twelve month amplitude and phase were calculated following equation 7 and the results are shown in fig 7 b and d a discussion about these figures is presented in the following paragraphs on the one hand the amplitude function fig 7 b shows a minimum somewhere in the domain between 0 and 0 25 since the amplitude is related to the energy in a wave that minimum could entail a minimum state of energy in the rainfall process connected with tna states on the other hand the phase function fig 7 d shows a severe change around the interval previously mentioned which implies a delay or advance of the seasonality depending on the tna state these interesting interactions involving amplitude and energy as well as phase and seasonality show the potentialities of the technique for unveiling relations with possible physical properties and conceptual effects furthermore the phase and amplitude could be used as an alternative way to the more conventional suggestions for analyzing the seasonal replicability as the one suggested by walsh and lawler 1981 for example fig 7 a and c also show the standard error bounds for the non parametric functions the limits in fig 7 b and d correspond to the upper and lower variations which were estimated using the higher and lower bounds of the corresponding non parametric functions as expected the bounds are wider where the observations of state variables are less frequent therefore the nonlinear information estimated by sdp must be carefully treated when analyzing the extremes due to the increment of standard error bounds it is worth mentioning that while the confidence bounds in fig 7 a and c are rigorously estimated through application of the kf fis algorithm the bounds of fig 7 b and d are proxy s containing information of the variations of amplitude and phase variables in summary the nonlinear features of sdp functions might encode climate information which is potentially useful for improving our understanding of the effects that global climate states have on the local rainfall trend and seasonality furthermore the analysis of the sdp functions could help as a complementary technique for enhancing other climate studies in the area involving similar descriptions between different spatial scales for instance it has been argued that a significative portion of rain in the studied region is traced from the caribbean zone esquivel hernández et al 2019 in turn the atlantic warm pool awp has an influence over mass transportation from the caribbean wang and lee 2007 interestingly tna tsa and car located in the same awp areas are the global signals handling the rainfall trends in this region wang et al 2007 furthermore amo nao and enso signals complement the set of rainfall drivers annex 1 which are the same climate indices that strongly interact with the awp area in previous literature i e interact with tna tsa and car sorí et al 2015 campozano et al 2018 these are some examples of the potential interactions that our research could have with other climate studies 5 conclusions a novel modelling procedure combining the well known dynamic harmonic regressions dhr and the state dependent parameter sdp techniques is proposed in this work in the first stage the rainfall signals are decomposed into their trends and quasi periodicities by dhr the fundamental and main frequencies as well as the decomposed signals are useful for the construction and identification of a harmonic model with seasonal and trend parameters defined by state variables functions in the second stage the non parametric functions are estimated through the state dependent parameter sdp algorithm as a result an improved and efficient model based on exogenous variables as the only inputs is built the proposed model was tested on local monthly rainfall signals belonging to the andean complex climate areas the exogenous state variables constitute typical climate signals from the atlantic and the pacific regions tsa tna tni car enso and other climate signals were included as state variables the non parametric functions estimated through sdp encode particular features which could lead to understanding global climate states with significant incidence in the local climate this could be useful for unveiling combinations of global climate states driving local climate anomalies regarding the model s predictivity it reveals itself as a useful tool for forecasting interpolation purposes which performs better than its related dhr model and another additive sdp model when one step ahead predictions are compared furthermore the model is able to provide larger future predictions beyond just one month ahead and is robust with respect to data limitations nonetheless possible improvements for the model might be attained by exploring the following ideas 1 including local or regional climate variables into the model as well as other complementary stochastic techniques 2 considering different lags of global information driving the rainfall components the proposed modelling technique could be beneficial for agriculture engineering and environmental issues in the rainfall and climate context it opens new opportunities for further research such as the possibility of coupling it with gcm s for interpolating large global climate fluctuations to local climate effects this could be useful for complex climate regions where limitations to represent climate processes exist as for the case of the andean mountain system and its convective activity funding this work is supported by the research department of the university of cuenca diuc through its research project named description and modelling of rain through global climate information declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this manuscript is an outcome of the doctoral program in water resources offered by universidad de cuenca escuela politécnica nacional and universidad técnica particular de loja the authors acknowledge the valuable comments of dr lenin campozano also the authors are grateful to dr daniela ballari as well as to the department of water resources and environmental sciences of university of cuenca for including this work within scientific discussion spaces appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104786 annexe 1 final gcs variables included in the model specified by components peñas m217 jacarin m197 labrado m141 trend 12 m 6 m 4 m trend 12 m 6 m 4 m trend 12 m 6 m 4 m pdo nao best mei enso 3 4 enso 1 2 enso 3 enso 4 tni qbo soi tna tsa amo car mjo the indicates the global climate signals included in the final model for each component as indicated in equation 6 a and b annexe 2 ensembled out of sample one step ahead predictions from the k fold cross validation process for subsets of 31 one months k 12 image 1 annexe 3 cross correlation of errors and climate global variables inputs in the model image 2 
