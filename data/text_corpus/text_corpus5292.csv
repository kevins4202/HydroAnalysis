index,text
26460,statistical modelling has been successfully used to estimate the variations of no2 concentration but employing new modelling techniques can make these estimations far more accurate to do so for the first time in application to spatiotemporal air pollution modelling we employed a soft computing algorithm called adaptive neuro fuzzy inference system anfis to estimate the no2 variations comprehensive data sets were investigated to determine the most effective predictors for the modelling process including land use meteorological satellite and traffic variables we have demonstrated that using selected satellite traffic meteorological and land use predictors in modelling increased the r2 by 21 and decreased the root mean square error rmse by 47 compared with the model only trained by land use and meteorological predictors the anfis model found to have better performance and higher accuracy than the multiple regression model our best model captures 91 of the spatiotemporal variability of monthly mean no2 concentrations at 1 km spatial resolution rmse 1 49 ppb in a selected area of australia keywords no2 satellite data anfis spatiotemporal transport model australia data availability the type and source of the data set considered in this study name of the data set data source developer all websites accessed on april 2016 data format software required data availability omi tropospheric no2 column density molecules 1015 cm2 aura omi tropospheric no2 column density product via nasa giovanni interface http giovanni sci gsfc nasa gov giovanni instance id omil2g hdf netcdffiles arcgis freely available major road psma australia transport and topography product https www psma com au products transport topography esri shape files price depends on the area of interest minor road industrial point source nox emissions australia national pollutant inventory http www npi gov au reporting industry reporting materials xml files microsoft excel r freely available australia population density australian bureau of statistics http www abs gov au ausstats abs nsf mf 1270 0 55 007 pngesri gridgeotiff arcgis australia land use classification australian bureau of statistics http www abs gov au websitedbs censushome nsf home meshblockcounts excel spreadsheets csv files microsoft excel r arcgis elevation u s geological survey https www usgs gov products maps topo maps pnggeotiff arcgis normalized difference vegetation index terrestrial ecosystem research network http www auscover org au node 9 netcdf files temperaturerainfallhumiditysolar exposure australian bureau of meteorology http www bom gov au climate maps tabs maps esri gridgif traffic data department of transport and main roads http www tmr qld gov au travel and transport road and traffic info traffic reports and road conditions esri shape files price depends on the area of interest software availability the following software has been used in this study for statistical analysis spatial data processing map creation and calculating the meteorological and traffic related parameters r v 3 2 3 r foundation for statistical computing vienna austria matlab r2014b mathworks inc natick usa arcgis v 10 2 esri inc redlands usa weather research and forecasting v 3 8 1 powers et al 2008 south east queensland strategic transport model ryan et al 2008 note no specific software component has been developed for this study 1 introduction exposure to ambient air pollution is a major environmental risk factor associated with adverse health effects forouzanfar et al 2015 nitrogen dioxide no2 is a major component of ambient air pollution and a strong marker of traffic related emissions briggs et al 1997 richter et al 2005 to date epidemiological studies have demonstrated that there are adverse health effects associated with exposure to no2 crouse et al 2010 2015 filleul et al 2005 mölter et al 2014 parent et al 2013 perez et al 2012 in addition no2 is recognized as a good proxy of particle number concentration in urban environments grundström et al 2015 hence more precise estimates of no2 concentration is needed to investigate its associated role on health effects fossil fuel combustion including coal gas and oil are the major sources of no2 in australia australian government 2010 as a subset of this traffic related emissions are a major source of no2 in urban areas derwent and hertel 1998 about 80 of the no2 in australian urban areas comes from motor vehicle exhaust australian government 2010 this indicates that traffic flow needs to be carefully investigated for estimating the no2 concentration in australia as one of the most urbanized countries in the world different approaches have been used to provide a proxy for traffic flow including calculating the length of the roads or road classification henderson et al 2007 knibbs et al 2014 sahsuvaroglu et al 2006 and using nearest traffic count dirgawati et al 2015 ducret stich et al 2013 some studies used transportation models to obtain more accurate estimates of traffic flow and this approach has been found to provide better results than previous approaches costabile and allegrini 2008 kim and guldmann 2011 2015 shekarrizfard et al 2015 moreover traffic dynamics can also significantly affect the no2 emissions as congested traffic e g stop and go traffic results in particulate matters and gaseous emissions peak beyond the free flow traffic condition davis and peckham 2007 giakoumis et al 2012 hagena et al 2006 keuken et al 2010 rakopoulos and giakoumis 2009 consequently traffic dynamics and condition plays a significant role in the emission of no2 from vehicles in urban areas and should be investigated during the no2 modelling process estimates of air pollution concentration have been traditionally provided by ground monitoring networks the sparse ground measurement network in many parts of the world including australia makes it difficult to evaluate the spatiotemporal variability of ambient air pollution even a dense network could not adequately monitor the spatiotemporal variability of ambient air pollution since it is changing on scales much smaller than monitoring networks density this represents a significant limitation on evaluating the adverse health effects associated with ambient air pollution satellite imagery is another important tool rapidly gaining interest in air pollution monitoring as it provides sequential observations with extensive spatial coverage kloog et al 2014 data derived from satellite sensors can be combined with ground based measurements at different spatiotemporal scales reis et al 2015 the availability of satellite derived data has helped to overcome the problems associated with sparse monitoring networks by providing observations where previously there were none martin 2008 reis et al 2015 observation based statistical methods have emerged as a powerful tool for exploring the quantitative relationship between ground level no2 concentrations and satellite derived data and a variety of these methods have been used to quantify this relationship bechle et al 2015 carnevale et al 2016 hoek et al 2015 horsburgh et al 2016 hystad et al 2011 knibbs et al 2014 novotny et al 2011 reggente et al 2014 vienneau et al 2013 machine learning refers to computational techniques which are able to achieve optimal solutions for analyzing complicated phenomena at reasonable costs kruse et al 2013 ovaska 2004 in recent years machine learning algorithms including support vector machine svm moazami et al 2016 reid et al 2015 yeganeh et al 2012 bayesian models corani and scanagatta 2016 mcbride et al 2007 k nearest neighbours knn reid et al 2015 and artificial neural network ann agirre basurko et al 2006 al alawi et al 2008 ordieres et al 2005 wu et al 2012 yeganeh et al 2017 have also been gaining popularity because of their high flexibility and proven prediction abilities the literature however indicates that adaptive neuro fuzzy inference system anfis which is accepted as a robust and effective method for multivariate analysis has not been used to date to estimate the spatiotemporal variation of no2 concentrations using satellite based data while some studies have been conducted to investigate the spatial variation of the no2 in australia dirgawati et al 2015 knibbs et al 2014 recent studies have shown that the combination of temporal variables e g satellite based observations and metrological data with temporally fixed geographical factors can improve the predictive ability and extend the temporal coverage of the statistical models applied for quantifying the ambient air pollution bechle et al 2015 eeftens et al 2011 gulliver et al 2013 mölter et al 2010 rose et al 2011 sampson et al 2011 therefore there is a clear need to develop a spatiotemporal no2 model accounting for both spatial and spatiotemporal variables in the context of australia in this study we aimed to significantly enhance the spatiotemporal estimates of no2 concentration by using satellite based and traffic data in conjunction with comprehensive meteorological and geographical data further for the first time we used a transport model to estimate the volume of traffic flow and traffic dynamics congestion for all of the individual roads and employed them for estimating no2 concentration adaptive neuro fuzzy inference system anfis was employed to estimate the monthly average concentration of no2 in a selected area of australia from 2006 to 2011 and to calculate population weighted no2 concentration in urban areas located in the study area in turn a cross validation technique was used for model validation 2 materials and methods 2 1 data collection 2 1 1 study location and ground level no2 measurements this study was carried out in south east queensland seq which is located in the state of queensland australia seq has an area of 22 420 km2 and it is home to 3 05 million people based on the 2011 australian census australian bureau of statistics 2012 the study area consists of brisbane the state s capital city as well as other urban and rural centres including ipswich logan city gold coast sunshine coast and the lockyer valley fossil fuels combustion such as oil coal and gas is the main source of no2 in australia motor vehicle exhaust has the highest contribution to no2 emission in the australian urban areas australian government 2010 hourly no2 concentration is routinely measured by the queensland government and other organizations in charge of regulatory ambient air pollution monitoring standard chemiluminescence method was used to measure no2 concentrations in seq monitoring network we used quality assured hourly no2 measurements from january 2006 to december 2011 at 12 monitoring sites across seq to obtain monthly averages of the no2 concentration the location and the distance between the air quality monitoring stations are provided in supplements 2 1 2 satellite data daily measurements of no2 tropospheric column abundance were derived from the ozone monitoring instrument omi aboard the aura satellite at a spatial resolution of 13 km 24 km omi detects ultraviolet and visible solar backscatter radiation with a wide field telescope using hyperspectral imaging which provides almost worldwide coverage each day nasa 2007 differential optical absorption spectroscopy doas retrieval method was employed to obtain the no2 tropospheric column density from omi data levelt et al 2006 aura crosses the equator in a sun synchronous polar orbit for the daylight ascending orbit torres et al 2007 and passes over seq at approximately 14 00 local time we chose the omno2d v003 data set derived from omi which is capable of providing near global daily 30 cloud screened tropospheric 0 25 spatial resolution no2 column density the monthly data sets were used to account for the sub tropical location of the seq where seasonal maritime air mass advection combined with topography often creates cloud cover during a large proportion of the austral summer we obtained the monthly average tropospheric no2 column density over seq from nasa giovanni interface for each month from 2006 to 2011 normalized difference vegetation index ndvi was used to provide a measure of greenness and vegetation density ndvi was employed to examine the impact of vegetation cover on no2 concentration in this study the monthly mean ndvi data were retrieved from an advanced very high resolution radiometer avhrr sensor carried on the national oceanic and atmospheric administration noaa satellite and processed by the australian bureau of meteorology bom at a spatial resolution of 1 km bureau of meteorology 2015 2 1 3 meteorological data meteorological parameters including planetary boundary layer height pblh wind direction wd and wind speed ws were carefully investigated in this study these parameters were calculated at a spatial resolution of 3 km at 14 00 local time to match the over pass time of the aura satellite using weather research and forecasting model wrf details on the wrf configuration are provided in the supplement pages s3 s6 other surface meteorological parameters including mean maximum and minimum temperatures rainfall and humidity were also examined in this study these parameters were obtained from high quality spatial climate data sets developed by the australian bureau of meteorology bom which provides gridded climatological maps for each month of the year jones et al 2009 in addition monthly solar exposure maps are obtained from bom for the study period weymouth and le marshall 2001 2 1 4 land use data we obtained information on anthropogenic and natural land use data spatial variables that were potential predictors of no2 concentration these variables provided estimates of emissions from point sources changing land cover conditions and distance to emission sources table 1 2 1 5 traffic data in this paper we used a four stage aggregate transport model the south east queensland strategic transport model seqstm to simulate current and future transport network of the study area ryan et al 2008 the modelled road network includes all freeways highways arterials and a selection of collector roads all public transport routes including three modes namely bus rail and ferry are also included with the details of service frequencies the seqstm is based on the detailed knowledge of the factors affecting transport behaviours such as future changes in population car ownership employment growth in households and changes in the road and public transport networks this model is typically used to predict variations in travel patterns network flows transport mode used increase in general travel volume and the routing of trips evans et al 2007 seqstm is able to calculate the average daily traffic volume trfv for each road in the road network this information was used to provide accurate proxies of traffic and to evaluate the traffic effect on no2 concentration a number of studies highlighted the impact of traffic congestion as a traffic attribute on air pollution bureau of transport and regional economics 2007 luk et al 2009 traffic congestion mainly falls into two types recurrent and non recurrent random events such as adverse weather and work zones are the main reasons of non recurrent congestion as these events temporarily decrease the capacity of a road hence the peak demands exceed the normal amount hojati et al 2014 recurrent congestion in contrast is triggered by chronically surpassed road capacity which is a predictable event and can be determined by using transport models volume to capacity v c ratio is a common congestion performance metric which has been used in many studies lomax 1997 the threshold of this measure to be considered congestion depends on the road types as suggested by klop and guderian 2008 a v c ratio equal or higher than 0 8 can be considered congested seqstm is used to calculate the average daily congestion based on the v c measure on each link to provide a measure of congestion and assess its effect on no2 concentration the independent variables summarized in table 1 were examined to discover which ones improved the prediction of no2 in our modelling process 2 2 modelling approach in this study independent variables were examined either as point or buffer variables point variables were extracted at each monitoring site e g ndvi humidity wind speed while buffer variables were computed within all buffer sizes e g traffic volume land use type following similar studies knibbs et al 2014 novotny et al 2011 22 circular buffers were created around each monitoring site to obtain local and more remote sources of no2 table 1 in total 262 independent variables were obtained including 20 point variables and 242 buffer variables 11 variables computed at 22 buffer sizes to include the traffic data we used trfv which represents the average daily traffic volume for a link in a road network per year a gis layer containing trfv data derived from seqstm was used to calculate trfv within the buffer in order to evaluate the effect of the distance traveled by each vehicle the trfv of each road segment was multiplied by its length to calculate the amount of traffic length interaction trfl in that given buffer using eq 1 1 t r f l i j 1 n t r f v j l e n g t h j where i and j indicate the buffer sizes and number of roads within each buffer respectively we also used seqstm results to calculate the average daily congestion based on the v c ratio on each link v c 0 8 the total number of congested roads was then counted to provide the frequency of the congestion occurrence within a buffer conf fig 1 illustrates a diagram outlining the overall research approach 2 3 input selection we followed a distance decay regression selection strategy developed by su et al 2009 to select the best predictors for no2 models in the following section a brief description of this method is provided for more detailed information see su et al 2009 as the first step the correlations of the all independent variables with measured no2 were computed to obtain variable distance decay curves the independent variable which had the highest correlation with the no2 measurements was selected as the first important predictor a regression model was built by the first important predictor and no2 measurements in the next step the variable which had the highest correlation with the residuals of the regression model was chosen as the potential second important predictor and the procedure replicated to avoid multicollinearity the new predictor was added to the model if the inclusion criteria were met the criteria specified for this step were a the variance inflation factor vif with parameters already in the regression model was less than 3 and b the significance level less than 0 05 su et al 2009 the process stopped when a new selected predictor failed to meet one of the inclusion criteria 2 4 adaptive neuro fuzzy inference system anfis anfis is a hybrid system that incorporates the strengths of fuzzy logic and artificial neural network jang 1993 lin and lee 1991 anfis combines the fuzzy principles with neural network learning abilities which provide an efficient technique for modelling similar to multi layer neural network anfis consists of 5 layers a fuzzy inference system is constructed in the first layer to extract a set of rules in the following 4 layers the adaptive learning algorithm is used to optimize the resulting parameters amini et al 2008 in the first layer known as a fuzzifier a fuzzy inference system fis needs to be created in order to construct membership functions mf and extract the if then rules for the input variables amini et al 2008 jang 1993 in this study sugeno type fuzzy inference system with gaussian membership function is used which has the following form 2 m f i j e x p x x 2 δ 2 f o r i 1 n j 1 m where j shows the number of membership function associated with independent variable i and δ and x indicate the variance and the mean of the gaussian membership function jang 1993 these functions minimize the number of rules by using subtractive clustering method and provide an effective model of the data behaviour for more details see jang and gulley 1995 as the fuzzy logic is not able to learn from the input data mathur et al 2016 naji et al 2016 a combination of the back propagation gradient descent and least squares learning procedures was utilized to provide anfis with adaptive learning abilities jang 1991 1993 the mathworks inc 2005 this adaptive learning process consists of 4 main steps in the first step the product operator is used to calculate the firing strength of the fuzzy rule w based on the membership grade μ resulting from membership function in the second step the normalized firing strength w for each rule is calculated amini et al 2008 in the third step an individual function is created for each variable using the consequent parameters derived from fuzzy rules f and its normalized firing strength mathur et al 2016 lastly the sum of all the outputs resulting from the previous step is calculated to provide the final anfis outputs amini et al 2008 fig 2 shows the structure of anfis model used in this study as described in section 2 3 a distance decay regression selection strategy was employed to select the input data predictors for anfis model the selected predictors were composed of different types of data including meteorological traffic land use and satellite data these predictors were matched with the no2 concentration aligning measurement timings at 12 monitoring sites during the study period 72 months which resulted in more than 800 observation sets in total these observations were divided into training validation and test subsets the majority of the observations 70 were used for training the anfis model training dataset was used to adjust the model weights and derive the required coefficients in order to avoid overfitting 15 of the observations were used for internal validation process and checking the model s generalization wu et al 2012 this dataset was employed to minimize overfitting which means anfis verified that any increase in accuracy over the training dataset resulted in a rise in accuracy over an out of sample dataset that has not been shown to the model before finally the remaining 15 of the observations were employed as the test subset to estimate the no2 concentration by the models at this stage the predictive ability of the anfis model was tested against the measurements and the r2 value was also calculated 2 5 model evaluation in this study 10 fold cross validation cv and leave one out cross validation loo cv methods were employed to evaluate the model performance as these method has the ability to examine the model s predictive ability beckerman et al 2013 the 10 fold cross validation was accomplished by splitting all data into 10 equal sized folds subsequently one of the folds was used to test the model and the remaining 9 folds were used to train the model kim 2009 refaeilzadeh et al 2009 this process was repeated 10 times for each model while all folds were used as the test subset and the r2 and rmse obtained from all models were averaged to calculate the final statistics dirgawati et al 2015 refaeilzadeh et al 2009 with loo cv technique one monitoring site was left out and the model was fitted using the training sets derived from other sites brauer et al 2003 hochadel et al 2006 then the model was used to predict the concentrations at the left out site and calculate the r2 and rmse values this procedure was replicated while all sites were used once as the left out sample finally the results were averaged to calculate the overall r2 and rmse we used r v 3 2 3 r foundation for statistical computing vienna austria and matlab r2014b mathworks inc natick usa for all statistical and soft computing analyses and arcgis version 10 2 esri inc redlands usa for spatial data processing and map creation 3 results 3 1 modelling results in this study a wide range of ground based no2 measurements land use meteorological and satellite data were employed to estimate no2 concentration using anfis as described in subsection 2 4 the effect of satellite and traffic variables on no2 estimations were evaluated for this purpose four different combinations of the variables were created and used for modelling process the first model model a consisted of the meteorological and land use spatial variables satellite based variables ndvi and omi tropospheric no2 column density were included in the previous variables to create model b with model c the traffic variables trfl and conf described in section 2 2 were replaced with road density variables the sum of minor and major roads length used in model b finally all effective spatial and spatiotemporal variables were included in model d for all models the most effective predictors were determined using input selection process described in subsection 2 3 the results of the variable selection process are provided in the supplement table s1 then anfis was used to estimate the no2 concentration and 10 fold cv and loo cv methods were also employed for assessing the model s generalization table 2 summarizes the r2 and rmse values obtained from model fitting and cross validation model a consisted of the land use spatial and meteorological variables and was able to explain 70 of the no2 variations but it has the highest rmse among the other models 2 79 ppb the satellite spatiotemporal variables included in model b increased the r2 by 14 and decreased the rmse by 28 compared to model a using traffic variables rather than the road density in model c r2 increased by 5 and rmse decreased by 18 compared to model b moreover cross validation analyses showed that traffic variables increased the model s generality and model c is less over fitted compared to model b due to the small difference between r2 and rmse values obtained from model fitting and cross validation analyses finally model d which included all spatial and spatiotemporal variables had the best performance across anfis runs with lowest rmse and highest r2 cross validation results also indicated that the data set used in model d increased the spatial predictive ability and model s generality we compared the observed no2 concentrations to the predicted values of the model a model b model c and model d fig 3 the predicted observed plot of the model d indicates that the values are more equally scattered across the line of agreement at the low and high no2 concentrations compared to other models in addition the predicted observed plot shows relatively weak correlation between predictions of the model a and actual observations bland altman analysis was used to evaluate the agreement between the observation and predictions of model a b c and d fig 4 the bland altman plots demonstrated low bias in all models however model d predictions had the best agreement with the observations and the fewest large residuals among other models in addition a conventional land use regression lur model has been used to predict the no2 concentration and compare the prediction ability of the anfis and multiple regression which is mostly used in conventional lur models hence the optimum dataset used in model d were utilized for training both models and no2 concentrations were then predicted a summary of the observed and predicted no2 concentrations is presented in fig 5 table 3 compares the r2 and rmse for model fitting and cross validation results for the model fit the r2 values are 0 91 and 0 82 for the anfis and multiple regression models respectively the rmse values are 1 49 ppb and 2 09 ppb for the anfis and multiple regression models respectively compared with the multiple regression model the anfis model could capture more variability 9 while its rmse decreased by 28 comparing the difference between r2 values derived from model fittings and cross validations results showed that the multiple regression model overfits more than anfis model this also indicated that the anfis model had higher predictive ability and model s generality compared with the multiple regression model 3 2 application of the model to provide spatial estimates of no2 concentration anfis model was applied to the centroid locations of the population grid in seq using the optimal subset of the predictors used in model d best performance model fig 6 a illustrates the land use map of seq and fig 6b and c presents the spatial distribution of annual average no2 concentration across seq in 2006 and 2011 respectively in both years concentration level ranged from 1 to 31 ppb areas with higher concentrations 20 31 ppb corresponded to cities and major towns higher concentrations were predicted in locations with extensive adjacent industrial areas and major roads this pattern was observed in all 6 cities of the study area the highest levels were predicted for the three largest cities brisbane logan and ipswich the annual average no2 concentration in 2006 and 2011 were calculated to compare the concentration levels in 5 local urban centres located in the study area fig 7 shows the annual average no2 concentration levels in 5 local urban centres in 2006 and 2011 in general the annual average concentration dropped from 7 5 ppb in 2006 to 6 4 ppb in 2011 the most significant reduction of no2 concentration was observed in ipswich by 18 3 and the least significant at the sunshine coast by 6 6 also the highest and the lowest average no2 concentrations were predicted in brisbane and the sunshine coast respectively population weighted no2 concentrations were also calculated in local urban centres across seq using anfis model the annual no2 concentrations were predicted at the centroids of the australian population grid 1 km 1 km grid provided by australian bureau of statistics 2011a b the population density of each grid was multiplied by the predicted no2 concentration for each grid and the sum of this value for all grids was calculated and divided by the total population to calculate the population weighted concentration table 4 summarized the population weighted annual average no2 concentration in urban areas of seq in 2011 maximum and minimum average exposure to no2 was observed in brisbane 9 65 ppb km2 and sunshine coast 3 83 ppb km2 respectively fig 8 illustrates the seasonal predictions of no2 concentration in 2006 mean predicted concentrations ppb were higher in winter 6 56 ppb and autumn 4 35 ppb than for spring 4 24 ppb and summer 2 98 ppb maximum and minimum monthly average no2 concentrations were predicted in july 6 89 ppb and february 2 58 ppb respectively monthly prediction maps for 2006 and 2011 are given in the supplement figs s2 and s3 4 discussion we employed anfis to improve the spatiotemporal modelling of no2 concentration using satellite meteorological land use and traffic variables in south east queensland australia to our knowledge this study used the anfis model to estimate no2 concentration for the first time using a cross validation technique the anfis results were found to have good agreement with the no2 measurements the results provide estimates of monthly and annually no2 concentrations for seq from 2006 to 2011 10 fold cv and loo cv methods were employed to evaluate the predictive ability of the different combination of the predictors both cv methods used similar types of predictors for each model model a b c and d unlike loo cv the 10 fold cv used some training sets derived from the left out site to make its predictions hence higher r2 and lower rmse were obtained from 10 fold cv compared to loo cv results in this research seqstm was used to calculate trfl and conf both parameters were highly associated with no2 concentration across anfis runs the wrf model was also employed to calculate pblh and ws based on the input selection results ws was among the important predictors for estimating no2 concentration prior studies mostly considered spatial predictors focused on road density and population related data recent studies demonstrated that meteorological parameters could considerably affect the no2 concentrations elminir 2005 kim and guldmann 2011 2015 hence we evaluated spatiotemporal predictors including satellite meteorological and traffic data in order to improve the temporal resolution of no2 estimates based on results summarized in table 2 a comparison of the model mainly fed with spatial data model a and a model which used a combination of spatial and spatiotemporal data model d reveals an interesting finding adding the spatiotemporal data components to spatial data substantially improved the modelling accuracy and performance as rmse decreased by 47 and r2 increased by 21 fig 3 in addition cross validation results showed model d was less overfit than model a which means model a is not as general as model d the anfis model was found to have better performance and higher accuracy compared with multiple regression model and better agreement with the observed data our results also corroborated with sorek hamer et al 2013 who showed that non linear models like anfis have higher predictive ability compared with multiple regression model used in conventional lur the results obtained from model a and model b show that including satellite based variables increased the r2 by 14 and decreased the rmse by 28 which indicates the key role of satellite based data in model s performance in addition table 2 shows that the use of traffic data model c rather than road density data model b decreased rmse by 18 and increased r2 by 5 these results support the concept that traffic data derived from transport models provide more reliable information about traffic dynamics compared to traditional measures of traffic such as road density in general our results corroborate previous research recommending the use of spatiotemporal variables for no2 prediction bechle et al 2015 dirgawati et al 2015 gulliver et al 2013 vienneau et al 2013 but also demonstrate an improved performance when spatiotemporal data is used to estimate the no2 concentration including satellite and traffic data data sets with different spatial resolutions have been used in our study the resolution of ndvi and wrf outputs for example is finer than the omi satellite data individually omi tropospheric no2 column density data is provided at a coarse spatial resolution which is not accurate enough for assessing the exposure to no2 in epidemiological studies however method of combining different buffer sizes of land use parameters with meteorological traffic and satellite based data enabled the model to combine fine and more spatially coarse data sets to estimate no2 concentration and provide more informative results for epidemiological studies these findings corroborate the result of other studies showing the advantages of combining several data with different spatiotemporal resolutions reid et al 2015 reis et al 2015 another important finding was that the average no2 concentration levels were higher in 2006 compared to 2011 which could be due to the difference in vehicle emission standards in australia prior to 2006 euro 3 standard was applied to passenger vehicles and trucks in seq while then any new vehicle manufactured after 2006 complied with euro 4 standard in which lower level of no2 emission is permitted different methodologies make it difficult to compare our results to other studies however we have attempted to compare our results with other studies which have demonstrated the ability of statistical models for predicting no2 concentration dirgawati et al 2015 lee et al 2014 rahman et al 2017 these studies developed lur models to estimate the variation of no2 and reported r2 lower than 0 75 our model performed better than these models which can be due to either 1 the comprehensive input variables used or 2 spatiotemporal modelling approach or 3 the more robust modelling algorithm used there are some limitations attributed to the data availability in this study consequently 9 of the no2 variations were unexplained by the anfis model the number of observations plays a key role in statistical models basagaña et al 2012 therefore special attention has been paid to the number of observations the use of the temporal data provided further variability for air quality measurements in this study the number of observations used in this study was enough to meet the minimum requirements suggested by basagaña et al 2012 but sparse spatial distribution of the monitoring sites in the study area remain as a limitation of this study although the traffic congestion found to be a better metrics for the traffic data compared with the road density the use of seqstm limited our model as seqstm provided only the annual average congestion which smoothed out the monthly variation of the congestion data another issue was that the concentrations were measured from 2006 to 2011 while the land use parameters were only available for 2011 moreover the type of the air quality monitoring stations was not included as a potential predictor levy et al 2015 suggested that including this predictor could improve the performance of lur model aura passes over seq at 14 p m hence the omi tropospheric no2 column density might underestimate the actual no2 concentration due to the high rate of the photochemical reactions in the atmosphere at this hour of the day the cloud cover during a proportion of the austral summer resulted in missing satellite data hence the modelling outputs may be affected by the loss of the satellite data during the austral summer 5 conclusions in this study a satellite based model was developed for estimating the spatiotemporal variation of no2 concentration on a monthly base an anfis model used for predicting no2 concentration performed very well and exhibited satisfactory performance with r2 and rmse equal to 0 91 and 1 49 ppb respectively it provided estimates of monthly and annually no2 concentrations during 2006 2011 the traffic and satellite data used in this study was found to enhance the estimation of no2 concentrations the method of combining spatiotemporal data with different resolution such as that from satellite traffic and meteorological data with local land use parameters significantly improved the model s performance and provided more informative results this novel approach can be applied to precisely estimate the no2 concentration in different environments the model is particularly useful for epidemiological studies and other researches looking for accurate estimates of no2 concentration at different times finally our study demonstrates the great potential of the anfis model trained by traffic data incorporated with satellite meteorological and land use data to improve the accuracy of spatiotemporal no2 estimations further analysis such as sensitivity and uncertainty analyses can also be done to assess the importance of input parameters and uncertainty in the modelling results acknowledgment a phd scholarship to bijan yeganeh has been provided by the centre for air quality health research and evaluation national health and medical research council centre of research excellence we thank the scientists and staff of nasa for the aura mission as well as the finnish meteorological institute and the netherlands agency for aerospace programs for the omi sensor we also thank the australian government bureau of meteorology nasa noaa australian national pollutant inventory australian bureau of statistics csiro and the terrestrial ecosystem research network for auscover land use ndvi and related data sets we thank the queensland government for no2 data and department of transport and main roads for providing the seqstm s result wrf was processed on the national computational infrastructure nci facility in canberra australia which is supported by the australian government appendix a supplementary data the following is the supplementary data related to this article supplementary material clean supplementary material clean appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2017 11 031 
26460,statistical modelling has been successfully used to estimate the variations of no2 concentration but employing new modelling techniques can make these estimations far more accurate to do so for the first time in application to spatiotemporal air pollution modelling we employed a soft computing algorithm called adaptive neuro fuzzy inference system anfis to estimate the no2 variations comprehensive data sets were investigated to determine the most effective predictors for the modelling process including land use meteorological satellite and traffic variables we have demonstrated that using selected satellite traffic meteorological and land use predictors in modelling increased the r2 by 21 and decreased the root mean square error rmse by 47 compared with the model only trained by land use and meteorological predictors the anfis model found to have better performance and higher accuracy than the multiple regression model our best model captures 91 of the spatiotemporal variability of monthly mean no2 concentrations at 1 km spatial resolution rmse 1 49 ppb in a selected area of australia keywords no2 satellite data anfis spatiotemporal transport model australia data availability the type and source of the data set considered in this study name of the data set data source developer all websites accessed on april 2016 data format software required data availability omi tropospheric no2 column density molecules 1015 cm2 aura omi tropospheric no2 column density product via nasa giovanni interface http giovanni sci gsfc nasa gov giovanni instance id omil2g hdf netcdffiles arcgis freely available major road psma australia transport and topography product https www psma com au products transport topography esri shape files price depends on the area of interest minor road industrial point source nox emissions australia national pollutant inventory http www npi gov au reporting industry reporting materials xml files microsoft excel r freely available australia population density australian bureau of statistics http www abs gov au ausstats abs nsf mf 1270 0 55 007 pngesri gridgeotiff arcgis australia land use classification australian bureau of statistics http www abs gov au websitedbs censushome nsf home meshblockcounts excel spreadsheets csv files microsoft excel r arcgis elevation u s geological survey https www usgs gov products maps topo maps pnggeotiff arcgis normalized difference vegetation index terrestrial ecosystem research network http www auscover org au node 9 netcdf files temperaturerainfallhumiditysolar exposure australian bureau of meteorology http www bom gov au climate maps tabs maps esri gridgif traffic data department of transport and main roads http www tmr qld gov au travel and transport road and traffic info traffic reports and road conditions esri shape files price depends on the area of interest software availability the following software has been used in this study for statistical analysis spatial data processing map creation and calculating the meteorological and traffic related parameters r v 3 2 3 r foundation for statistical computing vienna austria matlab r2014b mathworks inc natick usa arcgis v 10 2 esri inc redlands usa weather research and forecasting v 3 8 1 powers et al 2008 south east queensland strategic transport model ryan et al 2008 note no specific software component has been developed for this study 1 introduction exposure to ambient air pollution is a major environmental risk factor associated with adverse health effects forouzanfar et al 2015 nitrogen dioxide no2 is a major component of ambient air pollution and a strong marker of traffic related emissions briggs et al 1997 richter et al 2005 to date epidemiological studies have demonstrated that there are adverse health effects associated with exposure to no2 crouse et al 2010 2015 filleul et al 2005 mölter et al 2014 parent et al 2013 perez et al 2012 in addition no2 is recognized as a good proxy of particle number concentration in urban environments grundström et al 2015 hence more precise estimates of no2 concentration is needed to investigate its associated role on health effects fossil fuel combustion including coal gas and oil are the major sources of no2 in australia australian government 2010 as a subset of this traffic related emissions are a major source of no2 in urban areas derwent and hertel 1998 about 80 of the no2 in australian urban areas comes from motor vehicle exhaust australian government 2010 this indicates that traffic flow needs to be carefully investigated for estimating the no2 concentration in australia as one of the most urbanized countries in the world different approaches have been used to provide a proxy for traffic flow including calculating the length of the roads or road classification henderson et al 2007 knibbs et al 2014 sahsuvaroglu et al 2006 and using nearest traffic count dirgawati et al 2015 ducret stich et al 2013 some studies used transportation models to obtain more accurate estimates of traffic flow and this approach has been found to provide better results than previous approaches costabile and allegrini 2008 kim and guldmann 2011 2015 shekarrizfard et al 2015 moreover traffic dynamics can also significantly affect the no2 emissions as congested traffic e g stop and go traffic results in particulate matters and gaseous emissions peak beyond the free flow traffic condition davis and peckham 2007 giakoumis et al 2012 hagena et al 2006 keuken et al 2010 rakopoulos and giakoumis 2009 consequently traffic dynamics and condition plays a significant role in the emission of no2 from vehicles in urban areas and should be investigated during the no2 modelling process estimates of air pollution concentration have been traditionally provided by ground monitoring networks the sparse ground measurement network in many parts of the world including australia makes it difficult to evaluate the spatiotemporal variability of ambient air pollution even a dense network could not adequately monitor the spatiotemporal variability of ambient air pollution since it is changing on scales much smaller than monitoring networks density this represents a significant limitation on evaluating the adverse health effects associated with ambient air pollution satellite imagery is another important tool rapidly gaining interest in air pollution monitoring as it provides sequential observations with extensive spatial coverage kloog et al 2014 data derived from satellite sensors can be combined with ground based measurements at different spatiotemporal scales reis et al 2015 the availability of satellite derived data has helped to overcome the problems associated with sparse monitoring networks by providing observations where previously there were none martin 2008 reis et al 2015 observation based statistical methods have emerged as a powerful tool for exploring the quantitative relationship between ground level no2 concentrations and satellite derived data and a variety of these methods have been used to quantify this relationship bechle et al 2015 carnevale et al 2016 hoek et al 2015 horsburgh et al 2016 hystad et al 2011 knibbs et al 2014 novotny et al 2011 reggente et al 2014 vienneau et al 2013 machine learning refers to computational techniques which are able to achieve optimal solutions for analyzing complicated phenomena at reasonable costs kruse et al 2013 ovaska 2004 in recent years machine learning algorithms including support vector machine svm moazami et al 2016 reid et al 2015 yeganeh et al 2012 bayesian models corani and scanagatta 2016 mcbride et al 2007 k nearest neighbours knn reid et al 2015 and artificial neural network ann agirre basurko et al 2006 al alawi et al 2008 ordieres et al 2005 wu et al 2012 yeganeh et al 2017 have also been gaining popularity because of their high flexibility and proven prediction abilities the literature however indicates that adaptive neuro fuzzy inference system anfis which is accepted as a robust and effective method for multivariate analysis has not been used to date to estimate the spatiotemporal variation of no2 concentrations using satellite based data while some studies have been conducted to investigate the spatial variation of the no2 in australia dirgawati et al 2015 knibbs et al 2014 recent studies have shown that the combination of temporal variables e g satellite based observations and metrological data with temporally fixed geographical factors can improve the predictive ability and extend the temporal coverage of the statistical models applied for quantifying the ambient air pollution bechle et al 2015 eeftens et al 2011 gulliver et al 2013 mölter et al 2010 rose et al 2011 sampson et al 2011 therefore there is a clear need to develop a spatiotemporal no2 model accounting for both spatial and spatiotemporal variables in the context of australia in this study we aimed to significantly enhance the spatiotemporal estimates of no2 concentration by using satellite based and traffic data in conjunction with comprehensive meteorological and geographical data further for the first time we used a transport model to estimate the volume of traffic flow and traffic dynamics congestion for all of the individual roads and employed them for estimating no2 concentration adaptive neuro fuzzy inference system anfis was employed to estimate the monthly average concentration of no2 in a selected area of australia from 2006 to 2011 and to calculate population weighted no2 concentration in urban areas located in the study area in turn a cross validation technique was used for model validation 2 materials and methods 2 1 data collection 2 1 1 study location and ground level no2 measurements this study was carried out in south east queensland seq which is located in the state of queensland australia seq has an area of 22 420 km2 and it is home to 3 05 million people based on the 2011 australian census australian bureau of statistics 2012 the study area consists of brisbane the state s capital city as well as other urban and rural centres including ipswich logan city gold coast sunshine coast and the lockyer valley fossil fuels combustion such as oil coal and gas is the main source of no2 in australia motor vehicle exhaust has the highest contribution to no2 emission in the australian urban areas australian government 2010 hourly no2 concentration is routinely measured by the queensland government and other organizations in charge of regulatory ambient air pollution monitoring standard chemiluminescence method was used to measure no2 concentrations in seq monitoring network we used quality assured hourly no2 measurements from january 2006 to december 2011 at 12 monitoring sites across seq to obtain monthly averages of the no2 concentration the location and the distance between the air quality monitoring stations are provided in supplements 2 1 2 satellite data daily measurements of no2 tropospheric column abundance were derived from the ozone monitoring instrument omi aboard the aura satellite at a spatial resolution of 13 km 24 km omi detects ultraviolet and visible solar backscatter radiation with a wide field telescope using hyperspectral imaging which provides almost worldwide coverage each day nasa 2007 differential optical absorption spectroscopy doas retrieval method was employed to obtain the no2 tropospheric column density from omi data levelt et al 2006 aura crosses the equator in a sun synchronous polar orbit for the daylight ascending orbit torres et al 2007 and passes over seq at approximately 14 00 local time we chose the omno2d v003 data set derived from omi which is capable of providing near global daily 30 cloud screened tropospheric 0 25 spatial resolution no2 column density the monthly data sets were used to account for the sub tropical location of the seq where seasonal maritime air mass advection combined with topography often creates cloud cover during a large proportion of the austral summer we obtained the monthly average tropospheric no2 column density over seq from nasa giovanni interface for each month from 2006 to 2011 normalized difference vegetation index ndvi was used to provide a measure of greenness and vegetation density ndvi was employed to examine the impact of vegetation cover on no2 concentration in this study the monthly mean ndvi data were retrieved from an advanced very high resolution radiometer avhrr sensor carried on the national oceanic and atmospheric administration noaa satellite and processed by the australian bureau of meteorology bom at a spatial resolution of 1 km bureau of meteorology 2015 2 1 3 meteorological data meteorological parameters including planetary boundary layer height pblh wind direction wd and wind speed ws were carefully investigated in this study these parameters were calculated at a spatial resolution of 3 km at 14 00 local time to match the over pass time of the aura satellite using weather research and forecasting model wrf details on the wrf configuration are provided in the supplement pages s3 s6 other surface meteorological parameters including mean maximum and minimum temperatures rainfall and humidity were also examined in this study these parameters were obtained from high quality spatial climate data sets developed by the australian bureau of meteorology bom which provides gridded climatological maps for each month of the year jones et al 2009 in addition monthly solar exposure maps are obtained from bom for the study period weymouth and le marshall 2001 2 1 4 land use data we obtained information on anthropogenic and natural land use data spatial variables that were potential predictors of no2 concentration these variables provided estimates of emissions from point sources changing land cover conditions and distance to emission sources table 1 2 1 5 traffic data in this paper we used a four stage aggregate transport model the south east queensland strategic transport model seqstm to simulate current and future transport network of the study area ryan et al 2008 the modelled road network includes all freeways highways arterials and a selection of collector roads all public transport routes including three modes namely bus rail and ferry are also included with the details of service frequencies the seqstm is based on the detailed knowledge of the factors affecting transport behaviours such as future changes in population car ownership employment growth in households and changes in the road and public transport networks this model is typically used to predict variations in travel patterns network flows transport mode used increase in general travel volume and the routing of trips evans et al 2007 seqstm is able to calculate the average daily traffic volume trfv for each road in the road network this information was used to provide accurate proxies of traffic and to evaluate the traffic effect on no2 concentration a number of studies highlighted the impact of traffic congestion as a traffic attribute on air pollution bureau of transport and regional economics 2007 luk et al 2009 traffic congestion mainly falls into two types recurrent and non recurrent random events such as adverse weather and work zones are the main reasons of non recurrent congestion as these events temporarily decrease the capacity of a road hence the peak demands exceed the normal amount hojati et al 2014 recurrent congestion in contrast is triggered by chronically surpassed road capacity which is a predictable event and can be determined by using transport models volume to capacity v c ratio is a common congestion performance metric which has been used in many studies lomax 1997 the threshold of this measure to be considered congestion depends on the road types as suggested by klop and guderian 2008 a v c ratio equal or higher than 0 8 can be considered congested seqstm is used to calculate the average daily congestion based on the v c measure on each link to provide a measure of congestion and assess its effect on no2 concentration the independent variables summarized in table 1 were examined to discover which ones improved the prediction of no2 in our modelling process 2 2 modelling approach in this study independent variables were examined either as point or buffer variables point variables were extracted at each monitoring site e g ndvi humidity wind speed while buffer variables were computed within all buffer sizes e g traffic volume land use type following similar studies knibbs et al 2014 novotny et al 2011 22 circular buffers were created around each monitoring site to obtain local and more remote sources of no2 table 1 in total 262 independent variables were obtained including 20 point variables and 242 buffer variables 11 variables computed at 22 buffer sizes to include the traffic data we used trfv which represents the average daily traffic volume for a link in a road network per year a gis layer containing trfv data derived from seqstm was used to calculate trfv within the buffer in order to evaluate the effect of the distance traveled by each vehicle the trfv of each road segment was multiplied by its length to calculate the amount of traffic length interaction trfl in that given buffer using eq 1 1 t r f l i j 1 n t r f v j l e n g t h j where i and j indicate the buffer sizes and number of roads within each buffer respectively we also used seqstm results to calculate the average daily congestion based on the v c ratio on each link v c 0 8 the total number of congested roads was then counted to provide the frequency of the congestion occurrence within a buffer conf fig 1 illustrates a diagram outlining the overall research approach 2 3 input selection we followed a distance decay regression selection strategy developed by su et al 2009 to select the best predictors for no2 models in the following section a brief description of this method is provided for more detailed information see su et al 2009 as the first step the correlations of the all independent variables with measured no2 were computed to obtain variable distance decay curves the independent variable which had the highest correlation with the no2 measurements was selected as the first important predictor a regression model was built by the first important predictor and no2 measurements in the next step the variable which had the highest correlation with the residuals of the regression model was chosen as the potential second important predictor and the procedure replicated to avoid multicollinearity the new predictor was added to the model if the inclusion criteria were met the criteria specified for this step were a the variance inflation factor vif with parameters already in the regression model was less than 3 and b the significance level less than 0 05 su et al 2009 the process stopped when a new selected predictor failed to meet one of the inclusion criteria 2 4 adaptive neuro fuzzy inference system anfis anfis is a hybrid system that incorporates the strengths of fuzzy logic and artificial neural network jang 1993 lin and lee 1991 anfis combines the fuzzy principles with neural network learning abilities which provide an efficient technique for modelling similar to multi layer neural network anfis consists of 5 layers a fuzzy inference system is constructed in the first layer to extract a set of rules in the following 4 layers the adaptive learning algorithm is used to optimize the resulting parameters amini et al 2008 in the first layer known as a fuzzifier a fuzzy inference system fis needs to be created in order to construct membership functions mf and extract the if then rules for the input variables amini et al 2008 jang 1993 in this study sugeno type fuzzy inference system with gaussian membership function is used which has the following form 2 m f i j e x p x x 2 δ 2 f o r i 1 n j 1 m where j shows the number of membership function associated with independent variable i and δ and x indicate the variance and the mean of the gaussian membership function jang 1993 these functions minimize the number of rules by using subtractive clustering method and provide an effective model of the data behaviour for more details see jang and gulley 1995 as the fuzzy logic is not able to learn from the input data mathur et al 2016 naji et al 2016 a combination of the back propagation gradient descent and least squares learning procedures was utilized to provide anfis with adaptive learning abilities jang 1991 1993 the mathworks inc 2005 this adaptive learning process consists of 4 main steps in the first step the product operator is used to calculate the firing strength of the fuzzy rule w based on the membership grade μ resulting from membership function in the second step the normalized firing strength w for each rule is calculated amini et al 2008 in the third step an individual function is created for each variable using the consequent parameters derived from fuzzy rules f and its normalized firing strength mathur et al 2016 lastly the sum of all the outputs resulting from the previous step is calculated to provide the final anfis outputs amini et al 2008 fig 2 shows the structure of anfis model used in this study as described in section 2 3 a distance decay regression selection strategy was employed to select the input data predictors for anfis model the selected predictors were composed of different types of data including meteorological traffic land use and satellite data these predictors were matched with the no2 concentration aligning measurement timings at 12 monitoring sites during the study period 72 months which resulted in more than 800 observation sets in total these observations were divided into training validation and test subsets the majority of the observations 70 were used for training the anfis model training dataset was used to adjust the model weights and derive the required coefficients in order to avoid overfitting 15 of the observations were used for internal validation process and checking the model s generalization wu et al 2012 this dataset was employed to minimize overfitting which means anfis verified that any increase in accuracy over the training dataset resulted in a rise in accuracy over an out of sample dataset that has not been shown to the model before finally the remaining 15 of the observations were employed as the test subset to estimate the no2 concentration by the models at this stage the predictive ability of the anfis model was tested against the measurements and the r2 value was also calculated 2 5 model evaluation in this study 10 fold cross validation cv and leave one out cross validation loo cv methods were employed to evaluate the model performance as these method has the ability to examine the model s predictive ability beckerman et al 2013 the 10 fold cross validation was accomplished by splitting all data into 10 equal sized folds subsequently one of the folds was used to test the model and the remaining 9 folds were used to train the model kim 2009 refaeilzadeh et al 2009 this process was repeated 10 times for each model while all folds were used as the test subset and the r2 and rmse obtained from all models were averaged to calculate the final statistics dirgawati et al 2015 refaeilzadeh et al 2009 with loo cv technique one monitoring site was left out and the model was fitted using the training sets derived from other sites brauer et al 2003 hochadel et al 2006 then the model was used to predict the concentrations at the left out site and calculate the r2 and rmse values this procedure was replicated while all sites were used once as the left out sample finally the results were averaged to calculate the overall r2 and rmse we used r v 3 2 3 r foundation for statistical computing vienna austria and matlab r2014b mathworks inc natick usa for all statistical and soft computing analyses and arcgis version 10 2 esri inc redlands usa for spatial data processing and map creation 3 results 3 1 modelling results in this study a wide range of ground based no2 measurements land use meteorological and satellite data were employed to estimate no2 concentration using anfis as described in subsection 2 4 the effect of satellite and traffic variables on no2 estimations were evaluated for this purpose four different combinations of the variables were created and used for modelling process the first model model a consisted of the meteorological and land use spatial variables satellite based variables ndvi and omi tropospheric no2 column density were included in the previous variables to create model b with model c the traffic variables trfl and conf described in section 2 2 were replaced with road density variables the sum of minor and major roads length used in model b finally all effective spatial and spatiotemporal variables were included in model d for all models the most effective predictors were determined using input selection process described in subsection 2 3 the results of the variable selection process are provided in the supplement table s1 then anfis was used to estimate the no2 concentration and 10 fold cv and loo cv methods were also employed for assessing the model s generalization table 2 summarizes the r2 and rmse values obtained from model fitting and cross validation model a consisted of the land use spatial and meteorological variables and was able to explain 70 of the no2 variations but it has the highest rmse among the other models 2 79 ppb the satellite spatiotemporal variables included in model b increased the r2 by 14 and decreased the rmse by 28 compared to model a using traffic variables rather than the road density in model c r2 increased by 5 and rmse decreased by 18 compared to model b moreover cross validation analyses showed that traffic variables increased the model s generality and model c is less over fitted compared to model b due to the small difference between r2 and rmse values obtained from model fitting and cross validation analyses finally model d which included all spatial and spatiotemporal variables had the best performance across anfis runs with lowest rmse and highest r2 cross validation results also indicated that the data set used in model d increased the spatial predictive ability and model s generality we compared the observed no2 concentrations to the predicted values of the model a model b model c and model d fig 3 the predicted observed plot of the model d indicates that the values are more equally scattered across the line of agreement at the low and high no2 concentrations compared to other models in addition the predicted observed plot shows relatively weak correlation between predictions of the model a and actual observations bland altman analysis was used to evaluate the agreement between the observation and predictions of model a b c and d fig 4 the bland altman plots demonstrated low bias in all models however model d predictions had the best agreement with the observations and the fewest large residuals among other models in addition a conventional land use regression lur model has been used to predict the no2 concentration and compare the prediction ability of the anfis and multiple regression which is mostly used in conventional lur models hence the optimum dataset used in model d were utilized for training both models and no2 concentrations were then predicted a summary of the observed and predicted no2 concentrations is presented in fig 5 table 3 compares the r2 and rmse for model fitting and cross validation results for the model fit the r2 values are 0 91 and 0 82 for the anfis and multiple regression models respectively the rmse values are 1 49 ppb and 2 09 ppb for the anfis and multiple regression models respectively compared with the multiple regression model the anfis model could capture more variability 9 while its rmse decreased by 28 comparing the difference between r2 values derived from model fittings and cross validations results showed that the multiple regression model overfits more than anfis model this also indicated that the anfis model had higher predictive ability and model s generality compared with the multiple regression model 3 2 application of the model to provide spatial estimates of no2 concentration anfis model was applied to the centroid locations of the population grid in seq using the optimal subset of the predictors used in model d best performance model fig 6 a illustrates the land use map of seq and fig 6b and c presents the spatial distribution of annual average no2 concentration across seq in 2006 and 2011 respectively in both years concentration level ranged from 1 to 31 ppb areas with higher concentrations 20 31 ppb corresponded to cities and major towns higher concentrations were predicted in locations with extensive adjacent industrial areas and major roads this pattern was observed in all 6 cities of the study area the highest levels were predicted for the three largest cities brisbane logan and ipswich the annual average no2 concentration in 2006 and 2011 were calculated to compare the concentration levels in 5 local urban centres located in the study area fig 7 shows the annual average no2 concentration levels in 5 local urban centres in 2006 and 2011 in general the annual average concentration dropped from 7 5 ppb in 2006 to 6 4 ppb in 2011 the most significant reduction of no2 concentration was observed in ipswich by 18 3 and the least significant at the sunshine coast by 6 6 also the highest and the lowest average no2 concentrations were predicted in brisbane and the sunshine coast respectively population weighted no2 concentrations were also calculated in local urban centres across seq using anfis model the annual no2 concentrations were predicted at the centroids of the australian population grid 1 km 1 km grid provided by australian bureau of statistics 2011a b the population density of each grid was multiplied by the predicted no2 concentration for each grid and the sum of this value for all grids was calculated and divided by the total population to calculate the population weighted concentration table 4 summarized the population weighted annual average no2 concentration in urban areas of seq in 2011 maximum and minimum average exposure to no2 was observed in brisbane 9 65 ppb km2 and sunshine coast 3 83 ppb km2 respectively fig 8 illustrates the seasonal predictions of no2 concentration in 2006 mean predicted concentrations ppb were higher in winter 6 56 ppb and autumn 4 35 ppb than for spring 4 24 ppb and summer 2 98 ppb maximum and minimum monthly average no2 concentrations were predicted in july 6 89 ppb and february 2 58 ppb respectively monthly prediction maps for 2006 and 2011 are given in the supplement figs s2 and s3 4 discussion we employed anfis to improve the spatiotemporal modelling of no2 concentration using satellite meteorological land use and traffic variables in south east queensland australia to our knowledge this study used the anfis model to estimate no2 concentration for the first time using a cross validation technique the anfis results were found to have good agreement with the no2 measurements the results provide estimates of monthly and annually no2 concentrations for seq from 2006 to 2011 10 fold cv and loo cv methods were employed to evaluate the predictive ability of the different combination of the predictors both cv methods used similar types of predictors for each model model a b c and d unlike loo cv the 10 fold cv used some training sets derived from the left out site to make its predictions hence higher r2 and lower rmse were obtained from 10 fold cv compared to loo cv results in this research seqstm was used to calculate trfl and conf both parameters were highly associated with no2 concentration across anfis runs the wrf model was also employed to calculate pblh and ws based on the input selection results ws was among the important predictors for estimating no2 concentration prior studies mostly considered spatial predictors focused on road density and population related data recent studies demonstrated that meteorological parameters could considerably affect the no2 concentrations elminir 2005 kim and guldmann 2011 2015 hence we evaluated spatiotemporal predictors including satellite meteorological and traffic data in order to improve the temporal resolution of no2 estimates based on results summarized in table 2 a comparison of the model mainly fed with spatial data model a and a model which used a combination of spatial and spatiotemporal data model d reveals an interesting finding adding the spatiotemporal data components to spatial data substantially improved the modelling accuracy and performance as rmse decreased by 47 and r2 increased by 21 fig 3 in addition cross validation results showed model d was less overfit than model a which means model a is not as general as model d the anfis model was found to have better performance and higher accuracy compared with multiple regression model and better agreement with the observed data our results also corroborated with sorek hamer et al 2013 who showed that non linear models like anfis have higher predictive ability compared with multiple regression model used in conventional lur the results obtained from model a and model b show that including satellite based variables increased the r2 by 14 and decreased the rmse by 28 which indicates the key role of satellite based data in model s performance in addition table 2 shows that the use of traffic data model c rather than road density data model b decreased rmse by 18 and increased r2 by 5 these results support the concept that traffic data derived from transport models provide more reliable information about traffic dynamics compared to traditional measures of traffic such as road density in general our results corroborate previous research recommending the use of spatiotemporal variables for no2 prediction bechle et al 2015 dirgawati et al 2015 gulliver et al 2013 vienneau et al 2013 but also demonstrate an improved performance when spatiotemporal data is used to estimate the no2 concentration including satellite and traffic data data sets with different spatial resolutions have been used in our study the resolution of ndvi and wrf outputs for example is finer than the omi satellite data individually omi tropospheric no2 column density data is provided at a coarse spatial resolution which is not accurate enough for assessing the exposure to no2 in epidemiological studies however method of combining different buffer sizes of land use parameters with meteorological traffic and satellite based data enabled the model to combine fine and more spatially coarse data sets to estimate no2 concentration and provide more informative results for epidemiological studies these findings corroborate the result of other studies showing the advantages of combining several data with different spatiotemporal resolutions reid et al 2015 reis et al 2015 another important finding was that the average no2 concentration levels were higher in 2006 compared to 2011 which could be due to the difference in vehicle emission standards in australia prior to 2006 euro 3 standard was applied to passenger vehicles and trucks in seq while then any new vehicle manufactured after 2006 complied with euro 4 standard in which lower level of no2 emission is permitted different methodologies make it difficult to compare our results to other studies however we have attempted to compare our results with other studies which have demonstrated the ability of statistical models for predicting no2 concentration dirgawati et al 2015 lee et al 2014 rahman et al 2017 these studies developed lur models to estimate the variation of no2 and reported r2 lower than 0 75 our model performed better than these models which can be due to either 1 the comprehensive input variables used or 2 spatiotemporal modelling approach or 3 the more robust modelling algorithm used there are some limitations attributed to the data availability in this study consequently 9 of the no2 variations were unexplained by the anfis model the number of observations plays a key role in statistical models basagaña et al 2012 therefore special attention has been paid to the number of observations the use of the temporal data provided further variability for air quality measurements in this study the number of observations used in this study was enough to meet the minimum requirements suggested by basagaña et al 2012 but sparse spatial distribution of the monitoring sites in the study area remain as a limitation of this study although the traffic congestion found to be a better metrics for the traffic data compared with the road density the use of seqstm limited our model as seqstm provided only the annual average congestion which smoothed out the monthly variation of the congestion data another issue was that the concentrations were measured from 2006 to 2011 while the land use parameters were only available for 2011 moreover the type of the air quality monitoring stations was not included as a potential predictor levy et al 2015 suggested that including this predictor could improve the performance of lur model aura passes over seq at 14 p m hence the omi tropospheric no2 column density might underestimate the actual no2 concentration due to the high rate of the photochemical reactions in the atmosphere at this hour of the day the cloud cover during a proportion of the austral summer resulted in missing satellite data hence the modelling outputs may be affected by the loss of the satellite data during the austral summer 5 conclusions in this study a satellite based model was developed for estimating the spatiotemporal variation of no2 concentration on a monthly base an anfis model used for predicting no2 concentration performed very well and exhibited satisfactory performance with r2 and rmse equal to 0 91 and 1 49 ppb respectively it provided estimates of monthly and annually no2 concentrations during 2006 2011 the traffic and satellite data used in this study was found to enhance the estimation of no2 concentrations the method of combining spatiotemporal data with different resolution such as that from satellite traffic and meteorological data with local land use parameters significantly improved the model s performance and provided more informative results this novel approach can be applied to precisely estimate the no2 concentration in different environments the model is particularly useful for epidemiological studies and other researches looking for accurate estimates of no2 concentration at different times finally our study demonstrates the great potential of the anfis model trained by traffic data incorporated with satellite meteorological and land use data to improve the accuracy of spatiotemporal no2 estimations further analysis such as sensitivity and uncertainty analyses can also be done to assess the importance of input parameters and uncertainty in the modelling results acknowledgment a phd scholarship to bijan yeganeh has been provided by the centre for air quality health research and evaluation national health and medical research council centre of research excellence we thank the scientists and staff of nasa for the aura mission as well as the finnish meteorological institute and the netherlands agency for aerospace programs for the omi sensor we also thank the australian government bureau of meteorology nasa noaa australian national pollutant inventory australian bureau of statistics csiro and the terrestrial ecosystem research network for auscover land use ndvi and related data sets we thank the queensland government for no2 data and department of transport and main roads for providing the seqstm s result wrf was processed on the national computational infrastructure nci facility in canberra australia which is supported by the australian government appendix a supplementary data the following is the supplementary data related to this article supplementary material clean supplementary material clean appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2017 11 031 
26461,this paper proposes an innovative framework for solving stochastic multi criteria decision making mcdm problems when uncertainties exist in criteria performance values pvs and criteria weights cws simultaneously methods for quantifying uncertainties in criteria pvs and cws are presented we establish the smaa topsis model by combining stochastic multicriteria acceptability analysis smaa and technique for order preference by similarity to ideal solution topsis the risk of decision making errors is proposed to assess the impact of uncertainties on mcdm we develop the lhs based monte carlo simulation algorithm and corresponding computer program for solving the smaa topsis model we also suggest a three stage mcdm procedure for stochastic mcdm problems we apply the proposed methodology to a flood control operation case study to demonstrate its applicability our results indicate that the proposed methods can provide valuable risk information and enable risk informed decisions to be made with higher reliabilities keywords multi criteria decision making flood control operation stochastic multicriteria acceptability analysis monte carlo simulation software availability name of software smaa topsis for stochastic mcdm developer feilin zhu email zhufeilin hhu edu cn software required visual basic 6 0 visual basic for applications r project availability the software is freely available for noncommercial use upon request from the first author 1 introduction multi criteria decision making mcdm is a methodology which has been extensively employed to assist water resources and environmental decision making fowler et al 2015 ganji et al 2016 hyde and maier 2006 romañach et al 2014 su and tung 2014 since it facilitates multi stakeholder participation and allows the consideration of multiple criteria measured in incommensurable units reservoir flood control operation requires concurrent optimization of several conflicting objectives such as hydropower generation flood control water supply irrigation and etc it is difficult to determine an optimal solution that optimizes all objectives simultaneously luo et al 2015 qin et al 2010 instead mcdm methods are typically employed to evaluate and select the non inferior alternatives generated by multi objective optimization models so as to obtain the most preferred alternative and put it into practice hajkowicz and collins 2007 reviewed applications of mcdm methods for a diverse range of water resources problems and classified these methods into six categories 1 multi criteria value functions 2 outranking approaches 3 distance to ideal point methods 4 pairwise comparisons 5 fuzzy set analysis and 6 tailored methods in reservoir flood control operation uncertainties mainly come from two aspects including the uncertainty of criteria performance values pvs and criteria weights cws on one hand numerous uncertainty factors e g inflow forecasting errors reservoir capacity curve errors river flood routing errors and etc lead to the randomness of flood control target factors which usually serve as criteria to evaluate the performance of alternatives when considering uncertainties these target factors are no longer constants but random variables with certain probability distributions since the 1980s hydrologists have paid much attention to the risk analysis of flood control operation bogner and pappenberger 2011 chen et al 2015 yan et al 2014 however little research attempted to combine risk analysis with mcdm models on the other hand cws used to measure criteria s relative importance and coordinate multiple operation objectives have been found to be a potential source of uncertainty in mcdm ganji et al 2016 the cws can be either subjective or objective in the context of group decision making all decision makers need to express subjective preferences to incorporate their knowledge and experience into mcdm models it is difficult for multiple decision makers with conflicting interests to reach consensus over cws de brito and evers 2016 madani and lund 2011 moreover the weighting process usually contains fuzziness and subjectivity of human judgments which will lead to imprecise or uncertain cws objective cws are determined based on the information of decision matrices and it has been found that different cws elicitation methods may elicit diverse cws hajkowicz and collins 2007 hyde and maier 2006 zhu et al 2017 furthermore considerable information loss will occur and the real uncertainty of cws will be concealed if cws obtained from multiple decision makers or diverse methods are aggregated or averaged as a deterministic cw cai et al 2004 therefore uncertainties exist in criteria pvs and cws simultaneously during the mcdm modelling process conventional mcdm methods for reservoir flood control operation are mainly developed and applied under deterministic or fuzzy environments chen and hou 2004 cheng and chau 2002 fu 2008 wang et al 2011 zhu et al 2016 in deterministic or fuzzy cases an absolutely fixed ranking of alternatives is obtained however when considering uncertainties in input parameters i e criteria pvs and cws any alternative is likely to get better or worse ranks and a reversal of the fixed ranking will occur which further lead to the risk of decision making errors xiong and qi 2010 presented a method for stochastic mcdm problems with incomplete weight information in this method a stochastic mcdm problem was converted into an interval mcdm problem by interval estimation qin 2011 introduced a relative dominance based mcdm method when criteria pvs are normal random variables qin s study is meaningful since it first combines risk analysis with mcdm models but they disregard the uncertainty in cws and limit criteria pvs as normal random variables in addition madani and lund 2011 proposed a monte carlo game theory mcgt approach for dealing with uncertainty in the performance of alternatives which mapped the stochastic mcdm problem into many deterministic strategic games and solved them using non cooperative stability definitions however little attention has been paid to consider uncertainties in criteria pvs and cws simultaneously the subject of multi criteria group decision making under uncertainty has not yet been addressed thoroughly in a unified framework stochastic multicriteria acceptability analysis smaa is a family of methods for assisting mulita criteria group decision making in situations where criteria pvs and cws are uncertain lahdelma et al 1998 plenty of smaa variants have been developed such as smaa 2 lahdelma and salminen 2001 smaa ahp durbach et al 2014 smaa promethee corrente et al 2014 and so on smaa 2 forms the basis of other smaa variants and is regarded as the most representative one in smaa 2 an additive utility function is used to represent decision makers preference and map different alternatives to real values in reality any type of utility function can be used in smaa 2 namely smaa 2 can be applied in combination with other mcdm methods the technique for order preference by similarity to ideal solution topsis first introduced by hwang and yoon 1981 has been extensively applied to water resources and environmental problems zagonari and rossi 2013 based on the concepts of ideal and anti ideal points the best alternative determined by topsis should be the one which is simultaneously closest to the ideal alternative and farthest from the anti ideal alternative topsis is recommended by the united nation environmental program unep to evaluate water resources development projects moreover some fuzzy versions of topsis have also been developed triantaphyllou and lin 1996 torfi et al 2010 okul et al 2014 presented the idea of integrating the smaa theory and topsis method and they applied the combined method to the problem of light machine gun selection the main purpose of their study was to improve the basic topsis method and allow topsis to handle imprecise data however the issue of how to deal with related uncertainties was ignored in addition the superiority of smaa topsis was not well evaluated this paper proposes an innovative framework for solving stochastic mcdm problems when uncertainties exist in criteria pvs and cws simultaneously this helps to allow all expected uncertainties to be incorporated into the mcdm modelling process and make risk informed decisions with higher reliabilities first we define the formulation of mcdm problems methods for quantifying uncertainties in criteria pvs are discussed we introduce the feasible weight space fws in combination with fuzzy analytic hierarchy process fahp to quantify the weight uncertainties we then establish the smaa topsis model by integrating topsis and smaa 2 moreover we propose the concept of the risk of decision making errors and its quantitative calculation method to assess the effect of existing uncertainties on mcdm results the monte carlo simulation algorithm based on latin hypercube sampling lhs and corresponding computer program are developed for solving the smaa topsis model in addition we suggest a three stage mcdm procedure for stochastic mcdm problems we summarize the difference between smaa topsis and deterministic mcdm models the proposed methodology is applied to a flood control operation case study to demonstrate its applicability and insights it can provide beyond traditional methods the remainder of this paper is organized as follows the proposed methodology is presented in section 2 followed by results and discussion of the case study in section 3 section 4 contains summary and conclusions 2 methodology fig 1 shows the flowchart of the proposed methodology details of each step are presented in the following subsections 2 1 formulation of mcdm problems there are many terminologies used to refer to mcdm some other terms include multi criteria decision analysis mcda multi attribute decision making madm and multi objective decision support mods all these terms share the same theoretical basis and are jointly referred to in this paper as mcdm generally an mcdm problem comprises 1 a set of alternatives which need to be evaluated ranked and selected 2 a set of criteria measured in incommensurable units and 3 an mcdm model the set of alternatives can be either implicitly defined by constraints in multi objective optimization models or explicitly defined and discrete in number durbach and stewart 2012 each criterion should be associated with a measurable attribute which provides a qualitative or quantitative scale for assessing performances of alternatives this can be done via mathematical models and or expert judgment mcdm models basically differ depend on 1 how they determine marginal evaluation on each criterion and 2 how they aggregate marginal evaluations across criteria to achieve a global evaluation formulation of an mcdm problem includes the identification of alternatives criteria and criteria weights in the rest of this paper we consider an mcdm problem consisting of m alternatives and n criteria alternative set is denoted as a a 1 a 2 a m and criterion set as c c 1 c 2 c n w w 1 w 2 w n represents the weight vector let x ij be the marginal evaluation of alternative a i with respect to criterion c j and the decision matrix can be expressed as x x i j m n where i and j denote index of the alternative and criterion respectively the global evaluation g i measures the overall performance of alternative a i with respect to all criteria by which the ranking results are determined here we take the weighted sum method which is one of the simplest mcdm methods as an example when using this method the global evaluation g i can be expressed as 1 g i j 1 n x i j w j as a result of the above definition the best alternative is the one that corresponds to the largest global evaluation g i generally the essence of an mcdm problem can be concluded as transforming multiple criteria values i e marginal evaluation into a single overall measure i e global evaluation by which all alternatives are evaluated ranked and selected 2 2 handling uncertainties in the criteria pvs in reservoir flood control operation uncertainties come from many aspects such as flood forecasting errors water level at flood conditions outflow discharge capacity delay time of operation and so on these uncertainty factors result in the randomness of flood control target factors e g the peak discharge from the reservoir the highest water level the end water level and etc which further cause risks for flood control decision making since the 1980s risk analysis of flood control operation has attracted much attention from academia and plenty of methods have been proposed for assessing risks bogner and pappenberger 2011 chen et al 2015 yan et al 2014 generally risk analysis methods can be classified as stochastic differential equation method direct integration method structural reliability analysis method fuzzy sets based method and monte carlo simulation and they differ in terms of mathematical complexity and data requirements before mcdm for reservoir flood control operation a crucial issue is to quantify the uncertainties in criteria pvs namely to determine the probability distributions of flood control target factors using risk analysis approaches in a stochastic mcdm problem deterministic criteria pvs x i j are replaced by random variables ξ i j with estimated probability distribution functions accordingly the stochastic decision matrix can be expressed as 2 x ξ i j m n ξ 11 ξ 12 ξ 1 n ξ 21 ξ 22 ξ 2 n ξ m 1 ξ m 2 ξ m n 2 3 handling uncertainties in the cws in the following we introduce the fws coupled with fahp to quantify the weight uncertainties as probability distributions with interval constraints 2 3 1 feasible weight space under the condition of no cws we define the basic weight space as a union of all possible weight vectors in a three criterion case the basic weight space should be a triangular plane shown in fig 2 a when the weight vector is deterministic the basic weight space shrinks to a point w 1 w 2 w 3 on the plane as shown in fig 2 b in the basic weight space only one point is usually insufficient to represent the preferences from a group of decision makers therefore the interval estimation of cws is more practical and appropriate than point estimations this is our motivation to propose the concept of fws which can be regarded as a subspace in the basic weight space as shown in fig 2 c when cws follow the uniform distribution with interval constraints the fws is represented by a hexagonal plane and can be formulated as 3 f w s w r 3 w j 0 w j min w j w j max j 1 3 w j 1 smaa topsis is flexible to model weight uncertainties via probability distributions arbitrary kind of probability distribution can be used in smaa topsis and the resulting fws is demonstrated as an arbitrary shape plane in fig 2 d here we just take the three criterion case for an example the concept of fws can be extended to higher dimension cases as illustrated in fig 3 we also use a bar graph to illustrate the fws consider a multi criteria group decision making problem with three criteria in which three flood control experts are invited to express preferences fig 3 shows that the cws are usually different from each other the red dashed line and solid line represent the maximum and minimum constraints of the cws respectively as can be seen fws is not just defined as the interval between the red lines but a wider region over the interval extended to n dimensional case fws can be formulated as follows 4 f w s w r n w j 0 w j min α w j max w j min w j w j max α w j max w j min j 1 n w j 1 where we use the parameter α to reflect the confidence degree of experts for the fws assessment if experts are confident about their cws α can be assigned with a small value or even zero otherwise a large α representing a higher uncertainty should be used to generate a wider fws according to our experience the α with the range of 0 1 0 4 is appropriate in practical applications in conclusion fahp is employed to help flood control experts elicit cws and fws is then used to extend the weight vector form one point to a subspace in group decision making the fws is useful to cover all preferences and avoid information loss the weight uncertainties are quantified by probability distributions with interval constraints which allow all expected uncertainty and subjectivity in the cws to be incorporated in the next section we introduce the fahp for weight assessment 2 3 2 fuzzy analytic hierarchy process for weight assessment fahp is an extension of classical ahp considering the fuzziness in decision makers subjective judgments triangular fuzzy number is often used in combination with fahp to transform fuzzy or imprecise linguistic variables into crisp values according to fuzzy sets theory a fuzzy number a denoted as l m u becomes a triangular fuzzy number if its membership function μ x satisfies 5 μ x x l m l l x m x u m u m x u 0 o t h e r w i s e where l and u represent lower and upper bounds of a respectively m is the point where the membership degree is equal to 1 in this paper we use fahp coupled with triangular fuzzy numbers to obtain all decision makers preferences and quantify the preference information as quantitative cws namely to determine the bars in fig 3 the following steps are involved step 1 expert participation and construction of fuzzy judgment matrix in real time flood control operation multiple flood control experts are invited to express their preference information according to the rating scale for pairwise comparisons shown in table 1 the judgment results are represented by a triangular fuzzy judgment matrix 6 a a i j m n a 11 a 12 a 1 n a 21 a 22 a 2 n a n 1 a n 2 a n n where a i j l i j m i j u i j step 2 computation of the initial weight firstly summarize each row of the fuzzy judgment matrix secondly normalize the row sums by the following fuzzy arithmetic operations 7 s i j 1 n a i j k 1 n j 1 n a i j 1 j 1 n l i j k 1 n j 1 n u k j j 1 n m i j k 1 n j 1 n m k j j 1 n u i j k 1 n j 1 n l k j step 3 defuzzification we use the formula recommended by chang 1996 to calculate the degree of possibility of s i over all other n 1 fuzzy numbers to get a crisp value 8 v s i s j j 1 2 n j i min j 1 2 n j i v s i s j where s i l i m i u i and s j l j m j u j v s i s j is the degree of possibility of s i s j which is illustrated in fig 4 and expressed as follows 9 v s i s j 1 u i l j u i m i m j l j 0 m i m j l j u i o t h e r w i s e step 4 normalization finally we can obtain the weight vector w w 1 w 2 w n by the following formula 10 w i v s i s j j 1 2 n j i k 1 n v s k s j j 1 2 n j k 2 4 the basic smaa 2 model in smaa 2 a real valued utility function u is employed to map different alternatives to utility values 11 u i u x i w where x i represents the criteria pv vector of alternative a i smaa 2 is developed especially for mcdm problems with uncertain or missing information uncertain criteria pvs are represented by random variables ξ i j with assumed or estimated joint probability distribution function f x ξ in the space x r m n similarly uncertain cws are represented by weight distributions with joint probability distribution function f w w in the weight space w defined as follows 12 w w r n w j 0 j 1 n w j 1 the ranking function is used to determine the rank of each alternative from the best rank 1 to the worst rank m as follows 13 rank ξ i w 1 k 1 m ρ u ξ k w u ξ i w where ρ t r u e 1 and ρ f a l s e 0 ξ i denotes the stochastic criteria pv vector of alternative a i the favourable rank weights w i r ξ is an important concept in smaa 2 a weight vector w w i r ξ assigns utilities for alternative a i to guarantee it obtains rank r w i r ξ can be expressed as follows 14 w i r ξ w w rank ξ i w r in smaa 2 stochastic criteria pvs and cws serve as model inputs based on the analysis of w i r ξ four descriptive measures are provided to decision makers including the rank acceptability index the holistic acceptability index the central weight vector and the confidence factor the rank acceptability index b i r is a measure of the variety of different preferences that grant alternative a i rank r b i r is calculated as a multi dimensional integral over the criteria distributions and the favourable rank weights as follows 15 b i r x f x ξ w i r ξ f w w d w d ξ the holistic acceptability index a i h considers all rank acceptability indices and examines the overall acceptability of each alternative 16 a i h r 1 m α r b i r where α r are so called meta weights which indicate the contribution of each rank acceptability index to the holistic acceptability index there are three kinds of meta weights for choice i e linear weights α r m r m 1 inverse weights α r 1 r and centroid weights α r i r m 1 i i 1 m 1 i as shown in fig 5 the central weight vector w i c is the expected center of gravity of the favourable first rank weight space the central weight vector represents the typical weight vector supporting alternative a i and is calculated as a multi dimensional integral over the criteria distributions and the favourable first rank weights 17 w i c x f x ξ w i 1 ξ f w w w d w d ξ b i 1 the confidence factor p i c is defined as the probability that an alternative obtains the first rank when its central weight vector is used p i c measures whether the criteria pvs are accurate enough to discern efficient alternatives it is calculated as a multi dimensional integral over the criteria distributions as 18 p i c ξ x rank ξ w i c 1 f x ξ d ξ 2 5 the smaa topsis model the smaa topsis model is established by combing smaa 2 and topsis the main advantages of using topsis lie in 1 its robust logical structure 2 its powerful computation capability and 3 its ability to consider ideal and anti ideal points concurrently instead of the additive utility function in the original smaa 2 model the topsis algorithm is used main steps of the topsis type utility function are as follows step 1 normalize the decision matrix x x i j m n to y y i j m n by the vector normalization formula 19 y i j x i j i 1 m x i j 2 step 2 multiply the normalized decision matrix y y i j m n by the weight vector w w 1 w 2 w n to obtain the weighted decision matrix z z i j m n where z i j y i j w j step 3 determine the ideal alternative s s 1 s 2 s n and anti ideal alternative s s 1 s 2 s n as follows 20 s j max 1 i m z i j for benefit criteria min 1 i m z i j for cost criteria 21 s j min 1 i m z i j for benefit criteria max 1 i m z i j for cost criteria step 4 calculate the euclidean distances from alternative a i to the ideal d i and anti ideal d i alternatives respectively 22 d i j 1 n s j z i j 2 i 1 2 m d i j 1 n s j z i j 2 i 1 2 m step 5 the closeness coefficient c i provides the global evaluation for alternative a i regarding all criteria as mentioned in section 2 1 the best alternative is the one that corresponds to the largest c i the closeness coefficient can be calculated by 23 c i d i d i d i 2 6 the risk of decision making errors the basic topsis model produces deterministic closeness coefficient to evaluate candidate alternatives and a fixed ranking is obtained however due to the influence of uncertainties the closeness coefficients will no longer be constants but random variables fluctuating around their mean values as illustrated in fig 6 it can be seen from fig 6 that any alternative is likely to obtain better or worse ranks and a reversal of the fixed ranking will occur which further lead to the risks of decision making errors in reservoir flood control operation the best alternative is usually concerned by decision makers and will be adopted into flood control practice hence it will cause more serious flood control consequences if worse alternatives get the first rank due to the uncertainties strictly speaking risk should be defined as a function of the probability of an uncertain event happening and its consequences however it is usually difficult to estimate the consequences of an uncertain event especially when the pre event evaluation is required i e before the uncertain event happens therefore the risk definition is simplified as the probability of an uncertain event happening which has been widely accepted in the field of water resources management chen et al 2015 yan et al 2014 in order to assess the ranking uncertainty we define the risk of decision making errors as the weighted probability that non optimal alternatives get the first rank 24 p f k 2 m β k b k 1 where b k 1 represents the first rank acceptability index of the alternative which gets the kth rank when using the basic topsis model β k k 2 3 m is defined as risk weights to measure the contribution of each non optimal alternative to p f the parameter β k is used to reflect the characteristic that worse alternatives obtaining the first rank will lead to greater risks we define β k as an incremental nonnegative and m 1 dimensional vector 25 β k k k 2 m k the risk of decision making errors can be viewed as a new descriptive measure of smaa topsis in addition to the other four measures i e the rank acceptability index the holistic acceptability index the central weight vector and the confidence factor in group decision making processes all of these descriptive measures should be jointly analyzed before making a final decision it should be noted that the multi dimensional integrals in eqs 15 18 and 24 are difficult to be calculated analytically since the distributions f x and f w can be complex in real world applications besides the integrals will have a quite high dimension for the mcdm problems with a large number of alternatives or criteria hence this paper proposes a lhs based monte carlo simulation algorithm to numerically solve the smaa topsis model in which lhs is used due to its sampling efficiency sheikholeslami and razavi 2017 the developed algorithm and program are presented in appendix 2 7 three stage mcdm procedure under deterministic situations the best alternative is determined based on the fixed ranking of all candidate alternatives which can be referred to as the single value and deterministic rank mcdm procedure although smaa 2 allows uncertainties in the criteria pvs and cws to be considered the full ranking sequence is still provided to decision makers and used for making final decisions which is actually the same as the single value and deterministic rank mcdm procedure however it is quite unreliable to make a final decision only based on the full ranking sequence since ranking results are heavily subject to uncertainties in other words even the best alternative still has certain probabilities to obtain worse ranks therefore decision makers must explicitly consider the uncertainty of ranking results as well as the corresponding risk of decision making errors in this paper we suggest a three stage mcdm procedure for guiding stochastic mcdm under uncertainty this helps decision makers make risk informed decisions with higher reliabilities stage 1 no preference information is required in the beginning of mcdm modelling processes instead smaa topsis produces the rank acceptability indices to discern efficient and inefficient alternatives via exploring the basic weight space the alternatives a i with a large first rank acceptability index b i 1 can be considered as the efficient alternative since it has a large probability to obtain the best rank contrarily the alternative a i is inefficient if it is dominated by other alternatives since strictly better alternatives exist and this alternative is impossible to get the best rank even exploring the entire weight space in addition the central weight vectors is used to describe what kind of cws will favor what kind of best alternatives then help decision makers express their preferences considering various flood control situations stage 2 in this stage the fws coupled with fahp is used to obtain decision makers preferences and quantify the weight uncertainties as probability distributions with interval constraints the stochastic pvs and cws are incorporated into the smaa topsis modelling process smaa topsis can provide a better discrimination for efficient and inefficient alternatives and determine the probabilities that each alternative obtains each rank stage 3 the holistic acceptability indices are used to determine the full ranking sequence of alternatives in this stage the rank acceptability indices the holistic acceptability indices and the risk of decision making errors should be jointly analyzed to inform decision makers of potential risks before making a final decision 2 8 smaa topsis versus deterministic mcdm models comparison between the smaa topsis model and deterministic mcdm models is shown in fig 7 deterministic mcdm models transform the decision matrix together with weight information into a deterministic overall measure to obtain a fixed ranking of alternatives in traditional mcdm procedures the decision matrix i e criteria pvs the cws and the mcdm model are three indispensable components to guarantee a final decision can be made however a deterministic mcdm model will lose its ability to rank alternatives when uncertainties exist in criteria pvs or cws smaa topsis breaks through the traditional mcdm framework in the early stage of the mcdm modelling process smaa topsis does not require any preference information from decision makers instead the inverse weight space analysis i e exploring the entire weight space is performed to evaluate the cws that make each alternative the best one or that would assign a certain rank for a specific alternative when no weight information is available smaa topsis can discern efficient and inefficient alternatives besides smaa topsis produces the central weight vector to assist in preference expression in more cases uncertainties in criteria pvs and cws will be incorporated into the smaa topsis modelling process via appropriate probability distributions thus the deterministic closeness coefficients become random variables and probabilistic ranking of alternatives is produced 3 results and discussion we apply the proposed methodology to a flood control operation case study consisting of 10 alternatives and five criteria details of the case study are presented below 3 1 risk analysis of flood control operation of the three gorges reservoir the monte carlo simulation method is used to perform the risk analysis for real time flood control operation of the three gorges reservoir main characteristics of the three gorges reservoir are listed in table 2 firstly the multi objective cultured differential evolution model qin et al 2010 is employed to generate 10 feasible flood control operation alternatives of 0 2 frequency typical flood in 1981 secondly assume that forecasted inflow is the mean process of the stochastic inflow and the forecasting errors follow normal distributions 100 000 inflows are sampled via monte carlo simulations thirdly all of the 10 outflow strategies are used for reservoir flood routing for each of the 100 000 sampled inflows five criteria are selected to evaluate the performance of alternatives the peak discharge of reservoir outflow q max and the risk of q max exceeding its threshold p q are used to reflect the safety of downstream control points the highest water level z max and the risk of reservoir overtopping p z are used to reflect the safety of the three gorges reservoir the end water level z e is used to balance flood control and water use among these criteria z max q max and z e are quantified as random variables following normal distributions after conducting goodness of fit tests then p z and p q are calculated by performing statistical analysis p z n z n p q n q n after 100 000 monte carlo simulations where n z and n q represent the number of times that z max and q max exceed their thresholds n is the number of simulations finally we can obtain the stochastic decision matrix consisting of 10 alternatives and five criteria as shown in table 3 it is clear that p q and the mean value of q max show an increasing trend while the other three criteria show an opposite trend in next analyses we use the stochastic decision matrix as the input to the smaa topsis model to demonstrate the effectiveness and advantage of the proposed methodology 3 2 no weight information in the beginning of group decision making process decision makers may be unable to express their preference information due to time pressure or difficulty of the problem when no weight information is available the stochastic decision matrix and the basic weight space are used as model inputs we run the smaa topsis model to compute the rank acceptability indices and central weight vectors via the lhs based monte carlo simulation algorithm 100 000 simulations the rank acceptability indices are illustrated in fig 8 to visually show the probabilities that each alternative obtains the rank from 1 to 10 fig 8 indicates that the rank acceptability indices of alternatives a1 a2 a9 and a10 are pretty small for best ranks and quite large for worst ranks for example a1 has quite large rank acceptability indices for the worst ranks 55 6 for rank 10 11 5 for rank 9 23 0 for rank 8 and they add up to 90 1 and has quite small rank acceptability indices for the best ranks 0 2 for rank 1 0 3 for rank 2 the results indicate that a1 a2 a9 and a10 can be identified as inefficient alternatives since they are almost impossible to obtain the best rank after exploring the basic weight space the rank acceptability indices for rank 10 of alternatives a3 to a8 equal 0 indicating that these six alternatives never obtain the last rank in the 100 000 monte carlo simulations besides these six alternatives have better rank acceptability indices for intermediate ranks efficient alternatives are not obvious in this case study because none of the 10 alternatives has a particularly large first rank acceptability index in conclusion despite no weight information smaa topsis can make a preliminary discrimination for inefficient alternatives which can be eliminated from the alternative set beforehand the central weight vectors are given in fig 9 according to the definition in equation 17 an alternative will have a larger probability to get the best rank if the sampled weight vector is closer to its central weight vector in the case study all of the criteria are cost type i e the smaller the better hence those criteria with small pvs should be assigned with relatively large weights to guarantee the best rank for a specific alternative for instance a1 has the maximum values of q max and p q among all alternatives while the values of z max p z and z e are the smallest thus a1 is the most unfavourable alternative for the safety of downstream flood control sections but it is the most favourable for the safety of the three gorges reservoir consequently smaa topsis assigns small weights for q max and p q 0 026 for q max 0 002 for p q and large weights for z max p z z e 0 399 for z max 0 201 for p z 0 372 for z e to let a1 be the best alternative overall the central weight vectors in fig 9 rationally reflect the weighting rule when each alternative gets the first rank due to the irreversibility of flood control operation problems decision makers usually face huge psychological pressure about cws because mcdm results depend greatly on the weight information and will have significant impacts on the safety of reservoirs and downstream flood control sections in conclusion even with no weight information smaa topsis can still provide valuable results like figs 8 and 9 which are useful to help decision makers express their preferences considering various flood control situations 3 3 fws assessment four flood control experts are invited to express their preferences in the form of triangular fuzzy numbers by analyzing current flood control situations as well as the obtained central weight vectors table 4 shows the fuzzy judgment matrices obtained from the experts then the weight vectors of the four experts are quantified using eqs 7 10 and illustrated by the radar graph in fig 10 it is clear that although the flood control experts are more concerned about the safety of downstream regions i e relatively large weights for p q and q max and relatively small weights for p z z max and z e the weight vectors differ from each other as illustrated in fig 10 the fws is generated using eq 4 with α 0 2 to cover all experts preference information in this study cws are defined as random variables following uniform and normal distributions within the fws in addition we average the cws to get a deterministic weight vector the deterministic weights serve as mean values of normal distributions and the standard deviations equal 1 10 of the mean values the resulting cws will have a probability of 95 44 to fall within the fws according to the 3σ rule of normal distributions the three types of cws are given in table 5 during each monte carlo simulation weight vectors are sampled from the fws and re normalized to ensure that the sum of cws equals 1 3 4 smaa topsis versus smaa 2 in this section the stochastic decision matrix listed in table 3 and the three types of cws shown in table 5 serve as the inputs of smaa topsis and smaa 2 the rank acceptability indices are compared in fig 11 it can be seen that smaa topsis makes a better discrimination for efficient and inefficient alternatives compared with fig 8 actually each alternative can get arbitrary ranks with certain probabilities rather than a single rank in deterministic cases taking fig 11 e for an example the confidence factors and the first rank acceptability indices of alternatives a1 to a4 and a8 to a10 equal 0 indicating that these seven alternatives are impossible to get the first rank alternative a1 and a10 have 100 rank acceptability indices for rank 10 and rank 9 indicating that these two alternatives must be the worst alternatives and should be deleted directly besides a2 has a probability of 91 1 for rank 7 a4 has a probability of 98 9 for rank 4 a5 has a probability of 97 8 for rank 3 a9 has a probability of 91 1 for rank 8 indicating that these four alternatives have pretty large probabilities for their respective ranks and are almost impossible to get other ranks alternatives a2 to a6 never get the ranks after rank 7 because their rank acceptability indices for the worst ranks after rank 7 equal 0 here we just take fig 11 e as an example for results interpretation similar analyses can be undertaken for other cases as well by comparing the rank acceptability indices of smaa topsis and smaa 2 it can be seen that when smaa topsis is used with deterministic cws see fig 11 a the alternatives have pretty large probabilities to obtain certain ranks e g b 1 10 100 b 2 7 100 b 3 5 99 6 b 4 4 99 8 b 5 3 99 5 b 6 1 63 6 b 7 2 63 5 b 8 6 99 6 b 9 8 100 b 10 9 100 the ranks of alternatives are very clear and the alternatives are almost impossible to get other ranks however when smaa 2 is used with deterministic cws see fig 11 b alternatives a3 to a8 have certain probabilities to obtain several ranks and the ranking results are disperse and not clear similarly the same conclusion can also be reached when using the other two kinds of cws see fig 11 c and d fig 11 e and f the results demonstrates the superiority of the smaa topsis over the original smaa 2 i e the smaa topsis model leads to more distinctive rank acceptability indices than smaa 2 the holistic acceptability indices of smaa topsis and smaa 2 are compared in fig 12 to measure the overall performance of alternatives the full ranking sequence of the 10 alternatives determined by smaa topsis are a6 a7 a5 a4 a3 a8 a2 a9 a10 a1 while smaa 2 gives the full ranking sequence as a6 a5 a7 a4 a3 a8 a2 a9 a10 a1 the ranking results of smaa topsis and smaa 2 are basically consistent and reasonable since a6 with a zero p z and p q favoring the subjective preferences obtains the best rank while a1 with a large p q and a9 a10 with a large p z get the worst ranks fig 13 illustrates the risks of decision making errors determined by smaa topsis and smaa 2 it is clear that stochastic mcdm for flood control operation faces certain risks of decision making errors smaa topsis produces considerably lower risks than smaa 2 which indicates that using the topsis type utility function can reduce the influence of uncertainties on mcdm results as well as reduce the risks of decision making errors comparing the sub plots a c and e in fig 11 there is no significant difference between the rank acceptability indices of the deterministic and stochastic cws particularly for alternatives a4 to a7 this behaviour can be also observed from fig 12 a by comparing the holistic acceptability indices under different types of cws furthermore considering the risk of decision making errors as shown in fig 13 the deterministic and stochastic cws leads to almost same risks i e 1 35 for deterministic weights 1 39 and 1 38 for stochastic weights the average difference is 0 035 the results indicate that smaa topsis is insensitive to the type of cws used the uncertainty of criteria pvs has a dominant influence on the smaa topsis results this finding highlights the importance of performing risk analysis to quantify related uncertainties before mcdm 3 5 smaa topsis versus the basic topsis model fig 14 compares the results of smaa topsis and the basic topsis model when using the basic topsis model we do not consider uncertainties in the pvs and cws that is the mean values of probability distributions serve as model inputs as can be seen the basic topsis model produces deterministic closeness coefficients whereas the closeness coefficients determined by smaa topsis are quantified as normally distributed random variables via goodness of fit tests the results indicate that the uncertainty level of each closeness coefficient differs from each other and deterministic closeness coefficients approximatively equal the mean values of their respective distributions smaa topsis enables the basic topsis model to deal with uncertain information via probability distributions namely to make topsis stochastic in fact smaa topsis can be regarded as a stochastic version of topsis as well as a new smaa variant with a topsis type utility function 3 6 robustness analysis of the sampling based algorithm for smaa topsis in this paper we propose the lhs based monte carlo simulation algorithm to solve the smaa topsis model numerically see appendix one of the most important characteristics of any newly developed method for stochastic mcdm is its robustness this fact becomes crucial when performing sampling based analyses sheikholeslami and razavi 2017 therefore we carry out robustness analysis to assess the performance of the proposed algorithm in terms of its reliability against sampling variability specifically we use the stochastic decision matrix table 3 and the uniformly distributed cws shown in table 5 as model inputs we run the smaa topsis model with 100 replicates by setting different random seeds each of the replicates is conducted individually using the lhs based monte carlo simulation algorithm we then evaluate the robustness of the algorithm by performing statistical analysis for the rank acceptability index holistic acceptability index risk of decision making errors and execution time first we select three rank acceptability indices i e b 3 6 b 6 1 b 5 3 with low medium and high values to perform the analysis fig 15 illustrates the box plot of these rank acceptability indices under the 100 replicates as can be seen all of the three indices vary within very small ranges the enlarged box plots in fig 15 exhibit the confidence intervals and standard deviations of the rank acceptability indices the results indicate that the proposed algorithm is robust to sampling variability in terms of the rank acceptability index similarly three holistic acceptability indices with different values low medium and high are also selected to test the algorithm s robustness according to fig 16 the holistic acceptability indices vary within smaller ranges compared to the rank acceptability indices and the box plots almost narrow into lines as can be seen from the enlarged box plots in fig 16 the holistic acceptability indices of a8 and a5 can only take three values with a difference of 0 001 in addition the standard deviations are quite small as well the results suggest that the proposed algorithm shows good robustness in terms of the holistic acceptability index the robustness analysis is performed on a personal computer with an intel xeon e3 1505m eight core 2 8 ghz cpu and 16 gb ram for each replicate we compute the risk of decision making errors and record the execution time as shown in fig 17 the risk of decision making errors varies within a very small range with a standard deviation of 0 00017 the mean value of the execution time is 8 92 s for 100 replicates when implementing 100 000 monte carlo iterations which is fast enough for real time reservoir operations overall the results demonstrate the efficiency and robustness of the proposed algorithm for solving the smaa topsis model 4 summary and conclusions this paper introduced an innovative framework for solving stochastic mcdm problems when there are uncertainties in criteria pvs and cws simultaneously the formulation of mcdm problems was defined we discussed the methods for handling uncertainties in criteria pvs moreover the fws coupled with fahp was proposed to quantify the weight uncertainties as probability distributions with interval constraints we established the smaa topsis model by combining smaa 2 and topsis to assess the effect of uncertainties on mcdm results we proposed the concept of the risk of decision making errors as well as its quantitative calculation method in addition the lhs based monte carlo simulation algorithm and corresponding computer program were developed for solving the smaa topsis model we suggested a three stage mcdm procedure for stochastic mcdm problems and summarized the difference between smaa topsis and deterministic mcdm models the results from the case study indicate that 1 despite no weight information smaa topsis can make a preliminary discrimination for inefficient alternatives and provide valuable reference to help decision makers express their preferences 2 fws coupled with fahp is useful in covering decision makers preferences and avoiding information loss which allows all expected uncertainty and subjectivity in the cws to be considered during the mcdm modelling process 3 when the weight information is considered smaa topsis can make a better discrimination for efficient and inefficient alternatives each alternative can get arbitrary ranks with certain probabilities rather than a single rank in deterministic cases 4 smaa topsis produces significantly lower risks than smaa 2 indicating that using topsis type utility function can reduce the impact of uncertainties as well as reduce the risks of decision making errors 5 smaa topsis can be characterized as an extension of topsis to handle uncertain information i e a stochastic version of topsis in addition the integrated method can be regarded as a new smaa variant with a topsis type utility function as well 6 the results of robustness analysis demonstrate the efficiency and robustness of the lhs based monte carlo simulation algorithm 7 the proposed methodology can provide significant risk information and help decision makers make risk informed decisions with higher reliabilities the proposed methodology is applicable in many kinds of real world mcdm problems due to its multiple advantages first it is suitable for multi criteria group decision making situations where decision makers with conflicting interests are unable or unwilling to provide their preferences or it is hard for them to reach an agreement over the cws in such cases fws coupled with fahp allows for extending the weight vector form one point to a subspace and quantifying the weight uncertainties as probability distributions in real life problems decision makers can use the uniform distribution for simplicity or when they are not sure about their preferences second smaa topsis supports a very flexible way to model different types of uncertain or missing information through any type of probability distributions third the smaa topsis computations can be implemented very efficiently via the lhs based monte carlo simulation algorithm making it easy to apply the proposed methodology in many other decision making contexts in real life applications the proposed methodology can be easily applied to decision support systems for supporting group decision making under uncertainty acknowledgments this work was supported by the national key r d program of china grant no 2017yfc0405606 the national natural science foundation of china grant no 51579068 the special fund for public welfare industry of the ministry of water resources of china grant no 201501007 the fundamental research funds for the central universities grant no 2017b40614 china postdoctoral science foundation funded project grant no 2017m611864 the first author was also supported by a fellowship from the china scholarship council for his visit to the university of california los angeles appendix the lhs based monte carlo simulation algorithm and program description monte carlo simulation is a numerical method for calculating multi dimensional integrals in this method the required number of iterations is inversely proportional to the square of the expected accuracy but does not depend on the dimensionality of integrals significantly tervonen and lahdelma 2007 therefore monte carlo simulation can achieve a precision of a few decimal places with reasonable effort this paper develops a lhs based monte carlo simulation algorithm and corresponding computer program to solve the smaa topsis model pseudo code of the proposed algorithm is provided at the end of the appendix the following symbols are used h i j is the number of times that alternative a i obtains the rank j in monte carlo simulations k c is the total number of iterations r r 1 r 2 r m represents the vector of ranks c c 1 c 2 c m denotes the vector of closeness coefficients the following functions and subroutines are defined in the algorithm lhs x function is used to generate a stochastic decision matrix of size m n from the criteria pv distribution f x each row of the matrix is the criteria pv vector of a certain alternative elements in the matrix are generated from their respective distributions using the lhs technique the distributions may have arbitrary types such as uniform normal lognormal and etc lhs w function returns a weight vector from the weight distribution f w weights are sampled from the fws using the lhs technique and re normalized to ensure that the sum of weights equals to 1 topsis x i w function returns the closeness coefficient c i of alternative a i in which eqs 19 13 are executed rank c function returns a vector of ranks based on the values in vector c for example if c 0 4 0 3 0 7 the resulting rank vector rank c 2 3 1 in order to calculate the rank acceptability index b i r and the central weight vector w i c multi dimensional integrals over criteria and weight distributions need to be computed straight forward calculation requires m n times of monte carlo iterations for b i r and m iterations for w i c which is complex and time consuming the proposed algorithm can speed up the calculation process remarkably by determining the ranks that all alternatives obtained in a single run and then updating b i r and w i c successively similarly we must integrate over the criteria distribution regarding the central weight vectors to calculate the confidence factor p i c direct calculation of p i c requires executing the iteration m times once for each alternative the proposed algorithm can determine all integrals simultaneously in a single run by evaluating for each alternative whether the alternative is the best one when using its central weight vector the proposed algorithm is coded by the combination of r project visual basic 6 0 and visual basic for applications firstly an r package developed by carnell 2016 is used for implementing the lhs method to generate random numbers from probability distributions of criteria pvs and cws the benefit of using r is that it provides a powerful software environment for statistical computing and a large number of simulated values can be easily obtained with small effort secondly the main part of the algorithm is written in visual basic 6 0 which allow for analysis and management of data as well as model computations finally all of the model outputs are collected in microsoft excel then visual basic for applications the programming language incorporated in excel is used for automatic plotting and result visualization pseudo code of the lhs based monte carlo simulation algorithm image 1 appendix a supplementary data the following is the supplementary data related to this article supplementary material supplementary material appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2017 11 032 
26461,this paper proposes an innovative framework for solving stochastic multi criteria decision making mcdm problems when uncertainties exist in criteria performance values pvs and criteria weights cws simultaneously methods for quantifying uncertainties in criteria pvs and cws are presented we establish the smaa topsis model by combining stochastic multicriteria acceptability analysis smaa and technique for order preference by similarity to ideal solution topsis the risk of decision making errors is proposed to assess the impact of uncertainties on mcdm we develop the lhs based monte carlo simulation algorithm and corresponding computer program for solving the smaa topsis model we also suggest a three stage mcdm procedure for stochastic mcdm problems we apply the proposed methodology to a flood control operation case study to demonstrate its applicability our results indicate that the proposed methods can provide valuable risk information and enable risk informed decisions to be made with higher reliabilities keywords multi criteria decision making flood control operation stochastic multicriteria acceptability analysis monte carlo simulation software availability name of software smaa topsis for stochastic mcdm developer feilin zhu email zhufeilin hhu edu cn software required visual basic 6 0 visual basic for applications r project availability the software is freely available for noncommercial use upon request from the first author 1 introduction multi criteria decision making mcdm is a methodology which has been extensively employed to assist water resources and environmental decision making fowler et al 2015 ganji et al 2016 hyde and maier 2006 romañach et al 2014 su and tung 2014 since it facilitates multi stakeholder participation and allows the consideration of multiple criteria measured in incommensurable units reservoir flood control operation requires concurrent optimization of several conflicting objectives such as hydropower generation flood control water supply irrigation and etc it is difficult to determine an optimal solution that optimizes all objectives simultaneously luo et al 2015 qin et al 2010 instead mcdm methods are typically employed to evaluate and select the non inferior alternatives generated by multi objective optimization models so as to obtain the most preferred alternative and put it into practice hajkowicz and collins 2007 reviewed applications of mcdm methods for a diverse range of water resources problems and classified these methods into six categories 1 multi criteria value functions 2 outranking approaches 3 distance to ideal point methods 4 pairwise comparisons 5 fuzzy set analysis and 6 tailored methods in reservoir flood control operation uncertainties mainly come from two aspects including the uncertainty of criteria performance values pvs and criteria weights cws on one hand numerous uncertainty factors e g inflow forecasting errors reservoir capacity curve errors river flood routing errors and etc lead to the randomness of flood control target factors which usually serve as criteria to evaluate the performance of alternatives when considering uncertainties these target factors are no longer constants but random variables with certain probability distributions since the 1980s hydrologists have paid much attention to the risk analysis of flood control operation bogner and pappenberger 2011 chen et al 2015 yan et al 2014 however little research attempted to combine risk analysis with mcdm models on the other hand cws used to measure criteria s relative importance and coordinate multiple operation objectives have been found to be a potential source of uncertainty in mcdm ganji et al 2016 the cws can be either subjective or objective in the context of group decision making all decision makers need to express subjective preferences to incorporate their knowledge and experience into mcdm models it is difficult for multiple decision makers with conflicting interests to reach consensus over cws de brito and evers 2016 madani and lund 2011 moreover the weighting process usually contains fuzziness and subjectivity of human judgments which will lead to imprecise or uncertain cws objective cws are determined based on the information of decision matrices and it has been found that different cws elicitation methods may elicit diverse cws hajkowicz and collins 2007 hyde and maier 2006 zhu et al 2017 furthermore considerable information loss will occur and the real uncertainty of cws will be concealed if cws obtained from multiple decision makers or diverse methods are aggregated or averaged as a deterministic cw cai et al 2004 therefore uncertainties exist in criteria pvs and cws simultaneously during the mcdm modelling process conventional mcdm methods for reservoir flood control operation are mainly developed and applied under deterministic or fuzzy environments chen and hou 2004 cheng and chau 2002 fu 2008 wang et al 2011 zhu et al 2016 in deterministic or fuzzy cases an absolutely fixed ranking of alternatives is obtained however when considering uncertainties in input parameters i e criteria pvs and cws any alternative is likely to get better or worse ranks and a reversal of the fixed ranking will occur which further lead to the risk of decision making errors xiong and qi 2010 presented a method for stochastic mcdm problems with incomplete weight information in this method a stochastic mcdm problem was converted into an interval mcdm problem by interval estimation qin 2011 introduced a relative dominance based mcdm method when criteria pvs are normal random variables qin s study is meaningful since it first combines risk analysis with mcdm models but they disregard the uncertainty in cws and limit criteria pvs as normal random variables in addition madani and lund 2011 proposed a monte carlo game theory mcgt approach for dealing with uncertainty in the performance of alternatives which mapped the stochastic mcdm problem into many deterministic strategic games and solved them using non cooperative stability definitions however little attention has been paid to consider uncertainties in criteria pvs and cws simultaneously the subject of multi criteria group decision making under uncertainty has not yet been addressed thoroughly in a unified framework stochastic multicriteria acceptability analysis smaa is a family of methods for assisting mulita criteria group decision making in situations where criteria pvs and cws are uncertain lahdelma et al 1998 plenty of smaa variants have been developed such as smaa 2 lahdelma and salminen 2001 smaa ahp durbach et al 2014 smaa promethee corrente et al 2014 and so on smaa 2 forms the basis of other smaa variants and is regarded as the most representative one in smaa 2 an additive utility function is used to represent decision makers preference and map different alternatives to real values in reality any type of utility function can be used in smaa 2 namely smaa 2 can be applied in combination with other mcdm methods the technique for order preference by similarity to ideal solution topsis first introduced by hwang and yoon 1981 has been extensively applied to water resources and environmental problems zagonari and rossi 2013 based on the concepts of ideal and anti ideal points the best alternative determined by topsis should be the one which is simultaneously closest to the ideal alternative and farthest from the anti ideal alternative topsis is recommended by the united nation environmental program unep to evaluate water resources development projects moreover some fuzzy versions of topsis have also been developed triantaphyllou and lin 1996 torfi et al 2010 okul et al 2014 presented the idea of integrating the smaa theory and topsis method and they applied the combined method to the problem of light machine gun selection the main purpose of their study was to improve the basic topsis method and allow topsis to handle imprecise data however the issue of how to deal with related uncertainties was ignored in addition the superiority of smaa topsis was not well evaluated this paper proposes an innovative framework for solving stochastic mcdm problems when uncertainties exist in criteria pvs and cws simultaneously this helps to allow all expected uncertainties to be incorporated into the mcdm modelling process and make risk informed decisions with higher reliabilities first we define the formulation of mcdm problems methods for quantifying uncertainties in criteria pvs are discussed we introduce the feasible weight space fws in combination with fuzzy analytic hierarchy process fahp to quantify the weight uncertainties we then establish the smaa topsis model by integrating topsis and smaa 2 moreover we propose the concept of the risk of decision making errors and its quantitative calculation method to assess the effect of existing uncertainties on mcdm results the monte carlo simulation algorithm based on latin hypercube sampling lhs and corresponding computer program are developed for solving the smaa topsis model in addition we suggest a three stage mcdm procedure for stochastic mcdm problems we summarize the difference between smaa topsis and deterministic mcdm models the proposed methodology is applied to a flood control operation case study to demonstrate its applicability and insights it can provide beyond traditional methods the remainder of this paper is organized as follows the proposed methodology is presented in section 2 followed by results and discussion of the case study in section 3 section 4 contains summary and conclusions 2 methodology fig 1 shows the flowchart of the proposed methodology details of each step are presented in the following subsections 2 1 formulation of mcdm problems there are many terminologies used to refer to mcdm some other terms include multi criteria decision analysis mcda multi attribute decision making madm and multi objective decision support mods all these terms share the same theoretical basis and are jointly referred to in this paper as mcdm generally an mcdm problem comprises 1 a set of alternatives which need to be evaluated ranked and selected 2 a set of criteria measured in incommensurable units and 3 an mcdm model the set of alternatives can be either implicitly defined by constraints in multi objective optimization models or explicitly defined and discrete in number durbach and stewart 2012 each criterion should be associated with a measurable attribute which provides a qualitative or quantitative scale for assessing performances of alternatives this can be done via mathematical models and or expert judgment mcdm models basically differ depend on 1 how they determine marginal evaluation on each criterion and 2 how they aggregate marginal evaluations across criteria to achieve a global evaluation formulation of an mcdm problem includes the identification of alternatives criteria and criteria weights in the rest of this paper we consider an mcdm problem consisting of m alternatives and n criteria alternative set is denoted as a a 1 a 2 a m and criterion set as c c 1 c 2 c n w w 1 w 2 w n represents the weight vector let x ij be the marginal evaluation of alternative a i with respect to criterion c j and the decision matrix can be expressed as x x i j m n where i and j denote index of the alternative and criterion respectively the global evaluation g i measures the overall performance of alternative a i with respect to all criteria by which the ranking results are determined here we take the weighted sum method which is one of the simplest mcdm methods as an example when using this method the global evaluation g i can be expressed as 1 g i j 1 n x i j w j as a result of the above definition the best alternative is the one that corresponds to the largest global evaluation g i generally the essence of an mcdm problem can be concluded as transforming multiple criteria values i e marginal evaluation into a single overall measure i e global evaluation by which all alternatives are evaluated ranked and selected 2 2 handling uncertainties in the criteria pvs in reservoir flood control operation uncertainties come from many aspects such as flood forecasting errors water level at flood conditions outflow discharge capacity delay time of operation and so on these uncertainty factors result in the randomness of flood control target factors e g the peak discharge from the reservoir the highest water level the end water level and etc which further cause risks for flood control decision making since the 1980s risk analysis of flood control operation has attracted much attention from academia and plenty of methods have been proposed for assessing risks bogner and pappenberger 2011 chen et al 2015 yan et al 2014 generally risk analysis methods can be classified as stochastic differential equation method direct integration method structural reliability analysis method fuzzy sets based method and monte carlo simulation and they differ in terms of mathematical complexity and data requirements before mcdm for reservoir flood control operation a crucial issue is to quantify the uncertainties in criteria pvs namely to determine the probability distributions of flood control target factors using risk analysis approaches in a stochastic mcdm problem deterministic criteria pvs x i j are replaced by random variables ξ i j with estimated probability distribution functions accordingly the stochastic decision matrix can be expressed as 2 x ξ i j m n ξ 11 ξ 12 ξ 1 n ξ 21 ξ 22 ξ 2 n ξ m 1 ξ m 2 ξ m n 2 3 handling uncertainties in the cws in the following we introduce the fws coupled with fahp to quantify the weight uncertainties as probability distributions with interval constraints 2 3 1 feasible weight space under the condition of no cws we define the basic weight space as a union of all possible weight vectors in a three criterion case the basic weight space should be a triangular plane shown in fig 2 a when the weight vector is deterministic the basic weight space shrinks to a point w 1 w 2 w 3 on the plane as shown in fig 2 b in the basic weight space only one point is usually insufficient to represent the preferences from a group of decision makers therefore the interval estimation of cws is more practical and appropriate than point estimations this is our motivation to propose the concept of fws which can be regarded as a subspace in the basic weight space as shown in fig 2 c when cws follow the uniform distribution with interval constraints the fws is represented by a hexagonal plane and can be formulated as 3 f w s w r 3 w j 0 w j min w j w j max j 1 3 w j 1 smaa topsis is flexible to model weight uncertainties via probability distributions arbitrary kind of probability distribution can be used in smaa topsis and the resulting fws is demonstrated as an arbitrary shape plane in fig 2 d here we just take the three criterion case for an example the concept of fws can be extended to higher dimension cases as illustrated in fig 3 we also use a bar graph to illustrate the fws consider a multi criteria group decision making problem with three criteria in which three flood control experts are invited to express preferences fig 3 shows that the cws are usually different from each other the red dashed line and solid line represent the maximum and minimum constraints of the cws respectively as can be seen fws is not just defined as the interval between the red lines but a wider region over the interval extended to n dimensional case fws can be formulated as follows 4 f w s w r n w j 0 w j min α w j max w j min w j w j max α w j max w j min j 1 n w j 1 where we use the parameter α to reflect the confidence degree of experts for the fws assessment if experts are confident about their cws α can be assigned with a small value or even zero otherwise a large α representing a higher uncertainty should be used to generate a wider fws according to our experience the α with the range of 0 1 0 4 is appropriate in practical applications in conclusion fahp is employed to help flood control experts elicit cws and fws is then used to extend the weight vector form one point to a subspace in group decision making the fws is useful to cover all preferences and avoid information loss the weight uncertainties are quantified by probability distributions with interval constraints which allow all expected uncertainty and subjectivity in the cws to be incorporated in the next section we introduce the fahp for weight assessment 2 3 2 fuzzy analytic hierarchy process for weight assessment fahp is an extension of classical ahp considering the fuzziness in decision makers subjective judgments triangular fuzzy number is often used in combination with fahp to transform fuzzy or imprecise linguistic variables into crisp values according to fuzzy sets theory a fuzzy number a denoted as l m u becomes a triangular fuzzy number if its membership function μ x satisfies 5 μ x x l m l l x m x u m u m x u 0 o t h e r w i s e where l and u represent lower and upper bounds of a respectively m is the point where the membership degree is equal to 1 in this paper we use fahp coupled with triangular fuzzy numbers to obtain all decision makers preferences and quantify the preference information as quantitative cws namely to determine the bars in fig 3 the following steps are involved step 1 expert participation and construction of fuzzy judgment matrix in real time flood control operation multiple flood control experts are invited to express their preference information according to the rating scale for pairwise comparisons shown in table 1 the judgment results are represented by a triangular fuzzy judgment matrix 6 a a i j m n a 11 a 12 a 1 n a 21 a 22 a 2 n a n 1 a n 2 a n n where a i j l i j m i j u i j step 2 computation of the initial weight firstly summarize each row of the fuzzy judgment matrix secondly normalize the row sums by the following fuzzy arithmetic operations 7 s i j 1 n a i j k 1 n j 1 n a i j 1 j 1 n l i j k 1 n j 1 n u k j j 1 n m i j k 1 n j 1 n m k j j 1 n u i j k 1 n j 1 n l k j step 3 defuzzification we use the formula recommended by chang 1996 to calculate the degree of possibility of s i over all other n 1 fuzzy numbers to get a crisp value 8 v s i s j j 1 2 n j i min j 1 2 n j i v s i s j where s i l i m i u i and s j l j m j u j v s i s j is the degree of possibility of s i s j which is illustrated in fig 4 and expressed as follows 9 v s i s j 1 u i l j u i m i m j l j 0 m i m j l j u i o t h e r w i s e step 4 normalization finally we can obtain the weight vector w w 1 w 2 w n by the following formula 10 w i v s i s j j 1 2 n j i k 1 n v s k s j j 1 2 n j k 2 4 the basic smaa 2 model in smaa 2 a real valued utility function u is employed to map different alternatives to utility values 11 u i u x i w where x i represents the criteria pv vector of alternative a i smaa 2 is developed especially for mcdm problems with uncertain or missing information uncertain criteria pvs are represented by random variables ξ i j with assumed or estimated joint probability distribution function f x ξ in the space x r m n similarly uncertain cws are represented by weight distributions with joint probability distribution function f w w in the weight space w defined as follows 12 w w r n w j 0 j 1 n w j 1 the ranking function is used to determine the rank of each alternative from the best rank 1 to the worst rank m as follows 13 rank ξ i w 1 k 1 m ρ u ξ k w u ξ i w where ρ t r u e 1 and ρ f a l s e 0 ξ i denotes the stochastic criteria pv vector of alternative a i the favourable rank weights w i r ξ is an important concept in smaa 2 a weight vector w w i r ξ assigns utilities for alternative a i to guarantee it obtains rank r w i r ξ can be expressed as follows 14 w i r ξ w w rank ξ i w r in smaa 2 stochastic criteria pvs and cws serve as model inputs based on the analysis of w i r ξ four descriptive measures are provided to decision makers including the rank acceptability index the holistic acceptability index the central weight vector and the confidence factor the rank acceptability index b i r is a measure of the variety of different preferences that grant alternative a i rank r b i r is calculated as a multi dimensional integral over the criteria distributions and the favourable rank weights as follows 15 b i r x f x ξ w i r ξ f w w d w d ξ the holistic acceptability index a i h considers all rank acceptability indices and examines the overall acceptability of each alternative 16 a i h r 1 m α r b i r where α r are so called meta weights which indicate the contribution of each rank acceptability index to the holistic acceptability index there are three kinds of meta weights for choice i e linear weights α r m r m 1 inverse weights α r 1 r and centroid weights α r i r m 1 i i 1 m 1 i as shown in fig 5 the central weight vector w i c is the expected center of gravity of the favourable first rank weight space the central weight vector represents the typical weight vector supporting alternative a i and is calculated as a multi dimensional integral over the criteria distributions and the favourable first rank weights 17 w i c x f x ξ w i 1 ξ f w w w d w d ξ b i 1 the confidence factor p i c is defined as the probability that an alternative obtains the first rank when its central weight vector is used p i c measures whether the criteria pvs are accurate enough to discern efficient alternatives it is calculated as a multi dimensional integral over the criteria distributions as 18 p i c ξ x rank ξ w i c 1 f x ξ d ξ 2 5 the smaa topsis model the smaa topsis model is established by combing smaa 2 and topsis the main advantages of using topsis lie in 1 its robust logical structure 2 its powerful computation capability and 3 its ability to consider ideal and anti ideal points concurrently instead of the additive utility function in the original smaa 2 model the topsis algorithm is used main steps of the topsis type utility function are as follows step 1 normalize the decision matrix x x i j m n to y y i j m n by the vector normalization formula 19 y i j x i j i 1 m x i j 2 step 2 multiply the normalized decision matrix y y i j m n by the weight vector w w 1 w 2 w n to obtain the weighted decision matrix z z i j m n where z i j y i j w j step 3 determine the ideal alternative s s 1 s 2 s n and anti ideal alternative s s 1 s 2 s n as follows 20 s j max 1 i m z i j for benefit criteria min 1 i m z i j for cost criteria 21 s j min 1 i m z i j for benefit criteria max 1 i m z i j for cost criteria step 4 calculate the euclidean distances from alternative a i to the ideal d i and anti ideal d i alternatives respectively 22 d i j 1 n s j z i j 2 i 1 2 m d i j 1 n s j z i j 2 i 1 2 m step 5 the closeness coefficient c i provides the global evaluation for alternative a i regarding all criteria as mentioned in section 2 1 the best alternative is the one that corresponds to the largest c i the closeness coefficient can be calculated by 23 c i d i d i d i 2 6 the risk of decision making errors the basic topsis model produces deterministic closeness coefficient to evaluate candidate alternatives and a fixed ranking is obtained however due to the influence of uncertainties the closeness coefficients will no longer be constants but random variables fluctuating around their mean values as illustrated in fig 6 it can be seen from fig 6 that any alternative is likely to obtain better or worse ranks and a reversal of the fixed ranking will occur which further lead to the risks of decision making errors in reservoir flood control operation the best alternative is usually concerned by decision makers and will be adopted into flood control practice hence it will cause more serious flood control consequences if worse alternatives get the first rank due to the uncertainties strictly speaking risk should be defined as a function of the probability of an uncertain event happening and its consequences however it is usually difficult to estimate the consequences of an uncertain event especially when the pre event evaluation is required i e before the uncertain event happens therefore the risk definition is simplified as the probability of an uncertain event happening which has been widely accepted in the field of water resources management chen et al 2015 yan et al 2014 in order to assess the ranking uncertainty we define the risk of decision making errors as the weighted probability that non optimal alternatives get the first rank 24 p f k 2 m β k b k 1 where b k 1 represents the first rank acceptability index of the alternative which gets the kth rank when using the basic topsis model β k k 2 3 m is defined as risk weights to measure the contribution of each non optimal alternative to p f the parameter β k is used to reflect the characteristic that worse alternatives obtaining the first rank will lead to greater risks we define β k as an incremental nonnegative and m 1 dimensional vector 25 β k k k 2 m k the risk of decision making errors can be viewed as a new descriptive measure of smaa topsis in addition to the other four measures i e the rank acceptability index the holistic acceptability index the central weight vector and the confidence factor in group decision making processes all of these descriptive measures should be jointly analyzed before making a final decision it should be noted that the multi dimensional integrals in eqs 15 18 and 24 are difficult to be calculated analytically since the distributions f x and f w can be complex in real world applications besides the integrals will have a quite high dimension for the mcdm problems with a large number of alternatives or criteria hence this paper proposes a lhs based monte carlo simulation algorithm to numerically solve the smaa topsis model in which lhs is used due to its sampling efficiency sheikholeslami and razavi 2017 the developed algorithm and program are presented in appendix 2 7 three stage mcdm procedure under deterministic situations the best alternative is determined based on the fixed ranking of all candidate alternatives which can be referred to as the single value and deterministic rank mcdm procedure although smaa 2 allows uncertainties in the criteria pvs and cws to be considered the full ranking sequence is still provided to decision makers and used for making final decisions which is actually the same as the single value and deterministic rank mcdm procedure however it is quite unreliable to make a final decision only based on the full ranking sequence since ranking results are heavily subject to uncertainties in other words even the best alternative still has certain probabilities to obtain worse ranks therefore decision makers must explicitly consider the uncertainty of ranking results as well as the corresponding risk of decision making errors in this paper we suggest a three stage mcdm procedure for guiding stochastic mcdm under uncertainty this helps decision makers make risk informed decisions with higher reliabilities stage 1 no preference information is required in the beginning of mcdm modelling processes instead smaa topsis produces the rank acceptability indices to discern efficient and inefficient alternatives via exploring the basic weight space the alternatives a i with a large first rank acceptability index b i 1 can be considered as the efficient alternative since it has a large probability to obtain the best rank contrarily the alternative a i is inefficient if it is dominated by other alternatives since strictly better alternatives exist and this alternative is impossible to get the best rank even exploring the entire weight space in addition the central weight vectors is used to describe what kind of cws will favor what kind of best alternatives then help decision makers express their preferences considering various flood control situations stage 2 in this stage the fws coupled with fahp is used to obtain decision makers preferences and quantify the weight uncertainties as probability distributions with interval constraints the stochastic pvs and cws are incorporated into the smaa topsis modelling process smaa topsis can provide a better discrimination for efficient and inefficient alternatives and determine the probabilities that each alternative obtains each rank stage 3 the holistic acceptability indices are used to determine the full ranking sequence of alternatives in this stage the rank acceptability indices the holistic acceptability indices and the risk of decision making errors should be jointly analyzed to inform decision makers of potential risks before making a final decision 2 8 smaa topsis versus deterministic mcdm models comparison between the smaa topsis model and deterministic mcdm models is shown in fig 7 deterministic mcdm models transform the decision matrix together with weight information into a deterministic overall measure to obtain a fixed ranking of alternatives in traditional mcdm procedures the decision matrix i e criteria pvs the cws and the mcdm model are three indispensable components to guarantee a final decision can be made however a deterministic mcdm model will lose its ability to rank alternatives when uncertainties exist in criteria pvs or cws smaa topsis breaks through the traditional mcdm framework in the early stage of the mcdm modelling process smaa topsis does not require any preference information from decision makers instead the inverse weight space analysis i e exploring the entire weight space is performed to evaluate the cws that make each alternative the best one or that would assign a certain rank for a specific alternative when no weight information is available smaa topsis can discern efficient and inefficient alternatives besides smaa topsis produces the central weight vector to assist in preference expression in more cases uncertainties in criteria pvs and cws will be incorporated into the smaa topsis modelling process via appropriate probability distributions thus the deterministic closeness coefficients become random variables and probabilistic ranking of alternatives is produced 3 results and discussion we apply the proposed methodology to a flood control operation case study consisting of 10 alternatives and five criteria details of the case study are presented below 3 1 risk analysis of flood control operation of the three gorges reservoir the monte carlo simulation method is used to perform the risk analysis for real time flood control operation of the three gorges reservoir main characteristics of the three gorges reservoir are listed in table 2 firstly the multi objective cultured differential evolution model qin et al 2010 is employed to generate 10 feasible flood control operation alternatives of 0 2 frequency typical flood in 1981 secondly assume that forecasted inflow is the mean process of the stochastic inflow and the forecasting errors follow normal distributions 100 000 inflows are sampled via monte carlo simulations thirdly all of the 10 outflow strategies are used for reservoir flood routing for each of the 100 000 sampled inflows five criteria are selected to evaluate the performance of alternatives the peak discharge of reservoir outflow q max and the risk of q max exceeding its threshold p q are used to reflect the safety of downstream control points the highest water level z max and the risk of reservoir overtopping p z are used to reflect the safety of the three gorges reservoir the end water level z e is used to balance flood control and water use among these criteria z max q max and z e are quantified as random variables following normal distributions after conducting goodness of fit tests then p z and p q are calculated by performing statistical analysis p z n z n p q n q n after 100 000 monte carlo simulations where n z and n q represent the number of times that z max and q max exceed their thresholds n is the number of simulations finally we can obtain the stochastic decision matrix consisting of 10 alternatives and five criteria as shown in table 3 it is clear that p q and the mean value of q max show an increasing trend while the other three criteria show an opposite trend in next analyses we use the stochastic decision matrix as the input to the smaa topsis model to demonstrate the effectiveness and advantage of the proposed methodology 3 2 no weight information in the beginning of group decision making process decision makers may be unable to express their preference information due to time pressure or difficulty of the problem when no weight information is available the stochastic decision matrix and the basic weight space are used as model inputs we run the smaa topsis model to compute the rank acceptability indices and central weight vectors via the lhs based monte carlo simulation algorithm 100 000 simulations the rank acceptability indices are illustrated in fig 8 to visually show the probabilities that each alternative obtains the rank from 1 to 10 fig 8 indicates that the rank acceptability indices of alternatives a1 a2 a9 and a10 are pretty small for best ranks and quite large for worst ranks for example a1 has quite large rank acceptability indices for the worst ranks 55 6 for rank 10 11 5 for rank 9 23 0 for rank 8 and they add up to 90 1 and has quite small rank acceptability indices for the best ranks 0 2 for rank 1 0 3 for rank 2 the results indicate that a1 a2 a9 and a10 can be identified as inefficient alternatives since they are almost impossible to obtain the best rank after exploring the basic weight space the rank acceptability indices for rank 10 of alternatives a3 to a8 equal 0 indicating that these six alternatives never obtain the last rank in the 100 000 monte carlo simulations besides these six alternatives have better rank acceptability indices for intermediate ranks efficient alternatives are not obvious in this case study because none of the 10 alternatives has a particularly large first rank acceptability index in conclusion despite no weight information smaa topsis can make a preliminary discrimination for inefficient alternatives which can be eliminated from the alternative set beforehand the central weight vectors are given in fig 9 according to the definition in equation 17 an alternative will have a larger probability to get the best rank if the sampled weight vector is closer to its central weight vector in the case study all of the criteria are cost type i e the smaller the better hence those criteria with small pvs should be assigned with relatively large weights to guarantee the best rank for a specific alternative for instance a1 has the maximum values of q max and p q among all alternatives while the values of z max p z and z e are the smallest thus a1 is the most unfavourable alternative for the safety of downstream flood control sections but it is the most favourable for the safety of the three gorges reservoir consequently smaa topsis assigns small weights for q max and p q 0 026 for q max 0 002 for p q and large weights for z max p z z e 0 399 for z max 0 201 for p z 0 372 for z e to let a1 be the best alternative overall the central weight vectors in fig 9 rationally reflect the weighting rule when each alternative gets the first rank due to the irreversibility of flood control operation problems decision makers usually face huge psychological pressure about cws because mcdm results depend greatly on the weight information and will have significant impacts on the safety of reservoirs and downstream flood control sections in conclusion even with no weight information smaa topsis can still provide valuable results like figs 8 and 9 which are useful to help decision makers express their preferences considering various flood control situations 3 3 fws assessment four flood control experts are invited to express their preferences in the form of triangular fuzzy numbers by analyzing current flood control situations as well as the obtained central weight vectors table 4 shows the fuzzy judgment matrices obtained from the experts then the weight vectors of the four experts are quantified using eqs 7 10 and illustrated by the radar graph in fig 10 it is clear that although the flood control experts are more concerned about the safety of downstream regions i e relatively large weights for p q and q max and relatively small weights for p z z max and z e the weight vectors differ from each other as illustrated in fig 10 the fws is generated using eq 4 with α 0 2 to cover all experts preference information in this study cws are defined as random variables following uniform and normal distributions within the fws in addition we average the cws to get a deterministic weight vector the deterministic weights serve as mean values of normal distributions and the standard deviations equal 1 10 of the mean values the resulting cws will have a probability of 95 44 to fall within the fws according to the 3σ rule of normal distributions the three types of cws are given in table 5 during each monte carlo simulation weight vectors are sampled from the fws and re normalized to ensure that the sum of cws equals 1 3 4 smaa topsis versus smaa 2 in this section the stochastic decision matrix listed in table 3 and the three types of cws shown in table 5 serve as the inputs of smaa topsis and smaa 2 the rank acceptability indices are compared in fig 11 it can be seen that smaa topsis makes a better discrimination for efficient and inefficient alternatives compared with fig 8 actually each alternative can get arbitrary ranks with certain probabilities rather than a single rank in deterministic cases taking fig 11 e for an example the confidence factors and the first rank acceptability indices of alternatives a1 to a4 and a8 to a10 equal 0 indicating that these seven alternatives are impossible to get the first rank alternative a1 and a10 have 100 rank acceptability indices for rank 10 and rank 9 indicating that these two alternatives must be the worst alternatives and should be deleted directly besides a2 has a probability of 91 1 for rank 7 a4 has a probability of 98 9 for rank 4 a5 has a probability of 97 8 for rank 3 a9 has a probability of 91 1 for rank 8 indicating that these four alternatives have pretty large probabilities for their respective ranks and are almost impossible to get other ranks alternatives a2 to a6 never get the ranks after rank 7 because their rank acceptability indices for the worst ranks after rank 7 equal 0 here we just take fig 11 e as an example for results interpretation similar analyses can be undertaken for other cases as well by comparing the rank acceptability indices of smaa topsis and smaa 2 it can be seen that when smaa topsis is used with deterministic cws see fig 11 a the alternatives have pretty large probabilities to obtain certain ranks e g b 1 10 100 b 2 7 100 b 3 5 99 6 b 4 4 99 8 b 5 3 99 5 b 6 1 63 6 b 7 2 63 5 b 8 6 99 6 b 9 8 100 b 10 9 100 the ranks of alternatives are very clear and the alternatives are almost impossible to get other ranks however when smaa 2 is used with deterministic cws see fig 11 b alternatives a3 to a8 have certain probabilities to obtain several ranks and the ranking results are disperse and not clear similarly the same conclusion can also be reached when using the other two kinds of cws see fig 11 c and d fig 11 e and f the results demonstrates the superiority of the smaa topsis over the original smaa 2 i e the smaa topsis model leads to more distinctive rank acceptability indices than smaa 2 the holistic acceptability indices of smaa topsis and smaa 2 are compared in fig 12 to measure the overall performance of alternatives the full ranking sequence of the 10 alternatives determined by smaa topsis are a6 a7 a5 a4 a3 a8 a2 a9 a10 a1 while smaa 2 gives the full ranking sequence as a6 a5 a7 a4 a3 a8 a2 a9 a10 a1 the ranking results of smaa topsis and smaa 2 are basically consistent and reasonable since a6 with a zero p z and p q favoring the subjective preferences obtains the best rank while a1 with a large p q and a9 a10 with a large p z get the worst ranks fig 13 illustrates the risks of decision making errors determined by smaa topsis and smaa 2 it is clear that stochastic mcdm for flood control operation faces certain risks of decision making errors smaa topsis produces considerably lower risks than smaa 2 which indicates that using the topsis type utility function can reduce the influence of uncertainties on mcdm results as well as reduce the risks of decision making errors comparing the sub plots a c and e in fig 11 there is no significant difference between the rank acceptability indices of the deterministic and stochastic cws particularly for alternatives a4 to a7 this behaviour can be also observed from fig 12 a by comparing the holistic acceptability indices under different types of cws furthermore considering the risk of decision making errors as shown in fig 13 the deterministic and stochastic cws leads to almost same risks i e 1 35 for deterministic weights 1 39 and 1 38 for stochastic weights the average difference is 0 035 the results indicate that smaa topsis is insensitive to the type of cws used the uncertainty of criteria pvs has a dominant influence on the smaa topsis results this finding highlights the importance of performing risk analysis to quantify related uncertainties before mcdm 3 5 smaa topsis versus the basic topsis model fig 14 compares the results of smaa topsis and the basic topsis model when using the basic topsis model we do not consider uncertainties in the pvs and cws that is the mean values of probability distributions serve as model inputs as can be seen the basic topsis model produces deterministic closeness coefficients whereas the closeness coefficients determined by smaa topsis are quantified as normally distributed random variables via goodness of fit tests the results indicate that the uncertainty level of each closeness coefficient differs from each other and deterministic closeness coefficients approximatively equal the mean values of their respective distributions smaa topsis enables the basic topsis model to deal with uncertain information via probability distributions namely to make topsis stochastic in fact smaa topsis can be regarded as a stochastic version of topsis as well as a new smaa variant with a topsis type utility function 3 6 robustness analysis of the sampling based algorithm for smaa topsis in this paper we propose the lhs based monte carlo simulation algorithm to solve the smaa topsis model numerically see appendix one of the most important characteristics of any newly developed method for stochastic mcdm is its robustness this fact becomes crucial when performing sampling based analyses sheikholeslami and razavi 2017 therefore we carry out robustness analysis to assess the performance of the proposed algorithm in terms of its reliability against sampling variability specifically we use the stochastic decision matrix table 3 and the uniformly distributed cws shown in table 5 as model inputs we run the smaa topsis model with 100 replicates by setting different random seeds each of the replicates is conducted individually using the lhs based monte carlo simulation algorithm we then evaluate the robustness of the algorithm by performing statistical analysis for the rank acceptability index holistic acceptability index risk of decision making errors and execution time first we select three rank acceptability indices i e b 3 6 b 6 1 b 5 3 with low medium and high values to perform the analysis fig 15 illustrates the box plot of these rank acceptability indices under the 100 replicates as can be seen all of the three indices vary within very small ranges the enlarged box plots in fig 15 exhibit the confidence intervals and standard deviations of the rank acceptability indices the results indicate that the proposed algorithm is robust to sampling variability in terms of the rank acceptability index similarly three holistic acceptability indices with different values low medium and high are also selected to test the algorithm s robustness according to fig 16 the holistic acceptability indices vary within smaller ranges compared to the rank acceptability indices and the box plots almost narrow into lines as can be seen from the enlarged box plots in fig 16 the holistic acceptability indices of a8 and a5 can only take three values with a difference of 0 001 in addition the standard deviations are quite small as well the results suggest that the proposed algorithm shows good robustness in terms of the holistic acceptability index the robustness analysis is performed on a personal computer with an intel xeon e3 1505m eight core 2 8 ghz cpu and 16 gb ram for each replicate we compute the risk of decision making errors and record the execution time as shown in fig 17 the risk of decision making errors varies within a very small range with a standard deviation of 0 00017 the mean value of the execution time is 8 92 s for 100 replicates when implementing 100 000 monte carlo iterations which is fast enough for real time reservoir operations overall the results demonstrate the efficiency and robustness of the proposed algorithm for solving the smaa topsis model 4 summary and conclusions this paper introduced an innovative framework for solving stochastic mcdm problems when there are uncertainties in criteria pvs and cws simultaneously the formulation of mcdm problems was defined we discussed the methods for handling uncertainties in criteria pvs moreover the fws coupled with fahp was proposed to quantify the weight uncertainties as probability distributions with interval constraints we established the smaa topsis model by combining smaa 2 and topsis to assess the effect of uncertainties on mcdm results we proposed the concept of the risk of decision making errors as well as its quantitative calculation method in addition the lhs based monte carlo simulation algorithm and corresponding computer program were developed for solving the smaa topsis model we suggested a three stage mcdm procedure for stochastic mcdm problems and summarized the difference between smaa topsis and deterministic mcdm models the results from the case study indicate that 1 despite no weight information smaa topsis can make a preliminary discrimination for inefficient alternatives and provide valuable reference to help decision makers express their preferences 2 fws coupled with fahp is useful in covering decision makers preferences and avoiding information loss which allows all expected uncertainty and subjectivity in the cws to be considered during the mcdm modelling process 3 when the weight information is considered smaa topsis can make a better discrimination for efficient and inefficient alternatives each alternative can get arbitrary ranks with certain probabilities rather than a single rank in deterministic cases 4 smaa topsis produces significantly lower risks than smaa 2 indicating that using topsis type utility function can reduce the impact of uncertainties as well as reduce the risks of decision making errors 5 smaa topsis can be characterized as an extension of topsis to handle uncertain information i e a stochastic version of topsis in addition the integrated method can be regarded as a new smaa variant with a topsis type utility function as well 6 the results of robustness analysis demonstrate the efficiency and robustness of the lhs based monte carlo simulation algorithm 7 the proposed methodology can provide significant risk information and help decision makers make risk informed decisions with higher reliabilities the proposed methodology is applicable in many kinds of real world mcdm problems due to its multiple advantages first it is suitable for multi criteria group decision making situations where decision makers with conflicting interests are unable or unwilling to provide their preferences or it is hard for them to reach an agreement over the cws in such cases fws coupled with fahp allows for extending the weight vector form one point to a subspace and quantifying the weight uncertainties as probability distributions in real life problems decision makers can use the uniform distribution for simplicity or when they are not sure about their preferences second smaa topsis supports a very flexible way to model different types of uncertain or missing information through any type of probability distributions third the smaa topsis computations can be implemented very efficiently via the lhs based monte carlo simulation algorithm making it easy to apply the proposed methodology in many other decision making contexts in real life applications the proposed methodology can be easily applied to decision support systems for supporting group decision making under uncertainty acknowledgments this work was supported by the national key r d program of china grant no 2017yfc0405606 the national natural science foundation of china grant no 51579068 the special fund for public welfare industry of the ministry of water resources of china grant no 201501007 the fundamental research funds for the central universities grant no 2017b40614 china postdoctoral science foundation funded project grant no 2017m611864 the first author was also supported by a fellowship from the china scholarship council for his visit to the university of california los angeles appendix the lhs based monte carlo simulation algorithm and program description monte carlo simulation is a numerical method for calculating multi dimensional integrals in this method the required number of iterations is inversely proportional to the square of the expected accuracy but does not depend on the dimensionality of integrals significantly tervonen and lahdelma 2007 therefore monte carlo simulation can achieve a precision of a few decimal places with reasonable effort this paper develops a lhs based monte carlo simulation algorithm and corresponding computer program to solve the smaa topsis model pseudo code of the proposed algorithm is provided at the end of the appendix the following symbols are used h i j is the number of times that alternative a i obtains the rank j in monte carlo simulations k c is the total number of iterations r r 1 r 2 r m represents the vector of ranks c c 1 c 2 c m denotes the vector of closeness coefficients the following functions and subroutines are defined in the algorithm lhs x function is used to generate a stochastic decision matrix of size m n from the criteria pv distribution f x each row of the matrix is the criteria pv vector of a certain alternative elements in the matrix are generated from their respective distributions using the lhs technique the distributions may have arbitrary types such as uniform normal lognormal and etc lhs w function returns a weight vector from the weight distribution f w weights are sampled from the fws using the lhs technique and re normalized to ensure that the sum of weights equals to 1 topsis x i w function returns the closeness coefficient c i of alternative a i in which eqs 19 13 are executed rank c function returns a vector of ranks based on the values in vector c for example if c 0 4 0 3 0 7 the resulting rank vector rank c 2 3 1 in order to calculate the rank acceptability index b i r and the central weight vector w i c multi dimensional integrals over criteria and weight distributions need to be computed straight forward calculation requires m n times of monte carlo iterations for b i r and m iterations for w i c which is complex and time consuming the proposed algorithm can speed up the calculation process remarkably by determining the ranks that all alternatives obtained in a single run and then updating b i r and w i c successively similarly we must integrate over the criteria distribution regarding the central weight vectors to calculate the confidence factor p i c direct calculation of p i c requires executing the iteration m times once for each alternative the proposed algorithm can determine all integrals simultaneously in a single run by evaluating for each alternative whether the alternative is the best one when using its central weight vector the proposed algorithm is coded by the combination of r project visual basic 6 0 and visual basic for applications firstly an r package developed by carnell 2016 is used for implementing the lhs method to generate random numbers from probability distributions of criteria pvs and cws the benefit of using r is that it provides a powerful software environment for statistical computing and a large number of simulated values can be easily obtained with small effort secondly the main part of the algorithm is written in visual basic 6 0 which allow for analysis and management of data as well as model computations finally all of the model outputs are collected in microsoft excel then visual basic for applications the programming language incorporated in excel is used for automatic plotting and result visualization pseudo code of the lhs based monte carlo simulation algorithm image 1 appendix a supplementary data the following is the supplementary data related to this article supplementary material supplementary material appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2017 11 032 
26462,storm induced saltwater intrusion siswi often starts with i overtopping breaching of a coastal barrier followed by ii hinterland inundation and iii subsequent vertical seawater intrusion behind the barrier though these three processes are naturally successive they are often analysed separately however the necessity of considering these processes as fully coupled has been increasingly recognised this study therefore addresses the modelling of these processes in an integrated approach the previous related studies are examined and four coupling scenarios are proposed thus a new modelling scenario utilising the model xbeach for simulating overtopping breaching and subsequent flooding and seawat for simulating the siswi is chosen for application to a case study in northern germany moreover the study addresses the mitigation of siswi using a subsurface drainage network the simulation results illustrate the high efficiency of such drainage in shortening the remediation time as well as in limiting salt intrusion to the deeper freshwater aquifers graphical abstract image 1 keywords coastal barrier breaching coastal floods seawater intrusion xbeach seawat subsurface drainage 1 introduction in regions of limited surface water availability groundwater resources are extremely important since they are often intensively used for drinking domestic irrigation or industrial purposes abdullah 2017 barlow and reichard 2010 irrigational demands from aquifers in general account for ca 70 of the world s freshwater usage narayan et al 2007 siebert et al 2010 walther et al 2014 as more than 60 of the world s population lives within 100 km of coastlines a large part of these demands are withdrawn from coastal aquifers yang et al 2013 tomaszkiewicz et al 2014 the latter might explain why coastal aquifers have a special weightiness as freshwater sources especially with the really limited surface water availability in coastal zones oude essink 2001 moreover demographic studies e g neumann et al 2015 as well as the united nations environment programme unep predict that the percentage of the nearshore population will increase from 60 to 75 by 2020 which might lead to an overexploitation of the freshwater aquifers kalaoun et al 2016 besides the latter effect of dense population on coastal aquifers extreme storm surges and tropical storms are among the main indirect threats to coastal aquifers since any subsequent coastal flood might be a real source of coastal aquifers contamination holding and allen 2015 williams 2010 yang et al 2015a 2015b 2013 in fact coastal areas and coastal aquifers are highly vulnerable environments and may experience severe impacts from coastal storms yu et al 2016b elsayed and oumeraci 2017c with global warming and sea level rise slr many coastal systems may experience accelerated coastal erosion coastal barrier breaching coastal flooding and subsequent seawater intrusion into fresh groundwater elsayed and oumeraci 2016a 2016b giambastiani et al 2017 ranasinghe 2016 taylor et al 2012 changing climate might lead to changes in the frequency intensity spatial extent duration and timing of weather events possibly resulting in unprecedented extreme events de winter and ruessink 2017 parry et al 2007 stocker et al 2014 vousdoukas et al 2017 therefore coastal barriers such as dunes dykes and other engineered structures are often required however marine floods resulting from the overtopping breaching of coastal barriers during extreme storm surges besides being a threat to people assets and further resources onshore may result in swi into coastal aquifers induced by the vertical infiltration of saltwater behind the overtopped breached coastal barriers chaumillon et al 2017 steyer et al 2007 villholth and neupane 2011 williams 2010 yang et al 2015a 2015b 2013 yu et al 2016b vertical swi contaminates the originally fresh groundwater by increasing its salinity elsayed and oumeraci 2016b mahmoodzadeh and karamouz 2017 post and houben 2017 this may thus significantly reduce the water quality and the environmental values of groundwater and may possibly hinder any possible sustainable development in coastal zones exposed to coastal flooding mainly the four reasons r1 r4 illustrated in fig 1 may lead to swi in coastal aquifers the first three reasons r1 r3 are mainly related to the hydraulic interconnection between seawater and groundwater in fact the mean sea level msl and the groundwater table gwt in aquifers are interconnected like in a u tube manometer as shown in fig 2 a the location of the salt freshwater interface sea water in red colour inside the u tube and freshwater in sky blue colour depends indeed on the head difference δh between msl and gwt the value of δh increases spatially landward and thus results in lowering the interface in the same direction h 40 δh according to herzberg 1901 in the case of a long term slr i e r1 induced by global warming for instance the hydraulic head δh decreases as a result the interface moves landward as shown in fig 2 b throughout the black dashed line to satisfy again the hydrostatic equilibrium such lateral shift of the interface represents a lateral seawater intrusion see e g ketabchi et al 2016 mahmoodzadeh and karamouz 2017 the same type of intrusion may take place as shown in fig 2 b by the blue dashed line if the gwt decreases i e r2 due either to reduced rainfall rates or to human activities such as excessive pumping e g mishra and dwibedy 2015 reason r3 related to local lateral intrusion upconing represents a special case of r2 that can take place in the case of a local lowering of the gwt under excessive pumping effect leading to a local shift of the interface that often takes the form of an inverted cone e g werner et al 2009 the fourth reason r4 related to coastal flooding represents the most complex type of intrusion the complexity arises indeed from the high diversity of the involved processes and interactions in fact different flood paths might be possible i direct inundation in the case without coastal defences as it is often the case in atoll islands under overwash events e g chui and terry 2015 oberle et al 2017 gingerich et al 2017 ii flooding through a coastal barrier breach and iii inundation induced by overtopping overflow over a coastal barrier moreover different flow domains are involved starting from the sea where waves propagate toward the coastal barriers which might result in overtopping and or breaching thus leading to coastal floods behind the barriers and subsequently to swi due to the infiltrating seawater in the hinterland elsayed 2017 on the other hand diverse processes are involved e g coastal hydrodynamics sediment transport soil avalanching on barriers slopes and or from breaching wedges surface runoff of seawater over the hinterland and subsurface flow of the infiltrating seawater to the groundwater aquifers in addition several interactions among the latter processes exist for instance the breaching of a coastal barrier represents the outcome of complex interactions between nearshore hydrodynamics coastal sediment transport and morphodynamics as well as soil avalanching from the barrier front and or breaching wedges elsayed and oumeraci 2016a moreover propagation of saltwater over the hinterland and subsequent infiltration to aquifers represent a surface subsurface interacting transport of a conservative solute holding and allen 2015 mahmoodzadeh and karamouz 2017 violette et al 2009 werner et al 2013 wilson et al 2011 yang et al 2015a 2015b 2013 yu et al 2016b the overtopping breaching of a coastal barrier the induced inundation and the subsequent swi are naturally successive processes therefore fluxes of water are continuous between the breach induced inlet the surface flow propagation and the subsurface intrusion so that the flow through breaches represents the source for both surface runoff behind the breached barrier and the induced saltwater infiltration to predict the migration pathway and concentrations of contaminants e g saltwater in groundwater and to evaluate possible remediation scenarios of contaminated groundwater after a coastal flood a proper process based numerical model is required for this purpose many of the previous modelling studies for siswi e g holding and allen 2015 yang et al 2015a 2015b 2013 yu et al 2016b used the surface subsurface model system hydrogeosphere hgs developed by therrien et al 2010 the model has the advantage to account for the hydraulic coupling between the surface flow domain simulated by diffusive wave approximation dsw e g collier et al 2011 of the nonlinear shallow water equations nlswes and the subsurface flow domain simulated by coupling the richards equation richards 1931 and the advection dispersion equation for solute transport as a result hgs simplifies the flow and automatically transfer the boundary conditions between the surface and the subsurface domains brunner and simmons 2012 furman 2008 maxwell et al 2014 tian et al 2016 yu et al 2016a however the model does not simulate coastal erosion and the induced breaching process using hgs the studies of yang et al 2015a b 2013 and yu et al 2016b reported that the natural remediation process nrp of a contaminated aquifer after a coastal flood event takes 10 20 years to reach again the pre inundation freshwater condition yet these studies have four main limitations i the inland discharge is roughly estimated by considering a maximum admissible overtopping rate 200 l s obtained from the manual of the eurotop model of pullen et al 2007 as in yang et al 2013 and yu et al 2016b or by assuming the overwash and breaching dimensions as in yang et al 2015a b as shown by gallien 2016 and gallien et al 2014 using eurotop results in incorrect estimates of the inland discharge and thus in an incorrect coastal flood extent while assuming breaching dimensions in yang et al 2015a results in incorrect estimates of the inland flow thus the studies of yang et al 2015a b 2013 and yu et al 2016b lack a proper estimation of the morphological evolution and the dimensions of the induced breaching therefore the predicted inland discharge might also be inaccurate which thereby results in inaccurate predictions of flood depths extents accordingly this would result in inaccurate swi calculated by the hgs model ii the hgs model does not simulate the short waves and their induced effects e g breaking run up run down which are among the main drivers for barrier erosion and breaching elsayed and oumeraci 2016b iii the hgs model predicts the surface propagation kinematics and inundation extent based on the diffusive wave approximation dsw which is unacceptable approximation when calculating the overtopping rates or the flow through breach channels due to the associated high flow velocities bisschop et al 2010 elsayed and oumeraci 2017a in fact the dsw approximation of the nlswes omits the inertial acceleration term in the nlswes which is dominant over the barrier during overtopping or through breaching induced inlet s because of the high flow velocity there thus the dsw is only valid when the inertial acceleration term is negligible as compared to other acceleration terms in the nlswes or in other words when subcritical flow primarily prevails which is often not the case through breaches or over barriers during overtopping thus the inland discharges over barriers as in yang et al 2013 or through assumed breaches as in yang et al 2015a b are often underestimated when using hgs because of the omission of the inertial term in the dsw that is utilised in the hgs model iv in addition these studies are limited to the determination of the time interval for the natural remediation process nrp of coastal aquifers after an swi event thus no structural mitigation measures are yet proposed to shorten such long remediation intervals after an siswi event such incorrect estimates of the inland discharge and subsequent incorrect predictions of flood depths extents would affect the natural remediation interval of coastal aquifers after an siswi event moreover a suitable mitigation measure needs to be implemented in order to shorten the long natural remediation intervals after an siswi event which are significantly long taking from 3 to 20 years according to violette et al 2009 and yang et al 2013 therefore this paper aims at i examining all possible modelling approaches of siswi so that the most suitable approach can be applied to an exemplarily pilot site near bremerhaven northern germany in order to predict the contamination extent and the time interval for the nrp of the near bremerhaven aquifers after an siswi scenario ii proposing a new overall methodology to reliably assess the possible implications of extreme storm surges on the safety of coastal barriers the induced inundation as well as the subsequent saltwater contamination of coastal aquifers iii analysing the feasibility of a subsurface drainage network as a structural mitigation measure to shorten long remediation intervals of contaminated aquifers after a coastal flood event in order to achieve these objectives this paper is structured as follows section 2 provides an overview of the previous studies related to siswi moreover the possible modelling approaches for siswi are examined and the possible alternatives to mitigate such siswi are explored so that the most suitable modelling approach and mitigation tool are selected to be applied for a case study that is presented in section 3 the results are described in section 4 and discussed in section 5 finally the conclusions and the implications for the improvement of the overall modelling approach and the modelling of the suggested mitigation measure are drawn in section 6 2 siswi problem statement and possible modelling and management approaches during moderate sea conditions fig 3 a wave attack and induced coastal erosion are often limited to the nearshore area under such conditions fresh groundwater is in equilibrium with the laterally intruding sea waters as long as the msl and the hydrogeological conditions at the sea land boundary are stationary the regional freshwater flow toward the sea controls the interface between saltwater and freshwater in the aquifers post et al 2013 sawyer et al 2016 taylor et al 2012 during extreme storm surges fig 3b the storm induced higher water level may temporally lead to onshore inundation elsayed and oumeraci 2016a yu et al 2016b in fact the shortwaves riding on the temporally rising sea level msl during a storm surge event may directly affect the barrier possibly causing wave overtopping or overflow through combined surge and waves elsayed and oumeraci 2017a 2016b muller et al 2016 overwash and breaching of the coastal barriers might result in coastal inundation and subsequent vertical swi behind the breached barrier as shown in fig 3 b swi into coastal aquifers increases the salinity of the originally fresh groundwater thus affecting the usability of groundwater due to salinity influence on many aspects associated with the chemical and biological processes in natural waters bear and cheng 2010 cheng and ouazar 2016 kresic 2006 seawater typically has a salinity of around 35 35000 mg l although lower values prevail near coasts where rivers pour freshwater off the sea ocean around 25 according to yang et al 2013 therefore saline water cannot be used for drinking irrigation and industrial purposes in fact the guidelines of the world health organization who for drinking water quality who 2011 and the european water framework directive eu wfd in kaika 2003 consider water as usable for the latter purposes if the salt concentration is less than 0 5 500 mg l fresh groundwater of a salinity concentration less than 500 mg l is formed in aquifers by rainwater percolation into the substrate and or by seepage conductance from freshwater bodies therefore the natural input to groundwater is conductance from surface water which is known by recharge while the natural outputs from groundwater are natural springs and seepage to streams e g seas in the form of a seaward directed flow also known as submarine fresh groundwater discharge sfgd as shown in figs 2 and 3 the latter flow represents a natural remediation of coastal aquifers chui and terry 2012 narayan et al 2007 post et al 2013 as it continuously dilutes contaminants e g salt and transports them seaward sawyer et al 2016 taylor et al 2012 nevertheless such an nrp of contaminated aquifers may take many years after an siswi event before reusing the aquifers for subsistence purposes chang and clement 2013 elsayed and oumeraci 2016b illangasekare et al 2006 yang et al 2015a 2015b 2013 aquifers under siswi events represent very complex and highly dynamic hydrological systems with diverse interacting physical processes i variability of saturated flow above and beneath the phreatic line fig 3a ii spatiotemporal variations of fluid density the polluted water sea water being denser than the ambient fresh groundwater thus favouring the process of vertical percolation driving mechanism for flow described by pressure gradients as well as by density gradients iii tidal fluctuations in the nearshore zone see e g levanon et al 2016 li et al 2004 robinson et al 2014 iv possible open wells that represent a very fast pathway for contamination of the subsurface due to their direct contact with groundwater illangasekare et al 2006 and v surface runoff of the overtopping breaching induced flooding the latter process is often represented by the nlswes while the first two processes i ii depend on the degree of soil saturation in order to consider these diverse interacting physical processes the most recent studies on siswi by yang et al 2015a 2015b 2013 and yu et al 2016b used the surface subsurface model hgs the surface module of hgs is based on solving the dsw approximation of the nlswes which include an infiltration exfiltration term in the continuity equation to calculate the conductance rate from the ground surface to the subsurface domain the latter rate acts as a source term in richards equation richards 1931 for the subsurface calculations at both the unsaturated vadose and saturated zones above and beneath the gwt respectively the subsurface module in addition includes the advection dispersion equation to calculate the inundation induced contaminant solute transport the advection term in the latter equation is responsible for translating the solute field by moving the solute with the flow velocity without changing the shape of the contaminant plume at all however the dispersion term describes the combined effect of mechanical dispersion which is responsible for the net spreading of the solute plume and molecular diffusion which is responsible for the spread of the contaminant particles in random motion from regions of higher concentration to regions of lower concentration thereby hgs represents a variable density model that takes into consideration the existence of a mixing zone between seawater and fresh groundwater because of accounting for both the advection and dispersion phenomena therefore hgs takes a step forward as compared to sharp interface models e g sharp of essaid 1990 which consider only advective transport processes whilst dispersion is neglected hence assuming that seawater and freshwater are separated without mixing moreover hgs as a surface subsurface model is capable to integrally simulate the whole hydrogeological cycle in both surface and subsurface domains the subsurface domain includes processes like infiltration groundwater flow and contaminants transport whilst the surface domain includes surface runoff and wave propagation by utilising the dsw approximation thus hgs through considering the surface subsurface domains takes again a step forward as compared to other common subsurface models which simulate only the processes under the ground level e g seawat by guo and langevin 2002 visual modflow seawat by waterloo hydrogeologic 2015 sutra by voss and provost 2010 and opengeosys by kolditz et al 2012 though the advanced modelling approach used in the studies of yang et al 2015a 2015b 2013 and yu et al 2016b to simulate the siswi using the surface subsurface model hgs is well established such modelling suffers the limitations mentioned in the introductory section section 1 these limitations imply that it is indeed necessary to account for the coupling of breaching induced inundation and subsequent swi by considering all the interactions among the involved processes as discussed in the following subsection 2 1 coupling scenarios for modelling breaching inundation and saltwater intrusion in order to identify an appropriate model tool in terms of both scope and scale or at least a systemic modelling approach which is able to simulate the mutual interactions between the breaching of coastal barriers the induced inundation and the subsequent vertical intrusion of the seawater to the groundwater aquifers four different coupling scenarios might be considered fig 4 i scenario sc1 in this scenario each process is modelled separately so that the outcomes of the breaching model are manually transferred to the runoff modelling and so on one way coupling approach ii scenario sc2 in this scenario the breaching is modelled separately to transfer its outcomes to a surface subsurface model e g hgs to simulate the mutual interaction between surface and subsurface flow such approach is almost the one followed in the studies of yang et al 2015a 2015b with the difference that the breaching dimensions are roughly assumed not modelled so that the predicted inland discharge subsequent runoff and swi may lack reliability iii scenario sc3 here breaching and induced hinterland inundation are modelled in combination to feed an swi model e g seawat by the breaching inundation outcomes for such purpose the combined modelling can be performed using the hydro geo morphodynamical open source code xbeach roelvink et al 2009 as successfully demonstrated by elsayed and oumeraci 2016a 2016b iv scenario sc4 such scenario considers the three processes as a fully coupled by combining the three processes in a single model besides the need for a manual transfer of the boundary conditions from one model to another in sc1 the omission of the mutual interaction among such processes might lead to unreliable outcomes for instance elsayed and oumeraci 2016a have shown that the separate modelling of the breaching and the induced inundation might lead to unreliable predictions of inundation extents inundation depths and free surface flow kinematics therefore such a modelling scenario is not appropriate moreover the implementation of sc2 in the studies of yang et al 2013 2015a b using the hgs model showed that this modelling scenario suffers the aforementioned limitations to achieve better results based on scenario sc2 surface subsurface models e g hgs should include a module for the coastal processes and the induced morphological evolution in order to simulate coastal barrier breaching and the resulting inflow conditions at the breach the latter means that a single model system for sc4 is not yet available though the model xbeach which simulates both breaching and inundation has a groundwater flow module mccall 2015 mccall et al 2012 solving the 3d darcy equation like the united states geological survey usgs modflow groundwater model of harbaugh 2005 xbeach cannot yet simulate contaminant transport because it does not yet include the advection dispersion equation such a scenario though being computationally costly might provide a greater potential for future studies therefore scenario sc3 is selected for the case study in section 3 because of the following main reasons i the combined modelling of coastal barrier breaching and the induced inundation would result in reasonable inundation extents and water depths as demonstrated in elsayed and oumeraci 2016a 2016b ii the splitting of the modelling after the inundation is less critical since both breaching and inundation processes have the same timescale often in the range of days while the swi process extends for years and decades such a splitting would thus contribute to considerably decrease the computational effort iii the reliability of swi modelling would be better achieved with sc3 since it is based on a more reliable estimate of flood characteristics than those estimated using sc2 by assuming the breaching dimensions as in yang et al 2015a 2015b for applying sc3 to the case study near bremerhaven the xbeach model will be used to perform the combined modelling of breaching and inundation while the swi will be simulated separately using the seawat module of visual modflow of waterloo hydrogeologic 2015 2 2 approaches for the management of storm induced saltwater intrusion large scale natural disasters e g swi cannot be prevented but they can certainly be mitigated through structural and non structural mitigation measures tools in fact besides being a major limitation for the usability of contaminated groundwater for drinking and other subsistence purposes crops in hinterlands can suffer stress and thereby not grow properly or can die due to intolerance to salt faneca sànchez et al 2015 felisa et al 2013 thus leading to a decrease in the agriculture yield fakhruddin 2016 williams 2010 given the vulnerability of coastal fresh groundwater reserves their sustainable management is of paramount importance moreover socioeconomic and environmental impacts caused by swi have claimed the attention of the scientific community worldwide during the last decades gaaloul et al 2012 management of coastal aquifers involves decisions regarding the amount of water to be extracted and or injected into the aquifer kourakos and mantoglou 2015 taking into account the interplay between the conditions of the aquifer and economic social factors and in some cases environmental impacts werner et al 2013 for instance some studies e g gaaloul et al 2012 kumar 2016 roy and datta 2017 have listed many strategies approaches for controlling the traditional swi in coastal aquifer systems swi induced by reasons r1 to r3 as described in figs 1 and 2 which is either in the form of upconing induced by excessive pumping and or in the form of landward shifting of the salt freshwater interface because of a long term slr or long term gwt decline these strategies are listed in fig 5 control methods 1 and 2 in fig 5 are often used to reduce the cone of depression by reducing the rate at which water is withdrawn and by spreading wells apart so that concentrated areas of drawdown are avoided methods 3 to 5 involve creating a hydrodynamic barrier of freshwater that blocks the further encroachment of seawater extraction techniques methods 6 and 7 require the use of extraction wells that pump sea water from the aquifer before it can reach freshwater supply wells e g abd elhamid 2010 method 8 installing impermeable barriers such as grout and steel sheet piles is normally limited to areas where the contaminated aquifer is relatively shallow and the subsurface geology allows for a proper seal each of these methods can be applied to certain situations and the method used depends on the specific problem to be solved moreover simulation tools are often used to evaluate the effectiveness of possible decisions interestingly none of the previous traditional techniques for managing common swi is suitable for managing vertical swi induced by marine flooding for instance illangasekare et al 2006 attempted to overcome swi induced by the 2004 tsunami in sri lanka using widespread pumping of wells to remove seawater the latter approach was effective in some areas but over pumping has led to upconing of the saltwater interface and rising salinity rather than its removal from the upper part of the saturated zone in addition the purged well water was often discharged on the land surface close to the wells allowing the contaminated water to re enter the aquifer and the wells after vadose zone infiltration nevertheless rather than do nothing scenario which is equivalent to relying on natural remediation villholth and neupane 2011 suggested guidelines for future protection of vulnerable coastal groundwater resources based on the 2004 tsunami experience indeed these guidelines focus on open wells as they easily get contaminated and exacerbate the problem of groundwater salinization simply they suggested sealing the wells in order to improve the resilience of the water supply system in addition they suggested re enforcing well heads and raising standpipes in the terrain either by placing them in naturally higher locations or placing them on raised platforms as an option in the case of placing wells outside the flood prone zone is not feasible moreover kumar 2006 reported that increasing surface sealing in urban areas roads roofs paved areas reduces the groundwater recharge at the same time and thereby the vertical percolation of saltwater although it leads to higher stormflow peak discharges in populated areas and in urban drainage networks despite the importance of the aforementioned mitigation measures there is still an urgent need for new solutions to mitigate vertical swi induced by marine floods especially in highly vulnerable coastal zones of intensive use of groundwater as well as in islands where aquifers represent the main source of freshwater in fact siswi is a hazard that cannot be mitigated through nonstructural measures such as warning and emergency planning therefore structural mitigation measures are crucial the best mitigation measure is to make the existing coastal barriers overtopping resistant so that they can cope with extreme overtopping without breaching for the residual inland discharge due to overtopping this study suggests using subsurface drainage system fig 6 especially in flood prone agriculture areas the drainage in general would absorb the contaminant sea water before reaching the fresh groundwater however surface drainage is repulsive since it could enlarge the contamination extent because surface drains would act as preferential pathways for landwards movement of saltwater as showed by yang et al 2015a though the subsurface drainage is a well established technique to increase the agricultural productivity and crops yield e g blann et al 2009 fausey et al 1995 its role in mitigating siswi and in shortening subsequent long remediation intervals are not yet investigated therefore the effect of subsurface drainage on aquifers remediation time after an siswi event will be explored based on the data for the case study in section 3 using the subsurface drains parametrization in section 4 3 1 thus the visual modflow model will be used for this purpose because of its inclusion to a ready to use drainage package that can be adapted to simulate the subsurface drainage effect including the drainage package makes seawat the most suitable model for the purposes of this study despite its incapability to directly simulate the unsaturated flow through the vadose zone which is a weak point that might affect the modelling outcomes seawat can implicitly account for the flow through the vadose zone using a simplified approach that assumes a significantly lower and constant hydraulic conductivity through the vadose zone such unsaturated hydraulic conductivity is used inside the model to calculate spatially varying conductance of the saltwater from the ground level to the gwt based on the vertical distance between them pérez paricio et al 2010 waterloo hydrogeologic 2015 3 case study the following case study makes use of hydrogeological data and geophysical information available from the study of yang et al 2013 for the same study area see acknowledgements the elements of this case study are detailed in the following subsections 3 1 study area and available data the site selected for the case study belongs to the german bight which is situated north of bremerhaven northern germany fig 7 in the german bight increases in wind velocity are expected in the future yang et al 2015a which may enhance the probability of higher and longer storm surges the river weser discharges into the german bight and the catchment of the lower part of this river incorporates several cities major ports a variety of industries as well as agriculture including livestock farming the latter makes it crucial to study the impact of possible storm surge event on the sustainable development at this zone of germany the discharge of the river weser results in dilation of the seawater in the north sea near the study area and thus reduces the average seawater concentration from 35 000 mg l to 25 000 mg l yang et al 2015a 2013 a 12 km long cross shore cross section is considered which is perpendicular to the coastline as indicated by the red line in fig 7 a therefore the study area consists of a two dimensional vertical 2dv cross section of an unconfined coastal aquifer initially saturated with freshwater salt concentration 0 0 mg l the ground surface elevation fig 8 was obtained from a digital elevation model dem showing that the seaside west has a minimum seafloor bathymetry of 20 6 m a s l the inland area is protected by a dyke with a height of 7 3 m a s l whereas the elevation of the area behind the dyke ranges from 0 5 m a s l to 14 66 m a s l a constant domain bottom elevation of 100 m a s l is used as the aquifer bottom which is considered impermeable groundwater level at the landside boundary was measured to be 4 0 m a s l and the effective groundwater surcharge precipitation minus evapotranspiration is estimated to be 300 mm yr 3 2 aquifer parameters all values of aquifer parameter are listed in table 1 which are the same values of yang et al 2013 a homogeneous saturated hydraulic conductivity value ks 5 10 3 m s 43 m day is considered as representative for the gravel sand aquifer of the entire domain in addition uniform value for the longitudinal dispersivity d l the lateral dispersivity d t and the molecular diffusion coefficient d are considered by 100 m 10 m and 10 9 m 2 s 1 respectively salt concentration in the sea is 25 000 mg l which is less than average seawater concentration of 35 000 mg l in the north sea because of water dilution by the river weser therefore seawater density of 1018 3 k g m 3 is considered representative to the salt concentration of 25 000 mg l while freshwater c 0 density is 1000 k g m 3 viscosity is assumed to be independent from salt concentration and hence has a constant value of 1 124 x 10 3 k g m s the aquifer storage parameters are respectively 0 005 m 1 0 18 and 0 2 for the specific storage specific yield and the effective porosity n 3 3 storm surge scenario the impact of a single storm surge event on coastal flow dynamics and on the investigated coastal aquifers is considered the storm surge results in overtopping flow over the dyke crest subsequently sea water inundates the hinterland behind the dyke where the sea water infiltrates into the soil and percolates through the unsaturated vadose zone towards the gwt therefore the storm surge event includes the processes of i sea level rise slr ii overtopping overflow and ponding iii sea level dropping and pond reduction and iv recovery of aquifer salinity to the initial state remediation the considered storm surge fig 9 induces a maximum slr up to 8 5 m a s l which is about 1 1 m higher than the dyke crest without consideration of the effects induced by short waves e g wave runup though the application of more realistic storm surge would be better it is preferred to use the same idealized storm surge curve of yang et al 2013 in order to reproduce the same event but with the proposed modelling scenario sc3 thus facilitating the compassion of the outcomes of this modelling approach sc3 using xbeach visual modflow seawat with the outcomes of the modelling approach sc2 using hgs by yang et al 2013 according to the estimations of yang et al 2013 the overflow lasts for 2 8 h and results in a maximum overflow rate of 200 l s 1 per meter dyke as reported by yang et al 2013 the 200 l s is considered as the maximum admissible value for overtopping flow according to eurotop 2007 by pullen et al 2007 thus a total of 1045 m 3 of seawater overtop the dyke during the 2 8 h of overtopping it is assumed that the salt concentration of the overflow water is as seawater concentration i e 25 000 mg l therefore a total of 26125 kg of salt is delivered to the hinterland during this overflow event 4 results in this study the modelling scenario sc3 fig 4 is tracked to simulate the naturally successive processes of water overflow flood propagation and induced saltwater intrusion in this way the xbeach model is used to perform the combined modelling of wave overtopping and induced flood propagation as discussed in section 4 1 below whilst the vertical infiltration of the flooding seawater to aquifer is discussed in section 4 2 the results of using a subsurface drainage system as a mitigation measure are discussed in section 4 3 4 1 combined modelling of overtopping and induced flood propagation as a first step of the modelling scenario sc3 wave overtopping and subsequent flood propagation are simulated using the hydro geo morphodynamic open source model xbeach roelvink et al 2009 the model is capable of simulating the nearshore hydrodynamics related morphodynamics and coastal erosion avalanching of coastal barriers induced by erosion induced instabilities and thus the overwash and breaching of coastal barriers are also simulated moreover the model capability to simulate both breaching and induced marine flooding has been recently demonstrated by elsayed and oumeraci 2016a 2016b nevertheless the groundwater module of xbeach does not yet account for advection dispersion and therefore it is not yet capable of simulating the swi this might represent one of the candidate priority topics for further improvement of xbeach so that barrier breaching induced inundation and subsequent swi can be simulated using a single fully coupled system 4 1 1 model set up in xbeach and inundation outcomes the cross shore profile in fig 8 is reproduced in xbeach in the form of a numerical wave flume with a width of 1 0 m alongshore and a cross shore length of 12349 2 m as a result the storm surge scenario in fig 9 is applied as the hydraulic marine load causing the overflow and the inundation behind the dyke the main parameters and boundary conditions used within xbeach to simulate this profile and the imposed hydraulic load are shown in table 2 while all other xbeach parameters see e g roelvink et al 2015 are kept by the default of xbeach as indicated in table 2 median grain size of 1 mm is considered as the representative median sediment size in the study area the latter value is obtained from a survey of the study area by meilianda et al 2011 the cross shore spatial step dx is considered regular of 12 36 m whilst one longshore spatial step dy of 1 0 m is considered like yang et al 2015b 2013 three different values for the manning coefficient n are used as indicated in fig 8 to account for the bed friction to the free surface flow in other words the land surface of the considered cross shore profile is divided into three parts where different n values are assigned i n 1 for the beach area ii n 2 for the dyke surface area and iii n 3 for the area behind the dyke for the beach area n 1 10 7 d a y m 1 3 0 00864 m 1 3 s is selected and behind the dyke n 3 6 10 7 d a y m 1 3 0 05184 m 1 3 s is used over the dyke n 2 0 00003 d a y m 1 3 2 592 m 1 3 s was calculated so that only maximum inland discharge of 200 l s is admitted yang et al 2013 considered that the water overflow over the dyke should not exceed an admissible value for overtopping obtained from eurotop 2007 page 49 in fact such assumption is illogical for such overflow case meanwhile it is the reason behind adjusting such very high manning coefficient n 2 over the dyke because no data are available for the short wave parameters e g significant wave height and peak period the short wave module of xbeach is switched off using the xbeach keyword swave 0 the latter means that many short wave processes e g wave breaking run up and run down are accordingly omitted nevertheless the slr in fig 9 is considered as the trigger for the overtopping flow process though water flow in the hinterland stretch in the downstream direction is permitted lateral flow is prevented using impermeable wall boundaries as lateral flow boundaries in order to simplify the flow to a 1d flow along the considered cross shore profile the model is run for 10 h using two modelling scenarios as described below while the output time step is 1 s two modelling scenarios for the free surface flow are preliminarily considered i morpho off scenario considers that no morphological evolution takes place because both the dyke and the beach area are highly protected against coastal erosion and overwash and ii morpho on scenario considers that the whole study area consents the coastal erosion and the induced morphological evolution the idea behind these scenarios it to understand the effect of unprotected coastal barriers on coastal flooding and subsequent swi the inland discharge q is calculated at the crest of the dyke point a in fig 8 showing that the inland discharges for both scenarios are identical as shown in fig s1 in appendix a accordingly the same inland water volume 2196 m3 propagates in the hinterland in both scenarios due to this overflow event thus 54 9 tonnes of salt are supplied to the hinterland the identical inland discharges for both simulation scenarios mean that considering the morphological evolution in the morpho on scenario has no effect on the inland flow for this specific case the latter becomes clearer from fig 10 a which illustrates the comparison of the evolution of bed levels and water surfaces along the whole profile for the simulation scenarios morpho on and morpho off it shows that the flood extents for both scenarios are identically increasing with the time marching until water flow is blocked after 10 h at a cross shore distance of 6400 m because of the local increase of the ground elevation at this point therefore the flood extends 5000 m behind the dyke for both simulation scenarios water depths along the 5000 m are spatially varying because of the spatial variation of the ground surface fig 10 b clearly shows these variations through comparing the initial at t 0 h and the final at t 10 h bed and water levels for both simulation scenarios table 3 summarises the main outcomes of the combined modelling of the overtopping flow and the flood propagation using xbeach including a comparison with the outcomes of yang et al 2013 for the same study area the latter volumes for the inland discharge and salt mass are more than twice the values estimated by yang et al 2013 using the surface module of hgs such significant differences arose from the fact that inland discharges calculated by hgs are based on the dsw approximation of the nlswes which ignore the inertial terms in the two momentum equations of the nlswes with the very high overflow velocities over coastal barriers which are common during overtopping and overflow conditions omitting the inertial term local and convective acceleration terms might reduce inland discharges as the very high flow velocities over the barrier are omitted in the calculation because of the dsw approximation xbeach however uses full terms of the nlswes to calculate the surface flow including the inertial term and hence accounts for the high flow velocities over the dyke thus calculating higher inland discharges bearing on a dsw approximation in the free surface flow calculations of hgs represents indeed one of the weaknesses of yang s study as aforementioned in section 1 in fact such incorrect estimation of the inland flow rates in yang s study results in an incorrect simulation of the flood propagation induced water depths and flood extents as demonstrated by elsayed and oumeraci 2016a yet incorrect estimation of flood extent and water depths might also affect the results of the subsurface flow and the contaminant transport as shown in section 4 2 4 1 2 role of micro and meso topographies in marine flood propagation microtopography refers to small depressions and bed forms and it is usually parameterized as manning s roughness coefficient while mesotopography refers to landscape scale features e g ponds and land elevations mcdonnell 2013 yu et al 2016b with the same micro and meso topographies the bed levels in the two modelling scenarios morpho on and morpho off show a non evolutive behaviour even with the permission of such evolution in the morpho on scenario in order to understand the reason behind such a behaviour a third modelling scenario is performed the latter scenario is identical to the scenario morpho on but a unique value for bed friction coefficient is used n n 1 0 00864 m 1 3 s this means that the used friction value for the beach area in front of the dyke n 1 0 00864 m 1 3 s is generalized over the whole cross shore profile thus the higher roughness values at the dyke n 2 and behind it n 3 are reduced to the value n 1 the idea behind this scenario is to allow higher flow velocities over the dyke crest and behind it by reducing the bed friction higher flow velocities definitely stir and move more sediment which is transported landwards and deposited behind the dyke the evolution of the bed and water surface for this scenario is shown in fig 11 when a lower manning value is assigned for both the dyke zone and the hinterland the dyke is totally overwashed accordingly resulting in higher water depths and wider flood extends behind the dyke the latter indicates indeed the importance of a suitable parametrization of the microtopography on the other side the mesotopography plays an important role in limiting the flood extent because of blocking the water flow at higher ground elevations see also yu et al 2016b after the storm peak at t 3 10 h the flood extent retreats with the decrease of the swl as well as with the induced return of the water to the sea ebbing recession conditions however part of the flood water is stored in the depressions as shown in fig 11 now for the morpho on scenario it is clear that the high manning value at the dyke zone n 2 2 592 m 1 3 s limits the flow velocity over the dyke to be always under the threshold value for the onset of sediment motion as described by the shields criterion shields 1936 as a result no morphological evolution takes place with morpho on scenario as this is the case for the morpho off scenario this is indeed the reason why both morpho on and morpho off scenarios provide identical outcomes for the flood extent and water depths 4 2 modelling storm induced saltwater intrusion near bremerhaven using visual modflow seawat for simulating the swi induced by marine flooding for the study area near bremerhaven due to the storm surge scenario in fig 9 the visual modflow 2011 1 graphical interface of waterloo hydrologic waterloo hydrogeologic 2015 is applied in this study visual modflow is a finite difference based interface supporting among others simulation of the conservative nonreactive and single phase miscible transport of seawater in coastal aquifers using its built in seawat module besides their applications in many swi studies e g ding et al 2014 gopinath et al 2016 nofal et al 2014 the key advantage of seawat visual modflow is the inclusion of diverse easy to use boundary condition packages the most important package for the purposes of this study are the river and drainage packages that can respectively simulate the surface subsurface interaction and drainage from the subsurface the inclusion of the drainage package makes visual modflow the most suitable model for the purposes of this study to avoid further developments in other models e g opengeosys moreover seawat is verified with semi analytical solutions of benchmarks e g henry problem henry 1964 see e g elsayed and oumeraci 2017b for the verification results the near bremerhaven model setup using seawat visual modflow and the simulation outcomes are presented in the following subsections 4 2 1 modelling hypotheses conceptualization and setup the aim of modelling the swi induced by the inundation event aforementioned in section 4 1 for near bremerhaven aquifer is threefold i studying the distribution of the saltwater in the aquifer in addition to determine the maximum subsurface extent of the contaminant after the inundation event ii estimating the natural remediation time interval that is needed for the aquifer to get remediated naturally and iii examining the suitability of a subsurface drainage network as a mitigation measure to shorten the natural remediation time for these purposes the water depths in the hinterland as shown in fig 12 a need to be transferred to an seawat model for the aquifer near bremerhaven however water depths during such a flood event vary in both space and time as shown in fig 12 a because of the temporal variation of the inland flow rates and the spatial variation of the ground elevations as shown in fig 12 b the spatiotemporal variation of the water depth in the hinterland extends until the water flow in the downstream direction is blocked by a local rise in the ground elevation as shown in fig 10 b which means that marine flood reaches its maximum extent 5 km behind the dyke after 10 h therefore the water depth at the time of maximum flood extent as shown in fig 12 b i e at t 10 h is selected to be transferred to an seawat model for the aquifer near bremerhaven nevertheless the latter water depths still vary spatially due to the spatial variation of the ground elevation therefore to simplify the analysis some modelling hypotheses need to be addressed as follows i the inland flow 2196 m 3 is uniformly distributed along the flood extent of 5 km as shown in fig 12 b which means that a water depth h 0 44 m averaged over the flood extent and not the spatially varying water depth is considered as the external head causing the vertical intrusion of saltwater this substitution aims at simplifying the input head to the swi model ii tidal fluctuations in the sea can be omitted and hence a stable sea level of 0 00 m can be considered as the mean sea level msl iii all water overtopping the dyke will infiltrate into the aquifer along the flood extent which means that no evaporation is considered during the percolation time iv the infiltration during the flood propagation i e before flood water gets standing interval of 10 h can be omitted v the standing water after the interval of 10 h infiltrates into the aquifer by a rate depending on both the hydraulic head difference and the conductance rate from the land surface to the gwt this is being performed using the modflow river riv package as will be discussed in section 4 2 2 using these hypotheses as well as the model data from section 3 the conceptual model of the aquifer can be drawn as shown in fig 13 a constant domain bottom elevation of 100 m a s l is used as the aquifer bottom which is assumed impermeable at the seaside a constant water head h 0 m and a constant salt concentration c 25000 mg l with seawater density ρ s 1018 3 k g m 3 are considered as indicated in table 1 at the landside boundary a constant water head h 4 m and a constant concentration c 0 mg l are also considered but with a freshwater density ρ 1000 k g m 3 along the ground surface from the shore line to inland an effective surcharge is considered with 300 mm yr which represents a feeding source of freshwater another external load vertical saltwater intrusion is considered along with the 5 km flood extent behind the dyke which represents the contamination source at the ground surface fig 12 b the aquifer properties e g hydraulic conductivity porosity etc are assigned using the values in table 1 after considering the installation of subsurface water collectors for the purpose of faster remediation see section 4 3 subsurface drains are added beneath the ground surface directly exposed to vertical swi from coastal flooding the conceptual siswi model from fig 13 of the aquifer near bremerhaven is set up in visual modflow using 500 columns in x direction δx 24 6 m 1 row in y direction δy 1 0 m and 24 layers the layers thicknesses are small near the ground surface while they have uniform thicknesses of 5 m downwards for each cell prism the aquifer properties in table 1 are assigned for instance a kx ky and kz value of 0 005 m s are assigned for each cell to represent a homogenous soil of hydraulic conductivity equals 0 005 m s after assigning the aquifer properties e g hydraulic conductivity porosity etc two initial conditions are defined first the initial water head in the aquifer is set at 0 25 m above the reference level which is the msl 0 00 m a s l this initial head is roughly chosen as an initial condition for the head in the aquifer second the initial salt concentration in the aquifer is set at 0 mg l except at the seaside boundary it is set at 25000 mg l the other boundary conditions are defined in fig 13 for instance constant water head boundaries of 4 00 m and 0 00 m are defined at the landward and seaward boundaries respectively applying this model two modelling situations are considered i pre storm conditions which is run for 5 years before applying the saltwater head at the ground level and ii storm and post storm conditions which is run for 45 years after applying the saltwater head in order to cover the time needed for the natural remediation of the aquifer after such inundation event thus the model is run for 50 years 18250 days the first 5 years are considered as warm up phase of the model in order to eliminate the effect of initial conditions the outcomes of these successive runs are summarised in the next section 4 2 2 effect of saltwater inundation on freshwater in the aquifer near bremerhaven out of the 5 years of warming up which aim at eliminating the effect of the initial conditions and at reaching the steady state before applying the inundation induced water head from the ground surface the system reached the steady state after three years 1095 days after these three years the mismatch between water inflow q i n from both the ground level as a recharge and the landward boundary as a seaward directed flow as in figs 2 and 3 and outflow q o u t rates from the aquifer throughout the sea land boundary becomes constant because of the stationary hydrogeological conditions on the other side the saltwater in the sea intrudes laterally into the aquifer to the landward direction during this warming up phase because of being heavier than freshwater in fact saltwater intrudes laterally through the seaside boundary and recirculates after the dilatation process to leave the aquifer from near ground level through the same boundary the lateral swi reaches the equilibrium condition after 290 days 1 year this equilibrium condition represents the position when the mismatch between the source in mass salt entering the freshwater aquifer and the sinking out mass salt leaving the aquifer becomes stationary in other words the salt freshwater interface represented by the 50 isoconcentration contour which is corresponding to c c s e a w a t e r c f r e s h w a t e r 2 25000 0 2 12500 mg l becomes stationary after 290 days because the net mass source in sink out in the aquifer is constant the equilibrium condition achieved after the three years out of the five years for the warming up is a function of the model inputs and initial as well as boundary conditions this means that any increase in the sea level will definitely result in further saltwater intrusion landward r1 in figs 1 and 2 the same could be achieved by decreasing the effective recharge value less than 300 mm year which might decrease the gwt r2 in figs 1 and 2 therefore the sea level rise and the recharge value and or the level of the gwt are the triggering factors which determine the equilibrium condition in the aquifer see also yang et al 2017 these factors could be indeed affected by the changing climate parry et al 2007 stocker et al 2014 vousdoukas et al 2017 nevertheless they are assumed stationary in this study considering that the model has reached the steady state after 3 years the inundation load is applied 5 years 1825 days after the simulation start i e 2 years after reaching the steady state the inundation effect is assigned to the model through the modflow river riv package waterloo hydrogeologic 2015 the latter package enables a virtual definition of an external and uniform water head above the ground surface for a certain time as a result the interaction between the surface head and subsurface flow in the aquifer is possible through the conductance value calculated by the riv package which represents a proxy coefficient measure for calculating river aquifer interaction korkmaz et al 2016 pérez paricio et al 2010 in other words the riv package enables the flow of the vertically infiltrating seawater through the vadose zone using a simplified model that assumes uniform and so much lower conductivity through the vadose zone than the saturated zone in fact the hydraulic conductivity decreases significantly with the volumetric water content see e g nimmo 2009 therefore the hydraulic conductivity through the vadose zone is assessed based on the soil type by 5 10 6 m s this conductivity value is used through the riv package to calculate the conductance rate from the ground level to the gwt see waterloo hydrogeologic 2015 for the details as a result it is found that the inland volume of 2196 m 3 that is calculated by xbeach as aforementioned in section 4 1 1 see table 3 will percolate to the aquifer within 4 days in fact the reliability of the latter percolation time is not of high importance since this time is very short relative to the natural remediation interval 44 3 years as computed below therefore conducting the flood volume to the aquifer in 4 days will not differ so much than conducting the same volume in 1 0 h or even in two months this is because the movement of water in the aquifer is very slow which means that the conductance time is not of high importance to the salt transport in the aquifer what is really important is the conductance volume 2196 m 3 which should equal to the inland water volume in order to ensure that the same overtopping volume is conducted to the aquifer using the riv package of modflow the inundation volume is transferred to the aquifer along the 5 km behind the dyke so that the substitute water head 0 44 m from fig 12 b remains constant until the same overtopping volume 2196 m 3 infiltrate into the aquifer along the flood extent being denser and miscible with freshwater in the aquifer saltwater at the ground surface along the flood extent infiltrates downward and blends with freshwater in the aquifer which increases the groundwater salinity therefore a defection of the salt mass budget of the aquifer is expected after such coastal flooding in fact this defection can be explained using the three curves in fig 14 namely the accumulative source in mass curve red curve the accumulative sink out mass curve green curve and the curve of total mass remaining in the aquifer blue curve the latter curve represents indeed the mismatch between the two former curves so that its values can be read separately from the vertical axis on the right fig 14 as shown in fig 14 the net mass blue curve increases dramatically at the beginning of the simulation warming up phase because of the significant amount of salt entering the aquifer from the seaside boundary to satisfy the steady state see elsayed 2017 and elsayed and oumeraci 2017b for more illustrative details after 290 days the net mass becomes almost constant red and green curves become parallel which means that the salt freshwater interface represented by the 50 isoconcentration contour as shown in fig 15 becomes stationary after 290 days because the net mass source in sink out in the aquifer is constant then the vertical leakage of saltwater within the conductance vertical infiltration interval t 1825 1829 days increases the accumulative source mass in the aquifer by 54 9 tonnes see detail b in fig 14 such increase of the source in mass cannot sink out immediately from the aquifer the latter fact can indeed be interpreted through the accumulative sink out mass curve green curve which still shows the same gradual increase without any defection during the conductance interval despite the increase of the accumulative source in mass by 54 9 tonnes during the same interval this means that the increase of the source in mass during the conductance interval is totally stored in the aquifer as represented by the blue curve this stored mass sinks out the aquifer gradually until the aquifer is totally remediated after 44 3 years the natural remediation interval is therefore the time needed after a coastal flood event in order to totally drain the contaminant salt from an aquifer and to reach again the pre flooding situation in this case the bremerhaven aquifer will be remediated totally and reach again the pre flooding condition after 44 3 years almost 4 and half decades in fact this reflects the significant threat of coastal floods to coastal aquifers a flood over few hours can contaminate aquifers for decades which limits the use of aquifers and increases the water treatment costs moreover it hinders the dependence on coastal aquifers in possible sustainable development planning for coastal zones the latter remediation time means that freshwater zones in the aquifer which are affected by the vertical leakage of saltwater will return to its initial state 0 mg l of salt concentration after this very long time the aquifer recovery is due to the natural remediation of the aquifer owing to the seaward directed flow in addition to the recharged part of rain precipitations on the ground surface in fact seaward directed freshwater flow dilutes the infiltrating saltwater and moves it seaward gradually until the aquifer is totally remediated the latter processes are extremely slow and hence very long intervals are needed for total aquifer recovery nevertheless shorter intervals might be admissible if higher concentration values are accepted for instance in the pre flood freshwater zones a salt concentration of 500 mg l can be reached after 25 years see panel g of fig 15 the latter concentration is acceptable as the maximum allowable salt concentration in drinkable water by who and the eu fwd the zones of higher lower concentration can be determined by investigating the salt distribution in the aquifer at different times as shown in fig 15 before the flood event there is only lateral saltwater intrusion panel a in fig 15 after seawater overtopping the saltwater infiltrates into the aquifer along the 5 km flood extent the contaminant salt spreads vertically during the inundation interval as shown in panel b of fig 15 since the infiltrating saltwater is heavier in weight than the prevailing freshwater in the aquifer even after 3 months panel c and one year panel d the salt diffusion is still in the vertical direction therefore saltwater moves vertically beneath the flood extent until it mixes with the freshwater along the aquifer depth such a vertical salt infiltration deviates toward the sea under the effect of seaward directed freshwater flow in fact freshwater moving seaward triggers the dilation process in the aquifer as shown in panels e h this dilation process results in a process of natural remediation of the aquifer until the aquifer is almost remediated totally after 44 3 years as shown in panel h of fig 15 by comparing the iso concentration contours of 50 and 2 in panels a and h one may notice that these contours are almost in the same spatial position which means that the aquifer is almost remediated after this relatively long interval the zone in the right and beneath of the 0 contour line c 0 1 mg l is purely freshwater without any dissolved salts therefore it is the safest zone for pumping the 2 isoconcentration contour corresponds to the maximum allowable dissolved salts in drinkable water according to the standards of who who 2011 and the eu wfd kaika 2003 therefore any pumping from the zone at the right side or beneath of the 2 isoconcentration contour could also be possible of course the farther the pumping well is from the 2 contour line in the landward direction the more usable the extracted water and also the better to avoid further intrusions and upconing in the case of excessive pumping the contamination of the same aquifer near bremerhaven has also been studied by yang et al 2013 using hgs yang s study reported that salt concentrations higher than 500 m g l can still be found close to the aquifer bottom even after 20 years which is consistent with the outcomes of this study in fig 15 after 20 years see panel e the difference between both outcomes is the shape of saltwater spread in yang et al 2013 plume fingers developed in the aquifer while the analysis in this study show a uniform spread the latter behaviour arose from applying a uniform hydraulic head as discussed in the modelling assumptions in section 4 2 1 the former behaviour may be explained by the use of a surface subsurface model in yang et al 2013 which enables earlier and longer infiltration time from the depressions resulting in plume fingers yu et al 2016b unlike this study which specify the time interval for the total remediation by 44 3 years yang et al 2013 did not specify this time interval because they were interested only in determining the remediation interval until reaching the limit 500 mg l the maximum salt concentration for drinkable water according to who to overcome such a long term remediation interval it is crucial to look for an effective mitigation measure that can make coastal aquifers more resilient during and after coastal floods for the latter purpose subsurface drainage network fig 6 might be the proper choice in flood prone coastal zones especially agricultural areas because saltwater infiltrating during and after a flood event can be partially absorbed and evacuated through drains before it contaminates the whole aquifer the results of the examination of this suggestion using the proposed modelling approach are discussed in the next section 4 3 subsurface drainage effect on the resilience of coastal aquifers against coastal floods a subsurface drainage system fig 6 is a man made system that can cause excess water and dissolved salts to flow through the soil to pipes from where they can be evacuated therefore subsurface drainage in coastal areas might function for the two purposes i improving the agricultural yield and crops productivity and ii absorbing the infiltrating saltwater that intrudes vertically from the land surface to the freshwater aquifers during a coastal flood event thus subsurface drainage is used where the soil is permeable enough to allow economical spacing of the drains and performance enough to justify the investment moreover it should provide almost trouble free service for many years the feasibility of using a subsurface drainage system as a mitigation measure for siswi is demonstrated in the following subsections 4 3 1 model set up and parametrization for the aquifer with subsurface drainage in visual modflow a subsurface drainage system consists of a surface or subsurface outlet and subsurface main and lateral drains water is carried into the outlet by the main drains which receives water from the lateral drains also known as inceptors or water collators because subsurface drainage is used primarily to lower the water table or to remove excess water that is percolating through the soil over a general area the drains are placed in a pattern determined by the characteristics of the area kalita et al 2007 in homogeneous soils parallel patterns are used to lower the water table at the same rate on both sides of each drain in heterogeneous soils however random patterns might be more appropriate the spacing and depth of drains influence the groundwater level between drains the required drain spacings and depths depend on soil permeability and on the amount and frequency of rainfall as well as on land topography according to drablos and moe 1984 drain spacings in highly permeable soils should be between 60 and 90 m and between 30 and 60 m in moderately permeable soils where soil permeability is moderate spacing should be between 25 and 30 m in soils with a low permeability or moderate permeability drain spacing should be respectively spaced 10 22 m or 18 25 m regarding the drains depth subsurface drains are often installed based on the desired water table so the depth ranges between 1 and 1 5 m beneath the pre installation water table for highly to moderately permeable soils the latter depth decreases to 0 3 m in the case of lowly permeable soils because the rate of lateral water movement does not increase in this case with depth the range of slopes on which drains can be placed depends to some degree upon the topography of the land the slope should be steep enough to prevent silting but flat enough to prevent flow from exceeding the allowable velocity and from subjecting the drain to excessive pressure the inflow rate to the drains depends on the soil texture and permeability as well as on the surcharge rate to the ground surface and head distribution around the drain indeed there is no general formula for calculating subsurface drain conductance c d c d refers to the absorption rate of a subsurface drain from the ambient soil by m3 day m length of the drain this is due to lack of the detailed information required for the calculation such information may include the detailed head distribution around the drain the aquifer hydraulic conductivity near the drain the distribution of fill material the number and size of the drain pipe openings the amount of clogging materials and the hydraulic conductivity of clogging materials nevertheless the available drainage manuals e g drablos and moe 1984 provide tentative values for subsurface drains conductance based on soil permeability for instance drain conductance in sandy soils ranges from 0 56 to 2 m 3 d a y m while the range of 1 2 8 m 3 d a y m is common for coarse sand and gravel soils silty soil conductance ranges between 0 32 and 0 8 m 3 d a y m while the range of 0 16 0 8 m 3 d a y m is common for clayey soils based on these common values for drain spacing depth and conductance the effect of subsurface drainage on the saltwater intrusion at bremerhaven aquifer can be studied based on the conceptual model in fig 13 thereby the conceptual model is the same as that used for calculating the natural remediation time but subsurface drains are added beneath the ground surface directly exposed to vertical saltwater intrusion from coastal flooding because the soil in bremerhaven can be considered as homogeneous parallel pattern consisting of parallel lateral drains are used in order to avoid further lateral intrusion when using subsurface drainage the balance between freshwater in the aquifer and saltwater in the sea should be considered this means that deepening the drains levels than necessary might allow further lateral intrusion therefore the drains are set at level of 0 50 m a s l which is at least 0 5 m lower than the ground level at the lowest ground level in the hinterland the soil in bremerhaven is highly permeable ks 0 005 m s hence the spacing between the collectors is set at 73 8 m centre to centre circular collectors as shown in fig 13 cannot be defined in visual modflow because of being a finite difference based model utilising rectangular meshes therefore cells containing subsurface drains are defined as drainage cells using the modflow drainage package drn see detail c in fig 13 since the conceptual model is discretized uniformly in the cross shore direction with δ x 24 6 m the face to face distance between drainage cells is 49 2 m as referred to in detail c in fig 13 drain conductance is indeed the most important and the highly uncertain parameter when analysing the subsurface drainage effect it is usually adjusted during model calibrations based on surcharge rate to the ground surface and head difference between drains and gwt based on the latter factors and the aforementioned tentative values for the subsurface drain conductance the conductance value for each drain is assumed along the simulation time 50 years as in fig 16 with the reasoning of these choices in table 4 based on the parameterized values for drains spacing depths and conductance the effects of the considered subsurface drainage on both water flow and mass transport in bremerhaven aquifer are respectively discussed in the following two sections 4 3 2 subsurface drainage effect on water flow in bremerhaven aquifer drainage is an important factor in the aquifer budget since it is a sink out facility this is the reason why it is sidestepped during the first 3 years of warming up by setting c d 0 as in table 4 to allow the model to reach the steady state as aforementioned in section 4 2 2 after the first 3 years the drains start to absorb water from the aquifer as shown in fig 17 based on the assigned value for drain conductance c d during the interval of the inundation induced infiltration 4 days the drainage rate reaches its maximum value which is an evidence that drainage rate depends on both the drain conductance and the surcharge rate on the ground surface therefore it was wise to change the drain conductance value with the surface hydraulic load though the conductance value is constant during the first year after the flood c d 2 0 the drainage rate decreases because of the drainage induced lowering of the gwt during this year the sudden drop in the conductance value from 2 0 to 1 0 at the start of the second year after the flood results in a sudden decrease in drainage rate which become uniform during this year because of the constant conductance value a similar behaviour takes place with the decrease of the conductance value at the end of the latter year during the last 43 years the drainage rate is constant because of the constant conductance and the steady state in the aquifer in fact fig 17 shows that the drained water through the subsurface drains would sink part of the infiltrating saltwater out the aquifer around 50 of the infiltrating water from the land surface for this particular spacing and conductance of the drains except during the high conductance values the latter percentage might range between 150 and 350 so that the time needed for the natural remediation would accordingly decrease this aspect is examined in the next section 4 3 3 subsurface drainage effect on saltwater intrusion to the aquifer near bremerhaven subsurface drainage is selected as a mitigation measure tool for the siswi but does it really solve the problem in order to answer this question the accumulative source in and sink out masses should be compared side by side with the salt mass remaining in the aquifer as in fig 18 a the accumulative input mass red curve in fig 18 a increases by 54 9 tonnes owing to the inundation such increase of the input leads to a sudden like increase of the salt mass remaining in the aquifer blue curve in fig 18 a after flooding where higher conductance values of drains are assumed the accumulative output mass decreases as shown in detail e in fig 18 a because the gwt sinks owing to drainage the decrease of the accumulative output mass increases indeed the mass remaining in the aquifer even more than the case without drainage as shown in fig 18 b which compares the salt masses remaining in the same aquifer under the same conditions with and without using the drainage system drainage in fact facilitates collection of the infiltrating saltwater but at the same time it decreases the level of the gwt leading to more lateral intrusion from the seaside because of the balance defect between freshwater head in the aquifer and seawater head in the sea as previously shown in fig 2 b as shown in fig 18 b the mass remaining in the aquifer during the warming up phase the first 3 years is the same with and without drainage because the drains conductance is set at zero during the following two years which are also before applying the flood effect the salt mass remaining in the aquifer increases with the drainage than without it the latter is because the drainage reduces the gwt leading to a lateral shift in the salt freshwater interface landward the reason r2 as indicated in fig 2b during and after the flood by one year the conductance value is the highest but the mass of salt in the aquifer is increasing dramatically than without drainage this increase extends to the second year after flooding until it reaches its peak with the reduction of the conductance value from 1 to 0 06 m 3 day m with the latter reduction the mass in the aquifer decreases gradually nevertheless the remaining salt mass in the aquifer in the case of using subsurface drainage remain higher than without drainage even after the 45 years after the flood event the previous explanations can be clearer by visualising the salt distribution in the aquifer at different times as in fig 19 as shown in fig 19 panels c and d the subsurface drains collect part of the infiltrating saltwater especially during intervals of higher drain conductance nevertheless the rest escapes downward among the drains leading to the zigzag shape of the concentrations among drains panels e g show that drainage has proved its efficiency in confining the high salt concentration near to the ground surface in fact drainage was capable of controlling the unwanted deeper infiltration of the high salt concentrations in the aquifer highly concentrated saltwater is collected from the shallow zones within the three years after flooding as shown in panels g and h therefore shorter remediation intervals 3 years can indeed be achieved using closer drains as the wider drains spacing allows more saltwater to escape to the deeper freshwater in the aquifer however the efficient role of the drainage in shortening the remediation time of the vertical intrusion is generally at the expense of more lateral intrusion because of the drainage induced lowering of the gwt 5 summary and discussion of the results in this study the implications of storm surge induced barrier overtopping breaching for coastal aquifer contamination are explored vertical saltwater intrusion swi due to coastal inundation may increase the salinity of the originally fresh groundwater which may significantly reduce the water quality and the environmental values of groundwater thus possibly hindering any possible sustainable development in coastal zones prone to marine floods in fact storm surge induced saltwater intrusion siswi may result in widespread aquifer contamination that might last for several years or even decades until aquifers get remediated naturally even a moderate storm surge event may significantly affect the usability of coastal aquifers for many years siswi often starts with overtopping overflow or and breaching of a coastal barrier inducing inland inundation and subsequent vertical intrusion of seawater into coastal aquifers thus the feasibility of modelling these three processes in a single model system is addressed in this study showing that this modelling scenario is the most appropriate but not yet feasible in fact the most suitable two models to perform this task i e xbeach and hydrogeosphere still need further developments fig 20 in order to be able to simulate these three processes successively for instance the groundwater module of xbeach still needs further development to account for density dependent mass transport moreover darcy equation for saturated flow in xbeach needs to be replaced by richards equation for the variably saturated flow so that the flow above and beneath the phreatic line can be simulated furthermore xbeach might need definitions of new boundary condition packages so that recharge and seaward directed flow can be assigned by these three improvements xbeach might be able to successively simulate breaching inundation and intrusion in a single model on the other side a proper geo morphodynamic module as shown in fig 20 is necessary in the surface module of hydrogeosphere in order to simulate coastal erosion and breaching of coastal barriers moreover the diffusive wave approximation dsw needs to be replaced by the full nlswes as in xbeach so that the inertial acceleration term can be considered in order to achieve reliable inland discharges by hydrogeosphere the model should also include a short wave module to account for the short wave action and the associated processes such as breaking run up etc despite the need for many improvement and extensions as shown in fig 20 xbeach and hydrogeosphere still remain as the most eligible models for future improvement in order to successively simulate barrier overtopping breaching induced floods and subsequent swi in a single model system as a fully coupled modelling system is not yet valid a new modelling scenario is proposed in this study using the hydro geo morphodynamic model xbeach for overtopping barrier breaching and subsequent coastal flooding and seawat visual modflow for groundwater flow and swi in coastal aquifers the application of this modelling scenario to a pilot site near bremerhaven northern germany showed that a storm induced barrier overtopping breaching might result in a relatively wide flood extent depending on the inland discharge and on the mesotopography of the hinterland where propagating saltwater in the hinterland infiltrates to freshwater aquifers resulting in their contamination the natural remediation of an aquifer after such coastal floods is possible owing to the continuous dilation of the saltwater in the aquifer by both the recharged part of precipitation and the seaward directed flow however such remediation process is so long and may take decades to reach again the pre flooding fresh groundwater conditions for the study area near bremerhaven which was also considered by yang et al 2013 using the hydrogeosphere model xbeach is used to calculate in combination the overtopping rates over the dyke and the induced flood extent and water depths in the hinterland behind the overtopped coastal barrier the outcomes of this step exhibited more overtopping discharge and hence a wider flood extent than that were predicted in the study of yang et al 2013 possibly because of using a dsw approximation in the hgs model to calculate the flood propagation which omits the inertial acceleration term of the nlswes the outcomes of xbeach are manually transferred to the visual modflow model which simulates the subsequent swi using its seawat module the inundation induced swi takes around 45 years after the inundation until aquifers are remediated and reached again the pre flooding conditions nevertheless 25 years might be sufficient to achieve the maximum allowable concentration for drinkable water 500 mg l the latter interval is only 5 years more than the interval calculated by yang et al 2013 to achieve the same concentration in the aquifer such longer five years might be due to the wider flood and contamination extents calculated by the current study in order to shorten such long remediation intervals subsurface drainage is suggested as a structural mitigation measure drainage manuals e g blann et al 2009 and drablos and moe 1984 are used to specify the spacing between the drains the installation depth and expected drain conductance the use of the subsurface drainage system as a structural mitigation measure allows us to shorten the long intervals needed for the natural remediation to less than three years but generally at the expense of landward shifting of the salt freshwater interface i e further lateral intrusion such a lateral intrusion arises from the fact that subsurface drains reduce the gwt or at least absorb a large part of the recharge from precipitation which represents a continuous feeder for freshwater in the aquifer therefore subsurface drainage represents an appropriate choice since it reduces the contamination extent in a relatively shorter time and widens the usable part of the aquifer moreover the installation of such subsurface drainage system might improve the agricultural yield because of lowering the groundwater table as schematically shown in fig 3 a the side effect of this choice is the possibility of further lateral intrusion around 500 m in the study case near bremerhaven which might be acceptable in order to benefit from drainage for instance in increasing the agricultural yield as well as in shortening the long remediation intervals after a coastal flood event 6 concluding remarks and outlook despite the encouraging results of applying the proposed modelling scenario sc3 fig 4 to simulate the vertical swi in the aquifer near bremerhaven the modelling approach suffers from the following limitations i many modelling assumptions need to be introduced as discussed in section 4 2 1 which may affect the reliability of the modelling outcomes ii a manual transfer from one model to another could not be fully avoided using modelling scenario sc3 which may also affect the accuracy of the modelling outcomes i e assuming an equivalent uniform water head as indicated in fig 12 b results in uniform salt migration downward rather than forming salt plumes as the case in the study of yang et al 2013 therefore the best alternative to simulate an siswi induced by barrier breaching overtopping is to simulate the involved processes in a single fully coupled model system nevertheless bearing in mind the present lack of such a model system the application of xbeach to simulate barrier overtopping breaching and subsequent inundation besides using for instance hydrogeosphere to simulate the surface subsurface interaction and the accompanied contamination transport might represent an optimal modelling option in this way flood propagation in the hinterland will be calculated first by xbeach simulating breaching and subsequent inundation and second by feeding the inland hydrograph calculated by xbeach to the surface subsurface hgs model as explained in elsayed and oumeraci 2016a this might help to confine the modelling inaccuracies as another alternative xbeach can be coupled with another open source subsurface model e g opengeosys by kolditz et al 2012 so that both models can be linked to successively simulate coastal erosion and barrier breaching as well as inundation using xbeach then the inundation outcomes are automatically transferred to opengeosys to simulate the subsurface process and the accompanying swi using the latter approach will definitely help in avoiding licensed models which are very expensive e g hydrogeosphere moreover it will provide flexible environment for further model developments to suit the requirements of each individual research project this study represents the foremost study that attempts to mitigate storm induced saltwater intrusion through the use and modelling of subsurface drainage network making it quite relevant for the coastal engineering community for flood risk managers for groundwater suppliers as well as for sustainable development planners despite these encouraging results on the efficiency of subsurface drainage as a structural siswi mitigation measure further research is still needed to better evaluate drains conductance in the light of experimental work by considering various surcharge rates and different soil types moreover drainage effect needs to be studied using a surface subsurface model e g hydrogeosphere so that the flow through the vadose zone can be better represented in the simulation rather than the simplified approach using the riv package in this study in fact the simple representation of the flow through the vadose zone using the conductance concept might affect the simulation outcomes therefore the outlook for future research based on the outcomes of this study might be outlined as follows i drain conductance represents the main uncertain parameter which needs to be investigated in the light of experimental work by considering a wide range of surcharge rates and different soil types ii the effect of soil heterogeneity on spacing and conductance of subsurface drains need to be investigated iii the drainage effect needs to be studied using a surface subsurface model e g hydrogeosphere so that the flow through the vadose zone can be better simulated in fact this suggestion was not possible through this study due to the expensive license of hydrogeosphere iv open source models e g xbeach and opengeosys need to be coupled in order to successively simulate coastal erosion barrier breaching marine flooding and subsequent swi being open source models their available source codes for diverse researchers worldwide might facilitate this task acknowledgements this research is a part of the phd study of the first author which is supported by the daad in the frame of the exceed swindon project at technische universität braunschweig this support is gratefully acknowledged the data for the case study of near bremerhaven is gratefully provided by prof dr thomas graf and dr jie yang appendix a supplementary data the following is the supplementary data related to this article supplementary data supplementary data appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2017 11 030 
26462,storm induced saltwater intrusion siswi often starts with i overtopping breaching of a coastal barrier followed by ii hinterland inundation and iii subsequent vertical seawater intrusion behind the barrier though these three processes are naturally successive they are often analysed separately however the necessity of considering these processes as fully coupled has been increasingly recognised this study therefore addresses the modelling of these processes in an integrated approach the previous related studies are examined and four coupling scenarios are proposed thus a new modelling scenario utilising the model xbeach for simulating overtopping breaching and subsequent flooding and seawat for simulating the siswi is chosen for application to a case study in northern germany moreover the study addresses the mitigation of siswi using a subsurface drainage network the simulation results illustrate the high efficiency of such drainage in shortening the remediation time as well as in limiting salt intrusion to the deeper freshwater aquifers graphical abstract image 1 keywords coastal barrier breaching coastal floods seawater intrusion xbeach seawat subsurface drainage 1 introduction in regions of limited surface water availability groundwater resources are extremely important since they are often intensively used for drinking domestic irrigation or industrial purposes abdullah 2017 barlow and reichard 2010 irrigational demands from aquifers in general account for ca 70 of the world s freshwater usage narayan et al 2007 siebert et al 2010 walther et al 2014 as more than 60 of the world s population lives within 100 km of coastlines a large part of these demands are withdrawn from coastal aquifers yang et al 2013 tomaszkiewicz et al 2014 the latter might explain why coastal aquifers have a special weightiness as freshwater sources especially with the really limited surface water availability in coastal zones oude essink 2001 moreover demographic studies e g neumann et al 2015 as well as the united nations environment programme unep predict that the percentage of the nearshore population will increase from 60 to 75 by 2020 which might lead to an overexploitation of the freshwater aquifers kalaoun et al 2016 besides the latter effect of dense population on coastal aquifers extreme storm surges and tropical storms are among the main indirect threats to coastal aquifers since any subsequent coastal flood might be a real source of coastal aquifers contamination holding and allen 2015 williams 2010 yang et al 2015a 2015b 2013 in fact coastal areas and coastal aquifers are highly vulnerable environments and may experience severe impacts from coastal storms yu et al 2016b elsayed and oumeraci 2017c with global warming and sea level rise slr many coastal systems may experience accelerated coastal erosion coastal barrier breaching coastal flooding and subsequent seawater intrusion into fresh groundwater elsayed and oumeraci 2016a 2016b giambastiani et al 2017 ranasinghe 2016 taylor et al 2012 changing climate might lead to changes in the frequency intensity spatial extent duration and timing of weather events possibly resulting in unprecedented extreme events de winter and ruessink 2017 parry et al 2007 stocker et al 2014 vousdoukas et al 2017 therefore coastal barriers such as dunes dykes and other engineered structures are often required however marine floods resulting from the overtopping breaching of coastal barriers during extreme storm surges besides being a threat to people assets and further resources onshore may result in swi into coastal aquifers induced by the vertical infiltration of saltwater behind the overtopped breached coastal barriers chaumillon et al 2017 steyer et al 2007 villholth and neupane 2011 williams 2010 yang et al 2015a 2015b 2013 yu et al 2016b vertical swi contaminates the originally fresh groundwater by increasing its salinity elsayed and oumeraci 2016b mahmoodzadeh and karamouz 2017 post and houben 2017 this may thus significantly reduce the water quality and the environmental values of groundwater and may possibly hinder any possible sustainable development in coastal zones exposed to coastal flooding mainly the four reasons r1 r4 illustrated in fig 1 may lead to swi in coastal aquifers the first three reasons r1 r3 are mainly related to the hydraulic interconnection between seawater and groundwater in fact the mean sea level msl and the groundwater table gwt in aquifers are interconnected like in a u tube manometer as shown in fig 2 a the location of the salt freshwater interface sea water in red colour inside the u tube and freshwater in sky blue colour depends indeed on the head difference δh between msl and gwt the value of δh increases spatially landward and thus results in lowering the interface in the same direction h 40 δh according to herzberg 1901 in the case of a long term slr i e r1 induced by global warming for instance the hydraulic head δh decreases as a result the interface moves landward as shown in fig 2 b throughout the black dashed line to satisfy again the hydrostatic equilibrium such lateral shift of the interface represents a lateral seawater intrusion see e g ketabchi et al 2016 mahmoodzadeh and karamouz 2017 the same type of intrusion may take place as shown in fig 2 b by the blue dashed line if the gwt decreases i e r2 due either to reduced rainfall rates or to human activities such as excessive pumping e g mishra and dwibedy 2015 reason r3 related to local lateral intrusion upconing represents a special case of r2 that can take place in the case of a local lowering of the gwt under excessive pumping effect leading to a local shift of the interface that often takes the form of an inverted cone e g werner et al 2009 the fourth reason r4 related to coastal flooding represents the most complex type of intrusion the complexity arises indeed from the high diversity of the involved processes and interactions in fact different flood paths might be possible i direct inundation in the case without coastal defences as it is often the case in atoll islands under overwash events e g chui and terry 2015 oberle et al 2017 gingerich et al 2017 ii flooding through a coastal barrier breach and iii inundation induced by overtopping overflow over a coastal barrier moreover different flow domains are involved starting from the sea where waves propagate toward the coastal barriers which might result in overtopping and or breaching thus leading to coastal floods behind the barriers and subsequently to swi due to the infiltrating seawater in the hinterland elsayed 2017 on the other hand diverse processes are involved e g coastal hydrodynamics sediment transport soil avalanching on barriers slopes and or from breaching wedges surface runoff of seawater over the hinterland and subsurface flow of the infiltrating seawater to the groundwater aquifers in addition several interactions among the latter processes exist for instance the breaching of a coastal barrier represents the outcome of complex interactions between nearshore hydrodynamics coastal sediment transport and morphodynamics as well as soil avalanching from the barrier front and or breaching wedges elsayed and oumeraci 2016a moreover propagation of saltwater over the hinterland and subsequent infiltration to aquifers represent a surface subsurface interacting transport of a conservative solute holding and allen 2015 mahmoodzadeh and karamouz 2017 violette et al 2009 werner et al 2013 wilson et al 2011 yang et al 2015a 2015b 2013 yu et al 2016b the overtopping breaching of a coastal barrier the induced inundation and the subsequent swi are naturally successive processes therefore fluxes of water are continuous between the breach induced inlet the surface flow propagation and the subsurface intrusion so that the flow through breaches represents the source for both surface runoff behind the breached barrier and the induced saltwater infiltration to predict the migration pathway and concentrations of contaminants e g saltwater in groundwater and to evaluate possible remediation scenarios of contaminated groundwater after a coastal flood a proper process based numerical model is required for this purpose many of the previous modelling studies for siswi e g holding and allen 2015 yang et al 2015a 2015b 2013 yu et al 2016b used the surface subsurface model system hydrogeosphere hgs developed by therrien et al 2010 the model has the advantage to account for the hydraulic coupling between the surface flow domain simulated by diffusive wave approximation dsw e g collier et al 2011 of the nonlinear shallow water equations nlswes and the subsurface flow domain simulated by coupling the richards equation richards 1931 and the advection dispersion equation for solute transport as a result hgs simplifies the flow and automatically transfer the boundary conditions between the surface and the subsurface domains brunner and simmons 2012 furman 2008 maxwell et al 2014 tian et al 2016 yu et al 2016a however the model does not simulate coastal erosion and the induced breaching process using hgs the studies of yang et al 2015a b 2013 and yu et al 2016b reported that the natural remediation process nrp of a contaminated aquifer after a coastal flood event takes 10 20 years to reach again the pre inundation freshwater condition yet these studies have four main limitations i the inland discharge is roughly estimated by considering a maximum admissible overtopping rate 200 l s obtained from the manual of the eurotop model of pullen et al 2007 as in yang et al 2013 and yu et al 2016b or by assuming the overwash and breaching dimensions as in yang et al 2015a b as shown by gallien 2016 and gallien et al 2014 using eurotop results in incorrect estimates of the inland discharge and thus in an incorrect coastal flood extent while assuming breaching dimensions in yang et al 2015a results in incorrect estimates of the inland flow thus the studies of yang et al 2015a b 2013 and yu et al 2016b lack a proper estimation of the morphological evolution and the dimensions of the induced breaching therefore the predicted inland discharge might also be inaccurate which thereby results in inaccurate predictions of flood depths extents accordingly this would result in inaccurate swi calculated by the hgs model ii the hgs model does not simulate the short waves and their induced effects e g breaking run up run down which are among the main drivers for barrier erosion and breaching elsayed and oumeraci 2016b iii the hgs model predicts the surface propagation kinematics and inundation extent based on the diffusive wave approximation dsw which is unacceptable approximation when calculating the overtopping rates or the flow through breach channels due to the associated high flow velocities bisschop et al 2010 elsayed and oumeraci 2017a in fact the dsw approximation of the nlswes omits the inertial acceleration term in the nlswes which is dominant over the barrier during overtopping or through breaching induced inlet s because of the high flow velocity there thus the dsw is only valid when the inertial acceleration term is negligible as compared to other acceleration terms in the nlswes or in other words when subcritical flow primarily prevails which is often not the case through breaches or over barriers during overtopping thus the inland discharges over barriers as in yang et al 2013 or through assumed breaches as in yang et al 2015a b are often underestimated when using hgs because of the omission of the inertial term in the dsw that is utilised in the hgs model iv in addition these studies are limited to the determination of the time interval for the natural remediation process nrp of coastal aquifers after an swi event thus no structural mitigation measures are yet proposed to shorten such long remediation intervals after an siswi event such incorrect estimates of the inland discharge and subsequent incorrect predictions of flood depths extents would affect the natural remediation interval of coastal aquifers after an siswi event moreover a suitable mitigation measure needs to be implemented in order to shorten the long natural remediation intervals after an siswi event which are significantly long taking from 3 to 20 years according to violette et al 2009 and yang et al 2013 therefore this paper aims at i examining all possible modelling approaches of siswi so that the most suitable approach can be applied to an exemplarily pilot site near bremerhaven northern germany in order to predict the contamination extent and the time interval for the nrp of the near bremerhaven aquifers after an siswi scenario ii proposing a new overall methodology to reliably assess the possible implications of extreme storm surges on the safety of coastal barriers the induced inundation as well as the subsequent saltwater contamination of coastal aquifers iii analysing the feasibility of a subsurface drainage network as a structural mitigation measure to shorten long remediation intervals of contaminated aquifers after a coastal flood event in order to achieve these objectives this paper is structured as follows section 2 provides an overview of the previous studies related to siswi moreover the possible modelling approaches for siswi are examined and the possible alternatives to mitigate such siswi are explored so that the most suitable modelling approach and mitigation tool are selected to be applied for a case study that is presented in section 3 the results are described in section 4 and discussed in section 5 finally the conclusions and the implications for the improvement of the overall modelling approach and the modelling of the suggested mitigation measure are drawn in section 6 2 siswi problem statement and possible modelling and management approaches during moderate sea conditions fig 3 a wave attack and induced coastal erosion are often limited to the nearshore area under such conditions fresh groundwater is in equilibrium with the laterally intruding sea waters as long as the msl and the hydrogeological conditions at the sea land boundary are stationary the regional freshwater flow toward the sea controls the interface between saltwater and freshwater in the aquifers post et al 2013 sawyer et al 2016 taylor et al 2012 during extreme storm surges fig 3b the storm induced higher water level may temporally lead to onshore inundation elsayed and oumeraci 2016a yu et al 2016b in fact the shortwaves riding on the temporally rising sea level msl during a storm surge event may directly affect the barrier possibly causing wave overtopping or overflow through combined surge and waves elsayed and oumeraci 2017a 2016b muller et al 2016 overwash and breaching of the coastal barriers might result in coastal inundation and subsequent vertical swi behind the breached barrier as shown in fig 3 b swi into coastal aquifers increases the salinity of the originally fresh groundwater thus affecting the usability of groundwater due to salinity influence on many aspects associated with the chemical and biological processes in natural waters bear and cheng 2010 cheng and ouazar 2016 kresic 2006 seawater typically has a salinity of around 35 35000 mg l although lower values prevail near coasts where rivers pour freshwater off the sea ocean around 25 according to yang et al 2013 therefore saline water cannot be used for drinking irrigation and industrial purposes in fact the guidelines of the world health organization who for drinking water quality who 2011 and the european water framework directive eu wfd in kaika 2003 consider water as usable for the latter purposes if the salt concentration is less than 0 5 500 mg l fresh groundwater of a salinity concentration less than 500 mg l is formed in aquifers by rainwater percolation into the substrate and or by seepage conductance from freshwater bodies therefore the natural input to groundwater is conductance from surface water which is known by recharge while the natural outputs from groundwater are natural springs and seepage to streams e g seas in the form of a seaward directed flow also known as submarine fresh groundwater discharge sfgd as shown in figs 2 and 3 the latter flow represents a natural remediation of coastal aquifers chui and terry 2012 narayan et al 2007 post et al 2013 as it continuously dilutes contaminants e g salt and transports them seaward sawyer et al 2016 taylor et al 2012 nevertheless such an nrp of contaminated aquifers may take many years after an siswi event before reusing the aquifers for subsistence purposes chang and clement 2013 elsayed and oumeraci 2016b illangasekare et al 2006 yang et al 2015a 2015b 2013 aquifers under siswi events represent very complex and highly dynamic hydrological systems with diverse interacting physical processes i variability of saturated flow above and beneath the phreatic line fig 3a ii spatiotemporal variations of fluid density the polluted water sea water being denser than the ambient fresh groundwater thus favouring the process of vertical percolation driving mechanism for flow described by pressure gradients as well as by density gradients iii tidal fluctuations in the nearshore zone see e g levanon et al 2016 li et al 2004 robinson et al 2014 iv possible open wells that represent a very fast pathway for contamination of the subsurface due to their direct contact with groundwater illangasekare et al 2006 and v surface runoff of the overtopping breaching induced flooding the latter process is often represented by the nlswes while the first two processes i ii depend on the degree of soil saturation in order to consider these diverse interacting physical processes the most recent studies on siswi by yang et al 2015a 2015b 2013 and yu et al 2016b used the surface subsurface model hgs the surface module of hgs is based on solving the dsw approximation of the nlswes which include an infiltration exfiltration term in the continuity equation to calculate the conductance rate from the ground surface to the subsurface domain the latter rate acts as a source term in richards equation richards 1931 for the subsurface calculations at both the unsaturated vadose and saturated zones above and beneath the gwt respectively the subsurface module in addition includes the advection dispersion equation to calculate the inundation induced contaminant solute transport the advection term in the latter equation is responsible for translating the solute field by moving the solute with the flow velocity without changing the shape of the contaminant plume at all however the dispersion term describes the combined effect of mechanical dispersion which is responsible for the net spreading of the solute plume and molecular diffusion which is responsible for the spread of the contaminant particles in random motion from regions of higher concentration to regions of lower concentration thereby hgs represents a variable density model that takes into consideration the existence of a mixing zone between seawater and fresh groundwater because of accounting for both the advection and dispersion phenomena therefore hgs takes a step forward as compared to sharp interface models e g sharp of essaid 1990 which consider only advective transport processes whilst dispersion is neglected hence assuming that seawater and freshwater are separated without mixing moreover hgs as a surface subsurface model is capable to integrally simulate the whole hydrogeological cycle in both surface and subsurface domains the subsurface domain includes processes like infiltration groundwater flow and contaminants transport whilst the surface domain includes surface runoff and wave propagation by utilising the dsw approximation thus hgs through considering the surface subsurface domains takes again a step forward as compared to other common subsurface models which simulate only the processes under the ground level e g seawat by guo and langevin 2002 visual modflow seawat by waterloo hydrogeologic 2015 sutra by voss and provost 2010 and opengeosys by kolditz et al 2012 though the advanced modelling approach used in the studies of yang et al 2015a 2015b 2013 and yu et al 2016b to simulate the siswi using the surface subsurface model hgs is well established such modelling suffers the limitations mentioned in the introductory section section 1 these limitations imply that it is indeed necessary to account for the coupling of breaching induced inundation and subsequent swi by considering all the interactions among the involved processes as discussed in the following subsection 2 1 coupling scenarios for modelling breaching inundation and saltwater intrusion in order to identify an appropriate model tool in terms of both scope and scale or at least a systemic modelling approach which is able to simulate the mutual interactions between the breaching of coastal barriers the induced inundation and the subsequent vertical intrusion of the seawater to the groundwater aquifers four different coupling scenarios might be considered fig 4 i scenario sc1 in this scenario each process is modelled separately so that the outcomes of the breaching model are manually transferred to the runoff modelling and so on one way coupling approach ii scenario sc2 in this scenario the breaching is modelled separately to transfer its outcomes to a surface subsurface model e g hgs to simulate the mutual interaction between surface and subsurface flow such approach is almost the one followed in the studies of yang et al 2015a 2015b with the difference that the breaching dimensions are roughly assumed not modelled so that the predicted inland discharge subsequent runoff and swi may lack reliability iii scenario sc3 here breaching and induced hinterland inundation are modelled in combination to feed an swi model e g seawat by the breaching inundation outcomes for such purpose the combined modelling can be performed using the hydro geo morphodynamical open source code xbeach roelvink et al 2009 as successfully demonstrated by elsayed and oumeraci 2016a 2016b iv scenario sc4 such scenario considers the three processes as a fully coupled by combining the three processes in a single model besides the need for a manual transfer of the boundary conditions from one model to another in sc1 the omission of the mutual interaction among such processes might lead to unreliable outcomes for instance elsayed and oumeraci 2016a have shown that the separate modelling of the breaching and the induced inundation might lead to unreliable predictions of inundation extents inundation depths and free surface flow kinematics therefore such a modelling scenario is not appropriate moreover the implementation of sc2 in the studies of yang et al 2013 2015a b using the hgs model showed that this modelling scenario suffers the aforementioned limitations to achieve better results based on scenario sc2 surface subsurface models e g hgs should include a module for the coastal processes and the induced morphological evolution in order to simulate coastal barrier breaching and the resulting inflow conditions at the breach the latter means that a single model system for sc4 is not yet available though the model xbeach which simulates both breaching and inundation has a groundwater flow module mccall 2015 mccall et al 2012 solving the 3d darcy equation like the united states geological survey usgs modflow groundwater model of harbaugh 2005 xbeach cannot yet simulate contaminant transport because it does not yet include the advection dispersion equation such a scenario though being computationally costly might provide a greater potential for future studies therefore scenario sc3 is selected for the case study in section 3 because of the following main reasons i the combined modelling of coastal barrier breaching and the induced inundation would result in reasonable inundation extents and water depths as demonstrated in elsayed and oumeraci 2016a 2016b ii the splitting of the modelling after the inundation is less critical since both breaching and inundation processes have the same timescale often in the range of days while the swi process extends for years and decades such a splitting would thus contribute to considerably decrease the computational effort iii the reliability of swi modelling would be better achieved with sc3 since it is based on a more reliable estimate of flood characteristics than those estimated using sc2 by assuming the breaching dimensions as in yang et al 2015a 2015b for applying sc3 to the case study near bremerhaven the xbeach model will be used to perform the combined modelling of breaching and inundation while the swi will be simulated separately using the seawat module of visual modflow of waterloo hydrogeologic 2015 2 2 approaches for the management of storm induced saltwater intrusion large scale natural disasters e g swi cannot be prevented but they can certainly be mitigated through structural and non structural mitigation measures tools in fact besides being a major limitation for the usability of contaminated groundwater for drinking and other subsistence purposes crops in hinterlands can suffer stress and thereby not grow properly or can die due to intolerance to salt faneca sànchez et al 2015 felisa et al 2013 thus leading to a decrease in the agriculture yield fakhruddin 2016 williams 2010 given the vulnerability of coastal fresh groundwater reserves their sustainable management is of paramount importance moreover socioeconomic and environmental impacts caused by swi have claimed the attention of the scientific community worldwide during the last decades gaaloul et al 2012 management of coastal aquifers involves decisions regarding the amount of water to be extracted and or injected into the aquifer kourakos and mantoglou 2015 taking into account the interplay between the conditions of the aquifer and economic social factors and in some cases environmental impacts werner et al 2013 for instance some studies e g gaaloul et al 2012 kumar 2016 roy and datta 2017 have listed many strategies approaches for controlling the traditional swi in coastal aquifer systems swi induced by reasons r1 to r3 as described in figs 1 and 2 which is either in the form of upconing induced by excessive pumping and or in the form of landward shifting of the salt freshwater interface because of a long term slr or long term gwt decline these strategies are listed in fig 5 control methods 1 and 2 in fig 5 are often used to reduce the cone of depression by reducing the rate at which water is withdrawn and by spreading wells apart so that concentrated areas of drawdown are avoided methods 3 to 5 involve creating a hydrodynamic barrier of freshwater that blocks the further encroachment of seawater extraction techniques methods 6 and 7 require the use of extraction wells that pump sea water from the aquifer before it can reach freshwater supply wells e g abd elhamid 2010 method 8 installing impermeable barriers such as grout and steel sheet piles is normally limited to areas where the contaminated aquifer is relatively shallow and the subsurface geology allows for a proper seal each of these methods can be applied to certain situations and the method used depends on the specific problem to be solved moreover simulation tools are often used to evaluate the effectiveness of possible decisions interestingly none of the previous traditional techniques for managing common swi is suitable for managing vertical swi induced by marine flooding for instance illangasekare et al 2006 attempted to overcome swi induced by the 2004 tsunami in sri lanka using widespread pumping of wells to remove seawater the latter approach was effective in some areas but over pumping has led to upconing of the saltwater interface and rising salinity rather than its removal from the upper part of the saturated zone in addition the purged well water was often discharged on the land surface close to the wells allowing the contaminated water to re enter the aquifer and the wells after vadose zone infiltration nevertheless rather than do nothing scenario which is equivalent to relying on natural remediation villholth and neupane 2011 suggested guidelines for future protection of vulnerable coastal groundwater resources based on the 2004 tsunami experience indeed these guidelines focus on open wells as they easily get contaminated and exacerbate the problem of groundwater salinization simply they suggested sealing the wells in order to improve the resilience of the water supply system in addition they suggested re enforcing well heads and raising standpipes in the terrain either by placing them in naturally higher locations or placing them on raised platforms as an option in the case of placing wells outside the flood prone zone is not feasible moreover kumar 2006 reported that increasing surface sealing in urban areas roads roofs paved areas reduces the groundwater recharge at the same time and thereby the vertical percolation of saltwater although it leads to higher stormflow peak discharges in populated areas and in urban drainage networks despite the importance of the aforementioned mitigation measures there is still an urgent need for new solutions to mitigate vertical swi induced by marine floods especially in highly vulnerable coastal zones of intensive use of groundwater as well as in islands where aquifers represent the main source of freshwater in fact siswi is a hazard that cannot be mitigated through nonstructural measures such as warning and emergency planning therefore structural mitigation measures are crucial the best mitigation measure is to make the existing coastal barriers overtopping resistant so that they can cope with extreme overtopping without breaching for the residual inland discharge due to overtopping this study suggests using subsurface drainage system fig 6 especially in flood prone agriculture areas the drainage in general would absorb the contaminant sea water before reaching the fresh groundwater however surface drainage is repulsive since it could enlarge the contamination extent because surface drains would act as preferential pathways for landwards movement of saltwater as showed by yang et al 2015a though the subsurface drainage is a well established technique to increase the agricultural productivity and crops yield e g blann et al 2009 fausey et al 1995 its role in mitigating siswi and in shortening subsequent long remediation intervals are not yet investigated therefore the effect of subsurface drainage on aquifers remediation time after an siswi event will be explored based on the data for the case study in section 3 using the subsurface drains parametrization in section 4 3 1 thus the visual modflow model will be used for this purpose because of its inclusion to a ready to use drainage package that can be adapted to simulate the subsurface drainage effect including the drainage package makes seawat the most suitable model for the purposes of this study despite its incapability to directly simulate the unsaturated flow through the vadose zone which is a weak point that might affect the modelling outcomes seawat can implicitly account for the flow through the vadose zone using a simplified approach that assumes a significantly lower and constant hydraulic conductivity through the vadose zone such unsaturated hydraulic conductivity is used inside the model to calculate spatially varying conductance of the saltwater from the ground level to the gwt based on the vertical distance between them pérez paricio et al 2010 waterloo hydrogeologic 2015 3 case study the following case study makes use of hydrogeological data and geophysical information available from the study of yang et al 2013 for the same study area see acknowledgements the elements of this case study are detailed in the following subsections 3 1 study area and available data the site selected for the case study belongs to the german bight which is situated north of bremerhaven northern germany fig 7 in the german bight increases in wind velocity are expected in the future yang et al 2015a which may enhance the probability of higher and longer storm surges the river weser discharges into the german bight and the catchment of the lower part of this river incorporates several cities major ports a variety of industries as well as agriculture including livestock farming the latter makes it crucial to study the impact of possible storm surge event on the sustainable development at this zone of germany the discharge of the river weser results in dilation of the seawater in the north sea near the study area and thus reduces the average seawater concentration from 35 000 mg l to 25 000 mg l yang et al 2015a 2013 a 12 km long cross shore cross section is considered which is perpendicular to the coastline as indicated by the red line in fig 7 a therefore the study area consists of a two dimensional vertical 2dv cross section of an unconfined coastal aquifer initially saturated with freshwater salt concentration 0 0 mg l the ground surface elevation fig 8 was obtained from a digital elevation model dem showing that the seaside west has a minimum seafloor bathymetry of 20 6 m a s l the inland area is protected by a dyke with a height of 7 3 m a s l whereas the elevation of the area behind the dyke ranges from 0 5 m a s l to 14 66 m a s l a constant domain bottom elevation of 100 m a s l is used as the aquifer bottom which is considered impermeable groundwater level at the landside boundary was measured to be 4 0 m a s l and the effective groundwater surcharge precipitation minus evapotranspiration is estimated to be 300 mm yr 3 2 aquifer parameters all values of aquifer parameter are listed in table 1 which are the same values of yang et al 2013 a homogeneous saturated hydraulic conductivity value ks 5 10 3 m s 43 m day is considered as representative for the gravel sand aquifer of the entire domain in addition uniform value for the longitudinal dispersivity d l the lateral dispersivity d t and the molecular diffusion coefficient d are considered by 100 m 10 m and 10 9 m 2 s 1 respectively salt concentration in the sea is 25 000 mg l which is less than average seawater concentration of 35 000 mg l in the north sea because of water dilution by the river weser therefore seawater density of 1018 3 k g m 3 is considered representative to the salt concentration of 25 000 mg l while freshwater c 0 density is 1000 k g m 3 viscosity is assumed to be independent from salt concentration and hence has a constant value of 1 124 x 10 3 k g m s the aquifer storage parameters are respectively 0 005 m 1 0 18 and 0 2 for the specific storage specific yield and the effective porosity n 3 3 storm surge scenario the impact of a single storm surge event on coastal flow dynamics and on the investigated coastal aquifers is considered the storm surge results in overtopping flow over the dyke crest subsequently sea water inundates the hinterland behind the dyke where the sea water infiltrates into the soil and percolates through the unsaturated vadose zone towards the gwt therefore the storm surge event includes the processes of i sea level rise slr ii overtopping overflow and ponding iii sea level dropping and pond reduction and iv recovery of aquifer salinity to the initial state remediation the considered storm surge fig 9 induces a maximum slr up to 8 5 m a s l which is about 1 1 m higher than the dyke crest without consideration of the effects induced by short waves e g wave runup though the application of more realistic storm surge would be better it is preferred to use the same idealized storm surge curve of yang et al 2013 in order to reproduce the same event but with the proposed modelling scenario sc3 thus facilitating the compassion of the outcomes of this modelling approach sc3 using xbeach visual modflow seawat with the outcomes of the modelling approach sc2 using hgs by yang et al 2013 according to the estimations of yang et al 2013 the overflow lasts for 2 8 h and results in a maximum overflow rate of 200 l s 1 per meter dyke as reported by yang et al 2013 the 200 l s is considered as the maximum admissible value for overtopping flow according to eurotop 2007 by pullen et al 2007 thus a total of 1045 m 3 of seawater overtop the dyke during the 2 8 h of overtopping it is assumed that the salt concentration of the overflow water is as seawater concentration i e 25 000 mg l therefore a total of 26125 kg of salt is delivered to the hinterland during this overflow event 4 results in this study the modelling scenario sc3 fig 4 is tracked to simulate the naturally successive processes of water overflow flood propagation and induced saltwater intrusion in this way the xbeach model is used to perform the combined modelling of wave overtopping and induced flood propagation as discussed in section 4 1 below whilst the vertical infiltration of the flooding seawater to aquifer is discussed in section 4 2 the results of using a subsurface drainage system as a mitigation measure are discussed in section 4 3 4 1 combined modelling of overtopping and induced flood propagation as a first step of the modelling scenario sc3 wave overtopping and subsequent flood propagation are simulated using the hydro geo morphodynamic open source model xbeach roelvink et al 2009 the model is capable of simulating the nearshore hydrodynamics related morphodynamics and coastal erosion avalanching of coastal barriers induced by erosion induced instabilities and thus the overwash and breaching of coastal barriers are also simulated moreover the model capability to simulate both breaching and induced marine flooding has been recently demonstrated by elsayed and oumeraci 2016a 2016b nevertheless the groundwater module of xbeach does not yet account for advection dispersion and therefore it is not yet capable of simulating the swi this might represent one of the candidate priority topics for further improvement of xbeach so that barrier breaching induced inundation and subsequent swi can be simulated using a single fully coupled system 4 1 1 model set up in xbeach and inundation outcomes the cross shore profile in fig 8 is reproduced in xbeach in the form of a numerical wave flume with a width of 1 0 m alongshore and a cross shore length of 12349 2 m as a result the storm surge scenario in fig 9 is applied as the hydraulic marine load causing the overflow and the inundation behind the dyke the main parameters and boundary conditions used within xbeach to simulate this profile and the imposed hydraulic load are shown in table 2 while all other xbeach parameters see e g roelvink et al 2015 are kept by the default of xbeach as indicated in table 2 median grain size of 1 mm is considered as the representative median sediment size in the study area the latter value is obtained from a survey of the study area by meilianda et al 2011 the cross shore spatial step dx is considered regular of 12 36 m whilst one longshore spatial step dy of 1 0 m is considered like yang et al 2015b 2013 three different values for the manning coefficient n are used as indicated in fig 8 to account for the bed friction to the free surface flow in other words the land surface of the considered cross shore profile is divided into three parts where different n values are assigned i n 1 for the beach area ii n 2 for the dyke surface area and iii n 3 for the area behind the dyke for the beach area n 1 10 7 d a y m 1 3 0 00864 m 1 3 s is selected and behind the dyke n 3 6 10 7 d a y m 1 3 0 05184 m 1 3 s is used over the dyke n 2 0 00003 d a y m 1 3 2 592 m 1 3 s was calculated so that only maximum inland discharge of 200 l s is admitted yang et al 2013 considered that the water overflow over the dyke should not exceed an admissible value for overtopping obtained from eurotop 2007 page 49 in fact such assumption is illogical for such overflow case meanwhile it is the reason behind adjusting such very high manning coefficient n 2 over the dyke because no data are available for the short wave parameters e g significant wave height and peak period the short wave module of xbeach is switched off using the xbeach keyword swave 0 the latter means that many short wave processes e g wave breaking run up and run down are accordingly omitted nevertheless the slr in fig 9 is considered as the trigger for the overtopping flow process though water flow in the hinterland stretch in the downstream direction is permitted lateral flow is prevented using impermeable wall boundaries as lateral flow boundaries in order to simplify the flow to a 1d flow along the considered cross shore profile the model is run for 10 h using two modelling scenarios as described below while the output time step is 1 s two modelling scenarios for the free surface flow are preliminarily considered i morpho off scenario considers that no morphological evolution takes place because both the dyke and the beach area are highly protected against coastal erosion and overwash and ii morpho on scenario considers that the whole study area consents the coastal erosion and the induced morphological evolution the idea behind these scenarios it to understand the effect of unprotected coastal barriers on coastal flooding and subsequent swi the inland discharge q is calculated at the crest of the dyke point a in fig 8 showing that the inland discharges for both scenarios are identical as shown in fig s1 in appendix a accordingly the same inland water volume 2196 m3 propagates in the hinterland in both scenarios due to this overflow event thus 54 9 tonnes of salt are supplied to the hinterland the identical inland discharges for both simulation scenarios mean that considering the morphological evolution in the morpho on scenario has no effect on the inland flow for this specific case the latter becomes clearer from fig 10 a which illustrates the comparison of the evolution of bed levels and water surfaces along the whole profile for the simulation scenarios morpho on and morpho off it shows that the flood extents for both scenarios are identically increasing with the time marching until water flow is blocked after 10 h at a cross shore distance of 6400 m because of the local increase of the ground elevation at this point therefore the flood extends 5000 m behind the dyke for both simulation scenarios water depths along the 5000 m are spatially varying because of the spatial variation of the ground surface fig 10 b clearly shows these variations through comparing the initial at t 0 h and the final at t 10 h bed and water levels for both simulation scenarios table 3 summarises the main outcomes of the combined modelling of the overtopping flow and the flood propagation using xbeach including a comparison with the outcomes of yang et al 2013 for the same study area the latter volumes for the inland discharge and salt mass are more than twice the values estimated by yang et al 2013 using the surface module of hgs such significant differences arose from the fact that inland discharges calculated by hgs are based on the dsw approximation of the nlswes which ignore the inertial terms in the two momentum equations of the nlswes with the very high overflow velocities over coastal barriers which are common during overtopping and overflow conditions omitting the inertial term local and convective acceleration terms might reduce inland discharges as the very high flow velocities over the barrier are omitted in the calculation because of the dsw approximation xbeach however uses full terms of the nlswes to calculate the surface flow including the inertial term and hence accounts for the high flow velocities over the dyke thus calculating higher inland discharges bearing on a dsw approximation in the free surface flow calculations of hgs represents indeed one of the weaknesses of yang s study as aforementioned in section 1 in fact such incorrect estimation of the inland flow rates in yang s study results in an incorrect simulation of the flood propagation induced water depths and flood extents as demonstrated by elsayed and oumeraci 2016a yet incorrect estimation of flood extent and water depths might also affect the results of the subsurface flow and the contaminant transport as shown in section 4 2 4 1 2 role of micro and meso topographies in marine flood propagation microtopography refers to small depressions and bed forms and it is usually parameterized as manning s roughness coefficient while mesotopography refers to landscape scale features e g ponds and land elevations mcdonnell 2013 yu et al 2016b with the same micro and meso topographies the bed levels in the two modelling scenarios morpho on and morpho off show a non evolutive behaviour even with the permission of such evolution in the morpho on scenario in order to understand the reason behind such a behaviour a third modelling scenario is performed the latter scenario is identical to the scenario morpho on but a unique value for bed friction coefficient is used n n 1 0 00864 m 1 3 s this means that the used friction value for the beach area in front of the dyke n 1 0 00864 m 1 3 s is generalized over the whole cross shore profile thus the higher roughness values at the dyke n 2 and behind it n 3 are reduced to the value n 1 the idea behind this scenario is to allow higher flow velocities over the dyke crest and behind it by reducing the bed friction higher flow velocities definitely stir and move more sediment which is transported landwards and deposited behind the dyke the evolution of the bed and water surface for this scenario is shown in fig 11 when a lower manning value is assigned for both the dyke zone and the hinterland the dyke is totally overwashed accordingly resulting in higher water depths and wider flood extends behind the dyke the latter indicates indeed the importance of a suitable parametrization of the microtopography on the other side the mesotopography plays an important role in limiting the flood extent because of blocking the water flow at higher ground elevations see also yu et al 2016b after the storm peak at t 3 10 h the flood extent retreats with the decrease of the swl as well as with the induced return of the water to the sea ebbing recession conditions however part of the flood water is stored in the depressions as shown in fig 11 now for the morpho on scenario it is clear that the high manning value at the dyke zone n 2 2 592 m 1 3 s limits the flow velocity over the dyke to be always under the threshold value for the onset of sediment motion as described by the shields criterion shields 1936 as a result no morphological evolution takes place with morpho on scenario as this is the case for the morpho off scenario this is indeed the reason why both morpho on and morpho off scenarios provide identical outcomes for the flood extent and water depths 4 2 modelling storm induced saltwater intrusion near bremerhaven using visual modflow seawat for simulating the swi induced by marine flooding for the study area near bremerhaven due to the storm surge scenario in fig 9 the visual modflow 2011 1 graphical interface of waterloo hydrologic waterloo hydrogeologic 2015 is applied in this study visual modflow is a finite difference based interface supporting among others simulation of the conservative nonreactive and single phase miscible transport of seawater in coastal aquifers using its built in seawat module besides their applications in many swi studies e g ding et al 2014 gopinath et al 2016 nofal et al 2014 the key advantage of seawat visual modflow is the inclusion of diverse easy to use boundary condition packages the most important package for the purposes of this study are the river and drainage packages that can respectively simulate the surface subsurface interaction and drainage from the subsurface the inclusion of the drainage package makes visual modflow the most suitable model for the purposes of this study to avoid further developments in other models e g opengeosys moreover seawat is verified with semi analytical solutions of benchmarks e g henry problem henry 1964 see e g elsayed and oumeraci 2017b for the verification results the near bremerhaven model setup using seawat visual modflow and the simulation outcomes are presented in the following subsections 4 2 1 modelling hypotheses conceptualization and setup the aim of modelling the swi induced by the inundation event aforementioned in section 4 1 for near bremerhaven aquifer is threefold i studying the distribution of the saltwater in the aquifer in addition to determine the maximum subsurface extent of the contaminant after the inundation event ii estimating the natural remediation time interval that is needed for the aquifer to get remediated naturally and iii examining the suitability of a subsurface drainage network as a mitigation measure to shorten the natural remediation time for these purposes the water depths in the hinterland as shown in fig 12 a need to be transferred to an seawat model for the aquifer near bremerhaven however water depths during such a flood event vary in both space and time as shown in fig 12 a because of the temporal variation of the inland flow rates and the spatial variation of the ground elevations as shown in fig 12 b the spatiotemporal variation of the water depth in the hinterland extends until the water flow in the downstream direction is blocked by a local rise in the ground elevation as shown in fig 10 b which means that marine flood reaches its maximum extent 5 km behind the dyke after 10 h therefore the water depth at the time of maximum flood extent as shown in fig 12 b i e at t 10 h is selected to be transferred to an seawat model for the aquifer near bremerhaven nevertheless the latter water depths still vary spatially due to the spatial variation of the ground elevation therefore to simplify the analysis some modelling hypotheses need to be addressed as follows i the inland flow 2196 m 3 is uniformly distributed along the flood extent of 5 km as shown in fig 12 b which means that a water depth h 0 44 m averaged over the flood extent and not the spatially varying water depth is considered as the external head causing the vertical intrusion of saltwater this substitution aims at simplifying the input head to the swi model ii tidal fluctuations in the sea can be omitted and hence a stable sea level of 0 00 m can be considered as the mean sea level msl iii all water overtopping the dyke will infiltrate into the aquifer along the flood extent which means that no evaporation is considered during the percolation time iv the infiltration during the flood propagation i e before flood water gets standing interval of 10 h can be omitted v the standing water after the interval of 10 h infiltrates into the aquifer by a rate depending on both the hydraulic head difference and the conductance rate from the land surface to the gwt this is being performed using the modflow river riv package as will be discussed in section 4 2 2 using these hypotheses as well as the model data from section 3 the conceptual model of the aquifer can be drawn as shown in fig 13 a constant domain bottom elevation of 100 m a s l is used as the aquifer bottom which is assumed impermeable at the seaside a constant water head h 0 m and a constant salt concentration c 25000 mg l with seawater density ρ s 1018 3 k g m 3 are considered as indicated in table 1 at the landside boundary a constant water head h 4 m and a constant concentration c 0 mg l are also considered but with a freshwater density ρ 1000 k g m 3 along the ground surface from the shore line to inland an effective surcharge is considered with 300 mm yr which represents a feeding source of freshwater another external load vertical saltwater intrusion is considered along with the 5 km flood extent behind the dyke which represents the contamination source at the ground surface fig 12 b the aquifer properties e g hydraulic conductivity porosity etc are assigned using the values in table 1 after considering the installation of subsurface water collectors for the purpose of faster remediation see section 4 3 subsurface drains are added beneath the ground surface directly exposed to vertical swi from coastal flooding the conceptual siswi model from fig 13 of the aquifer near bremerhaven is set up in visual modflow using 500 columns in x direction δx 24 6 m 1 row in y direction δy 1 0 m and 24 layers the layers thicknesses are small near the ground surface while they have uniform thicknesses of 5 m downwards for each cell prism the aquifer properties in table 1 are assigned for instance a kx ky and kz value of 0 005 m s are assigned for each cell to represent a homogenous soil of hydraulic conductivity equals 0 005 m s after assigning the aquifer properties e g hydraulic conductivity porosity etc two initial conditions are defined first the initial water head in the aquifer is set at 0 25 m above the reference level which is the msl 0 00 m a s l this initial head is roughly chosen as an initial condition for the head in the aquifer second the initial salt concentration in the aquifer is set at 0 mg l except at the seaside boundary it is set at 25000 mg l the other boundary conditions are defined in fig 13 for instance constant water head boundaries of 4 00 m and 0 00 m are defined at the landward and seaward boundaries respectively applying this model two modelling situations are considered i pre storm conditions which is run for 5 years before applying the saltwater head at the ground level and ii storm and post storm conditions which is run for 45 years after applying the saltwater head in order to cover the time needed for the natural remediation of the aquifer after such inundation event thus the model is run for 50 years 18250 days the first 5 years are considered as warm up phase of the model in order to eliminate the effect of initial conditions the outcomes of these successive runs are summarised in the next section 4 2 2 effect of saltwater inundation on freshwater in the aquifer near bremerhaven out of the 5 years of warming up which aim at eliminating the effect of the initial conditions and at reaching the steady state before applying the inundation induced water head from the ground surface the system reached the steady state after three years 1095 days after these three years the mismatch between water inflow q i n from both the ground level as a recharge and the landward boundary as a seaward directed flow as in figs 2 and 3 and outflow q o u t rates from the aquifer throughout the sea land boundary becomes constant because of the stationary hydrogeological conditions on the other side the saltwater in the sea intrudes laterally into the aquifer to the landward direction during this warming up phase because of being heavier than freshwater in fact saltwater intrudes laterally through the seaside boundary and recirculates after the dilatation process to leave the aquifer from near ground level through the same boundary the lateral swi reaches the equilibrium condition after 290 days 1 year this equilibrium condition represents the position when the mismatch between the source in mass salt entering the freshwater aquifer and the sinking out mass salt leaving the aquifer becomes stationary in other words the salt freshwater interface represented by the 50 isoconcentration contour which is corresponding to c c s e a w a t e r c f r e s h w a t e r 2 25000 0 2 12500 mg l becomes stationary after 290 days because the net mass source in sink out in the aquifer is constant the equilibrium condition achieved after the three years out of the five years for the warming up is a function of the model inputs and initial as well as boundary conditions this means that any increase in the sea level will definitely result in further saltwater intrusion landward r1 in figs 1 and 2 the same could be achieved by decreasing the effective recharge value less than 300 mm year which might decrease the gwt r2 in figs 1 and 2 therefore the sea level rise and the recharge value and or the level of the gwt are the triggering factors which determine the equilibrium condition in the aquifer see also yang et al 2017 these factors could be indeed affected by the changing climate parry et al 2007 stocker et al 2014 vousdoukas et al 2017 nevertheless they are assumed stationary in this study considering that the model has reached the steady state after 3 years the inundation load is applied 5 years 1825 days after the simulation start i e 2 years after reaching the steady state the inundation effect is assigned to the model through the modflow river riv package waterloo hydrogeologic 2015 the latter package enables a virtual definition of an external and uniform water head above the ground surface for a certain time as a result the interaction between the surface head and subsurface flow in the aquifer is possible through the conductance value calculated by the riv package which represents a proxy coefficient measure for calculating river aquifer interaction korkmaz et al 2016 pérez paricio et al 2010 in other words the riv package enables the flow of the vertically infiltrating seawater through the vadose zone using a simplified model that assumes uniform and so much lower conductivity through the vadose zone than the saturated zone in fact the hydraulic conductivity decreases significantly with the volumetric water content see e g nimmo 2009 therefore the hydraulic conductivity through the vadose zone is assessed based on the soil type by 5 10 6 m s this conductivity value is used through the riv package to calculate the conductance rate from the ground level to the gwt see waterloo hydrogeologic 2015 for the details as a result it is found that the inland volume of 2196 m 3 that is calculated by xbeach as aforementioned in section 4 1 1 see table 3 will percolate to the aquifer within 4 days in fact the reliability of the latter percolation time is not of high importance since this time is very short relative to the natural remediation interval 44 3 years as computed below therefore conducting the flood volume to the aquifer in 4 days will not differ so much than conducting the same volume in 1 0 h or even in two months this is because the movement of water in the aquifer is very slow which means that the conductance time is not of high importance to the salt transport in the aquifer what is really important is the conductance volume 2196 m 3 which should equal to the inland water volume in order to ensure that the same overtopping volume is conducted to the aquifer using the riv package of modflow the inundation volume is transferred to the aquifer along the 5 km behind the dyke so that the substitute water head 0 44 m from fig 12 b remains constant until the same overtopping volume 2196 m 3 infiltrate into the aquifer along the flood extent being denser and miscible with freshwater in the aquifer saltwater at the ground surface along the flood extent infiltrates downward and blends with freshwater in the aquifer which increases the groundwater salinity therefore a defection of the salt mass budget of the aquifer is expected after such coastal flooding in fact this defection can be explained using the three curves in fig 14 namely the accumulative source in mass curve red curve the accumulative sink out mass curve green curve and the curve of total mass remaining in the aquifer blue curve the latter curve represents indeed the mismatch between the two former curves so that its values can be read separately from the vertical axis on the right fig 14 as shown in fig 14 the net mass blue curve increases dramatically at the beginning of the simulation warming up phase because of the significant amount of salt entering the aquifer from the seaside boundary to satisfy the steady state see elsayed 2017 and elsayed and oumeraci 2017b for more illustrative details after 290 days the net mass becomes almost constant red and green curves become parallel which means that the salt freshwater interface represented by the 50 isoconcentration contour as shown in fig 15 becomes stationary after 290 days because the net mass source in sink out in the aquifer is constant then the vertical leakage of saltwater within the conductance vertical infiltration interval t 1825 1829 days increases the accumulative source mass in the aquifer by 54 9 tonnes see detail b in fig 14 such increase of the source in mass cannot sink out immediately from the aquifer the latter fact can indeed be interpreted through the accumulative sink out mass curve green curve which still shows the same gradual increase without any defection during the conductance interval despite the increase of the accumulative source in mass by 54 9 tonnes during the same interval this means that the increase of the source in mass during the conductance interval is totally stored in the aquifer as represented by the blue curve this stored mass sinks out the aquifer gradually until the aquifer is totally remediated after 44 3 years the natural remediation interval is therefore the time needed after a coastal flood event in order to totally drain the contaminant salt from an aquifer and to reach again the pre flooding situation in this case the bremerhaven aquifer will be remediated totally and reach again the pre flooding condition after 44 3 years almost 4 and half decades in fact this reflects the significant threat of coastal floods to coastal aquifers a flood over few hours can contaminate aquifers for decades which limits the use of aquifers and increases the water treatment costs moreover it hinders the dependence on coastal aquifers in possible sustainable development planning for coastal zones the latter remediation time means that freshwater zones in the aquifer which are affected by the vertical leakage of saltwater will return to its initial state 0 mg l of salt concentration after this very long time the aquifer recovery is due to the natural remediation of the aquifer owing to the seaward directed flow in addition to the recharged part of rain precipitations on the ground surface in fact seaward directed freshwater flow dilutes the infiltrating saltwater and moves it seaward gradually until the aquifer is totally remediated the latter processes are extremely slow and hence very long intervals are needed for total aquifer recovery nevertheless shorter intervals might be admissible if higher concentration values are accepted for instance in the pre flood freshwater zones a salt concentration of 500 mg l can be reached after 25 years see panel g of fig 15 the latter concentration is acceptable as the maximum allowable salt concentration in drinkable water by who and the eu fwd the zones of higher lower concentration can be determined by investigating the salt distribution in the aquifer at different times as shown in fig 15 before the flood event there is only lateral saltwater intrusion panel a in fig 15 after seawater overtopping the saltwater infiltrates into the aquifer along the 5 km flood extent the contaminant salt spreads vertically during the inundation interval as shown in panel b of fig 15 since the infiltrating saltwater is heavier in weight than the prevailing freshwater in the aquifer even after 3 months panel c and one year panel d the salt diffusion is still in the vertical direction therefore saltwater moves vertically beneath the flood extent until it mixes with the freshwater along the aquifer depth such a vertical salt infiltration deviates toward the sea under the effect of seaward directed freshwater flow in fact freshwater moving seaward triggers the dilation process in the aquifer as shown in panels e h this dilation process results in a process of natural remediation of the aquifer until the aquifer is almost remediated totally after 44 3 years as shown in panel h of fig 15 by comparing the iso concentration contours of 50 and 2 in panels a and h one may notice that these contours are almost in the same spatial position which means that the aquifer is almost remediated after this relatively long interval the zone in the right and beneath of the 0 contour line c 0 1 mg l is purely freshwater without any dissolved salts therefore it is the safest zone for pumping the 2 isoconcentration contour corresponds to the maximum allowable dissolved salts in drinkable water according to the standards of who who 2011 and the eu wfd kaika 2003 therefore any pumping from the zone at the right side or beneath of the 2 isoconcentration contour could also be possible of course the farther the pumping well is from the 2 contour line in the landward direction the more usable the extracted water and also the better to avoid further intrusions and upconing in the case of excessive pumping the contamination of the same aquifer near bremerhaven has also been studied by yang et al 2013 using hgs yang s study reported that salt concentrations higher than 500 m g l can still be found close to the aquifer bottom even after 20 years which is consistent with the outcomes of this study in fig 15 after 20 years see panel e the difference between both outcomes is the shape of saltwater spread in yang et al 2013 plume fingers developed in the aquifer while the analysis in this study show a uniform spread the latter behaviour arose from applying a uniform hydraulic head as discussed in the modelling assumptions in section 4 2 1 the former behaviour may be explained by the use of a surface subsurface model in yang et al 2013 which enables earlier and longer infiltration time from the depressions resulting in plume fingers yu et al 2016b unlike this study which specify the time interval for the total remediation by 44 3 years yang et al 2013 did not specify this time interval because they were interested only in determining the remediation interval until reaching the limit 500 mg l the maximum salt concentration for drinkable water according to who to overcome such a long term remediation interval it is crucial to look for an effective mitigation measure that can make coastal aquifers more resilient during and after coastal floods for the latter purpose subsurface drainage network fig 6 might be the proper choice in flood prone coastal zones especially agricultural areas because saltwater infiltrating during and after a flood event can be partially absorbed and evacuated through drains before it contaminates the whole aquifer the results of the examination of this suggestion using the proposed modelling approach are discussed in the next section 4 3 subsurface drainage effect on the resilience of coastal aquifers against coastal floods a subsurface drainage system fig 6 is a man made system that can cause excess water and dissolved salts to flow through the soil to pipes from where they can be evacuated therefore subsurface drainage in coastal areas might function for the two purposes i improving the agricultural yield and crops productivity and ii absorbing the infiltrating saltwater that intrudes vertically from the land surface to the freshwater aquifers during a coastal flood event thus subsurface drainage is used where the soil is permeable enough to allow economical spacing of the drains and performance enough to justify the investment moreover it should provide almost trouble free service for many years the feasibility of using a subsurface drainage system as a mitigation measure for siswi is demonstrated in the following subsections 4 3 1 model set up and parametrization for the aquifer with subsurface drainage in visual modflow a subsurface drainage system consists of a surface or subsurface outlet and subsurface main and lateral drains water is carried into the outlet by the main drains which receives water from the lateral drains also known as inceptors or water collators because subsurface drainage is used primarily to lower the water table or to remove excess water that is percolating through the soil over a general area the drains are placed in a pattern determined by the characteristics of the area kalita et al 2007 in homogeneous soils parallel patterns are used to lower the water table at the same rate on both sides of each drain in heterogeneous soils however random patterns might be more appropriate the spacing and depth of drains influence the groundwater level between drains the required drain spacings and depths depend on soil permeability and on the amount and frequency of rainfall as well as on land topography according to drablos and moe 1984 drain spacings in highly permeable soils should be between 60 and 90 m and between 30 and 60 m in moderately permeable soils where soil permeability is moderate spacing should be between 25 and 30 m in soils with a low permeability or moderate permeability drain spacing should be respectively spaced 10 22 m or 18 25 m regarding the drains depth subsurface drains are often installed based on the desired water table so the depth ranges between 1 and 1 5 m beneath the pre installation water table for highly to moderately permeable soils the latter depth decreases to 0 3 m in the case of lowly permeable soils because the rate of lateral water movement does not increase in this case with depth the range of slopes on which drains can be placed depends to some degree upon the topography of the land the slope should be steep enough to prevent silting but flat enough to prevent flow from exceeding the allowable velocity and from subjecting the drain to excessive pressure the inflow rate to the drains depends on the soil texture and permeability as well as on the surcharge rate to the ground surface and head distribution around the drain indeed there is no general formula for calculating subsurface drain conductance c d c d refers to the absorption rate of a subsurface drain from the ambient soil by m3 day m length of the drain this is due to lack of the detailed information required for the calculation such information may include the detailed head distribution around the drain the aquifer hydraulic conductivity near the drain the distribution of fill material the number and size of the drain pipe openings the amount of clogging materials and the hydraulic conductivity of clogging materials nevertheless the available drainage manuals e g drablos and moe 1984 provide tentative values for subsurface drains conductance based on soil permeability for instance drain conductance in sandy soils ranges from 0 56 to 2 m 3 d a y m while the range of 1 2 8 m 3 d a y m is common for coarse sand and gravel soils silty soil conductance ranges between 0 32 and 0 8 m 3 d a y m while the range of 0 16 0 8 m 3 d a y m is common for clayey soils based on these common values for drain spacing depth and conductance the effect of subsurface drainage on the saltwater intrusion at bremerhaven aquifer can be studied based on the conceptual model in fig 13 thereby the conceptual model is the same as that used for calculating the natural remediation time but subsurface drains are added beneath the ground surface directly exposed to vertical saltwater intrusion from coastal flooding because the soil in bremerhaven can be considered as homogeneous parallel pattern consisting of parallel lateral drains are used in order to avoid further lateral intrusion when using subsurface drainage the balance between freshwater in the aquifer and saltwater in the sea should be considered this means that deepening the drains levels than necessary might allow further lateral intrusion therefore the drains are set at level of 0 50 m a s l which is at least 0 5 m lower than the ground level at the lowest ground level in the hinterland the soil in bremerhaven is highly permeable ks 0 005 m s hence the spacing between the collectors is set at 73 8 m centre to centre circular collectors as shown in fig 13 cannot be defined in visual modflow because of being a finite difference based model utilising rectangular meshes therefore cells containing subsurface drains are defined as drainage cells using the modflow drainage package drn see detail c in fig 13 since the conceptual model is discretized uniformly in the cross shore direction with δ x 24 6 m the face to face distance between drainage cells is 49 2 m as referred to in detail c in fig 13 drain conductance is indeed the most important and the highly uncertain parameter when analysing the subsurface drainage effect it is usually adjusted during model calibrations based on surcharge rate to the ground surface and head difference between drains and gwt based on the latter factors and the aforementioned tentative values for the subsurface drain conductance the conductance value for each drain is assumed along the simulation time 50 years as in fig 16 with the reasoning of these choices in table 4 based on the parameterized values for drains spacing depths and conductance the effects of the considered subsurface drainage on both water flow and mass transport in bremerhaven aquifer are respectively discussed in the following two sections 4 3 2 subsurface drainage effect on water flow in bremerhaven aquifer drainage is an important factor in the aquifer budget since it is a sink out facility this is the reason why it is sidestepped during the first 3 years of warming up by setting c d 0 as in table 4 to allow the model to reach the steady state as aforementioned in section 4 2 2 after the first 3 years the drains start to absorb water from the aquifer as shown in fig 17 based on the assigned value for drain conductance c d during the interval of the inundation induced infiltration 4 days the drainage rate reaches its maximum value which is an evidence that drainage rate depends on both the drain conductance and the surcharge rate on the ground surface therefore it was wise to change the drain conductance value with the surface hydraulic load though the conductance value is constant during the first year after the flood c d 2 0 the drainage rate decreases because of the drainage induced lowering of the gwt during this year the sudden drop in the conductance value from 2 0 to 1 0 at the start of the second year after the flood results in a sudden decrease in drainage rate which become uniform during this year because of the constant conductance value a similar behaviour takes place with the decrease of the conductance value at the end of the latter year during the last 43 years the drainage rate is constant because of the constant conductance and the steady state in the aquifer in fact fig 17 shows that the drained water through the subsurface drains would sink part of the infiltrating saltwater out the aquifer around 50 of the infiltrating water from the land surface for this particular spacing and conductance of the drains except during the high conductance values the latter percentage might range between 150 and 350 so that the time needed for the natural remediation would accordingly decrease this aspect is examined in the next section 4 3 3 subsurface drainage effect on saltwater intrusion to the aquifer near bremerhaven subsurface drainage is selected as a mitigation measure tool for the siswi but does it really solve the problem in order to answer this question the accumulative source in and sink out masses should be compared side by side with the salt mass remaining in the aquifer as in fig 18 a the accumulative input mass red curve in fig 18 a increases by 54 9 tonnes owing to the inundation such increase of the input leads to a sudden like increase of the salt mass remaining in the aquifer blue curve in fig 18 a after flooding where higher conductance values of drains are assumed the accumulative output mass decreases as shown in detail e in fig 18 a because the gwt sinks owing to drainage the decrease of the accumulative output mass increases indeed the mass remaining in the aquifer even more than the case without drainage as shown in fig 18 b which compares the salt masses remaining in the same aquifer under the same conditions with and without using the drainage system drainage in fact facilitates collection of the infiltrating saltwater but at the same time it decreases the level of the gwt leading to more lateral intrusion from the seaside because of the balance defect between freshwater head in the aquifer and seawater head in the sea as previously shown in fig 2 b as shown in fig 18 b the mass remaining in the aquifer during the warming up phase the first 3 years is the same with and without drainage because the drains conductance is set at zero during the following two years which are also before applying the flood effect the salt mass remaining in the aquifer increases with the drainage than without it the latter is because the drainage reduces the gwt leading to a lateral shift in the salt freshwater interface landward the reason r2 as indicated in fig 2b during and after the flood by one year the conductance value is the highest but the mass of salt in the aquifer is increasing dramatically than without drainage this increase extends to the second year after flooding until it reaches its peak with the reduction of the conductance value from 1 to 0 06 m 3 day m with the latter reduction the mass in the aquifer decreases gradually nevertheless the remaining salt mass in the aquifer in the case of using subsurface drainage remain higher than without drainage even after the 45 years after the flood event the previous explanations can be clearer by visualising the salt distribution in the aquifer at different times as in fig 19 as shown in fig 19 panels c and d the subsurface drains collect part of the infiltrating saltwater especially during intervals of higher drain conductance nevertheless the rest escapes downward among the drains leading to the zigzag shape of the concentrations among drains panels e g show that drainage has proved its efficiency in confining the high salt concentration near to the ground surface in fact drainage was capable of controlling the unwanted deeper infiltration of the high salt concentrations in the aquifer highly concentrated saltwater is collected from the shallow zones within the three years after flooding as shown in panels g and h therefore shorter remediation intervals 3 years can indeed be achieved using closer drains as the wider drains spacing allows more saltwater to escape to the deeper freshwater in the aquifer however the efficient role of the drainage in shortening the remediation time of the vertical intrusion is generally at the expense of more lateral intrusion because of the drainage induced lowering of the gwt 5 summary and discussion of the results in this study the implications of storm surge induced barrier overtopping breaching for coastal aquifer contamination are explored vertical saltwater intrusion swi due to coastal inundation may increase the salinity of the originally fresh groundwater which may significantly reduce the water quality and the environmental values of groundwater thus possibly hindering any possible sustainable development in coastal zones prone to marine floods in fact storm surge induced saltwater intrusion siswi may result in widespread aquifer contamination that might last for several years or even decades until aquifers get remediated naturally even a moderate storm surge event may significantly affect the usability of coastal aquifers for many years siswi often starts with overtopping overflow or and breaching of a coastal barrier inducing inland inundation and subsequent vertical intrusion of seawater into coastal aquifers thus the feasibility of modelling these three processes in a single model system is addressed in this study showing that this modelling scenario is the most appropriate but not yet feasible in fact the most suitable two models to perform this task i e xbeach and hydrogeosphere still need further developments fig 20 in order to be able to simulate these three processes successively for instance the groundwater module of xbeach still needs further development to account for density dependent mass transport moreover darcy equation for saturated flow in xbeach needs to be replaced by richards equation for the variably saturated flow so that the flow above and beneath the phreatic line can be simulated furthermore xbeach might need definitions of new boundary condition packages so that recharge and seaward directed flow can be assigned by these three improvements xbeach might be able to successively simulate breaching inundation and intrusion in a single model on the other side a proper geo morphodynamic module as shown in fig 20 is necessary in the surface module of hydrogeosphere in order to simulate coastal erosion and breaching of coastal barriers moreover the diffusive wave approximation dsw needs to be replaced by the full nlswes as in xbeach so that the inertial acceleration term can be considered in order to achieve reliable inland discharges by hydrogeosphere the model should also include a short wave module to account for the short wave action and the associated processes such as breaking run up etc despite the need for many improvement and extensions as shown in fig 20 xbeach and hydrogeosphere still remain as the most eligible models for future improvement in order to successively simulate barrier overtopping breaching induced floods and subsequent swi in a single model system as a fully coupled modelling system is not yet valid a new modelling scenario is proposed in this study using the hydro geo morphodynamic model xbeach for overtopping barrier breaching and subsequent coastal flooding and seawat visual modflow for groundwater flow and swi in coastal aquifers the application of this modelling scenario to a pilot site near bremerhaven northern germany showed that a storm induced barrier overtopping breaching might result in a relatively wide flood extent depending on the inland discharge and on the mesotopography of the hinterland where propagating saltwater in the hinterland infiltrates to freshwater aquifers resulting in their contamination the natural remediation of an aquifer after such coastal floods is possible owing to the continuous dilation of the saltwater in the aquifer by both the recharged part of precipitation and the seaward directed flow however such remediation process is so long and may take decades to reach again the pre flooding fresh groundwater conditions for the study area near bremerhaven which was also considered by yang et al 2013 using the hydrogeosphere model xbeach is used to calculate in combination the overtopping rates over the dyke and the induced flood extent and water depths in the hinterland behind the overtopped coastal barrier the outcomes of this step exhibited more overtopping discharge and hence a wider flood extent than that were predicted in the study of yang et al 2013 possibly because of using a dsw approximation in the hgs model to calculate the flood propagation which omits the inertial acceleration term of the nlswes the outcomes of xbeach are manually transferred to the visual modflow model which simulates the subsequent swi using its seawat module the inundation induced swi takes around 45 years after the inundation until aquifers are remediated and reached again the pre flooding conditions nevertheless 25 years might be sufficient to achieve the maximum allowable concentration for drinkable water 500 mg l the latter interval is only 5 years more than the interval calculated by yang et al 2013 to achieve the same concentration in the aquifer such longer five years might be due to the wider flood and contamination extents calculated by the current study in order to shorten such long remediation intervals subsurface drainage is suggested as a structural mitigation measure drainage manuals e g blann et al 2009 and drablos and moe 1984 are used to specify the spacing between the drains the installation depth and expected drain conductance the use of the subsurface drainage system as a structural mitigation measure allows us to shorten the long intervals needed for the natural remediation to less than three years but generally at the expense of landward shifting of the salt freshwater interface i e further lateral intrusion such a lateral intrusion arises from the fact that subsurface drains reduce the gwt or at least absorb a large part of the recharge from precipitation which represents a continuous feeder for freshwater in the aquifer therefore subsurface drainage represents an appropriate choice since it reduces the contamination extent in a relatively shorter time and widens the usable part of the aquifer moreover the installation of such subsurface drainage system might improve the agricultural yield because of lowering the groundwater table as schematically shown in fig 3 a the side effect of this choice is the possibility of further lateral intrusion around 500 m in the study case near bremerhaven which might be acceptable in order to benefit from drainage for instance in increasing the agricultural yield as well as in shortening the long remediation intervals after a coastal flood event 6 concluding remarks and outlook despite the encouraging results of applying the proposed modelling scenario sc3 fig 4 to simulate the vertical swi in the aquifer near bremerhaven the modelling approach suffers from the following limitations i many modelling assumptions need to be introduced as discussed in section 4 2 1 which may affect the reliability of the modelling outcomes ii a manual transfer from one model to another could not be fully avoided using modelling scenario sc3 which may also affect the accuracy of the modelling outcomes i e assuming an equivalent uniform water head as indicated in fig 12 b results in uniform salt migration downward rather than forming salt plumes as the case in the study of yang et al 2013 therefore the best alternative to simulate an siswi induced by barrier breaching overtopping is to simulate the involved processes in a single fully coupled model system nevertheless bearing in mind the present lack of such a model system the application of xbeach to simulate barrier overtopping breaching and subsequent inundation besides using for instance hydrogeosphere to simulate the surface subsurface interaction and the accompanied contamination transport might represent an optimal modelling option in this way flood propagation in the hinterland will be calculated first by xbeach simulating breaching and subsequent inundation and second by feeding the inland hydrograph calculated by xbeach to the surface subsurface hgs model as explained in elsayed and oumeraci 2016a this might help to confine the modelling inaccuracies as another alternative xbeach can be coupled with another open source subsurface model e g opengeosys by kolditz et al 2012 so that both models can be linked to successively simulate coastal erosion and barrier breaching as well as inundation using xbeach then the inundation outcomes are automatically transferred to opengeosys to simulate the subsurface process and the accompanying swi using the latter approach will definitely help in avoiding licensed models which are very expensive e g hydrogeosphere moreover it will provide flexible environment for further model developments to suit the requirements of each individual research project this study represents the foremost study that attempts to mitigate storm induced saltwater intrusion through the use and modelling of subsurface drainage network making it quite relevant for the coastal engineering community for flood risk managers for groundwater suppliers as well as for sustainable development planners despite these encouraging results on the efficiency of subsurface drainage as a structural siswi mitigation measure further research is still needed to better evaluate drains conductance in the light of experimental work by considering various surcharge rates and different soil types moreover drainage effect needs to be studied using a surface subsurface model e g hydrogeosphere so that the flow through the vadose zone can be better represented in the simulation rather than the simplified approach using the riv package in this study in fact the simple representation of the flow through the vadose zone using the conductance concept might affect the simulation outcomes therefore the outlook for future research based on the outcomes of this study might be outlined as follows i drain conductance represents the main uncertain parameter which needs to be investigated in the light of experimental work by considering a wide range of surcharge rates and different soil types ii the effect of soil heterogeneity on spacing and conductance of subsurface drains need to be investigated iii the drainage effect needs to be studied using a surface subsurface model e g hydrogeosphere so that the flow through the vadose zone can be better simulated in fact this suggestion was not possible through this study due to the expensive license of hydrogeosphere iv open source models e g xbeach and opengeosys need to be coupled in order to successively simulate coastal erosion barrier breaching marine flooding and subsequent swi being open source models their available source codes for diverse researchers worldwide might facilitate this task acknowledgements this research is a part of the phd study of the first author which is supported by the daad in the frame of the exceed swindon project at technische universität braunschweig this support is gratefully acknowledged the data for the case study of near bremerhaven is gratefully provided by prof dr thomas graf and dr jie yang appendix a supplementary data the following is the supplementary data related to this article supplementary data supplementary data appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2017 11 030 
26463,we present a web based software platform for assimilation of field data on groundwater vulnerability and assessment of groundwater pollution risk groundwater vulnerability and risk assessments couple hydrogeologic characterization and pollution source information to provide a relative measure of risk to a groundwater supply vulnerability is based solely on hydrogeologic factors while risk also considers pollution hazards previous studies have identified two key limitations in the collection of pollution hazards information detailed investigations throughout a study area are extremely time consuming and pollution inventories are not regularly updated to reflect changes in land use this paper presents a methodology to address these two limitations by allowing broad based effort to collect pollution hazard parameters in the field using gps tagged smartphone forms and incorporating the new information rapidly into a web based risk modelling framework to demonstrate this approach a case study is presented from the natuf basin in the ramallah al bireh governorate of palestine keywords distributed data collection web based gis risk assessment groundwater pollution 1 introduction this paper focuses on the development and application of a networked software platform supporting efficient and flexible groundwater pollution risk modelling this section will introduce key principles of groundwater vulnerability and risk assessments and motivate the use of new software tools to improve these models intrinsic groundwater vulnerability can be defined as an intrinsic property of a groundwater system that depends on the sensitivity of that system to human and or natural impacts vrba and zaporožec 1994 in this study vulnerability was considered to relate to features of local hydrology soil and geologic layers that may mitigate contaminants introduced by human activities vulnerability maps portray the effectiveness of natural filtration processes in certain areas relative to others within a geographic study domain such maps are commonly used by land use planners to evaluate the potential relative impact of human development on groundwater quality in different areas vrba and zaporožec 1994 vulnerability is solely a function of hydrogeologic factors characterizing the overlying soil and geological materials and does not consider human activities doerfliger et al 1999 vulnerability assessments can be expanded to obtain overall groundwater pollution risk measures which consider both natural and anthropogenic factors in general risk is defined as the likelihood of certain events occurring and the magnitude of their possible consequences simpson et al 2014 this study considers a rather narrow definition of risk namely that human activities at or near the land surface will introduce contaminants to the groundwater supply which may be harmful to human health this definition was adopted for simplicity as our primary goal is to demonstrate the concepts and ideas behind the risk platform the risk maps we consider portray a relative measure of the risk of aquifer contamination resulting from human activities in the study area these maps are obtained by combining intrinsic vulnerability with an inventory of potential groundwater pollution hazards hazards are generally assessed using a combination of information about the type of potential contaminant release the extent of a potential release and the likelihood of such a release given containment measures in place at the site published groundwater pollution risk studies zwahlen 2003 simpson et al 2014 have identified limitations in current methodologies particularly related to creating and updating the hazard inventory for one hazard inventories are generally time consuming and labor intensive to collect detailed on site investigations throughout the study area which are ideal for hazard data collection are extremely time consuming and are therefore rarely done simpson et al 2014 instead hazard inventories are often created from existing land use data and may represent general rather than specific assessments in such assessments basic land use data for each type such as activity type size and age are used to estimate a generalized hazard value in some cases more specific information about certain point hazards is added to land use data simpson et al 2014 the second main limitation in many hazard analyses is that hazard inventories are produced during the data collection phase of the risk assessment and reflect conditions only as they were at that point in time however changes in land use may introduce new hazards remove old hazards or change the nature of certain hazards zwahlen 2003 some of these changes can occur on a daily or weekly time scale when subsurface residence times are short as in karst systems new hazards can quickly impact the water supply and should result in rapid changes in the risk assessment zwahlen 2003 however without ongoing hazard data collection these changes are not reflected in the risk map being used by planners risk values may no longer reflect the current land use distribution and decisions based on this information may be affected this study addresses the two main limitations of hazard inventories outlined above a methodology for distributing the data collection burden collecting and evaluating hazard information in situ and introducing temporal variation into a risk assessment algorithm is developed in this approach information about pollution hazards is stored in an online database which is available selectively to multiple users up to date information about hazards can be collected by multiple users gps tagged in the field using a smartphone and submitted to the database where it can be analyzed and shared in real time rubin and michaelis 2017 new additions changes or deletions in the database are reflected in the risk model which is recalculated daily or as otherwise directed previous work for example by granell et al 2010 has introduced decentralized geospatial processing tools in order to reduce tedious or repetitive tasks being carried out by researchers during the modelling process this approach extends that work to also include decentralized collection and integration of model data from the field it applies decentralized data collection principles such as those discussed in horsburgh et al 2011 directly to an environmental modelling problem resulting in a framework that allows for decentralized collection and integration of new data into the model and also automates the task of updating the model over time by collecting storing and geoprocessing model data all within the same system this approach avoids interoperability issues and challenges integrating data and modelling services peckham and goodall 2013 castronova et al 2013 this methodology is applied in a case study and the results are discussed we show that the risk model changes significantly as new hazard information is added by researchers in the field and that the proposed software platform supports rapid updating through community participation in a developed model this new information could represent previously non existent hazards such as new garbage dump sites or chemical spills or the removal or modification of previously existing ones 2 methods a simplified risk modelling problem definition has been adopted here in order to focus our presentation on the networked software platform s ability to overcome key risk modelling limitations a similar approach can be taken with more complex risk modelling methodologies and in different settings the procedure was designed to allow flexibility in updating the model on an ongoing basis using data transmitted through multiple widely used mobile devices to accomplish this the risk assessment was split into components such that parameters that change over relatively long time scales were assessed offline using esri arcgis desktop and quantum gis qgis while those changing over relatively short time scales were analyzed on a web based gis powered environmental information management system eims putting the shorter time scale analyses on the web allowed information to be submitted by mobile devices in use by field researchers this introduced time dependence in the form of new information with which to rapidly update the model the general scheme of the analysis is depicted in fig 1 whose terms will be discussed in the following sub section 2 1 vulnerability assessment several commonly used groundwater vulnerability assessment methods were reviewed including drastic aller et al 1987 god foster 1987 and avi van stempoort et al 1993 since the application area for this study overlies a karstified aquifer the pi method which is designed for karst terrains was chosen the pi method considers two factors to determine vulnerability protective cover p and infiltration conditions i goldscheider 2005 the other vulnerability assessment methods considered do not provide tools for karst terrains or need to be modified for such environments ravbar and goldscheider 2009 the pi method specializes in karstified aquifers by introducing a factor to describe the degree to which protective cover is bypassed as a result of lateral surface and subsurface flows through karst conduits furthermore in a comparative study ravbar and goldscheider 2009 found that the pi method more closely matched multi tracer tests at least during low flow conditions compared with other vulnerability methods designed for karst aquifers ravbar and goldscheider 2009 since the study area is located in karst terrain in a semi arid climate with a distinct seasonal precipitation pattern the pi method was selected in order to best represent prevailing low flow conditions during the dry season the vulnerability assessment was carried out in arcgis by overlaying thematic layers of soil cover aquifer geology elevation slope vegetation and recharge following the pi methodology outlined in the european commission s cost action 620 final report zwahlen 2003 soil shapefiles were obtained from the palestinian authority ministry of agriculture geology maps were obtained from the geological survey of israel and vectorized in arcgis a digital elevation model dem of the study area was extracted from aster tiles and was low pass and majority filtered in arcgis the slope raster was calculated using the filtered dem vegetation was obtained from a land use shapefile provided by the ministry of agriculture recharge was estimated using the approach in the cost 620 final report s time input method zwahlen 2003 a rainfall map was obtained from the palestinian hydrology group and vectorized for the study area in arcgis runoff ratios were calculated using soil slope and vegetation layers and recharge was estimated using runoff ratios and average annual rainfall amounts 2 2 calculation of risk indexes once the pi map was completed it was uploaded as a raster to a web based environmental information management system for subsequent risk calculation the principal step in the risk assessment outlined here was the collection of pollution hazard information and its subsequent processing into a hazards database pollution hazards information was then combined with vulnerability values based on equations presented in the cost action 620 final report eqs 1 and 2 to obtain risk intensity indices r i i i zwahlen 2003 1 r i i i 1 h i i p i i 2 h i i h i q i r i a risk intensity index r i i is computed for each cell in the raster coverage area using eq 1 where p i i is the vulnerability value obtained from the pi raster and h i i is the hazard index computed using eq 2 the subscript i indicates that the formula is applied to each cell in the raster extent in eq 2 h is a weighting factor depending on the land use category of the site q is a relative weighting factor to account for the physical size of the site relative to others of the same type and r is a reduction factor between 0 and 1 that can be used to reduce the impact of a site if containment and spill prevention measures are in place as described in the cost action 620 final report the h factor is generally determined by the type of land use with sites having higher potential negative impacts on water quality e g mines and animal feedlots being assigned higher h values than sites with lower potential impacts e g arable lands and forests to obtain h factors hazards are classified based on information including the process or nature of their activities and the type of harmful substances that may be present if practical and feasible these classifications may be subdivided to reduce uncertainty by differentiating between hazards in more detail for this application we chose a relatively small number of land use types to consider as diffuse and point hazards to simplify field surveys for the researchers the hazard types we included were thought to be the most prominent readily identifiable and numerous the number of classified hazards would be expanded in a full scale application the classifications decided upon for h will determine the dropdown list available to users of the mobile application when conducting field surveys which will be discussed in section 2 4 in this study eqs 1 and 2 were modified to explicitly reflect their treatment as either short or long time scale variables following fig 1 short time scale variables are those that may change noticeably during the time scales of interest in this study weeks to months they were modified to make them explicitly transient long time scale variables which are variables that are not expected to change noticeably during the time scales of interest were treated as if they were static an example of the modified risk calculation methodology using four sample grid blocks is given in fig 2 a time index n superscript was added to the hazard index parameter in eq 1 resulting in eq 1 of fig 2 since the pi vulnerability index is based on hydrogeologic parameters that do not change significantly in the time scale of interest no time dependence was introduced to that term 2 3 calculation of hazard indexes the hazard index was divided into two parts diffuse hazards h i i d and p oint h azards h i i p n which were added together to obtain t otal h azards h i i n similarly to the procedure used by simpson et al 2014 the resulting total hazards calculation is shown in eq 2 of fig 2 diffuse hazard indices h i i d are based on land use shapefiles and were treated as being relatively constant over short to medium time frames as such no time index n was added to the diffuse hazard variable in all raster cells covered by the land use shapefile diffuse hazard was calculated using eq 3 of fig 2 where parameters were defined following eq 2 diffuse hazard shapefiles cover the entire study area so eq 3 of fig 2 could be directly applied in each cell point hazards however are discrete points and were interpolated over the study domain inverse distance weighted interpolation was applied following eq 4 of fig 2 in raster cells containing a point hazard the point hazard index was the sum of the hazard index contained in the cell and the interpolated value from any surrounding hazards in raster cells that did not contain a point hazard the calculated point hazard index was only the interpolated value in the resulting raster the effect of each point hazard decreased with increasing distance and increasing densities of point hazards resulted in higher point hazard indexes in the surrounding areas a search radius r was defined such that the impact of each point hazard was limited to an area surrounding the hazard location while subsurface flow directions are important to determining risk along potential pollutant pathways they were not considered in this study since we define risk as the likelihood that pollutants will reach the water table without considering where they migrate afterward instead interpolation was applied to distribute the risk radially from the pollution hazard covering an implied area of release similarly to simpson et al 2014 future applications should consider flow directions during the vulnerability assessment by increasing vulnerability along potential pollutant pathways in fig 2 the superscript n represents the time index since the above formulae are recalculated as new point hazards are added or existing hazards are changed or deleted from the dataset in reality parameters without the superscript n may also be time dependent however since this application considers a time scale of weeks to months and those parameters are unlikely to change appreciably in that time they were considered static during this study s time scales of interest the subscript i indicates that the calculation is carried out in each raster cell in the study area in eq 4 of fig 2 n is the number of unique point hazards within the search radius n is 0 if no point hazards lie within the search radius and the resulting point hazard index will be 0 in such cells in eq 5 of fig 2 p is the power parameter which can be any positive number increasing p corresponds to smaller impacts of more distant point hazards on each estimated cell value with sufficiently high p eq 4 of fig 2 becomes a voronoi interpolator in this study a p value of 10 and a search radius of 500 m were used the point hazard dataset was created and stored in an online data management system with each hazard entry including geographic coordinates and all parameters necessary for eq 4 of fig 2 a raster calculator built into the online system automatically computed eq 4 of fig 2 nightly using the data stored in the point hazard dataset at the time eqs 1 and 2 of fig 2 were then computed automatically using the updated point hazards raster resulting in a new overall risk raster since risk was recomputed nightly the theoretical time step in the model is one day however if no changes are recorded in the point hazards dataset on a given day the risk map would not change such that r i i i n r i i i n 1 fig 2 demonstrates risk intensity index calculation in four grid cells in the first time step no point hazard has been collected and the subsequent risk assessment is simply the combination of vulnerability and diffuse hazards layers note that higher vulnerability does not necessarily correlate with higher risk see the top left grid cell for example this situation may arise if a vulnerable area such as an infiltration zone is not exposed to high pollution hazard if it is in a natural forest for example conversely two areas exposed to the same level of hazard may have different risk values resulting from their vulnerabilities see for example the top right and bottom left cells in the second time step in fig 2 one point hazard has been identified blue circle and the subsequent risk calculation includes vulnerability diffuse hazards and point hazards resulting in increased risk intensity near the point hazard point hazards increase risk in areas where diffuse hazard may be low based on land use alone see the top right cell for example such a situation may occur for example if a naturally forested area becomes home to an isolated hazardous feature such as a garbage dump or fuel installation point hazards can also enhance risk where it is already high the bottom right cell for example this may occur for example in an urban area with high diffuse hazard but where isolated points such as fuel depots or chemical industries should increase the local risk above its background level the point hazard does not affect risk in the left hand cells because their grid centroids lie outside of the search radius and are therefore considered too far away to be affected 2 4 collection of point hazard information using mobile forms new point hazard entries were added to the dataset using a mobile data collection smartphone app the web based platform used for data acquisition management and analysis is described in rubin and michaelis 2017 a custom data collection form was created for this purpose allowing the user to input the necessary point hazard parameters and recording the gps location of the smartphone at the time of data collection fig 3 this allowed users to collect parameters for eq 2 while they were observing hazards in the field in fig 3 the hazard type dropdown allowed the user to select a land use type which was then used to assign a h parameter following the land use classification discussed in section 2 2 the condition field allowed the user to enter a r parameter between 0 and 1 depending on the strength of containment measures observed the q parameter was neglected in this application since its range is typically small 0 8 1 2 and it was found to be difficult to explain to survey participants a field to record the q parameter could be added to a custom form for future applications these hazard forms were synchronized with the rest of the online point hazard database using a web connection resulting in a centralized database with contributions from different users in addition to making data collection easier the use of a standard form limited data heterogeneity issues that result from syntactic differences among researchers ames et al 2012 in this study point hazard locations and parameters were recorded by five researchers using their personal mobile devices after data collection the resulting dataset could be edited by any users with appropriate permissions to maintain data integrity editing privileges were limited to one administrator while the other four researchers could only submit fig 4 the data processing workflow and storage structure in this study were designed to allow point hazard data to be collected by many users and integrated automatically into an updated risk assessment other variables in the risk assessment were considered static since the time scales over which these data change are much longer than those of the point hazards components of the analysis that were considered static were processed in arcgis and qgis while dynamic components were processed in the online environmental information management system eims following fig 1 the eims was set up to automatically exclude submitted data that violated quality assurance rules such as negative hazard indexes there is considerable flexibility in the eims to accommodate different parameter definitions risk calculations data sharing and other modelling schemes it is possible to automate model verification and calibration using water quality data stored in the online database although this was not done in this study 3 study area to evaluate the methodology presented here a case study was undertaken in the natuf surface water catchment in ramallah al bireh directorate of the west bank palestine fig 5 the catchment has an area of approximately 414 k m 2 its population is around 80 000 inhabitants palestinian central bureau 2007 it is situated over the western aquifer and is underlain by thick sequences of layered limestone dolomite chalk and marl shalash and ghanem 2008 the west bank has a predominant mediterranean climate and the natuf basin is classified as semi arid with rainfall ranging between about 400 and 900 mm per year meteorological service 2003 groundwater provides nearly 90 of all water supplies in the west bank and gaza strip palestinian water authority 2013 52 of this groundwater supply is used for domestic purposes drastically reducing the amount available for agricultural and industrial uses palestinian water authority 2013 in addition many springs emerge in the area from perched aquifers and outcropping limestone and dolomite formations with an average total discharge of 0 3 0 6 mcm per year shalash and ghanem 2008 the natuf basin is located mainly in the hills in a primary recharge area for the western aquifer due in part to extensive karstification and thin soil cover the aquifer receives direct rainfall recharge in the mountains and foothills un escwabgr 2013 as a result pollution introduced in the catchment poses a threat throughout the western aquifer untreated wastewater may pose an immediate threat to local water consumers since short residence times in karst systems mean that water moves through the system at time scales shorter than the lifetime of bacteria mahler et al 2000 the sensitivity of the natuf basin s water supply and the limited financial and human resources available for remediation make aquifer protection an important component of groundwater quality improvement in the study area 4 results application of the risk model to the natuf basin this section presents the results of the risk analysis demonstrates the addition of point hazards via field data collection and shows the resulting changes in the risk map over time it will show how the collection of additional point hazards changed the total hazards layer which in turn resulted in changes to the risk layer figs 8 and 9 respectively because of the relatively short data collection period our study focused on collecting new point hazards and demonstrating the increase in risk as new information is collected all rasters presented here have a grid cell size of 30 m and 446 801 active cells 4 1 vulnerability and risk in the study area the pi groundwater vulnerability map fig 6 indicates that the greater part of the study area is classified as high vulnerability due to limited soil maps and samples in the study area areas with no soil information were assumed to fit a larger pattern of thin soils without significant low permeability near surface layers m ghanem pers comm seasonal streams wadis and the surrounding areas are classified as very high vulnerability overall 94 1 of the area is high vulnerability 3 68 is very high vulnerability 2 16 is moderate vulnerability and 0 05 is low vulnerability the pi raster shown in fig 6 contains the p i i factor for each cell that will be used in the subsequent risk analysis the diffuse hazards layer h i i d was created from land use data obtained from the palestinian authority ministry of agriculture and computed following eq 3 of fig 2 diffuse hazards were based on a shapefile identifying agricultural areas arable land forests industrial and commercial areas open spaces mines dumps construction sites and residential areas for the initial analysis q uantity q and reduction r factors of 1 were used in the calculation of d iffuse hazard i ndexes hazard w eighting f actors h were assigned to each polygon in the land use shapefile according to table 1 the resulting raster of diffuse hazards h i d is presented in fig 7 the pi and diffuse hazards rasters were uploaded as geotiffs to a web based environmental information management system eims over a three month period point hazards were collected using the data collection form shown in fig 3 eq 1 and eq 4 of fig 2 were implemented in the eims using the pi and diffuse hazard rasters and the point hazard dataset and were configured to recalculate nightly initially the point hazard dataset was empty and eq 1 of fig 2 simplified to r i i i 1 h i i d p i i meaning that the risk intensity was governed by v ulnerability and d iffuse h azards only this initial calculation can be thought of as a base r isk assessment and is shown in fig 9a as point hazards were collected the risk assessment changed to reflect increased risk around each new hazard fig 9b d 4 2 collection of point hazards field surveys of point pollution hazards were conducted in the natuf basin over a three month period to demonstrate changes in the risk map as more point hazards were identified in the field snapshots of the total hazard h i i n and r isk r i i i n maps were taken at three points in the course of the field collection phase the first snapshot was on january 19th 2015 and contained 87 point hazard records the second was on may 5th 2015 and contained 135 point hazard records the third was on may 11th 2015 and contained 232 point hazard records the t otal h azard raster at the end of each time interval is shown in fig 8b d in fig 8 red dots represent point hazards while multi colored dots represent clusters of point hazards that are aggregated at the current zoom level in the eims point hazards can be clicked to see more information including parameters for eq 4 of fig 2 and a photo if one was taken see fig 10 for an example the r isk rasters calculated using each t otal h azard dataset are shown in fig 9 changes in the total hazard raster fig 8 clearly affect the risk rasters shown in fig 9 as risk increases near newly collected point hazards data these changes result from the daily recalculation of eq 1 and eq 4 of fig 2 by the eims built in raster calculator using the point hazard dataset collected by field researchers over the course of the three month period these changes to the point hazard dataset and subsequent recalculation of the risk raster form the basis for the time dependence introduced by this approach fig 11 shows percent changes in the risk intensity index between the first and second data collection rounds changes in risk occur near new point hazards identified during field surveys black squares and are limited to a 500 m radius around those points by the search radius imposed during interpolation of the point hazards garbage dumps were found more frequently than other hazard types and had a higher h factor which caused changes in risk to appear fairly constant around many of the new point hazard locations as such a model matured changes in risk would become more heterogeneous with risk increasing substantially around significant new hazards and less around less hazardous features risk would also decrease in some areas as hazards were mitigated or remediated the supervisor of such a model would need to evaluate when the model had reached sufficient maturity i e initial hazard collection surveys had achieved sufficient coverage in order to use changes in the risk calculation to support decision making 5 discussion the methodology and case study outlined above address the pollution hazard data collection challenges outlined in the introduction namely the prohibitive time and effort required to collect initial data and the difficulty of updating the dataset over time an online data collection system provides the advantage of being accessible to many different partners thereby distributing the data collection burden in this case study five different researchers using their personal mobile devices collected information about 232 point hazards in three day long surveys conducted over a three month period since the eims can be configured to allow more data collection partners more information can be collected in shorter time frames this includes the addition of new hazards removal of previously existing ones and changes in hazard status over time some of these changes e g a temporary chemical leak or seasonal pasturing locations may occur over daily to monthly time scales with appropriate quality control measures such a system could also allow community based data collection whereby hazards are reported by engaged citizens and analyzed remotely by water resource managers the case study demonstrates the methodology s ability to automatically incorporate changes in point hazards data and re compute the overall risk values based on the updated datasets the point hazards dataset is updated as soon as a new feature is identified and the user is connected to the web this feature along with the fact that risk is computed daily allows the methodology to capture changes on a daily time scale without any manual gis operations in the case study presented here new point hazard locations were added when researchers identified them in the field such that risk generally increased in the study area table 2 shows that the proportion of high and very high risk cells increased over the course of the field survey as previously unmarked hazards were identified by researchers the reverse could also happen such that risk decreases in areas where a point hazard is remediated or removed however such a scenario was not encountered in the case study presented here in addition to reducing data collection burdens there are several key advantages to implementing a dynamic risk modelling approach first it provides a centralized tool for water managers to evaluate groundwater protection and remediation plans on an ongoing basis second it reduces the probability of planning decisions being made using outdated information third it allows time dependent trends in groundwater risk evolution to be identified at an early stage more work can be done to elucidate and expand these benefits and uses for water managers in this study an inverse distance weighted interpolation was applied to the point hazard dataset prior to calculation of total hazards this approach was based on the physical assumptions that a greater density of hazards would lead to a higher average risk in the surrounding area and also that within the assumed area of release search radius hazard threat is greater for closer point sources than for more distant ones this contrasts with simpson et al 2014 who map the assumed area of release and apply a uniform extent rating over that area other possible approaches such as a simple buffer around each point hazard would not lead to a cumulative increase in risk in areas with high hazard density more exploration of different point hazard integration approaches and their physical assumptions would be interesting since risk was recomputed nightly the theoretical time step in the model is one day however if no changes are recorded in the point hazards dataset on a given day the risk map would not change such that r i i i n r i i i n 1 as a consequence the risk map only becomes dynamic with consistent participation from data collection partners more work can be done to address possible implementation strategies that invoke robust community participation while also maintaining the integrity of the dataset this should include controls to detect duplicate submissions and to ensure that changes to hazards are applied to the correct sites ideally the time interval of new point hazard data collection would be informed by travel times in the domain for example if the travel time from a pollution source to the water supply is 5 days collecting data in a 2 day increment would cause the system to indicate increased risk before a spill had reached the supply such a short interval is prohibitively expensive and time consuming if individual field workers are tasked with data collection using a system that allows for input from many data collection partners makes such an approach more feasible because the risk model is updated automatically when new hazard information is submitted quality assurance is a key consideration of this approach some quality control measures can be integrated into the database itself for example automatic flagging of negative or otherwise unreasonable values other measures also need to be taken by professionals and experts overseeing the database storing water quality measurements in the same online database would allow the risk model to be evaluated on an ongoing basis new risk estimates could be compared with water quality to provide ongoing model verification and calibration future work is required to further explore this topic 6 conclusion published studies have identified limitations in current groundwater risk mapping methodologies particularly related to the hazard inventory for one hazard inventories are generally time consuming and labor intensive to collect while detailed onsite investigations throughout the study area are ideal for hazard data collection such investigations are extremely time consuming and are therefore rarely done simpson et al 2014 instead hazard inventories are often created from existing land use data and may represent general rather than specific assessments the second main limitation is that hazard inventories are produced during the data collection phase of the risk assessment and only reflect conditions as they were at a specific point in time however changes in land use may introduce new hazards remove old hazards or change the nature of certain hazards zwahlen 2003 without ongoing hazard data collection these changes are not reflected in the risk map being used by planners risk values may no longer reflect the current land use distribution and decisions based on this information will be affected this study addresses the two main limitations of hazard inventories outlined above a methodology for collecting and evaluating hazard information from multiple researchers in the field and for introducing temporal variation into a risk assessment algorithm was presented in this approach information about pollution hazards was stored in an online database available to multiple users new information about hazards was collected and gps tagged in the field using a smartphone and submitted to the database distributing the hazard collection workload new additions changes or deletions in the database were reflected in the risk model which was recalculated daily this methodology was applied in a case study in the natuf basin in ramallah palestine 7 software availability as a web application there is no software specifically to download and install hardware and software requirements are any desktop or laptop computer with internet access and a modern web browser we recommend google chrome for the best experience any user can sign up for free at www my observatory com the service is free for nonprofit use and for commercial use a thirty day free trial is automatically applied for all new users acknowledgments this work was supported by the al falah fund for science and technology the authors gratefully acknowledge webh2o llc for permission to use their environmental information management system myobservatory free of charge the authors would also like to thank hassan jebreen hana hidmi and dalal thaher for help with field work and translation appendix a smartphone gps accuracy assessment this approach relies to some degree on the reliability of each user s smartphone gps chip the precision of each gps reading made by a smartphone in the field introduces uncertainty in the risk model equivalent to the gps measurement error at the time of data collection the gps precision of one of the researchers htc one remix smartphone on glonass and without a gps was assessed by comparing field readings with those of a garmin etrex vista hcx handheld gps system and by comparing with gps coordinates of identifiable landmarks on google earth readings were taken using the smartphone and handheld gps in a variety of conditions including sunny and cloudy weather under tree cover and in clear sky conditions and in urbanized locations at each location the device was placed on a solid surface and kept stationary for one minute before readings were taken on the smartphone gps locations were recorded in a data collection form similar to fig 3 on the garmin handheld gps the locations were recorded as waypoints in the device s internal computer and added to the eims for analysis for comparison with google s map server readily identifiable landmarks were selected from a desktop computer on google earth then visited in the field where gps readings were taken by both devices a total of 40 measurement pairs were taken in the field using both an htc one remix smartphone and a garmin etrex vista hcx handheld gps of these 14 pairs were in clear sky conditions no tree canopy or buildings within 50 feet 13 were in canopy conditions in the shade of at least one tree and 13 were in urban areas surrounded by at least two buildings greater than two floors for each measurement pair the smartphone and handheld gps were placed side by side on a solid object and kept still for one minute prior to reading results showed that urban conditions resulted in the greatest average disparity between the two gps devices nearly 10 m while clear sky and canopy conditions resulted in discrepancies of around 4 6 m figure a 12 discrepancy between htc one remix smartphone and garmin etrex vista hcx handheld gps in sunny and cloudy conditions and in clear sky canopy and urban conditions n 40 figure a 12 three locations landmarks within ramallah city limits were chosen on google earth s satellite imagery using a desktop computer in order to compare the two devices described above with the web based map server a specific identifiable point on each landmark was chosen from the satellite image this point was visited in the field and both the htc smartphone and the garmin handheld gps were placed at this point to collect gps readings all three landmarks were visited on sunny days and each represented a different cover condition clear sky canopy or urban after each landmark was identified and visited the two gps measurements were compared with the desktop identified point on google earth in order to compute discrepancies an example is shown in figure a 13 table a 3 linear distance discrepancy from each gps device s measurement reading to the point identified on google earth landmark table a 3 landmark id type garmin discrepancy m htc discrepancy m l1 clear sky 6 8 1 l2 canopy 2 11 5 l3 urban 3 11 figure a 13 landmark 1 showing the original desktop identified point and the two gps readings from the htc one remix and the garmin etrex vista hcx handheld gps figure a 13 a brief analysis of the accuracy of the gps antenna housed in one smartphone model htc one remix demonstrates that there is no significant effect of cover or weather in rural areas clear sky or canopy conditions when compared with a commercial handheld gps device taking into account weather and cover the mean discrepancies between the htc one remix and garmin devices are only statistically significant at a 95 confidence interval in cloudy weather between urban and clear sky cover scenarios in this case the maximum difference between the htc and garmin devices is around 16 m comparing each gps unit to an identifiable location on google earth yields a similar figure adding this accuracy to garmin s reported accuracy of 10 m without waas results in a potential tolerance radius of around 26 m these results indicate that a smartphone can provide reasonable location accuracy when recording hazard information in a variety of weather and cover conditions in rural areas and when raster cell sizes are intended to be larger than about 50 m smartphone accuracy is comparable to a handheld device 
26463,we present a web based software platform for assimilation of field data on groundwater vulnerability and assessment of groundwater pollution risk groundwater vulnerability and risk assessments couple hydrogeologic characterization and pollution source information to provide a relative measure of risk to a groundwater supply vulnerability is based solely on hydrogeologic factors while risk also considers pollution hazards previous studies have identified two key limitations in the collection of pollution hazards information detailed investigations throughout a study area are extremely time consuming and pollution inventories are not regularly updated to reflect changes in land use this paper presents a methodology to address these two limitations by allowing broad based effort to collect pollution hazard parameters in the field using gps tagged smartphone forms and incorporating the new information rapidly into a web based risk modelling framework to demonstrate this approach a case study is presented from the natuf basin in the ramallah al bireh governorate of palestine keywords distributed data collection web based gis risk assessment groundwater pollution 1 introduction this paper focuses on the development and application of a networked software platform supporting efficient and flexible groundwater pollution risk modelling this section will introduce key principles of groundwater vulnerability and risk assessments and motivate the use of new software tools to improve these models intrinsic groundwater vulnerability can be defined as an intrinsic property of a groundwater system that depends on the sensitivity of that system to human and or natural impacts vrba and zaporožec 1994 in this study vulnerability was considered to relate to features of local hydrology soil and geologic layers that may mitigate contaminants introduced by human activities vulnerability maps portray the effectiveness of natural filtration processes in certain areas relative to others within a geographic study domain such maps are commonly used by land use planners to evaluate the potential relative impact of human development on groundwater quality in different areas vrba and zaporožec 1994 vulnerability is solely a function of hydrogeologic factors characterizing the overlying soil and geological materials and does not consider human activities doerfliger et al 1999 vulnerability assessments can be expanded to obtain overall groundwater pollution risk measures which consider both natural and anthropogenic factors in general risk is defined as the likelihood of certain events occurring and the magnitude of their possible consequences simpson et al 2014 this study considers a rather narrow definition of risk namely that human activities at or near the land surface will introduce contaminants to the groundwater supply which may be harmful to human health this definition was adopted for simplicity as our primary goal is to demonstrate the concepts and ideas behind the risk platform the risk maps we consider portray a relative measure of the risk of aquifer contamination resulting from human activities in the study area these maps are obtained by combining intrinsic vulnerability with an inventory of potential groundwater pollution hazards hazards are generally assessed using a combination of information about the type of potential contaminant release the extent of a potential release and the likelihood of such a release given containment measures in place at the site published groundwater pollution risk studies zwahlen 2003 simpson et al 2014 have identified limitations in current methodologies particularly related to creating and updating the hazard inventory for one hazard inventories are generally time consuming and labor intensive to collect detailed on site investigations throughout the study area which are ideal for hazard data collection are extremely time consuming and are therefore rarely done simpson et al 2014 instead hazard inventories are often created from existing land use data and may represent general rather than specific assessments in such assessments basic land use data for each type such as activity type size and age are used to estimate a generalized hazard value in some cases more specific information about certain point hazards is added to land use data simpson et al 2014 the second main limitation in many hazard analyses is that hazard inventories are produced during the data collection phase of the risk assessment and reflect conditions only as they were at that point in time however changes in land use may introduce new hazards remove old hazards or change the nature of certain hazards zwahlen 2003 some of these changes can occur on a daily or weekly time scale when subsurface residence times are short as in karst systems new hazards can quickly impact the water supply and should result in rapid changes in the risk assessment zwahlen 2003 however without ongoing hazard data collection these changes are not reflected in the risk map being used by planners risk values may no longer reflect the current land use distribution and decisions based on this information may be affected this study addresses the two main limitations of hazard inventories outlined above a methodology for distributing the data collection burden collecting and evaluating hazard information in situ and introducing temporal variation into a risk assessment algorithm is developed in this approach information about pollution hazards is stored in an online database which is available selectively to multiple users up to date information about hazards can be collected by multiple users gps tagged in the field using a smartphone and submitted to the database where it can be analyzed and shared in real time rubin and michaelis 2017 new additions changes or deletions in the database are reflected in the risk model which is recalculated daily or as otherwise directed previous work for example by granell et al 2010 has introduced decentralized geospatial processing tools in order to reduce tedious or repetitive tasks being carried out by researchers during the modelling process this approach extends that work to also include decentralized collection and integration of model data from the field it applies decentralized data collection principles such as those discussed in horsburgh et al 2011 directly to an environmental modelling problem resulting in a framework that allows for decentralized collection and integration of new data into the model and also automates the task of updating the model over time by collecting storing and geoprocessing model data all within the same system this approach avoids interoperability issues and challenges integrating data and modelling services peckham and goodall 2013 castronova et al 2013 this methodology is applied in a case study and the results are discussed we show that the risk model changes significantly as new hazard information is added by researchers in the field and that the proposed software platform supports rapid updating through community participation in a developed model this new information could represent previously non existent hazards such as new garbage dump sites or chemical spills or the removal or modification of previously existing ones 2 methods a simplified risk modelling problem definition has been adopted here in order to focus our presentation on the networked software platform s ability to overcome key risk modelling limitations a similar approach can be taken with more complex risk modelling methodologies and in different settings the procedure was designed to allow flexibility in updating the model on an ongoing basis using data transmitted through multiple widely used mobile devices to accomplish this the risk assessment was split into components such that parameters that change over relatively long time scales were assessed offline using esri arcgis desktop and quantum gis qgis while those changing over relatively short time scales were analyzed on a web based gis powered environmental information management system eims putting the shorter time scale analyses on the web allowed information to be submitted by mobile devices in use by field researchers this introduced time dependence in the form of new information with which to rapidly update the model the general scheme of the analysis is depicted in fig 1 whose terms will be discussed in the following sub section 2 1 vulnerability assessment several commonly used groundwater vulnerability assessment methods were reviewed including drastic aller et al 1987 god foster 1987 and avi van stempoort et al 1993 since the application area for this study overlies a karstified aquifer the pi method which is designed for karst terrains was chosen the pi method considers two factors to determine vulnerability protective cover p and infiltration conditions i goldscheider 2005 the other vulnerability assessment methods considered do not provide tools for karst terrains or need to be modified for such environments ravbar and goldscheider 2009 the pi method specializes in karstified aquifers by introducing a factor to describe the degree to which protective cover is bypassed as a result of lateral surface and subsurface flows through karst conduits furthermore in a comparative study ravbar and goldscheider 2009 found that the pi method more closely matched multi tracer tests at least during low flow conditions compared with other vulnerability methods designed for karst aquifers ravbar and goldscheider 2009 since the study area is located in karst terrain in a semi arid climate with a distinct seasonal precipitation pattern the pi method was selected in order to best represent prevailing low flow conditions during the dry season the vulnerability assessment was carried out in arcgis by overlaying thematic layers of soil cover aquifer geology elevation slope vegetation and recharge following the pi methodology outlined in the european commission s cost action 620 final report zwahlen 2003 soil shapefiles were obtained from the palestinian authority ministry of agriculture geology maps were obtained from the geological survey of israel and vectorized in arcgis a digital elevation model dem of the study area was extracted from aster tiles and was low pass and majority filtered in arcgis the slope raster was calculated using the filtered dem vegetation was obtained from a land use shapefile provided by the ministry of agriculture recharge was estimated using the approach in the cost 620 final report s time input method zwahlen 2003 a rainfall map was obtained from the palestinian hydrology group and vectorized for the study area in arcgis runoff ratios were calculated using soil slope and vegetation layers and recharge was estimated using runoff ratios and average annual rainfall amounts 2 2 calculation of risk indexes once the pi map was completed it was uploaded as a raster to a web based environmental information management system for subsequent risk calculation the principal step in the risk assessment outlined here was the collection of pollution hazard information and its subsequent processing into a hazards database pollution hazards information was then combined with vulnerability values based on equations presented in the cost action 620 final report eqs 1 and 2 to obtain risk intensity indices r i i i zwahlen 2003 1 r i i i 1 h i i p i i 2 h i i h i q i r i a risk intensity index r i i is computed for each cell in the raster coverage area using eq 1 where p i i is the vulnerability value obtained from the pi raster and h i i is the hazard index computed using eq 2 the subscript i indicates that the formula is applied to each cell in the raster extent in eq 2 h is a weighting factor depending on the land use category of the site q is a relative weighting factor to account for the physical size of the site relative to others of the same type and r is a reduction factor between 0 and 1 that can be used to reduce the impact of a site if containment and spill prevention measures are in place as described in the cost action 620 final report the h factor is generally determined by the type of land use with sites having higher potential negative impacts on water quality e g mines and animal feedlots being assigned higher h values than sites with lower potential impacts e g arable lands and forests to obtain h factors hazards are classified based on information including the process or nature of their activities and the type of harmful substances that may be present if practical and feasible these classifications may be subdivided to reduce uncertainty by differentiating between hazards in more detail for this application we chose a relatively small number of land use types to consider as diffuse and point hazards to simplify field surveys for the researchers the hazard types we included were thought to be the most prominent readily identifiable and numerous the number of classified hazards would be expanded in a full scale application the classifications decided upon for h will determine the dropdown list available to users of the mobile application when conducting field surveys which will be discussed in section 2 4 in this study eqs 1 and 2 were modified to explicitly reflect their treatment as either short or long time scale variables following fig 1 short time scale variables are those that may change noticeably during the time scales of interest in this study weeks to months they were modified to make them explicitly transient long time scale variables which are variables that are not expected to change noticeably during the time scales of interest were treated as if they were static an example of the modified risk calculation methodology using four sample grid blocks is given in fig 2 a time index n superscript was added to the hazard index parameter in eq 1 resulting in eq 1 of fig 2 since the pi vulnerability index is based on hydrogeologic parameters that do not change significantly in the time scale of interest no time dependence was introduced to that term 2 3 calculation of hazard indexes the hazard index was divided into two parts diffuse hazards h i i d and p oint h azards h i i p n which were added together to obtain t otal h azards h i i n similarly to the procedure used by simpson et al 2014 the resulting total hazards calculation is shown in eq 2 of fig 2 diffuse hazard indices h i i d are based on land use shapefiles and were treated as being relatively constant over short to medium time frames as such no time index n was added to the diffuse hazard variable in all raster cells covered by the land use shapefile diffuse hazard was calculated using eq 3 of fig 2 where parameters were defined following eq 2 diffuse hazard shapefiles cover the entire study area so eq 3 of fig 2 could be directly applied in each cell point hazards however are discrete points and were interpolated over the study domain inverse distance weighted interpolation was applied following eq 4 of fig 2 in raster cells containing a point hazard the point hazard index was the sum of the hazard index contained in the cell and the interpolated value from any surrounding hazards in raster cells that did not contain a point hazard the calculated point hazard index was only the interpolated value in the resulting raster the effect of each point hazard decreased with increasing distance and increasing densities of point hazards resulted in higher point hazard indexes in the surrounding areas a search radius r was defined such that the impact of each point hazard was limited to an area surrounding the hazard location while subsurface flow directions are important to determining risk along potential pollutant pathways they were not considered in this study since we define risk as the likelihood that pollutants will reach the water table without considering where they migrate afterward instead interpolation was applied to distribute the risk radially from the pollution hazard covering an implied area of release similarly to simpson et al 2014 future applications should consider flow directions during the vulnerability assessment by increasing vulnerability along potential pollutant pathways in fig 2 the superscript n represents the time index since the above formulae are recalculated as new point hazards are added or existing hazards are changed or deleted from the dataset in reality parameters without the superscript n may also be time dependent however since this application considers a time scale of weeks to months and those parameters are unlikely to change appreciably in that time they were considered static during this study s time scales of interest the subscript i indicates that the calculation is carried out in each raster cell in the study area in eq 4 of fig 2 n is the number of unique point hazards within the search radius n is 0 if no point hazards lie within the search radius and the resulting point hazard index will be 0 in such cells in eq 5 of fig 2 p is the power parameter which can be any positive number increasing p corresponds to smaller impacts of more distant point hazards on each estimated cell value with sufficiently high p eq 4 of fig 2 becomes a voronoi interpolator in this study a p value of 10 and a search radius of 500 m were used the point hazard dataset was created and stored in an online data management system with each hazard entry including geographic coordinates and all parameters necessary for eq 4 of fig 2 a raster calculator built into the online system automatically computed eq 4 of fig 2 nightly using the data stored in the point hazard dataset at the time eqs 1 and 2 of fig 2 were then computed automatically using the updated point hazards raster resulting in a new overall risk raster since risk was recomputed nightly the theoretical time step in the model is one day however if no changes are recorded in the point hazards dataset on a given day the risk map would not change such that r i i i n r i i i n 1 fig 2 demonstrates risk intensity index calculation in four grid cells in the first time step no point hazard has been collected and the subsequent risk assessment is simply the combination of vulnerability and diffuse hazards layers note that higher vulnerability does not necessarily correlate with higher risk see the top left grid cell for example this situation may arise if a vulnerable area such as an infiltration zone is not exposed to high pollution hazard if it is in a natural forest for example conversely two areas exposed to the same level of hazard may have different risk values resulting from their vulnerabilities see for example the top right and bottom left cells in the second time step in fig 2 one point hazard has been identified blue circle and the subsequent risk calculation includes vulnerability diffuse hazards and point hazards resulting in increased risk intensity near the point hazard point hazards increase risk in areas where diffuse hazard may be low based on land use alone see the top right cell for example such a situation may occur for example if a naturally forested area becomes home to an isolated hazardous feature such as a garbage dump or fuel installation point hazards can also enhance risk where it is already high the bottom right cell for example this may occur for example in an urban area with high diffuse hazard but where isolated points such as fuel depots or chemical industries should increase the local risk above its background level the point hazard does not affect risk in the left hand cells because their grid centroids lie outside of the search radius and are therefore considered too far away to be affected 2 4 collection of point hazard information using mobile forms new point hazard entries were added to the dataset using a mobile data collection smartphone app the web based platform used for data acquisition management and analysis is described in rubin and michaelis 2017 a custom data collection form was created for this purpose allowing the user to input the necessary point hazard parameters and recording the gps location of the smartphone at the time of data collection fig 3 this allowed users to collect parameters for eq 2 while they were observing hazards in the field in fig 3 the hazard type dropdown allowed the user to select a land use type which was then used to assign a h parameter following the land use classification discussed in section 2 2 the condition field allowed the user to enter a r parameter between 0 and 1 depending on the strength of containment measures observed the q parameter was neglected in this application since its range is typically small 0 8 1 2 and it was found to be difficult to explain to survey participants a field to record the q parameter could be added to a custom form for future applications these hazard forms were synchronized with the rest of the online point hazard database using a web connection resulting in a centralized database with contributions from different users in addition to making data collection easier the use of a standard form limited data heterogeneity issues that result from syntactic differences among researchers ames et al 2012 in this study point hazard locations and parameters were recorded by five researchers using their personal mobile devices after data collection the resulting dataset could be edited by any users with appropriate permissions to maintain data integrity editing privileges were limited to one administrator while the other four researchers could only submit fig 4 the data processing workflow and storage structure in this study were designed to allow point hazard data to be collected by many users and integrated automatically into an updated risk assessment other variables in the risk assessment were considered static since the time scales over which these data change are much longer than those of the point hazards components of the analysis that were considered static were processed in arcgis and qgis while dynamic components were processed in the online environmental information management system eims following fig 1 the eims was set up to automatically exclude submitted data that violated quality assurance rules such as negative hazard indexes there is considerable flexibility in the eims to accommodate different parameter definitions risk calculations data sharing and other modelling schemes it is possible to automate model verification and calibration using water quality data stored in the online database although this was not done in this study 3 study area to evaluate the methodology presented here a case study was undertaken in the natuf surface water catchment in ramallah al bireh directorate of the west bank palestine fig 5 the catchment has an area of approximately 414 k m 2 its population is around 80 000 inhabitants palestinian central bureau 2007 it is situated over the western aquifer and is underlain by thick sequences of layered limestone dolomite chalk and marl shalash and ghanem 2008 the west bank has a predominant mediterranean climate and the natuf basin is classified as semi arid with rainfall ranging between about 400 and 900 mm per year meteorological service 2003 groundwater provides nearly 90 of all water supplies in the west bank and gaza strip palestinian water authority 2013 52 of this groundwater supply is used for domestic purposes drastically reducing the amount available for agricultural and industrial uses palestinian water authority 2013 in addition many springs emerge in the area from perched aquifers and outcropping limestone and dolomite formations with an average total discharge of 0 3 0 6 mcm per year shalash and ghanem 2008 the natuf basin is located mainly in the hills in a primary recharge area for the western aquifer due in part to extensive karstification and thin soil cover the aquifer receives direct rainfall recharge in the mountains and foothills un escwabgr 2013 as a result pollution introduced in the catchment poses a threat throughout the western aquifer untreated wastewater may pose an immediate threat to local water consumers since short residence times in karst systems mean that water moves through the system at time scales shorter than the lifetime of bacteria mahler et al 2000 the sensitivity of the natuf basin s water supply and the limited financial and human resources available for remediation make aquifer protection an important component of groundwater quality improvement in the study area 4 results application of the risk model to the natuf basin this section presents the results of the risk analysis demonstrates the addition of point hazards via field data collection and shows the resulting changes in the risk map over time it will show how the collection of additional point hazards changed the total hazards layer which in turn resulted in changes to the risk layer figs 8 and 9 respectively because of the relatively short data collection period our study focused on collecting new point hazards and demonstrating the increase in risk as new information is collected all rasters presented here have a grid cell size of 30 m and 446 801 active cells 4 1 vulnerability and risk in the study area the pi groundwater vulnerability map fig 6 indicates that the greater part of the study area is classified as high vulnerability due to limited soil maps and samples in the study area areas with no soil information were assumed to fit a larger pattern of thin soils without significant low permeability near surface layers m ghanem pers comm seasonal streams wadis and the surrounding areas are classified as very high vulnerability overall 94 1 of the area is high vulnerability 3 68 is very high vulnerability 2 16 is moderate vulnerability and 0 05 is low vulnerability the pi raster shown in fig 6 contains the p i i factor for each cell that will be used in the subsequent risk analysis the diffuse hazards layer h i i d was created from land use data obtained from the palestinian authority ministry of agriculture and computed following eq 3 of fig 2 diffuse hazards were based on a shapefile identifying agricultural areas arable land forests industrial and commercial areas open spaces mines dumps construction sites and residential areas for the initial analysis q uantity q and reduction r factors of 1 were used in the calculation of d iffuse hazard i ndexes hazard w eighting f actors h were assigned to each polygon in the land use shapefile according to table 1 the resulting raster of diffuse hazards h i d is presented in fig 7 the pi and diffuse hazards rasters were uploaded as geotiffs to a web based environmental information management system eims over a three month period point hazards were collected using the data collection form shown in fig 3 eq 1 and eq 4 of fig 2 were implemented in the eims using the pi and diffuse hazard rasters and the point hazard dataset and were configured to recalculate nightly initially the point hazard dataset was empty and eq 1 of fig 2 simplified to r i i i 1 h i i d p i i meaning that the risk intensity was governed by v ulnerability and d iffuse h azards only this initial calculation can be thought of as a base r isk assessment and is shown in fig 9a as point hazards were collected the risk assessment changed to reflect increased risk around each new hazard fig 9b d 4 2 collection of point hazards field surveys of point pollution hazards were conducted in the natuf basin over a three month period to demonstrate changes in the risk map as more point hazards were identified in the field snapshots of the total hazard h i i n and r isk r i i i n maps were taken at three points in the course of the field collection phase the first snapshot was on january 19th 2015 and contained 87 point hazard records the second was on may 5th 2015 and contained 135 point hazard records the third was on may 11th 2015 and contained 232 point hazard records the t otal h azard raster at the end of each time interval is shown in fig 8b d in fig 8 red dots represent point hazards while multi colored dots represent clusters of point hazards that are aggregated at the current zoom level in the eims point hazards can be clicked to see more information including parameters for eq 4 of fig 2 and a photo if one was taken see fig 10 for an example the r isk rasters calculated using each t otal h azard dataset are shown in fig 9 changes in the total hazard raster fig 8 clearly affect the risk rasters shown in fig 9 as risk increases near newly collected point hazards data these changes result from the daily recalculation of eq 1 and eq 4 of fig 2 by the eims built in raster calculator using the point hazard dataset collected by field researchers over the course of the three month period these changes to the point hazard dataset and subsequent recalculation of the risk raster form the basis for the time dependence introduced by this approach fig 11 shows percent changes in the risk intensity index between the first and second data collection rounds changes in risk occur near new point hazards identified during field surveys black squares and are limited to a 500 m radius around those points by the search radius imposed during interpolation of the point hazards garbage dumps were found more frequently than other hazard types and had a higher h factor which caused changes in risk to appear fairly constant around many of the new point hazard locations as such a model matured changes in risk would become more heterogeneous with risk increasing substantially around significant new hazards and less around less hazardous features risk would also decrease in some areas as hazards were mitigated or remediated the supervisor of such a model would need to evaluate when the model had reached sufficient maturity i e initial hazard collection surveys had achieved sufficient coverage in order to use changes in the risk calculation to support decision making 5 discussion the methodology and case study outlined above address the pollution hazard data collection challenges outlined in the introduction namely the prohibitive time and effort required to collect initial data and the difficulty of updating the dataset over time an online data collection system provides the advantage of being accessible to many different partners thereby distributing the data collection burden in this case study five different researchers using their personal mobile devices collected information about 232 point hazards in three day long surveys conducted over a three month period since the eims can be configured to allow more data collection partners more information can be collected in shorter time frames this includes the addition of new hazards removal of previously existing ones and changes in hazard status over time some of these changes e g a temporary chemical leak or seasonal pasturing locations may occur over daily to monthly time scales with appropriate quality control measures such a system could also allow community based data collection whereby hazards are reported by engaged citizens and analyzed remotely by water resource managers the case study demonstrates the methodology s ability to automatically incorporate changes in point hazards data and re compute the overall risk values based on the updated datasets the point hazards dataset is updated as soon as a new feature is identified and the user is connected to the web this feature along with the fact that risk is computed daily allows the methodology to capture changes on a daily time scale without any manual gis operations in the case study presented here new point hazard locations were added when researchers identified them in the field such that risk generally increased in the study area table 2 shows that the proportion of high and very high risk cells increased over the course of the field survey as previously unmarked hazards were identified by researchers the reverse could also happen such that risk decreases in areas where a point hazard is remediated or removed however such a scenario was not encountered in the case study presented here in addition to reducing data collection burdens there are several key advantages to implementing a dynamic risk modelling approach first it provides a centralized tool for water managers to evaluate groundwater protection and remediation plans on an ongoing basis second it reduces the probability of planning decisions being made using outdated information third it allows time dependent trends in groundwater risk evolution to be identified at an early stage more work can be done to elucidate and expand these benefits and uses for water managers in this study an inverse distance weighted interpolation was applied to the point hazard dataset prior to calculation of total hazards this approach was based on the physical assumptions that a greater density of hazards would lead to a higher average risk in the surrounding area and also that within the assumed area of release search radius hazard threat is greater for closer point sources than for more distant ones this contrasts with simpson et al 2014 who map the assumed area of release and apply a uniform extent rating over that area other possible approaches such as a simple buffer around each point hazard would not lead to a cumulative increase in risk in areas with high hazard density more exploration of different point hazard integration approaches and their physical assumptions would be interesting since risk was recomputed nightly the theoretical time step in the model is one day however if no changes are recorded in the point hazards dataset on a given day the risk map would not change such that r i i i n r i i i n 1 as a consequence the risk map only becomes dynamic with consistent participation from data collection partners more work can be done to address possible implementation strategies that invoke robust community participation while also maintaining the integrity of the dataset this should include controls to detect duplicate submissions and to ensure that changes to hazards are applied to the correct sites ideally the time interval of new point hazard data collection would be informed by travel times in the domain for example if the travel time from a pollution source to the water supply is 5 days collecting data in a 2 day increment would cause the system to indicate increased risk before a spill had reached the supply such a short interval is prohibitively expensive and time consuming if individual field workers are tasked with data collection using a system that allows for input from many data collection partners makes such an approach more feasible because the risk model is updated automatically when new hazard information is submitted quality assurance is a key consideration of this approach some quality control measures can be integrated into the database itself for example automatic flagging of negative or otherwise unreasonable values other measures also need to be taken by professionals and experts overseeing the database storing water quality measurements in the same online database would allow the risk model to be evaluated on an ongoing basis new risk estimates could be compared with water quality to provide ongoing model verification and calibration future work is required to further explore this topic 6 conclusion published studies have identified limitations in current groundwater risk mapping methodologies particularly related to the hazard inventory for one hazard inventories are generally time consuming and labor intensive to collect while detailed onsite investigations throughout the study area are ideal for hazard data collection such investigations are extremely time consuming and are therefore rarely done simpson et al 2014 instead hazard inventories are often created from existing land use data and may represent general rather than specific assessments the second main limitation is that hazard inventories are produced during the data collection phase of the risk assessment and only reflect conditions as they were at a specific point in time however changes in land use may introduce new hazards remove old hazards or change the nature of certain hazards zwahlen 2003 without ongoing hazard data collection these changes are not reflected in the risk map being used by planners risk values may no longer reflect the current land use distribution and decisions based on this information will be affected this study addresses the two main limitations of hazard inventories outlined above a methodology for collecting and evaluating hazard information from multiple researchers in the field and for introducing temporal variation into a risk assessment algorithm was presented in this approach information about pollution hazards was stored in an online database available to multiple users new information about hazards was collected and gps tagged in the field using a smartphone and submitted to the database distributing the hazard collection workload new additions changes or deletions in the database were reflected in the risk model which was recalculated daily this methodology was applied in a case study in the natuf basin in ramallah palestine 7 software availability as a web application there is no software specifically to download and install hardware and software requirements are any desktop or laptop computer with internet access and a modern web browser we recommend google chrome for the best experience any user can sign up for free at www my observatory com the service is free for nonprofit use and for commercial use a thirty day free trial is automatically applied for all new users acknowledgments this work was supported by the al falah fund for science and technology the authors gratefully acknowledge webh2o llc for permission to use their environmental information management system myobservatory free of charge the authors would also like to thank hassan jebreen hana hidmi and dalal thaher for help with field work and translation appendix a smartphone gps accuracy assessment this approach relies to some degree on the reliability of each user s smartphone gps chip the precision of each gps reading made by a smartphone in the field introduces uncertainty in the risk model equivalent to the gps measurement error at the time of data collection the gps precision of one of the researchers htc one remix smartphone on glonass and without a gps was assessed by comparing field readings with those of a garmin etrex vista hcx handheld gps system and by comparing with gps coordinates of identifiable landmarks on google earth readings were taken using the smartphone and handheld gps in a variety of conditions including sunny and cloudy weather under tree cover and in clear sky conditions and in urbanized locations at each location the device was placed on a solid surface and kept stationary for one minute before readings were taken on the smartphone gps locations were recorded in a data collection form similar to fig 3 on the garmin handheld gps the locations were recorded as waypoints in the device s internal computer and added to the eims for analysis for comparison with google s map server readily identifiable landmarks were selected from a desktop computer on google earth then visited in the field where gps readings were taken by both devices a total of 40 measurement pairs were taken in the field using both an htc one remix smartphone and a garmin etrex vista hcx handheld gps of these 14 pairs were in clear sky conditions no tree canopy or buildings within 50 feet 13 were in canopy conditions in the shade of at least one tree and 13 were in urban areas surrounded by at least two buildings greater than two floors for each measurement pair the smartphone and handheld gps were placed side by side on a solid object and kept still for one minute prior to reading results showed that urban conditions resulted in the greatest average disparity between the two gps devices nearly 10 m while clear sky and canopy conditions resulted in discrepancies of around 4 6 m figure a 12 discrepancy between htc one remix smartphone and garmin etrex vista hcx handheld gps in sunny and cloudy conditions and in clear sky canopy and urban conditions n 40 figure a 12 three locations landmarks within ramallah city limits were chosen on google earth s satellite imagery using a desktop computer in order to compare the two devices described above with the web based map server a specific identifiable point on each landmark was chosen from the satellite image this point was visited in the field and both the htc smartphone and the garmin handheld gps were placed at this point to collect gps readings all three landmarks were visited on sunny days and each represented a different cover condition clear sky canopy or urban after each landmark was identified and visited the two gps measurements were compared with the desktop identified point on google earth in order to compute discrepancies an example is shown in figure a 13 table a 3 linear distance discrepancy from each gps device s measurement reading to the point identified on google earth landmark table a 3 landmark id type garmin discrepancy m htc discrepancy m l1 clear sky 6 8 1 l2 canopy 2 11 5 l3 urban 3 11 figure a 13 landmark 1 showing the original desktop identified point and the two gps readings from the htc one remix and the garmin etrex vista hcx handheld gps figure a 13 a brief analysis of the accuracy of the gps antenna housed in one smartphone model htc one remix demonstrates that there is no significant effect of cover or weather in rural areas clear sky or canopy conditions when compared with a commercial handheld gps device taking into account weather and cover the mean discrepancies between the htc one remix and garmin devices are only statistically significant at a 95 confidence interval in cloudy weather between urban and clear sky cover scenarios in this case the maximum difference between the htc and garmin devices is around 16 m comparing each gps unit to an identifiable location on google earth yields a similar figure adding this accuracy to garmin s reported accuracy of 10 m without waas results in a potential tolerance radius of around 26 m these results indicate that a smartphone can provide reasonable location accuracy when recording hazard information in a variety of weather and cover conditions in rural areas and when raster cell sizes are intended to be larger than about 50 m smartphone accuracy is comparable to a handheld device 
26464,this study compares two methods of downscaling the weather research and forecasting model output temperatures to 1 km resolution over the largest vineyard area in new zealand the wrf dynamical downscaling is obtained via a four level nested grid configuration to create a 1 km grid the statistical downscaling is achieved using a support vector regression svr between wrf 3 km output temperatures and terrain at 1 km resolution the bias of the two approaches is evaluated using automatic weather stations and the averages of both 1 km and 3 km model output are associated with a cold bias the sensitivity of the methods to the input sample size is assessed using statistical indicators the results demonstrate that for an equivalent sample size there is no need to dynamically downscale the model temperatures from 3 to 1 km as statistical downscaling seems to provide results very close to those of dynamical downscaling while requiring less computer resources 1 introduction many atmospheric models can simulate climate at various scales over the earth s surface general circulation models gcms with a low spatial resolution between 1 25 et 3 are mainly used for developing climate change scenarios and for large scale studies obviously this kind of model cannot be used to represent climate variability at the local scale such as in vineyard areas so it is important to downscale from larger scale models to a much finer scale in order to investigate climate variability at a more appropriate resolution downscaling methods can be split into two broad groups dynamical downscaling and statistical downscaling the dynamical approach involves regionalizing gcm output to specific areas on the earth s surface by combining the primitive equations associated with continuity momentum and thermodynamic processes with surface characteristics of the region of interest such as terrain and land use a number of different regional models like rams regional atmospheric modelling system pielke et al 1992 or wrf weather research and forecasting skamarock et al 2008 have been widely used to dynamically downscale from the synoptic and larger scale atmospheric circulation in order to provide a high resolution analysis of weather and climate in regions of complex terrain caldwell et al 2009 evaluated wrf output over california at 12 km resolution wrf was also used to study fire weather over new zealand simpson et al 2014 while it has been used in vineyard case studies in both south africa bonnardot et al 2011 soltanzadeh et al 2016 and new zealand sturman et al 2014 one of the main issues for dynamical downscaling is the large amount of computing time needed to achieve fine scale resolution a way to address this issue is to use statistical methods of downscaling in this case local climate is predicted through establishing a relationship between larger scale and local scale variables there are several different methods of statistical downscaling including the use of regression models and stochastic weather generators or weather typing as documented by wilby and wigley 1997 herrera et al 2006 and abatzoglou and brown 2012 although a number of different regression models exist support vector regression svr was chosen for this study because of its capability to handle a large amount of multi dimensional data without requiring significant quantities of computation time support vector regression has already been carried out in other situations with useful results including wetland vegetation mapping using temporal series by betbeder et al 2014 atmospheric temperature prediction radhika and shashi 2009 and time series prediction sapankevych and sankar 2009 in climate model downscaling it has been used for downscaling temperature anandhi et al 2009 and precipitation tripathi et al 2006 chen et al 2010 dynamical and statistical present advantages and disadvantages and a number of studies comparing and combining dynamical and statistical downscaling have been applied in climate modelling boé et al 2007 campozano et al 2016 gutmann et al 2012 these kind of studies are essential to improve the resolution of climate model without increasing the resources required in the modelling process 2 methodology 2 1 study area and data the study area for this research is the marlborough vineyard region which is the largest wine producing area of new zealand 23 000 ha are covered by vineyards and the total study site represents around 6000 km2 the area is surrounded by high mountains some of which reach over 2000 m and is bordered by the pacific ocean on the eastern side fig 1 the complex terrain combined with the oceanic influence creates a very specific and complicated local climate influenced by both mountain valley and land sea breeze circulations a network of automatic weather stations aws has been established over recent years to analyse climate variability over the area it records climate variables such as temperature relative humidity pressure and wind speed and direction every 15 min some aws have missing data due to technical issues while others have only recently been installed as a result only 18 aws with complete data sets over the study period 2013 09 01 to 2014 04 30 have been used in this study this period was selected as it represents the winegrape growing season in the region 2 2 statistical and dynamical downscaling the wrf model uses dynamical downscaling via a four level nested grid configuration to create a 1 km grid a data set of at least this resolution is required for analysing grapevine response to climate in vineyard regions because the vine is very sensitive to local conditions the model was run twice per day producing hourly output data of meteorological parameters including air temperature and pressure wind speed and direction and atmospheric humidity however this required access to powerful computer facilities and substantial computing time previous research indicates that climate is strongly influenced by complex terrain at the fine scale creating significant spatial variability guyot 1997 however it has been shown that regression functions can be used to represent the relationship between temperature and terrain schoof and pryor 2001 cortes and vapnik 1995 developed the machine learning theory on which svr is based and thanks to the use of kernel functions svr is able to estimate complex relationships between a dependent variable temperature in this case named y in practice and a range of independent variables x in this analysis there are six of these latter variables which were derived from a 1 km digital elevation model dem elevation slope aspect separated into two components north south and east west and geographic coordinates latitude and longitude these environmental parameters have been chosen based on previous work le roux et al 2017 the main idea of svr consists of automatically identifying a number of n support vectors s i selected from the data between which a non linear regression function can be estimated through a kernel k the regression function is as follows y x β 0 i 1 n β i k x s i ε where s i i 1 n are the vector support parameters and β i the associated coefficients variables s i n and β i are automatically computed from the training set using the svr approach the only parameter to be selected is the kernel function k as can be seen in the equation above it is a function of the dot product between a variable x and each support vector s i and is therefore always a number of dimension 1 in the case of a linear regression k x s i x t s i where x t s i is simply a number representing the usual dot product between x and s i the main idea behind this formulation is its ability to choose a non linear kernel able to model more complex relationships between the predicted y variable temperature and the explanatory variables x the six parameters mentioned above a large number of possibilities are available for a kernel function k in practice we use a gaussian kernel k x s i exp g x s i 2 where g is a parameter to set by cross validation it has been shown that this kernel is able to extract very complex relationships between variables to predict y based on the explanatory variables x takeda et al 2007 and is therefore very appropriate for the problem being addressed here more details of this method are available in freedman 2009 while fig 2 illustrates the difference between the application of a linear regression model and svr and fig 3 provides a schematic overview of the methodology 2 3 sensitivity to sampling size as the svr method is based on the relationship between a number of explanatory variables and a dependent variable training of the statistical downscaling is dependent on the size of the input dataset reducing the number of input points without reducing the accuracy of the downscaled outputs may be possible for studying smaller areas than the marlborough vineyard region while at the same time further reducing computation time the initial dataset of approximately 2000 input points was sampled randomly to obtain a different sized training dataset the first one with 50 of the initial dataset about 1000 points a second one with 25 500 points and another one with only 5 100 points then a final sub sampling was undertaken using a k means classification k means clustering macqueen 1967 is a method commonly used to automatically partition a data set into k groups with similar characteristics wagstaff et al 2001 summarized the method by stating that k means proceeds by selecting k initial cluster centres and then iteratively refining them by each instance being assigned to its closest cluster centre each cluster centre being updated to be the mean of the instances inside the cluster the algorithm converges when there is no further change in assignment of instances to clusters this algorithm is applied to the terrain variables elevation slope aspect and location with an equivalent sample being taken from each cluster 100 points this last step is essential in order to know whether the locations of the input points play a role in the quality of the downscaling the output of each statistical model was compared to the 18 aws and the dynamical downscaling output 3 results 3 1 comparison between statistical and dynamical outputs comparison of all the hourly values for every grid point more than 30 million values in total indicates that the two downscaling methods present very similar results most of the differences are very close to 0 fig 4 a the hexbin plot in fig 4b is a form of bivariate scatterplot allowing the visualization of big dataset structures each data point is associated with a hexagon and the frequency of occurrence of points in each hexagon is counted and plotted using a colour ramp lewin koh 2011 this plot confirms that there is good agreement between the two downscaling approaches although dynamical downscaling seems to be warmer for the simulated minimum temperatures and a bit cooler for the maximum temperatures the spatial variation of the average difference in temperature between the two methods is shown in fig 5 confirming the fact that the dynamical downscaling method produces warmer values for low temperatures mainly observed at higher altitude while producing cooler values in lower areas the wrf model also produces warmer values for the coastal strip and the blenheim urban area 3 2 assessing bias comparison of the two downscaling methods shows that some spatial differences are observed in the simulated near surface temperatures produced by each method the predictive power of each downscaling method is evaluated using the independent observation data set extract from the aws network three classical indicators were computed to evaluate the accuracy of each method coefficient of correlation pearson the index of agreement ioa and the root mean square error rmse table 1 the rmse gives high weight to large errors while the coefficient of correlation represents the strength of the linear relationship between two variables the index of agreement is a standardized measure of model predictive ability willmott et al 2012 with a value of 1 indicating a perfect prediction and 0 indicating no agreement at all between predicted and observed values the downscaled temperatures obtained from both methods is very similar to the 18 aws based on hourly data although the statistical downscaling produced better results in most cases ridge 902 and blind river reserve produced the worst results ridge 902 is located at 902 m above sea level and blind river reserve is very close to the sea and next to a river microclimatic effects due to the specific climatic environment at these locations might explain why neither of the two downscaling methods can reproduce the temperature variations at these aws very well a cold bias is evident in the maps of spatial mean bias of average temperature over the study period for both downscaling methods fig 6 this bias occurs in the main wairau valley particularly at inland sites but is less evident for statistical downscaling than for dynamical downscaling close to the coast dynamical downscaling produced a cold or lack of bias except for blind river reserve where a warm bias was detected on the other hand statistical downscaling produced a weak warm bias or no bias for the coastal region except for the northernmost aws rarangi an anomalously warm bias occurred at ridge 902 for both downscaling methods as mentioned above 3 3 sensitivity the same indicators applied in the previous section ioa coefficient of correlation and rmse were used to compare the impact of sample size on statistical downscaling fig 7 the boxplots represent the median and the 0 25 0 75 quantiles both the ioa and rmse results improved as sample size increased see t2 nz 05 t2 nz 25 t2 nz 50 and t2 nz in fig 7 the terrain clustering based on the k means approach described in section 2 3 above did not seem to show sensitivity of the results to the location of sites in relation to the terrain t2 nz kmeans in fig 7 the coefficient of correlation increased as sample size decreased meaning that the strength of the linear relationship increased with a smaller sample size this may be explained by the fact that with more inputs points the relationships between outputs and observations are increasingly non linear 4 discussion and conclusion in this paper statistical downscaling from wrf 3 km temperature output using svr has been compared to a dynamical method of downscaling to obtain 1 km spatial resolution the svr method is fully automatic and could be reproduced wherever high resolution mesoscale models are needed the climate and especially temperature variability of the marlborough vineyard region has previously been described and related to vine phenology felix et al 2013 sturman et al 2014 and the wrf model has been run every 12 h to provide a short term forecast and the ongoing status of the grapevine growing season for 2013 14 2014 15 and 2015 16 http wineclimate co nz hence the research presented here does not directly improve knowledge of the temperature variability of this area however the results have demonstrated that statistical downscaling is able to provide results very close to those of dynamical downscaling similar results has been shown by spak et al 2007 statistical downcaling requires significantly less computer power 100 time less computation on a similar computer and it is possible to statistically downscale from the 3 km grid to 1 km resolution over a larger area than for the 1 km dynamic downscaling more efficient use of computing time and is beneficial for environmental studies because it allows the same approach to be applied in many different places and repeated over multiple seasons however the statistical approach only allows the downscaling of one climatic variable at a time and it is assumed that the relationship between this variable and environmental parameters in this case terrain are consistent over time herrera et al 2006 sunyer et al 2012 moreover statistical downscaling required that wrf was run over the study site to produce the 3 km resolution data set needed to create the training set for svr another issue is the sensitivity of the statistical downscaling to the sample size chosen for training the svr model obviously at some sample size the model fit would become close to perfect the model is highly predictive and as the number of training points decreases the predictive power will drop so it is important to define the optimal number of samples required to achieve accurate temperature patterns although it is expected that dynamical downscaling should be more robust in representing the atmospheric processes involved the results show that statistical downscaling produces an acceptable result in this case with the added benefit of reducing the processing time involved the complexity of the terrain provides a good challenge for this approach and emphasises the robustness of the methods used ghosh and mujumdar 2008 suggested the use of the relevance vector machine rvm approach instead of svr to downscale climate models they observed that rvm outperforms svr for regression based statistical downscaling in terms of goodness of fit and rvm involves a fewer number of relevant vectors and the chance of overfitting is less than that of svrs in this case svr has provided satisfactory results but using and comparing rvm and svr might be something to explore in future research statistical downscaling can also be used for other climate parameters and applications as boé et al 2007 demonstrated that statistical downscaling produced better results for temperature and precipitation when they ran an hydrological model however gutmann et al 2012 have shown that statistical downscaling cannot represent well the changes in the spatial pattern of winter precipitation over complex terrain quality of the downscaling for both methods appears very dependent on study site characteristics although both methods used the same altitude data set they are subject to a potential problem associated with the accuracy of the digital elevation model used in this case only 1 km resolution it was noticed that the resulting smoothed terrain resulted in altitude errors when comparing both methods with measurements from automatic weather stations as much as 300 m this paper has demonstrated that statistical downscaling of temperature from wrf 3 km resolution output data to 1 km resolution using the svr approach provides very similar results to those achieved by dynamical downscaling but using fewer resources in a climate change context where a lot of data are often required combining regional dynamical models and statistical downscaling provides an interesting framework to estimate local future climate pierce et al 2012 martin et al 2013 tang et al 2016 as climate is an important component of the terroir of vineyard regions climate change will affect every wine producing region so that improved spatial resolution of climate models can be an asset for wine producers and assist in the development of finely tuned adaptation strategies for ensuring the future sustainability of high quality wine production acknowledgements the research team are grateful for the contribution of the life financial instrument of the european union under the contract number life13 env fr 001512 and the funding provided for this research by the ministry for primary industries new zealand and ongoing support of the department of geography university of canterbury 
26464,this study compares two methods of downscaling the weather research and forecasting model output temperatures to 1 km resolution over the largest vineyard area in new zealand the wrf dynamical downscaling is obtained via a four level nested grid configuration to create a 1 km grid the statistical downscaling is achieved using a support vector regression svr between wrf 3 km output temperatures and terrain at 1 km resolution the bias of the two approaches is evaluated using automatic weather stations and the averages of both 1 km and 3 km model output are associated with a cold bias the sensitivity of the methods to the input sample size is assessed using statistical indicators the results demonstrate that for an equivalent sample size there is no need to dynamically downscale the model temperatures from 3 to 1 km as statistical downscaling seems to provide results very close to those of dynamical downscaling while requiring less computer resources 1 introduction many atmospheric models can simulate climate at various scales over the earth s surface general circulation models gcms with a low spatial resolution between 1 25 et 3 are mainly used for developing climate change scenarios and for large scale studies obviously this kind of model cannot be used to represent climate variability at the local scale such as in vineyard areas so it is important to downscale from larger scale models to a much finer scale in order to investigate climate variability at a more appropriate resolution downscaling methods can be split into two broad groups dynamical downscaling and statistical downscaling the dynamical approach involves regionalizing gcm output to specific areas on the earth s surface by combining the primitive equations associated with continuity momentum and thermodynamic processes with surface characteristics of the region of interest such as terrain and land use a number of different regional models like rams regional atmospheric modelling system pielke et al 1992 or wrf weather research and forecasting skamarock et al 2008 have been widely used to dynamically downscale from the synoptic and larger scale atmospheric circulation in order to provide a high resolution analysis of weather and climate in regions of complex terrain caldwell et al 2009 evaluated wrf output over california at 12 km resolution wrf was also used to study fire weather over new zealand simpson et al 2014 while it has been used in vineyard case studies in both south africa bonnardot et al 2011 soltanzadeh et al 2016 and new zealand sturman et al 2014 one of the main issues for dynamical downscaling is the large amount of computing time needed to achieve fine scale resolution a way to address this issue is to use statistical methods of downscaling in this case local climate is predicted through establishing a relationship between larger scale and local scale variables there are several different methods of statistical downscaling including the use of regression models and stochastic weather generators or weather typing as documented by wilby and wigley 1997 herrera et al 2006 and abatzoglou and brown 2012 although a number of different regression models exist support vector regression svr was chosen for this study because of its capability to handle a large amount of multi dimensional data without requiring significant quantities of computation time support vector regression has already been carried out in other situations with useful results including wetland vegetation mapping using temporal series by betbeder et al 2014 atmospheric temperature prediction radhika and shashi 2009 and time series prediction sapankevych and sankar 2009 in climate model downscaling it has been used for downscaling temperature anandhi et al 2009 and precipitation tripathi et al 2006 chen et al 2010 dynamical and statistical present advantages and disadvantages and a number of studies comparing and combining dynamical and statistical downscaling have been applied in climate modelling boé et al 2007 campozano et al 2016 gutmann et al 2012 these kind of studies are essential to improve the resolution of climate model without increasing the resources required in the modelling process 2 methodology 2 1 study area and data the study area for this research is the marlborough vineyard region which is the largest wine producing area of new zealand 23 000 ha are covered by vineyards and the total study site represents around 6000 km2 the area is surrounded by high mountains some of which reach over 2000 m and is bordered by the pacific ocean on the eastern side fig 1 the complex terrain combined with the oceanic influence creates a very specific and complicated local climate influenced by both mountain valley and land sea breeze circulations a network of automatic weather stations aws has been established over recent years to analyse climate variability over the area it records climate variables such as temperature relative humidity pressure and wind speed and direction every 15 min some aws have missing data due to technical issues while others have only recently been installed as a result only 18 aws with complete data sets over the study period 2013 09 01 to 2014 04 30 have been used in this study this period was selected as it represents the winegrape growing season in the region 2 2 statistical and dynamical downscaling the wrf model uses dynamical downscaling via a four level nested grid configuration to create a 1 km grid a data set of at least this resolution is required for analysing grapevine response to climate in vineyard regions because the vine is very sensitive to local conditions the model was run twice per day producing hourly output data of meteorological parameters including air temperature and pressure wind speed and direction and atmospheric humidity however this required access to powerful computer facilities and substantial computing time previous research indicates that climate is strongly influenced by complex terrain at the fine scale creating significant spatial variability guyot 1997 however it has been shown that regression functions can be used to represent the relationship between temperature and terrain schoof and pryor 2001 cortes and vapnik 1995 developed the machine learning theory on which svr is based and thanks to the use of kernel functions svr is able to estimate complex relationships between a dependent variable temperature in this case named y in practice and a range of independent variables x in this analysis there are six of these latter variables which were derived from a 1 km digital elevation model dem elevation slope aspect separated into two components north south and east west and geographic coordinates latitude and longitude these environmental parameters have been chosen based on previous work le roux et al 2017 the main idea of svr consists of automatically identifying a number of n support vectors s i selected from the data between which a non linear regression function can be estimated through a kernel k the regression function is as follows y x β 0 i 1 n β i k x s i ε where s i i 1 n are the vector support parameters and β i the associated coefficients variables s i n and β i are automatically computed from the training set using the svr approach the only parameter to be selected is the kernel function k as can be seen in the equation above it is a function of the dot product between a variable x and each support vector s i and is therefore always a number of dimension 1 in the case of a linear regression k x s i x t s i where x t s i is simply a number representing the usual dot product between x and s i the main idea behind this formulation is its ability to choose a non linear kernel able to model more complex relationships between the predicted y variable temperature and the explanatory variables x the six parameters mentioned above a large number of possibilities are available for a kernel function k in practice we use a gaussian kernel k x s i exp g x s i 2 where g is a parameter to set by cross validation it has been shown that this kernel is able to extract very complex relationships between variables to predict y based on the explanatory variables x takeda et al 2007 and is therefore very appropriate for the problem being addressed here more details of this method are available in freedman 2009 while fig 2 illustrates the difference between the application of a linear regression model and svr and fig 3 provides a schematic overview of the methodology 2 3 sensitivity to sampling size as the svr method is based on the relationship between a number of explanatory variables and a dependent variable training of the statistical downscaling is dependent on the size of the input dataset reducing the number of input points without reducing the accuracy of the downscaled outputs may be possible for studying smaller areas than the marlborough vineyard region while at the same time further reducing computation time the initial dataset of approximately 2000 input points was sampled randomly to obtain a different sized training dataset the first one with 50 of the initial dataset about 1000 points a second one with 25 500 points and another one with only 5 100 points then a final sub sampling was undertaken using a k means classification k means clustering macqueen 1967 is a method commonly used to automatically partition a data set into k groups with similar characteristics wagstaff et al 2001 summarized the method by stating that k means proceeds by selecting k initial cluster centres and then iteratively refining them by each instance being assigned to its closest cluster centre each cluster centre being updated to be the mean of the instances inside the cluster the algorithm converges when there is no further change in assignment of instances to clusters this algorithm is applied to the terrain variables elevation slope aspect and location with an equivalent sample being taken from each cluster 100 points this last step is essential in order to know whether the locations of the input points play a role in the quality of the downscaling the output of each statistical model was compared to the 18 aws and the dynamical downscaling output 3 results 3 1 comparison between statistical and dynamical outputs comparison of all the hourly values for every grid point more than 30 million values in total indicates that the two downscaling methods present very similar results most of the differences are very close to 0 fig 4 a the hexbin plot in fig 4b is a form of bivariate scatterplot allowing the visualization of big dataset structures each data point is associated with a hexagon and the frequency of occurrence of points in each hexagon is counted and plotted using a colour ramp lewin koh 2011 this plot confirms that there is good agreement between the two downscaling approaches although dynamical downscaling seems to be warmer for the simulated minimum temperatures and a bit cooler for the maximum temperatures the spatial variation of the average difference in temperature between the two methods is shown in fig 5 confirming the fact that the dynamical downscaling method produces warmer values for low temperatures mainly observed at higher altitude while producing cooler values in lower areas the wrf model also produces warmer values for the coastal strip and the blenheim urban area 3 2 assessing bias comparison of the two downscaling methods shows that some spatial differences are observed in the simulated near surface temperatures produced by each method the predictive power of each downscaling method is evaluated using the independent observation data set extract from the aws network three classical indicators were computed to evaluate the accuracy of each method coefficient of correlation pearson the index of agreement ioa and the root mean square error rmse table 1 the rmse gives high weight to large errors while the coefficient of correlation represents the strength of the linear relationship between two variables the index of agreement is a standardized measure of model predictive ability willmott et al 2012 with a value of 1 indicating a perfect prediction and 0 indicating no agreement at all between predicted and observed values the downscaled temperatures obtained from both methods is very similar to the 18 aws based on hourly data although the statistical downscaling produced better results in most cases ridge 902 and blind river reserve produced the worst results ridge 902 is located at 902 m above sea level and blind river reserve is very close to the sea and next to a river microclimatic effects due to the specific climatic environment at these locations might explain why neither of the two downscaling methods can reproduce the temperature variations at these aws very well a cold bias is evident in the maps of spatial mean bias of average temperature over the study period for both downscaling methods fig 6 this bias occurs in the main wairau valley particularly at inland sites but is less evident for statistical downscaling than for dynamical downscaling close to the coast dynamical downscaling produced a cold or lack of bias except for blind river reserve where a warm bias was detected on the other hand statistical downscaling produced a weak warm bias or no bias for the coastal region except for the northernmost aws rarangi an anomalously warm bias occurred at ridge 902 for both downscaling methods as mentioned above 3 3 sensitivity the same indicators applied in the previous section ioa coefficient of correlation and rmse were used to compare the impact of sample size on statistical downscaling fig 7 the boxplots represent the median and the 0 25 0 75 quantiles both the ioa and rmse results improved as sample size increased see t2 nz 05 t2 nz 25 t2 nz 50 and t2 nz in fig 7 the terrain clustering based on the k means approach described in section 2 3 above did not seem to show sensitivity of the results to the location of sites in relation to the terrain t2 nz kmeans in fig 7 the coefficient of correlation increased as sample size decreased meaning that the strength of the linear relationship increased with a smaller sample size this may be explained by the fact that with more inputs points the relationships between outputs and observations are increasingly non linear 4 discussion and conclusion in this paper statistical downscaling from wrf 3 km temperature output using svr has been compared to a dynamical method of downscaling to obtain 1 km spatial resolution the svr method is fully automatic and could be reproduced wherever high resolution mesoscale models are needed the climate and especially temperature variability of the marlborough vineyard region has previously been described and related to vine phenology felix et al 2013 sturman et al 2014 and the wrf model has been run every 12 h to provide a short term forecast and the ongoing status of the grapevine growing season for 2013 14 2014 15 and 2015 16 http wineclimate co nz hence the research presented here does not directly improve knowledge of the temperature variability of this area however the results have demonstrated that statistical downscaling is able to provide results very close to those of dynamical downscaling similar results has been shown by spak et al 2007 statistical downcaling requires significantly less computer power 100 time less computation on a similar computer and it is possible to statistically downscale from the 3 km grid to 1 km resolution over a larger area than for the 1 km dynamic downscaling more efficient use of computing time and is beneficial for environmental studies because it allows the same approach to be applied in many different places and repeated over multiple seasons however the statistical approach only allows the downscaling of one climatic variable at a time and it is assumed that the relationship between this variable and environmental parameters in this case terrain are consistent over time herrera et al 2006 sunyer et al 2012 moreover statistical downscaling required that wrf was run over the study site to produce the 3 km resolution data set needed to create the training set for svr another issue is the sensitivity of the statistical downscaling to the sample size chosen for training the svr model obviously at some sample size the model fit would become close to perfect the model is highly predictive and as the number of training points decreases the predictive power will drop so it is important to define the optimal number of samples required to achieve accurate temperature patterns although it is expected that dynamical downscaling should be more robust in representing the atmospheric processes involved the results show that statistical downscaling produces an acceptable result in this case with the added benefit of reducing the processing time involved the complexity of the terrain provides a good challenge for this approach and emphasises the robustness of the methods used ghosh and mujumdar 2008 suggested the use of the relevance vector machine rvm approach instead of svr to downscale climate models they observed that rvm outperforms svr for regression based statistical downscaling in terms of goodness of fit and rvm involves a fewer number of relevant vectors and the chance of overfitting is less than that of svrs in this case svr has provided satisfactory results but using and comparing rvm and svr might be something to explore in future research statistical downscaling can also be used for other climate parameters and applications as boé et al 2007 demonstrated that statistical downscaling produced better results for temperature and precipitation when they ran an hydrological model however gutmann et al 2012 have shown that statistical downscaling cannot represent well the changes in the spatial pattern of winter precipitation over complex terrain quality of the downscaling for both methods appears very dependent on study site characteristics although both methods used the same altitude data set they are subject to a potential problem associated with the accuracy of the digital elevation model used in this case only 1 km resolution it was noticed that the resulting smoothed terrain resulted in altitude errors when comparing both methods with measurements from automatic weather stations as much as 300 m this paper has demonstrated that statistical downscaling of temperature from wrf 3 km resolution output data to 1 km resolution using the svr approach provides very similar results to those achieved by dynamical downscaling but using fewer resources in a climate change context where a lot of data are often required combining regional dynamical models and statistical downscaling provides an interesting framework to estimate local future climate pierce et al 2012 martin et al 2013 tang et al 2016 as climate is an important component of the terroir of vineyard regions climate change will affect every wine producing region so that improved spatial resolution of climate models can be an asset for wine producers and assist in the development of finely tuned adaptation strategies for ensuring the future sustainability of high quality wine production acknowledgements the research team are grateful for the contribution of the life financial instrument of the european union under the contract number life13 env fr 001512 and the funding provided for this research by the ministry for primary industries new zealand and ongoing support of the department of geography university of canterbury 
