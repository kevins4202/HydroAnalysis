index,text
26355,a bayesian total uncertainty analysis framework for assessment of management practices using watershed models ali tasdighi a b mazdak arabi a daren harmel c daniel line d a department of civil and environmental engineering colorado state university campus delivery 1372 fort collins co 80523 1372 usa department of civil and environmental engineering colorado state university campus delivery 1372 fort collins co 80523 1372 usa b department of civil and environmental engineering university of california irvine e4130 engineering gateway irvine ca 92697 2175 usa department of civil and environmental engineering university of california irvine e4130 engineering gateway irvine ca 92697 2175 usa c usda ars center for agricultural resources research 2150 centre ave fort collins co 80526 8119 usa usda ars center for agricultural resources research 2150 centre ave fort collins co 80526 8119 usa d department of biological and agricultural engineering north carolina state university campus box 7625 raleigh nc 27695 7619 usa department of biological and agricultural engineering north carolina state university campus box 7625 raleigh nc 27695 7619 usa corresponding author department of civil and environmental engineering colorado state university campus delivery 1372 fort collins co 80523 1372 usa department of civil and environmental engineering colorado state university campus delivery 1372 fort collins co 80523 1372 usa a bayesian total uncertainty analysis framework is presented to assess the model estimates of the effectiveness of watershed management practices in reducing nonpoint source nps pollution the framework entails a two stage procedure first various sources of modeling uncertainties are characterized during the period before implementing best management practices bmps second the effectiveness of bmps are probabilistically quantified during the post bmp period the framework was used to assess the uncertainties in effectiveness of two bmps in reducing daily total nitrogen tn loads in a 54 ha agricultural watershed in north carolina using the swat model the results indicated that the modeling uncertainties in quantifying the effectiveness of selected bmps were relatively large assessment of measured data uncertainty revealed that higher errors were observed in simulating tn loads during high flow events the results of this study have important implications for decision making under uncertainty when models are used for water quality simulation keywords watershed management bmp effectiveness water quality conservation watershed modeling nonpoint source pollution bayesian uncertainty analysis 1 introduction watershed management practices are regularly used for abatement of nonpoint source nps pollution yet their effectiveness has been a subject of debate arabi et al 2007 park et al 2011 studies have quantified the effectiveness of nps conservation practices using monitoring campaigns clausen et al 1996 bishop et al 2005 byers et al 2005 line et al 2016 however this approach can be costly and the results obtained are often site specific and inconsistent between studies hence models are increasingly used along with the monitoring data to better assess the effectiveness of best management practices bmps santhi et al 2003 arabi et al 2006 lin et al 2009 park and roesner 2012 taylor et al 2016 jang et al 2017 the majority of studies that have investigated the effectiveness of bmps using models used a deterministic approach which often entails calibrating a watershed model by changing the parameters to obtain a good fit between model simulations and measured observations lin et al 2009 ullrich and volk 2009 liu et al 2013 jingyuan et al 2014 motallebi et al 2017 the calibrated model is then checked against a record of observations from a different time period to perform a so called validation process the model is then used to quantify the effectiveness of a specific bmp or a combination of bmps by changing some parameters that reflect the bmp operation arabi et al 2008 liu et al 2013 jang et al 2017 this approach can be inadequate and in many cases misleading due to lack of accounting for different sources of modeling uncertainties ajami et al 2007 arabi et al 2007 tasdighi 2017 propagation of uncertainties from different sources into model predictions during any modeling practice may result in biased and unrealistic decisions models are mere representation of reality which makes them subject to uncertainty the uncertainty in any modeling practice stems from different sources including model parameters input data climate land use etc model structure conceptualization and measurement data used for training testing the model streamflow nutrient concentrations or loads etc there are many studies that investigate the effects of modeling uncertainties in simulating streamflow or various water quality constituents beven and binley 1992 vrugt et al 2003 ajami et al 2007 rojas et al 2008 harmel et al 2014 yen et al 2014 ajami et al 2007 developed a bayesian framework for estimating various sources of uncertainty in simulating streamflow with hydrologic models yen et al 2014 also developed a similar framework for propagation of uncertainty from various sources to simulated streamflow and nitrate while these studies have investigated the effects of modeling uncertainties on streamflow and water quality simulations the impacts of modeling uncertainty when quantifying the effectiveness of bmps has not been addressed sufficiently arabi et al 2007 specifically a framework that incorporates various sources of modeling uncertainty and determines uncertainty bounds around bmp effectiveness has not been developed to the best of our knowledge an important source of uncertainty when assessing the effectiveness of bmps using a model is the measured data used for training and testing the model in studies where only water quantity is explored the uncertainty from streamflow measurements is often the focus while the uncertainties from measured constituent concentration are also relevant in water quality studies harmel et al 2009 water quality constituent loads are often derived by multiplying streamflow volume and corresponding constituent concentration thus uncertainties in streamflow and concentration measurement contribute to uncertainty in load estimates assessing the effectiveness of nps bmps often entails a two stage approach first a model is developed for quantifying the nps pollution second by changing selected model parameters to mimic the operation of bmps effectiveness of bmps is quantified this two stage approach makes the uncertainty analysis of bmps cumbersome and time consuming within an uncertainty assessment framework each model parameter has a probability distribution derived during model training instead of one optimal set of parameters deterministic approach and some of these parameters and or other additional parameters are changed later to reflect the operation of bmps hence adopting a probabilistic approach for changing these bmp parameters can quickly result in a large number of iterations in this regard a technique that can be used to efficiently conduct the bmp uncertainty analysis without compromising the statistical inferences gained during any step of the analysis is essential the overall goal of this study was to develop a probabilistic approach to assess the effectiveness of nps conservation practices in reducing nutrient loads the specific objectives were to i assess the effects of uncertainty in measured training testing data on simulating nutrient loads and how it can impact the inferences about effectiveness of bmps and ii determine the total uncertainty bounds 95 confidence interval of model simulations around the model estimates of bmps effectiveness in reducing nutrient loads while previous studies have determined the effectiveness of nps bmps through monitoring campaigns or deterministic modeling approaches using a total uncertainty assessment framework i e accounting for various sources of uncertainties including model parameters input structure and observation data to probabilistically assess the effectiveness of bmps is novel specifically incorporation of measurement data uncertainty which plays an important role in realistic simulation of nps nutrient loads can enhance assessing the water quality benefits of bmps using watershed models application of the framework developed in this study can produce useful information for decision making in water quality improvement through application of nps conservation practices 2 material and methods three separate swat models were developed the models were identical except for the rainfall runoff mechanism accounting for different model structures and the precipitation time step accommodating the mechanism the analysis period was 2008 2015 of which 2008 to 2011 was the pre bmp and 2012 to 2015 represented the post bmp period a 2 yr warmup period was used when running the models a two stage bayesian total uncertainty assessment framework was developed and applied to assess the effectiveness of the bmps nutrient management and cattle exclusion fencing described further in 2 7 in reducing daily total nitrogen tn loads at the outlet of a 54 ha agricultural watershed in north carolina in the first stage modeling uncertainties were characterized during the pre bmp period 2008 2011 a markov chain monte carlo mcmc sampling scheme along with a statistically correct likelihood function which accounts for various statistical conditions including normality lack of autocorrelation and heteroscedasticity of error residuals stedinger et al 2008 vrugt 2016 set to consider the tn load errors at the outlet of the watershed was used to derive the posterior distribution of model parameters while incorporating various sources of modeling uncertainties in the second stage the effectiveness of bmps was assessed by comparing the simulated tn loads before and after implementing bmps during the post bmp period 2012 2015 the size of the posterior distribution of model parameters was reduced while maintaining their statistical characteristics using a uniform random sampling algorithm combined with a linear interpolation technique the models were then run using each set of reduced model parameters to generate prediction intervals of tn loads before implementing bmps then for each set of model parameters bmp parameters were sampled using a uniform random sampling algorithm the models were run using each combination of model and bmp parameters generating tn load after implementing bmps using the post bmp field observation data the value of the likelihood function was also computed for each parameter set the posterior distributions of bmp parameters were then derived and tn load prediction intervals after implementing bmps were determined finally the tn load reduction prediction intervals were quantified to determine bands of uncertainty around effectiveness of bmps 2 1 study watershed the study watershed was a 54 ha pasture dominated watershed located in central north carolina fig 1 the land use composition within the watershed is 78 pasture 14 forest 6 developed and 2 cultivated crops national land cover database nlcd 2011 the watershed was selected due to availability of comprehensive monitoring data streamflow nutrients and sediments from a published paired watershed study line et al 2016 more importantly the monitoring dataset provides a record of event based measurements for 4 yr before installing bmps and another 4 yr after bmps installation the main stream in the watershed known as the mud lick creek flows much of the year except some periods during late summer and early fall where it dries up in general the streams in this region are known for low baseflow line et al 2016 2 2 watershed model swat is a continuous time distributed parameter process based watershed model which has been used extensively for hydrologic and water quality assessments under varying climatic land use and management conditions in small watersheds to large river basins gassman et al 2007 arnold et al 2012 card staff 2016 the model has the capability to run on daily or smaller time steps in swat the watershed is split into smaller subwatersheds which are further discretized into hydrologic response units hrus hrus are the smallest spatial units in swat and are defined as areas within each subwatershed with unique combinations of land use soil and slope class climate inputs drive hydrologic responses and provide moisture and energy inputs in swat hydrologic processes simulated in the model include canopy storage surface runoff infiltration evapotranspiration lateral flow tile drainage redistribution of water within the soil profile return flow and recharge arnold et al 2012 surface runoff is simulated using either the modified green and ampt method g a green and ampt 1911 with subdaily rainfall or curve number method cn usda nrcs 2004 with daily rainfall 2 3 model inputs terrain soils land use climate and hydrography the elevation data for building the swat model was the 1 3 arc second 10 m resolution digital elevation model dem obtained from united states geological survey the national map usgs tnm 2016 the soil survey geographic ssurgo database from united states department of agriculture natural resources conservation services usda nrcs 2016 was used to represent soil characteristics and variability in the watershed the national agricultural statistics service nass land use data for year 2011 were obtained from the usda usda cropscape 2016 the resolution of the ssurgo and nass data was 1 arc second 30 m stream flowlines were obtained from usgs national hydrography dataset usgs nhd 2016 the stream flowlines were used for more accurate stream delineation in swat by superimposing the nhd flowlines on the dem in the process of watershed and stream delineation the hydrographic segmentation subwatershed boundary and stream delineation is improved especially in smaller scales such as the watershed in this study or locations where the dem does not provide enough accuracy winchell et al 2007 observed climate data for the closest meteorological station burlington alamance regional airport ghcnd usw00093783 were obtained from the national climatic data center ncdc quality controlled local climatological data qclcd database noaa 2016 the station was located about 20 km from the watershed daily and hourly precipitation minimum and maximum temperature were collected for 2005 to 2015 2 4 measurements stream discharge and nutrient concentrations the monitoring station was located at the outlet of the watershed fig 1 streamflow and nutrient measurements were available from 2008 to 2015 this period includes pre bmp 1 jan 2008 to 5 oct 2011 and post bmp 6 oct 2011 to 31 dec 2015 periods stream discharge and nutrient concentrations were sampled during storm events the discharge measurement and nutrient sampling procedures collecting storing and analyzing are explained in details in line et al 2016 mean daily nutrient loads were computed by multiplying the mean daily storm discharge and corresponding nutrient or sediment concentration 2 5 cattle grazing and manure deposition there were 75 beef cows grazing on various parts of the pasture within the watershed based on the observations and communication with land owners line et al 2016 reported a density of 1 2 cows per ha which was used in this study for computing grazing and manure deposition rates manure deposition rate for beef cows was set at 29 5 kg day 500 kg animal usda nrcs 2016 assuming an average weight of a beef cow equal to 600 kg the average manure deposition rate was computed as 35 4 kg ha day the cattle deposit their manure either on land or in streams on average cattle spend 7 of their time in the streams byers et al 2005 therefore it was assumed that 2 5 kg ha day of manure was directly deposited into streams while the remaining fraction 32 9 kg ha day was deposited on the land each beef cow consumes on average 45 kg ha day of grass usda nrcs 2016 which along with the 1 2 cows ha density gives a grazing rate of about 54 kg ha day 2 6 the swat model setup arcswat 2012 usda ars 2014 was used to develop three swat models the models were completely identical except for the runoff estimation method and precipitation time step a model was developed with daily precipitation and cn method based on i soil moisture cn i and ii plant evapotranspiration cn ii and the other model was developed using the hourly precipitation and g a method for runoff simulation therefore three model setups were prepared for the analyses corresponding to three different model structures the dem was used along with the nhd flowlines to delineate the watershed and streams the watershed was divided into 4 subwatersheds the hru definition was based on land use and soil data since the topographic variability was small a single class slope was assumed within each subwatershed using these settings 29 hrus were defined for the watershed fig 2 2 7 representing bmps in the swat model the bmps implemented in the watershed were nutrient management and cattle exclusion fencing while nutrient management can be readily modeled by changing the rate of fertilizer application arabi et al 2008 ahmadi et al 2014 incorporating cattle exclusion fencing in the model is more complicated few studies have discussed methods for representing cattle exclusion fencing in swat one such method is application of point sources to represent the presence of cattle in streams lin et al 2009 this approach provides the capability to directly change the rate of nutrients or sediments introduced into the streams during the implementation of the exclusion fencing mimicing the operation of the bmp limiting the access of the cattle to streams during the pre bmp period 336 kg ha of 15 15 15 n p k fertilizer was applied uniformly to the pasture in subwatersheds 2 3 and 4 biosolids were applied to the pasture in subwatershed 1 analysis of biosolids showed that 130 and 116 kg ha of n and p were applied during each application the 15 15 15 fertilizer is a standard fertilizer in swat databases and hence was readily incorporated in the model however for biosolids their elemental n and p content were used to incorporate them in the model nutrient management was implemented by replacing the biosolids and fertilizer with a granular n only fertilizer 21 0 0 at a rate of 70 kg ha during the post bmp period this bmp was directly implemented in the swat model by replacing and changing the fertilizer application rate the cattle exclusion fencing was installed in october 2011 the fences were installed along approximately 520 m of the mainstream of mud lick creek in the watershed about half of the upper part of the mainstream was not fenced so cattle were still able to have access to streams in the upper half part of the watershed we modeled the cattle exclusion fencing by changing the rate of organic n introducing into the streams via point sources in subbasins 3 and 4 where stream fencing was installed 2 8 the bmp uncertainty assessment framework a probabilistic framework was developed for assessing the effectiveness of nps conservation practices the bayesian based approach explicitly accounts for uncertainties from model parameterization climate input data i e precipitation model structure cn i cn ii or g a and measured data for model training testing i e streamflow and nutrient loads the framework was developed in matlab the mathworks inc the framework was developed building on the work by ajami et al 2007 and yen et al 2014 both of these studies have developed methods for estimating modeling uncertainties from various sources in simulating streamflow and or water quality however neither addresses estimating the uncertainties in model predicted bmp effectiveness in order to quantify the effectiveness of bmps the model should be run first before implementing the bmps in the model the bmps are then represented in the model through manipulation of some parameters the model is run again with the altered parameters to simulate the responses after implementing the bmps the difference between the simulated responses e g streamflow nutrient loads before and after implementation of the bmps quantifies their effectiveness the framework entails a two stage procedure first the modeling uncertainties in simulating nutrient loads are characterized during the pre bmp period 2008 2011 during this stage a markov chain monte carlo mcmc sampling scheme the dream method vrugt et al 2009 along with a statistically correct likelihood function sorooshian and dracup 1980 stedinger et al 2008 vrugt 2016 which accounts for various statistical conditions including normality lack of autocorrelation and heteroscedasticity of error residuals is used to sample the parameter space and derive the posterior distributions input data uncertainty is incorporated by using precipitation multipliers drawn from a gaussian distribution with an uncertain mean and standard deviation sampled along with model parameters during the mcmc procedure the measured data uncertainty is also incorporated by applying correction factors on model residuals r e s i d u a l o b s e r v e d s i m u l a t e d computed based on a probable error range for each measurement harmel and smith 2007 finally bayesian model averaging bma is used to account for model structural uncertainty hoeting et al 1999 having characterized different sources of modeling uncertainties in the first stage the model can be used for simulation of nutrient loads during the post bmp period 2012 2015 without incorporating the bmps in models the outputs provide prediction intervals for nutrient loads during the post bmp period assuming no bmps were implemented the second stage entails iterating on bmp parameters for each model parameter set from stage 1 to characterize the performance of the bmps under various sources of modeling uncertainty during the post bmp period 2012 2015 the first stage often results in too many parameter sets due to large number of iterations required for the mcmc algorithm to converge especially in highly parametrized complex models such as swat a method was proposed to reduce the number of model parameter sets without compromising the statistical characteristics of the inferred posterior distributions the method involves a uniform random sampling scheme along with a linear interpolation algorithm during which model parameter sets are drawn randomly from the posterior distributions while maintaining statistical characteristics of the distributions the bmp parameters were then sampled for each set of the reduced model parameter sets using a uniform random sampling scheme the model is then run using each new joint parameter set model parameters and bmp parameters and the posterior distribution of bmp parameters are derived with the new likelihood function during the post bmp period the outputs from this stage provide prediction intervals for simulated nutrient loads during the post bmp period after bmps are implemented which along with nutrient loads from stage 1 are used to compute prediction intervals for nutrient load reductions i e bmps effectiveness availability of monitoring data during the post bmp conditions provided a unique opportunity to compute values of a new likelihood function in contrast to previous studies where the same likelihood function was used during both pre bmp and post bmp period due to lack of measurements during post bmp conditions arabi et al 2007 2 8 1 model and bmp parameters uncertainty the generic equation for quantifying the effectiveness of a specific bmp using simulations from a watershed model is 1 η b m p q ˆ p r e b m p q ˆ p o s t b m p where η b m p denotes the efficiency or effectiveness of the bmp q ˆ p r e b m p and q ˆ p o s t b m p are the simulated response variables before and after application of the bmp respectively the parameter uncertainty in quantifying the simulated response variables in eq 1 is due to i model parameters θ m and ii bmp parameters θ b m p it should be noted that the uncertainty from model parameters propagates to predictions during both pre bmp and post bmp periods however the bmp parameter uncertainty is only manifested in the post bmp simulations the simulated response variables are also subject to other errors stemming from measured climate inputs r insufficiency of the model conceptualization m model structure and measurement data used for training and testing the model q p r e b m p and q p o s t b m p the simulation error residuals e then take the form 2 e r θ m q q q ˆ q m r θ during the post bmp conditions the error residuals are a function of joint probability density of model and bmp parameters e f θ m θ b m p with expected value of 3 e e f θ m f θ b m p d θ b m p d θ m where e e denotes the expected value of the error residual as a function of both model and bmp parameters eq 3 is conditioned on the independence of model and bmp parameters analytical solution to this equation is often infeasible in case of complex models hence we resort to mcmc methods applying the bayes theorem the parameter set θ is assigned posterior probability distribution p θ q which is proportional to the product of the parameter prior probability distribution p θ and a likelihood function l θ q the likelihood function assuming normally and independently distributed model residuals e with mean zero and variable standard deviation at each observation time step σ t can be expressed as vrugt 2016 4 l θ q σ t 1 n 1 2 π σ t 2 e x p 1 2 σ t 2 q t q ˆ t θ 2 where n is the number of time steps often it is easier to maximize the logarithm of the likelihood function due to numerical stability and algebraic simplicity ajami et al 2007 vrugt 2016 hence the natural log of the likelihood function was adopted for optimization streamflow and nutrient error residuals are often not independently distributed and in most cases temporal autocorrelation exists in the residuals evin et al 2014 a first order auto regressive scheme ar 1 was employed to reduce autocorrelation in error residuals applying the natural log and ar 1 transformation the log likelihood function takes the form 5 l θ q σ ρ n 2 ln 2 π 1 2 ln σ ν 2 1 ρ 2 1 2 σ ν 2 1 ρ 2 e 1 2 t 2 n e t ρ e t 1 2 parameters ρ and σ are determined along with model parameters at each model realization during the mcmc sampling algorithm the model parameters for the uncertainty analysis were selected based on experience and sensitivity analysis performed previously arabi et al 2007 arnold et al 2012 tasdighi et al 2017 table 1 lists the model parameters selected for uncertainty analysis along with their ranges the ranges for parameters were also selected based on the swat user manual and experience from previous study tasdighi et al 2017 arnold et al 2012 in this study uniform noninformative prior distributions were assumed for parameters within predefined ranges the same assumption has been used in many other hydrological modeling studies since prior knowledge of the model parameters is often not available and is case specific ajami et al 2007 the bmp parameters for uncertainty analysis were selected based on the swat capabilities in representing bmps and studies where similar bmps were implemented in swat arabi et al 2008 lin et al 2009 ahmadi et al 2014 table 2 lists the parameters selected for uncertainty analysis of bmps similar to model parameters uniform noninformative prior distributions were assumed for bmp parameters as well the rate of fertilizer application was selected to represent nutrient management practices the rate of organic n introducing into streams via point sources in subbasins 3 and 4 where cattle exclusion fencing was implemented were selected as parameters for simulating exclusion fencing in swat 2 8 2 model input uncertainty a method proposed by ajami et al 2007 was implemented to account for precipitation uncertainty through application of multipliers on precipitation events using this method instead of iterating on each single multiplier the iteration is performed on the mean and standard deviation of a random gaussian distribution from which the multipliers are randomly drawn at each time step 6 p ˆ t φ t p t φ t n μ φ σ φ 2 where p ˆ t and p t are the corrected and observed precipitation depths respectively φ t is the random multiplier drawn from a normal distribution with a random mean μ φ μ φ 0 9 1 1 and variance σ φ 2 σ φ 2 1 e 5 1 e 3 ajami et al 2007 incorporating precipitation multipliers using this approach instead of iterating on each precipitation multiplier reduces the dimensionality issue and improves the identifiability 2 8 3 model structural uncertainty bayesian model averaging bma was used to account for model structural uncertainty hoeting et al 1999 georgakakos et al 2004 the bma is a probabilistic algorithm for combining competing models based on their predictive skills ajami et al 2007 madadgar and moradkhani 2014 in this study the three rainfall runoff model structures in swat m 1 c n i m 2 c n i i m 3 g a were used to explore the effects of model structural uncertainty using the bma the three model structures were combined using probabilistic weights to reduce the model structural uncertainty the posterior distribution of the bma prediction q ˆ b m a is 7 p q ˆ b m a m 1 m 2 m 3 q i 1 3 p m i q p i q ˆ i m i q where p m i q is the posterior probability of the model m i this term can be assumed as a probabilistic weight w i for model m i in the bma prediction q ˆ b m a the constraint for bma weights is i 1 3 w i 1 higher values of w i can be interpreted as higher predictive skill for a given model structure the model weights can be determined using different optimization techniques the expectation maximization em algorithm dempster et al 1977 was used in this study to estimate model weights 2 8 4 incorporating the uncertainty in measured data for model training testing the uncertainty inherent in measured data used for training and testing of models often stems from errors in monitoring design instrumentation data processing storage and operator human errors harmel et al 2009 this type of uncertainty is rarely accounted for in evaluation of model performance harmel and smith 2007 yen et al 2014 the measured data uncertainty is often manifested in heteroscedasticity of error residuals variable error variance some observations may be less reliable than others which results in their errors to have different variances different approaches have been proposed to circumvent this issue including application of different transformations such as natural log or box cox to stabilize the error variances sorooshian and dracup 1980 others have proposed alternative forms for the likelihood function schoups and vrugt 2010 which accounts for error heteroscedasticity by assuming a variable variance for each model residual the main challenge in this approach is determination of the variance for each residual as it requires having repeats of each measurement which most often are not available as a result in such studies variance was subject to inference along with model parameters at each model realization during the sampling procedure schoups and vrugt 2010 vrugt 2016 this approach can lead to dimensionality issues in case of highly parameterized models or when a relatively long record of measurements is used for model training in this study we employed a method based on the study by harmel and smith 2007 in this method each error residual is modified using a correction factor computed based on the properties of the probability distribution of each measured value previous experience and expert s opinion can be used to make informed assumptions about probability distribution for each measured value in the record assuming a normal distribution for each measured value q i mean and median of the distribution is represented by q i the variance can then be computed based on a probable error range p e r which can be assumed based on literature or professional judgment p e r can be constant or variable for all measurements depending on the experience and level of knowledge about the monitoring design once the p e r s are determined the variance σ i 2 for each record of measurement can be computed as 8 σ i 2 p e r i q i 3 9 100 2 eq 8 is useful when the measured values are not transformed since we used a natural log transformation on streamflow and nutrient loads the taylor series expansion for the second moment of a function of random variable f x was used to estimate the variance v a r f x as 9 v a r f x f e x 2 v a r x f μ x 2 σ x 2 applying 9 on the natural log transform function gives 10 v a r ln x σ x x 2 11 s d ln x σ x x c o v p e r where s d stands for the standard deviation and c o v denotes the coefficient of variation using this approach σ for log transformed observations can be estimated with p e r it should be noted that this assumption holds for smaller p e r s 0 3 and higher approximation errors are introduced in higher values of p e r we used expert s opinion to determine p e r s for each streamflow or nutrient load measurement q i the correction factors were then computed as the area under the standard normal distribution 12a c f i f x i μ i σ i 1 σ i 2 π x e x μ 2 2 σ i 2 d x 0 5 if μ x 12b c f i f x i μ i σ i 1 σ i 2 π x e x μ 2 2 σ i 2 d x 0 5 if μ x where f denotes the normal cumulative distribution function x denotes the simulated data q ˆ i and μ represents the observed data q i and σ is determined using eq 11 using the correction factors the residuals are adjusted based on the estimated measured data uncertainty it should be noted that this method requires some level of knowledge about the characteristics of the measurement errors in this case this information was obtained using the description of measurement techniques in the study watershed by line et al 2016 these characteristics are often case specific experience form previous studies can be used to enhance the assumptions and generate better estimates 2 8 5 the dream algorithm for mcmc analyses several bayesian algorithms are available which have been widely used for uncertainty assessment in hydrologic modeling including the generalized likelihood uncertainty estimation glue beven and binley 1992 the shuffled complex evolution metropolis scem ua vrugt et al 2003 and the differential evolution adaptive metropolis dream vrugt et al 2009 dream is a multi chain mcmc method that randomly samples the parameter space and automatically tunes the scale and orientation of the sampling distribution to move toward the target distribution by maximizing the value of the likelihood function the method has been used extensively for parameter estimation of complex environmental models vrugt 2016 the convergence of the algorithm can be monitored using the procedure proposed by gelman and rubin 1992 in this procedure a scale reduction score r is monitored to check whether each parameter has reached a stationary distribution gelman and rubin 1992 the common convergence criterion of r 1 2 was used in this study as well dream is specifically beneficial in the optimization of complex high dimensional problems in this study dream was employed to sample the parameter space and derive the posterior distributions 2 9 evaluation of model performance the performance of the models in simulating daily tn loads under the total uncertainty assessment framework was evaluated using different error statistics computed at each model realization during the pre bmp period 2008 2011 the uncertainty analysis was performed using the values of the likelihood function eq 5 as the objective function computed with daily tn loads at the monitoring station the error statistics during the post bmp 2012 2015 period were calculated using the joint distribution of reduced model parameters and bmp parameters 3 results and discussion 3 1 evaluation of models during the pre and post bmp periods table 3 summarizes the error statistics for models during the pre and post bmp conditions compared to models with cn the model with g a method had a poor performance in terms of various error statistics during the pre bmp period and it was excluded from further analysis for that reason similar results were reported on better performance of the cn method over the g a in other agricultural watersheds kannan et al 2007 cheng et al 2016 in contrast ficklin and zhang 2013 concluded that models with g a are more likely to generate better daily simulations in agricultural watersheds it should be noted that these studies have used a deterministic approach tasdighi et al 2018 compared the performance of the cn and g a methods based on upstream land use conditions using a probabilistic approach they concluded that the g a method had a better performance in highly developed subwatersheds while the cn method had a slightly better performance in agricultural watersheds between models with cn i and cn ii methods cn ii models had a slightly better performance during the pre bmp period during the post bmp period the superiority of the cn ii model was more accentuated generating better error statistics 3 2 characterizing the modeling uncertainties during the pre bmp period stage 1 3 2 1 model parameters uncertainty the posterior cumulative distribution functions cdf of parameters under the cn i and cn ii model structures are illustrated in fig 3 the distributions are derived after the pre bmp uncertainty analysis 2008 2011 note that only the most sensitive parameters are included in this figure as observed in fig 3 models with cn i and cn ii resulted in different distributions for the same parameters while the model structure determined the level of skewness for most parameters for some parameters esco solz cdn the skewness changed from positive to negative under different model structures in general cn i model showed higher sensitivity to parameters pertaining to soil characteristics sol alb sol awc and sol z which conforms to intuition as the cn i method uses the soil water content for determining the curve number high skewness from normality indicates deficiency in identifiability which is often present in the uncertainty analysis of highly parametrized complex models ajami et al 2007 the distributions were derived using 20000 samples after the convergence of the dream algorithm the uniform random sampling algorithm along with the linear interpolation technique discussed in section 2 8 was then used to reduce the number of model parameter sets while maintaining the statistical characteristics of the distributions to be used for post bmp analysis the algorithm was set to draw 1000 parameter sets from the posterior distributions the reduced parameter sets act as priors for the post bmp period in other words the prior distributions of model parameters for the post bmp period are inferred from the posterior distribution of parameters from the pre bmp period for each of the new model parameter sets the uniform random sampling technique discussed in section 2 8 was used to generate bmp parameter sets since only 3 parameters were used for bmps 30 random bmp parameter samples were assumed to adequately represent the bmp parameter space the bmp parameter sets combined with the model parameter sets resulted in 30000 joint parameter sets for the second stage of the analysis post bmp 3 2 2 model input uncertainty the posterior cdfs of mean and standard deviation for normal distributions from which precipitation multipliers were drawn were almost similar and close to uniform the uniformity of the distributions indicates that precipitation multipliers did not have a major impact on tn load simulations from each model the similarity of the distributions on the other hand could be predicted as the daily precipitation data for models with cn i and cn ii were identical hence any difference in distributions resulted from the model structural difference another explanation in this regard is the large number of sensitive model parameters which can diminish the impact of precipitation multipliers in the range assumed using a wider range for mean and standard deviation of the normal distributions from which precipitation multipliers were drawn could result in higher impact from multipliers and probably better performance of the input uncertainty estimation routine 3 2 3 model structural uncertainty bayesian model averaging was used at each model realization to probabilistically combine the models and reduce the model structural uncertainty fig 4 shows the boxplots of bma weights generated during the mcmc procedure based on bma weights cn ii had a better performance in simulating daily tn loads a two sample t test was performed to test the significance of the difference between the distribution of the bma weights from the cn i and cn ii models the results showed that at the 0 05 significance level bma weights for cn ii model were significantly higher than weights for cn i model these results are congruent to the results in section 3 1 where cn ii model showed superior performance in terms of various error statistics one explanation for these results is that in cn ii method the curve number is determined based on plant evapotranspiration and since the study watershed is a pasture dominated agricultural watershed the cn ii has a better performance similar results were obtained in other studies tasdighi et al 2018 yen et al 2014 3 2 4 measured training testing data uncertainty measured data uncertainty was incorporated using correction factors the correction factors were applied on the daily tn load residuals during the computation of the likelihood function at each model realization fig 5 illustrates the probability distribution pdf of correction factors for each model during the pre bmp period the correction factors were categorized based on flow conditions low medium and high flows before generating the pdfs to assess the effects of flow regime on measurement errors the highest values of correction factors were observed during high flow events medium and low flow events resulted in relatively lower values for correction factors this is an important finding as it indicates higher errors during high flow events compared to medium and low flows the phenomenon of higher errors during high flow events is often described as the heteroscedasticity of error residuals which is deemed to be attenuated by applying correction factors other studies that have investigated the uncertainties in measurement data have reported similar behavior sorooshian and dracup 1980 harmel and smith 2007 the results also conform to intuition as monitoring during high flow events often entails higher errors due to difficulties in measurements harmel et al 2006 3 3 assessing the effectiveness of bmps under various sources of modeling uncertainty stage 2 3 3 1 bmp parameters uncertainty the posterior distribution of bmp parameters are illustrated in fig 6 interestingly the parameter pertaining to nutrient management bmp frt kg showed higher sensitivity when quantifying the tn loads this is while the posterior distributions of parameters pertaining to the cattle exclusion fencing were close to uniform which indicates lower sensitivity this outcome also indicates that the nutrient management had a higher impact on nutrient load reductions a possible explanation for this can be the higher uncertainty in the nature of representing cattle exclusion fencing in the swat model several assumptions were used when representing the exclusion fencing such as changing the nutrient introducing into the streams via point sources and the rate adjustments for nutrients introducing into streams a more rigorous approach for representing cattle exclusion fencing may enhance the performance of this bmp and result in more meaningful inferences all bmp parameter posterior distributions showed high deviations from normality assuming wider prior distributions and larger number of iterations on bmp parameters may be effective in reducing these effects 3 3 2 estimating tn load prediction intervals before and after implementation of bmps the cumulative exceedance probability curves for daily tn loads were developed these curves along with bands of uncertainty before and after implementing bmps provide an easily readable informative measure for assessing the effectiveness of bmps in reducing tn loads under uncertainty fig 7 illustrates the 95 confidence interval for cumulative exceedance probability curves for tn loads comparing the prediction intervals before and after implementation of bmps it is evident that the combination of bmps nutrient management and cattle exclusion fencing was successful in reducing the tn loads in the watershed compared to cn i the model with cn ii had a better performance in capturing observed tn loads at all ranges especially the high and medium loads however they showed wider bands of uncertainty the bma had a close performance to the cn ii model which was expected as the cn ii method better simulated the tn loads resulting in higher bma weights while the cumulative exceedance probability curves provide valuable insights into the statistical characteristic of bmps effectiveness they can be misleading too as the serial structure and autocorrelation of the sequence of the simulated and observed records are removed in them vogel and fennessey 1994 in this regard the fraction of observations lying within the prediction intervals should be determined using the time series of simulations and observations for this reason the coverage percent of observations lying inside the 95 confidence interval of simulation ensembles and spread average width of the 95 confidence interval uncertainty band of simulations were determined based on the time series of simulations and observations of tn loads table 4 summarizes the coverage and spread for the models during the pre and post bmp conditions 3 3 3 quantifying the effectiveness of bmps in terms of tn load reductions the effectiveness of bmps in reducing the tn loads was computed by subtracting the tn loads from models before and after implementation of bmps during the common post bmp period 2012 2015 fig 8 depicts the cumulative exceedance probability curves for tn load reductions under different models the highest reductions were observed for higher loads in general the results demonstrate high level of uncertainty in simulating the daily tn load reductions for example for cn ii model in fig 8 at exceedance probability of 25 the tn load reduction from bmps can be any number between 0 1 and 1 kg ha this outcome indicates the importance of accounting for various sources of uncertainty in modeling the performance of the bmps as they directly affect the decision making process in general modeling pollution loads from npss is subject to high levels of uncertainty however most often this uncertainty is ignored and models are used deterministically to compute pollution loads which can result in unrealistic and biased decisions the average annual tn load reduction from the ensemble of cn i cn ii models and bma were 1 5 1 7 and 1 8 kg ha respectively in terms of percentage the average annual reductions were 58 65 and 69 respectively fig 9 shows the pdfs of average annual load reductions for each model these values conform to the reductions determined by the paired watershed study on this watershed conducted by line et al 2016 they found statistically significant reductions in total kjeldahl n 34 and ammonia n 54 while changes in nitrate n loads were not significant the tn load reductions were resulting from the performance of the nutrient management and cattle exclusion fencing combined while it was not feasible to decompose the total tn load reductions between each bmp to determine the performance of each bmp the nutrient management was determined to be more effective as the related bmp parameters showed higher sensitivities when quantifying tn loads 4 conclusions a total uncertainty estimation framework was developed for assessing the water quality benefits of management practices using watershed models the two stage framework first characterizes various sources of modeling uncertainties during the pre bmp period the second stage of the framework uses the inferences on modeling uncertainties from the first stage and quantifies the effectiveness of bmps using a probabilistic approach in general the modeling uncertainties were large resulting in wide bands of uncertainty around both tn loads and load reductions the framework however was successful in capturing the effects of different sources of modeling uncertainties on simulations and propagating them to the bmp effectiveness assessment stage between the three model structures cn i cn ii and g a cn ii showed the best performance in terms of various performance measures including error statistics and bma weights this was attributed to the intensive agricultural land use in the watershed the g a method had an unsatisfactory performance in simulating the tn loads application of the bma slightly enhanced the quality of simulating tn load reductions under uncertainty as it resulted in higher coverage 51 and 35 during the pre and post bmp periods respectively the distribution of the correction factors for measurement uncertainty indicated higher uncertainty for high flow events correctly capturing the heteroscedasticity of error residuals for daily tn load simulations between the two bmps nutrient management had the highest impact on the tn load reductions the parameters pertaining to cattle exclusion fencing did not show much sensitivity when quantifying daily tn loads a possible explanation for this is the higher uncertainty in the nature of representing cattle exclusion fencing in swat several assumptions were used when representing the exclusion fencing such as changing the nutrient introducing into the streams via point sources and the rate adjustments for nutrients introducing streams it should be noted that the intent of this study was to develop a framework for probabilistic assessment of bmp effectiveness in reducing pollutants in streams and not to develop a method to represent specific bmps in models the results of this study have important implications for decision making when models are used for water quality simulation while numerous uncertainty analysis frameworks have been developed to explore modeling uncertainties in quantification of streamflow and water quality components the lack of pragmatic applications of such methods to tackle decision making challenges is a major shortcoming the framework presented in this study is deemed a pioneer attempt to fill this gap in the literature and add to the pragmatic aspects of the uncertainty analysis in hydrologic and water quality simulations software availability all the codes are developed within matlab and can be made available upon request from the first author acknowledgements this publication was made possible by u s epa grant rd835570 its contents are solely the responsibility of the grantee and do not necessarily represent the official views of the u s epa further u s epa does not endorse the purchase of any commercial products or services mentioned in the publication usda is an equal opportunity employer and provider 
26355,a bayesian total uncertainty analysis framework for assessment of management practices using watershed models ali tasdighi a b mazdak arabi a daren harmel c daniel line d a department of civil and environmental engineering colorado state university campus delivery 1372 fort collins co 80523 1372 usa department of civil and environmental engineering colorado state university campus delivery 1372 fort collins co 80523 1372 usa b department of civil and environmental engineering university of california irvine e4130 engineering gateway irvine ca 92697 2175 usa department of civil and environmental engineering university of california irvine e4130 engineering gateway irvine ca 92697 2175 usa c usda ars center for agricultural resources research 2150 centre ave fort collins co 80526 8119 usa usda ars center for agricultural resources research 2150 centre ave fort collins co 80526 8119 usa d department of biological and agricultural engineering north carolina state university campus box 7625 raleigh nc 27695 7619 usa department of biological and agricultural engineering north carolina state university campus box 7625 raleigh nc 27695 7619 usa corresponding author department of civil and environmental engineering colorado state university campus delivery 1372 fort collins co 80523 1372 usa department of civil and environmental engineering colorado state university campus delivery 1372 fort collins co 80523 1372 usa a bayesian total uncertainty analysis framework is presented to assess the model estimates of the effectiveness of watershed management practices in reducing nonpoint source nps pollution the framework entails a two stage procedure first various sources of modeling uncertainties are characterized during the period before implementing best management practices bmps second the effectiveness of bmps are probabilistically quantified during the post bmp period the framework was used to assess the uncertainties in effectiveness of two bmps in reducing daily total nitrogen tn loads in a 54 ha agricultural watershed in north carolina using the swat model the results indicated that the modeling uncertainties in quantifying the effectiveness of selected bmps were relatively large assessment of measured data uncertainty revealed that higher errors were observed in simulating tn loads during high flow events the results of this study have important implications for decision making under uncertainty when models are used for water quality simulation keywords watershed management bmp effectiveness water quality conservation watershed modeling nonpoint source pollution bayesian uncertainty analysis 1 introduction watershed management practices are regularly used for abatement of nonpoint source nps pollution yet their effectiveness has been a subject of debate arabi et al 2007 park et al 2011 studies have quantified the effectiveness of nps conservation practices using monitoring campaigns clausen et al 1996 bishop et al 2005 byers et al 2005 line et al 2016 however this approach can be costly and the results obtained are often site specific and inconsistent between studies hence models are increasingly used along with the monitoring data to better assess the effectiveness of best management practices bmps santhi et al 2003 arabi et al 2006 lin et al 2009 park and roesner 2012 taylor et al 2016 jang et al 2017 the majority of studies that have investigated the effectiveness of bmps using models used a deterministic approach which often entails calibrating a watershed model by changing the parameters to obtain a good fit between model simulations and measured observations lin et al 2009 ullrich and volk 2009 liu et al 2013 jingyuan et al 2014 motallebi et al 2017 the calibrated model is then checked against a record of observations from a different time period to perform a so called validation process the model is then used to quantify the effectiveness of a specific bmp or a combination of bmps by changing some parameters that reflect the bmp operation arabi et al 2008 liu et al 2013 jang et al 2017 this approach can be inadequate and in many cases misleading due to lack of accounting for different sources of modeling uncertainties ajami et al 2007 arabi et al 2007 tasdighi 2017 propagation of uncertainties from different sources into model predictions during any modeling practice may result in biased and unrealistic decisions models are mere representation of reality which makes them subject to uncertainty the uncertainty in any modeling practice stems from different sources including model parameters input data climate land use etc model structure conceptualization and measurement data used for training testing the model streamflow nutrient concentrations or loads etc there are many studies that investigate the effects of modeling uncertainties in simulating streamflow or various water quality constituents beven and binley 1992 vrugt et al 2003 ajami et al 2007 rojas et al 2008 harmel et al 2014 yen et al 2014 ajami et al 2007 developed a bayesian framework for estimating various sources of uncertainty in simulating streamflow with hydrologic models yen et al 2014 also developed a similar framework for propagation of uncertainty from various sources to simulated streamflow and nitrate while these studies have investigated the effects of modeling uncertainties on streamflow and water quality simulations the impacts of modeling uncertainty when quantifying the effectiveness of bmps has not been addressed sufficiently arabi et al 2007 specifically a framework that incorporates various sources of modeling uncertainty and determines uncertainty bounds around bmp effectiveness has not been developed to the best of our knowledge an important source of uncertainty when assessing the effectiveness of bmps using a model is the measured data used for training and testing the model in studies where only water quantity is explored the uncertainty from streamflow measurements is often the focus while the uncertainties from measured constituent concentration are also relevant in water quality studies harmel et al 2009 water quality constituent loads are often derived by multiplying streamflow volume and corresponding constituent concentration thus uncertainties in streamflow and concentration measurement contribute to uncertainty in load estimates assessing the effectiveness of nps bmps often entails a two stage approach first a model is developed for quantifying the nps pollution second by changing selected model parameters to mimic the operation of bmps effectiveness of bmps is quantified this two stage approach makes the uncertainty analysis of bmps cumbersome and time consuming within an uncertainty assessment framework each model parameter has a probability distribution derived during model training instead of one optimal set of parameters deterministic approach and some of these parameters and or other additional parameters are changed later to reflect the operation of bmps hence adopting a probabilistic approach for changing these bmp parameters can quickly result in a large number of iterations in this regard a technique that can be used to efficiently conduct the bmp uncertainty analysis without compromising the statistical inferences gained during any step of the analysis is essential the overall goal of this study was to develop a probabilistic approach to assess the effectiveness of nps conservation practices in reducing nutrient loads the specific objectives were to i assess the effects of uncertainty in measured training testing data on simulating nutrient loads and how it can impact the inferences about effectiveness of bmps and ii determine the total uncertainty bounds 95 confidence interval of model simulations around the model estimates of bmps effectiveness in reducing nutrient loads while previous studies have determined the effectiveness of nps bmps through monitoring campaigns or deterministic modeling approaches using a total uncertainty assessment framework i e accounting for various sources of uncertainties including model parameters input structure and observation data to probabilistically assess the effectiveness of bmps is novel specifically incorporation of measurement data uncertainty which plays an important role in realistic simulation of nps nutrient loads can enhance assessing the water quality benefits of bmps using watershed models application of the framework developed in this study can produce useful information for decision making in water quality improvement through application of nps conservation practices 2 material and methods three separate swat models were developed the models were identical except for the rainfall runoff mechanism accounting for different model structures and the precipitation time step accommodating the mechanism the analysis period was 2008 2015 of which 2008 to 2011 was the pre bmp and 2012 to 2015 represented the post bmp period a 2 yr warmup period was used when running the models a two stage bayesian total uncertainty assessment framework was developed and applied to assess the effectiveness of the bmps nutrient management and cattle exclusion fencing described further in 2 7 in reducing daily total nitrogen tn loads at the outlet of a 54 ha agricultural watershed in north carolina in the first stage modeling uncertainties were characterized during the pre bmp period 2008 2011 a markov chain monte carlo mcmc sampling scheme along with a statistically correct likelihood function which accounts for various statistical conditions including normality lack of autocorrelation and heteroscedasticity of error residuals stedinger et al 2008 vrugt 2016 set to consider the tn load errors at the outlet of the watershed was used to derive the posterior distribution of model parameters while incorporating various sources of modeling uncertainties in the second stage the effectiveness of bmps was assessed by comparing the simulated tn loads before and after implementing bmps during the post bmp period 2012 2015 the size of the posterior distribution of model parameters was reduced while maintaining their statistical characteristics using a uniform random sampling algorithm combined with a linear interpolation technique the models were then run using each set of reduced model parameters to generate prediction intervals of tn loads before implementing bmps then for each set of model parameters bmp parameters were sampled using a uniform random sampling algorithm the models were run using each combination of model and bmp parameters generating tn load after implementing bmps using the post bmp field observation data the value of the likelihood function was also computed for each parameter set the posterior distributions of bmp parameters were then derived and tn load prediction intervals after implementing bmps were determined finally the tn load reduction prediction intervals were quantified to determine bands of uncertainty around effectiveness of bmps 2 1 study watershed the study watershed was a 54 ha pasture dominated watershed located in central north carolina fig 1 the land use composition within the watershed is 78 pasture 14 forest 6 developed and 2 cultivated crops national land cover database nlcd 2011 the watershed was selected due to availability of comprehensive monitoring data streamflow nutrients and sediments from a published paired watershed study line et al 2016 more importantly the monitoring dataset provides a record of event based measurements for 4 yr before installing bmps and another 4 yr after bmps installation the main stream in the watershed known as the mud lick creek flows much of the year except some periods during late summer and early fall where it dries up in general the streams in this region are known for low baseflow line et al 2016 2 2 watershed model swat is a continuous time distributed parameter process based watershed model which has been used extensively for hydrologic and water quality assessments under varying climatic land use and management conditions in small watersheds to large river basins gassman et al 2007 arnold et al 2012 card staff 2016 the model has the capability to run on daily or smaller time steps in swat the watershed is split into smaller subwatersheds which are further discretized into hydrologic response units hrus hrus are the smallest spatial units in swat and are defined as areas within each subwatershed with unique combinations of land use soil and slope class climate inputs drive hydrologic responses and provide moisture and energy inputs in swat hydrologic processes simulated in the model include canopy storage surface runoff infiltration evapotranspiration lateral flow tile drainage redistribution of water within the soil profile return flow and recharge arnold et al 2012 surface runoff is simulated using either the modified green and ampt method g a green and ampt 1911 with subdaily rainfall or curve number method cn usda nrcs 2004 with daily rainfall 2 3 model inputs terrain soils land use climate and hydrography the elevation data for building the swat model was the 1 3 arc second 10 m resolution digital elevation model dem obtained from united states geological survey the national map usgs tnm 2016 the soil survey geographic ssurgo database from united states department of agriculture natural resources conservation services usda nrcs 2016 was used to represent soil characteristics and variability in the watershed the national agricultural statistics service nass land use data for year 2011 were obtained from the usda usda cropscape 2016 the resolution of the ssurgo and nass data was 1 arc second 30 m stream flowlines were obtained from usgs national hydrography dataset usgs nhd 2016 the stream flowlines were used for more accurate stream delineation in swat by superimposing the nhd flowlines on the dem in the process of watershed and stream delineation the hydrographic segmentation subwatershed boundary and stream delineation is improved especially in smaller scales such as the watershed in this study or locations where the dem does not provide enough accuracy winchell et al 2007 observed climate data for the closest meteorological station burlington alamance regional airport ghcnd usw00093783 were obtained from the national climatic data center ncdc quality controlled local climatological data qclcd database noaa 2016 the station was located about 20 km from the watershed daily and hourly precipitation minimum and maximum temperature were collected for 2005 to 2015 2 4 measurements stream discharge and nutrient concentrations the monitoring station was located at the outlet of the watershed fig 1 streamflow and nutrient measurements were available from 2008 to 2015 this period includes pre bmp 1 jan 2008 to 5 oct 2011 and post bmp 6 oct 2011 to 31 dec 2015 periods stream discharge and nutrient concentrations were sampled during storm events the discharge measurement and nutrient sampling procedures collecting storing and analyzing are explained in details in line et al 2016 mean daily nutrient loads were computed by multiplying the mean daily storm discharge and corresponding nutrient or sediment concentration 2 5 cattle grazing and manure deposition there were 75 beef cows grazing on various parts of the pasture within the watershed based on the observations and communication with land owners line et al 2016 reported a density of 1 2 cows per ha which was used in this study for computing grazing and manure deposition rates manure deposition rate for beef cows was set at 29 5 kg day 500 kg animal usda nrcs 2016 assuming an average weight of a beef cow equal to 600 kg the average manure deposition rate was computed as 35 4 kg ha day the cattle deposit their manure either on land or in streams on average cattle spend 7 of their time in the streams byers et al 2005 therefore it was assumed that 2 5 kg ha day of manure was directly deposited into streams while the remaining fraction 32 9 kg ha day was deposited on the land each beef cow consumes on average 45 kg ha day of grass usda nrcs 2016 which along with the 1 2 cows ha density gives a grazing rate of about 54 kg ha day 2 6 the swat model setup arcswat 2012 usda ars 2014 was used to develop three swat models the models were completely identical except for the runoff estimation method and precipitation time step a model was developed with daily precipitation and cn method based on i soil moisture cn i and ii plant evapotranspiration cn ii and the other model was developed using the hourly precipitation and g a method for runoff simulation therefore three model setups were prepared for the analyses corresponding to three different model structures the dem was used along with the nhd flowlines to delineate the watershed and streams the watershed was divided into 4 subwatersheds the hru definition was based on land use and soil data since the topographic variability was small a single class slope was assumed within each subwatershed using these settings 29 hrus were defined for the watershed fig 2 2 7 representing bmps in the swat model the bmps implemented in the watershed were nutrient management and cattle exclusion fencing while nutrient management can be readily modeled by changing the rate of fertilizer application arabi et al 2008 ahmadi et al 2014 incorporating cattle exclusion fencing in the model is more complicated few studies have discussed methods for representing cattle exclusion fencing in swat one such method is application of point sources to represent the presence of cattle in streams lin et al 2009 this approach provides the capability to directly change the rate of nutrients or sediments introduced into the streams during the implementation of the exclusion fencing mimicing the operation of the bmp limiting the access of the cattle to streams during the pre bmp period 336 kg ha of 15 15 15 n p k fertilizer was applied uniformly to the pasture in subwatersheds 2 3 and 4 biosolids were applied to the pasture in subwatershed 1 analysis of biosolids showed that 130 and 116 kg ha of n and p were applied during each application the 15 15 15 fertilizer is a standard fertilizer in swat databases and hence was readily incorporated in the model however for biosolids their elemental n and p content were used to incorporate them in the model nutrient management was implemented by replacing the biosolids and fertilizer with a granular n only fertilizer 21 0 0 at a rate of 70 kg ha during the post bmp period this bmp was directly implemented in the swat model by replacing and changing the fertilizer application rate the cattle exclusion fencing was installed in october 2011 the fences were installed along approximately 520 m of the mainstream of mud lick creek in the watershed about half of the upper part of the mainstream was not fenced so cattle were still able to have access to streams in the upper half part of the watershed we modeled the cattle exclusion fencing by changing the rate of organic n introducing into the streams via point sources in subbasins 3 and 4 where stream fencing was installed 2 8 the bmp uncertainty assessment framework a probabilistic framework was developed for assessing the effectiveness of nps conservation practices the bayesian based approach explicitly accounts for uncertainties from model parameterization climate input data i e precipitation model structure cn i cn ii or g a and measured data for model training testing i e streamflow and nutrient loads the framework was developed in matlab the mathworks inc the framework was developed building on the work by ajami et al 2007 and yen et al 2014 both of these studies have developed methods for estimating modeling uncertainties from various sources in simulating streamflow and or water quality however neither addresses estimating the uncertainties in model predicted bmp effectiveness in order to quantify the effectiveness of bmps the model should be run first before implementing the bmps in the model the bmps are then represented in the model through manipulation of some parameters the model is run again with the altered parameters to simulate the responses after implementing the bmps the difference between the simulated responses e g streamflow nutrient loads before and after implementation of the bmps quantifies their effectiveness the framework entails a two stage procedure first the modeling uncertainties in simulating nutrient loads are characterized during the pre bmp period 2008 2011 during this stage a markov chain monte carlo mcmc sampling scheme the dream method vrugt et al 2009 along with a statistically correct likelihood function sorooshian and dracup 1980 stedinger et al 2008 vrugt 2016 which accounts for various statistical conditions including normality lack of autocorrelation and heteroscedasticity of error residuals is used to sample the parameter space and derive the posterior distributions input data uncertainty is incorporated by using precipitation multipliers drawn from a gaussian distribution with an uncertain mean and standard deviation sampled along with model parameters during the mcmc procedure the measured data uncertainty is also incorporated by applying correction factors on model residuals r e s i d u a l o b s e r v e d s i m u l a t e d computed based on a probable error range for each measurement harmel and smith 2007 finally bayesian model averaging bma is used to account for model structural uncertainty hoeting et al 1999 having characterized different sources of modeling uncertainties in the first stage the model can be used for simulation of nutrient loads during the post bmp period 2012 2015 without incorporating the bmps in models the outputs provide prediction intervals for nutrient loads during the post bmp period assuming no bmps were implemented the second stage entails iterating on bmp parameters for each model parameter set from stage 1 to characterize the performance of the bmps under various sources of modeling uncertainty during the post bmp period 2012 2015 the first stage often results in too many parameter sets due to large number of iterations required for the mcmc algorithm to converge especially in highly parametrized complex models such as swat a method was proposed to reduce the number of model parameter sets without compromising the statistical characteristics of the inferred posterior distributions the method involves a uniform random sampling scheme along with a linear interpolation algorithm during which model parameter sets are drawn randomly from the posterior distributions while maintaining statistical characteristics of the distributions the bmp parameters were then sampled for each set of the reduced model parameter sets using a uniform random sampling scheme the model is then run using each new joint parameter set model parameters and bmp parameters and the posterior distribution of bmp parameters are derived with the new likelihood function during the post bmp period the outputs from this stage provide prediction intervals for simulated nutrient loads during the post bmp period after bmps are implemented which along with nutrient loads from stage 1 are used to compute prediction intervals for nutrient load reductions i e bmps effectiveness availability of monitoring data during the post bmp conditions provided a unique opportunity to compute values of a new likelihood function in contrast to previous studies where the same likelihood function was used during both pre bmp and post bmp period due to lack of measurements during post bmp conditions arabi et al 2007 2 8 1 model and bmp parameters uncertainty the generic equation for quantifying the effectiveness of a specific bmp using simulations from a watershed model is 1 η b m p q ˆ p r e b m p q ˆ p o s t b m p where η b m p denotes the efficiency or effectiveness of the bmp q ˆ p r e b m p and q ˆ p o s t b m p are the simulated response variables before and after application of the bmp respectively the parameter uncertainty in quantifying the simulated response variables in eq 1 is due to i model parameters θ m and ii bmp parameters θ b m p it should be noted that the uncertainty from model parameters propagates to predictions during both pre bmp and post bmp periods however the bmp parameter uncertainty is only manifested in the post bmp simulations the simulated response variables are also subject to other errors stemming from measured climate inputs r insufficiency of the model conceptualization m model structure and measurement data used for training and testing the model q p r e b m p and q p o s t b m p the simulation error residuals e then take the form 2 e r θ m q q q ˆ q m r θ during the post bmp conditions the error residuals are a function of joint probability density of model and bmp parameters e f θ m θ b m p with expected value of 3 e e f θ m f θ b m p d θ b m p d θ m where e e denotes the expected value of the error residual as a function of both model and bmp parameters eq 3 is conditioned on the independence of model and bmp parameters analytical solution to this equation is often infeasible in case of complex models hence we resort to mcmc methods applying the bayes theorem the parameter set θ is assigned posterior probability distribution p θ q which is proportional to the product of the parameter prior probability distribution p θ and a likelihood function l θ q the likelihood function assuming normally and independently distributed model residuals e with mean zero and variable standard deviation at each observation time step σ t can be expressed as vrugt 2016 4 l θ q σ t 1 n 1 2 π σ t 2 e x p 1 2 σ t 2 q t q ˆ t θ 2 where n is the number of time steps often it is easier to maximize the logarithm of the likelihood function due to numerical stability and algebraic simplicity ajami et al 2007 vrugt 2016 hence the natural log of the likelihood function was adopted for optimization streamflow and nutrient error residuals are often not independently distributed and in most cases temporal autocorrelation exists in the residuals evin et al 2014 a first order auto regressive scheme ar 1 was employed to reduce autocorrelation in error residuals applying the natural log and ar 1 transformation the log likelihood function takes the form 5 l θ q σ ρ n 2 ln 2 π 1 2 ln σ ν 2 1 ρ 2 1 2 σ ν 2 1 ρ 2 e 1 2 t 2 n e t ρ e t 1 2 parameters ρ and σ are determined along with model parameters at each model realization during the mcmc sampling algorithm the model parameters for the uncertainty analysis were selected based on experience and sensitivity analysis performed previously arabi et al 2007 arnold et al 2012 tasdighi et al 2017 table 1 lists the model parameters selected for uncertainty analysis along with their ranges the ranges for parameters were also selected based on the swat user manual and experience from previous study tasdighi et al 2017 arnold et al 2012 in this study uniform noninformative prior distributions were assumed for parameters within predefined ranges the same assumption has been used in many other hydrological modeling studies since prior knowledge of the model parameters is often not available and is case specific ajami et al 2007 the bmp parameters for uncertainty analysis were selected based on the swat capabilities in representing bmps and studies where similar bmps were implemented in swat arabi et al 2008 lin et al 2009 ahmadi et al 2014 table 2 lists the parameters selected for uncertainty analysis of bmps similar to model parameters uniform noninformative prior distributions were assumed for bmp parameters as well the rate of fertilizer application was selected to represent nutrient management practices the rate of organic n introducing into streams via point sources in subbasins 3 and 4 where cattle exclusion fencing was implemented were selected as parameters for simulating exclusion fencing in swat 2 8 2 model input uncertainty a method proposed by ajami et al 2007 was implemented to account for precipitation uncertainty through application of multipliers on precipitation events using this method instead of iterating on each single multiplier the iteration is performed on the mean and standard deviation of a random gaussian distribution from which the multipliers are randomly drawn at each time step 6 p ˆ t φ t p t φ t n μ φ σ φ 2 where p ˆ t and p t are the corrected and observed precipitation depths respectively φ t is the random multiplier drawn from a normal distribution with a random mean μ φ μ φ 0 9 1 1 and variance σ φ 2 σ φ 2 1 e 5 1 e 3 ajami et al 2007 incorporating precipitation multipliers using this approach instead of iterating on each precipitation multiplier reduces the dimensionality issue and improves the identifiability 2 8 3 model structural uncertainty bayesian model averaging bma was used to account for model structural uncertainty hoeting et al 1999 georgakakos et al 2004 the bma is a probabilistic algorithm for combining competing models based on their predictive skills ajami et al 2007 madadgar and moradkhani 2014 in this study the three rainfall runoff model structures in swat m 1 c n i m 2 c n i i m 3 g a were used to explore the effects of model structural uncertainty using the bma the three model structures were combined using probabilistic weights to reduce the model structural uncertainty the posterior distribution of the bma prediction q ˆ b m a is 7 p q ˆ b m a m 1 m 2 m 3 q i 1 3 p m i q p i q ˆ i m i q where p m i q is the posterior probability of the model m i this term can be assumed as a probabilistic weight w i for model m i in the bma prediction q ˆ b m a the constraint for bma weights is i 1 3 w i 1 higher values of w i can be interpreted as higher predictive skill for a given model structure the model weights can be determined using different optimization techniques the expectation maximization em algorithm dempster et al 1977 was used in this study to estimate model weights 2 8 4 incorporating the uncertainty in measured data for model training testing the uncertainty inherent in measured data used for training and testing of models often stems from errors in monitoring design instrumentation data processing storage and operator human errors harmel et al 2009 this type of uncertainty is rarely accounted for in evaluation of model performance harmel and smith 2007 yen et al 2014 the measured data uncertainty is often manifested in heteroscedasticity of error residuals variable error variance some observations may be less reliable than others which results in their errors to have different variances different approaches have been proposed to circumvent this issue including application of different transformations such as natural log or box cox to stabilize the error variances sorooshian and dracup 1980 others have proposed alternative forms for the likelihood function schoups and vrugt 2010 which accounts for error heteroscedasticity by assuming a variable variance for each model residual the main challenge in this approach is determination of the variance for each residual as it requires having repeats of each measurement which most often are not available as a result in such studies variance was subject to inference along with model parameters at each model realization during the sampling procedure schoups and vrugt 2010 vrugt 2016 this approach can lead to dimensionality issues in case of highly parameterized models or when a relatively long record of measurements is used for model training in this study we employed a method based on the study by harmel and smith 2007 in this method each error residual is modified using a correction factor computed based on the properties of the probability distribution of each measured value previous experience and expert s opinion can be used to make informed assumptions about probability distribution for each measured value in the record assuming a normal distribution for each measured value q i mean and median of the distribution is represented by q i the variance can then be computed based on a probable error range p e r which can be assumed based on literature or professional judgment p e r can be constant or variable for all measurements depending on the experience and level of knowledge about the monitoring design once the p e r s are determined the variance σ i 2 for each record of measurement can be computed as 8 σ i 2 p e r i q i 3 9 100 2 eq 8 is useful when the measured values are not transformed since we used a natural log transformation on streamflow and nutrient loads the taylor series expansion for the second moment of a function of random variable f x was used to estimate the variance v a r f x as 9 v a r f x f e x 2 v a r x f μ x 2 σ x 2 applying 9 on the natural log transform function gives 10 v a r ln x σ x x 2 11 s d ln x σ x x c o v p e r where s d stands for the standard deviation and c o v denotes the coefficient of variation using this approach σ for log transformed observations can be estimated with p e r it should be noted that this assumption holds for smaller p e r s 0 3 and higher approximation errors are introduced in higher values of p e r we used expert s opinion to determine p e r s for each streamflow or nutrient load measurement q i the correction factors were then computed as the area under the standard normal distribution 12a c f i f x i μ i σ i 1 σ i 2 π x e x μ 2 2 σ i 2 d x 0 5 if μ x 12b c f i f x i μ i σ i 1 σ i 2 π x e x μ 2 2 σ i 2 d x 0 5 if μ x where f denotes the normal cumulative distribution function x denotes the simulated data q ˆ i and μ represents the observed data q i and σ is determined using eq 11 using the correction factors the residuals are adjusted based on the estimated measured data uncertainty it should be noted that this method requires some level of knowledge about the characteristics of the measurement errors in this case this information was obtained using the description of measurement techniques in the study watershed by line et al 2016 these characteristics are often case specific experience form previous studies can be used to enhance the assumptions and generate better estimates 2 8 5 the dream algorithm for mcmc analyses several bayesian algorithms are available which have been widely used for uncertainty assessment in hydrologic modeling including the generalized likelihood uncertainty estimation glue beven and binley 1992 the shuffled complex evolution metropolis scem ua vrugt et al 2003 and the differential evolution adaptive metropolis dream vrugt et al 2009 dream is a multi chain mcmc method that randomly samples the parameter space and automatically tunes the scale and orientation of the sampling distribution to move toward the target distribution by maximizing the value of the likelihood function the method has been used extensively for parameter estimation of complex environmental models vrugt 2016 the convergence of the algorithm can be monitored using the procedure proposed by gelman and rubin 1992 in this procedure a scale reduction score r is monitored to check whether each parameter has reached a stationary distribution gelman and rubin 1992 the common convergence criterion of r 1 2 was used in this study as well dream is specifically beneficial in the optimization of complex high dimensional problems in this study dream was employed to sample the parameter space and derive the posterior distributions 2 9 evaluation of model performance the performance of the models in simulating daily tn loads under the total uncertainty assessment framework was evaluated using different error statistics computed at each model realization during the pre bmp period 2008 2011 the uncertainty analysis was performed using the values of the likelihood function eq 5 as the objective function computed with daily tn loads at the monitoring station the error statistics during the post bmp 2012 2015 period were calculated using the joint distribution of reduced model parameters and bmp parameters 3 results and discussion 3 1 evaluation of models during the pre and post bmp periods table 3 summarizes the error statistics for models during the pre and post bmp conditions compared to models with cn the model with g a method had a poor performance in terms of various error statistics during the pre bmp period and it was excluded from further analysis for that reason similar results were reported on better performance of the cn method over the g a in other agricultural watersheds kannan et al 2007 cheng et al 2016 in contrast ficklin and zhang 2013 concluded that models with g a are more likely to generate better daily simulations in agricultural watersheds it should be noted that these studies have used a deterministic approach tasdighi et al 2018 compared the performance of the cn and g a methods based on upstream land use conditions using a probabilistic approach they concluded that the g a method had a better performance in highly developed subwatersheds while the cn method had a slightly better performance in agricultural watersheds between models with cn i and cn ii methods cn ii models had a slightly better performance during the pre bmp period during the post bmp period the superiority of the cn ii model was more accentuated generating better error statistics 3 2 characterizing the modeling uncertainties during the pre bmp period stage 1 3 2 1 model parameters uncertainty the posterior cumulative distribution functions cdf of parameters under the cn i and cn ii model structures are illustrated in fig 3 the distributions are derived after the pre bmp uncertainty analysis 2008 2011 note that only the most sensitive parameters are included in this figure as observed in fig 3 models with cn i and cn ii resulted in different distributions for the same parameters while the model structure determined the level of skewness for most parameters for some parameters esco solz cdn the skewness changed from positive to negative under different model structures in general cn i model showed higher sensitivity to parameters pertaining to soil characteristics sol alb sol awc and sol z which conforms to intuition as the cn i method uses the soil water content for determining the curve number high skewness from normality indicates deficiency in identifiability which is often present in the uncertainty analysis of highly parametrized complex models ajami et al 2007 the distributions were derived using 20000 samples after the convergence of the dream algorithm the uniform random sampling algorithm along with the linear interpolation technique discussed in section 2 8 was then used to reduce the number of model parameter sets while maintaining the statistical characteristics of the distributions to be used for post bmp analysis the algorithm was set to draw 1000 parameter sets from the posterior distributions the reduced parameter sets act as priors for the post bmp period in other words the prior distributions of model parameters for the post bmp period are inferred from the posterior distribution of parameters from the pre bmp period for each of the new model parameter sets the uniform random sampling technique discussed in section 2 8 was used to generate bmp parameter sets since only 3 parameters were used for bmps 30 random bmp parameter samples were assumed to adequately represent the bmp parameter space the bmp parameter sets combined with the model parameter sets resulted in 30000 joint parameter sets for the second stage of the analysis post bmp 3 2 2 model input uncertainty the posterior cdfs of mean and standard deviation for normal distributions from which precipitation multipliers were drawn were almost similar and close to uniform the uniformity of the distributions indicates that precipitation multipliers did not have a major impact on tn load simulations from each model the similarity of the distributions on the other hand could be predicted as the daily precipitation data for models with cn i and cn ii were identical hence any difference in distributions resulted from the model structural difference another explanation in this regard is the large number of sensitive model parameters which can diminish the impact of precipitation multipliers in the range assumed using a wider range for mean and standard deviation of the normal distributions from which precipitation multipliers were drawn could result in higher impact from multipliers and probably better performance of the input uncertainty estimation routine 3 2 3 model structural uncertainty bayesian model averaging was used at each model realization to probabilistically combine the models and reduce the model structural uncertainty fig 4 shows the boxplots of bma weights generated during the mcmc procedure based on bma weights cn ii had a better performance in simulating daily tn loads a two sample t test was performed to test the significance of the difference between the distribution of the bma weights from the cn i and cn ii models the results showed that at the 0 05 significance level bma weights for cn ii model were significantly higher than weights for cn i model these results are congruent to the results in section 3 1 where cn ii model showed superior performance in terms of various error statistics one explanation for these results is that in cn ii method the curve number is determined based on plant evapotranspiration and since the study watershed is a pasture dominated agricultural watershed the cn ii has a better performance similar results were obtained in other studies tasdighi et al 2018 yen et al 2014 3 2 4 measured training testing data uncertainty measured data uncertainty was incorporated using correction factors the correction factors were applied on the daily tn load residuals during the computation of the likelihood function at each model realization fig 5 illustrates the probability distribution pdf of correction factors for each model during the pre bmp period the correction factors were categorized based on flow conditions low medium and high flows before generating the pdfs to assess the effects of flow regime on measurement errors the highest values of correction factors were observed during high flow events medium and low flow events resulted in relatively lower values for correction factors this is an important finding as it indicates higher errors during high flow events compared to medium and low flows the phenomenon of higher errors during high flow events is often described as the heteroscedasticity of error residuals which is deemed to be attenuated by applying correction factors other studies that have investigated the uncertainties in measurement data have reported similar behavior sorooshian and dracup 1980 harmel and smith 2007 the results also conform to intuition as monitoring during high flow events often entails higher errors due to difficulties in measurements harmel et al 2006 3 3 assessing the effectiveness of bmps under various sources of modeling uncertainty stage 2 3 3 1 bmp parameters uncertainty the posterior distribution of bmp parameters are illustrated in fig 6 interestingly the parameter pertaining to nutrient management bmp frt kg showed higher sensitivity when quantifying the tn loads this is while the posterior distributions of parameters pertaining to the cattle exclusion fencing were close to uniform which indicates lower sensitivity this outcome also indicates that the nutrient management had a higher impact on nutrient load reductions a possible explanation for this can be the higher uncertainty in the nature of representing cattle exclusion fencing in the swat model several assumptions were used when representing the exclusion fencing such as changing the nutrient introducing into the streams via point sources and the rate adjustments for nutrients introducing into streams a more rigorous approach for representing cattle exclusion fencing may enhance the performance of this bmp and result in more meaningful inferences all bmp parameter posterior distributions showed high deviations from normality assuming wider prior distributions and larger number of iterations on bmp parameters may be effective in reducing these effects 3 3 2 estimating tn load prediction intervals before and after implementation of bmps the cumulative exceedance probability curves for daily tn loads were developed these curves along with bands of uncertainty before and after implementing bmps provide an easily readable informative measure for assessing the effectiveness of bmps in reducing tn loads under uncertainty fig 7 illustrates the 95 confidence interval for cumulative exceedance probability curves for tn loads comparing the prediction intervals before and after implementation of bmps it is evident that the combination of bmps nutrient management and cattle exclusion fencing was successful in reducing the tn loads in the watershed compared to cn i the model with cn ii had a better performance in capturing observed tn loads at all ranges especially the high and medium loads however they showed wider bands of uncertainty the bma had a close performance to the cn ii model which was expected as the cn ii method better simulated the tn loads resulting in higher bma weights while the cumulative exceedance probability curves provide valuable insights into the statistical characteristic of bmps effectiveness they can be misleading too as the serial structure and autocorrelation of the sequence of the simulated and observed records are removed in them vogel and fennessey 1994 in this regard the fraction of observations lying within the prediction intervals should be determined using the time series of simulations and observations for this reason the coverage percent of observations lying inside the 95 confidence interval of simulation ensembles and spread average width of the 95 confidence interval uncertainty band of simulations were determined based on the time series of simulations and observations of tn loads table 4 summarizes the coverage and spread for the models during the pre and post bmp conditions 3 3 3 quantifying the effectiveness of bmps in terms of tn load reductions the effectiveness of bmps in reducing the tn loads was computed by subtracting the tn loads from models before and after implementation of bmps during the common post bmp period 2012 2015 fig 8 depicts the cumulative exceedance probability curves for tn load reductions under different models the highest reductions were observed for higher loads in general the results demonstrate high level of uncertainty in simulating the daily tn load reductions for example for cn ii model in fig 8 at exceedance probability of 25 the tn load reduction from bmps can be any number between 0 1 and 1 kg ha this outcome indicates the importance of accounting for various sources of uncertainty in modeling the performance of the bmps as they directly affect the decision making process in general modeling pollution loads from npss is subject to high levels of uncertainty however most often this uncertainty is ignored and models are used deterministically to compute pollution loads which can result in unrealistic and biased decisions the average annual tn load reduction from the ensemble of cn i cn ii models and bma were 1 5 1 7 and 1 8 kg ha respectively in terms of percentage the average annual reductions were 58 65 and 69 respectively fig 9 shows the pdfs of average annual load reductions for each model these values conform to the reductions determined by the paired watershed study on this watershed conducted by line et al 2016 they found statistically significant reductions in total kjeldahl n 34 and ammonia n 54 while changes in nitrate n loads were not significant the tn load reductions were resulting from the performance of the nutrient management and cattle exclusion fencing combined while it was not feasible to decompose the total tn load reductions between each bmp to determine the performance of each bmp the nutrient management was determined to be more effective as the related bmp parameters showed higher sensitivities when quantifying tn loads 4 conclusions a total uncertainty estimation framework was developed for assessing the water quality benefits of management practices using watershed models the two stage framework first characterizes various sources of modeling uncertainties during the pre bmp period the second stage of the framework uses the inferences on modeling uncertainties from the first stage and quantifies the effectiveness of bmps using a probabilistic approach in general the modeling uncertainties were large resulting in wide bands of uncertainty around both tn loads and load reductions the framework however was successful in capturing the effects of different sources of modeling uncertainties on simulations and propagating them to the bmp effectiveness assessment stage between the three model structures cn i cn ii and g a cn ii showed the best performance in terms of various performance measures including error statistics and bma weights this was attributed to the intensive agricultural land use in the watershed the g a method had an unsatisfactory performance in simulating the tn loads application of the bma slightly enhanced the quality of simulating tn load reductions under uncertainty as it resulted in higher coverage 51 and 35 during the pre and post bmp periods respectively the distribution of the correction factors for measurement uncertainty indicated higher uncertainty for high flow events correctly capturing the heteroscedasticity of error residuals for daily tn load simulations between the two bmps nutrient management had the highest impact on the tn load reductions the parameters pertaining to cattle exclusion fencing did not show much sensitivity when quantifying daily tn loads a possible explanation for this is the higher uncertainty in the nature of representing cattle exclusion fencing in swat several assumptions were used when representing the exclusion fencing such as changing the nutrient introducing into the streams via point sources and the rate adjustments for nutrients introducing streams it should be noted that the intent of this study was to develop a framework for probabilistic assessment of bmp effectiveness in reducing pollutants in streams and not to develop a method to represent specific bmps in models the results of this study have important implications for decision making when models are used for water quality simulation while numerous uncertainty analysis frameworks have been developed to explore modeling uncertainties in quantification of streamflow and water quality components the lack of pragmatic applications of such methods to tackle decision making challenges is a major shortcoming the framework presented in this study is deemed a pioneer attempt to fill this gap in the literature and add to the pragmatic aspects of the uncertainty analysis in hydrologic and water quality simulations software availability all the codes are developed within matlab and can be made available upon request from the first author acknowledgements this publication was made possible by u s epa grant rd835570 its contents are solely the responsibility of the grantee and do not necessarily represent the official views of the u s epa further u s epa does not endorse the purchase of any commercial products or services mentioned in the publication usda is an equal opportunity employer and provider 
26356,we developed a dispersion model rapidair to estimate air pollution concentrations at fine spatial resolution over large geographical areas with fast run times concentrations were modelled at 5 m spatial resolution over an area of 3500 km2 in 10 min rapidair was evaluated by estimating nox and no2 concentrations at 86 continuous monitoring sites in london uk during 2008 the model predictions explained 66 of the spatial variation r 0 81 in annual nox concentrations observed at the monitoring sites we included discrete canyon models or geospatial surrogates sky view factor hill shading and wind effect to improve the accuracy of model predictions at kerbside locations geospatial surrogates provide alternatives to discrete street canyon models where it is impractical to run canyon models for thousands of streets within a large city dispersion model with advantages including ease of operation faster run times and more complete treatment of building effects graphical abstract image 1 keywords dispersion modelling air pollution gis nox no2 street canyon software availability name of software rapidair developer ricardo energy and environment hardware information general purpose computer 4 16 gb ram intel r core tm i5 processor 64 bit operating system programming language python 2 6 and r availability contact the developers 1 introduction the estimation of population exposures to air pollution is increasingly important as numerous studies highlight the detrimental effects of air pollution on human health world health organization 2013 2016 the use of air pollution monitors allows direct measurement of ambient concentrations and the on going development of portable real time monitors is providing improvements in temporally resolved concentration estimates dons et al 2012 spinelle et al 2017 2015 however monitoring only provides concentration estimates at specific locations whereas it has been observed that pollution concentrations can vary substantially over small areas gillespie et al 2017 lin et al 2016 models can overcome some of the limitations associated with monitoring as concentrations can be estimated at multiple locations within a study area however inherent uncertainties within models require to be quantified by comparison of predictions against air pollution measurements two main types of models are commonly used to estimate urban air pollution land use regression lur models and dispersion models we do not include discussion of computational fluid dynamics cfd models in this paper as cfd models have not been used widely in operational predictions of spatial patterns of urban air pollution due to excessive computational constraints when operating over large geographical areas land use regression lur models use geographical information systems gis to quantify relationships between measured pollutant concentrations and land use variables including traffic and population which can then be extrapolated to estimate human exposure to air pollution at fine spatial resolution briggs et al 1997 lur models have been widely applied in in cohort epidemiological studies gillespie et al 2016 johnson et al 2013 wang et al 2013 and in personal monitoring studies dons et al 2014a 2014b lur models are frequently used to estimate longer term e g annual pollution exposure and often do not take into account the effects of meteorology additionally the transfer of lur models between study areas has been shown have substantial limitations including differences in monitoring location type which can lead to model bias gillespie et al 2016 mukerjee et al 2012 patton et al 2015 many regulatory organisations are interested in source apportionment to inform policy on air pollution controls which requires preparation of spatially accurate multi source air quality emissions however lur models seldom use direct quantitative estimates of emissions from sources instead more commonly they assess the effects of receptor proximity to sources and consequently lur models have had limited application in air quality management policy development dispersion models simulate atmospheric transport and transformation of air pollutants emitted from sources to allow estimation of concentrations at receptors the most commonly used models are based on gaussian plume concepts dispersion models can be used to estimate short term e g hourly variations in pollution concentrations gibson et al 2013 and to estimate population exposures in cohort studies bellander et al 2001 nyberg et al 2000 additionally projected emissions estimates if available can be used to estimate future concentrations commercially available software packages have been developed to simplify user inputs and modelling procedures however this has often resulted in high license costs gulliver and briggs 2011 particularly when it is necessary to apply models over large geographical areas furthermore gaussian dispersion model run times for large urban area can quickly become prohibitive due the computational demands of calculating concentrations at what can extend to millions of discrete locations this may necessitate the use of gis interpolation routines to increase the spatial resolution of the model estimates which may introduce other errors into estimated exposures wong et al 2004 some studies have addressed these challenges to achieve fine spatial and temporal resolution by combining dispersion and lur models korek et al 2016 michanowicz et al 2016 wilton et al 2010 and or including meteorological information within lur models su et al 2008a tan et al 2016 a hybrid gis dispersion model stems air has been developed to enable fine spatial and temporal resolution while minimising run times with readily available computer software gulliver and briggs 2011 the stems air model estimates pollution concentrations from emission sources in 45 upwind wedge shaped gis buffer areas scaled by the distance between sources and receptors in built up urban areas air pollution can become trapped in street canyons surrounded by tall buildings especially if the wind is blowing from a direction perpendicular to the street leading to recirculation of pollutants within the canyon as a result pollution concentrations in street canyons can become elevated and may be underestimated by standard air pollution models including lur or gaussian plume models exposure estimates may be improved by combining additional models that take into account urban topography in such locations with background pollution estimates from gaussian based air pollution models street canyon models range from complex computational fluid dynamic cfd models to simpler empirical e g usepa street box model described by dabberdt et al 1973 and johnson et al 1973 and semi empirical models e g danish operational street pollution model ospm described by vardoulakis et al 2003 some dispersion models include additional software modules for street canyon effects however these may increase model run time fallah shorshani et al 2017 jackson et al 2016 geospatial surrogates can be used to estimate the effect of street canyons on air quality in urban locations such metrics are commonly used in studies of urban climate where temperature and hence comfort levels are affected by building density and height for example sky view factor svf which estimates the percentage of sky that can be observed using a fish eye lens pointed vertically carrasco hernandez et al 2015 with areas with low svf corresponding to the presence of tall buildings has been incorporated into a lur model to estimate the presence of street canyons eeftens et al 2013 building height and or volume information has also been observed to improve the accuracy of lur model estimates gillespie et al 2016 su et al 2008b tang et al 2013 geospatial surrogates can be readily applied across entire cities in automated processes which are likely to be more reproducible than use of currently available gui based street canyon models as the latter require user judgement to identify street canyon locations and detailed information e g on traffic flow for each location the use of geospatial surrogates also has potential to improve the reproducibility of dispersion model pollution estimates as the number of model design choices is reduced substantially with corresponding substantial reduction in manpower costs in this paper we describe the development and evaluation of a new dispersion model rapidair ricardo aea ltd that uses modern scientific computing methods based on open source python libraries www python org a key motivation for the development of rapidair was our experience of a lack of a cost effective operational city scale dispersion model with convenient run times which does not require large amounts of manpower to operate we focused on operational convenience of the modelling process and accuracy of model predictions in a case study and compared our results to results from other published studies which evaluated other models in a similar geographical study area the design concept for rapidair is similar to the stems air model described by gulliver and briggs 2011 with some additional enhancements rapidair includes a dispersion model aermod with detailed treatment of boundary layer meteorology and street canyon models additionally we investigated the incorporation of geospatial surrogates to represent street canyon effects on spatial variations in pollution concentrations and we established methods for efficient post processing of the output from fine resolution dispersion models over large geographical areas using these surrogates 2 methods 2 1 study area and receptor locations we modelled concentrations of oxides of nitrogen nox in greater london urban conurbation approximately bounded by the m25 orbital motorway although nox and no2 were the pollutants of focus in this work the rapidair model can be run for any pollutants for which there are supporting emissions data including pm2 5 greater london was chosen as the study area because it contains a large network of air pollution monitoring sites and has detailed traffic and building height data additionally this was the study area used in a previous department for environment food and rural affairs defra urban model evaluation exercise which evaluated several commercially available and industry accepted models carslaw 2011 we modelled annual average nox and no2 concentrations for 2008 which was the same year as used in the defra study to enable comparison between rapidair and the models assessed in the defra comparison the rapidair model can be run at higher temporal resolutions provided that the model input data described below is also available at the same higher temporal resolution we evaluated the rapidair model at 86 continuous monitoring locations from the london air quality network laqn monitoring network fig a1 table a1 london datastore 2016 all of these sites are maintained by the environmental research group kings college london and local authorities in the city boroughs the data collected were subject to national ratification and detailed qa qc procedures defra 2017a b targa and loader 2008 for model evaluation purposes the monitoring sites were classified as kerbside roadside suburban and urban background receptors according to proximity to road traffic kerbside sites were located within 1 m of a busy road roadside sites were located within 1 5 m of a busy road suburban sites were located in a residential area on the edge of the urban conurbation and urban background sites were located in urban areas but were free from the immediate influence of local sources to provide a good indication of background concentrations defra 2016 similar to the defra urban model evaluation carslaw 2011 we excluded sites which had less than 75 data during 2008 it was not possible to use exactly the same locations as the defra urban model evaluation when we imported the locations used in the defra into a gis programme some were incorrect in a few cases up to several kilometres from their true location we relocated receptors to a best approximation of their true location using aerial photography and street level photographs but small discrepancies in the locations may still persist this may have affected our evaluation of the accuracy of model predictions at measurement sites and comparisons of our estimates with the estimates of other groups in this paper 2 2 model description a summary of the rapidair model is provided below and a technical description can be found in appendix a rapidair uses open source python libraries to rapidly estimate concentrations at fine spatial resolution over extended geographical areas rapidair is conceptually similar to the stems air model gulliver and briggs 2011 with technical development primarily based on inclusion of open source aermet and aermod software for automated processing of meteorological input data in this evaluation study surface and upper air meteorological data were obtained from the nearest meteorological stations to the study area heathrow airport national climatic data centre noaa 2018 and camborne earth systems research laboratory noaa 2018 for surface and upper air data respectively aermod is used to produce dispersion model plume estimates the kernel for a small idealised area source a theoretical source is located at the centre of the kernel in aermod assigned with a nominal emission rate of 1 g s and a kernel of size 55 x 55 cells was produced this kernel is rotated by 180 to represent the contribution of cells within the kernel to the central cell i e the cell in which we are trying to estimate the pollution concentration this produces a plume which identifies pollution sources that contributed to the central cell and estimates a scaling factor for each source that falls within the plume based on its distance and location to the source the rapidair dispersion model then uses a kernel convolution procedure which is similar to algorithms used in image processing software the kernel produced above is passed over a road traffic emission raster at the same resolution pixel by pixel so the final city wide model comprises millions of overlapping plumes from the road source emissions fig a2 for each receptor cell in this case at 5 m resolution the sum of concentrations falling within the kernel plume weighted by their distance to the source are written to the centre cell of the concentration raster fig a3 in this way the pollution surface is created by the convolution step iterating over the gridded emission data this means that model run time is linearly dependent on the spatial resolution of the output number of cells and is unaffected by the number of emissions sources in the domain this is a key benefit compared with other gaussian models whose run time is linearly dependent both on resolution number of receptors and number of sources our experience suggests that run times in the order of several days weeks can be expected for city scale gaussian models with only a few hundred thousand receptor locations which are then interpolated to provide continuous pollution surfaces in contrast the rapidair model computes concentrations at 100 million discrete receptors in less than 10 min using a 64 bit intel i5 8 gb processor nox emissions data for each road link were obtained for london in 2008 from the london atmospheric emissions inventory laei london atmospheric emissions inventory 2008 fig a1 emissions from laei individual road links were converted to a 5 m raster using the esri arcgis line density tool esri 2014 subsequent versions of rapidair use open source routines for preparing the emissions grid and this emissions raster used in the convolution step described above 1 1 km regional background concentrations calculated by the pollution climate mapping pcm model defra 2018 were added to the pollution raster fig a4 categorisations of the pcm model sources allowed us to remove road transport sources prior to adding the pcm model to the modelled pollution concentrations above to prevent double counting of traffic related pollutants 2 3 street canyon models concentrations of nox within street canyons were estimated using two street canyon models the street model dabberdt et al 1973 johnson et al 1973 and the aeolius model buckland and middleton 1999 cfd models are complex requiring very detailed emissions data which is difficult to obtain and have long run times this means they are not an operationally feasible solution for large scale model correction for canyon effects therefore were not considered during this study the street model estimates pollution concentrations empirically within a street canyon based on the emissions estimates within the canyon and takes into account vehicle induced turbulence and entry of air from the top of the canyon concentrations were calculated for the windward cw and leeward cl sides of the canyon using equations 1 and 2 1 c l k q u 0 5 x 2 z 2 1 2 l 0 2 c w k q h z w u 0 5 h where k is a scaling constant set to 14 here q is the emission rate g m s u is the wind speed m s l 0 is the length of individual vehicles set to 3 m w is the width of the canyon m h is the average building height of the canyon m x is the distance from emission source to receptor m and z is the receptor height set to 1 m the aeolius model was developed by the uk meteorological office in the 1990s buckland and middleton 1999 and the scientific basis for the model is presented in a series of papers buckland 1998 manning et al 2000 middleton 1999 1998a 1998b the aeolius model shares many common features with the operational street pollution model ospm berkowicz 2000 hertel and berkowicz 1989 which underpins many street canyon models included in commercial road source dispersion models there are three principal contributions to concentrations estimated by the aeolius model a direct contribution from the source to the receptor a recirculating component within a vortex caused by winds flowing across the top of the canyon and the urban background concentration the rapidair model only takes the recirculating component from the canyon model and sums this with the kernel derived concentrations the aeolius model is written in python 2 7 and implements the equations as described in appendix a 2 4 surrogates for street canyons building height data were used to calculate simple surrogates that could readily indicate locations that were located within street canyons and consequently allow modelled concentrations in these areas to be corrected accordingly a 5 m raster of maximum building height was created from building height data for london emu analytics 2018 derived by the suppliers from national scale lidar surveys survey open data 2018 we investigated three surrogates for street canyons fig a5 sky view factor svf representing amount of sky visible from each location when looking vertically up to the sky with a fish eye lens dimensionless ratio between 0 and 1 where 1 is all visible sky the relief visualization toolbox rvt kokalj et al 2011 zaksek et al 2011 was used to calculate svf using building height raster as input and a search radius of 200 m eeftens et al 2013 hill shading hs identifying areas in shade of surrounding topographical features zaksek et al 2011 in our analysis we used wind direction in place of the direction of the sun and the shading identified was anticipated to represent areas of higher concentration on the windward side of a street canyon the analytical hill shading option was run within rvt using an elevation angle of 45 kokalj et al 2013 suggested this value to be most appropriate for steep terrain encountered in an urban environment we calculated hs dimensionless value between 0 and 255 representing shaded and unshaded areas respectively for 8 sectors i e every 45 and averaged these calculated hs values for each 5 m raster cell in the study area wind effect we is a module in saga gis conrad et al 2015 which predicts if an area is wind shadowed or exposed where dimensionless values below and above 1 represent shadowed and exposed areas respectively böhner and antonić 2009 we was calculated for 8 sectors and averaged values calculated as above a search radius of 200 m was used surrogate svf hs we values for 5 m buffers around each receptor location were calculated to allow for slight errors in the coordinates of receptor locations e g receptors located within buildings rather than on lampposts on the road 2 5 nox to no2 conversion legislative limit values specified by the european union and uk government are for no2 and not nox therefore we converted rapidair nox concentrations to no2 concentrations using the defra nox to no2 model defra 2017a b which is recommended for use in uk air quality assessment for statutory purposes further information about the defra nox to no2 model is provided in the appendix briefly we derived a polynomial regression equation between predicted nox and no2 concentrations from the finite difference model within the defra tool the model was set to use the built in fleet composition for london which automatically sets the fraction of nox emissions as no2 f no2 and the average nox background concentration over the study area from the pcm model estimated no2 concentrations were plotted against nox concentrations and fitted with a polynomial regression equation equation 3 and fig a6 subsequently applied to the kernel model output to estimate no2 concentrations over the study area 3 n o 2 0 0001 n o x 2 0 2737 n o x 18 648 r 2 0 997 where nox and no2 concentrations are in μg m3 the expression is valid between the upper and lower nox concentrations in the curve in fig a6 the calculator uses estimates of regional no2 nox and o3 concentrations from the pcm model for individual local authority areas being modelled since london comprises many local authorities we compared no2 conversion estimates for two local authorities within our study area which had different regional no2 nox and o3 concentrations and found little effect on the nox to no2 conversion rate fig a6 2 6 model evaluation modelled concentrations of nox and no2 were extracted from the model outputs at the grid references for pollution monitoring sites to enable comparison the r package openair carslaw and ropkins 2012 was used to generate model evaluation statistics commonly used to evaluate pollution models including fac2 mean bias mb normalised mean bias nmb root mean square error rmse coefficient of efficiency coe and index of agreement ioa carslaw 2011 chang and hanna 2004 derwent et al 2010 we used simple data assimilation methods to calibrate model output against observed pollution concentrations at monitoring sites gulliver and briggs 2011 we present results of the evaluation of the kernel modelled nox vs measured nox and kernel modelled no2 vs measured no2 below this is followed by description of the estimation of nox concentrations from the kernel and street canyons surrogates and subsequent evaluation of modelled no2 concentrations after accounting for street canyon effects 3 results and discussion 3 1 rapidair model evaluation nox the baseline rapidair kernel model i e model not including urban morphology effects highlighted expected contributions to nox concentrations from major roads in london and heathrow airport in the west of the study area fig a10 the modelled concentrations at the monitoring sites were extracted and showed that the rapidair model systematically underestimated observed nox concentrations table 1 possible causes of this model underestimation are discussed further below using a similar conceptual approach to gulliver and briggs 2011 we corrected our modelled concentrations to account for potential systematic linear biases by linear regression between modelled and observed nox the receptor locations were randomly split into training n 57 and test n 29 data sets with the latter used as an independent verification data set the linear regression using the training data fig 1 was 4 m e a s u r e d n o x 1 98 k e r n e l m o d e l l e d n o x where measured no x and kernel modelled no x are concentrations in μg m3 a map of the modelled nox concentrations in the study area after correction for the systematic biases discussed previously is provided in fig a10 3 1 1 discussion of causes of systematic bias in air pollution models dispersion modelling involves multiple data inputs over several stages any of which has potential to contribute to inaccuracies in pollution estimates the under prediction of nox concentrations in our analyses may be due to uncertainties in emissions and or meteorological data and or uncertainties of representation of physical processes in aermod the simplest errors to characterise are for road traffic emissions and meteorology data it is likely that road traffic nox emissions data are underestimated in laei inventory we used this inventory was prepared by a statutory body greater london authority gla and remains the officially recognised emissions dataset for london the european environment agency s copert road traffic emissions model which was used by gla to create the laei has been observed to under predict historical nox emissions from diesel vehicles in the uk fleet carslaw et al 2011 consequently it is likely that reported under prediction of emissions in the diesel fleet biases the inventory towards under prediction of atmospheric concentrations nox emissions in the gla inventory are reported to have been underestimated by approximately 31 in 2008 beevers et al 2012b consistent with predictions of a coupled regional cmaq and road source dispersion model cmaq urban developed by other researchers for london average nox underestimation by cmaq urban of 32 beevers et al 2012a a correction for 31 underestimated emissions in our analyses would change the slope of modelled vs observed concentrations 1 1 98 in the training dataset regression analyses above from 0 51 49 underestimation of observations to 0 73 27 underestimation which is of a consistent magnitude with the above underestimation of cmaq urban modelled vs observed nox concentrations calculated by beevers et al 2012a the effect of using the most recent release of copert road traffic emissions model on the emissions in london is discussed further in the appendix and table a2 meteorological input data is a further potentially important source of systematic bias concentrations are inversely proportional to wind speed in the gaussian dispersion equation meaning uncertainties in wind speed estimates can lead to model bias for example gulliver and briggs 2011 noted that differences in windspeed measured at the relatively open heathrow airport meteorological station and windspeeds measured during short duration periods at pollution monitoring sites in central london resulted in pm10 model predictions using windspeeds measured in central london being on average 67 5 lower than pm10 predicted using windspeeds measured at heathrow similarly beevers et al 2012a noted that windspeeds measured at heathrow were systematically higher than windspeeds forecast using the weather research forecast wrf model specifically beevers et al illustrate how average midday windspeeds for 2006 measured at heathrow and modelled by wrf were 5 m s and 3 5 m s respectively difference representing 43 of wrf estimate approximated from fig 7 in beevers et al 2012a these differences suggest that use of measured heathrow windspeed data could result in an approximate 30 underestimation of pollution concentrations compared to equivalent concentrations estimated using wrf windspeed data the impact of using wind speeds from model vs heathrow for our study period and the consequent impact on the kernels created is discussed further in the appendix and figs a7 to a9 and table a3 the multiplicative combination of 31 underestimated nox emissions from the laei and 43 higher windspeeds from london heathrow measurements cf wrf windspeed estimates used by beevers et al 2012a suggests that the rapidair pollution estimates in our analyses may have underestimated nox concentration observations in central london by approximately 48 0 69 1 43 in context of above equivalent model observation comparisons made for cmaq urban beevers et al 2012a this difference is of similar magnitude to the underestimation of initial rapidair model estimates compared to monitoring site observations e g underestimation of 49 of observed concentrations represented in fig 1 3 2 rapidair model evaluation no2 concentrations of no2 estimated from rapidair fig 2 were compared to no2 concentrations measured at the receptor locations in table a1 no2 concentrations predicted by rapidair were similar to measured no2 concentrations at most monitoring stations however the model underestimated concentrations at some very high concentration kerbside measurement sites fig 3 table 2a underestimation by rapidair model might be attributed to urban morphologies including street canyon effects or underestimation in the location specific emissions rates used to predict the nox concentrations beevers et al 2012b the correlation between modelled and observed no2 concentrations r 0 77 was of similar magnitude to previous evaluations of dispersion models e g r 0 74 reported by de hoogh et al 2014 during evaluation of a nox dispersion model in the escape study defra suggest that an air quality model is acceptable for use if more than half of its observations fall within a factor of 2 of the observations williams et al 2011 the no2 rapidair model meets the fac2 criterion for all site types with the lowest fac2 value calculated for kerbside sites fac2 0 88 table 2a kerbside concentrations represent the worst case exposure scenarios that are not representative of population exposures over extended periods and consequently annual limit values do not apply at these sites defra 2016 similar findings were reported in the defra urban model evaluation exercise for no2 which found that fac2 values were lower for the kerbside sites than the three other site types tested however all models met the above defra criterion at the different site types carslaw 2011 another criterion suggested by defra to indicate the acceptability of a model is that nmb values should lie between 0 2 and 0 2 williams et al 2011 nmb values for rapidair met this criterion when all sites were considered together and for the individual site types with the exception of the kerbside sites table 2a none of the models tested during the defra model evaluation exercise met the nmb acceptance values proposed by defra at the kerbside sites carslaw 2011 the numbers of models meeting the criteria was progressively higher for kerbside roadside and urban background site classifications with all models meeting the nmb criterion at urban background locations carslaw 2011 3 3 accounting for street canyon effects in rapidair we investigated the inclusion of two techniques within the rapidair model to describe the effects of street canyons on pollution concentrations the first technique used geospatial surrogates to account for building morphologies within a study area and the second applied industry standard street canyon models to user defined street canyon geometries these techniques are discussed in the following sub sections 3 3 1 gis surrogates for street canyons we investigated if street canyon surrogates measured at each receptor could be used to estimate and subsequently correct for the effects of urban morphology on modelled nox concentrations and nox concentrations converted to no2 concentrations using the method described above the nox receptors were split randomly into the same training n 57 and test n 29 datasets used to derive the ols correction for bias described at the start of section 3 with the former used to develop surrogate correction equations and the latter used as an independent dataset to test the correction equations derived a multiple linear calibration equation was derived between unadjusted modelled no x measured no x and surrogate for each of the three surrogate values investigated using the training dataset table 3 a the multiple linear calibrations developed were then applied to the test nox table 3b shows the measured vs modelled nox after application of the surrogate calibrations for the test dataset the correlation between the concentrations and surrogates was unaffected by the surrogate used r 0 75 3 3 2 street canyon models of the 86 receptor locations we identified 19 sites that were located within urban street canyons through observations of the urban morphology using gis and google maps street view map data 2017 google table a1 a representative subset of the annual hourly meteorological data was used in the street canyon models to reduce model run times discussed in appendix a the effect of using a subset of meteorological data on computed annual average concentrations compared to the whole dataset was minimal for both canyon models aeolius was slightly more sensitive to the use of a sampled meteorological record street model slope 1 00 intercept 0 21 r 2 1 00 aeolius model slope 0 91 intercept 0 71 r 2 0 99 fig a11 the windward and leeward concentrations predicted by each of the street canyon models were averaged on the assumption that over a year concentrations are well mixed within the street canyon the concentrations predicted within by the canyon model were then added to the baseline nox concentrations predicted by the rapidair model representing the urban background in the area and the models corrected for systematic bias following the guidance in defra technical guidance 2016 defra 2016 table 4 3 3 3 evaluation of rapidair no2 estimates after accounting for street canyon effects at the receptor locations in street canyons the underestimation of the receptor concentrations was lowest for the street canyon models with the surrogates model and kernel models similarly under predicting the concentrations no2 nmb 0 18 for kernel average nmb 0 16 for surrogates 0 14 for street and 0 17 for aeolius models n 19 table 2b no2 and table a4 nox the street model predicted higher concentrations than the aeolius model which resulted in the smaller nmb values fig a12 the difference in modelled concentrations between the street and aeolius models was very small which is similar to previously published findings ganguly and broderick 2011 2010 gualtieri 2010 zhu et al 2015 when all types of receptor locations were considered there was little difference between the pollution concentrations estimated at the receptor locations for the rapidair model surrogates and the street canyon models fig a13 consequently there was limited difference in the model evaluation statistics when the surrogates and street canyon models were included table 2b no2 and table a4 nox inclusion of the street canyon models reduced the no2 nmb values compared to the standard kernel model however inclusion of the surrogates had little impact on nmb values at the kerbside sites kernel 0 25 surrogates 0 24 street 0 24 and aeolius 0 26 table 2b no2 and table a4 nox lur models for no2 incorporating svf street canyon surrogates also found little improvement in coefficient of determination values after surrogate inclusion r 2 0 76 vs 0 78 eeftens et al 2013 despite the negligible change in model evaluation statistics the combined kernel canyon models required less adjustment for systematic bias than the uncorrected kernel model table 4 therefore when a combined kernel canyon model is applied to areas of the city which do not have any measurements the model may be subject to less over or under estimation than the kernel model which does not attempt to address urban morphology for instance the combined kernel street model required adjustment using the linear regression equation adjusted no x 1 04 modelled nox 34 45 the slope here is significantly lower than the regression equation used to correct the kernel only model i e 1 04 vs 1 98 predicted concentrations were similar for the combined kernel street and combined kernel aeolius models the inclusion of the street canyon models is therefore an important step in accounting for urban morphology which can in practice be as influential to air pollution concentrations as spatial variations in emissions in an urban setting the use of surrogates to account for urban morphology effects including street canyons has computational simplicity advantages over street canyon models surrogate values can be rapidly calculated in a gis across a large study area canyon models require user selection of canyon locations and require additional information about canyon widths heights and traffic information such as speed and therefore cannot be easily computed for large areas additionally the transition from built up to open within the city for example at boundaries between buildings and parkland is treated in a gradual manner in surrogate models unlike street canyon models which impose a hard boundary at the canyon edge which is smoothed artificially in a gis with interpolation routines currently surrogates do not take wind speed into account which for annual averages we anticipate to have little influence on the model accuracy however if the surrogates were to be applied to a dispersion model with higher e g hourly temporal resolution then some modification of the surrogates to account for wind speed effects may be required in order to obtain similar modelled and measured pollution concentrations 3 4 advantages and limitations of rapidair the main aim of this work was to evaluate an air quality modelling platform designed for operational settings where time is often a priority and manpower computational resources are limited an example of an operational use of rapidair is given in appendix a rapidair succeeds as an operational air quality model in the context of very large urban areas and as a decision support tool but its efficiency comes with some drawbacks therefore it is appropriate to outline the key benefits and limitations of the approach to enable practitioners to interpret this work in light of their current experiences in running city scale dispersion models clearly a significant benefit with rapidair is reduced computational burden run times of 10 min or less for a very large city with 8 million inhabitants present a significant benefit for the operational modeller and decision makers who require fast but robust analyses the rapidair platform allows extremely efficient policy testing and other what if model runs for new emission scenarios to be undertaken in a few minutes on a standard office computer which is to our knowledge not possible using existing platforms the model performance metrics for rapidair in table 2 are very similar to those computed for other dispersion modelling systems in the defra inter comparison exercise for example the rapidair outputs for kerbside locations in london have no2 rmse values of 38 91 45 26 μg m3 depending on the method taking street canyons into consideration r 0 65 0 84 n 8 where the models in the inter comparison have rmse values ranging from 29 39 to 67 09 μg m3 r 0 15 0 93 n 7 at roadside locations the rapidair outputs have no2 rmse values of 12 78 14 28 μg m3 r 0 70 0 76 n 40 where the models in the inter comparison have rmse values ranging from 9 94 to 19 69 μg m3 r 0 38 0 89 n 30 some of the variation between rapidair and the other models will be due to the different number of receptors in each category which in reality may help or hinder our model performance but it is impossible for us to match the locations exactly for the reasons explained earlier the model results also yielded good results for the coe and ioa when compared with the definitions for these metrics provided by carslaw and ropkins 2012 the key model metrics for the 2008 model run in london are very similar to standard modelling suites used in the uk and which are used and accepted by defra for use in compliance assessments at the highest level of statutory european air quality reporting the high spatial resolution possible with the rapidair model makes it a suitable candidate for use as an exposure metric for epidemiology studies for example in our view the potential drawbacks of the model must be balanced against the benefits described above there may be the suggestion that the kernel based model represents a significantly simplified treatment of urban dispersion compared with models currently in use in the uk which iterate over thousands of receptors and calculate contributions at those receptors as a function of those sources with very significant run times in fact all gaussian and empirical models are already a greatly simplified picture of reality in urban settings and the methodology in rapidair does not significantly alter the overall level of simplification compared with the real situation in any case the model results are compared against pollution measurements as with all other models using the same metrics and the results of that performance assessment are comparable with other platforms the performance statistics for the surrogates for urban morphology are reasonably close to those from the models which treat canyons discretely again our focus is on operational modelling where reproducible and efficient workflows are as important as the tools selected for use based on this work we would suggest that for compliance assessment rapidair is used with either the street or aeolius model options included as the run times are not significantly impacted by including these models the model results should be compared with measured concentrations and the modeller may choose the best performing street canyon model for their case the surrogate models should be used as screening tools and perhaps to spatially delineate locations where the street canyon models should be invoked which is often difficult for a large and complex urban environment where resources do not permit thorough investigation and spatial treatment of the morphological conditions 4 conclusions we developed a kernel based dispersion model rapidair combining aermod and open source scientific computing methods to estimate pollution concentrations at fine spatial resolution model input data was obtained from public sources to allow comparison with pollution models for the same location with the same input data the rapidair dispersion model took approximately 7 min to model the greater london conurbation 3500 km2 at 5 5 m resolution using an intel i5 64 bit laptop with 8 gb ram we evaluated nox and no2 model predictions at 86 sites across london after correction for systematic under estimation bias in the initial rapidair model fac2 values for modelled concentrations were 0 85 at the 86 evaluation sites rmse values decreased through the site categories kerbside roadside urban background and suburban rmse 45 14 6 and 4 μg m3 respectively this finding is consistent with results from other modelling groups participating in the defra inter comparison whose rmse values ranged from 3 to 70 μg m3 respectively the larger rmse values at the sites in proximity to traffic sources may have resulted from the presence of street canyons that trap pollutants leading to elevated concentrations an effect that cannot be described in dispersion models unless urban morphologies are taken into consideration correspondingly we used geospatial surrogates sky view factor hill shading and wind effect and separate street canyon models street and aeolius to improve modelled concentrations at roadside sites the street canyon model and street canyon surrogates improved the model rmse at kerbside sites rapidair base kernel 45 2 sky view factor surrogate 44 0 street model 38 9 and aeolius 42 8 μg m3 when all sites were considered the lowest rmse values were observed for the kernel model combined with the street canyon model rmse rapidair base kernel 17 1 vs street model 15 9 μg m3 consequently the combined models may be anticipated to provide more accurate estimates when extrapolated to locations without monitoring the geospatial surrogates have potential as simple means of incorporating canyon effects into a large city scale dispersion model the advantage of using simple geospatial surrogates for street canyons instead of modelling canyons discretely include reduced run times smaller user input required and the transition from built up to open environments is treated gradually acknowledgements we gratefully acknowledge the work of professor bryan harris at the university of bath who previously developed a mathcad version of the aeolius model and who diligently described the model formulations he obtained from the met office s original author dr d r middleton and much of our implementation of the aeolius model is based on professor harris description of the system of equations which make up aeolius harris 2004 see appendix a nicola masey is funded through a uk natural environment research council case phd studentship ne k007319 1 with support from ricardo energy and environment we acknowledge access to the laqn measurement data and the pcm model data which were obtained from uk air defra gov uk and are subject to crown 2014 copyright defra licensed under the open government licence ogl rapidair is a registered trade mark of ricardo aea limited used under permission of ricardo aea limited appendix a supplementary data the following is the supplementary data related to this article appendix appendix appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2018 05 014 
26356,we developed a dispersion model rapidair to estimate air pollution concentrations at fine spatial resolution over large geographical areas with fast run times concentrations were modelled at 5 m spatial resolution over an area of 3500 km2 in 10 min rapidair was evaluated by estimating nox and no2 concentrations at 86 continuous monitoring sites in london uk during 2008 the model predictions explained 66 of the spatial variation r 0 81 in annual nox concentrations observed at the monitoring sites we included discrete canyon models or geospatial surrogates sky view factor hill shading and wind effect to improve the accuracy of model predictions at kerbside locations geospatial surrogates provide alternatives to discrete street canyon models where it is impractical to run canyon models for thousands of streets within a large city dispersion model with advantages including ease of operation faster run times and more complete treatment of building effects graphical abstract image 1 keywords dispersion modelling air pollution gis nox no2 street canyon software availability name of software rapidair developer ricardo energy and environment hardware information general purpose computer 4 16 gb ram intel r core tm i5 processor 64 bit operating system programming language python 2 6 and r availability contact the developers 1 introduction the estimation of population exposures to air pollution is increasingly important as numerous studies highlight the detrimental effects of air pollution on human health world health organization 2013 2016 the use of air pollution monitors allows direct measurement of ambient concentrations and the on going development of portable real time monitors is providing improvements in temporally resolved concentration estimates dons et al 2012 spinelle et al 2017 2015 however monitoring only provides concentration estimates at specific locations whereas it has been observed that pollution concentrations can vary substantially over small areas gillespie et al 2017 lin et al 2016 models can overcome some of the limitations associated with monitoring as concentrations can be estimated at multiple locations within a study area however inherent uncertainties within models require to be quantified by comparison of predictions against air pollution measurements two main types of models are commonly used to estimate urban air pollution land use regression lur models and dispersion models we do not include discussion of computational fluid dynamics cfd models in this paper as cfd models have not been used widely in operational predictions of spatial patterns of urban air pollution due to excessive computational constraints when operating over large geographical areas land use regression lur models use geographical information systems gis to quantify relationships between measured pollutant concentrations and land use variables including traffic and population which can then be extrapolated to estimate human exposure to air pollution at fine spatial resolution briggs et al 1997 lur models have been widely applied in in cohort epidemiological studies gillespie et al 2016 johnson et al 2013 wang et al 2013 and in personal monitoring studies dons et al 2014a 2014b lur models are frequently used to estimate longer term e g annual pollution exposure and often do not take into account the effects of meteorology additionally the transfer of lur models between study areas has been shown have substantial limitations including differences in monitoring location type which can lead to model bias gillespie et al 2016 mukerjee et al 2012 patton et al 2015 many regulatory organisations are interested in source apportionment to inform policy on air pollution controls which requires preparation of spatially accurate multi source air quality emissions however lur models seldom use direct quantitative estimates of emissions from sources instead more commonly they assess the effects of receptor proximity to sources and consequently lur models have had limited application in air quality management policy development dispersion models simulate atmospheric transport and transformation of air pollutants emitted from sources to allow estimation of concentrations at receptors the most commonly used models are based on gaussian plume concepts dispersion models can be used to estimate short term e g hourly variations in pollution concentrations gibson et al 2013 and to estimate population exposures in cohort studies bellander et al 2001 nyberg et al 2000 additionally projected emissions estimates if available can be used to estimate future concentrations commercially available software packages have been developed to simplify user inputs and modelling procedures however this has often resulted in high license costs gulliver and briggs 2011 particularly when it is necessary to apply models over large geographical areas furthermore gaussian dispersion model run times for large urban area can quickly become prohibitive due the computational demands of calculating concentrations at what can extend to millions of discrete locations this may necessitate the use of gis interpolation routines to increase the spatial resolution of the model estimates which may introduce other errors into estimated exposures wong et al 2004 some studies have addressed these challenges to achieve fine spatial and temporal resolution by combining dispersion and lur models korek et al 2016 michanowicz et al 2016 wilton et al 2010 and or including meteorological information within lur models su et al 2008a tan et al 2016 a hybrid gis dispersion model stems air has been developed to enable fine spatial and temporal resolution while minimising run times with readily available computer software gulliver and briggs 2011 the stems air model estimates pollution concentrations from emission sources in 45 upwind wedge shaped gis buffer areas scaled by the distance between sources and receptors in built up urban areas air pollution can become trapped in street canyons surrounded by tall buildings especially if the wind is blowing from a direction perpendicular to the street leading to recirculation of pollutants within the canyon as a result pollution concentrations in street canyons can become elevated and may be underestimated by standard air pollution models including lur or gaussian plume models exposure estimates may be improved by combining additional models that take into account urban topography in such locations with background pollution estimates from gaussian based air pollution models street canyon models range from complex computational fluid dynamic cfd models to simpler empirical e g usepa street box model described by dabberdt et al 1973 and johnson et al 1973 and semi empirical models e g danish operational street pollution model ospm described by vardoulakis et al 2003 some dispersion models include additional software modules for street canyon effects however these may increase model run time fallah shorshani et al 2017 jackson et al 2016 geospatial surrogates can be used to estimate the effect of street canyons on air quality in urban locations such metrics are commonly used in studies of urban climate where temperature and hence comfort levels are affected by building density and height for example sky view factor svf which estimates the percentage of sky that can be observed using a fish eye lens pointed vertically carrasco hernandez et al 2015 with areas with low svf corresponding to the presence of tall buildings has been incorporated into a lur model to estimate the presence of street canyons eeftens et al 2013 building height and or volume information has also been observed to improve the accuracy of lur model estimates gillespie et al 2016 su et al 2008b tang et al 2013 geospatial surrogates can be readily applied across entire cities in automated processes which are likely to be more reproducible than use of currently available gui based street canyon models as the latter require user judgement to identify street canyon locations and detailed information e g on traffic flow for each location the use of geospatial surrogates also has potential to improve the reproducibility of dispersion model pollution estimates as the number of model design choices is reduced substantially with corresponding substantial reduction in manpower costs in this paper we describe the development and evaluation of a new dispersion model rapidair ricardo aea ltd that uses modern scientific computing methods based on open source python libraries www python org a key motivation for the development of rapidair was our experience of a lack of a cost effective operational city scale dispersion model with convenient run times which does not require large amounts of manpower to operate we focused on operational convenience of the modelling process and accuracy of model predictions in a case study and compared our results to results from other published studies which evaluated other models in a similar geographical study area the design concept for rapidair is similar to the stems air model described by gulliver and briggs 2011 with some additional enhancements rapidair includes a dispersion model aermod with detailed treatment of boundary layer meteorology and street canyon models additionally we investigated the incorporation of geospatial surrogates to represent street canyon effects on spatial variations in pollution concentrations and we established methods for efficient post processing of the output from fine resolution dispersion models over large geographical areas using these surrogates 2 methods 2 1 study area and receptor locations we modelled concentrations of oxides of nitrogen nox in greater london urban conurbation approximately bounded by the m25 orbital motorway although nox and no2 were the pollutants of focus in this work the rapidair model can be run for any pollutants for which there are supporting emissions data including pm2 5 greater london was chosen as the study area because it contains a large network of air pollution monitoring sites and has detailed traffic and building height data additionally this was the study area used in a previous department for environment food and rural affairs defra urban model evaluation exercise which evaluated several commercially available and industry accepted models carslaw 2011 we modelled annual average nox and no2 concentrations for 2008 which was the same year as used in the defra study to enable comparison between rapidair and the models assessed in the defra comparison the rapidair model can be run at higher temporal resolutions provided that the model input data described below is also available at the same higher temporal resolution we evaluated the rapidair model at 86 continuous monitoring locations from the london air quality network laqn monitoring network fig a1 table a1 london datastore 2016 all of these sites are maintained by the environmental research group kings college london and local authorities in the city boroughs the data collected were subject to national ratification and detailed qa qc procedures defra 2017a b targa and loader 2008 for model evaluation purposes the monitoring sites were classified as kerbside roadside suburban and urban background receptors according to proximity to road traffic kerbside sites were located within 1 m of a busy road roadside sites were located within 1 5 m of a busy road suburban sites were located in a residential area on the edge of the urban conurbation and urban background sites were located in urban areas but were free from the immediate influence of local sources to provide a good indication of background concentrations defra 2016 similar to the defra urban model evaluation carslaw 2011 we excluded sites which had less than 75 data during 2008 it was not possible to use exactly the same locations as the defra urban model evaluation when we imported the locations used in the defra into a gis programme some were incorrect in a few cases up to several kilometres from their true location we relocated receptors to a best approximation of their true location using aerial photography and street level photographs but small discrepancies in the locations may still persist this may have affected our evaluation of the accuracy of model predictions at measurement sites and comparisons of our estimates with the estimates of other groups in this paper 2 2 model description a summary of the rapidair model is provided below and a technical description can be found in appendix a rapidair uses open source python libraries to rapidly estimate concentrations at fine spatial resolution over extended geographical areas rapidair is conceptually similar to the stems air model gulliver and briggs 2011 with technical development primarily based on inclusion of open source aermet and aermod software for automated processing of meteorological input data in this evaluation study surface and upper air meteorological data were obtained from the nearest meteorological stations to the study area heathrow airport national climatic data centre noaa 2018 and camborne earth systems research laboratory noaa 2018 for surface and upper air data respectively aermod is used to produce dispersion model plume estimates the kernel for a small idealised area source a theoretical source is located at the centre of the kernel in aermod assigned with a nominal emission rate of 1 g s and a kernel of size 55 x 55 cells was produced this kernel is rotated by 180 to represent the contribution of cells within the kernel to the central cell i e the cell in which we are trying to estimate the pollution concentration this produces a plume which identifies pollution sources that contributed to the central cell and estimates a scaling factor for each source that falls within the plume based on its distance and location to the source the rapidair dispersion model then uses a kernel convolution procedure which is similar to algorithms used in image processing software the kernel produced above is passed over a road traffic emission raster at the same resolution pixel by pixel so the final city wide model comprises millions of overlapping plumes from the road source emissions fig a2 for each receptor cell in this case at 5 m resolution the sum of concentrations falling within the kernel plume weighted by their distance to the source are written to the centre cell of the concentration raster fig a3 in this way the pollution surface is created by the convolution step iterating over the gridded emission data this means that model run time is linearly dependent on the spatial resolution of the output number of cells and is unaffected by the number of emissions sources in the domain this is a key benefit compared with other gaussian models whose run time is linearly dependent both on resolution number of receptors and number of sources our experience suggests that run times in the order of several days weeks can be expected for city scale gaussian models with only a few hundred thousand receptor locations which are then interpolated to provide continuous pollution surfaces in contrast the rapidair model computes concentrations at 100 million discrete receptors in less than 10 min using a 64 bit intel i5 8 gb processor nox emissions data for each road link were obtained for london in 2008 from the london atmospheric emissions inventory laei london atmospheric emissions inventory 2008 fig a1 emissions from laei individual road links were converted to a 5 m raster using the esri arcgis line density tool esri 2014 subsequent versions of rapidair use open source routines for preparing the emissions grid and this emissions raster used in the convolution step described above 1 1 km regional background concentrations calculated by the pollution climate mapping pcm model defra 2018 were added to the pollution raster fig a4 categorisations of the pcm model sources allowed us to remove road transport sources prior to adding the pcm model to the modelled pollution concentrations above to prevent double counting of traffic related pollutants 2 3 street canyon models concentrations of nox within street canyons were estimated using two street canyon models the street model dabberdt et al 1973 johnson et al 1973 and the aeolius model buckland and middleton 1999 cfd models are complex requiring very detailed emissions data which is difficult to obtain and have long run times this means they are not an operationally feasible solution for large scale model correction for canyon effects therefore were not considered during this study the street model estimates pollution concentrations empirically within a street canyon based on the emissions estimates within the canyon and takes into account vehicle induced turbulence and entry of air from the top of the canyon concentrations were calculated for the windward cw and leeward cl sides of the canyon using equations 1 and 2 1 c l k q u 0 5 x 2 z 2 1 2 l 0 2 c w k q h z w u 0 5 h where k is a scaling constant set to 14 here q is the emission rate g m s u is the wind speed m s l 0 is the length of individual vehicles set to 3 m w is the width of the canyon m h is the average building height of the canyon m x is the distance from emission source to receptor m and z is the receptor height set to 1 m the aeolius model was developed by the uk meteorological office in the 1990s buckland and middleton 1999 and the scientific basis for the model is presented in a series of papers buckland 1998 manning et al 2000 middleton 1999 1998a 1998b the aeolius model shares many common features with the operational street pollution model ospm berkowicz 2000 hertel and berkowicz 1989 which underpins many street canyon models included in commercial road source dispersion models there are three principal contributions to concentrations estimated by the aeolius model a direct contribution from the source to the receptor a recirculating component within a vortex caused by winds flowing across the top of the canyon and the urban background concentration the rapidair model only takes the recirculating component from the canyon model and sums this with the kernel derived concentrations the aeolius model is written in python 2 7 and implements the equations as described in appendix a 2 4 surrogates for street canyons building height data were used to calculate simple surrogates that could readily indicate locations that were located within street canyons and consequently allow modelled concentrations in these areas to be corrected accordingly a 5 m raster of maximum building height was created from building height data for london emu analytics 2018 derived by the suppliers from national scale lidar surveys survey open data 2018 we investigated three surrogates for street canyons fig a5 sky view factor svf representing amount of sky visible from each location when looking vertically up to the sky with a fish eye lens dimensionless ratio between 0 and 1 where 1 is all visible sky the relief visualization toolbox rvt kokalj et al 2011 zaksek et al 2011 was used to calculate svf using building height raster as input and a search radius of 200 m eeftens et al 2013 hill shading hs identifying areas in shade of surrounding topographical features zaksek et al 2011 in our analysis we used wind direction in place of the direction of the sun and the shading identified was anticipated to represent areas of higher concentration on the windward side of a street canyon the analytical hill shading option was run within rvt using an elevation angle of 45 kokalj et al 2013 suggested this value to be most appropriate for steep terrain encountered in an urban environment we calculated hs dimensionless value between 0 and 255 representing shaded and unshaded areas respectively for 8 sectors i e every 45 and averaged these calculated hs values for each 5 m raster cell in the study area wind effect we is a module in saga gis conrad et al 2015 which predicts if an area is wind shadowed or exposed where dimensionless values below and above 1 represent shadowed and exposed areas respectively böhner and antonić 2009 we was calculated for 8 sectors and averaged values calculated as above a search radius of 200 m was used surrogate svf hs we values for 5 m buffers around each receptor location were calculated to allow for slight errors in the coordinates of receptor locations e g receptors located within buildings rather than on lampposts on the road 2 5 nox to no2 conversion legislative limit values specified by the european union and uk government are for no2 and not nox therefore we converted rapidair nox concentrations to no2 concentrations using the defra nox to no2 model defra 2017a b which is recommended for use in uk air quality assessment for statutory purposes further information about the defra nox to no2 model is provided in the appendix briefly we derived a polynomial regression equation between predicted nox and no2 concentrations from the finite difference model within the defra tool the model was set to use the built in fleet composition for london which automatically sets the fraction of nox emissions as no2 f no2 and the average nox background concentration over the study area from the pcm model estimated no2 concentrations were plotted against nox concentrations and fitted with a polynomial regression equation equation 3 and fig a6 subsequently applied to the kernel model output to estimate no2 concentrations over the study area 3 n o 2 0 0001 n o x 2 0 2737 n o x 18 648 r 2 0 997 where nox and no2 concentrations are in μg m3 the expression is valid between the upper and lower nox concentrations in the curve in fig a6 the calculator uses estimates of regional no2 nox and o3 concentrations from the pcm model for individual local authority areas being modelled since london comprises many local authorities we compared no2 conversion estimates for two local authorities within our study area which had different regional no2 nox and o3 concentrations and found little effect on the nox to no2 conversion rate fig a6 2 6 model evaluation modelled concentrations of nox and no2 were extracted from the model outputs at the grid references for pollution monitoring sites to enable comparison the r package openair carslaw and ropkins 2012 was used to generate model evaluation statistics commonly used to evaluate pollution models including fac2 mean bias mb normalised mean bias nmb root mean square error rmse coefficient of efficiency coe and index of agreement ioa carslaw 2011 chang and hanna 2004 derwent et al 2010 we used simple data assimilation methods to calibrate model output against observed pollution concentrations at monitoring sites gulliver and briggs 2011 we present results of the evaluation of the kernel modelled nox vs measured nox and kernel modelled no2 vs measured no2 below this is followed by description of the estimation of nox concentrations from the kernel and street canyons surrogates and subsequent evaluation of modelled no2 concentrations after accounting for street canyon effects 3 results and discussion 3 1 rapidair model evaluation nox the baseline rapidair kernel model i e model not including urban morphology effects highlighted expected contributions to nox concentrations from major roads in london and heathrow airport in the west of the study area fig a10 the modelled concentrations at the monitoring sites were extracted and showed that the rapidair model systematically underestimated observed nox concentrations table 1 possible causes of this model underestimation are discussed further below using a similar conceptual approach to gulliver and briggs 2011 we corrected our modelled concentrations to account for potential systematic linear biases by linear regression between modelled and observed nox the receptor locations were randomly split into training n 57 and test n 29 data sets with the latter used as an independent verification data set the linear regression using the training data fig 1 was 4 m e a s u r e d n o x 1 98 k e r n e l m o d e l l e d n o x where measured no x and kernel modelled no x are concentrations in μg m3 a map of the modelled nox concentrations in the study area after correction for the systematic biases discussed previously is provided in fig a10 3 1 1 discussion of causes of systematic bias in air pollution models dispersion modelling involves multiple data inputs over several stages any of which has potential to contribute to inaccuracies in pollution estimates the under prediction of nox concentrations in our analyses may be due to uncertainties in emissions and or meteorological data and or uncertainties of representation of physical processes in aermod the simplest errors to characterise are for road traffic emissions and meteorology data it is likely that road traffic nox emissions data are underestimated in laei inventory we used this inventory was prepared by a statutory body greater london authority gla and remains the officially recognised emissions dataset for london the european environment agency s copert road traffic emissions model which was used by gla to create the laei has been observed to under predict historical nox emissions from diesel vehicles in the uk fleet carslaw et al 2011 consequently it is likely that reported under prediction of emissions in the diesel fleet biases the inventory towards under prediction of atmospheric concentrations nox emissions in the gla inventory are reported to have been underestimated by approximately 31 in 2008 beevers et al 2012b consistent with predictions of a coupled regional cmaq and road source dispersion model cmaq urban developed by other researchers for london average nox underestimation by cmaq urban of 32 beevers et al 2012a a correction for 31 underestimated emissions in our analyses would change the slope of modelled vs observed concentrations 1 1 98 in the training dataset regression analyses above from 0 51 49 underestimation of observations to 0 73 27 underestimation which is of a consistent magnitude with the above underestimation of cmaq urban modelled vs observed nox concentrations calculated by beevers et al 2012a the effect of using the most recent release of copert road traffic emissions model on the emissions in london is discussed further in the appendix and table a2 meteorological input data is a further potentially important source of systematic bias concentrations are inversely proportional to wind speed in the gaussian dispersion equation meaning uncertainties in wind speed estimates can lead to model bias for example gulliver and briggs 2011 noted that differences in windspeed measured at the relatively open heathrow airport meteorological station and windspeeds measured during short duration periods at pollution monitoring sites in central london resulted in pm10 model predictions using windspeeds measured in central london being on average 67 5 lower than pm10 predicted using windspeeds measured at heathrow similarly beevers et al 2012a noted that windspeeds measured at heathrow were systematically higher than windspeeds forecast using the weather research forecast wrf model specifically beevers et al illustrate how average midday windspeeds for 2006 measured at heathrow and modelled by wrf were 5 m s and 3 5 m s respectively difference representing 43 of wrf estimate approximated from fig 7 in beevers et al 2012a these differences suggest that use of measured heathrow windspeed data could result in an approximate 30 underestimation of pollution concentrations compared to equivalent concentrations estimated using wrf windspeed data the impact of using wind speeds from model vs heathrow for our study period and the consequent impact on the kernels created is discussed further in the appendix and figs a7 to a9 and table a3 the multiplicative combination of 31 underestimated nox emissions from the laei and 43 higher windspeeds from london heathrow measurements cf wrf windspeed estimates used by beevers et al 2012a suggests that the rapidair pollution estimates in our analyses may have underestimated nox concentration observations in central london by approximately 48 0 69 1 43 in context of above equivalent model observation comparisons made for cmaq urban beevers et al 2012a this difference is of similar magnitude to the underestimation of initial rapidair model estimates compared to monitoring site observations e g underestimation of 49 of observed concentrations represented in fig 1 3 2 rapidair model evaluation no2 concentrations of no2 estimated from rapidair fig 2 were compared to no2 concentrations measured at the receptor locations in table a1 no2 concentrations predicted by rapidair were similar to measured no2 concentrations at most monitoring stations however the model underestimated concentrations at some very high concentration kerbside measurement sites fig 3 table 2a underestimation by rapidair model might be attributed to urban morphologies including street canyon effects or underestimation in the location specific emissions rates used to predict the nox concentrations beevers et al 2012b the correlation between modelled and observed no2 concentrations r 0 77 was of similar magnitude to previous evaluations of dispersion models e g r 0 74 reported by de hoogh et al 2014 during evaluation of a nox dispersion model in the escape study defra suggest that an air quality model is acceptable for use if more than half of its observations fall within a factor of 2 of the observations williams et al 2011 the no2 rapidair model meets the fac2 criterion for all site types with the lowest fac2 value calculated for kerbside sites fac2 0 88 table 2a kerbside concentrations represent the worst case exposure scenarios that are not representative of population exposures over extended periods and consequently annual limit values do not apply at these sites defra 2016 similar findings were reported in the defra urban model evaluation exercise for no2 which found that fac2 values were lower for the kerbside sites than the three other site types tested however all models met the above defra criterion at the different site types carslaw 2011 another criterion suggested by defra to indicate the acceptability of a model is that nmb values should lie between 0 2 and 0 2 williams et al 2011 nmb values for rapidair met this criterion when all sites were considered together and for the individual site types with the exception of the kerbside sites table 2a none of the models tested during the defra model evaluation exercise met the nmb acceptance values proposed by defra at the kerbside sites carslaw 2011 the numbers of models meeting the criteria was progressively higher for kerbside roadside and urban background site classifications with all models meeting the nmb criterion at urban background locations carslaw 2011 3 3 accounting for street canyon effects in rapidair we investigated the inclusion of two techniques within the rapidair model to describe the effects of street canyons on pollution concentrations the first technique used geospatial surrogates to account for building morphologies within a study area and the second applied industry standard street canyon models to user defined street canyon geometries these techniques are discussed in the following sub sections 3 3 1 gis surrogates for street canyons we investigated if street canyon surrogates measured at each receptor could be used to estimate and subsequently correct for the effects of urban morphology on modelled nox concentrations and nox concentrations converted to no2 concentrations using the method described above the nox receptors were split randomly into the same training n 57 and test n 29 datasets used to derive the ols correction for bias described at the start of section 3 with the former used to develop surrogate correction equations and the latter used as an independent dataset to test the correction equations derived a multiple linear calibration equation was derived between unadjusted modelled no x measured no x and surrogate for each of the three surrogate values investigated using the training dataset table 3 a the multiple linear calibrations developed were then applied to the test nox table 3b shows the measured vs modelled nox after application of the surrogate calibrations for the test dataset the correlation between the concentrations and surrogates was unaffected by the surrogate used r 0 75 3 3 2 street canyon models of the 86 receptor locations we identified 19 sites that were located within urban street canyons through observations of the urban morphology using gis and google maps street view map data 2017 google table a1 a representative subset of the annual hourly meteorological data was used in the street canyon models to reduce model run times discussed in appendix a the effect of using a subset of meteorological data on computed annual average concentrations compared to the whole dataset was minimal for both canyon models aeolius was slightly more sensitive to the use of a sampled meteorological record street model slope 1 00 intercept 0 21 r 2 1 00 aeolius model slope 0 91 intercept 0 71 r 2 0 99 fig a11 the windward and leeward concentrations predicted by each of the street canyon models were averaged on the assumption that over a year concentrations are well mixed within the street canyon the concentrations predicted within by the canyon model were then added to the baseline nox concentrations predicted by the rapidair model representing the urban background in the area and the models corrected for systematic bias following the guidance in defra technical guidance 2016 defra 2016 table 4 3 3 3 evaluation of rapidair no2 estimates after accounting for street canyon effects at the receptor locations in street canyons the underestimation of the receptor concentrations was lowest for the street canyon models with the surrogates model and kernel models similarly under predicting the concentrations no2 nmb 0 18 for kernel average nmb 0 16 for surrogates 0 14 for street and 0 17 for aeolius models n 19 table 2b no2 and table a4 nox the street model predicted higher concentrations than the aeolius model which resulted in the smaller nmb values fig a12 the difference in modelled concentrations between the street and aeolius models was very small which is similar to previously published findings ganguly and broderick 2011 2010 gualtieri 2010 zhu et al 2015 when all types of receptor locations were considered there was little difference between the pollution concentrations estimated at the receptor locations for the rapidair model surrogates and the street canyon models fig a13 consequently there was limited difference in the model evaluation statistics when the surrogates and street canyon models were included table 2b no2 and table a4 nox inclusion of the street canyon models reduced the no2 nmb values compared to the standard kernel model however inclusion of the surrogates had little impact on nmb values at the kerbside sites kernel 0 25 surrogates 0 24 street 0 24 and aeolius 0 26 table 2b no2 and table a4 nox lur models for no2 incorporating svf street canyon surrogates also found little improvement in coefficient of determination values after surrogate inclusion r 2 0 76 vs 0 78 eeftens et al 2013 despite the negligible change in model evaluation statistics the combined kernel canyon models required less adjustment for systematic bias than the uncorrected kernel model table 4 therefore when a combined kernel canyon model is applied to areas of the city which do not have any measurements the model may be subject to less over or under estimation than the kernel model which does not attempt to address urban morphology for instance the combined kernel street model required adjustment using the linear regression equation adjusted no x 1 04 modelled nox 34 45 the slope here is significantly lower than the regression equation used to correct the kernel only model i e 1 04 vs 1 98 predicted concentrations were similar for the combined kernel street and combined kernel aeolius models the inclusion of the street canyon models is therefore an important step in accounting for urban morphology which can in practice be as influential to air pollution concentrations as spatial variations in emissions in an urban setting the use of surrogates to account for urban morphology effects including street canyons has computational simplicity advantages over street canyon models surrogate values can be rapidly calculated in a gis across a large study area canyon models require user selection of canyon locations and require additional information about canyon widths heights and traffic information such as speed and therefore cannot be easily computed for large areas additionally the transition from built up to open within the city for example at boundaries between buildings and parkland is treated in a gradual manner in surrogate models unlike street canyon models which impose a hard boundary at the canyon edge which is smoothed artificially in a gis with interpolation routines currently surrogates do not take wind speed into account which for annual averages we anticipate to have little influence on the model accuracy however if the surrogates were to be applied to a dispersion model with higher e g hourly temporal resolution then some modification of the surrogates to account for wind speed effects may be required in order to obtain similar modelled and measured pollution concentrations 3 4 advantages and limitations of rapidair the main aim of this work was to evaluate an air quality modelling platform designed for operational settings where time is often a priority and manpower computational resources are limited an example of an operational use of rapidair is given in appendix a rapidair succeeds as an operational air quality model in the context of very large urban areas and as a decision support tool but its efficiency comes with some drawbacks therefore it is appropriate to outline the key benefits and limitations of the approach to enable practitioners to interpret this work in light of their current experiences in running city scale dispersion models clearly a significant benefit with rapidair is reduced computational burden run times of 10 min or less for a very large city with 8 million inhabitants present a significant benefit for the operational modeller and decision makers who require fast but robust analyses the rapidair platform allows extremely efficient policy testing and other what if model runs for new emission scenarios to be undertaken in a few minutes on a standard office computer which is to our knowledge not possible using existing platforms the model performance metrics for rapidair in table 2 are very similar to those computed for other dispersion modelling systems in the defra inter comparison exercise for example the rapidair outputs for kerbside locations in london have no2 rmse values of 38 91 45 26 μg m3 depending on the method taking street canyons into consideration r 0 65 0 84 n 8 where the models in the inter comparison have rmse values ranging from 29 39 to 67 09 μg m3 r 0 15 0 93 n 7 at roadside locations the rapidair outputs have no2 rmse values of 12 78 14 28 μg m3 r 0 70 0 76 n 40 where the models in the inter comparison have rmse values ranging from 9 94 to 19 69 μg m3 r 0 38 0 89 n 30 some of the variation between rapidair and the other models will be due to the different number of receptors in each category which in reality may help or hinder our model performance but it is impossible for us to match the locations exactly for the reasons explained earlier the model results also yielded good results for the coe and ioa when compared with the definitions for these metrics provided by carslaw and ropkins 2012 the key model metrics for the 2008 model run in london are very similar to standard modelling suites used in the uk and which are used and accepted by defra for use in compliance assessments at the highest level of statutory european air quality reporting the high spatial resolution possible with the rapidair model makes it a suitable candidate for use as an exposure metric for epidemiology studies for example in our view the potential drawbacks of the model must be balanced against the benefits described above there may be the suggestion that the kernel based model represents a significantly simplified treatment of urban dispersion compared with models currently in use in the uk which iterate over thousands of receptors and calculate contributions at those receptors as a function of those sources with very significant run times in fact all gaussian and empirical models are already a greatly simplified picture of reality in urban settings and the methodology in rapidair does not significantly alter the overall level of simplification compared with the real situation in any case the model results are compared against pollution measurements as with all other models using the same metrics and the results of that performance assessment are comparable with other platforms the performance statistics for the surrogates for urban morphology are reasonably close to those from the models which treat canyons discretely again our focus is on operational modelling where reproducible and efficient workflows are as important as the tools selected for use based on this work we would suggest that for compliance assessment rapidair is used with either the street or aeolius model options included as the run times are not significantly impacted by including these models the model results should be compared with measured concentrations and the modeller may choose the best performing street canyon model for their case the surrogate models should be used as screening tools and perhaps to spatially delineate locations where the street canyon models should be invoked which is often difficult for a large and complex urban environment where resources do not permit thorough investigation and spatial treatment of the morphological conditions 4 conclusions we developed a kernel based dispersion model rapidair combining aermod and open source scientific computing methods to estimate pollution concentrations at fine spatial resolution model input data was obtained from public sources to allow comparison with pollution models for the same location with the same input data the rapidair dispersion model took approximately 7 min to model the greater london conurbation 3500 km2 at 5 5 m resolution using an intel i5 64 bit laptop with 8 gb ram we evaluated nox and no2 model predictions at 86 sites across london after correction for systematic under estimation bias in the initial rapidair model fac2 values for modelled concentrations were 0 85 at the 86 evaluation sites rmse values decreased through the site categories kerbside roadside urban background and suburban rmse 45 14 6 and 4 μg m3 respectively this finding is consistent with results from other modelling groups participating in the defra inter comparison whose rmse values ranged from 3 to 70 μg m3 respectively the larger rmse values at the sites in proximity to traffic sources may have resulted from the presence of street canyons that trap pollutants leading to elevated concentrations an effect that cannot be described in dispersion models unless urban morphologies are taken into consideration correspondingly we used geospatial surrogates sky view factor hill shading and wind effect and separate street canyon models street and aeolius to improve modelled concentrations at roadside sites the street canyon model and street canyon surrogates improved the model rmse at kerbside sites rapidair base kernel 45 2 sky view factor surrogate 44 0 street model 38 9 and aeolius 42 8 μg m3 when all sites were considered the lowest rmse values were observed for the kernel model combined with the street canyon model rmse rapidair base kernel 17 1 vs street model 15 9 μg m3 consequently the combined models may be anticipated to provide more accurate estimates when extrapolated to locations without monitoring the geospatial surrogates have potential as simple means of incorporating canyon effects into a large city scale dispersion model the advantage of using simple geospatial surrogates for street canyons instead of modelling canyons discretely include reduced run times smaller user input required and the transition from built up to open environments is treated gradually acknowledgements we gratefully acknowledge the work of professor bryan harris at the university of bath who previously developed a mathcad version of the aeolius model and who diligently described the model formulations he obtained from the met office s original author dr d r middleton and much of our implementation of the aeolius model is based on professor harris description of the system of equations which make up aeolius harris 2004 see appendix a nicola masey is funded through a uk natural environment research council case phd studentship ne k007319 1 with support from ricardo energy and environment we acknowledge access to the laqn measurement data and the pcm model data which were obtained from uk air defra gov uk and are subject to crown 2014 copyright defra licensed under the open government licence ogl rapidair is a registered trade mark of ricardo aea limited used under permission of ricardo aea limited appendix a supplementary data the following is the supplementary data related to this article appendix appendix appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2018 05 014 
26357,time series analysis with explanatory variables a systematic literature review paula medina maçaira antônio marcio tavares thomé fernando luiz cyrino oliveira ana luiza carvalho ferrer pontifícia universidade católica do rio de janeiro industrial engineering department rua marquês de são vicente 225 gávea rio de janeiro rj 22451 900 brazil pontifícia universidade católica do rio de janeiro industrial engineering department rua marquês de são vicente 225 gávea rio de janeiro rj 22451 900 brazil corresponding author time series analysis with explanatory variables encompasses methods to model and predict correlated data taking into account additional information known as exogenous variables a thorough search in literature returned a dearth of systematic literature reviews slr on time series models with explanatory variables the main objective is to fill this gap by applying a rigorous and reproducible slr and a bibliometric analysis to study the evolution of this area over time the study resulted in the identification of the main methods of time series that incorporate input variables per knowledge area and methodology the largest number of papers belongs to environmental sciences followed by economics and health regression model is the method with the highest number of applications followed by artificial neural networks and support vector machines which experienced rapid and recent growth a research agenda in time series analysis with exogenous variables closes the paper keywords regression analysis artificial intelligence exogenous variables forecast scenarios 1 introduction time series modelling involves the analysis of a dynamic system characterised by inputs and outputs series which relates to a function regardless of their ultimate purpose the various techniques in this field have the mutual goal of reproducing the output series with reliability and accuracy from the estimation of the function and input series time series techniques can essentially be divided into two sets of methods univariate and multivariate in the case of univariate approaches the output series is explained by a constant portion and or trend seasonality and in many cases by the series lagged in time multivariate methods on the other hand use the influence of other variables on the behaviour of the output series to obtain better results in the representation of the transfer function one of the main areas of application of such methods is in the environmental science espey et al 1997 estimated an econometric model to evaluate the price elasticity of residential demand of water using rate structure location season and others exploratory variables nunnari et al 2004 compared several statistical techniques for modelling so2 concentration using the information of wind direction wind speed solar radiation temperature and relative humidity andriyas and mckee 2013 used biophysical conditions in farmers fields and the irrigation delivery system during the growing season to anticipate irrigation water orders lima et al 2014 developed a forecasting model for the water inflow incorporating the effect of climate variables like precipitation and el niño there are studies showing the advantages and disadvantages of using both univariate and multivariate approaches the work of athanasopoulos et al 2011 performed a competition between univariate and multivariate methods for predicting the international demand of tourists sfetsos and coonick 2000 compared the approaches on predicting solar radiation and porporato and ridolfi 2001 forecasted river flows there are equally reviews of methods and techniques of both univariate and multivariate time series applied to specific areas milionis and davies 1994 for example reviewed regression methods and stochastic models applied to the analysis of air pollution ljung 1999 describes the theory methodology practice of armax models and nonlinear black box models among other durbin and koopman 2012 published a clear and comprehensive introduction to the state space approach to time series analysis together with a historical background and haykin 1999 presents an extensive state of the art review of neural networks young 2011 offers an introduction to recursive estimation and demonstrates its various forms and its use as an aid in the modelling of stochastic dynamic systems in time series however there is a paucity of systematic literature reviews of methods of time series analysis using exogenous variables in the modelling structure regardless of the application area or methodology given the relevance of time series analysis with exogenous variables the main objective is to fill the gap identified in the existing literature on models that use explanatory variables providing a science map through a systematic literature review slr and bibliometric analysis according to small 1999 p 799 a map of science is a spatial representation of how disciplines fields specialities and individual documents or authors are related to one another and can provide insight into a contemporaneous state of knowledge thus this study seeks to guide researchers and practitioners from various knowledge areas to identify what are the main areas of application of time series analysis with exogenous variables who are the most prolific authors by knowledge area what are the most influential papers and what are the main methods used this paper comprises this introduction followed in section 2 by a description of the methods applied to the literature review and bibliometric analysis section 3 presents the results from citation co citation and co word analyses and section 4 encompasses discussions and conclusions with implications for practice and future research 2 methodology this section presents the methods and basic statistics extracted from the slr as well as the bibliometric methods applied to the longitudinal analysis of thematic areas 2 1 systematic review and basic statistics thomé et al 2016a developed a step by step approach to conduct an slr in operations management consisting of eight steps i planning and formulating the problem ii searching the literature iii data gathering iv quality evaluation v data analysis and synthesis vi interpretation vii presenting the results and viii updating the review in step 1 planning and formulating the problem the research team comprised of the co authors of this paper assembled and defined the scope of the review the conceptualisation of the main research topic and the following research questions rq rq 1 who are the most prolific authors in time series analysis with explanatory variables rq 2 which are the most influential works in time series analysis with explanatory variables rq 3 which are the main themes in time series analysis with explanatory variables and how did they evolve rq 4 which are the main methods applied in time series analysis with explanatory variables and how did they evolve following the second slr step a literature search was carried out in seven stages comprising the selection of databases definition of keywords review of abstracts criteria for inclusion exclusion of papers full text review and backward and forward search in selected papers references the scopus database was chosen because it is one of the largest abstract and citation databases of research literature containing over 53 million records and almost 22 000 titles from 5000 publishers hlwiki 2017 the choice of scopus is justified on the grounds of the large coverage of research domains intended by the present slr see for example mongeon and paul hus 2016 there was no time restriction for the search table 1 shows the number of papers included in each phase of the keyword search keywords should be broad to do not artificially restrict the number of studies and specific enough to bring only the studies related to the topic cooper 2010 the first keywords were time series and exogenous variable generating 244 papers the search was later expanded with synonyms of exogenous in time series analysis leading to 2020 papers this was intended to comply with the two criteria for searching studies in slr suggested by petticrew and roberts 2006 p 81 sensitivity to retrieve everything of relevance and specificity to leave behind the irrelevant the papers exclusion criteria are the language of the paper and document type resulting in 1930 papers written in english the limitation of document type to article review and articles in press narrowed the selection to a final set of 1547 documents included in the bibliometric analysis the full bibliographic reference is available upon request to the lead author table 2 presents the top 10 areas with the greatest concentration of papers from 28 knowledge areas identified as expected there is a large array of subject areas ranging from computer science to medicine another consistent result is the concentration of 20 of the papers in the environmental area environmental science earth and planetary sciences given the complexity of natural phenomena series and the need to use models that incorporate outside information environmental science mathematics and medicine are responsible for approximately 31 of the total number of papers together with social sciences and economics econometrics and finance the first five areas correspond to approximately 50 of the total fig 1 illustrates the number of papers published by year the first paper appeared in 1967 scott jr and heady 1967 about the demand for new investment in farm buildings in the next three years there were no publications it is worth noting that until 1995 the total number of published articles per year was consistently lower than 20 publications peaked at 27 in 1996 five years later in 2001 the number of publications reached the mark of over 40 published articles per year there has been a fast growing trend since then reaching a record of 150 publications in 2015 and 124 publications in 2016 when analysing the distribution of papers by journals a large variety of 151 different sources emerges as expected in such a multidisciplinary field the top 20 journals in the number of citations are in table 3 together they account for 73 of the total number of citations the highest ranked journal in the number of citations relates to the economy where time series models are extensively used the journal of econometrics is also the journal with the highest number of publications in the theme it is equally worth mentioning the large presence of journals related to the environment environmental modelling and software water resources research journal of climate remote sensing of environment journal of hydrology international journal of climatology ecological modelling atmospheric environment water research ecological economics and water resources management together they represent approximately 30 of the total number of citations and 39 of papers respectively however spearman s rank correlation between the number of publications and the total number of citations equals 0 57 an average value showing that some of the most prolific journals are not necessarily the most influential ones the third and fourth step of the slr is data gathering and quality evaluation a computer template was used for data gathering and the calculation of simple frequencies and means the restriction of selected papers to peer reviewed journals provided an initial gauge of the quality of the papers retrieved for analysis the fifth slr step is data analysis and synthesis an inductive approach to qualitative content analysis was adopted seuring and gold 2012 thomé et al 2016a and combined with quantitative bibliometric analysis of co citations and co occurrence of keywords described in subsection 2 2 subsequently the sixth step interpretation refers to a qualitative research synthesis coupled with bibliometric indicators from the co citation and co word analyses this study consists of the seventh step of the slr approach presentation of results the step eight updating of the review lies past the purview of this study 2 2 bibliometric analysis bibliometric measures and indicators can be employed to carry out performance analyses and generate scientific maps these procedures quantify and measure the performance and impact of scientific research cobo et al 2011 three specific analyses are carried out citation co citation and co word analyses citation analysis is the first and simplest analysis performed in the database this analysis involves the study of the most influential authors measuring the number of papers and citations by knowledge areas institution and country co occurrence is the study of mutual appearances of pairs of units over a number of bibliographic records there are different types of bibliometric analysis of co occurrence co citation co author and co word analyses being the most common ones persson et al 2009 for instance if document b and c cite document a they both appear as co citing a this method can be used to describe the backbone of a research area mapping the network of co citation among the most influential authors it allows the identification of the main research streams 2 2 1 software and data availability there is a large number of software available for bibliometric analysis in literature reviews as evidenced by cobo et al 2012 for the citation co citation and co word analyses software bibexcel persson et al 2009 http homepage univie ac at juan gorraiz bibexcel pajek de nooy et al 2005 http mrvar fdv uni lj si pajek and scimat cobo et al 2012 http sci2s ugr es scimat were used all software is available free ware for non profit academic use bibexcel was used to prepare the matrix of co citation and generate the clusters pajek to prepare the network analysis and scimat to analyse dynamic maps of longitudinal co word analysis of themes using callon s thematic bipartite graphs or strategic diagrams cobo et al 2012 depicting the density and centrality of co occurrences of author s keywords the full dataset containing 1547 documents included in the bibliometric analysis is made available at mendeley repository thomé et al 2017 2 2 2 callon s strategic diagram bibliometric indexes fig 2 shows callon s strategic diagram callon et al 1991 following callon s strategic diagram in fig 2 the motor clusters top right represent the core themes of the research area with high centrality and high density the basic and transversal clusters bottom right are central themes that are also key to the research field but are not well developed as they combine high centrality with low density emerging or declining themes bottom left present low centrality and low density meaning they do not relate well to other themes and are not well represented in the research area highly developed and isolated clusters top left are usually well researched areas with high density commonly denoting classic themes in the research field the similarity index measures co occurrence it considers the number of documents in which two keywords occur and the number of documents in which each one occurs clusters are formed in scimat with the application of the simple centre algorithm cobo et al 2011 thomé et al 2016b in the simple centre algorithm the centrality and density are calculated as a linear function of the similarity index the similarity index is calculated as e i j c i j 2 c i c j with c i j being the number of documents in which two keywords i and j co occur c i and c j being the number of documents in which each one occurs in one hand the centrality is a measure of the interaction among networks of keywords calculated as c 10 e k h where k is a keyword belonging to the theme and h is a keyword belonging to the other theme on the other hand the density measures the internal strength of the network or theme calculated as d 100 e i j w where i and j are keywords belonging to the theme and w is the total number of keywords in the theme 3 results from citation co citation and co word analyses 3 1 citation analysis in an attempt to answer the first research question who are the most prolific authors in time series analysis with explanatory variable the citation analysis scrutinises the number of citations and number of papers by author table 4 depicts the ranking of authors by number of citations number of papers institution country and subject area in table 4 the number of papers per author and the number of citations per author is the number of articles and the sum of all citations from a given author appearing in the database respectively the most prolific author beck n is also the author with the highest number of citations however the number of documents and number of citations might not correlate in fact spearman s rank correlation among the number of papers and citations is not significant p 0 13 another look at table 4 reveals that the main areas for these authors are social science and economics and they are located mainly in the us and uk beck s most cited work found in the database beck et al 1998 is co authored with katz and tucker it presents a solution to analyse time series cross section data using logit analyses and applies the proposed methodology to estimate the relationship between economic interdependence democracy and peace it is worth mentioning that beck katz and tucker are frequent co authors in studies with time series cross section data meese and rogoff 1983 have the second most cited paper in the database their paper compares the out of sample forecasting accuracy of various structural and time series exchange rate models the work of anderson and hsiao 1982 comes next in number of citations it presents a statistical analysis of time series regression models for longitudinal data with and without lagged dependent variables wilby wigley and conway co authored the work that has more citations in the database for each author wilby et al 1998 it calibrates a range of different statistical downscaling models using both observed and general circulation models to generate and compare daily precipitation time series as expected most frequently cited authors published earlier since these authors had more time to obtain citations than authors that are more recent did 3 2 co citation analysis the co citation analysis addresses the second research question which are the most influential works in time series analysis with explanatory variables the co citation presents the most influential works in time series analysis with exogenous variables that is the backbone of the studied area the network in fig 3 illustrates the co citation relationships among the top 30 documents with the largest number of citations that is the most influential works in time series analysis with explanatory variables documents appear in fig 3 chronologically from top to bottom the sizes of the circumference of the circles are proportional to the number of citations and the width of the lines connecting the network is proportional to the number of co occurrences the following describes the major contribution of each node of the co citation tree granger 1969 defined the difference between causality and feedback by using the simple example of bivariate models he introduced the use of causal lag and causal strength to analyse the direction of causality between two related variables nash and sutcliffe 1970 discussed the river flows from rainfall evaporation and other factors by observing the r 2 coefficient from a linear regression the popular work of box and jenkins 1970 introduced the autoregressive integrated moving average arima time series models to characterise and predict time series observations at equally spaced periods this classic model was revised by the same authors in 1976 there are three different methods derived from the arima model autoregressive ar moving average ma and ar moving average arma models ar models are applied to a time series that can be represented by its own past values while ma models are used to represent a time series where the past errors disturbances determine its future values arma models are appropriate when a series is a function of unobserved errors as well as its own past behaviour the difference between arima and arma is that the first one is applied to series that are not stationary over time that is with some trend and thus need to be differentiated the i term to become stationary the akaike information criterion aic was originally developed under the name an information criterion by akaike 1973 and received its full denomination of aic in 1974 along with a formal definition of concepts the aic assists in model selection to estimate the relative quality of a model based on information loss and parsimony schwarz 1978 tried to solve the problem of selecting one from a number of models of different dimensions and ljung and box 1978 presented a modification to the lack of fit test proposed by box and pierce 1970 the dickey fuller test derives representations for the limiting distributions of the ar coefficient estimator and tests the null hypothesis of whether a unit root is present dickey and fuller 1979 later philips and perron 1988 developed a unit root test called phillips perron test that makes a nonparametric correction to the t test statistic present in the dickey fuller test heteroscedasticity in the disturbances of a properly specified linear model leads to inefficient parameter estimates which results in faulty inferences when testing statistical hypotheses white 1980 developed the so called white test to verify if there is heteroscedasticity in the disturbances of a linear model still dealing with heteroscedasticity engle 1982 introduced a new class of stochastic process called ar conditional heteroscedasticity arch that does not assume constant variance for the error term this type of process assumes that the error variance follows an ar model in 1986 bollerslev 1986 extended the idea of engle 1982 by introducing a more general class of process called generalised ar conditional heteroscedasticity garch which allows a much more flexible lag structure and admits an arma model to the error structure entering in the artificial intelligence ai area the first main reference appears in 1985 with the study of takagi and sugeno 1985 which suggested a mathematical tool to describe a system that can represent highly nonlinear relations fuzzily the main contribution of the study is dealing with fuzzy system representation as a linear system equally in the area of ai rumelhart et al 1986 presented a procedure called error propagation whereby the gradient can be determined by individual units of the network based only on locally available information working with the definition of co integration engle and granger 1987 suggested a test for estimating co integration relations using regression and combining the problems of unit root tests and test with parameters unidentified under the null in johansen s study 1988 instead of working with regression estimates to test co integration the author derived a maximum likelihood estimator that takes into account the error structure of the underlying process which is not possible with the regression estimates the method proposed earlier was modified by adding linear restrictions on the co integration vectors and ar weights in the method of johansen and juselius 1990 johansen 1991 presented maximum likelihood estimators and likelihood ratio tests for co integrations in gaussian vector ar models which allow for the introduction of the constant term and seasonal dummies in the model in 1989 harvey 1989 published a book that provided a unified and comprehensive theory of structural time series models and included a detailed treatment of the kalman filter various applications illustrated the properties of the models and methodological techniques returning to references that deal with ai methods hornik et al 1989 took the first step in a rigorous general investigation on the capabilities and properties of multilayer feedforward networks by establishing that this particular type is a class of universal approximation the book written by mccullagh and nelder 1989 provided a unified treatment of methods for the analysis of diverse types of data the work focused on examining the way a response variable depends on the combination of explanatory variables in his book non linear time series a dynamical system approach tong 1990 introduced the nonlinear time series theory and provided state of the art research at that time his book bridged the gap between linear and chaotic time series analysis becoming one of the main references in nonlinear time series the fuzzy modelling and identification first explored by takagi and sugeno 1985 is the main inspiration for the architecture called adaptive network based fuzzy inference system anfis developed by jang 1993 by using a hybrid learning procedure the proposed anfis can construct an input output mapping based on both health studies knowledge and stipulated input output data pairs in 1994 two books that covered the tools for modelling and analysing time series and introduced the latest developments that have occurred in the field over the past decade were published box et al 1994 published a new edition of the classical time series analysis forecasting and control from box and jenkins 1970 while hamilton 1994 explored the first edition of a book that synthesises the advances in economic and financial time series the work of beck and katz 1995 is the first reference that appears in the co citation network that deals with the transversal study a technique widely used in medical research and social sciences their work examined some issues in the estimation of time series cross sectional models and proposed a new method a book presenting a detailed analysis of artificial neural networks anns did not appear until 1999 when haykin 1999 presents an extensive state of the art ann main methods and background with the growth of computer power processing and consequently the use of ai methods in time series hybrid models such as the one from zhang 2003 which combine the classical time series models arima ai and anns to provide better error measures in time series forecasting appeared this methodology in particular takes advantage of the unique strength of arima and ann models in linear and nonlinear modelling the most recent reference in the co citation network is from zuur and pierce 2004 which used a technique called dynamic factor analysis presented in zuur et al 2003 to estimate common trends in time series of squid catch per unit of effort instead of applying arima models the method also allows the determination of the relationship between independent and exploratory variables such as sea surface temperature and the north atlantic oscillation index influence in the time series of squid catch with the co citation analysis it is possible to conclude that the works that primarily influence the researches in the studied area are also the most important references in time series methods and provided the main statistical tests the co citation tree from fig 3 offers a chronological guide to the study of time series methods with exogenous variables 3 3 co word analysis the co word analysis intends to answer the last two research questions which are the main themes in time series analysis with explanatory variables and how did they evolve and which are the main methods applied in time series analysis with explanatory variables and how did they evolve in order to allow a better understanding of the evolution of themes and methods and to portray research that is more recent in the field documents were subdivided into four successive periods 1967 1998 1999 2007 2008 2012 and 2013 2016 with 246 423 434 and 444 documents respectively the choice of the duration of the periods was made from the equalisation of the numbers of articles from the most recent to the oldest providing the final three periods with an average of around 400 articles and the last with about 250 the main themes in the research area by period are in table 5 with the number of core and secondary documents retrieved for analysis core documents present at least two co occurrences of keywords while secondary documents present a single co occurrence of keywords in the thematic cluster full text reading of the core documents corresponding to up to 80 of total citations within the cluster provided the basis for the analysis of each thematic cluster for example in the 1967 1998 period the six out of 16 core documents regrouping up to 80 of total citations for the forecasting cluster first line of table 5 were content analysed for the identification of themes and methods for the forecasting cluster in this period fig 4 shows the number of keywords per period and presents their evolution by showing the number of outgoing and incoming keywords and the number and percentage of keywords that remain from one period to the next as expected the number of keywords grows throughout the periods paralleling the increase in the number of documents over the years the number of keywords increases from 130 in the first period 1967 1998 to 377 in the fourth period 2013 2016 a 290 growth of the 130 words that appear in the first period 92 71 remain for the second period and 194 are added totalling 286 words for the third period 218 76 remain and 171 new words are included accounting for 389 in total for the fourth period 241 62 keywords from the third period remain and 136 new words appear resulting in 377 keywords this can be indicative that the areas that use explanatory variables in time series are diversifying and increasing over time and hence this is not yet a consolidated field the most frequently used keyword in all the periods is human followed by forecast regression analysis neural networks and algorithms the words mathematical models demography and developing countries appear with high frequency in the first period air pollution ozone temperature climate change and environmental monitoring are frequently used words from the second to the last period in the last period the word risk assessment and economic growth are used more prominently than in the previous periods fig 5 shows the evolution of themes per period based on the strength of the association among themes from one period to the next the inclusion index measures the strength of the association represented by the thickness of the lines connecting each cluster the inclusion index varies from zero to one where zero means that the thematic areas are not connected and is measured as u v min u v where u and v are disjoint sets themes in different periods the continuous lines represent a name association between thematic clusters i e either two clusters have the same name in consecutive periods or one thematic cluster contains the other one and the dotted lines represent associations on aspects other than the name the sphere sizes correspond to the number of core documents present in the cluster from table 5 and fig 5 in 1967 1998 forecasting represents 53 of the total number of core documents in the period while in 1999 2007 health studies and mathematical models are the main themes responsible for 48 and 39 of core documents respectively in the following two periods 2008 2012 and 2013 2016 the main theme is again forecasting accounting for 53 and 55 of the core documents respectively the second main theme is health studiesis in both periods responsible for 36 and 32 of core documents respectively the forecasting cluster in 1967 1998 continues as mathematical models in 1999 2007 and as forecasting for the remaining two periods the health studies area remains throughout all the periods but merges with statistical models in the third period which split into costs and health studies in the following period in the first period two isolated clusters are present developed countries and kalman filter an isolated theme decision trees appears only in the last period the algorithms cluster in 2008 2012 originates from mathematical models and remains with the same name in the next period the forecasting theme gathers studies that focus on the evaluation of prediction methods health studies concentrates on works related to health behaviour developed countries gathers works applied to countries such as sweden and australia and kalman filter focuses on works that use the filter papers that applied ai techniques form the mathematical models and algorithms clusters air pollutant cluster concentrates on works that deal with the influence of air quality in the development of diseases papers that applied decision trees methodology form the decision trees theme and costs cluster presents works related to prices and costs fig 6 synthesizes the evolution of strategic thematic diagrams for the four periods where the sizes of the spheres are proportional to the number of core documents in each cluster and period according to the positioning on the callon s diagram of fig 6 motor clusters evolved from forecasting and developed countries in 1967 1998 to health studies and air pollutant in 1999 2007 to health studies and forecasting in 2008 2012 and 2013 2016 since 2008 the core thematic clusters appear with a clear concentration in the health area and forecasting methods and techniques 3 4 main methods in time series analysis in order to identify the main methods applied to time series with explanatory variables a thorough reading of 150 articles corresponding to core documents responding to 80 of total citations was carried out only the methods that incorporate explanatory variables were reviewed that is if an article used exponential smoothing multiple linear regression and dynamic regression only the last two methods were attributed to that article through the analysis it was possible to identify 30 different types of methods present in the core documents the method applied more often was regression models followed by anns box and jenkins arima with the incorporation of explanatory variable arimax support vector machines svms and structural models which together are present in 70 of the papers fig 7 displays the temporal evolution of the top five methods despite the predominance of regression models and anns the use of svms is increasing over time the top five methods are summarised and discussed next 3 4 1 regression models according to the co word analysis in the period 1967 1998 regression models were the second most applied method led by goldstein et al 1994 who proposed a model that can incorporate explanatory variables to time series data where the measurements are made close together in time resulting in a possible correlation in the residuals goldstein et al 1994 were classified in the health studies cluster from 1999 to the present day regression type models lead the number of applications in 1999 2007 the most cited work that uses regression models is marcellino et al 2006 who compared forecasts from linear univariate and bivariate models to forecast monthly macroeconomic time series in 2008 2012 the study of sistrom et al 2009 the most cited work among regression models in this period was classified as health studies sistrom et al 2009 used piecewise linear regression to determine the effects of computerised order entry with integrated decision support on the growth of outpatient procedures in 2013 2016 the paper from ishak et al 2013 led the work in regression models and svms focused on the development of a hybrid model that uses two regression models anns and svms to better estimate wind speed using hydro meteorological parameters 3 4 2 artificial neural networks ann in 1967 1998 anns are led by the work of jorquera et al 1998 classified in the forecasting cluster which compared methods of linear time series anns and fuzzy models to forecast daily maximum ozone levels in the following periods gonzález et al 2005 proposed an input output hidden markov model to analyse and forecast electricity spot prices coman et al 2008 evaluated a dynamic model versus a static one using multilayer perceptron mlp in the context of predicting hourly ozone concentrations fernandez et al 2009 used fuzzy neuro networks to improve wastewater flow rate forecasting guresen et al 2011 evaluated the effectiveness of ann models in stock market predictions nourani et al 2013 applied a feed forward neural network to model a rainfall runoff process on a daily and multi step ahead of time scale 3 4 3 arimax in 1967 1998 the most prevalent method in number of papers is arimax where the work of yang et al 1996 is the most cited paper in their work classified in the forecasting cluster a new evolutionary programming approach to identify the armax model for one day to one week ahead hourly load demand forecasts is proposed the study of low et al 2006 is the most cited work that uses arimax methodology in the second period using stroke admissions as the response variable and day of the week holidays september 11 and other counts and levels as explanatory variables in the third period pisoni et al 2009 used a nonlinear ar model with exogenous variables to forecast peak air pollution levels the study by marcilio et al 2013 is the leading work in arimax methods in the last period used generalised linear models generalised estimating equations and seasonal ar integrated moving average with and without the effect of mean daily temperature as a predictive variable to forecast daily emergency department visits 3 4 4 support vector machine svm according to the co word analysis the two most prominent works that apply svr are lu et al 2009 that applied independent component analysis and support vector regression to forecast financial time series and ishak et al 2013 that use svr to accurately estimate wind speed 3 4 5 structural models in structural models the most prominent work in the period 1967 1998 is from velicer et al 1996 who developed a theoretical model that attempts to define more appropriate multivariate sets of dependent variables in the study of health behavior change allocated in the health studies cluster the second period is led by hernán et al 2002 who used marginal structural models to estimate the effect of zidovudine therapy on mean cd4 count among hiv infected men in the third period athanasopoulos and hyndman 2008 used a regression framework to estimate important economic relationships for domestic tourism demand in australia and innovation state space models to forecast the same the main work that applied structural models in the fourth period is from bergel hayat et al 2013 which highlighted the link between weather conditions and the risk of a road accident table 6 summarises the strengths and weaknesses of each method presented in fig 7 as well as the most prominent papers for the same periods extracted from table 5 and fig 5 4 conclusion this study applied a rigorous and reproducible slr protocol resulting in the selection of 1547 papers related to time series analysis with explanatory variables time series analysis with exogenous variables is not a recent methodology with the first publication dating back to 1969 however there is an exponential growth in the number of publications after 1996 the increasing number of research subject areas that apply the models ranging from computer science to medicine shows the prominence of time series analysis with explanatory variables in a much diversified array of disciplines the largest number of papers belongs to environmental sciences as expected given the complexity of modelling and forecasting of such data as saying el niño phenomenon sunspot water runoff inflows and stream flows concentration of co2 air quality four research questions guided the analysis the citation analysis was applied to answer the first question who are the most prolific authors in time series analysis with explanatory variables citation analysis shows that the most prolific author in the research area is beck n based on the number of papers he is equally the author with the larger number of citations however there is not a direct correlation between the number of papers and citations the analysis equally evidenced the most cited research in time series with exogenous variables from social sciences and economic research originated in the us and uk based institutions the co citation analysis answered the second question which are the most influential works in time series analysis with explanatory variables results present the most influential works in the area of interest in chronological order and show that they are the references for the most prominent time series methods and the main statistical tests co citation analysis shows the backbone of seminal work in time series analysis with explanatory variables and can be a chronological guide to any researcher that wants to deepen into the subject the last two questions which are the main themes in time series analysis with explanatory variables and how did they evolve and which are the main methods applied in time series analysis with explanatory variables and how did they evolve were answered by applying co word analysis and dividing the database into four different periods the co word analysis shows the main contemporary research streams and their evolution forecasting and health studies themes are the prevalent themes over the four periods regarding the main methods box and jenkins arima with the incorporation of explanatory variables led in the first period 1967 1998 followed by regression models currently 2013 2016 regression models retain the first place as the method of choice and anns hold the second place in the number of applications followed by svms the growth of ai methods can be attributed to the increase in the processing capacity of computers and their attractive features being semiparametric learning machines permitting universal approximation of arbitrary linear and nonlinear functions from examples without a priori assumptions on the model structure and often outperforming conventional statistical approach methods crone et al 2006 table 7 summarises the answers found for each one of the research questions rq drawn in subsection 2 1 the incorporation of exogenous variables in the forecasting models are more prevalent in research aiming at improving forecast accuracy identifying the most important co variables that affects the response e g causality relationships among variables verifying the elasticity between the output variable and their inputs and generate synthetic scenarios for the dependent variable taking into account different scenarios for the independent variables considerable effort is equally devoted to the choice of the set of exogenous variables themselves as well as to the methods to forecast and simulate different scenarios this review has some limitations due to the broad scope covering different research areas to the choice of the search parameters keywords and exclusion criteria and of the citations databases scopus alone therefore offering a limited sample of relevant articles these limitations lead the path to new research avenues first as the review is not focused on a specific research area further in depth analysis and mapping of the time series methods with exogenous variables in the more prevalent research areas of environmental sciences economics and health seem to be opportune second the main methods outlined here could be further described with guidelines as to when and in which cases such methods should or should not be applied third the analysis of the evolution of themes and time series methods by discipline and periods could be pursued further it could lead to a much more detailed analysis of differentials in the research front of time series with explanatory variables in different disciplines fourth the combination of other databases beyond scopus can lead to a more exhaustive review despite the large intersection between documents figuring in different citation databases fifth the inclusion of non peer reviewed material such as thesis and dissertations can enrich the analysis and lead to the inclusion of new methods and applications yet to be published in peer reviewed journals circumventing the risk of publication bias associated with peer reviewed literature acknowledgement the authors acknowledge the support of the following brazilian agencies national council for scientific and technological development grants 304931 2016 0 404682 2016 2 and 443595 2014 3 and the research support foundation of the state of rio de janeiro faperj grants e 26 203 252 2017 and 202 806 2015 appendix a supplementary data the following is the supplementary data related to this article online data online data appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2018 06 004 
26357,time series analysis with explanatory variables a systematic literature review paula medina maçaira antônio marcio tavares thomé fernando luiz cyrino oliveira ana luiza carvalho ferrer pontifícia universidade católica do rio de janeiro industrial engineering department rua marquês de são vicente 225 gávea rio de janeiro rj 22451 900 brazil pontifícia universidade católica do rio de janeiro industrial engineering department rua marquês de são vicente 225 gávea rio de janeiro rj 22451 900 brazil corresponding author time series analysis with explanatory variables encompasses methods to model and predict correlated data taking into account additional information known as exogenous variables a thorough search in literature returned a dearth of systematic literature reviews slr on time series models with explanatory variables the main objective is to fill this gap by applying a rigorous and reproducible slr and a bibliometric analysis to study the evolution of this area over time the study resulted in the identification of the main methods of time series that incorporate input variables per knowledge area and methodology the largest number of papers belongs to environmental sciences followed by economics and health regression model is the method with the highest number of applications followed by artificial neural networks and support vector machines which experienced rapid and recent growth a research agenda in time series analysis with exogenous variables closes the paper keywords regression analysis artificial intelligence exogenous variables forecast scenarios 1 introduction time series modelling involves the analysis of a dynamic system characterised by inputs and outputs series which relates to a function regardless of their ultimate purpose the various techniques in this field have the mutual goal of reproducing the output series with reliability and accuracy from the estimation of the function and input series time series techniques can essentially be divided into two sets of methods univariate and multivariate in the case of univariate approaches the output series is explained by a constant portion and or trend seasonality and in many cases by the series lagged in time multivariate methods on the other hand use the influence of other variables on the behaviour of the output series to obtain better results in the representation of the transfer function one of the main areas of application of such methods is in the environmental science espey et al 1997 estimated an econometric model to evaluate the price elasticity of residential demand of water using rate structure location season and others exploratory variables nunnari et al 2004 compared several statistical techniques for modelling so2 concentration using the information of wind direction wind speed solar radiation temperature and relative humidity andriyas and mckee 2013 used biophysical conditions in farmers fields and the irrigation delivery system during the growing season to anticipate irrigation water orders lima et al 2014 developed a forecasting model for the water inflow incorporating the effect of climate variables like precipitation and el niño there are studies showing the advantages and disadvantages of using both univariate and multivariate approaches the work of athanasopoulos et al 2011 performed a competition between univariate and multivariate methods for predicting the international demand of tourists sfetsos and coonick 2000 compared the approaches on predicting solar radiation and porporato and ridolfi 2001 forecasted river flows there are equally reviews of methods and techniques of both univariate and multivariate time series applied to specific areas milionis and davies 1994 for example reviewed regression methods and stochastic models applied to the analysis of air pollution ljung 1999 describes the theory methodology practice of armax models and nonlinear black box models among other durbin and koopman 2012 published a clear and comprehensive introduction to the state space approach to time series analysis together with a historical background and haykin 1999 presents an extensive state of the art review of neural networks young 2011 offers an introduction to recursive estimation and demonstrates its various forms and its use as an aid in the modelling of stochastic dynamic systems in time series however there is a paucity of systematic literature reviews of methods of time series analysis using exogenous variables in the modelling structure regardless of the application area or methodology given the relevance of time series analysis with exogenous variables the main objective is to fill the gap identified in the existing literature on models that use explanatory variables providing a science map through a systematic literature review slr and bibliometric analysis according to small 1999 p 799 a map of science is a spatial representation of how disciplines fields specialities and individual documents or authors are related to one another and can provide insight into a contemporaneous state of knowledge thus this study seeks to guide researchers and practitioners from various knowledge areas to identify what are the main areas of application of time series analysis with exogenous variables who are the most prolific authors by knowledge area what are the most influential papers and what are the main methods used this paper comprises this introduction followed in section 2 by a description of the methods applied to the literature review and bibliometric analysis section 3 presents the results from citation co citation and co word analyses and section 4 encompasses discussions and conclusions with implications for practice and future research 2 methodology this section presents the methods and basic statistics extracted from the slr as well as the bibliometric methods applied to the longitudinal analysis of thematic areas 2 1 systematic review and basic statistics thomé et al 2016a developed a step by step approach to conduct an slr in operations management consisting of eight steps i planning and formulating the problem ii searching the literature iii data gathering iv quality evaluation v data analysis and synthesis vi interpretation vii presenting the results and viii updating the review in step 1 planning and formulating the problem the research team comprised of the co authors of this paper assembled and defined the scope of the review the conceptualisation of the main research topic and the following research questions rq rq 1 who are the most prolific authors in time series analysis with explanatory variables rq 2 which are the most influential works in time series analysis with explanatory variables rq 3 which are the main themes in time series analysis with explanatory variables and how did they evolve rq 4 which are the main methods applied in time series analysis with explanatory variables and how did they evolve following the second slr step a literature search was carried out in seven stages comprising the selection of databases definition of keywords review of abstracts criteria for inclusion exclusion of papers full text review and backward and forward search in selected papers references the scopus database was chosen because it is one of the largest abstract and citation databases of research literature containing over 53 million records and almost 22 000 titles from 5000 publishers hlwiki 2017 the choice of scopus is justified on the grounds of the large coverage of research domains intended by the present slr see for example mongeon and paul hus 2016 there was no time restriction for the search table 1 shows the number of papers included in each phase of the keyword search keywords should be broad to do not artificially restrict the number of studies and specific enough to bring only the studies related to the topic cooper 2010 the first keywords were time series and exogenous variable generating 244 papers the search was later expanded with synonyms of exogenous in time series analysis leading to 2020 papers this was intended to comply with the two criteria for searching studies in slr suggested by petticrew and roberts 2006 p 81 sensitivity to retrieve everything of relevance and specificity to leave behind the irrelevant the papers exclusion criteria are the language of the paper and document type resulting in 1930 papers written in english the limitation of document type to article review and articles in press narrowed the selection to a final set of 1547 documents included in the bibliometric analysis the full bibliographic reference is available upon request to the lead author table 2 presents the top 10 areas with the greatest concentration of papers from 28 knowledge areas identified as expected there is a large array of subject areas ranging from computer science to medicine another consistent result is the concentration of 20 of the papers in the environmental area environmental science earth and planetary sciences given the complexity of natural phenomena series and the need to use models that incorporate outside information environmental science mathematics and medicine are responsible for approximately 31 of the total number of papers together with social sciences and economics econometrics and finance the first five areas correspond to approximately 50 of the total fig 1 illustrates the number of papers published by year the first paper appeared in 1967 scott jr and heady 1967 about the demand for new investment in farm buildings in the next three years there were no publications it is worth noting that until 1995 the total number of published articles per year was consistently lower than 20 publications peaked at 27 in 1996 five years later in 2001 the number of publications reached the mark of over 40 published articles per year there has been a fast growing trend since then reaching a record of 150 publications in 2015 and 124 publications in 2016 when analysing the distribution of papers by journals a large variety of 151 different sources emerges as expected in such a multidisciplinary field the top 20 journals in the number of citations are in table 3 together they account for 73 of the total number of citations the highest ranked journal in the number of citations relates to the economy where time series models are extensively used the journal of econometrics is also the journal with the highest number of publications in the theme it is equally worth mentioning the large presence of journals related to the environment environmental modelling and software water resources research journal of climate remote sensing of environment journal of hydrology international journal of climatology ecological modelling atmospheric environment water research ecological economics and water resources management together they represent approximately 30 of the total number of citations and 39 of papers respectively however spearman s rank correlation between the number of publications and the total number of citations equals 0 57 an average value showing that some of the most prolific journals are not necessarily the most influential ones the third and fourth step of the slr is data gathering and quality evaluation a computer template was used for data gathering and the calculation of simple frequencies and means the restriction of selected papers to peer reviewed journals provided an initial gauge of the quality of the papers retrieved for analysis the fifth slr step is data analysis and synthesis an inductive approach to qualitative content analysis was adopted seuring and gold 2012 thomé et al 2016a and combined with quantitative bibliometric analysis of co citations and co occurrence of keywords described in subsection 2 2 subsequently the sixth step interpretation refers to a qualitative research synthesis coupled with bibliometric indicators from the co citation and co word analyses this study consists of the seventh step of the slr approach presentation of results the step eight updating of the review lies past the purview of this study 2 2 bibliometric analysis bibliometric measures and indicators can be employed to carry out performance analyses and generate scientific maps these procedures quantify and measure the performance and impact of scientific research cobo et al 2011 three specific analyses are carried out citation co citation and co word analyses citation analysis is the first and simplest analysis performed in the database this analysis involves the study of the most influential authors measuring the number of papers and citations by knowledge areas institution and country co occurrence is the study of mutual appearances of pairs of units over a number of bibliographic records there are different types of bibliometric analysis of co occurrence co citation co author and co word analyses being the most common ones persson et al 2009 for instance if document b and c cite document a they both appear as co citing a this method can be used to describe the backbone of a research area mapping the network of co citation among the most influential authors it allows the identification of the main research streams 2 2 1 software and data availability there is a large number of software available for bibliometric analysis in literature reviews as evidenced by cobo et al 2012 for the citation co citation and co word analyses software bibexcel persson et al 2009 http homepage univie ac at juan gorraiz bibexcel pajek de nooy et al 2005 http mrvar fdv uni lj si pajek and scimat cobo et al 2012 http sci2s ugr es scimat were used all software is available free ware for non profit academic use bibexcel was used to prepare the matrix of co citation and generate the clusters pajek to prepare the network analysis and scimat to analyse dynamic maps of longitudinal co word analysis of themes using callon s thematic bipartite graphs or strategic diagrams cobo et al 2012 depicting the density and centrality of co occurrences of author s keywords the full dataset containing 1547 documents included in the bibliometric analysis is made available at mendeley repository thomé et al 2017 2 2 2 callon s strategic diagram bibliometric indexes fig 2 shows callon s strategic diagram callon et al 1991 following callon s strategic diagram in fig 2 the motor clusters top right represent the core themes of the research area with high centrality and high density the basic and transversal clusters bottom right are central themes that are also key to the research field but are not well developed as they combine high centrality with low density emerging or declining themes bottom left present low centrality and low density meaning they do not relate well to other themes and are not well represented in the research area highly developed and isolated clusters top left are usually well researched areas with high density commonly denoting classic themes in the research field the similarity index measures co occurrence it considers the number of documents in which two keywords occur and the number of documents in which each one occurs clusters are formed in scimat with the application of the simple centre algorithm cobo et al 2011 thomé et al 2016b in the simple centre algorithm the centrality and density are calculated as a linear function of the similarity index the similarity index is calculated as e i j c i j 2 c i c j with c i j being the number of documents in which two keywords i and j co occur c i and c j being the number of documents in which each one occurs in one hand the centrality is a measure of the interaction among networks of keywords calculated as c 10 e k h where k is a keyword belonging to the theme and h is a keyword belonging to the other theme on the other hand the density measures the internal strength of the network or theme calculated as d 100 e i j w where i and j are keywords belonging to the theme and w is the total number of keywords in the theme 3 results from citation co citation and co word analyses 3 1 citation analysis in an attempt to answer the first research question who are the most prolific authors in time series analysis with explanatory variable the citation analysis scrutinises the number of citations and number of papers by author table 4 depicts the ranking of authors by number of citations number of papers institution country and subject area in table 4 the number of papers per author and the number of citations per author is the number of articles and the sum of all citations from a given author appearing in the database respectively the most prolific author beck n is also the author with the highest number of citations however the number of documents and number of citations might not correlate in fact spearman s rank correlation among the number of papers and citations is not significant p 0 13 another look at table 4 reveals that the main areas for these authors are social science and economics and they are located mainly in the us and uk beck s most cited work found in the database beck et al 1998 is co authored with katz and tucker it presents a solution to analyse time series cross section data using logit analyses and applies the proposed methodology to estimate the relationship between economic interdependence democracy and peace it is worth mentioning that beck katz and tucker are frequent co authors in studies with time series cross section data meese and rogoff 1983 have the second most cited paper in the database their paper compares the out of sample forecasting accuracy of various structural and time series exchange rate models the work of anderson and hsiao 1982 comes next in number of citations it presents a statistical analysis of time series regression models for longitudinal data with and without lagged dependent variables wilby wigley and conway co authored the work that has more citations in the database for each author wilby et al 1998 it calibrates a range of different statistical downscaling models using both observed and general circulation models to generate and compare daily precipitation time series as expected most frequently cited authors published earlier since these authors had more time to obtain citations than authors that are more recent did 3 2 co citation analysis the co citation analysis addresses the second research question which are the most influential works in time series analysis with explanatory variables the co citation presents the most influential works in time series analysis with exogenous variables that is the backbone of the studied area the network in fig 3 illustrates the co citation relationships among the top 30 documents with the largest number of citations that is the most influential works in time series analysis with explanatory variables documents appear in fig 3 chronologically from top to bottom the sizes of the circumference of the circles are proportional to the number of citations and the width of the lines connecting the network is proportional to the number of co occurrences the following describes the major contribution of each node of the co citation tree granger 1969 defined the difference between causality and feedback by using the simple example of bivariate models he introduced the use of causal lag and causal strength to analyse the direction of causality between two related variables nash and sutcliffe 1970 discussed the river flows from rainfall evaporation and other factors by observing the r 2 coefficient from a linear regression the popular work of box and jenkins 1970 introduced the autoregressive integrated moving average arima time series models to characterise and predict time series observations at equally spaced periods this classic model was revised by the same authors in 1976 there are three different methods derived from the arima model autoregressive ar moving average ma and ar moving average arma models ar models are applied to a time series that can be represented by its own past values while ma models are used to represent a time series where the past errors disturbances determine its future values arma models are appropriate when a series is a function of unobserved errors as well as its own past behaviour the difference between arima and arma is that the first one is applied to series that are not stationary over time that is with some trend and thus need to be differentiated the i term to become stationary the akaike information criterion aic was originally developed under the name an information criterion by akaike 1973 and received its full denomination of aic in 1974 along with a formal definition of concepts the aic assists in model selection to estimate the relative quality of a model based on information loss and parsimony schwarz 1978 tried to solve the problem of selecting one from a number of models of different dimensions and ljung and box 1978 presented a modification to the lack of fit test proposed by box and pierce 1970 the dickey fuller test derives representations for the limiting distributions of the ar coefficient estimator and tests the null hypothesis of whether a unit root is present dickey and fuller 1979 later philips and perron 1988 developed a unit root test called phillips perron test that makes a nonparametric correction to the t test statistic present in the dickey fuller test heteroscedasticity in the disturbances of a properly specified linear model leads to inefficient parameter estimates which results in faulty inferences when testing statistical hypotheses white 1980 developed the so called white test to verify if there is heteroscedasticity in the disturbances of a linear model still dealing with heteroscedasticity engle 1982 introduced a new class of stochastic process called ar conditional heteroscedasticity arch that does not assume constant variance for the error term this type of process assumes that the error variance follows an ar model in 1986 bollerslev 1986 extended the idea of engle 1982 by introducing a more general class of process called generalised ar conditional heteroscedasticity garch which allows a much more flexible lag structure and admits an arma model to the error structure entering in the artificial intelligence ai area the first main reference appears in 1985 with the study of takagi and sugeno 1985 which suggested a mathematical tool to describe a system that can represent highly nonlinear relations fuzzily the main contribution of the study is dealing with fuzzy system representation as a linear system equally in the area of ai rumelhart et al 1986 presented a procedure called error propagation whereby the gradient can be determined by individual units of the network based only on locally available information working with the definition of co integration engle and granger 1987 suggested a test for estimating co integration relations using regression and combining the problems of unit root tests and test with parameters unidentified under the null in johansen s study 1988 instead of working with regression estimates to test co integration the author derived a maximum likelihood estimator that takes into account the error structure of the underlying process which is not possible with the regression estimates the method proposed earlier was modified by adding linear restrictions on the co integration vectors and ar weights in the method of johansen and juselius 1990 johansen 1991 presented maximum likelihood estimators and likelihood ratio tests for co integrations in gaussian vector ar models which allow for the introduction of the constant term and seasonal dummies in the model in 1989 harvey 1989 published a book that provided a unified and comprehensive theory of structural time series models and included a detailed treatment of the kalman filter various applications illustrated the properties of the models and methodological techniques returning to references that deal with ai methods hornik et al 1989 took the first step in a rigorous general investigation on the capabilities and properties of multilayer feedforward networks by establishing that this particular type is a class of universal approximation the book written by mccullagh and nelder 1989 provided a unified treatment of methods for the analysis of diverse types of data the work focused on examining the way a response variable depends on the combination of explanatory variables in his book non linear time series a dynamical system approach tong 1990 introduced the nonlinear time series theory and provided state of the art research at that time his book bridged the gap between linear and chaotic time series analysis becoming one of the main references in nonlinear time series the fuzzy modelling and identification first explored by takagi and sugeno 1985 is the main inspiration for the architecture called adaptive network based fuzzy inference system anfis developed by jang 1993 by using a hybrid learning procedure the proposed anfis can construct an input output mapping based on both health studies knowledge and stipulated input output data pairs in 1994 two books that covered the tools for modelling and analysing time series and introduced the latest developments that have occurred in the field over the past decade were published box et al 1994 published a new edition of the classical time series analysis forecasting and control from box and jenkins 1970 while hamilton 1994 explored the first edition of a book that synthesises the advances in economic and financial time series the work of beck and katz 1995 is the first reference that appears in the co citation network that deals with the transversal study a technique widely used in medical research and social sciences their work examined some issues in the estimation of time series cross sectional models and proposed a new method a book presenting a detailed analysis of artificial neural networks anns did not appear until 1999 when haykin 1999 presents an extensive state of the art ann main methods and background with the growth of computer power processing and consequently the use of ai methods in time series hybrid models such as the one from zhang 2003 which combine the classical time series models arima ai and anns to provide better error measures in time series forecasting appeared this methodology in particular takes advantage of the unique strength of arima and ann models in linear and nonlinear modelling the most recent reference in the co citation network is from zuur and pierce 2004 which used a technique called dynamic factor analysis presented in zuur et al 2003 to estimate common trends in time series of squid catch per unit of effort instead of applying arima models the method also allows the determination of the relationship between independent and exploratory variables such as sea surface temperature and the north atlantic oscillation index influence in the time series of squid catch with the co citation analysis it is possible to conclude that the works that primarily influence the researches in the studied area are also the most important references in time series methods and provided the main statistical tests the co citation tree from fig 3 offers a chronological guide to the study of time series methods with exogenous variables 3 3 co word analysis the co word analysis intends to answer the last two research questions which are the main themes in time series analysis with explanatory variables and how did they evolve and which are the main methods applied in time series analysis with explanatory variables and how did they evolve in order to allow a better understanding of the evolution of themes and methods and to portray research that is more recent in the field documents were subdivided into four successive periods 1967 1998 1999 2007 2008 2012 and 2013 2016 with 246 423 434 and 444 documents respectively the choice of the duration of the periods was made from the equalisation of the numbers of articles from the most recent to the oldest providing the final three periods with an average of around 400 articles and the last with about 250 the main themes in the research area by period are in table 5 with the number of core and secondary documents retrieved for analysis core documents present at least two co occurrences of keywords while secondary documents present a single co occurrence of keywords in the thematic cluster full text reading of the core documents corresponding to up to 80 of total citations within the cluster provided the basis for the analysis of each thematic cluster for example in the 1967 1998 period the six out of 16 core documents regrouping up to 80 of total citations for the forecasting cluster first line of table 5 were content analysed for the identification of themes and methods for the forecasting cluster in this period fig 4 shows the number of keywords per period and presents their evolution by showing the number of outgoing and incoming keywords and the number and percentage of keywords that remain from one period to the next as expected the number of keywords grows throughout the periods paralleling the increase in the number of documents over the years the number of keywords increases from 130 in the first period 1967 1998 to 377 in the fourth period 2013 2016 a 290 growth of the 130 words that appear in the first period 92 71 remain for the second period and 194 are added totalling 286 words for the third period 218 76 remain and 171 new words are included accounting for 389 in total for the fourth period 241 62 keywords from the third period remain and 136 new words appear resulting in 377 keywords this can be indicative that the areas that use explanatory variables in time series are diversifying and increasing over time and hence this is not yet a consolidated field the most frequently used keyword in all the periods is human followed by forecast regression analysis neural networks and algorithms the words mathematical models demography and developing countries appear with high frequency in the first period air pollution ozone temperature climate change and environmental monitoring are frequently used words from the second to the last period in the last period the word risk assessment and economic growth are used more prominently than in the previous periods fig 5 shows the evolution of themes per period based on the strength of the association among themes from one period to the next the inclusion index measures the strength of the association represented by the thickness of the lines connecting each cluster the inclusion index varies from zero to one where zero means that the thematic areas are not connected and is measured as u v min u v where u and v are disjoint sets themes in different periods the continuous lines represent a name association between thematic clusters i e either two clusters have the same name in consecutive periods or one thematic cluster contains the other one and the dotted lines represent associations on aspects other than the name the sphere sizes correspond to the number of core documents present in the cluster from table 5 and fig 5 in 1967 1998 forecasting represents 53 of the total number of core documents in the period while in 1999 2007 health studies and mathematical models are the main themes responsible for 48 and 39 of core documents respectively in the following two periods 2008 2012 and 2013 2016 the main theme is again forecasting accounting for 53 and 55 of the core documents respectively the second main theme is health studiesis in both periods responsible for 36 and 32 of core documents respectively the forecasting cluster in 1967 1998 continues as mathematical models in 1999 2007 and as forecasting for the remaining two periods the health studies area remains throughout all the periods but merges with statistical models in the third period which split into costs and health studies in the following period in the first period two isolated clusters are present developed countries and kalman filter an isolated theme decision trees appears only in the last period the algorithms cluster in 2008 2012 originates from mathematical models and remains with the same name in the next period the forecasting theme gathers studies that focus on the evaluation of prediction methods health studies concentrates on works related to health behaviour developed countries gathers works applied to countries such as sweden and australia and kalman filter focuses on works that use the filter papers that applied ai techniques form the mathematical models and algorithms clusters air pollutant cluster concentrates on works that deal with the influence of air quality in the development of diseases papers that applied decision trees methodology form the decision trees theme and costs cluster presents works related to prices and costs fig 6 synthesizes the evolution of strategic thematic diagrams for the four periods where the sizes of the spheres are proportional to the number of core documents in each cluster and period according to the positioning on the callon s diagram of fig 6 motor clusters evolved from forecasting and developed countries in 1967 1998 to health studies and air pollutant in 1999 2007 to health studies and forecasting in 2008 2012 and 2013 2016 since 2008 the core thematic clusters appear with a clear concentration in the health area and forecasting methods and techniques 3 4 main methods in time series analysis in order to identify the main methods applied to time series with explanatory variables a thorough reading of 150 articles corresponding to core documents responding to 80 of total citations was carried out only the methods that incorporate explanatory variables were reviewed that is if an article used exponential smoothing multiple linear regression and dynamic regression only the last two methods were attributed to that article through the analysis it was possible to identify 30 different types of methods present in the core documents the method applied more often was regression models followed by anns box and jenkins arima with the incorporation of explanatory variable arimax support vector machines svms and structural models which together are present in 70 of the papers fig 7 displays the temporal evolution of the top five methods despite the predominance of regression models and anns the use of svms is increasing over time the top five methods are summarised and discussed next 3 4 1 regression models according to the co word analysis in the period 1967 1998 regression models were the second most applied method led by goldstein et al 1994 who proposed a model that can incorporate explanatory variables to time series data where the measurements are made close together in time resulting in a possible correlation in the residuals goldstein et al 1994 were classified in the health studies cluster from 1999 to the present day regression type models lead the number of applications in 1999 2007 the most cited work that uses regression models is marcellino et al 2006 who compared forecasts from linear univariate and bivariate models to forecast monthly macroeconomic time series in 2008 2012 the study of sistrom et al 2009 the most cited work among regression models in this period was classified as health studies sistrom et al 2009 used piecewise linear regression to determine the effects of computerised order entry with integrated decision support on the growth of outpatient procedures in 2013 2016 the paper from ishak et al 2013 led the work in regression models and svms focused on the development of a hybrid model that uses two regression models anns and svms to better estimate wind speed using hydro meteorological parameters 3 4 2 artificial neural networks ann in 1967 1998 anns are led by the work of jorquera et al 1998 classified in the forecasting cluster which compared methods of linear time series anns and fuzzy models to forecast daily maximum ozone levels in the following periods gonzález et al 2005 proposed an input output hidden markov model to analyse and forecast electricity spot prices coman et al 2008 evaluated a dynamic model versus a static one using multilayer perceptron mlp in the context of predicting hourly ozone concentrations fernandez et al 2009 used fuzzy neuro networks to improve wastewater flow rate forecasting guresen et al 2011 evaluated the effectiveness of ann models in stock market predictions nourani et al 2013 applied a feed forward neural network to model a rainfall runoff process on a daily and multi step ahead of time scale 3 4 3 arimax in 1967 1998 the most prevalent method in number of papers is arimax where the work of yang et al 1996 is the most cited paper in their work classified in the forecasting cluster a new evolutionary programming approach to identify the armax model for one day to one week ahead hourly load demand forecasts is proposed the study of low et al 2006 is the most cited work that uses arimax methodology in the second period using stroke admissions as the response variable and day of the week holidays september 11 and other counts and levels as explanatory variables in the third period pisoni et al 2009 used a nonlinear ar model with exogenous variables to forecast peak air pollution levels the study by marcilio et al 2013 is the leading work in arimax methods in the last period used generalised linear models generalised estimating equations and seasonal ar integrated moving average with and without the effect of mean daily temperature as a predictive variable to forecast daily emergency department visits 3 4 4 support vector machine svm according to the co word analysis the two most prominent works that apply svr are lu et al 2009 that applied independent component analysis and support vector regression to forecast financial time series and ishak et al 2013 that use svr to accurately estimate wind speed 3 4 5 structural models in structural models the most prominent work in the period 1967 1998 is from velicer et al 1996 who developed a theoretical model that attempts to define more appropriate multivariate sets of dependent variables in the study of health behavior change allocated in the health studies cluster the second period is led by hernán et al 2002 who used marginal structural models to estimate the effect of zidovudine therapy on mean cd4 count among hiv infected men in the third period athanasopoulos and hyndman 2008 used a regression framework to estimate important economic relationships for domestic tourism demand in australia and innovation state space models to forecast the same the main work that applied structural models in the fourth period is from bergel hayat et al 2013 which highlighted the link between weather conditions and the risk of a road accident table 6 summarises the strengths and weaknesses of each method presented in fig 7 as well as the most prominent papers for the same periods extracted from table 5 and fig 5 4 conclusion this study applied a rigorous and reproducible slr protocol resulting in the selection of 1547 papers related to time series analysis with explanatory variables time series analysis with exogenous variables is not a recent methodology with the first publication dating back to 1969 however there is an exponential growth in the number of publications after 1996 the increasing number of research subject areas that apply the models ranging from computer science to medicine shows the prominence of time series analysis with explanatory variables in a much diversified array of disciplines the largest number of papers belongs to environmental sciences as expected given the complexity of modelling and forecasting of such data as saying el niño phenomenon sunspot water runoff inflows and stream flows concentration of co2 air quality four research questions guided the analysis the citation analysis was applied to answer the first question who are the most prolific authors in time series analysis with explanatory variables citation analysis shows that the most prolific author in the research area is beck n based on the number of papers he is equally the author with the larger number of citations however there is not a direct correlation between the number of papers and citations the analysis equally evidenced the most cited research in time series with exogenous variables from social sciences and economic research originated in the us and uk based institutions the co citation analysis answered the second question which are the most influential works in time series analysis with explanatory variables results present the most influential works in the area of interest in chronological order and show that they are the references for the most prominent time series methods and the main statistical tests co citation analysis shows the backbone of seminal work in time series analysis with explanatory variables and can be a chronological guide to any researcher that wants to deepen into the subject the last two questions which are the main themes in time series analysis with explanatory variables and how did they evolve and which are the main methods applied in time series analysis with explanatory variables and how did they evolve were answered by applying co word analysis and dividing the database into four different periods the co word analysis shows the main contemporary research streams and their evolution forecasting and health studies themes are the prevalent themes over the four periods regarding the main methods box and jenkins arima with the incorporation of explanatory variables led in the first period 1967 1998 followed by regression models currently 2013 2016 regression models retain the first place as the method of choice and anns hold the second place in the number of applications followed by svms the growth of ai methods can be attributed to the increase in the processing capacity of computers and their attractive features being semiparametric learning machines permitting universal approximation of arbitrary linear and nonlinear functions from examples without a priori assumptions on the model structure and often outperforming conventional statistical approach methods crone et al 2006 table 7 summarises the answers found for each one of the research questions rq drawn in subsection 2 1 the incorporation of exogenous variables in the forecasting models are more prevalent in research aiming at improving forecast accuracy identifying the most important co variables that affects the response e g causality relationships among variables verifying the elasticity between the output variable and their inputs and generate synthetic scenarios for the dependent variable taking into account different scenarios for the independent variables considerable effort is equally devoted to the choice of the set of exogenous variables themselves as well as to the methods to forecast and simulate different scenarios this review has some limitations due to the broad scope covering different research areas to the choice of the search parameters keywords and exclusion criteria and of the citations databases scopus alone therefore offering a limited sample of relevant articles these limitations lead the path to new research avenues first as the review is not focused on a specific research area further in depth analysis and mapping of the time series methods with exogenous variables in the more prevalent research areas of environmental sciences economics and health seem to be opportune second the main methods outlined here could be further described with guidelines as to when and in which cases such methods should or should not be applied third the analysis of the evolution of themes and time series methods by discipline and periods could be pursued further it could lead to a much more detailed analysis of differentials in the research front of time series with explanatory variables in different disciplines fourth the combination of other databases beyond scopus can lead to a more exhaustive review despite the large intersection between documents figuring in different citation databases fifth the inclusion of non peer reviewed material such as thesis and dissertations can enrich the analysis and lead to the inclusion of new methods and applications yet to be published in peer reviewed journals circumventing the risk of publication bias associated with peer reviewed literature acknowledgement the authors acknowledge the support of the following brazilian agencies national council for scientific and technological development grants 304931 2016 0 404682 2016 2 and 443595 2014 3 and the research support foundation of the state of rio de janeiro faperj grants e 26 203 252 2017 and 202 806 2015 appendix a supplementary data the following is the supplementary data related to this article online data online data appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2018 06 004 
26358,continental scale flood discharge modeling requires a high level of efficiency and flexibility to this end this study documents the implementation and application of a vector based river routing model in the community wrf hydro modeling framework using hurricane ike as a case study the hybrid vector grid modeling framework s sensitivity to the land grid resolution and the coupling interface is assessed results show the model is more sensitive to the coupling interface than the grid resolution and a 1 km land grid with an area weighted coupling interface exhibits the optimal simulation results a geographic information system gis based approach is adopted to improve the regional representativeness of the flow travel time estimation the model s computational efficiency and complexity are compared to a grid based routing scheme demonstrating its advantages for large scale offline hydrological applications with gis supported features trade offs between the modeling efficiency and complexity are then discussed to inform future large scale flood prediction applications keywords vector based river routing wrf hydro rapid flood discharge simulation hurricane ike 1 introduction vector based river routing models are receiving increased attention by land surface modeling groups to facilitate large scale hydrologic predictions at fine resolutions david et al 2011 goodall et al 2013 lehner and grill 2013 liu and hodges 2014 mizukami et al 2016 yamazaki et al 2013 compared to traditional grid based routing models vector based schemes are conceptually more object oriented with fewer linkages among objects lehner and grill 2013 and hence computationally more efficient and generally more accurate in depicting hydrologic features yamazaki et al 2013 found a 60 increase in computational efficiency when shifting from a grid based to a vector based routing approach for a global model cama flood in addition to efficiency vector based models also demonstrate other advantages for example using a vector based scheme treating the river network as links and nodes liu and hodges 2014 successfully adapted the ideas from microprocessor designs to efficiently solve the fully nonlinear 1 d saint venant equations for large scale routing problems with o 105 elements vector based schemes can also provide accurate descriptions of hydrologic features such as river reaches dams and gauges at exact geographic locations mizukami et al 2016 in comparison representations of these features in a grid based scheme largely depend on the grid resolution but increasing the spatial resolution over a large spatial domain remains an unachieved priority bierkens et al 2015 singh et al 2015 wood et al 2011 using a grid based scheme li et al 2013 discussed that neglecting the subgrid travel time could pose problems for the routing performance under coarse resolutions these previous studies all suggested that efficient large scale hydrologic prediction at fine resolutions could potentially benefit from a hybrid framework that combines both vector based river routing and grid based land surface modeling however as river routing has been mostly practiced for channel design and floodplain analysis by civil engineers and geographic information system gis modeling groups its coupling with traditional grid based land surface models lsms for large scale applications remains a challenging task clark et al 2015 mizukami et al 2016 kauffeldt et al 2016 summarized after reviewing 24 hydrological models several key criteria that make a modeling system a suitable candidate for large scale operational applications those include availability of model code existing user community flexibility in resolution as well as flexibility in grid structures and modeling units in light of these postulated criteria we implemented the routing application for parallel computation of discharge model rapid david et al 2011 into the community hydrological extension of the weather research and forecasting model wrf hydro gochis et al 2015 currently in a one way manner meaning that there is no feedback from vector based river routing to land surface modeling these two open source models were developed for different user communities wrf hydro is an architectural framework for broad scope hydrometeorological studies arnault et al 2016 gochis et al 2015 yucel et al 2015 while rapid is mainly practiced with gis based water resource management and flood related applications follum et al 2017 lin et al 2015 tavakoly et al 2017 the advantages in combining these models were first demonstrated by the national flood interoperability experiment for a continental scale application in which the 15 hr lead streamflow forecasting for 2 7 million river reaches across the continental united states us was completed within 10 min using 16 computing cores at the texas advanced computing center tacc stampede cluster maidment 2017 lin et al 2018 salas et al 2018 although these promising applications can be conducted with loosely coupled modeling strategies i e the outputs of a model are generated processed and then used as inputs to another model the need for a tightly coupled wrf hydro rapid software package lies in its flexibility in data structure to allow for researchers with interdisciplinary backgrounds to conduct flood prediction studies in addition such a modeling system may also make it straightforward to address uncertainties originating from weather inputs land surface parameterizations and river routing schemes the first objective of this study therefore is to document the coupling implementation and configuration of the wrf hydro rapid modeling system we then assess the system s performance in terms of simulating flood discharge and the sensitivity to land grid resolutions and grid to vector coupling strategies using hurricane ike as a case study radar based rainfall data are used as the rainfall input and a gis based approach is adopted to estimate the flow travel time for the regional modeling simulation finally the trade offs between model efficiency and complexity are discussed by comparing rapid with the grid based routing option in wrf hydro in order to inform future model development and application activities 2 model description and coupling 2 1 the community wrf hydro modeling framework wrf hydro is the hydrological extension of the wrf model a widely used numerical weather prediction nwp tool for weather climate research and operations developed by the national center for atmospheric research ncar wrf hydro provides a model coupling architecture to link multi scale hydrometeorological processes gochis et al 2015 which can be broadly categorized as 1 the atmospheric models and meteorological input data stubs 2 lsms for calculating land states and fluxes in 1 d vertical columns and 3 routing modules for representing horizontal water movement fig 1 each of these modular layers can be set active or inactive to allow for stand alone or full feature modeling experiments by modifying the hydro namelist the supported full feature modeling grid based framework currently includes switch activated modules for surface overland flow saturated subsurface flow base flow in conceptual groundwater reservoirs i e buckets and channel flow processes several state of the art lsms e g noah and noah mp are now supported to benefit multi model predictions to this end wrf hydro is not a hydrological model but instead is an extensible modeling coupling framework to serve both coupled fig 1a brown arrows and uncoupled fig 1b green arrows hydrometeorological applications utilizing these modeling capabilities studies have been using wrf hydro to investigate the two way interactions between soil moisture redistribution and hillslope hydrology atmosphere feedbacks in west africa and the central mediterranean e g arnault et al 2016 senatore et al 2015 while some have focused on offline hydrological applications to study the one way hydrologic response to the weather forcings e g silver et al 2017 yucel et al 2015 the grid based routing in wrf hydro separates the explicit representation of terrain and channel routing processes gochis et al 2015 similar to li et al 2013 before water reaches a predefined channel cell switch activated modules for overland subsurface and baseflow buckets are used to route the flow which allows for updating surface head along the pathway dashed black arrows in fig 2 a once the water reaches the predefined network the channel routing module takes over to route water down the routing performance of such a grid based scheme is thus sensitive to both the land and the channel grid resolutions fig 2a for example given a fixed land grid resolution a coarser channel resolution larger grid spacing can resolve less tributaries of lower strahler orders this will result in lower tributary density in one lsm grid cell and in this case terrain routing becomes more important due to a lack of channel pathways to route the correct amount of water compared to the case with a finer channel resolution in this study we focus on augmenting the offline hydrological modeling capability of wrf hydro using a vector based routing scheme which has no requirement for a grid based channel mesh but instead directly runs on a dense vectorized channel network 2 2 the rapid river network routing model rapid is a river network routing model that can directly perform streamflow computations on the vector based geospatial data fabrics such as a continental scale dataset from nhdplus david et al 2011 2013 follum et al 2017 lin et al 2015 tavakoly et al 2017 and a global scale dataset from hydrosheds david et al 2015 these two datasets provide vectorized channel hydrography for use in modeling and efficient hazard communications different from other models that also originated from the gis community such as the soil and water assessment tool swat rapid by design may be more easily integrated to traditional lsms using a tight coupling strategy due to its modular structure written in fortran and its ability to handle large scale routing problems the rapid version we implement in this study supports both the traditional and matrix based forms of the muskingum method for the reach by reach channel routing the vector matrix formulation of rapid stores the river network as links with certain degree of connectivity then the muskingum equation in matrix form is solved with the finite difference form of the continuity equation with a linear system solver which avoids spatial iterations and supports parallel speedup david et al 2011 since the united states national water model nwm and this study use the medium resolution nhdplus hydrography dataset 1 100 000 for neighborhood scale hydrologic applications this dataset is described with more details below nhdplus is an integrated suite of geospatial dataset developed by the united states environmental protection agency epa and the united states geological survey usgs http www horizon systems com nhdplus it includes the river flowlines extracted from the 30 m national elevation dataset ned where significant field survey corrections have been applied to improve its accuracy over the past two decades moore and dewald 2016 taking the texas hydrologic region 12 as an example the medium resolution nhdplus contains 68 143 river reaches in comparison it would require more than 1 million grid cells in a grid based framework to delineate these channels at comparable accuracy suggesting a reduction by two orders of magnitude in the number of modeling units with a vector based routing model the dataset also compiles useful data attributes for use in hazard communications such as the river names and the corresponding states fig 2 illustrates the conceptual difference between the grid based and vector based routing schemes while the latter performs the streamflow simulation on the vector based reaches with more accurate hydrologic delineations and less modeling units it may also be subject to less spatial fidelity than a grid based scheme that can be implemented using sufficiently high channel resolutions for example fig 2a shows a grid based scheme with high channel resolutions assuming 30 m in this case the river network is delineated at about the same accuracy as in fig 2b but each channel modeling unit in fig 2a has an explicit resolution of 30 m while each modeling unit in fig 2b has a resolution of 150 m based on the average reach length this suggests a possible trade off between computational efficiency and spatial fidelity between grid and vector based schemes that may influence the time stepping for model stability see section 6 nevertheless before grid based models can be implemented at fine resolutions over large spatial domains a research task yet to be accomplished singh et al 2015 vector based models still demonstrate potential advantages and flexibilities that warrant a hybrid modeling strategy 2 3 grid to vector coupling interface to implement rapid as an alternative routing option a fortran module the rapid coupler is added to the wrf hydro source code which determines the location where each vector based river reach can get lateral inflows from lsm simulated total runoff we refer it as the grid to vector coupling interface we also augment a simpler catchment centroid based coupling interface by david et al 2013 fig 3 b d with an area weighted coupling shown in fig 3 c e in the former approach the lsm grid cell where the catchment centroid is located hereafter centroid cell is treated as the location for a river reach to get lateral inflows for example for the river reach in red fig 3a this implementation could lead to reasonable representations of contributing runoff in fig 3b while problems could occur with the increased lsm resolution in fig 3d for reaches with consistently small catchment areas e g southern parts in fig 3a however such variations in the interface seems to be less important for calculating lateral inflows because the lsm grid resolution is coarse regardless if a catchment centroid based option fig 3 b d is chosen implemented as an option in the rapid namelist the contributing lateral inflows i i m3 is calculated as 1 i i r u n o f f n x n y a r e a 1000 0 where nx ny is indices of the centroid cell relative to the lsm grids a r e a is catchment area km2 r u n o f f is lsm simulated total runoff mm surface subsurface if the area weighted coupling interface fig 3 c e is chosen i i is computed eqn 2 m3 by adding up the runoff from all intersected lsm grid cell j r u n o f f j mm multiplied by the intersected area a r e a i n t e r c e p t j km2 and ratio of the intersected area in the total catchment area w e i g h t j n denotes total number of intersecting cells the geographic correspondence nx ny j and the catchment areas are obtained through a series of gis pre processing procedures to generate several pre defined csv files section 3 2 then the added module reads the information only once when initializing rapid after which lsm simulated runoff is dynamically passed to nhdplus river reaches at every model time step 2 i i j 1 j n r u n o f f j a r e a i n t e r c e p t j w e i g h t j 1000 0 to assess the model performance and its regional simulation sensitivity to the lsm grid resolution and the coupling approaches in this hybrid framework we then present a real world application to systematically assesse these aspects spatially varied estimates of flow travel time is also evaluated to improve the model s performance next the model setup implementation and performance for the real world application are documented to guide future modeling activities 3 a case study for hurricane ike flood discharge simulation 3 1 hydrometeorological analysis we perform a case study to simulate the inland riverine flood discharges in hurricane ike that hit the texas gulf coast in 2008 according to the national oceanographic and atmospheric administration noaa ike was the third costliest mainland united states tropical cyclones during 1900 2013 which was blamed for 21 direct and 64 indirect fatalities and 30 billion 2008 usd noaa 2009 see http www aoml noaa gov hrd tcfaq costliesttable html hurricane ike made its landfall near houston texas at 0700 utc on september 13 at a maximum wind speed of 48 m s 1 zhao and xue 2009 it intercepted with a cold front as it moved northeastward and crossed the texas louisiana border on september 14 after which ike gradually decayed into the midwest united states and the eastern great lakes regions two days after landfall the national centers for environmental prediction ncep stageiv quantitative precipitation estimation qpe product and the usgs gauge observations are used to first provide an event based hydrometeorological analysis before setting up the model domain fig 4 a shows the rainfall intensity spatial pattern mm hr 1 at 1200 utc on september 13 and 14 respectively while fig 4b shows the 24 hr accumulated rainfall amount mm that observed widespread heavy 4 16 mm h 1 to very heavy rainfall 16 50 mm h 1 on september 13 on september 14 the interception with the cold front resulted in geographically confined heavy rainfall but the localized rainfall intensity was much more severe 50 mm h 1 than the first day as the most impacted region the san jacinto river basin sjrb outlined in dark green received more than 300 mm of 24 hr accumulated rainfall on the first day fig 4b and the basin averaged 24 hr accumulated rainfall being 172 mm on september 13 and 63 mm on september 14 fig 4c we conduct a flood frequency analysis ffa at all available usgs gauges in the texas hydrologic region hereafter region 12 to understand the regional hydrologic response resulting from the event s rainfall spatiotemporal structures land surface properties and drainage network structures wright et al 2014 the ffa is conducted using the usgs peakfq v7 1 software veilleux et al 2014 that fits the historical annual maximum discharges to a log pearson type iii distribution using the bulletin 17b regional skew coefficients region 12 has more than 700 usgs gauges and 237 gauges with more than 25 year historical annual maximum discharges for reliable ffa are used for the analysis fig 5 in particular the sjrb has 18 such gauges that have sufficient data and are active and not subject to known water diversions before the hurricane made landfall on september 12 all locations in the sjrb were under normal flow conditions meaning that their maximum instantaneous river discharge of the day was smaller than that of a 1 5 yr flood level on september 13 ten gauges observed 1 5 to 2 yr floods one gauge observed a 5 yr flood and one gauge observed a 10 yr flood the most widespread and severe flooding happened on september 14 where several gauges observed 10 to 50 yr floods since september 15 the flow conditions at most gauges recessed below the 2 yr flood level but parts of the brazos trinity and lavaca river basins continued to observe 10 yr floods note that the basin average 24 hr accumulated rainfall on september 13 was twice the magnitude of that on september 14 but the most severe 50 yr flood only occurred on the second day such a hydrological response may be explained by the localized high rainfall intensity 50 mm h 1 from the cold front interception with ike on september 14 fig 4a right in may be also ascribed to the soil moisture storage in the northwestern part of sjrb that could allow portions of the rainfall to infiltrate soil moisture 0 2 m3 m 3 on the first day rather than running off before the soil became nearly saturated on the second day 0 35 m3 m 3 based on the basin averaged model outputs not shown here based on the regional ffa analysis 14 gauges experiencing inland riverine floods are separated from those with urban ponding and coastal storm surges i e flood types that are not expected to be captured by our current hydrologic modeling framework therefore these 14 gauges are eventually used to evaluate the model performance as they are flooded active and not influenced by water diversions see table 1 for a detailed gauge summary 3 2 study domain and model configurations 3 2 1 lsm input datasets the modeling domain is then set up for the sjrb the most impacted region encompassing the houston metropolitan area fig 6 this basin has a drainage area of 10 274 km2 and an annual rainfall of 1100 1400 mm which drains into the galveston bay its vicinity to the gulf of mexico makes it vulnerable to damages and economic losses from hurricane and storm surges of the atlantic origins keim et al 2007 fig 6b shows the domain land cover types as generated from the wrf pre processing tool wps http www2 mmm ucar edu wrf users wpsv3 8 wps archives a database of land cover type soil texture category and vegetation greenness for the entire globe and the domain specific input datasets are automatically created after the grid resolution geographic bounding box latitudes longitudes information and map projections are specified for the wps the land cover data for the default wps v3 8 are from the 20 category modis data which better captured the most recent urban expansions of the houston metropolitan area compared to the 24 category usgs land cover data for the default wps v3 7 as validated against the google earth professional and esri basemap thus the 20 category modis land cover data fig 6b are used in this study 3 2 2 river network input datasets several input files are required to route water horizontally with wrf hydro rapid those include 1 a file documenting the river network s topological connectivity 2 a file describing the coupling interface and 3 two files defining the muskingum routing parameters k and x for all nhdplus river reaches the sjrb contains 2332 nhdplus river reaches fig 6c with 1916 being natural drainage lines light blue and 416 being artificial water pathways such as pipelines and ditches dark blue although these 416 artificial water pathways hold promise for future consideration of urban drainage networks in this framework such a model development is beyond the scope of this current study here we only consider 1916 natural lines with flow directions for the routing fig 6d and each flowline is associated with one local catchment area and several geospatial attributes including the channel slope fig 6d and mean annual flows the information of which is used in the spatially distributed simulation an arcgis python toolbox https github com esri python toolbox for rapid is now publicly available and is used to generate the river connectivity file and for describing the area weighted coupling interface arcgis functions feature to point and extract values to point are used to obtain a file describing the centroid based coupling interface see an example when rapid was loosely coupled to a coarse resolution lsm http rapid hub org docs rapid lsm coupling procedure pdf as for the muskingum routing parameters an optimal set of k and x values derived from david et al 2013 is used david et al 2013 derived this optimal set of routing parameters by using hundreds of usgs gauge observations in an 8 yr experiment from 2000 to 2007 see more discussions in section 5 in our study domain there is a major reservoir lake conroe 85 km2 located at the main stream of the san jacinto river 20 km upstream of the evaluation site gauge 14 see fig 6b usgs gauge id 08068000 to account for the possible reservoir flood controls that can influence the simulation results we force the rapid model with daily observed streamflow at the exact location of the dam and the forcing is only applied to the particular river reach where the dam is located 3 2 3 model configurations wrf hydro is configured in an offline mode to conduct the experiments the noah mp lsm is used as the vertical column model here while future studies could take advantage of the multiple lsm feature of the framework to account for uncertainties with different hydrological models rather than the uncertainty with parameters or parameterizations in one single model noah mp supports multiple physical parameterization schemes including the free drainage fd and the simple groundwater based simgm options for runoff parameterization simgm is more physically complete that relates groundwater with runoff using exponential decay functions niu et al 2007 yet it is not used in this study for two reasons first simgm was previously evaluated in global major river basins yang et al 2011 but not in smaller basins where complex surface water groundwater interactions might exist second we find the simgm simulated flood hydrographs for this particular event are slightly worse than that by fd which may be related to the inappropriate subsurface parameters used by simgm therefore to avoid discussions on the simgm groundwater parameterizations and to keep it the same as the operational nwm the fd runoff parameterization schaake et al 1996 is consistently used in this study to simulate the flood discharges from september 12 to 17 the model is initialized on august 1 to allow for more than one month of spin up the soil moisture outputs from the noah model of the north american land data assimilation system nldas 2 xia et al 2012 are interpolated and used for the initialization on august 1 in general gochis et al 2015 suggested a longer spin up time for hydrological applications with wrf hydro yet we consider the 1 5 month spin up as adequate in this case because our results with rapid routing are minimally influenced in another one year spin up experiment this may be because for big floods with significant amounts of surface runoff the model sensitivity to spin up time might be intrinsically less in addition the grid based routing in wrf hydro has a dependence on the surface water head on the channel grids while rapid does not fig 2 which may make the spin up with rapid shorter than that with the grid based routing however for flood events with less surface runoff or studies using the grid based routing longer spin up time might still be needed in line with cunha et al 2012 and yilmaz et al 2008 we do not calibrate the lsm parameters aiming to better understand the model behavior through calibration free model evaluations thus the default noah mp parameters adopted by previous large scale modeling studies cai et al 2014 niu et al 2011 yang et al 2011 zheng and yang 2016 are used for all experiments table 2 stageiv is used as the rainfall forcing and the nldas 2 provides other forcing variables such as the wind temperature humidity and pressure the temporal resolution is 1 hr for the lsm and 15 min for rapid david et al 2013 exps 1 4 are conducted to assess the hybrid model s sensitivity to the lsm grid resolution i e 4 km and 1 km and the grid to vector coupling interface i e a centroid based versus an area weighted approach exps 5 6 examine the travel time representation in influencing the model s regional simulation results 4 results 4 1 resolution and coupling interface fig 7 shows the model simulated flood discharges compared against observations at the 14 usgs gauges overall the model can simulate the flood discharge reasonably well at all these gauges with contributing drainage areas ranging from 55 68 km2 to 2144 51 km2 despite lsm grid resolution and coupling interface considering half of these gauges have a contributing area of less than 200 km2 and locate in small creeks with mean annual flow less than 5 m3 s 1 these results seem promising because flood forecasting for smaller drainage basins remains a challenging task for many hydrological models e g cunha et al 2012 regionally the model also tends to perform better at gauges with a relatively faster rate of rise in more urban environments i e steeper rising limbs see locations with comid 1508045 1508279 1440311 1440237 1440211 and 1438725 with an average nse of 0 63 0 15 in comparison the model performs worse at those with slower rising limbs see locations with comid 1520249 1520237 1520083 1508047 1520091 1508121 and 1508043 with the average nse lower at 0 54 1 05 to more comprehensively assess the regional model performance the following skill metrics are used to quantitatively assess the model performance which include 1 nash sutcliffe efficiency nse 1 i n m i o i i n o i o 2 pearson correlation coefficient cc c o v m o σ m σ o 3 percentage bias in accumulated discharge pbias i n m i i n o i i n o i 100 and 4 normalized root mean square error nrmse i n m i o i 2 n o where m i and o i represent hourly river discharge from the model and the observation respectively n hours in total these skill metrics are commonly used in evaluating hydrological and environmental models gupta et al 2009 from different perspectives for example nse assesses the overall hydrograph containing both the bias and co variance components cc assesses the co variance of the simulated and observed time series and pbias and nrmse both assess the magnitude of error in the runoff volume below boxplots of the statistics are shown to summarize the model s regional performance with the best 75th median 25th and worst results see fig 8 among the four experiments exp4 consistently exhibits the highest nse median 0 43 and the lowest nrmse median 0 82 m3 s 1 meeting the expectation that the model performs better with a finer land grid resolution and an area weighted coupling interface exp2 comes as the second with only slightly worse median nse 0 38 and nrmse 0 90 m3 s 1 which may be due to the coarser land grid resolution that influences the runoff generation through changing the land cover greenness and soil texture data accuracy the lateral inflow calculation eqns 1 2 is also influenced in exp2 due to the change in spatial correspondence between the grid and vector modeling units fig 3 thus it is reasonable that results for cc and pbias can be sometimes mixed but are generally close to exp4 exp1 and exp3 both exhibit lower median nse and more negative pbias due to a lack of accurate accounting of the lateral runoff contribution however all the calibration free for the lsm part modeling experiments exhibit the highest nse skill reaching 0 71 to 0 82 suggesting the flood responses at those certain gauges are not sensitive to the examined grid resolutions and the coupling interface using an area weighted coupling interface exp2 and exp4 tends to result in a slightly narrower range in the 25th to 75th quartiles of these boxplots suggesting more spatially consistent model performance the hybrid model is more sensitive to the coupling interface than the grid resolution which can be seen from the smaller difference between exp1 and exp3 with different grid resolution than that between exp1 and exp2 with different coupling interface the same conclusion holds true for exp2 and exp4 exp3 exhibits a greater range in pbias suggesting a more variable model performance across the spatial domain than other experiments because exp3 is implemented at high resolutions 1 km its lateral inflow calculation becomes very problematic for places where the catchment size largely exceeds the grid cell size see the example catchment in fig 3d and the 1 km 1 km centroid cell to pass runoff for places whose catchment size is smaller than 1 km 1 km see some small catchments in the bottom of fig 3d however the centroid cell s runoff may represent the lateral inflows well the non linear spatial correspondence between the irregular nhdplus catchments and the regular lsm grid cells helps explain why exp3 exhibits a more variable pbias in exp1 because the grid cell size 16 km2 exceeds the mean catchment area of the sjrb 5 23 km2 runoff extracted from the 4 km 4 km centroid cell may be better able to represent the correct amount of lateral inflows in several occasions for a hybrid modeling framework with both regular i e lsm grid cells and irregular non uniform modeling units i e nhdplus catchments these results suggest that with high resolution lsm grids an area weighted coupling is key in calculating the correct amount of lateral inflows this coupling approach may become less important with coarser resolution lsm grids which may justify the use of a simpler centroid based coupling in previous studies performing the routing using the 1 8 lsm runoff david et al 2013 fig 9 further shows some interesting patterns when plotting the statistics of the 14 gauges against their contributing drainage areas nrmse is examined because the runoff volume simulation is expected to be influenced more by drainage areas which also normalizes the error using the mean observed discharge in comparison cc i e covariability of simulated and observed streamflow time series and nse i e the overall hydrograph may be also influenced by factors such as the travel time representations in the model we see from fig 9 that for all experiments the nrmse drainage area relationship follows a power law function with coefficients of determination r2 ranging from 0 23 to 0 31 which means that the model s ability to capture the right amount of runoff generally increases with contributing drainage areas as suggested by cunha et al 2012 such a relationship is described as the aggregation effect of the river network meaning that as a river goes downstream and aggregates a larger contributing area the model errors resulted from various upstream locations tend to be cancelled out therefore a river network routing model also tends to have less simulation uncertainties at downstream locations which is in accordance with our results here interestingly we also find that r2 for exp1 and exp3 is greater than that in exp2 and exp4 this suggests that the aggregation effect is more obvious in experiments using a centroid based coupling interface exp1 and exp3 than that using an area weighted coupling interface exp2 and exp4 this is reasonable because in the latter coupling the runoff volume is expected to be better simulated at both upstream and downstream gauges as a result its dependence on the drainage basins to filter out errors also becomes less this provides another evidence to support the area weighted coupling as a useful approach to reduce the aggregation effect exhibited in regional scale streamflow simulations 4 2 gis based flow travel time estimation using nhdplus in addition to the grid resolution and vector raster coupling we next examine the flow travel time representation in the model to help explain the spatially varied regional model performance described in section 4 1 in the muskingum method the routing parameter k is interpreted as flow travel time that dominates the overall simulated hydrograph while x being much less influential kim et al 2001 although an optimal set of k and x values from david et al 2013 was used it is noted that these parameters were derived from a global optimization algorithm that uses daily discharge observations at hundreds of usgs gauges in texas for an 8 year modeling experiment questions are then raised as regard to the validity of these parameters to resolve sub daily flow concentration processes in addition the factor to minimize the square error cost function in the optimization scheme was applied in the same direction see eqn 6 in david et al 2013 i e it increases or decreases in the same direction for gauge locations located in different places as a result their representativeness across a spatial domain also needs to be updated to examine this issue we adopt a gis based method to estimate spatially distributed flow travel time using the nhdplus geospatial data different from previous rapid applications that adjust total travel time k in seconds based on optimization or spatial variability that separates larger river basins e g david et al 2011 tavakoly et al 2017 the flow travel time is estimated for each nhdplus channel and catchment to better account for more spatial variability it is conceptualized that the lsm generated runoff first travels along the catchment plane with time t o v and is then routed along the channel network with time t c h from the gis data structure perspective t o v and t c h take place on polygon features catchments and line features flowlines respectively the estimation can then be related to the nhdplus geospatial attributes and land cover types using eqns 3 and 4 widely adopted in other hydrological models cho and engel 2017 neitsch et al 2011 that build upon the hydrologic response unit hru similar to rapid these models use irregular modeling units that may be inherently better for hydrological applications than traditional grid based lsm discretization the difference is that here nhdplus is a pre established data fabric and its catchments are local i e the drainage area of the current river minus that of the upstream reach for representing finer scale features from 30 m dem with an average catchment size of 2 3 km2 3 t c h 0 000172 l c h n c h 0 75 a r e a 0 125 s l p c h 0 375 4 t o v 419 3 l o v 0 6 n c a t 0 6 s l p c a t 0 3 p o v 0 4 in the above equations l c h n c h and s l p c h denote the flow length m manning s roughness coefficient and slope for the channel respectively while l o v n c a t and s l p c a t denote the flow length m manning s roughness coefficient and slope for the overland catchment respectively as the sub basin area in previous gis based models such as swat a r e a is the nhdplus catchment area km2 in our case the constants are used to convert the travel time to seconds the t o v formulation assumes an average net incoming flux of 6 35 mm h 1 and we replace that with p o v mm hr 1 as the domain average net incoming flux readers are directed to neitsch et al 2011 cho and engel 2017 and reference therein for more details on the equations to calculate t c h and t o v l c h and s l p c h are readily available from the nhdplus geospatial data attributes the overland flow length l o v is estimated as half of the nhdplus catchment area divided by the nhdplus flowline length 0 5 a r e a 1000000 l c h similar to li et al 2013 channel roughness n c h and overland slope s l p c a t are set to 0 035 and 0 01 for the entire domain respectively as they are reasonable initial estimates for large scale studies in the absence of further distributed data overland roughness coefficient n c a t is a function of the land cover type which has a one to one relationship based on the widely used empirical values see the lookup table in hydro tbl by gochis et al 2015 for the net incoming flux p o v 7 mm h 1 is used based on the event rainfall analysis averaged over the sjrb see fig 4c based on the pre processing in gis t ch and t ov hrs for the sjrb can be estimated as shown in fig 10 across the spatial domain river reaches with longer lengths and flatter channel slopes fig 6c generally have greater t ch b similarly catchments with rougher surfaces for example forest in the northeast sjrb and cropland in the southwest sjrb fig 6b also tend to have longer t ov than other areas of the domain fig 10b under this conceptualization the total travel time for each nhdplus modeling unit t total can be represented by the sum of t ch and t ov comparisons between the probability distribution function pdf of the calculated t total and that optimized by david et al 2013 suggest that the optimized travel time is shifted to the lower end of the pdf while the calculated t total has more modeling units with a longer travel time fig 10c this explains why the model performance in exps 1 4 tends to show peakier than observed hydrographs for gauges with forest and crop land cover types while exhibiting good performance for those with faster rises fig 7 such a problem was introduced by the optimization scheme that decreases the travel time k in the same direction for all gauges where the calculated t total showed better spatial heterogeneity across the domain in this case to test the sensitivity of the regional simulation to the travel time estimation we conduct two additional experiments by replacing k with the t total calculated above fig 11 shows the nse boxplots for the modeling experiments with optimized travel time k and the calculated t total in both the 4 km and 1 km modeling experiments the highest nse is achieved when an area weighted coupling interface and the calculated t total are adopted see fig 11 the regional median nse is brought up from 0 4 in exp2 and exp4 to 0 65 for exp5 and exp6 which is due to the calculated t total that slows down the flow concentration processes at locations experiencing a moderate rate of rise floods fig 7 however we note that this calculation only considered spatial heterogeneity in estimating flow travel time at each nhdplus modeling unit future research is warranted to introduce temporal variability to account for the dynamical flow travel time responses which may be better adapted to actual flood prediction scenarios where the routing parameters cannot be obtained prior to a particular event 5 discussions 5 1 computational efficiency model complexity and application settings fig 12 shows an independent test of the model computational costs based on several experimental runs for the texas region 12 this domain has a drainage area of 464 135 km2 and 68 143 nhdplus river reaches and the model is configured at 4 5 km grid resolution for a 3 day simulation in this test using the tacc stampede supercomputing facility it is shown that rapid took 5 of the lsm simulation time with streamflow outputs recorded at all 68 143 river reaches which highlights the computational efficiency of this hybrid model when it is applied to a large spatial domain the best computational performance is achieved using 8 to 16 computing cores we also compare the full feature grid based routing hereafter ffgr based on the 250 m grid with the rapid routing in table 3 both are now supported in wrf hydro to facilitate a discussion on the model efficiency complexity and their applicable settings in ffgr we turn on the saturated subsurface routing overland surface routing channel routing option 3 diffusive routing and groundwater baseflow bucket option 3 pass through to consider all processes represented in the grid based routing rapid on the other hand simplifies the explicit representation of these processes and only focuses on the reach scale streamflow response as an end product of the travel time estimation in the sjrb ffgr and rapid respectively takes 3212 66 s and 14 67 s for routing the efficiency of the latter is partly due to the object oriented feature of rapid and partly due to its coarser time step tied to the average length of the modeling unit i e 250 m for ffgr and 2470 m for rapid see more conceptual discussions in section 4 3 the model performance of the uncalibrated ffgr median nse 0 31 is slightly worse than rapid using optimized routing parameters with centroid based coupling median nse 0 35 exp1 and that with area weighted coupling median nse 0 38 exp2 we note that the nse comparison here is not to discuss the two routing approaches in terms of their skill in simulating floods because pronounced differences still exist in their degrees of calibration and the diffusive kinematic wave approximations on top of the differences in the modeling units the results only show that rapid can serve as a computationally efficient alternative for offline hydrological applications if carefully chosen or physically based routing parameters are given the ffgr on the other hand is more physically complete in explicitly representing the flow paths and moisture redistribution using a grid by grid routing approach which may be suggested for studies to understand detailed water movements and or their feedbacks to the atmosphere at a higher computational cost 5 2 model efficiency and complexity for operational use the considerations on efficiency and complexity are also seen in different model configurations of the nwm the new operational hydrologic prediction tool for the united states using wrf hydro as the backbone building upon the noah mp lsm and the channel routing on the 2 7 million nhdplus river reaches maidment 2017 the nwm is now producing real time predictions for streamflow and all lsm simulated states and fluxes at 18 hr short range 10 day medium range and 30 day long range forecasts together with one historical analysis run to support the water intelligence of the nation http water noaa gov about nwm while the short to medium range nwm forecasts implement the ffgr routing to account for the best supported physical representations albeit needing further performance evaluation and parameter calibration the long range forecast implements a scheme similar to the presented wrf hydro rapid framework because of trade off considerations between affordable computational power and model complexities thus the hybrid modeling strategy and the results for flow travel time estimation to account for more spatially distributed details in this study should be appropriate for the nwm long range forecast which is expected to improve upon those studies only separating the flow travel time estimations for larger river basins or using different river reach lengths however future research is also warranted to introduce temporal variability du et al 2009 to better adjust these parameters for use in actual predictions several underutilized nhdplus geospatial datasets are also discussed here for potential future implementation in this hybrid framework including artificial water pathways such as pipelines and ditches section 3 small to medium sized lake ponds and swamps and dams springs and other scattered features on land not shown these point features dams and springs line features urban drainages and polygon features lake ponds are well established and compiled as georeferenced gis data fabrics lehner and grill 2013 lin et al 2015 at the continental to global scales however these features are difficult to be systematically represented in traditional grid based lsms due to their geographically scattered characteristics the presented hybrid framework has the potential to explicitly incorporate them from the perspective of flexible data structures mizukami et al 2016 but future work that engages the gis modeling with the lsm development is needed to take better advantage of these vector based data 6 conclusions a vector based river routing model named rapid was implemented as a new routing option in the community wrf hydro modeling framework with the goal of facilitating large scale hydrological modeling this hybrid model combines the grid and vector based modeling units which was demonstrated to be computationally efficient and currently suitable for use in offline hydrological applications see appendix for the software availability such a model coupling work is in accordance with the need to advance land surface modeling and river routing by taking advantage of gis based modeling features clark et al 2015 li et al 2013 by conducting a case study to simulate the inland riverine flood during hurricane ike in 2008 we assess the hybrid model s sensitivity to the land grid resolution and the grid to vector coupling interface we show that the model has reasonable performance in predicting the inland flood discharge at 14 usgs gauges in a highly urbanized domain the hybrid model is more sensitive to the grid to vector coupling interface than to the lsm grid resolutions and a 1 km land grid and an area weighted coupling interface is suggested to have the optimal model performance a gis based approach to estimating flow travel time is adopted to further improve the regional model simulation results presented in this study may have implications for the nwm whose different operational configurations consider trade offs between the model efficiency and complexity the hybrid modeling framework holds promise for modeling other irregular hydrologic features yet future work remains to be done in taking advantage of the under exploited gis geospatial data and introducing temporal variability for the flow travel time estimation to further advance the current modeling capability presented in this study acknowledgement this work is supported in part by the national natural science foundation of china under grant number 41375088 and in part by the microsoft research and the jackson school of geoscience ut austin cédric h david is supported by the jet propulsion laboratory california institute of technology under a contract with the national aeronautics and space administration david gochis and wei yu are supported by the national science foundation through its cooperative funding of the national center for atmospheric research additional support for gochis and yu were provided by nsf earthcube grant 1343811 kevin sampson ncar is acknowledged in providing gis support p l z l y d j g d r m and c h d proposed the implementation of a vector based river network model in the wrf hydro framework p l worked on the code development with contributions from w y m a s v and c h d p l conducted the modeling experiments with inputs from z l y and d j g appendix a software availability the preliminary code package for the hybrid model is downloadable at https www ral ucar edu sites default files public projects wrf hydro v3 0 more updated versions are available upon request to the corresponding author liang jsg utexas edu the updated code package uses rapid v1 3 0 david et al 2013b with hash table features for efficient river network initializations model compilation needs installation of netcdf and petsc for model i o and matrix based computation suggested versions netcdf 3 6 3 and petsc 3 4 environment variables wrf hydro wrf hydro rapid need to be set to 1 and successful compilation will generate an executable wrf hydro exe in the hydro namelist specify the channel routing option as 4 for running rapid more details can be found at https github com c h david rapid and http rapid hub org docs rapid wrf hydro install pdf 
26358,continental scale flood discharge modeling requires a high level of efficiency and flexibility to this end this study documents the implementation and application of a vector based river routing model in the community wrf hydro modeling framework using hurricane ike as a case study the hybrid vector grid modeling framework s sensitivity to the land grid resolution and the coupling interface is assessed results show the model is more sensitive to the coupling interface than the grid resolution and a 1 km land grid with an area weighted coupling interface exhibits the optimal simulation results a geographic information system gis based approach is adopted to improve the regional representativeness of the flow travel time estimation the model s computational efficiency and complexity are compared to a grid based routing scheme demonstrating its advantages for large scale offline hydrological applications with gis supported features trade offs between the modeling efficiency and complexity are then discussed to inform future large scale flood prediction applications keywords vector based river routing wrf hydro rapid flood discharge simulation hurricane ike 1 introduction vector based river routing models are receiving increased attention by land surface modeling groups to facilitate large scale hydrologic predictions at fine resolutions david et al 2011 goodall et al 2013 lehner and grill 2013 liu and hodges 2014 mizukami et al 2016 yamazaki et al 2013 compared to traditional grid based routing models vector based schemes are conceptually more object oriented with fewer linkages among objects lehner and grill 2013 and hence computationally more efficient and generally more accurate in depicting hydrologic features yamazaki et al 2013 found a 60 increase in computational efficiency when shifting from a grid based to a vector based routing approach for a global model cama flood in addition to efficiency vector based models also demonstrate other advantages for example using a vector based scheme treating the river network as links and nodes liu and hodges 2014 successfully adapted the ideas from microprocessor designs to efficiently solve the fully nonlinear 1 d saint venant equations for large scale routing problems with o 105 elements vector based schemes can also provide accurate descriptions of hydrologic features such as river reaches dams and gauges at exact geographic locations mizukami et al 2016 in comparison representations of these features in a grid based scheme largely depend on the grid resolution but increasing the spatial resolution over a large spatial domain remains an unachieved priority bierkens et al 2015 singh et al 2015 wood et al 2011 using a grid based scheme li et al 2013 discussed that neglecting the subgrid travel time could pose problems for the routing performance under coarse resolutions these previous studies all suggested that efficient large scale hydrologic prediction at fine resolutions could potentially benefit from a hybrid framework that combines both vector based river routing and grid based land surface modeling however as river routing has been mostly practiced for channel design and floodplain analysis by civil engineers and geographic information system gis modeling groups its coupling with traditional grid based land surface models lsms for large scale applications remains a challenging task clark et al 2015 mizukami et al 2016 kauffeldt et al 2016 summarized after reviewing 24 hydrological models several key criteria that make a modeling system a suitable candidate for large scale operational applications those include availability of model code existing user community flexibility in resolution as well as flexibility in grid structures and modeling units in light of these postulated criteria we implemented the routing application for parallel computation of discharge model rapid david et al 2011 into the community hydrological extension of the weather research and forecasting model wrf hydro gochis et al 2015 currently in a one way manner meaning that there is no feedback from vector based river routing to land surface modeling these two open source models were developed for different user communities wrf hydro is an architectural framework for broad scope hydrometeorological studies arnault et al 2016 gochis et al 2015 yucel et al 2015 while rapid is mainly practiced with gis based water resource management and flood related applications follum et al 2017 lin et al 2015 tavakoly et al 2017 the advantages in combining these models were first demonstrated by the national flood interoperability experiment for a continental scale application in which the 15 hr lead streamflow forecasting for 2 7 million river reaches across the continental united states us was completed within 10 min using 16 computing cores at the texas advanced computing center tacc stampede cluster maidment 2017 lin et al 2018 salas et al 2018 although these promising applications can be conducted with loosely coupled modeling strategies i e the outputs of a model are generated processed and then used as inputs to another model the need for a tightly coupled wrf hydro rapid software package lies in its flexibility in data structure to allow for researchers with interdisciplinary backgrounds to conduct flood prediction studies in addition such a modeling system may also make it straightforward to address uncertainties originating from weather inputs land surface parameterizations and river routing schemes the first objective of this study therefore is to document the coupling implementation and configuration of the wrf hydro rapid modeling system we then assess the system s performance in terms of simulating flood discharge and the sensitivity to land grid resolutions and grid to vector coupling strategies using hurricane ike as a case study radar based rainfall data are used as the rainfall input and a gis based approach is adopted to estimate the flow travel time for the regional modeling simulation finally the trade offs between model efficiency and complexity are discussed by comparing rapid with the grid based routing option in wrf hydro in order to inform future model development and application activities 2 model description and coupling 2 1 the community wrf hydro modeling framework wrf hydro is the hydrological extension of the wrf model a widely used numerical weather prediction nwp tool for weather climate research and operations developed by the national center for atmospheric research ncar wrf hydro provides a model coupling architecture to link multi scale hydrometeorological processes gochis et al 2015 which can be broadly categorized as 1 the atmospheric models and meteorological input data stubs 2 lsms for calculating land states and fluxes in 1 d vertical columns and 3 routing modules for representing horizontal water movement fig 1 each of these modular layers can be set active or inactive to allow for stand alone or full feature modeling experiments by modifying the hydro namelist the supported full feature modeling grid based framework currently includes switch activated modules for surface overland flow saturated subsurface flow base flow in conceptual groundwater reservoirs i e buckets and channel flow processes several state of the art lsms e g noah and noah mp are now supported to benefit multi model predictions to this end wrf hydro is not a hydrological model but instead is an extensible modeling coupling framework to serve both coupled fig 1a brown arrows and uncoupled fig 1b green arrows hydrometeorological applications utilizing these modeling capabilities studies have been using wrf hydro to investigate the two way interactions between soil moisture redistribution and hillslope hydrology atmosphere feedbacks in west africa and the central mediterranean e g arnault et al 2016 senatore et al 2015 while some have focused on offline hydrological applications to study the one way hydrologic response to the weather forcings e g silver et al 2017 yucel et al 2015 the grid based routing in wrf hydro separates the explicit representation of terrain and channel routing processes gochis et al 2015 similar to li et al 2013 before water reaches a predefined channel cell switch activated modules for overland subsurface and baseflow buckets are used to route the flow which allows for updating surface head along the pathway dashed black arrows in fig 2 a once the water reaches the predefined network the channel routing module takes over to route water down the routing performance of such a grid based scheme is thus sensitive to both the land and the channel grid resolutions fig 2a for example given a fixed land grid resolution a coarser channel resolution larger grid spacing can resolve less tributaries of lower strahler orders this will result in lower tributary density in one lsm grid cell and in this case terrain routing becomes more important due to a lack of channel pathways to route the correct amount of water compared to the case with a finer channel resolution in this study we focus on augmenting the offline hydrological modeling capability of wrf hydro using a vector based routing scheme which has no requirement for a grid based channel mesh but instead directly runs on a dense vectorized channel network 2 2 the rapid river network routing model rapid is a river network routing model that can directly perform streamflow computations on the vector based geospatial data fabrics such as a continental scale dataset from nhdplus david et al 2011 2013 follum et al 2017 lin et al 2015 tavakoly et al 2017 and a global scale dataset from hydrosheds david et al 2015 these two datasets provide vectorized channel hydrography for use in modeling and efficient hazard communications different from other models that also originated from the gis community such as the soil and water assessment tool swat rapid by design may be more easily integrated to traditional lsms using a tight coupling strategy due to its modular structure written in fortran and its ability to handle large scale routing problems the rapid version we implement in this study supports both the traditional and matrix based forms of the muskingum method for the reach by reach channel routing the vector matrix formulation of rapid stores the river network as links with certain degree of connectivity then the muskingum equation in matrix form is solved with the finite difference form of the continuity equation with a linear system solver which avoids spatial iterations and supports parallel speedup david et al 2011 since the united states national water model nwm and this study use the medium resolution nhdplus hydrography dataset 1 100 000 for neighborhood scale hydrologic applications this dataset is described with more details below nhdplus is an integrated suite of geospatial dataset developed by the united states environmental protection agency epa and the united states geological survey usgs http www horizon systems com nhdplus it includes the river flowlines extracted from the 30 m national elevation dataset ned where significant field survey corrections have been applied to improve its accuracy over the past two decades moore and dewald 2016 taking the texas hydrologic region 12 as an example the medium resolution nhdplus contains 68 143 river reaches in comparison it would require more than 1 million grid cells in a grid based framework to delineate these channels at comparable accuracy suggesting a reduction by two orders of magnitude in the number of modeling units with a vector based routing model the dataset also compiles useful data attributes for use in hazard communications such as the river names and the corresponding states fig 2 illustrates the conceptual difference between the grid based and vector based routing schemes while the latter performs the streamflow simulation on the vector based reaches with more accurate hydrologic delineations and less modeling units it may also be subject to less spatial fidelity than a grid based scheme that can be implemented using sufficiently high channel resolutions for example fig 2a shows a grid based scheme with high channel resolutions assuming 30 m in this case the river network is delineated at about the same accuracy as in fig 2b but each channel modeling unit in fig 2a has an explicit resolution of 30 m while each modeling unit in fig 2b has a resolution of 150 m based on the average reach length this suggests a possible trade off between computational efficiency and spatial fidelity between grid and vector based schemes that may influence the time stepping for model stability see section 6 nevertheless before grid based models can be implemented at fine resolutions over large spatial domains a research task yet to be accomplished singh et al 2015 vector based models still demonstrate potential advantages and flexibilities that warrant a hybrid modeling strategy 2 3 grid to vector coupling interface to implement rapid as an alternative routing option a fortran module the rapid coupler is added to the wrf hydro source code which determines the location where each vector based river reach can get lateral inflows from lsm simulated total runoff we refer it as the grid to vector coupling interface we also augment a simpler catchment centroid based coupling interface by david et al 2013 fig 3 b d with an area weighted coupling shown in fig 3 c e in the former approach the lsm grid cell where the catchment centroid is located hereafter centroid cell is treated as the location for a river reach to get lateral inflows for example for the river reach in red fig 3a this implementation could lead to reasonable representations of contributing runoff in fig 3b while problems could occur with the increased lsm resolution in fig 3d for reaches with consistently small catchment areas e g southern parts in fig 3a however such variations in the interface seems to be less important for calculating lateral inflows because the lsm grid resolution is coarse regardless if a catchment centroid based option fig 3 b d is chosen implemented as an option in the rapid namelist the contributing lateral inflows i i m3 is calculated as 1 i i r u n o f f n x n y a r e a 1000 0 where nx ny is indices of the centroid cell relative to the lsm grids a r e a is catchment area km2 r u n o f f is lsm simulated total runoff mm surface subsurface if the area weighted coupling interface fig 3 c e is chosen i i is computed eqn 2 m3 by adding up the runoff from all intersected lsm grid cell j r u n o f f j mm multiplied by the intersected area a r e a i n t e r c e p t j km2 and ratio of the intersected area in the total catchment area w e i g h t j n denotes total number of intersecting cells the geographic correspondence nx ny j and the catchment areas are obtained through a series of gis pre processing procedures to generate several pre defined csv files section 3 2 then the added module reads the information only once when initializing rapid after which lsm simulated runoff is dynamically passed to nhdplus river reaches at every model time step 2 i i j 1 j n r u n o f f j a r e a i n t e r c e p t j w e i g h t j 1000 0 to assess the model performance and its regional simulation sensitivity to the lsm grid resolution and the coupling approaches in this hybrid framework we then present a real world application to systematically assesse these aspects spatially varied estimates of flow travel time is also evaluated to improve the model s performance next the model setup implementation and performance for the real world application are documented to guide future modeling activities 3 a case study for hurricane ike flood discharge simulation 3 1 hydrometeorological analysis we perform a case study to simulate the inland riverine flood discharges in hurricane ike that hit the texas gulf coast in 2008 according to the national oceanographic and atmospheric administration noaa ike was the third costliest mainland united states tropical cyclones during 1900 2013 which was blamed for 21 direct and 64 indirect fatalities and 30 billion 2008 usd noaa 2009 see http www aoml noaa gov hrd tcfaq costliesttable html hurricane ike made its landfall near houston texas at 0700 utc on september 13 at a maximum wind speed of 48 m s 1 zhao and xue 2009 it intercepted with a cold front as it moved northeastward and crossed the texas louisiana border on september 14 after which ike gradually decayed into the midwest united states and the eastern great lakes regions two days after landfall the national centers for environmental prediction ncep stageiv quantitative precipitation estimation qpe product and the usgs gauge observations are used to first provide an event based hydrometeorological analysis before setting up the model domain fig 4 a shows the rainfall intensity spatial pattern mm hr 1 at 1200 utc on september 13 and 14 respectively while fig 4b shows the 24 hr accumulated rainfall amount mm that observed widespread heavy 4 16 mm h 1 to very heavy rainfall 16 50 mm h 1 on september 13 on september 14 the interception with the cold front resulted in geographically confined heavy rainfall but the localized rainfall intensity was much more severe 50 mm h 1 than the first day as the most impacted region the san jacinto river basin sjrb outlined in dark green received more than 300 mm of 24 hr accumulated rainfall on the first day fig 4b and the basin averaged 24 hr accumulated rainfall being 172 mm on september 13 and 63 mm on september 14 fig 4c we conduct a flood frequency analysis ffa at all available usgs gauges in the texas hydrologic region hereafter region 12 to understand the regional hydrologic response resulting from the event s rainfall spatiotemporal structures land surface properties and drainage network structures wright et al 2014 the ffa is conducted using the usgs peakfq v7 1 software veilleux et al 2014 that fits the historical annual maximum discharges to a log pearson type iii distribution using the bulletin 17b regional skew coefficients region 12 has more than 700 usgs gauges and 237 gauges with more than 25 year historical annual maximum discharges for reliable ffa are used for the analysis fig 5 in particular the sjrb has 18 such gauges that have sufficient data and are active and not subject to known water diversions before the hurricane made landfall on september 12 all locations in the sjrb were under normal flow conditions meaning that their maximum instantaneous river discharge of the day was smaller than that of a 1 5 yr flood level on september 13 ten gauges observed 1 5 to 2 yr floods one gauge observed a 5 yr flood and one gauge observed a 10 yr flood the most widespread and severe flooding happened on september 14 where several gauges observed 10 to 50 yr floods since september 15 the flow conditions at most gauges recessed below the 2 yr flood level but parts of the brazos trinity and lavaca river basins continued to observe 10 yr floods note that the basin average 24 hr accumulated rainfall on september 13 was twice the magnitude of that on september 14 but the most severe 50 yr flood only occurred on the second day such a hydrological response may be explained by the localized high rainfall intensity 50 mm h 1 from the cold front interception with ike on september 14 fig 4a right in may be also ascribed to the soil moisture storage in the northwestern part of sjrb that could allow portions of the rainfall to infiltrate soil moisture 0 2 m3 m 3 on the first day rather than running off before the soil became nearly saturated on the second day 0 35 m3 m 3 based on the basin averaged model outputs not shown here based on the regional ffa analysis 14 gauges experiencing inland riverine floods are separated from those with urban ponding and coastal storm surges i e flood types that are not expected to be captured by our current hydrologic modeling framework therefore these 14 gauges are eventually used to evaluate the model performance as they are flooded active and not influenced by water diversions see table 1 for a detailed gauge summary 3 2 study domain and model configurations 3 2 1 lsm input datasets the modeling domain is then set up for the sjrb the most impacted region encompassing the houston metropolitan area fig 6 this basin has a drainage area of 10 274 km2 and an annual rainfall of 1100 1400 mm which drains into the galveston bay its vicinity to the gulf of mexico makes it vulnerable to damages and economic losses from hurricane and storm surges of the atlantic origins keim et al 2007 fig 6b shows the domain land cover types as generated from the wrf pre processing tool wps http www2 mmm ucar edu wrf users wpsv3 8 wps archives a database of land cover type soil texture category and vegetation greenness for the entire globe and the domain specific input datasets are automatically created after the grid resolution geographic bounding box latitudes longitudes information and map projections are specified for the wps the land cover data for the default wps v3 8 are from the 20 category modis data which better captured the most recent urban expansions of the houston metropolitan area compared to the 24 category usgs land cover data for the default wps v3 7 as validated against the google earth professional and esri basemap thus the 20 category modis land cover data fig 6b are used in this study 3 2 2 river network input datasets several input files are required to route water horizontally with wrf hydro rapid those include 1 a file documenting the river network s topological connectivity 2 a file describing the coupling interface and 3 two files defining the muskingum routing parameters k and x for all nhdplus river reaches the sjrb contains 2332 nhdplus river reaches fig 6c with 1916 being natural drainage lines light blue and 416 being artificial water pathways such as pipelines and ditches dark blue although these 416 artificial water pathways hold promise for future consideration of urban drainage networks in this framework such a model development is beyond the scope of this current study here we only consider 1916 natural lines with flow directions for the routing fig 6d and each flowline is associated with one local catchment area and several geospatial attributes including the channel slope fig 6d and mean annual flows the information of which is used in the spatially distributed simulation an arcgis python toolbox https github com esri python toolbox for rapid is now publicly available and is used to generate the river connectivity file and for describing the area weighted coupling interface arcgis functions feature to point and extract values to point are used to obtain a file describing the centroid based coupling interface see an example when rapid was loosely coupled to a coarse resolution lsm http rapid hub org docs rapid lsm coupling procedure pdf as for the muskingum routing parameters an optimal set of k and x values derived from david et al 2013 is used david et al 2013 derived this optimal set of routing parameters by using hundreds of usgs gauge observations in an 8 yr experiment from 2000 to 2007 see more discussions in section 5 in our study domain there is a major reservoir lake conroe 85 km2 located at the main stream of the san jacinto river 20 km upstream of the evaluation site gauge 14 see fig 6b usgs gauge id 08068000 to account for the possible reservoir flood controls that can influence the simulation results we force the rapid model with daily observed streamflow at the exact location of the dam and the forcing is only applied to the particular river reach where the dam is located 3 2 3 model configurations wrf hydro is configured in an offline mode to conduct the experiments the noah mp lsm is used as the vertical column model here while future studies could take advantage of the multiple lsm feature of the framework to account for uncertainties with different hydrological models rather than the uncertainty with parameters or parameterizations in one single model noah mp supports multiple physical parameterization schemes including the free drainage fd and the simple groundwater based simgm options for runoff parameterization simgm is more physically complete that relates groundwater with runoff using exponential decay functions niu et al 2007 yet it is not used in this study for two reasons first simgm was previously evaluated in global major river basins yang et al 2011 but not in smaller basins where complex surface water groundwater interactions might exist second we find the simgm simulated flood hydrographs for this particular event are slightly worse than that by fd which may be related to the inappropriate subsurface parameters used by simgm therefore to avoid discussions on the simgm groundwater parameterizations and to keep it the same as the operational nwm the fd runoff parameterization schaake et al 1996 is consistently used in this study to simulate the flood discharges from september 12 to 17 the model is initialized on august 1 to allow for more than one month of spin up the soil moisture outputs from the noah model of the north american land data assimilation system nldas 2 xia et al 2012 are interpolated and used for the initialization on august 1 in general gochis et al 2015 suggested a longer spin up time for hydrological applications with wrf hydro yet we consider the 1 5 month spin up as adequate in this case because our results with rapid routing are minimally influenced in another one year spin up experiment this may be because for big floods with significant amounts of surface runoff the model sensitivity to spin up time might be intrinsically less in addition the grid based routing in wrf hydro has a dependence on the surface water head on the channel grids while rapid does not fig 2 which may make the spin up with rapid shorter than that with the grid based routing however for flood events with less surface runoff or studies using the grid based routing longer spin up time might still be needed in line with cunha et al 2012 and yilmaz et al 2008 we do not calibrate the lsm parameters aiming to better understand the model behavior through calibration free model evaluations thus the default noah mp parameters adopted by previous large scale modeling studies cai et al 2014 niu et al 2011 yang et al 2011 zheng and yang 2016 are used for all experiments table 2 stageiv is used as the rainfall forcing and the nldas 2 provides other forcing variables such as the wind temperature humidity and pressure the temporal resolution is 1 hr for the lsm and 15 min for rapid david et al 2013 exps 1 4 are conducted to assess the hybrid model s sensitivity to the lsm grid resolution i e 4 km and 1 km and the grid to vector coupling interface i e a centroid based versus an area weighted approach exps 5 6 examine the travel time representation in influencing the model s regional simulation results 4 results 4 1 resolution and coupling interface fig 7 shows the model simulated flood discharges compared against observations at the 14 usgs gauges overall the model can simulate the flood discharge reasonably well at all these gauges with contributing drainage areas ranging from 55 68 km2 to 2144 51 km2 despite lsm grid resolution and coupling interface considering half of these gauges have a contributing area of less than 200 km2 and locate in small creeks with mean annual flow less than 5 m3 s 1 these results seem promising because flood forecasting for smaller drainage basins remains a challenging task for many hydrological models e g cunha et al 2012 regionally the model also tends to perform better at gauges with a relatively faster rate of rise in more urban environments i e steeper rising limbs see locations with comid 1508045 1508279 1440311 1440237 1440211 and 1438725 with an average nse of 0 63 0 15 in comparison the model performs worse at those with slower rising limbs see locations with comid 1520249 1520237 1520083 1508047 1520091 1508121 and 1508043 with the average nse lower at 0 54 1 05 to more comprehensively assess the regional model performance the following skill metrics are used to quantitatively assess the model performance which include 1 nash sutcliffe efficiency nse 1 i n m i o i i n o i o 2 pearson correlation coefficient cc c o v m o σ m σ o 3 percentage bias in accumulated discharge pbias i n m i i n o i i n o i 100 and 4 normalized root mean square error nrmse i n m i o i 2 n o where m i and o i represent hourly river discharge from the model and the observation respectively n hours in total these skill metrics are commonly used in evaluating hydrological and environmental models gupta et al 2009 from different perspectives for example nse assesses the overall hydrograph containing both the bias and co variance components cc assesses the co variance of the simulated and observed time series and pbias and nrmse both assess the magnitude of error in the runoff volume below boxplots of the statistics are shown to summarize the model s regional performance with the best 75th median 25th and worst results see fig 8 among the four experiments exp4 consistently exhibits the highest nse median 0 43 and the lowest nrmse median 0 82 m3 s 1 meeting the expectation that the model performs better with a finer land grid resolution and an area weighted coupling interface exp2 comes as the second with only slightly worse median nse 0 38 and nrmse 0 90 m3 s 1 which may be due to the coarser land grid resolution that influences the runoff generation through changing the land cover greenness and soil texture data accuracy the lateral inflow calculation eqns 1 2 is also influenced in exp2 due to the change in spatial correspondence between the grid and vector modeling units fig 3 thus it is reasonable that results for cc and pbias can be sometimes mixed but are generally close to exp4 exp1 and exp3 both exhibit lower median nse and more negative pbias due to a lack of accurate accounting of the lateral runoff contribution however all the calibration free for the lsm part modeling experiments exhibit the highest nse skill reaching 0 71 to 0 82 suggesting the flood responses at those certain gauges are not sensitive to the examined grid resolutions and the coupling interface using an area weighted coupling interface exp2 and exp4 tends to result in a slightly narrower range in the 25th to 75th quartiles of these boxplots suggesting more spatially consistent model performance the hybrid model is more sensitive to the coupling interface than the grid resolution which can be seen from the smaller difference between exp1 and exp3 with different grid resolution than that between exp1 and exp2 with different coupling interface the same conclusion holds true for exp2 and exp4 exp3 exhibits a greater range in pbias suggesting a more variable model performance across the spatial domain than other experiments because exp3 is implemented at high resolutions 1 km its lateral inflow calculation becomes very problematic for places where the catchment size largely exceeds the grid cell size see the example catchment in fig 3d and the 1 km 1 km centroid cell to pass runoff for places whose catchment size is smaller than 1 km 1 km see some small catchments in the bottom of fig 3d however the centroid cell s runoff may represent the lateral inflows well the non linear spatial correspondence between the irregular nhdplus catchments and the regular lsm grid cells helps explain why exp3 exhibits a more variable pbias in exp1 because the grid cell size 16 km2 exceeds the mean catchment area of the sjrb 5 23 km2 runoff extracted from the 4 km 4 km centroid cell may be better able to represent the correct amount of lateral inflows in several occasions for a hybrid modeling framework with both regular i e lsm grid cells and irregular non uniform modeling units i e nhdplus catchments these results suggest that with high resolution lsm grids an area weighted coupling is key in calculating the correct amount of lateral inflows this coupling approach may become less important with coarser resolution lsm grids which may justify the use of a simpler centroid based coupling in previous studies performing the routing using the 1 8 lsm runoff david et al 2013 fig 9 further shows some interesting patterns when plotting the statistics of the 14 gauges against their contributing drainage areas nrmse is examined because the runoff volume simulation is expected to be influenced more by drainage areas which also normalizes the error using the mean observed discharge in comparison cc i e covariability of simulated and observed streamflow time series and nse i e the overall hydrograph may be also influenced by factors such as the travel time representations in the model we see from fig 9 that for all experiments the nrmse drainage area relationship follows a power law function with coefficients of determination r2 ranging from 0 23 to 0 31 which means that the model s ability to capture the right amount of runoff generally increases with contributing drainage areas as suggested by cunha et al 2012 such a relationship is described as the aggregation effect of the river network meaning that as a river goes downstream and aggregates a larger contributing area the model errors resulted from various upstream locations tend to be cancelled out therefore a river network routing model also tends to have less simulation uncertainties at downstream locations which is in accordance with our results here interestingly we also find that r2 for exp1 and exp3 is greater than that in exp2 and exp4 this suggests that the aggregation effect is more obvious in experiments using a centroid based coupling interface exp1 and exp3 than that using an area weighted coupling interface exp2 and exp4 this is reasonable because in the latter coupling the runoff volume is expected to be better simulated at both upstream and downstream gauges as a result its dependence on the drainage basins to filter out errors also becomes less this provides another evidence to support the area weighted coupling as a useful approach to reduce the aggregation effect exhibited in regional scale streamflow simulations 4 2 gis based flow travel time estimation using nhdplus in addition to the grid resolution and vector raster coupling we next examine the flow travel time representation in the model to help explain the spatially varied regional model performance described in section 4 1 in the muskingum method the routing parameter k is interpreted as flow travel time that dominates the overall simulated hydrograph while x being much less influential kim et al 2001 although an optimal set of k and x values from david et al 2013 was used it is noted that these parameters were derived from a global optimization algorithm that uses daily discharge observations at hundreds of usgs gauges in texas for an 8 year modeling experiment questions are then raised as regard to the validity of these parameters to resolve sub daily flow concentration processes in addition the factor to minimize the square error cost function in the optimization scheme was applied in the same direction see eqn 6 in david et al 2013 i e it increases or decreases in the same direction for gauge locations located in different places as a result their representativeness across a spatial domain also needs to be updated to examine this issue we adopt a gis based method to estimate spatially distributed flow travel time using the nhdplus geospatial data different from previous rapid applications that adjust total travel time k in seconds based on optimization or spatial variability that separates larger river basins e g david et al 2011 tavakoly et al 2017 the flow travel time is estimated for each nhdplus channel and catchment to better account for more spatial variability it is conceptualized that the lsm generated runoff first travels along the catchment plane with time t o v and is then routed along the channel network with time t c h from the gis data structure perspective t o v and t c h take place on polygon features catchments and line features flowlines respectively the estimation can then be related to the nhdplus geospatial attributes and land cover types using eqns 3 and 4 widely adopted in other hydrological models cho and engel 2017 neitsch et al 2011 that build upon the hydrologic response unit hru similar to rapid these models use irregular modeling units that may be inherently better for hydrological applications than traditional grid based lsm discretization the difference is that here nhdplus is a pre established data fabric and its catchments are local i e the drainage area of the current river minus that of the upstream reach for representing finer scale features from 30 m dem with an average catchment size of 2 3 km2 3 t c h 0 000172 l c h n c h 0 75 a r e a 0 125 s l p c h 0 375 4 t o v 419 3 l o v 0 6 n c a t 0 6 s l p c a t 0 3 p o v 0 4 in the above equations l c h n c h and s l p c h denote the flow length m manning s roughness coefficient and slope for the channel respectively while l o v n c a t and s l p c a t denote the flow length m manning s roughness coefficient and slope for the overland catchment respectively as the sub basin area in previous gis based models such as swat a r e a is the nhdplus catchment area km2 in our case the constants are used to convert the travel time to seconds the t o v formulation assumes an average net incoming flux of 6 35 mm h 1 and we replace that with p o v mm hr 1 as the domain average net incoming flux readers are directed to neitsch et al 2011 cho and engel 2017 and reference therein for more details on the equations to calculate t c h and t o v l c h and s l p c h are readily available from the nhdplus geospatial data attributes the overland flow length l o v is estimated as half of the nhdplus catchment area divided by the nhdplus flowline length 0 5 a r e a 1000000 l c h similar to li et al 2013 channel roughness n c h and overland slope s l p c a t are set to 0 035 and 0 01 for the entire domain respectively as they are reasonable initial estimates for large scale studies in the absence of further distributed data overland roughness coefficient n c a t is a function of the land cover type which has a one to one relationship based on the widely used empirical values see the lookup table in hydro tbl by gochis et al 2015 for the net incoming flux p o v 7 mm h 1 is used based on the event rainfall analysis averaged over the sjrb see fig 4c based on the pre processing in gis t ch and t ov hrs for the sjrb can be estimated as shown in fig 10 across the spatial domain river reaches with longer lengths and flatter channel slopes fig 6c generally have greater t ch b similarly catchments with rougher surfaces for example forest in the northeast sjrb and cropland in the southwest sjrb fig 6b also tend to have longer t ov than other areas of the domain fig 10b under this conceptualization the total travel time for each nhdplus modeling unit t total can be represented by the sum of t ch and t ov comparisons between the probability distribution function pdf of the calculated t total and that optimized by david et al 2013 suggest that the optimized travel time is shifted to the lower end of the pdf while the calculated t total has more modeling units with a longer travel time fig 10c this explains why the model performance in exps 1 4 tends to show peakier than observed hydrographs for gauges with forest and crop land cover types while exhibiting good performance for those with faster rises fig 7 such a problem was introduced by the optimization scheme that decreases the travel time k in the same direction for all gauges where the calculated t total showed better spatial heterogeneity across the domain in this case to test the sensitivity of the regional simulation to the travel time estimation we conduct two additional experiments by replacing k with the t total calculated above fig 11 shows the nse boxplots for the modeling experiments with optimized travel time k and the calculated t total in both the 4 km and 1 km modeling experiments the highest nse is achieved when an area weighted coupling interface and the calculated t total are adopted see fig 11 the regional median nse is brought up from 0 4 in exp2 and exp4 to 0 65 for exp5 and exp6 which is due to the calculated t total that slows down the flow concentration processes at locations experiencing a moderate rate of rise floods fig 7 however we note that this calculation only considered spatial heterogeneity in estimating flow travel time at each nhdplus modeling unit future research is warranted to introduce temporal variability to account for the dynamical flow travel time responses which may be better adapted to actual flood prediction scenarios where the routing parameters cannot be obtained prior to a particular event 5 discussions 5 1 computational efficiency model complexity and application settings fig 12 shows an independent test of the model computational costs based on several experimental runs for the texas region 12 this domain has a drainage area of 464 135 km2 and 68 143 nhdplus river reaches and the model is configured at 4 5 km grid resolution for a 3 day simulation in this test using the tacc stampede supercomputing facility it is shown that rapid took 5 of the lsm simulation time with streamflow outputs recorded at all 68 143 river reaches which highlights the computational efficiency of this hybrid model when it is applied to a large spatial domain the best computational performance is achieved using 8 to 16 computing cores we also compare the full feature grid based routing hereafter ffgr based on the 250 m grid with the rapid routing in table 3 both are now supported in wrf hydro to facilitate a discussion on the model efficiency complexity and their applicable settings in ffgr we turn on the saturated subsurface routing overland surface routing channel routing option 3 diffusive routing and groundwater baseflow bucket option 3 pass through to consider all processes represented in the grid based routing rapid on the other hand simplifies the explicit representation of these processes and only focuses on the reach scale streamflow response as an end product of the travel time estimation in the sjrb ffgr and rapid respectively takes 3212 66 s and 14 67 s for routing the efficiency of the latter is partly due to the object oriented feature of rapid and partly due to its coarser time step tied to the average length of the modeling unit i e 250 m for ffgr and 2470 m for rapid see more conceptual discussions in section 4 3 the model performance of the uncalibrated ffgr median nse 0 31 is slightly worse than rapid using optimized routing parameters with centroid based coupling median nse 0 35 exp1 and that with area weighted coupling median nse 0 38 exp2 we note that the nse comparison here is not to discuss the two routing approaches in terms of their skill in simulating floods because pronounced differences still exist in their degrees of calibration and the diffusive kinematic wave approximations on top of the differences in the modeling units the results only show that rapid can serve as a computationally efficient alternative for offline hydrological applications if carefully chosen or physically based routing parameters are given the ffgr on the other hand is more physically complete in explicitly representing the flow paths and moisture redistribution using a grid by grid routing approach which may be suggested for studies to understand detailed water movements and or their feedbacks to the atmosphere at a higher computational cost 5 2 model efficiency and complexity for operational use the considerations on efficiency and complexity are also seen in different model configurations of the nwm the new operational hydrologic prediction tool for the united states using wrf hydro as the backbone building upon the noah mp lsm and the channel routing on the 2 7 million nhdplus river reaches maidment 2017 the nwm is now producing real time predictions for streamflow and all lsm simulated states and fluxes at 18 hr short range 10 day medium range and 30 day long range forecasts together with one historical analysis run to support the water intelligence of the nation http water noaa gov about nwm while the short to medium range nwm forecasts implement the ffgr routing to account for the best supported physical representations albeit needing further performance evaluation and parameter calibration the long range forecast implements a scheme similar to the presented wrf hydro rapid framework because of trade off considerations between affordable computational power and model complexities thus the hybrid modeling strategy and the results for flow travel time estimation to account for more spatially distributed details in this study should be appropriate for the nwm long range forecast which is expected to improve upon those studies only separating the flow travel time estimations for larger river basins or using different river reach lengths however future research is also warranted to introduce temporal variability du et al 2009 to better adjust these parameters for use in actual predictions several underutilized nhdplus geospatial datasets are also discussed here for potential future implementation in this hybrid framework including artificial water pathways such as pipelines and ditches section 3 small to medium sized lake ponds and swamps and dams springs and other scattered features on land not shown these point features dams and springs line features urban drainages and polygon features lake ponds are well established and compiled as georeferenced gis data fabrics lehner and grill 2013 lin et al 2015 at the continental to global scales however these features are difficult to be systematically represented in traditional grid based lsms due to their geographically scattered characteristics the presented hybrid framework has the potential to explicitly incorporate them from the perspective of flexible data structures mizukami et al 2016 but future work that engages the gis modeling with the lsm development is needed to take better advantage of these vector based data 6 conclusions a vector based river routing model named rapid was implemented as a new routing option in the community wrf hydro modeling framework with the goal of facilitating large scale hydrological modeling this hybrid model combines the grid and vector based modeling units which was demonstrated to be computationally efficient and currently suitable for use in offline hydrological applications see appendix for the software availability such a model coupling work is in accordance with the need to advance land surface modeling and river routing by taking advantage of gis based modeling features clark et al 2015 li et al 2013 by conducting a case study to simulate the inland riverine flood during hurricane ike in 2008 we assess the hybrid model s sensitivity to the land grid resolution and the grid to vector coupling interface we show that the model has reasonable performance in predicting the inland flood discharge at 14 usgs gauges in a highly urbanized domain the hybrid model is more sensitive to the grid to vector coupling interface than to the lsm grid resolutions and a 1 km land grid and an area weighted coupling interface is suggested to have the optimal model performance a gis based approach to estimating flow travel time is adopted to further improve the regional model simulation results presented in this study may have implications for the nwm whose different operational configurations consider trade offs between the model efficiency and complexity the hybrid modeling framework holds promise for modeling other irregular hydrologic features yet future work remains to be done in taking advantage of the under exploited gis geospatial data and introducing temporal variability for the flow travel time estimation to further advance the current modeling capability presented in this study acknowledgement this work is supported in part by the national natural science foundation of china under grant number 41375088 and in part by the microsoft research and the jackson school of geoscience ut austin cédric h david is supported by the jet propulsion laboratory california institute of technology under a contract with the national aeronautics and space administration david gochis and wei yu are supported by the national science foundation through its cooperative funding of the national center for atmospheric research additional support for gochis and yu were provided by nsf earthcube grant 1343811 kevin sampson ncar is acknowledged in providing gis support p l z l y d j g d r m and c h d proposed the implementation of a vector based river network model in the wrf hydro framework p l worked on the code development with contributions from w y m a s v and c h d p l conducted the modeling experiments with inputs from z l y and d j g appendix a software availability the preliminary code package for the hybrid model is downloadable at https www ral ucar edu sites default files public projects wrf hydro v3 0 more updated versions are available upon request to the corresponding author liang jsg utexas edu the updated code package uses rapid v1 3 0 david et al 2013b with hash table features for efficient river network initializations model compilation needs installation of netcdf and petsc for model i o and matrix based computation suggested versions netcdf 3 6 3 and petsc 3 4 environment variables wrf hydro wrf hydro rapid need to be set to 1 and successful compilation will generate an executable wrf hydro exe in the hydro namelist specify the channel routing option as 4 for running rapid more details can be found at https github com c h david rapid and http rapid hub org docs rapid wrf hydro install pdf 
26359,modelling the dynamics of wildfires is very computationally challenging although three dimensional computational fluid dynamics cfd models have been successfully applied to wildfires the computational time required makes them currently impractical for operational usage in this study we develop a two dimensional propagation model coupled to a pyrogenic potential flow formulation representing the inflow of air generated by the fire this model can accurately replicate features of fires previously unable to be simulated using current two dimensional models including development of a fire line into a parabolic shape attraction between nearby fires and the observed closing behaviour of v shaped fires the model is compared to experimental results with good agreement the pyrogenic potential model is orders of magnitude faster than a full cfd model and could be used for improved operational wildfire prediction keywords wildfire modelling laboratory experiments wildland fire fire behaviour fire spread software availability software name spark wildfire modelling toolkit developers spark development team contact information spark csiro au software and hardware required windows linux or mac device with opencl gpu drivers program languages c c availability graphical user interface and batch mode server version free to download from https research csiro au spark research toolkit freely available for non commercial use on request 1 introduction wildfires are driven by a complex set of physical and chemical processes that interact both between themselves and with the surrounding environment morvan 2010 these processes include thermal degradation pyrolysis and charring reactions of complex carbohydrate fuels the gas and solid phase oxidation reactions of thermal degradation products sullivan and ball 2012 sullivan 2017a and the transfer of heat liberated from these processes to adjacent fuels through advection of hot gases thermal radiation and transport of burning material sullivan 2017b the bulk behaviour and spread of a wildland fire can be reasonably successfully modelled using a variety of modelling approaches sullivan 2009a b c these range in a continuum from fully physical numerical models simeoni et al 2011 peace et al 2015 canfield et al 2014 through statistically based empirical models of the pseudo steady or median rate of forward spread of a fire cheney et al 2012 anderson et al 2015 cruz et al 2015a to mathematical analogue models hilton et al 2016b encinas et al 2007 however modelling of fine temporal and spatial scale fire behaviour in the order of seconds and metres has generally been less successful cruz and alexander 2013 of particular interest from both a fire science modelling perspective as well as for fire management and suppression is the ability to predict the behaviour and propagation of the fire perimeter this perimeter is defined by the boundary between the burning and un burnt regions of fuel central to this is understanding how the behaviour of the fire perimeter interacts with local physical processes namely thermal radiation and convection dominating the transfer of heat liberated during combustion in the flame zone to adjacent fuel the interaction of these processes along with the ambient environment can play a significant role in determining the non local behaviour of a wildland fire canfield et al 2014 as an example of the interaction of local heat transfer processes fig 1 shows an overhead view of a small scale laboratory fire burning in a reconstructed fuel bed two intersecting lines of fire are lit at a 90 angle the lines are 0 8 m long and burn in uniform dry eucalypt forest litter leaves twigs and bark 6 mm in diameter with a uniform background wind speed of 1 0 m s 1 shown as the vertical arrow in fig 1a a detailed description of these experiments is given in section 2 3 rather than spreading just in the direction of the wind the fire lines appear to move towards each other filling in the centre of the v approximate local spread directions are indicated with small arrows after 20 s the fire has filled in the centre of the v and appears to follow this inward spread direction even at the tips of the ignition region fig 1c dashed lines subsequently the fire spreads forward in the direction of the wind such effects represent important aspects of fire behaviour that must be incorporated into predictive models however the role and relative strengths of convectively induced pressure effects convergence of hot gases from the flame zones towards the convective centre and radiative effects from the increased flame view factor of the fuel at the centre in the dynamics of fire propagation are unclear a number of dedicated studies have investigated the relative strength of each of these effects anderson 1969 wolff et al 1991 morandini et al 2001 with no clear verdict on which is dominant in wildfires hilton et al 2016a some experimental studies have concluded that radiation is the primary mechanism for fire spread silvani and morandini 2009 particularly in the absence of wind albini 1985 anderson 1964 whereas others have reported convection as the mechanism of fire spread pitts 1991 anderson et al 2010 or a mixture of the two depending on fire conditions frankman et al 2010 2013 finney et al 2015 previously we investigated the use of perimeter curvature as a proxy for such local small scale fire spread effects hilton et al 2016a in this earlier study the propagation of the fire perimeter was modelled using a extra term based on local curvature of the fire perimeter in addition to bulk effects of wind and fuel this extra term imposed an additional rate of spread inversely proportional to the fire perimeter curvature negative curvature a concavity positively affected fire spread whereas positive curvature a convexity negatively affected fire spread this effect of curvature has been observed both in our experiments and detected in large scale fires using remote sensing methods ononye et al 2007 while this method showed a good fit to field scale experimental fires it had significant limitations as it could not model observed interactions between fire lines that were separated the study also left open the question of whether this local scale behaviour was the result of radiative or convective effects furthermore the application of curvature was found not to correctly account for behaviour of fires of certain geometries in large scale coupled wind fire models thomas et al 2017 finally the implementation of curvature based method required a semi implicit method for stability which was numerically intensive and therefore reasonably slow to compute in this study we show that a two dimensional model based on the air flow around a fire provides a more robust match than the curvature model to wildfire experiments over a parameter space with length scales ranging from metres up to tens of metres the model is based on a potential flow formulation in the near ground plane around a fire and is essentially a corrective pressure gradient accounting for effect of the updraught of the fire plume on the heat flux from the flame zone to adjacent fuel the model considers two dimensional flow in the near ground plane rather than modelling the full three dimensional inflow dynamics smith et al 1975 raupach 1990 potter 2012 to our knowledge no similar two dimensional models have been proposed for wildfire modelling although a laplacian pressure forcing term as input to a coupled meteorological interface model has been presented by achtemeier 2013 as part of a unique rabbit hopping wildfire model although the model presented here is a considerable simplification of the pressure field around a wildfire it shows remarkable performance in predicting the propagation of small scale fires additionally the model also provides a physical explanation for the curvature based model and can account for interaction effects between separated fire lines the pyrogenic potential model is very straightforward to implement and works in two dimensions making it more computationally efficient taking on the order of seconds to run than a fully three dimensional model which can take several days to run on high end supercomputers linn et al 2002 mell et al 2007 sullivan 2009a this relative efficiency may make the model suitable for improving operational fire prediction models the model is implemented in the level set based fire perimeter propagation framework called spark hilton et al 2015 the details of the model and comparison to experimental fires are detailed in the following sections 2 methodology 2 1 level set method the growth of the fire perimeter was modelled using the level set method sethian 2001 using the spark fire propagation framework hilton et al 2015 miller et al 2015 the level set method is a general purpose model for moving and merging interfaces rather than representing the interface directly the method updates the distance from the interface across a grid naturally handling complex topological changes such as breaking and merging the ability to handle merging interfaces makes the method well suited to fire propagation simulations where the interface represents the division between burnt and un burnt regions defining the fire perimeter the level set equation is 1 ϕ t s ϕ where ϕ is the distance to the nearest interface and s is the outward speed of the interface for wildfire simulations this is the speed of propagation of the fire expressed as the fire rate of spread in a given direction the distance ϕ is signed such that it is negative within the interface and positive outside the speed can vary at each point on the interface which is a further advantage in fire propagation modelling as different rates of spread in different fuel types topography or orientations with the wind can easily be incorporated in the one perimeter in this study eq 1 is time integrated using a second order runge kutta method we use the method given by sethian 1999 to evaluate the magnitude term on the right hand side of eq 1 to ensure numerical stability 2 2 pyrogenic potential model the air flow around the fire is modelled using a two dimensional potential flow formulation batchelor 1967 labelled pyrogenic potential to differentiate it from the conventional fluid flow potential the pyrogenic potential model only considers air flow near the ground at mid flame height which is assumed to flow horizontally close to the ground until it reaches the fire whereupon it moves immediately vertically upwards in the fire plume fig 2 a the model neglects the complexities of plume dynamics regarding the plume as simply a vertical sink of air at a particular level however the assumption that horizontal entrainment is directly proportional to the vertical plume speed at a given level is commonly used in theories of plume dynamics and has been shown to closely match experimental measurements morton et al 1956 any smooth well behaved vector field vanishing as distance r can be uniquely expressed via the helmholtz decomposition as 2 u ψ χ where the three dimensional air flow velocity is given by u u v w ψ is the scalar pyrogenic potential and χ is the vector pyrogenic potential taking the divergence of both sides of eq 2 gives the poisson equation 3 2 ψ u the vector pyrogenic potential is obtained by taking the curl of both sides of eq 2 giving the set of poisson equations 4 2 χ u the model can easily account to account for vorticity using eq 4 and could be used to model certain aspects of wildfire behaviour which appear to be a result of vorticity such as the spread of fires perpendicular to the wind direction along ridges under certain conditions simpson et al 2013 in this study however we have found that the dynamics of all behaviour observed in experiments can be approximated as irrotational u 0 requiring only the scalar potential term ψ the investigation and implementation of vorticity in the pyrogenic model will be investigated in future work the air flow is assumed to be incompressible u 0 in three dimensions eq 3 with the incompressibility condition simply gives a laplace equation for the potential involving no source terms however if we restrict the model to the two dimensional near ground plane and assume the vertical speed w to be zero outside the flaming zone the incompressibility condition u gives 5 x u y v z w within flaming region 0 outside flaming region where the term z w with units s 1 is parameterised as ν z w and is assumed to be a function of factors such as the fire line intensity fuel combustion rate heat release rate or flame height in the near ground plane the poisson equation becomes 6 x 2 ψ y 2 ψ x u y v ν within flaming region 0 outside flaming region where the sink or forcing term ν represents the upwards air flow away from the ground this will clearly be dependent on some function of the heat release or intensity of the fire if the intensity is zero there will be no heating of the air and no upward flow if the fire is very intense we expect a greater driving of upward flow a number of approaches have been explored for relating updraught velocity of a plume to the intensity of the fire these include wedge shaped plumes as resulting from line sources van wagner 1973 and conical plumes as from point sources gould et al 1997 raupach 1990 suggested a power law relating intensity to updraught velocity for the purposes of this study the linear assumption ν i x was used as a starting point where i x is the fire intensity at point x of the fire perimeter the intensity i x in kw m 1 is given by byram 1959 as i x h w s x where h is the heat of combustion in kj kg 1 w is the spatial fuel load in kg m 2 and s x is the local rate of spread in m s 1 byram s equation is however formulated for a one dimensional fire line to extend this definition from a one dimensional fire line to two dimensions the formula can be expressed as 7 i x γ h w s x δ x x d γ where γ is the one dimensional fire line x γ and δ ϕ is the dirac delta distribution using the identity towers 2007 8 γ f x d γ ω f x δ ϕ x ϕ x d ω where ϕ x is the scalar distance from γ at point x and ω is the two dimensional domain gives 9 i x h w s x δ ϕ x ϕ x where we have used f x h w s x δ x x in eq 8 a schematic illustration of the implementation at the fire line is shown in fig 2b the distance ϕ is calculated automatically in the level set function making computation of 9 very straightforward for numerical computations the delta distribution must be expressed in a smoothed form with the same properties under integration here we use the smoothed function 10 δ s ϕ lim ε 2 ε e ε ϕ e ε ϕ 2 where ε is a smoothing parameter with dimensions m 1 the value of 1 ε effectively controls the maximum distance the function is smoothed over and is typically set to ε 1 n δ where n is a number of cells the function is smoothed over and δ is the grid spacing results appear insensitive to values of n anywhere between 2 and 10 cells above 10 cells the smoothing becomes too diffuse whereas under 2 cells eq 10 becomes improperly represented introducing numerical error all simulations in this study used n 5 cells for smoothing eq 10 was derived from the derivative of the logistic function and although there are several other choices for the numerical implementation of the smoothed delta distribution this was chosen as it has compact support and superior numerical stability in comparison to other functional forms the intensity reaches a smooth maximum at the interface as ϕ 0 fig 2c the final form of the forcing term ν is then given by 11 ν k s x δ s ϕ x ϕ x where k is a parameter taking into account any proportionality factor between the forcing and intensity as well as the fuel parameters h and w in this study the parameter k groups both the proportionality factor and the fuel terms together such that the forcing can be expressed as a function of the rate of spread s the dependence of these two factors will be considered in future work to apply the method the forcing term eq 11 was evaluated over the domain using the known value of the distance function ϕ solving the poisson equation eq 6 gave the pyrogenic potential ψ the gradient of which eq 2 with χ 0 gave the velocity field for the air inflow due to the fire fig 2d zero neumann boundary conditions were used at the boundaries of the domain representing zero upward flow far from the flame region it can be noted that the potential flow method is linear so any ambient wind field u a can simply be added to this inflow field to give a total wind field u total u a u alternatively the inflow can be regarded as a perturbation of the ambient wind field in which the fire is burning achtemeier 2013 with the same result the poisson equation is extremely common in physical systems and a number of very efficient computational methods exist for its solution we used the multi grid algorithm briggs et al 2000 which was particularly well suited to the parallel stream processor gpu architecture used by the spark code base for the simulations in this study although this method is straightforward to apply using the level set the pyrogenic potential model makes no assumption about the underlying computational technique used and a suitable form of eq 9 could be used with other perimeter propagation techniques such as vector front tracking or rasterised cellular automata sullivan 2009c 2 3 experiments a number of studies have been carried out into the behaviour of line fires and merging line fires under different combustion conditions in the csiro pyrotron a large combustion wind tunnel sullivan et al 2013 mulvaney et al 2016 results from these experiments sullivan et al 2018 were used to compare to simulations using the pyrogenic potential model these experiments included both separated and joined angled fire lines burning in eucalypt fuel beds with 1 m s 1 wind still frames were extracted from planar video footage of the fire which were then calibrated and rectified using the open source software package opencv bradski 2000 the simulations were also compared to large scale outdoor experimental fires conducted in continuous grasslands cruz et al 2015b the experiments were carried out in 33 m by 33 m plots of grass cured to different levels the 2 m wind speed upwind of the ignition line was recorded for each experiment and used in the simulations the fires were recorded from above by an unmanned aerial vehicle uav and footage of the fires was rectified onto a flat plane to compare against the simulation results this rectification was carried out using the surf feature detection utilities of opencv two experiments from these grassland fire were used for comparison plot 24 and plot 32 these were chosen as they showed clear non linear interactions in fire propagation plot 24 was ignited from either end of the northern edge this lighting pattern initially caused an inverted v in the fire that was found to develop into a broad parabolic head stretched in the wind direction plot 32 was ignited from the half way points on the south eastern and south western edges resulting initially in a deep v shaped fire in a similar manner to plot 24 this v rapidly closed before the fire developed into a narrow parabolic shape 3 results 3 1 example application a simple scenario demonstrating the effect of pyrogenic potential on the propagation of a fire perimeter is a line fire under constant wind conditions the basic behaviour of the model is shown in fig 3 where the fire progression is shown as a set of isochrones at 5 s intervals over a simulation duration of 30 s in total with a spatial grid resolution of δ 0 1 m with a resulting shape parameter ε 2 for n 5 cells the wind direction was perpendicular to the fire line with a speed of 2 m s 1 as illustrated by the vertical arrow in fig 3 the perimeter propagation model used for this example is a simple first order model hilton et al 2016b 12 s u 0 u 1 max u ˆ n ˆ 0 where s is the outward speed of the perimeter u ˆ is the wind direction vector n ˆ is the normal vector to the fire front and u 0 and u 1 are model coefficients representing the rate of spread in the absence of wind and frontal rate of spread component respectively for the example shown in fig 3 the arbitrary values u 0 0 1 m s 1 and u 1 0 5 were used for demonstration with the forcing term k given under each image using no pyrogenic potential k 0 results in the initial condition a straight ignition line propagating steadily in the direction of the wind without much modification in overall shape however the addition of a pyrogenic potential effect rounds the perimeter and results in a decrease in the overall front speed the resolution independence of the model is demonstrated for three different grid spacing values δ in fig 4 for the k 5 case shown fig 3 each has the same speed function eq 12 with u 0 0 1 m s 1 u 1 0 5 and shape parameter ε 2 isochrones are coloured by the spatial grid resolution used in each simulation the resolutions used were double δ 0 2 m blue and half δ 0 05 m green the resolution used in fig 3 there is a small amount of additional rounding on the half resolution case δ 0 2 m with a slight increase in overall distance travelled in the wind direction compared to the δ 0 1 m case overall however the isochrone spacing and overall shape match well between the cases 3 2 laboratory experimental fire comparison to compare the pyrogenic model with real fires simulation results using the model are shown superimposed over images of a laboratory experimental line fire in fig 5 the experimental fire was lit from a line of length 0 8 m with an ambient wind speed of 1 m s 1 the simulations used eq 12 with u 0 0 5 mm s 1 u 1 0 04 k 80 with a spatial grid resolution of δ 2 m m with a resulting shape parameter ε 100 for n 5 cells the values for k were manually chosen for demonstration to provide a good fit to the experiment a quantitative match for these parameters was carried out for the grass fire experiments in the following section but the manual procedure used to identify the fire regions proved to be too subjective for these small scale experiments the simulation results are shown as a white line superimposed over the experimental images at 5 s intervals the experimental fire spread very slightly further on the right flank possibly due to a slight axial vortex in the pyrotron sullivan et al 2013 despite this the results show a good overall qualitative match to both the parabolic shape and extent of the fire over the duration of the experiment it should be emphasised that no current two dimensional perimeter propagation models can dynamically produce this rounding effect which develops as a consequence of the coupled flow field in the pyrogenic model an interesting consequence of the coupled flow field in the pyrogenic model is a predicted convergence of separated fire lines for example fig 6 shows isochrones every 10 s from simulations of two parallel fire lines ignited simultaneously these simulations used eq 12 with the same parameters as the earlier example shown in fig 3 u 0 0 5 mm s 1 u 1 0 04 k 5 and ε 2 each fire perimeter simulated without pyrogenic potential left side of fig 6 simply moves outwards from the ignition condition and the isochrones do not meet after 60 s the speed of propagation of each fire in this case is the same as a single fire by itself in contrast the fire perimeters simulated with pyrogenic potential right side of fig 6 move towards the convective centre of the pair of lines at an increased rate and converge the speed of propagation of each set of perimeters toward the other is much faster than that of a single fire by itself this convergent acceleration effect has been observed in free burning fires over a range of scales and has variously been labelled as a jump or junction fire the effect has been particularly evident in the behaviour of interacting v shaped ignition lines viegas et al 2012 sullivan et al 2018 fig 7 shows an overhead view of an experimental v shaped fire in the pyrotron the interior angle of the v for this experiment was 90 with ignition line lengths of 0 8 m the progression of the fire from 5 s to 20 s is shown a uniform ambient wind field of 1 0 m s 1 was applied during the experiment the direction of which is indicated in the top panel of fig 7 the flaming region is predominantly confined to interior of the v with the flames directed towards the centreline of the v the left column of fig 7 shows the simulation results without the pyrogenic potential model applied the right column of fig 7 shows the simulation results with the pyrogenic potential model applied the simulations used the same values for the experimental line comparison shown in fig 5 u 0 0 5 mm s 1 u 1 0 04 ε 100 and k 80 it can be seen that the rounded upper edges of fire and the convergence towards the centre of the v are replicated by the simulation although the pyrogenic model well matches the experimental results of the v shaped ignition lines our previously presented curvature based models had similar success however a particular case for which curvature based methods are not applicable is that of separated fire lines as the fire lines do not join the curvature at the apex is zero and curvature based models cannot predict any interaction between the fires to demonstrate applicability of the pyrogenic potential model to this case the simulation was applied to the results of a pyrotron experiment using the same set up and wind conditions as that described above but with a 150 mm separation at the apex of the v as shown in fig 8 in this case the orientation of the flame and the draw towards the centre of the v is similar although not as strong as the connected case above the simulation with the pyrogenic potential model applied fig 8 right replicates the convergence of the fire toward the convective centre and matches the overall envelope of the flaming region well again a comparison to an experimental simulation without pyrogenic potential fig 8 left illustrates the inhibiting effect of the pyrogenic potential in the spread at the tips of the v the convergent acceleration in v line fires may have a major effect on multiple coalescing small fires this is a scenario very common in wildfires during ember storms and deep flaming events where multiple firebrands land and start fires downwind of the head of the fire initial simulations using the pyrogenic model have shown an increased peak fire intensity compared to simulations without pyrogenic feedback on the wind field hilton et al 2017 experimental studies similar to those presented here are being planned to compare to the model 3 3 experimental grass fire comparison simulations using the pyrogenic potential model were compared with two larger scale experimental grass fires the value of the fire spread rate in the absence of wind was found to be negligible for these experimental fires so the simulations were run using eq 12 with u 0 0 video captured from an overhead drone of the progression of the fires was rectified onto the ground plane and manually digitised into a mask of burnt or burning and un burnt regions as shown schematically in fig 9 a full details of the processing technique used for the rectification and digitisation process is given in hilton et al 2016a the digitised experimental mask 5 s after ignition was used for the starting condition of simulations rather than at the start of the fire due to the complicated ignition sequence for the experimental fires frames were digitised and compared to the simulated results at 10 15 20 and 25 s after the start of the fire in this study we used the sørensen similarity index jean baptiste et al 2013 to compare the simulated burnt regions with the experimental masks this index ranges from 0 to 1 with 1 representing a perfect match in discrete form this is 13 s 2 a e a s a e a s where the sums are taken over all cells in the discretised domain the subscript e denotes experimental values s denotes simulated values and a e and a s have the values 1 for burnt or burning regions and 0 for un burnt regions for experimental and simulated domains respectively the values of a can represent the fractional amount of fuel left in the cell jean baptiste et al 2013 but this could not be determined from the experimental images and a binary value of 0 or 1 was used in lieu of more detailed information a number of simulations over a range of forcing parameters k and wind parameters u 1 were carried out to find the optimal value matching simulations to experiments the values of k ranged from 1 to 9 with a step of 0 4 and the values of u 1 ranged from 0 1 m s 1 to 0 9 m s 1 with a step of 0 04 m s 1 the experimental domain was 33 m 33 m of cured grass with a fire break of 1 m on each side giving a domain of 35 m 35 m a cell spacing of δ 0 1 m was used for the simulations with a shape parameter of ε 2 in all cases the contour plot of the average sørensen similarity index for all times and for both experimental fires is shown in fig 9b unlike our previous curvature study hilton et al 2016a there is no simple convex form to the plot although a maximum value of 0 961 can still be found for parameter values of u 1 0 58 m s 1 and k 5 4 marked as a solid circle on fig 9b a comparison of simulations using these parameter values to the experimental images are shown in fig 10 plot 24 and fig 11 plot 32 the simulation perimeter overlaid onto the experimental images is shown on the left side of the figure and a visualisation of the pyrogenic potential field is shown on the right in this figure the scalar pyrogenic potential value shown as a colour hue the resulting vectors from the gradient of the scalar field and the forcing term ν from eq 11 as a shaded dark band the pyrogenic model provides a good overall match in perimeters for both sets of simulations and experimental fires for the same optimal value of k and u 1 4 discussion the pyrogenic potential model represents perhaps the simplest pressure based feedback on the propagation of a fire line despite the range of assumptions used to create the model applying this pressure based forcing allows characteristics of fire behaviour to be modelled that are not possible using existing perimeter propagation techniques these include observed effects in the closing of v line fires convergence between disconnected fire lines and provides a mechanism for both the rounding effects and inhibiting of spread in the end of line fires the method appears to provide results which qualitatively match experimental results over a parameter space with length scales ranging from metres to tens of metres the end effects in line fires are perhaps the most interesting result as there is currently no technique in two dimensional wildfire modelling which naturally replicates this parabolic rounding as the results in figs 3 and 5 show this rounding occurs naturally within the model as the potential is greatest at the centre of the line creating an in draw effect which results in the reduction of wind at the extremes of the perimeter and thus reduces the rate of spread the same effect occurs in two parallel line fires but here the effect results in the fires being drawn towards each other before growing outward laterally from the centre as shown in fig 6 future experiments are planned to investigate this convergent effect in parallel fire lines for the v line fires the model provides a good fit to experimental fires as demonstrated in figs 7 and 8 it is however difficult to compare simulated and experimental perimeters at small scales as the presence of flames makes it difficult to define the flame zone precisely gould et al 2017 regardless it is possible to see that the direction of spread and end effects of the v line are replicated by the model very well for the grass fire experiments the dynamic effects of the v line closing are very apparent especially in the case of plot 32 despite the good match to experimental results it is likely that the model only applies under certain conditions which have not been fully explored in the experimental parameter space the applicability of the model may be dependent on the factors such as the relative buoyancy force compared to the wind shear force which could be summarised using the dimensionless richardson number r i the dependence of the model on such dimensionless groups will be explored in future work the current assumptions of the model are firstly that density variations within the system are negligible apart from within the plume where it acts as a buoyant force this assumption is essentially the boussinesq approximation and justifiable as the speed of sound controlling pressure re distribution is much greater than the vertical speed of the fire the second assumption of the model is that the plume is not significantly affected by the wind this assumption is more problematic as the wind tilts the flame resulting in increased radiative heat flux on the fuel under the flame at higher wind speeds this effect appears to trigger a transition at r i 1 where the flame attachment length significantly increases and alters the dynamics of the fire apte et al 1991 tang et al 2017 from visual observations of the cases used within this study it appears that the that the flames are tilted but the tilt angle is small suggesting r i is close to or slightly greater than unity it is likely that the current model applies only within this regime however an improved version of the model could take attachment into account with an additional advective component to the model moving the location of the local heat flux maximum away from the modelled fire perimeter furthermore there is the question of the most accurate way to map the forcing term from the fire the simplest which was found to provide the closest results to experimental observations is to map the forcing term as a smoothed dirac delta distribution representing a line of high flame intensity at the edge of the flame zone however there are other options for this forcing term for example mapping the entire flame zone as a forcing or modelling flame intensity or heat release rate using a decay function in the flaming regions and using this function as the forcing term overall however the results are remarkable considering the model has essentially one fitted parameter ν in this study we assumed this parameter was proportional to the fire intensity however ν could be based on actual experimental measurements for updraught strength as a function of fire intensity such as that of raupach 1990 or charland and clements 2013 alternatively a functional relationship for ν could be determined from a coupled fluid dynamic model of wildfire plumes thomas et al 2017 lastly it can be noted that the curvature of a scalar field is closely mathematically related to the laplace operator this explains why the previous developed curvature model hilton et al 2016a provided a good match in certain cases for continuous fire lines a curvature forcing term is functionally identical to the forcing resulting from the poisson equation in this model the pyrogenic potential model has the advantage of coupling between separate fire lines through the potential field and is more straightforward to implement computationally than the curvature based model which required a semi implicit numerical technique 5 conclusion we have shown that near ground level air flow effects in wildfires can be modelled using a potential flow formulation as this is based on fire line intensity we have called this pyrogenic potential to distinguish this plume driven perturbation term from regular pressure driven flow the model only requires one fitted physical parameter ν s 1 a measure of updraught strength within the plume this pyrogenic potential model was implemented and coupled with a level set perimeter propagation model the results of simulations using this coupled model match well to more complex computer models and real world experiments at both laboratory and field scale despite the wide range of assumptions in the formulation of the model it appears to improve predictions of perimeter propagation over the parameter space of the experiments used in this study the potential is based on a two dimensional poisson equation and is therefore straightforward to implement computationally once the pyrogenic potential has been calculated in the model the gradient can be added to a correction term to the ambient wind field the model is faster to calculate than our previously presented curvature based model and also allows disconnected fire lines to couple through the potential term the results from the pyrogenic potential model presented here indicate that a simple pressure forcing term can explain multiple phenomena in fire propagation these include the development of a line fire into a parabolic shape and the closing behaviour of junction fires furthermore we have found that the same value of ν applies to fires under different ignition conditions on the same length scale and a suitable parametrisation of ν may be able to predict these different phenomena under a range of conditions the model is well suited to operational usage and could easily incorporate additional factors such as vorticity and terrain forcing effects the applicability and extension of this model to additional types of fire behaviour and phenomena will be the subject of future investigation 
26359,modelling the dynamics of wildfires is very computationally challenging although three dimensional computational fluid dynamics cfd models have been successfully applied to wildfires the computational time required makes them currently impractical for operational usage in this study we develop a two dimensional propagation model coupled to a pyrogenic potential flow formulation representing the inflow of air generated by the fire this model can accurately replicate features of fires previously unable to be simulated using current two dimensional models including development of a fire line into a parabolic shape attraction between nearby fires and the observed closing behaviour of v shaped fires the model is compared to experimental results with good agreement the pyrogenic potential model is orders of magnitude faster than a full cfd model and could be used for improved operational wildfire prediction keywords wildfire modelling laboratory experiments wildland fire fire behaviour fire spread software availability software name spark wildfire modelling toolkit developers spark development team contact information spark csiro au software and hardware required windows linux or mac device with opencl gpu drivers program languages c c availability graphical user interface and batch mode server version free to download from https research csiro au spark research toolkit freely available for non commercial use on request 1 introduction wildfires are driven by a complex set of physical and chemical processes that interact both between themselves and with the surrounding environment morvan 2010 these processes include thermal degradation pyrolysis and charring reactions of complex carbohydrate fuels the gas and solid phase oxidation reactions of thermal degradation products sullivan and ball 2012 sullivan 2017a and the transfer of heat liberated from these processes to adjacent fuels through advection of hot gases thermal radiation and transport of burning material sullivan 2017b the bulk behaviour and spread of a wildland fire can be reasonably successfully modelled using a variety of modelling approaches sullivan 2009a b c these range in a continuum from fully physical numerical models simeoni et al 2011 peace et al 2015 canfield et al 2014 through statistically based empirical models of the pseudo steady or median rate of forward spread of a fire cheney et al 2012 anderson et al 2015 cruz et al 2015a to mathematical analogue models hilton et al 2016b encinas et al 2007 however modelling of fine temporal and spatial scale fire behaviour in the order of seconds and metres has generally been less successful cruz and alexander 2013 of particular interest from both a fire science modelling perspective as well as for fire management and suppression is the ability to predict the behaviour and propagation of the fire perimeter this perimeter is defined by the boundary between the burning and un burnt regions of fuel central to this is understanding how the behaviour of the fire perimeter interacts with local physical processes namely thermal radiation and convection dominating the transfer of heat liberated during combustion in the flame zone to adjacent fuel the interaction of these processes along with the ambient environment can play a significant role in determining the non local behaviour of a wildland fire canfield et al 2014 as an example of the interaction of local heat transfer processes fig 1 shows an overhead view of a small scale laboratory fire burning in a reconstructed fuel bed two intersecting lines of fire are lit at a 90 angle the lines are 0 8 m long and burn in uniform dry eucalypt forest litter leaves twigs and bark 6 mm in diameter with a uniform background wind speed of 1 0 m s 1 shown as the vertical arrow in fig 1a a detailed description of these experiments is given in section 2 3 rather than spreading just in the direction of the wind the fire lines appear to move towards each other filling in the centre of the v approximate local spread directions are indicated with small arrows after 20 s the fire has filled in the centre of the v and appears to follow this inward spread direction even at the tips of the ignition region fig 1c dashed lines subsequently the fire spreads forward in the direction of the wind such effects represent important aspects of fire behaviour that must be incorporated into predictive models however the role and relative strengths of convectively induced pressure effects convergence of hot gases from the flame zones towards the convective centre and radiative effects from the increased flame view factor of the fuel at the centre in the dynamics of fire propagation are unclear a number of dedicated studies have investigated the relative strength of each of these effects anderson 1969 wolff et al 1991 morandini et al 2001 with no clear verdict on which is dominant in wildfires hilton et al 2016a some experimental studies have concluded that radiation is the primary mechanism for fire spread silvani and morandini 2009 particularly in the absence of wind albini 1985 anderson 1964 whereas others have reported convection as the mechanism of fire spread pitts 1991 anderson et al 2010 or a mixture of the two depending on fire conditions frankman et al 2010 2013 finney et al 2015 previously we investigated the use of perimeter curvature as a proxy for such local small scale fire spread effects hilton et al 2016a in this earlier study the propagation of the fire perimeter was modelled using a extra term based on local curvature of the fire perimeter in addition to bulk effects of wind and fuel this extra term imposed an additional rate of spread inversely proportional to the fire perimeter curvature negative curvature a concavity positively affected fire spread whereas positive curvature a convexity negatively affected fire spread this effect of curvature has been observed both in our experiments and detected in large scale fires using remote sensing methods ononye et al 2007 while this method showed a good fit to field scale experimental fires it had significant limitations as it could not model observed interactions between fire lines that were separated the study also left open the question of whether this local scale behaviour was the result of radiative or convective effects furthermore the application of curvature was found not to correctly account for behaviour of fires of certain geometries in large scale coupled wind fire models thomas et al 2017 finally the implementation of curvature based method required a semi implicit method for stability which was numerically intensive and therefore reasonably slow to compute in this study we show that a two dimensional model based on the air flow around a fire provides a more robust match than the curvature model to wildfire experiments over a parameter space with length scales ranging from metres up to tens of metres the model is based on a potential flow formulation in the near ground plane around a fire and is essentially a corrective pressure gradient accounting for effect of the updraught of the fire plume on the heat flux from the flame zone to adjacent fuel the model considers two dimensional flow in the near ground plane rather than modelling the full three dimensional inflow dynamics smith et al 1975 raupach 1990 potter 2012 to our knowledge no similar two dimensional models have been proposed for wildfire modelling although a laplacian pressure forcing term as input to a coupled meteorological interface model has been presented by achtemeier 2013 as part of a unique rabbit hopping wildfire model although the model presented here is a considerable simplification of the pressure field around a wildfire it shows remarkable performance in predicting the propagation of small scale fires additionally the model also provides a physical explanation for the curvature based model and can account for interaction effects between separated fire lines the pyrogenic potential model is very straightforward to implement and works in two dimensions making it more computationally efficient taking on the order of seconds to run than a fully three dimensional model which can take several days to run on high end supercomputers linn et al 2002 mell et al 2007 sullivan 2009a this relative efficiency may make the model suitable for improving operational fire prediction models the model is implemented in the level set based fire perimeter propagation framework called spark hilton et al 2015 the details of the model and comparison to experimental fires are detailed in the following sections 2 methodology 2 1 level set method the growth of the fire perimeter was modelled using the level set method sethian 2001 using the spark fire propagation framework hilton et al 2015 miller et al 2015 the level set method is a general purpose model for moving and merging interfaces rather than representing the interface directly the method updates the distance from the interface across a grid naturally handling complex topological changes such as breaking and merging the ability to handle merging interfaces makes the method well suited to fire propagation simulations where the interface represents the division between burnt and un burnt regions defining the fire perimeter the level set equation is 1 ϕ t s ϕ where ϕ is the distance to the nearest interface and s is the outward speed of the interface for wildfire simulations this is the speed of propagation of the fire expressed as the fire rate of spread in a given direction the distance ϕ is signed such that it is negative within the interface and positive outside the speed can vary at each point on the interface which is a further advantage in fire propagation modelling as different rates of spread in different fuel types topography or orientations with the wind can easily be incorporated in the one perimeter in this study eq 1 is time integrated using a second order runge kutta method we use the method given by sethian 1999 to evaluate the magnitude term on the right hand side of eq 1 to ensure numerical stability 2 2 pyrogenic potential model the air flow around the fire is modelled using a two dimensional potential flow formulation batchelor 1967 labelled pyrogenic potential to differentiate it from the conventional fluid flow potential the pyrogenic potential model only considers air flow near the ground at mid flame height which is assumed to flow horizontally close to the ground until it reaches the fire whereupon it moves immediately vertically upwards in the fire plume fig 2 a the model neglects the complexities of plume dynamics regarding the plume as simply a vertical sink of air at a particular level however the assumption that horizontal entrainment is directly proportional to the vertical plume speed at a given level is commonly used in theories of plume dynamics and has been shown to closely match experimental measurements morton et al 1956 any smooth well behaved vector field vanishing as distance r can be uniquely expressed via the helmholtz decomposition as 2 u ψ χ where the three dimensional air flow velocity is given by u u v w ψ is the scalar pyrogenic potential and χ is the vector pyrogenic potential taking the divergence of both sides of eq 2 gives the poisson equation 3 2 ψ u the vector pyrogenic potential is obtained by taking the curl of both sides of eq 2 giving the set of poisson equations 4 2 χ u the model can easily account to account for vorticity using eq 4 and could be used to model certain aspects of wildfire behaviour which appear to be a result of vorticity such as the spread of fires perpendicular to the wind direction along ridges under certain conditions simpson et al 2013 in this study however we have found that the dynamics of all behaviour observed in experiments can be approximated as irrotational u 0 requiring only the scalar potential term ψ the investigation and implementation of vorticity in the pyrogenic model will be investigated in future work the air flow is assumed to be incompressible u 0 in three dimensions eq 3 with the incompressibility condition simply gives a laplace equation for the potential involving no source terms however if we restrict the model to the two dimensional near ground plane and assume the vertical speed w to be zero outside the flaming zone the incompressibility condition u gives 5 x u y v z w within flaming region 0 outside flaming region where the term z w with units s 1 is parameterised as ν z w and is assumed to be a function of factors such as the fire line intensity fuel combustion rate heat release rate or flame height in the near ground plane the poisson equation becomes 6 x 2 ψ y 2 ψ x u y v ν within flaming region 0 outside flaming region where the sink or forcing term ν represents the upwards air flow away from the ground this will clearly be dependent on some function of the heat release or intensity of the fire if the intensity is zero there will be no heating of the air and no upward flow if the fire is very intense we expect a greater driving of upward flow a number of approaches have been explored for relating updraught velocity of a plume to the intensity of the fire these include wedge shaped plumes as resulting from line sources van wagner 1973 and conical plumes as from point sources gould et al 1997 raupach 1990 suggested a power law relating intensity to updraught velocity for the purposes of this study the linear assumption ν i x was used as a starting point where i x is the fire intensity at point x of the fire perimeter the intensity i x in kw m 1 is given by byram 1959 as i x h w s x where h is the heat of combustion in kj kg 1 w is the spatial fuel load in kg m 2 and s x is the local rate of spread in m s 1 byram s equation is however formulated for a one dimensional fire line to extend this definition from a one dimensional fire line to two dimensions the formula can be expressed as 7 i x γ h w s x δ x x d γ where γ is the one dimensional fire line x γ and δ ϕ is the dirac delta distribution using the identity towers 2007 8 γ f x d γ ω f x δ ϕ x ϕ x d ω where ϕ x is the scalar distance from γ at point x and ω is the two dimensional domain gives 9 i x h w s x δ ϕ x ϕ x where we have used f x h w s x δ x x in eq 8 a schematic illustration of the implementation at the fire line is shown in fig 2b the distance ϕ is calculated automatically in the level set function making computation of 9 very straightforward for numerical computations the delta distribution must be expressed in a smoothed form with the same properties under integration here we use the smoothed function 10 δ s ϕ lim ε 2 ε e ε ϕ e ε ϕ 2 where ε is a smoothing parameter with dimensions m 1 the value of 1 ε effectively controls the maximum distance the function is smoothed over and is typically set to ε 1 n δ where n is a number of cells the function is smoothed over and δ is the grid spacing results appear insensitive to values of n anywhere between 2 and 10 cells above 10 cells the smoothing becomes too diffuse whereas under 2 cells eq 10 becomes improperly represented introducing numerical error all simulations in this study used n 5 cells for smoothing eq 10 was derived from the derivative of the logistic function and although there are several other choices for the numerical implementation of the smoothed delta distribution this was chosen as it has compact support and superior numerical stability in comparison to other functional forms the intensity reaches a smooth maximum at the interface as ϕ 0 fig 2c the final form of the forcing term ν is then given by 11 ν k s x δ s ϕ x ϕ x where k is a parameter taking into account any proportionality factor between the forcing and intensity as well as the fuel parameters h and w in this study the parameter k groups both the proportionality factor and the fuel terms together such that the forcing can be expressed as a function of the rate of spread s the dependence of these two factors will be considered in future work to apply the method the forcing term eq 11 was evaluated over the domain using the known value of the distance function ϕ solving the poisson equation eq 6 gave the pyrogenic potential ψ the gradient of which eq 2 with χ 0 gave the velocity field for the air inflow due to the fire fig 2d zero neumann boundary conditions were used at the boundaries of the domain representing zero upward flow far from the flame region it can be noted that the potential flow method is linear so any ambient wind field u a can simply be added to this inflow field to give a total wind field u total u a u alternatively the inflow can be regarded as a perturbation of the ambient wind field in which the fire is burning achtemeier 2013 with the same result the poisson equation is extremely common in physical systems and a number of very efficient computational methods exist for its solution we used the multi grid algorithm briggs et al 2000 which was particularly well suited to the parallel stream processor gpu architecture used by the spark code base for the simulations in this study although this method is straightforward to apply using the level set the pyrogenic potential model makes no assumption about the underlying computational technique used and a suitable form of eq 9 could be used with other perimeter propagation techniques such as vector front tracking or rasterised cellular automata sullivan 2009c 2 3 experiments a number of studies have been carried out into the behaviour of line fires and merging line fires under different combustion conditions in the csiro pyrotron a large combustion wind tunnel sullivan et al 2013 mulvaney et al 2016 results from these experiments sullivan et al 2018 were used to compare to simulations using the pyrogenic potential model these experiments included both separated and joined angled fire lines burning in eucalypt fuel beds with 1 m s 1 wind still frames were extracted from planar video footage of the fire which were then calibrated and rectified using the open source software package opencv bradski 2000 the simulations were also compared to large scale outdoor experimental fires conducted in continuous grasslands cruz et al 2015b the experiments were carried out in 33 m by 33 m plots of grass cured to different levels the 2 m wind speed upwind of the ignition line was recorded for each experiment and used in the simulations the fires were recorded from above by an unmanned aerial vehicle uav and footage of the fires was rectified onto a flat plane to compare against the simulation results this rectification was carried out using the surf feature detection utilities of opencv two experiments from these grassland fire were used for comparison plot 24 and plot 32 these were chosen as they showed clear non linear interactions in fire propagation plot 24 was ignited from either end of the northern edge this lighting pattern initially caused an inverted v in the fire that was found to develop into a broad parabolic head stretched in the wind direction plot 32 was ignited from the half way points on the south eastern and south western edges resulting initially in a deep v shaped fire in a similar manner to plot 24 this v rapidly closed before the fire developed into a narrow parabolic shape 3 results 3 1 example application a simple scenario demonstrating the effect of pyrogenic potential on the propagation of a fire perimeter is a line fire under constant wind conditions the basic behaviour of the model is shown in fig 3 where the fire progression is shown as a set of isochrones at 5 s intervals over a simulation duration of 30 s in total with a spatial grid resolution of δ 0 1 m with a resulting shape parameter ε 2 for n 5 cells the wind direction was perpendicular to the fire line with a speed of 2 m s 1 as illustrated by the vertical arrow in fig 3 the perimeter propagation model used for this example is a simple first order model hilton et al 2016b 12 s u 0 u 1 max u ˆ n ˆ 0 where s is the outward speed of the perimeter u ˆ is the wind direction vector n ˆ is the normal vector to the fire front and u 0 and u 1 are model coefficients representing the rate of spread in the absence of wind and frontal rate of spread component respectively for the example shown in fig 3 the arbitrary values u 0 0 1 m s 1 and u 1 0 5 were used for demonstration with the forcing term k given under each image using no pyrogenic potential k 0 results in the initial condition a straight ignition line propagating steadily in the direction of the wind without much modification in overall shape however the addition of a pyrogenic potential effect rounds the perimeter and results in a decrease in the overall front speed the resolution independence of the model is demonstrated for three different grid spacing values δ in fig 4 for the k 5 case shown fig 3 each has the same speed function eq 12 with u 0 0 1 m s 1 u 1 0 5 and shape parameter ε 2 isochrones are coloured by the spatial grid resolution used in each simulation the resolutions used were double δ 0 2 m blue and half δ 0 05 m green the resolution used in fig 3 there is a small amount of additional rounding on the half resolution case δ 0 2 m with a slight increase in overall distance travelled in the wind direction compared to the δ 0 1 m case overall however the isochrone spacing and overall shape match well between the cases 3 2 laboratory experimental fire comparison to compare the pyrogenic model with real fires simulation results using the model are shown superimposed over images of a laboratory experimental line fire in fig 5 the experimental fire was lit from a line of length 0 8 m with an ambient wind speed of 1 m s 1 the simulations used eq 12 with u 0 0 5 mm s 1 u 1 0 04 k 80 with a spatial grid resolution of δ 2 m m with a resulting shape parameter ε 100 for n 5 cells the values for k were manually chosen for demonstration to provide a good fit to the experiment a quantitative match for these parameters was carried out for the grass fire experiments in the following section but the manual procedure used to identify the fire regions proved to be too subjective for these small scale experiments the simulation results are shown as a white line superimposed over the experimental images at 5 s intervals the experimental fire spread very slightly further on the right flank possibly due to a slight axial vortex in the pyrotron sullivan et al 2013 despite this the results show a good overall qualitative match to both the parabolic shape and extent of the fire over the duration of the experiment it should be emphasised that no current two dimensional perimeter propagation models can dynamically produce this rounding effect which develops as a consequence of the coupled flow field in the pyrogenic model an interesting consequence of the coupled flow field in the pyrogenic model is a predicted convergence of separated fire lines for example fig 6 shows isochrones every 10 s from simulations of two parallel fire lines ignited simultaneously these simulations used eq 12 with the same parameters as the earlier example shown in fig 3 u 0 0 5 mm s 1 u 1 0 04 k 5 and ε 2 each fire perimeter simulated without pyrogenic potential left side of fig 6 simply moves outwards from the ignition condition and the isochrones do not meet after 60 s the speed of propagation of each fire in this case is the same as a single fire by itself in contrast the fire perimeters simulated with pyrogenic potential right side of fig 6 move towards the convective centre of the pair of lines at an increased rate and converge the speed of propagation of each set of perimeters toward the other is much faster than that of a single fire by itself this convergent acceleration effect has been observed in free burning fires over a range of scales and has variously been labelled as a jump or junction fire the effect has been particularly evident in the behaviour of interacting v shaped ignition lines viegas et al 2012 sullivan et al 2018 fig 7 shows an overhead view of an experimental v shaped fire in the pyrotron the interior angle of the v for this experiment was 90 with ignition line lengths of 0 8 m the progression of the fire from 5 s to 20 s is shown a uniform ambient wind field of 1 0 m s 1 was applied during the experiment the direction of which is indicated in the top panel of fig 7 the flaming region is predominantly confined to interior of the v with the flames directed towards the centreline of the v the left column of fig 7 shows the simulation results without the pyrogenic potential model applied the right column of fig 7 shows the simulation results with the pyrogenic potential model applied the simulations used the same values for the experimental line comparison shown in fig 5 u 0 0 5 mm s 1 u 1 0 04 ε 100 and k 80 it can be seen that the rounded upper edges of fire and the convergence towards the centre of the v are replicated by the simulation although the pyrogenic model well matches the experimental results of the v shaped ignition lines our previously presented curvature based models had similar success however a particular case for which curvature based methods are not applicable is that of separated fire lines as the fire lines do not join the curvature at the apex is zero and curvature based models cannot predict any interaction between the fires to demonstrate applicability of the pyrogenic potential model to this case the simulation was applied to the results of a pyrotron experiment using the same set up and wind conditions as that described above but with a 150 mm separation at the apex of the v as shown in fig 8 in this case the orientation of the flame and the draw towards the centre of the v is similar although not as strong as the connected case above the simulation with the pyrogenic potential model applied fig 8 right replicates the convergence of the fire toward the convective centre and matches the overall envelope of the flaming region well again a comparison to an experimental simulation without pyrogenic potential fig 8 left illustrates the inhibiting effect of the pyrogenic potential in the spread at the tips of the v the convergent acceleration in v line fires may have a major effect on multiple coalescing small fires this is a scenario very common in wildfires during ember storms and deep flaming events where multiple firebrands land and start fires downwind of the head of the fire initial simulations using the pyrogenic model have shown an increased peak fire intensity compared to simulations without pyrogenic feedback on the wind field hilton et al 2017 experimental studies similar to those presented here are being planned to compare to the model 3 3 experimental grass fire comparison simulations using the pyrogenic potential model were compared with two larger scale experimental grass fires the value of the fire spread rate in the absence of wind was found to be negligible for these experimental fires so the simulations were run using eq 12 with u 0 0 video captured from an overhead drone of the progression of the fires was rectified onto the ground plane and manually digitised into a mask of burnt or burning and un burnt regions as shown schematically in fig 9 a full details of the processing technique used for the rectification and digitisation process is given in hilton et al 2016a the digitised experimental mask 5 s after ignition was used for the starting condition of simulations rather than at the start of the fire due to the complicated ignition sequence for the experimental fires frames were digitised and compared to the simulated results at 10 15 20 and 25 s after the start of the fire in this study we used the sørensen similarity index jean baptiste et al 2013 to compare the simulated burnt regions with the experimental masks this index ranges from 0 to 1 with 1 representing a perfect match in discrete form this is 13 s 2 a e a s a e a s where the sums are taken over all cells in the discretised domain the subscript e denotes experimental values s denotes simulated values and a e and a s have the values 1 for burnt or burning regions and 0 for un burnt regions for experimental and simulated domains respectively the values of a can represent the fractional amount of fuel left in the cell jean baptiste et al 2013 but this could not be determined from the experimental images and a binary value of 0 or 1 was used in lieu of more detailed information a number of simulations over a range of forcing parameters k and wind parameters u 1 were carried out to find the optimal value matching simulations to experiments the values of k ranged from 1 to 9 with a step of 0 4 and the values of u 1 ranged from 0 1 m s 1 to 0 9 m s 1 with a step of 0 04 m s 1 the experimental domain was 33 m 33 m of cured grass with a fire break of 1 m on each side giving a domain of 35 m 35 m a cell spacing of δ 0 1 m was used for the simulations with a shape parameter of ε 2 in all cases the contour plot of the average sørensen similarity index for all times and for both experimental fires is shown in fig 9b unlike our previous curvature study hilton et al 2016a there is no simple convex form to the plot although a maximum value of 0 961 can still be found for parameter values of u 1 0 58 m s 1 and k 5 4 marked as a solid circle on fig 9b a comparison of simulations using these parameter values to the experimental images are shown in fig 10 plot 24 and fig 11 plot 32 the simulation perimeter overlaid onto the experimental images is shown on the left side of the figure and a visualisation of the pyrogenic potential field is shown on the right in this figure the scalar pyrogenic potential value shown as a colour hue the resulting vectors from the gradient of the scalar field and the forcing term ν from eq 11 as a shaded dark band the pyrogenic model provides a good overall match in perimeters for both sets of simulations and experimental fires for the same optimal value of k and u 1 4 discussion the pyrogenic potential model represents perhaps the simplest pressure based feedback on the propagation of a fire line despite the range of assumptions used to create the model applying this pressure based forcing allows characteristics of fire behaviour to be modelled that are not possible using existing perimeter propagation techniques these include observed effects in the closing of v line fires convergence between disconnected fire lines and provides a mechanism for both the rounding effects and inhibiting of spread in the end of line fires the method appears to provide results which qualitatively match experimental results over a parameter space with length scales ranging from metres to tens of metres the end effects in line fires are perhaps the most interesting result as there is currently no technique in two dimensional wildfire modelling which naturally replicates this parabolic rounding as the results in figs 3 and 5 show this rounding occurs naturally within the model as the potential is greatest at the centre of the line creating an in draw effect which results in the reduction of wind at the extremes of the perimeter and thus reduces the rate of spread the same effect occurs in two parallel line fires but here the effect results in the fires being drawn towards each other before growing outward laterally from the centre as shown in fig 6 future experiments are planned to investigate this convergent effect in parallel fire lines for the v line fires the model provides a good fit to experimental fires as demonstrated in figs 7 and 8 it is however difficult to compare simulated and experimental perimeters at small scales as the presence of flames makes it difficult to define the flame zone precisely gould et al 2017 regardless it is possible to see that the direction of spread and end effects of the v line are replicated by the model very well for the grass fire experiments the dynamic effects of the v line closing are very apparent especially in the case of plot 32 despite the good match to experimental results it is likely that the model only applies under certain conditions which have not been fully explored in the experimental parameter space the applicability of the model may be dependent on the factors such as the relative buoyancy force compared to the wind shear force which could be summarised using the dimensionless richardson number r i the dependence of the model on such dimensionless groups will be explored in future work the current assumptions of the model are firstly that density variations within the system are negligible apart from within the plume where it acts as a buoyant force this assumption is essentially the boussinesq approximation and justifiable as the speed of sound controlling pressure re distribution is much greater than the vertical speed of the fire the second assumption of the model is that the plume is not significantly affected by the wind this assumption is more problematic as the wind tilts the flame resulting in increased radiative heat flux on the fuel under the flame at higher wind speeds this effect appears to trigger a transition at r i 1 where the flame attachment length significantly increases and alters the dynamics of the fire apte et al 1991 tang et al 2017 from visual observations of the cases used within this study it appears that the that the flames are tilted but the tilt angle is small suggesting r i is close to or slightly greater than unity it is likely that the current model applies only within this regime however an improved version of the model could take attachment into account with an additional advective component to the model moving the location of the local heat flux maximum away from the modelled fire perimeter furthermore there is the question of the most accurate way to map the forcing term from the fire the simplest which was found to provide the closest results to experimental observations is to map the forcing term as a smoothed dirac delta distribution representing a line of high flame intensity at the edge of the flame zone however there are other options for this forcing term for example mapping the entire flame zone as a forcing or modelling flame intensity or heat release rate using a decay function in the flaming regions and using this function as the forcing term overall however the results are remarkable considering the model has essentially one fitted parameter ν in this study we assumed this parameter was proportional to the fire intensity however ν could be based on actual experimental measurements for updraught strength as a function of fire intensity such as that of raupach 1990 or charland and clements 2013 alternatively a functional relationship for ν could be determined from a coupled fluid dynamic model of wildfire plumes thomas et al 2017 lastly it can be noted that the curvature of a scalar field is closely mathematically related to the laplace operator this explains why the previous developed curvature model hilton et al 2016a provided a good match in certain cases for continuous fire lines a curvature forcing term is functionally identical to the forcing resulting from the poisson equation in this model the pyrogenic potential model has the advantage of coupling between separate fire lines through the potential field and is more straightforward to implement computationally than the curvature based model which required a semi implicit numerical technique 5 conclusion we have shown that near ground level air flow effects in wildfires can be modelled using a potential flow formulation as this is based on fire line intensity we have called this pyrogenic potential to distinguish this plume driven perturbation term from regular pressure driven flow the model only requires one fitted physical parameter ν s 1 a measure of updraught strength within the plume this pyrogenic potential model was implemented and coupled with a level set perimeter propagation model the results of simulations using this coupled model match well to more complex computer models and real world experiments at both laboratory and field scale despite the wide range of assumptions in the formulation of the model it appears to improve predictions of perimeter propagation over the parameter space of the experiments used in this study the potential is based on a two dimensional poisson equation and is therefore straightforward to implement computationally once the pyrogenic potential has been calculated in the model the gradient can be added to a correction term to the ambient wind field the model is faster to calculate than our previously presented curvature based model and also allows disconnected fire lines to couple through the potential term the results from the pyrogenic potential model presented here indicate that a simple pressure forcing term can explain multiple phenomena in fire propagation these include the development of a line fire into a parabolic shape and the closing behaviour of junction fires furthermore we have found that the same value of ν applies to fires under different ignition conditions on the same length scale and a suitable parametrisation of ν may be able to predict these different phenomena under a range of conditions the model is well suited to operational usage and could easily incorporate additional factors such as vorticity and terrain forcing effects the applicability and extension of this model to additional types of fire behaviour and phenomena will be the subject of future investigation 
