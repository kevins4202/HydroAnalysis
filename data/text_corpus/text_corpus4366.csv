index,text
21830,this paper addresses the problem of three dimensional 3 d coupled path following control for a six degrees of freedom underactuated underwater vehicle to compensate for nonzero roll angle an improved 3 d line of sight guidance law is proposed in the kinematic layer especially this novel guidance law can be applied to not only actuated but also underactuated underwater vehicles in roll by assigning the desired roll angle subsequently a simplified time delay controller is designed in the dynamics layer to robustly track the desired guidance angle such that the entire path following errors are bounded finally comparative numerical simulation results among different following paths initial states and controllers are provided to show the outperformance of the designed 3 d coupled path following controller keywords underwater vehicle path following guidance and control roll dynamics 1 introduction underwater vehicles including remotely operated vehicles autonomous underwater vehicles auvs and underwater gliders are powerful tools for ocean commercial scientific and military applications such as high resolution mapping of the deep sea floor measurements of ocean temperature and salinity and mine reconnaissance and neutralization bellingham and rajan 2007 bovio et al 2006 cohan 2008 in accomplishing underwater missions three dimensional 3 d motion control of the vehicle is usually required for instance a ribbon fins propelled underwater biomimetic vehicle manipulator system was introduced to surround and inspect a sunken ship along a spatial helical path wang et al 2018b compared with trajectory tracking path following is a more popular motion control scene in underwater vehicle field as its primary objective is to ensure that a vehicle converges to and follows a desired geometric path without any temporal requirements breivik and fossen 2005 liu et al 2019 wang et al 2019 compared with 3 d path following two dimensional 2 d path following with only three degrees of freedom dofs has been well studied xiang et al 2018 in lapierre and soetanto 2007 a nonlinear feedback control law composed of an approach angle and a model based backstepping dynamics control law was designed to yield the asymptotical convergence of the 2 d horizontal plane path following error to reject unknown model dynamics and disturbances a fuzzy uncertainty observer based path following controller was proposed for underactuated marine vehicles wang et al 2018a in peng and wang 2018 an output feedback path following controller of under actuated auvs moving in vertical plane was designed based on extended state observers and projection neural networks different from 2 d horizontal or vertical plane 3 d space has more coupled dofs which results in complicate kinematic design in path following control following the guidance idea in breivik and fossen 2005 an adaptive fuzzy proportional integral derivative pid controller with 3 d line of sight los was designed and validated xiang et al 2017 to simplify implementation of the above classic double input fuzzy controller a single input nonlinear fuzzy dynamics controller was proposed in yu et al 2017 to achieve robust 3 d path following of an underactuated auv in rout and subudhi 2017 an adaptive pid controller was developed using the derived parameters to accomplish the path following task of an auv in addition to pid control in miao et al 2017 a compound controller for the spatial path following of an underactuated auv based on backstepping and active disturbance rejection control methods was proposed to enhance the robustness against system uncertainties in chu and zhu 2015 an adaptive sliding mode controller was used to track the desired yaw angle pitch angle and surge velocity generated by the guidance system in the kinematic layer compared with pid control the number of control gains in the above adaptive controllers increases significantly which makes the adjustment of the controller difficult note that the roll dynamics in the above 3 d path following control is neglected which means that only five dofs in surge sway heave pitch and yaw directions are considered it is well known that the roll angle cannot always maintain at zero qiao and zhang 2019 in khodayari and balochian 2015 three decoupled pid controllers adjusted by mamdani fuzzy rules in speed heading and depth directions were designed for 3 d coupled motion control of an auv in kim and yuh 2002 a coupled neuro fuzzy controller was implemented on omni directional intelligent navigator odin dynamic model in qiao and zhang 2019 a double loop integral sliding mode control scheme was designed to ensure the finite time stability of tracking control for the 6 dof odin vehicle with system uncertainties yet the odin vehicle has the corresponding control input in each dof and hence it can be seen as a fully actuated vehicle in this paper we aim at addressing the problem of 3 d coupled path following control for a 6 dof underactuated underwater vehicle the main contributions are summarized as follows 1 an improved spatial los guidance law is proposed for 3 d coupled path following in the presence of nonzero roll angle especially this new guidance law can be applied to both fully actuated and under actuated underwater vehicles in addition its guidance steps are simpler compared to yu et al 2017 2 a simplified time delay controller is designed to robustly track the desired guidance angle generated in the guidance layer this dynamics controller has fewer control gains than adaptive fuzzy controllers and can be easily implemented in the actual control system 3 comparative numerical simulation results among different following paths initial states and controllers show the stronger adaptivity and robustness of the designed path following controller the rest of this paper is organized as follows the modeling of 6 dof underactuated underwater vehicles with roll dynamics is presented in section 2 the path following controller composed of the improved 3 d los and time delay control is derived in section 3 section 4 shows comparative numerical simulation results and concluding remarks and further research are contained in section 5 2 modeling of a 6 dof underwater vehicle according to fossen 1994 the motion equations of a 6 dof underwater vehicle in 3 d space can be written as 1 η j η v m v c v v d v v g η τ τ e substituting the first equation of eq 1 into the second one it has the following motion equation described in the inertial coordinate frame 2 m η η c v η η d v η η g η τ τ e with 3 m η j η m j 1 η c v η j η c v m j 1 η j η j 1 η d v η j η d v j 1 η g η j η g η τ j η τ τ e j η τ e here j η r 6 6 is the jacobian matrix with η x y x ϕ θ ψ being the position and attitude of this vehicle and v u v w p q r being the vector of linear and angular velocities m r 6 6 denotes the integrated mass matrix c v r 6 6 and d v r 6 6 are the coriolis and centripetal matrix and the damping matrix respectively g η r 6 1 is the vector of gravity and buoyancy τ r 6 1 and τ e r 6 1 are the resultant vectors of forces and moments generated by actuators and environmental disturbances respectively in addition and 1 denote the transpose and inverse of the matrix respectively therefore denotes the transpose of the inverse of the matrix assumption 1 zheng and feroskhan 2017 m is assumed to be a diagonal matrix namely m d i a g m 11 m 22 m 33 m 44 m 55 m 66 with m i i 0 assumption 2 shojaei 2015 throughout the paper the surge velocity u of the underwater vehicle is assumed to be constant in 3 d path following remark 1 m c d g and τ e are partially known or uncertain by considering the accuracy of experiment and simulation hydrodynamics tests and the effects of time varying and unknown environmental disturbances in this sense any accurate model based controller will be ineffective remark 2 for an underwater vehicle equipped with a stern thruster and two pair of control surfaces τ τ u 0 0 0 τ q τ r which means that this 6 dof vehicle has only three control inputs and is underactuated in sway heave and roll sankaranarayanan et al 2008 peng et al 2019 remark 3 due to the existence of an inherent metacentric height and the underactuated configuration in roll ϕ p 0 is usually assumed in 3 d path following in fact this assumption is not rigorous in practice because of coupling effects between different dofs in this paper the aim of 3 d path following is to let the underwater vehicle with the motion equations in eq 2 converge to and follow the desired spatial path without temporal constraints as shown in fig 1 since the desired path is time independent a virtual point q denoted by p q x q s y q s z q s is usually introduced on the path to guide the underwater vehicle here s is a scalar variable remark 4 if the distance between the vehicle center o and the virtual point q on the path is zero the control objective of 3 d path following has been achieved different from trajectory tracking the velocity of the point q on the path is associated with the motion of the vehicle rather than the accurate time stamp 3 design of a 3 d coupled path following controller a guidance and control architecture is resorted to achieve 3 d coupled path following where the guidance layer is to solve measurable attitude angles and the control layer is to track measurable attitude angles 3 1 attitude angle guidance in the guidance layer we first introduce the path frame f and define the corresponding transformation matrix r f between the path frame and the inertial frame as breivik and fossen 2005 4 r f r z χ q r y υ q where 5 r z χ q c χ q s χ q 0 s χ q c χ q 0 0 0 1 and 6 r y υ q c υ q 0 s υ q 0 1 0 s υ q 0 c υ q among eqs 5 and 6 1 1 throughout the paper the symbols s c and t denote the trigonometric functions sin cos and tan respectively twice rotation angles around the x and y axes are denoted by χ q s atan2 y q s x q s and υ q s arctan z q s x q s 2 y q s 2 respectively here the function atan2 is 2 argument arctangent and returns a single value ξ such that π ξ π and then the path following error vector p f e x e y e z e can be defined in the above path frame as 7 p f e r f p p q with p x y z being the position of the underwater vehicle define the resultant velocity vectors of the vehicle and the virtual point as u w u 0 0 with u u 2 v 2 w 2 and u q u q 0 0 with u q s x q s 2 y q s 2 z q s 2 respectively then the derivative of p f e is 8 p f e s f p f e r w f u w u q where s f is a skew symmetric matrix and is defined as 9 s f 0 χ q c υ q υ q χ q c υ q 0 χ q s υ q υ q χ q s υ q 0 and the rotation matrix r w f between the flow frame and the path frame is given by 10 r w f c χ e c υ e s χ e c χ e s υ e s χ e c υ e c χ e s χ e s υ e s υ e 0 c υ e note that χ e and υ e are two angles of rotation that align x axis of the flow frame with that of the path frame after obtaining the path following error and the error dynamics we can choose the positive lyapunov function candidate v e 0 5 p f e 2 and then calculate its derivative as 11 v e x e u c υ e c χ e u q z e u s υ e y e u s χ e c υ e to let the derivative v e be negative it is straight to assign 12 s u c χ e c υ e k x x e x q s 2 y q s 2 z q s 2 χ e arctan y e δ y υ e arctan z e δ z where k x δ y and δ z are positive constants which ensures that 13 v e k x x e 2 u c υ e y e 2 δ y 2 y e 2 u z e 2 δ z 2 z e 2 it can be concluded that the path following error is lyapunov stable using the guidance law in eq 12 remark 5 the guidance law in eq 12 links the flow frame of the vehicle to the path frame and needs to indirectly calculate the attitude angles defined in the flow frame yet the onboard navigation sensors only provide the information of attitude angles i e ϕ θ and ψ these measurable attitude angles are defined in the body frame rather than the flow frame for the sake of transforming unmeasurable attitude angles into measurable one the following equivalent transformation is introduced both the consecutive rotation of r f and r w f and the consecutive rotation of r b and r w b map the resultant velocity vector u w equivalently to the inertial frame this can be expressed as 14 r f r w f u w r b r w b u w where the transformation matrix from the body frame to the inertial frame r b r z ψ r y θ r x ϕ and its expansion is 15 r b c ψ c θ s ψ c ϕ s ϕ s θ c ψ s ψ s ϕ c ϕ s θ c ψ s ψ c θ c ψ c ϕ s ϕ s θ s ψ c ψ s ϕ c ϕ s θ s ψ s θ s ϕ c θ c ϕ c θ and the transformation matrix from the flow frame to the body frame r w b r y α r z β and its expansion is 16 r w b c α c β c α s β s α s β c β 0 s α c β s α s β c α with α arctan w u being the angle of attack of the underwater vehicle and β arcsin v u being its side slip angle substituting eqs 4 10 15 and 16 into eq 14 yields 17 a 1 c ψ b 1 s ψ c 1 a 2 s ψ b 2 c ψ c 2 a 3 s θ b 3 c θ c 3 with a 1 c α c β c θ s β s θ s ϕ s α c β s θ c ϕ b 1 s β c ϕ s α c β s ϕ d c 1 c χ q c χ e c υ q c υ e s χ q s χ e c υ e c υ q s χ q s υ e a 2 a 1 b 2 b 1 c 2 s χ q c υ q c χ e c υ e c χ q s χ e c υ e s χ q s υ q s υ e a 3 c β c α b 3 s β s ϕ s α c β c ϕ and c 3 s υ q c υ e c υ e c υ q s υ e the solution of eq 17 is 18 ϕ γ θ arcsin c 3 a 3 2 b 3 2 arctan b 3 a 3 ψ atan2 a 1 c 2 b 2 c 1 b 1 c 2 a 2 c 1 where γ is a design variable and will be assigned later remark 6 for a designated guidance law in eq 12 the corresponding eq 18 can be seen as the equivalent attitude guidance angles hence ϕ θ and ψ in eq 18 can be replaced with ϕ d θ d and ψ d remark 7 in the design of above attitude guidance angles the intermediate attitude angles of the underwater vehicle including the azimuth angle χ and the elevation angle υ are not required compared with breivik and fossen 2005 and yu et al 2017 this simplifies the guidance steps remark 8 compared with xiang et al 2017 yu et al 2017 rout and subudhi 2017 miao et al 2017 and chu and zhu 2015 the roll angle is not neglected in the above guidance law even if for an underwater vehicle that is underactuated in roll this consideration is more realistic 3 2 attitude angle tracking in the control layer we will design a time delay based controller to robustly track the above attitude guidance angles such that the control objective of the entire 3 d coupled path following can be achieved as the guidance layer has transformed the position following into the angle tracking only 3 dof motion equations are considered in the subsequent dynamics control layer hence according to eq 2 and assumption 1 we can rewrite the last three motion equations with respect to angle as 19 m 2 η 2 η 2 t 2 η η η τ 2 where η 2 ϕ θ ψ and t 2 η η η m 2 m 2 η 2 c 2 v η η d 2 v η η g 2 η τ e 2 here m 2 j 2 η 2 m 2 j 2 1 η 2 r 3 3 with m 2 d i a g m 44 m 55 m 66 and 20 j 2 η 2 1 s ϕ t θ c ϕ t θ 0 c ϕ s ϕ 0 s ϕ c θ c ϕ c θ and m 2 denotes an estimated value of m 2 c 2 r 3 6 d 2 r 3 6 g 2 r 3 1 τ 2 r 3 1 and τ e 2 are system matrices composed of the last three rows of c d g τ and τ e in eq 3 respectively remark 9 the mass matrix m is composed of the body mass matrix and the added mass matrix the former can be roughly calculated in the design of the underwater vehicle hence we can obtain a known matrix m 2 based on the theoretical calculation which is named as the nominal matrix while t 2 is usually unknown cui et al 2017 resorting to the feedback linearizing technique and time delay control the control laws in roll pitch and yaw are designed as 21 τ 2 m 2 η 2 η 2 d k 1 η 2 e k 2 η 2 e t ˆ 2 t where 22 m 2 j 2 η 2 m 0 j 2 1 η 2 t ˆ 2 t t 2 t λ here k 1 d i a g k 11 k 12 k 13 k 2 d i a g k 21 k 22 k 23 m 0 d i a g m 04 m 05 m 06 η 2 e η 2 d η 2 η 2 d ϕ d θ d ψ d and λ is a very small time delay constant that is equal to a control cycle in the simulation combining eqs 3 and 21 we can write 23 τ 2 m 0 j 2 1 η 2 η 2 d k 1 η 2 e k 2 η 2 e j 2 η 2 t 2 t λ substituting eq 19 into eq 23 yields the final time delay control law as 24 τ 2 t m 0 j 2 1 t η 2 d t k 1 η 2 e t k 2 η 2 e t τ 2 t λ m 0 j 2 1 t λ η 2 t λ remark 10 the above time delay control law does not relay on accurate hydrodynamic coefficients and has fewer gains compared to some adaptive controllers in miao et al 2017 and chu and zhu 2015 which simplifies the implementation in the actual system remark 11 it is obvious that the stability of the above control law can be proved by showing that e m 2 η 2 1 t 2 t t 2 t λ is bounded then the attitude angle error will be bounded kumar et al 2007 gives that e will be convergent if choosing proper m 2 η 2 such that the eigenvalues of i m 2 η 2 1 m 2 η 2 lie in the unit disk recalling i m 2 η 2 1 m 2 η 2 we can obtain 25 i m 2 η 2 1 m 2 η 2 i j 2 η 2 m 2 1 m 0 j 2 1 η 2 the corresponding eigenvalues are 26 eig i j 2 η 2 m 2 1 m 0 j 2 1 η 2 1 eig m 2 1 m 0 under assumption 1 three eigenvalues of i j 2 η 2 m 2 1 m 0 j 2 1 η 2 are m 44 m 04 m 44 m 55 m 05 m 55 m 66 m 06 m 66 hence for the stability of the time delay based controller m 04 m 05 and m 06 should be properly chosen such that 0 m 04 2 m 44 0 m 05 2 m 55 and 0 m 06 2 m 66 remark 12 in this paper γ ϕ is chosen such that ϕ d t ϕ t 0 further according to eq 24 τ q t τ q t λ τ q 0 0 when λ 0 4 numerical simulation results to demonstrate the performance of the designed 3 d coupled path following controller numerical simulations are carried out using the underwater vehicle dynamics model in do and pan 2009 the environmental disturbances are chosen as follows τ e 0 2 m 11 d 0 2 m 22 d 0 2 m 33 d 0 02 m 44 d 0 02 m 55 d 0 02 m 66 d and d 1 0 1 sin 0 2 π t in the simulation straight line circular and helical path followings are considered respectively while the corresponding control parameters are the same as follows k x 0 5 k 11 k 12 k 13 3 and k 21 k 22 k 23 0 5 4 1 straight line path following in straight line path following three different nominal matrices are investigated as listed in table 1 the path following results are depicted in figs 2 4 figs 2 and 3 show that the underwater vehicle with the designed control scheme can converge to the desired straight line regardless of the nominal matrices as strong positive environmental disturbances act on the vehicle the following errors increase slightly in the initial stage the roll angle curve in the entire path following stage is shown in fig 4 where it changes in the range of 3 5 and the corresponding sinusoidal and cosinoidal values also change around 0 and 1 respectively even if in the stable stage the roll angle is nonzero due to the external disturbances and coupled effects 4 2 circular path following in circular path following four different initial states in table 2 are considered for the designed controller with the same gains the path following results are depicted in figs 5 8 figs 5 and 6 show that both the underwater vehicles inside and outside the circle can converge to the desired circular path fig 7 directly shows the pitch and yaw guidance errors tend to a small value using the designed time delay control law in the entire path following stage the attitude angles including roll pitch and yaw angles are described in fig 8 where the actual roll angle is always equal to the guidance value and the pitch angle tends to a positive constant in order to compensate for the environmental disturbances in pitch 4 3 helical path following finally helical path following is carried out by comparing the designed time delay controller case 31 with the classic pid controller case 32 the corresponding path following results are depicted in figs 9 11 although both the underwater vehicles converge to the desired helix the performance is different as shown in fig 10 the off track error under the classic pid controller is larger than that of the designed time delay controller in addition control inputs of the time delay controller is smoother hence the designed time delay controller shows better performance than the classic pid controller 5 conclusion in this paper an improved los guidance law followed by time delay control law is proposed for 3 d coupled path following control of a 6 dof underactuated underwater vehicle by assigning the actual roll angle as the desired roll guidance yaw control input in roll can be always zero the simulation results illustrate the roll angle in path following is nonzero due to the environmental disturbances and coupled effects from other dofs in addition the designed 3 d coupled path following controller shows adaptive and robust performance in the presence of different paths and initial states future work will include the solution to the presence of control input saturation and testing the corresponding algorithms using an underwater vehicle prototype that we are developing at the moment credit authorship contribution statement caoyang yu conceptualization methodology software writing original draft writing review editing chunhu liu software visualization supervision xianbo xiang funding acquisition resources supervision zheng zeng visualization supervision zhaoyu wei visualization supervision lian lian conceptualization funding acquisition resources supervision declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work is supported in part by the national natural science foundation of china under grant 51909161 and 51579111 in part by the shanghai sailing program china under grant 19yf1424100 in part by the startup fund for youngman research at shanghai jiao tong university china under grant 19x100040001 and in part by the open research fund of state key laboratory of ocean engineering shanghai jiao tong university under grant 1914 
21830,this paper addresses the problem of three dimensional 3 d coupled path following control for a six degrees of freedom underactuated underwater vehicle to compensate for nonzero roll angle an improved 3 d line of sight guidance law is proposed in the kinematic layer especially this novel guidance law can be applied to not only actuated but also underactuated underwater vehicles in roll by assigning the desired roll angle subsequently a simplified time delay controller is designed in the dynamics layer to robustly track the desired guidance angle such that the entire path following errors are bounded finally comparative numerical simulation results among different following paths initial states and controllers are provided to show the outperformance of the designed 3 d coupled path following controller keywords underwater vehicle path following guidance and control roll dynamics 1 introduction underwater vehicles including remotely operated vehicles autonomous underwater vehicles auvs and underwater gliders are powerful tools for ocean commercial scientific and military applications such as high resolution mapping of the deep sea floor measurements of ocean temperature and salinity and mine reconnaissance and neutralization bellingham and rajan 2007 bovio et al 2006 cohan 2008 in accomplishing underwater missions three dimensional 3 d motion control of the vehicle is usually required for instance a ribbon fins propelled underwater biomimetic vehicle manipulator system was introduced to surround and inspect a sunken ship along a spatial helical path wang et al 2018b compared with trajectory tracking path following is a more popular motion control scene in underwater vehicle field as its primary objective is to ensure that a vehicle converges to and follows a desired geometric path without any temporal requirements breivik and fossen 2005 liu et al 2019 wang et al 2019 compared with 3 d path following two dimensional 2 d path following with only three degrees of freedom dofs has been well studied xiang et al 2018 in lapierre and soetanto 2007 a nonlinear feedback control law composed of an approach angle and a model based backstepping dynamics control law was designed to yield the asymptotical convergence of the 2 d horizontal plane path following error to reject unknown model dynamics and disturbances a fuzzy uncertainty observer based path following controller was proposed for underactuated marine vehicles wang et al 2018a in peng and wang 2018 an output feedback path following controller of under actuated auvs moving in vertical plane was designed based on extended state observers and projection neural networks different from 2 d horizontal or vertical plane 3 d space has more coupled dofs which results in complicate kinematic design in path following control following the guidance idea in breivik and fossen 2005 an adaptive fuzzy proportional integral derivative pid controller with 3 d line of sight los was designed and validated xiang et al 2017 to simplify implementation of the above classic double input fuzzy controller a single input nonlinear fuzzy dynamics controller was proposed in yu et al 2017 to achieve robust 3 d path following of an underactuated auv in rout and subudhi 2017 an adaptive pid controller was developed using the derived parameters to accomplish the path following task of an auv in addition to pid control in miao et al 2017 a compound controller for the spatial path following of an underactuated auv based on backstepping and active disturbance rejection control methods was proposed to enhance the robustness against system uncertainties in chu and zhu 2015 an adaptive sliding mode controller was used to track the desired yaw angle pitch angle and surge velocity generated by the guidance system in the kinematic layer compared with pid control the number of control gains in the above adaptive controllers increases significantly which makes the adjustment of the controller difficult note that the roll dynamics in the above 3 d path following control is neglected which means that only five dofs in surge sway heave pitch and yaw directions are considered it is well known that the roll angle cannot always maintain at zero qiao and zhang 2019 in khodayari and balochian 2015 three decoupled pid controllers adjusted by mamdani fuzzy rules in speed heading and depth directions were designed for 3 d coupled motion control of an auv in kim and yuh 2002 a coupled neuro fuzzy controller was implemented on omni directional intelligent navigator odin dynamic model in qiao and zhang 2019 a double loop integral sliding mode control scheme was designed to ensure the finite time stability of tracking control for the 6 dof odin vehicle with system uncertainties yet the odin vehicle has the corresponding control input in each dof and hence it can be seen as a fully actuated vehicle in this paper we aim at addressing the problem of 3 d coupled path following control for a 6 dof underactuated underwater vehicle the main contributions are summarized as follows 1 an improved spatial los guidance law is proposed for 3 d coupled path following in the presence of nonzero roll angle especially this new guidance law can be applied to both fully actuated and under actuated underwater vehicles in addition its guidance steps are simpler compared to yu et al 2017 2 a simplified time delay controller is designed to robustly track the desired guidance angle generated in the guidance layer this dynamics controller has fewer control gains than adaptive fuzzy controllers and can be easily implemented in the actual control system 3 comparative numerical simulation results among different following paths initial states and controllers show the stronger adaptivity and robustness of the designed path following controller the rest of this paper is organized as follows the modeling of 6 dof underactuated underwater vehicles with roll dynamics is presented in section 2 the path following controller composed of the improved 3 d los and time delay control is derived in section 3 section 4 shows comparative numerical simulation results and concluding remarks and further research are contained in section 5 2 modeling of a 6 dof underwater vehicle according to fossen 1994 the motion equations of a 6 dof underwater vehicle in 3 d space can be written as 1 η j η v m v c v v d v v g η τ τ e substituting the first equation of eq 1 into the second one it has the following motion equation described in the inertial coordinate frame 2 m η η c v η η d v η η g η τ τ e with 3 m η j η m j 1 η c v η j η c v m j 1 η j η j 1 η d v η j η d v j 1 η g η j η g η τ j η τ τ e j η τ e here j η r 6 6 is the jacobian matrix with η x y x ϕ θ ψ being the position and attitude of this vehicle and v u v w p q r being the vector of linear and angular velocities m r 6 6 denotes the integrated mass matrix c v r 6 6 and d v r 6 6 are the coriolis and centripetal matrix and the damping matrix respectively g η r 6 1 is the vector of gravity and buoyancy τ r 6 1 and τ e r 6 1 are the resultant vectors of forces and moments generated by actuators and environmental disturbances respectively in addition and 1 denote the transpose and inverse of the matrix respectively therefore denotes the transpose of the inverse of the matrix assumption 1 zheng and feroskhan 2017 m is assumed to be a diagonal matrix namely m d i a g m 11 m 22 m 33 m 44 m 55 m 66 with m i i 0 assumption 2 shojaei 2015 throughout the paper the surge velocity u of the underwater vehicle is assumed to be constant in 3 d path following remark 1 m c d g and τ e are partially known or uncertain by considering the accuracy of experiment and simulation hydrodynamics tests and the effects of time varying and unknown environmental disturbances in this sense any accurate model based controller will be ineffective remark 2 for an underwater vehicle equipped with a stern thruster and two pair of control surfaces τ τ u 0 0 0 τ q τ r which means that this 6 dof vehicle has only three control inputs and is underactuated in sway heave and roll sankaranarayanan et al 2008 peng et al 2019 remark 3 due to the existence of an inherent metacentric height and the underactuated configuration in roll ϕ p 0 is usually assumed in 3 d path following in fact this assumption is not rigorous in practice because of coupling effects between different dofs in this paper the aim of 3 d path following is to let the underwater vehicle with the motion equations in eq 2 converge to and follow the desired spatial path without temporal constraints as shown in fig 1 since the desired path is time independent a virtual point q denoted by p q x q s y q s z q s is usually introduced on the path to guide the underwater vehicle here s is a scalar variable remark 4 if the distance between the vehicle center o and the virtual point q on the path is zero the control objective of 3 d path following has been achieved different from trajectory tracking the velocity of the point q on the path is associated with the motion of the vehicle rather than the accurate time stamp 3 design of a 3 d coupled path following controller a guidance and control architecture is resorted to achieve 3 d coupled path following where the guidance layer is to solve measurable attitude angles and the control layer is to track measurable attitude angles 3 1 attitude angle guidance in the guidance layer we first introduce the path frame f and define the corresponding transformation matrix r f between the path frame and the inertial frame as breivik and fossen 2005 4 r f r z χ q r y υ q where 5 r z χ q c χ q s χ q 0 s χ q c χ q 0 0 0 1 and 6 r y υ q c υ q 0 s υ q 0 1 0 s υ q 0 c υ q among eqs 5 and 6 1 1 throughout the paper the symbols s c and t denote the trigonometric functions sin cos and tan respectively twice rotation angles around the x and y axes are denoted by χ q s atan2 y q s x q s and υ q s arctan z q s x q s 2 y q s 2 respectively here the function atan2 is 2 argument arctangent and returns a single value ξ such that π ξ π and then the path following error vector p f e x e y e z e can be defined in the above path frame as 7 p f e r f p p q with p x y z being the position of the underwater vehicle define the resultant velocity vectors of the vehicle and the virtual point as u w u 0 0 with u u 2 v 2 w 2 and u q u q 0 0 with u q s x q s 2 y q s 2 z q s 2 respectively then the derivative of p f e is 8 p f e s f p f e r w f u w u q where s f is a skew symmetric matrix and is defined as 9 s f 0 χ q c υ q υ q χ q c υ q 0 χ q s υ q υ q χ q s υ q 0 and the rotation matrix r w f between the flow frame and the path frame is given by 10 r w f c χ e c υ e s χ e c χ e s υ e s χ e c υ e c χ e s χ e s υ e s υ e 0 c υ e note that χ e and υ e are two angles of rotation that align x axis of the flow frame with that of the path frame after obtaining the path following error and the error dynamics we can choose the positive lyapunov function candidate v e 0 5 p f e 2 and then calculate its derivative as 11 v e x e u c υ e c χ e u q z e u s υ e y e u s χ e c υ e to let the derivative v e be negative it is straight to assign 12 s u c χ e c υ e k x x e x q s 2 y q s 2 z q s 2 χ e arctan y e δ y υ e arctan z e δ z where k x δ y and δ z are positive constants which ensures that 13 v e k x x e 2 u c υ e y e 2 δ y 2 y e 2 u z e 2 δ z 2 z e 2 it can be concluded that the path following error is lyapunov stable using the guidance law in eq 12 remark 5 the guidance law in eq 12 links the flow frame of the vehicle to the path frame and needs to indirectly calculate the attitude angles defined in the flow frame yet the onboard navigation sensors only provide the information of attitude angles i e ϕ θ and ψ these measurable attitude angles are defined in the body frame rather than the flow frame for the sake of transforming unmeasurable attitude angles into measurable one the following equivalent transformation is introduced both the consecutive rotation of r f and r w f and the consecutive rotation of r b and r w b map the resultant velocity vector u w equivalently to the inertial frame this can be expressed as 14 r f r w f u w r b r w b u w where the transformation matrix from the body frame to the inertial frame r b r z ψ r y θ r x ϕ and its expansion is 15 r b c ψ c θ s ψ c ϕ s ϕ s θ c ψ s ψ s ϕ c ϕ s θ c ψ s ψ c θ c ψ c ϕ s ϕ s θ s ψ c ψ s ϕ c ϕ s θ s ψ s θ s ϕ c θ c ϕ c θ and the transformation matrix from the flow frame to the body frame r w b r y α r z β and its expansion is 16 r w b c α c β c α s β s α s β c β 0 s α c β s α s β c α with α arctan w u being the angle of attack of the underwater vehicle and β arcsin v u being its side slip angle substituting eqs 4 10 15 and 16 into eq 14 yields 17 a 1 c ψ b 1 s ψ c 1 a 2 s ψ b 2 c ψ c 2 a 3 s θ b 3 c θ c 3 with a 1 c α c β c θ s β s θ s ϕ s α c β s θ c ϕ b 1 s β c ϕ s α c β s ϕ d c 1 c χ q c χ e c υ q c υ e s χ q s χ e c υ e c υ q s χ q s υ e a 2 a 1 b 2 b 1 c 2 s χ q c υ q c χ e c υ e c χ q s χ e c υ e s χ q s υ q s υ e a 3 c β c α b 3 s β s ϕ s α c β c ϕ and c 3 s υ q c υ e c υ e c υ q s υ e the solution of eq 17 is 18 ϕ γ θ arcsin c 3 a 3 2 b 3 2 arctan b 3 a 3 ψ atan2 a 1 c 2 b 2 c 1 b 1 c 2 a 2 c 1 where γ is a design variable and will be assigned later remark 6 for a designated guidance law in eq 12 the corresponding eq 18 can be seen as the equivalent attitude guidance angles hence ϕ θ and ψ in eq 18 can be replaced with ϕ d θ d and ψ d remark 7 in the design of above attitude guidance angles the intermediate attitude angles of the underwater vehicle including the azimuth angle χ and the elevation angle υ are not required compared with breivik and fossen 2005 and yu et al 2017 this simplifies the guidance steps remark 8 compared with xiang et al 2017 yu et al 2017 rout and subudhi 2017 miao et al 2017 and chu and zhu 2015 the roll angle is not neglected in the above guidance law even if for an underwater vehicle that is underactuated in roll this consideration is more realistic 3 2 attitude angle tracking in the control layer we will design a time delay based controller to robustly track the above attitude guidance angles such that the control objective of the entire 3 d coupled path following can be achieved as the guidance layer has transformed the position following into the angle tracking only 3 dof motion equations are considered in the subsequent dynamics control layer hence according to eq 2 and assumption 1 we can rewrite the last three motion equations with respect to angle as 19 m 2 η 2 η 2 t 2 η η η τ 2 where η 2 ϕ θ ψ and t 2 η η η m 2 m 2 η 2 c 2 v η η d 2 v η η g 2 η τ e 2 here m 2 j 2 η 2 m 2 j 2 1 η 2 r 3 3 with m 2 d i a g m 44 m 55 m 66 and 20 j 2 η 2 1 s ϕ t θ c ϕ t θ 0 c ϕ s ϕ 0 s ϕ c θ c ϕ c θ and m 2 denotes an estimated value of m 2 c 2 r 3 6 d 2 r 3 6 g 2 r 3 1 τ 2 r 3 1 and τ e 2 are system matrices composed of the last three rows of c d g τ and τ e in eq 3 respectively remark 9 the mass matrix m is composed of the body mass matrix and the added mass matrix the former can be roughly calculated in the design of the underwater vehicle hence we can obtain a known matrix m 2 based on the theoretical calculation which is named as the nominal matrix while t 2 is usually unknown cui et al 2017 resorting to the feedback linearizing technique and time delay control the control laws in roll pitch and yaw are designed as 21 τ 2 m 2 η 2 η 2 d k 1 η 2 e k 2 η 2 e t ˆ 2 t where 22 m 2 j 2 η 2 m 0 j 2 1 η 2 t ˆ 2 t t 2 t λ here k 1 d i a g k 11 k 12 k 13 k 2 d i a g k 21 k 22 k 23 m 0 d i a g m 04 m 05 m 06 η 2 e η 2 d η 2 η 2 d ϕ d θ d ψ d and λ is a very small time delay constant that is equal to a control cycle in the simulation combining eqs 3 and 21 we can write 23 τ 2 m 0 j 2 1 η 2 η 2 d k 1 η 2 e k 2 η 2 e j 2 η 2 t 2 t λ substituting eq 19 into eq 23 yields the final time delay control law as 24 τ 2 t m 0 j 2 1 t η 2 d t k 1 η 2 e t k 2 η 2 e t τ 2 t λ m 0 j 2 1 t λ η 2 t λ remark 10 the above time delay control law does not relay on accurate hydrodynamic coefficients and has fewer gains compared to some adaptive controllers in miao et al 2017 and chu and zhu 2015 which simplifies the implementation in the actual system remark 11 it is obvious that the stability of the above control law can be proved by showing that e m 2 η 2 1 t 2 t t 2 t λ is bounded then the attitude angle error will be bounded kumar et al 2007 gives that e will be convergent if choosing proper m 2 η 2 such that the eigenvalues of i m 2 η 2 1 m 2 η 2 lie in the unit disk recalling i m 2 η 2 1 m 2 η 2 we can obtain 25 i m 2 η 2 1 m 2 η 2 i j 2 η 2 m 2 1 m 0 j 2 1 η 2 the corresponding eigenvalues are 26 eig i j 2 η 2 m 2 1 m 0 j 2 1 η 2 1 eig m 2 1 m 0 under assumption 1 three eigenvalues of i j 2 η 2 m 2 1 m 0 j 2 1 η 2 are m 44 m 04 m 44 m 55 m 05 m 55 m 66 m 06 m 66 hence for the stability of the time delay based controller m 04 m 05 and m 06 should be properly chosen such that 0 m 04 2 m 44 0 m 05 2 m 55 and 0 m 06 2 m 66 remark 12 in this paper γ ϕ is chosen such that ϕ d t ϕ t 0 further according to eq 24 τ q t τ q t λ τ q 0 0 when λ 0 4 numerical simulation results to demonstrate the performance of the designed 3 d coupled path following controller numerical simulations are carried out using the underwater vehicle dynamics model in do and pan 2009 the environmental disturbances are chosen as follows τ e 0 2 m 11 d 0 2 m 22 d 0 2 m 33 d 0 02 m 44 d 0 02 m 55 d 0 02 m 66 d and d 1 0 1 sin 0 2 π t in the simulation straight line circular and helical path followings are considered respectively while the corresponding control parameters are the same as follows k x 0 5 k 11 k 12 k 13 3 and k 21 k 22 k 23 0 5 4 1 straight line path following in straight line path following three different nominal matrices are investigated as listed in table 1 the path following results are depicted in figs 2 4 figs 2 and 3 show that the underwater vehicle with the designed control scheme can converge to the desired straight line regardless of the nominal matrices as strong positive environmental disturbances act on the vehicle the following errors increase slightly in the initial stage the roll angle curve in the entire path following stage is shown in fig 4 where it changes in the range of 3 5 and the corresponding sinusoidal and cosinoidal values also change around 0 and 1 respectively even if in the stable stage the roll angle is nonzero due to the external disturbances and coupled effects 4 2 circular path following in circular path following four different initial states in table 2 are considered for the designed controller with the same gains the path following results are depicted in figs 5 8 figs 5 and 6 show that both the underwater vehicles inside and outside the circle can converge to the desired circular path fig 7 directly shows the pitch and yaw guidance errors tend to a small value using the designed time delay control law in the entire path following stage the attitude angles including roll pitch and yaw angles are described in fig 8 where the actual roll angle is always equal to the guidance value and the pitch angle tends to a positive constant in order to compensate for the environmental disturbances in pitch 4 3 helical path following finally helical path following is carried out by comparing the designed time delay controller case 31 with the classic pid controller case 32 the corresponding path following results are depicted in figs 9 11 although both the underwater vehicles converge to the desired helix the performance is different as shown in fig 10 the off track error under the classic pid controller is larger than that of the designed time delay controller in addition control inputs of the time delay controller is smoother hence the designed time delay controller shows better performance than the classic pid controller 5 conclusion in this paper an improved los guidance law followed by time delay control law is proposed for 3 d coupled path following control of a 6 dof underactuated underwater vehicle by assigning the actual roll angle as the desired roll guidance yaw control input in roll can be always zero the simulation results illustrate the roll angle in path following is nonzero due to the environmental disturbances and coupled effects from other dofs in addition the designed 3 d coupled path following controller shows adaptive and robust performance in the presence of different paths and initial states future work will include the solution to the presence of control input saturation and testing the corresponding algorithms using an underwater vehicle prototype that we are developing at the moment credit authorship contribution statement caoyang yu conceptualization methodology software writing original draft writing review editing chunhu liu software visualization supervision xianbo xiang funding acquisition resources supervision zheng zeng visualization supervision zhaoyu wei visualization supervision lian lian conceptualization funding acquisition resources supervision declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work is supported in part by the national natural science foundation of china under grant 51909161 and 51579111 in part by the shanghai sailing program china under grant 19yf1424100 in part by the startup fund for youngman research at shanghai jiao tong university china under grant 19x100040001 and in part by the open research fund of state key laboratory of ocean engineering shanghai jiao tong university under grant 1914 
21831,the flexibility of marine riser systems in deepwater settings is studied using flexible multibody dynamics with geometrically exact beam for static eigen and recoil analyses the aim is to form an accurate and efficient modelling method and the corresponding solution strategies algorithms for considering the spatial flexibility of the marine riser and movable constraints simultaneously where the former is usually oversimplified in rigid multibody dynamics based works and the latter is inefficient in the finite element method based works therefore a geometrically exact beam with the euler s rotation vector is adopted for the marine riser and its uniform formulation with rigid bodies is presented together with the preconditioning and solving strategies in static eigen and recoil cases some other technical details for speeding up the algorithm are also described such as parallelisation matrix sparsity and variable step size control to verify the present algorithm a model with a depth of 1547 m based on a real platform is built here considering the hydrostatic and hydrodynamic loads on riser hydro pneumatic tensioner and platform motion the numerical results are compared with the commercial software deepriser and up to 93 coincidence is obtained in all cases with six times speedup is achieved in addition an extended model with more details of the real system obtained by considering independent modelling of auxiliary lines is simulated in the recoil analysis with the presented algorithm and the results show that ignoring the stiffness of the auxiliary lines in the traditional method and commercial software may lead to incorrect conclusions regarding the tension safety of the marine riser keywords marine riser system flexible multibody dynamics geometrically exact beam auxiliary lines stiffness static eigen recoil analyses 1 introduction as shown in fig 1 a a marine riser system in deepwater drilling settings usually consists of a dynamically positioned platform hydro pneumatic tensioner subsystem heave compensational telescopic joint tj flex joints lower marine riser package lmrp blowout preventer bop wellhead conductor in the seabed and the main riser section which contains hundreds or thousands of metres of riser joints meanwhile buoyancy current and wave force on the riser should be considered together with drilling mud pressure inside the riser and wave induced motion of the platform for simulating a marine riser system in a deepwater setting the flexibility of the riser is essential because it can be up to several kilometres long and needs to be included in the problems of emergency disconnection vibration control fatigue analysis and so forth focus has been put on riser flexibility from the very beginning early research started from the differential equation of beams in 2d cases which fischer 1966 used for the design of floating vessel drilling risers and burke 1974 used on the static and dynamic analyses of a riser in deepwater settings in addition it still provides rough estimations for engineering design sparks 2007 dareing 2012 nowadays however more accurate results require the modelling of riser flexibility in 3d cases mainly because of 3d environmental conditions such as waves currents and wind all of which can come from different directions and the resulting spatial motion of vessel riser and conductor the finite element method fem was introduced into riser flexibility analyses for 3d cases for example gardner and kotch 1976 applied it with a bernoulli euler beam in the small angle theory and bernitsas et al 1985 applied it with coupling torsion and bending of thin walled circular beam for risers then large displacements are also taken into account with different theories including the geometrically exact beam theory gebt in fem neto et al 2014 absolute nodal coordinate formulation ancf chai and varyani 2006 rigid finite element method rfem adamiec wójcik et al 2015 and rigid finite segment method rfsm in multibody dynamics mbd adamiec wójcik and wojciech 2018 the applications of gebt in a marine riser neto et al 2014 uses accuracy and efficiency in flexible riser because it uses the accurate geometric nonlinearity to describe displacements in space and consumes only 6 degrees of freedom for a beam node whereas an ancf beam node consumes 12 degrees of freedom romero 2008 and rfem and rfsm adamiec wójcik et al 2015 adamiec wójcik and wojciech 2018 only provide simplified versions of 3d large displacements however applications based on mbd raman nair and baddour 2003 lee et al 2015 can take advantage of the natural definition and calculation efficiency of movable constraints i e the tj and flex joints whereas fem usually approximates them with contacts leading to much smaller integration step sizes for convergence however a deepwater drilling riser system usually contains both a long flexible riser that can move in space and that has movable constraints similar systems that require simultaneous consideration of the 3d large displacement of slender parts and movable constraints also widely exist in the industry flexible multibody dynamics fmbd with gebt is rather attractive and effort has been made to use this method by some authors including bauchau and bottasso 1999 ibrahimbegovic and mamouri 2000 romero 2008 liu et al 2018a 2018b 2018c and chen et al 2019 but corresponding applications for marine riser systems for drilling has been still sparse until now therefore the present paper applies a combination of gebt and mbd in the modelling of marine riser systems to investigate the flexibility of the riser and the influence of stiffness of auxiliary lines which is usually ignored in an engineering recoil analysis before the basic fmbd model of a marine riser system in the present paper is built as shown in fig 1 b where gebt is used on the modelling of the slender riser tj and conductor the tj is represented by two parallel beams and two cylindrical pairs on its ends are used regarding the boundary conditions the bottom of the conductor is linked to the seabed and the top of the riser is tensioned at the outer barrel of the tj by the tensioner the remainder of the current paper is organised as follows in section 2 a formulation and solving of the fmbd model of a marine riser system is presented including a uniform formulation of mbd and gebt formulation of loads on a riser preconditioning and solving of the system and the technical details needed for efficient algorithm then in section 3 a real marine riser system with a depth of 1547 m is calculated with the basic fmbd model and extended fmbd model where the former is compared with deepriser for verification and the latter analyses the influence of the independent modelling of auxiliary lines in a recoil analysis the conclusion is given in section 4 2 fmbd model introducing the gebt into mbd allows researchers to benefit from both the high accuracy for the flexible slender parts and modelling convenience of the movable constraints while keeping the solving ability for problems of large displacement large rotation and large deformation however the combination usually produces a system with more degrees of freedom than that with rigid multibody dynamics rmbd and one that is more complicated than that of fem because of the equations of constraints and the nonpositive character of the system matrix therefore correctly choosing the solving details in mbd and the corresponding adjustments for large scale problems are necessary for an efficient fmbd algorithm in the present paper lagrange equations of the first kind are adopted as the governing equations of the system compared with lagrange equations of the second kind although additional degrees of freedom dofs for constraint equations are necessary the cost is much smaller than the dofs of the flexible parts but a more uniform and simper presentation of the constraints for both the rigid and flexible parts can be gained attention should be paid to the preconditioning of the linearised system matrix with constraint equations which is essential when solving static eigen and dynamic problems this is detailed in section 2 3 for the formulation of rigid bodies and nodes of beam elements euler s rotation vector is chosen both to form a uniform description of the kinetic energy and constraint equations in addition euler s rotation vector has the advantage of nonsingularity when compared with the euler angle and has only three parameters for 3d rotation compared with the quaternions that have four parameters and one additional lagrange multiplier for its normalization constraint here the second characteristic is very important for large scale problems in addition the parallelisation strategy utilisation of matrix sparsity and specific solving strategies for a marine riser analysis are also discussed to form an efficient algorithm 2 1 uniform formulation of mbd and gebt the governing equations of an mbd system with lagrange equations of the first kind shabana 2013 are a set of differential algebraic equations daes 1 d d t l q q q l q q q φ q t λ f e φ q 0 wherein q is the dofs vector of rigid and flexible bodies with length n q λ is the dofs vector of constraint equations with length n λ and they are the variables that must be solved the lagrange function l q q is the difference between kinematic energy t and potential energy v φ q is the complete constraint equations and f e is the general force vector of the external loads we note the general forces of a rigid body and flexible body with f b which is produced by a differentiation of the lagrange function hence eq 1 can be rewritten as 2 f b f e φ q t λ 0 φ 0 to form the system governing equations f b f e and φ should be calculated for all the bodies loads and constraints in the system respectively and be assembled into eq 2 per the general coordinates where the loads or constraints are applied to in the following part we give a uniform formulation of f b and φ for the rigid bodies beam elements and their constraints used in the modelling of the marine riser system the spatial rotation of a rigid body or a beam node can be described by the euler s rotation vector φ φ e where φ is the rotation angle and e is the unit rotation axis rodrigues rotation formula shabana 2013 gives the 3 3 transformation matrix of attitude 3 g cos φ i 3 3 sin φ φ skew φ 1 cos φ φ 2 φφ t wherein i 3 3 is a 3 3 identity matrix and where the skew φ is the skew symmetric matrix of vector φ with the relationship skew ω g t g or ω axial g t g the angular velocity in the local coordinate system can be obtained and written compactly as 4 ω h t φ wherein 5 h sin φ φ i 3 3 1 cos φ φ 2 skew φ ϕ sin φ φ 3 φφ t the angular acceleration in local coordinate system is 6 ε ω h t φ h t φ the rotation general force of a rigid body is obtained by substituting eq 4 and eq 6 into eq 1 with the rotation kinematic energy t ω t j ω 2 wherein j diag j 1 j 2 j 3 is the rotation inertia matrix of a rigid body in the local principal axis coordinate system or we can simply calculate this by multiplying ω to the left of euler rotation equation saletan and cromer 1971 j ω ω j ω p to obtain the virtual work of moment p which balances the inertia moment of rotation and the result ϕ t h j ω ω j ω ϕ t h p indicates the rotation general force of a rigid body will be 7 f b r o t h j ε ω j ω the inertia force also includes the translation of inertia force with a mass m and displacement of origin of local coordinate system also inertia centre for rigid body in the frame of reference r r 1 r 2 r 3 t the total general inertia force of a rigid body is 8 f b i n e r m r h j ε ω j ω for simplification the inertia force for a beam element is calculated at its nodes by evenly dividing the inertia moment onto the nodes this means each node with its neighboring left half element if available and neighboring right half element if available is considered a rigid body in the calculation of the inertia force then the rotation inertia matrix j of a beam node is calculated with this rigid body and the origin of the local coordinate system of this rigid body is directly chosen at the node therefore eq 8 is also employed to calculate the general inertia force of the beam elements the general elastic force of a geometrically exact beam simo and vu quoc 1986 1988 is integrated at numerical integration points as shown in fig 2 by the total lagrangian framework the beam strain includes the normal strain ε 11 d r d s 2 1 2 in the axis direction engineering shear strains γ 12 d r d s y s γ 13 d r d s z s in the cross section and rotation strains κ κ 1 κ 2 κ 3 t h s t d φ d s along the three main axes the strain vector is 9 ε ε 11 γ 12 γ 13 κ 1 κ 2 κ 3 t with the same linear constitutive relation as used by neto et al 2014 with section area a elastic modulus e shear modulus g and section inertia moment in main direction j k k the beam stress is 10 σ d ε dia g e a g a g a e j 22 e j 11 e j 33 ε then the general elastic force of a beam element can be obtained by substituting 9 and 10 into eq 1 because the elastic energy v 0 l ε t σ 2 d s is only a function of displacement and attitude and we get 11 f b e l s t v q 0 l e a ε 11 ε 11 q g a γ 12 γ 12 q γ 13 γ 13 q j 1 3 e j j j κ j κ j q d s wherein the partial differential q is in fact q e with q e r 1 t φ 1 t r k t φ k t t as the general coordinates of the beam element when calculating eq 11 there is no explicit expression with the coordinates r i ϕ i put outside of the integration sign because of nonlinear expression of ε and numerical integration is usually applied for example the gauss integration other details such as the calculation of matrix g and h when φ approaches zero lagrange interpolation of displacement and attitude in beam element and calculation of mass matrix and stiffness matrix of a beam element are detailed in appendix a there is no difference between the definitions of constraints on a rigid body and a beam node because they use the same parameters usually in engineering a constraint is defined with the help of a so called maker which is a fixed coordinate frame at given a position and attitude on the body if r m i φ m i is the relative parameter of maker m i on body i r i φ i and r m j φ m j is the relative parameter of maker m j on body j r j φ j then the global position of maker m i is r i g φ i r m i and the global attitude matrix is g m i x m i y m i z m i g φ i g φ m i and it is similar to marker m j then the three kinds of constraints used in the riser system are obtained as table 1 is the inner product of two vectors 2 2 loads on the marine riser the loads on a marine riser mainly come from gravity buoyancy current wave and tensioner at the top and the friction force of the mud should be included in the recoil analysis in marine science gravity and buoyancy are usually considered together in marine science with the concept of effective gravity dareing 2012 which is the gravity of the structure minus that of displaced seawater and plus that of the fluid inside the structure if applicable because buoyancy is only valid for a subject with a closed surface in liquid when used for a slender structure in the sea a short section of the structure is surrounded with seawater and compensating pressure forces are added perpendicularly to the two cross sections outwards for the seawater outside inwards for the fluid inside it is worth mentioning that the effective gravity of a beam element is uniformly distributed along the length therefore the compensating pressure forces should also be uniformly distributed along the length rather than applied to the nodes at the ends directly then the general force of gravity and buoyancy is 12 f e g r a v b u o y 0 l m m d i s m i n l g r s q ρ s w a o u t ρ i n a i n l g x l g x 0 q d s wherein g 9 8 0 0 is the gravity acceleration vector x 0 and x l are the external normal vectors of the cross section at the ends m is the mass of beam element m d i s is the mass of displaced seawater m i n is the mass of fluid inside ρ s w is the density of seawater a o u t is the outer section area of the tubular beam ρ i n and a i n are the density and section area of liquid inside respectively the formula for a rigid body could be obtained from eq 12 by removing both the integration symbol and dividing it by l in engineering applications the inner surface of the riser is usually smooth and a i n and m i n could be obtained directly but m d i s should be calculated with the help of weight in seawater w s e a and weight in air w a i r where m d i s w s e a w a i r l a i n ρ s w in addition if the riser has additional buoyant foam attached to it m should be replaced with m m f o a m m d i s f o a m where m f o a m is the mass of the foam in the element length and m d i s f o a m is the mass of seawater displaced by the foam in the present paper the hydrodynamic forces of currents and waves are calculated using a morison empirical formula borgman 1958 where the perpendicular hydrodynamic force is a function of a transverse relative velocity and where the inertia force is a function of transverse relative acceleration let a s w and v s w be the acceleration and velocity vectors of seawater with the given current and wave a t r r s a s w r s a s w x s and v t r r s v s w r s v s w x s give the transverse relative acceleration and velocity then with the outer diameter d a coefficient of added mass c a m and coefficient of drag c d the morison force is 13 f m o r ρ s w c a m π d 2 4 a t r ρ s w c d d 2 v t r v t r and corresponding general force of current and wave is 14 f e c u r w a v e 0 l f m o r r q d s the tensioning force at the top of the riser is a group of nonlinear concentrated forces that act on the tensioner ring purple ring of the tensioner part in fig 1 in different directions in a static analysis it is usually simplified as a total upward constant force and an additional total stiffness is also considered for eigen analysis for the recoil analysis the detailed tensioner model presented by pestana et al 2016 for the direct acting tensioner is adopted in the present paper where the pressure loss of the gas flowing through pipelines is ignored the pressure loss of the oil flowing through pipelines is considered using the darcy weisbach formula the pressure loss of oil flowing through antirecoil valve is calculated with a definition formula of the flow coefficient cv and a controlled closure curve in recoil and the pressure change of the gas in the accumulators is considered an adiabatic process usually before the start of a recoil mode the antirecoil valve is fully opened and does not generate any pressure loss to the tensioner the parameters of the detailed tensioner used in the present paper are shown in appendix c table 7 here the pressure of the hp side is not given in the table which is in fact determined by the given total top tensioning force pressure of the lp side and the sizes of the tensioner therefore a prior static analysis is always executed before the recoil analysis to obtain the pressures of the hp side at the equilibrium status under the given top tensioning force when a recoil case is considered the lmrp is separated from the bop and the mud in the riser flows out from the bottom of the riser therefore the additional hydrostatic pressure of seawater over the newly cut truncation should be applied at the lower end of the lmrp after separation and the time varying friction forces of mud acting along the riser should be applied during recoil it should be noted that the area of a newly cut truncation is the total section area of the riser and the auxiliary lines not the section area of the lmrp the friction forces of mud acting on the riser are precalculated with the slug model presented by grytoyr et al 2011 and then applied onto the nodes of riser during recoil in which the mud column is considered a rigid body for velocity calculation and where the friction forces are proportional to the velocity squared the matlab code for calculating the mud friction forces by grønevik 2013 is directly adopted here 2 3 preconditioning and solving of the system equations the uniform formulations of the general force of bodies constraint equations and general force of external loads that is f b φ and f e in eq 2 were presented in the last section however the general force vector f f b f e and φ have distinct orders of magnitude and their jacobians stand in different rows and columns of the system matrix which finally lead to a condition number problem in the fmbd it is straightforward to precondition eq 2 by multiplying a coefficient η onto φ and its partial derivatives φ q t to balance the magnitude and the following equations are solved instead of eq 2 with λ λ η 15 f η φ q t λ 0 η φ 0 wherein f f q q q t φ φ q φ q t φ q t q however the value of η should be different for the cases of a dynamic solution and eigen solution in the fmbd because η is used to balance the jacobian matrix coefficient matrix of the linearised problem but the algebrisation and linearisation processes of eq 15 for the dynamic and eigen solutions are different for the former if q is chosen as the solving variable time discretisation is applied to q and q to get the nonlinear algebraic equations and then eq 15 is expanded at the pre estimated value of q for the latter letting q q 0 gives out the nonlinear equations and then eq 15 is expanded at the balance state the details of preconditioning the eigen problem has been presented in yang et al 2012 where the preconditioning coefficient is chosen as the maximum absolute value of the jacobian matrix of general forces that is 16 η f q for eigen problem for a dynamic problem it is related to the time discretisation and pre estimation formats for example if the n t h order backward differential formulas bdfs for stiff problems wanner and hairer 1996 are used for time discretisation and the same order polynomial extrapolations are used for pre estimation that is 17 q m j 1 n 1 α j q m j q m β 0 q m j 1 n β j q m j q m γ 0 q m j 1 n γ j q m j wherein q m is the pre estimated general coordinate at m t h time step t m q m and q m are the discretized velocity and acceleration with q m at t m and α j β 0 β j γ 0 and γ j are the coefficients only related to the time step size history see appendix b for deduction details in time integration of dynamic problem with bdfs method the inverse square of time step size multiplies the mass matrix as shown in appendix b and the order gap between jacobian of constraints and jacobian of general forces of bodies is largely amplified to prevent it γ 0 is chosen as the precondition coefficient of the dynamic problem that is 18 η γ 0 o 1 h m 2 for dynamic problem wherein h m is the current step size then for a dynamic problem the nonlinear equations 15 can be solved with a newton iteration by 19 f q γ 0 f q β 0 f q γ 0 φ q t γ 0 φ q 0 δ q m δ λ m f γ 0 φ q t λ m γ 0 φ at q m q m q m t m and q m δ q m t λ m δ λ m t t gives a better estimation of the solution at t m and the iteration stops until the error is satisfied the item f q is the mass matrix f q is the damping and gyroscopic matrix and f q is the stiffness matrix these matrixes are calculated at each rigid body and beam element and then assembled into the system matrix per their corresponding general coordinates the jacobian matrixes for the beam element are deduced in appendix a because the beam node and rigid body use the same inertia formula eq 3 8 these inertia matrixes are also applicable for a rigid body the jacobian matrixes of the external forces acting on the bodies are more suitable to be numerically calculated with the finite difference method the jacobian matrixes of constraints are trivial and their deductions are omitted here the jacobian matrix of the constraint force η φ q t λ q from eq 15 is ignored in 19 to obtain a stable algorithm because of an abrupt value change of λ in the iteration however it must be kept in the eigen problem to form a complete jacobian matrix for the correct calculation of eigen values and eigen modes the static problem of mbd is usually solved by the same process of a dynamic problem by adding artificial damping into the system that is using f q a f q b f q instead of f q in eq 19 wherein a and b are the rayleigh coefficients and therefore use the same preconditioning coefficient as shown in eq 18 although the direct iteration method is widely used in static fem problems and is very efficient it is not suitable for static mbd problems because of the nonlinearity of large rotations 2 4 technical details for an efficient algorithm the models for mbd with getb usually have much larger scales of calculation than those for rmbd when it comes to both modelling accuracy and object size therefore the technical details for medium and large scale problems should be implemented to form an efficient algorithm that has a high accuracy these details include the variable step size control parallelisation strategy utilisation of matrix sparsity fast convergence of static problem and integration adjustment for the recoil problem variable step size control is very important for a fast time integration using an implicit method the step size varies with the change of smoothness of the problem in the process where a larger step size is chosen for gentle response variations to speed up integration and a smaller step size for steep response variations to satisfy the integration error furthermore the implicit method itself has a much larger acceptable step size than explicit method because of the demand of stability and is widely used in problems of vibration and control the bdfs method presented with eq 17 is an implicit method and the intelligent control strategy of the step size adopted in the present paper is the same with that of the ode15i function in matlab https www mathworks com help matlab ref ode15i html which solves the ordinary differential equations with a variable step size by 1 5 order of the bdfs method the parallelisation and utilisation of matrix sparsity are indispensable for the speedup of medium and large scale problems parallelisation reduces the time consumption of the force vector and jacobian matrix in eq 19 by calculating f and f for different beam elements rigid bodies parallelly and the same is true for φ and φ q for different constraints parallelisation is also used on the matrix product of preconditioning in eq 19 however the parallelisable iteration method for solving linear equations is usually unsuitable for mbd because of the nonpositive character of system matrix in eq 19 therefore a direct method is adopted and the utilisation of sparsity could reduce the bandwidth of the jacobian matrix and save both the memory space and calculation time because the flexible deepwater marine riser system presented in the current paper is a medium scale problem with a number of dofs less than one hundred thousand the intel mkl pardiso package with shared memory parallelisation is used to reorder and solve the linear equations besides the general measures mentioned above specific strategies could be applied to speed up the time integration of the static and dynamic cases based on the characteristics of the problems for example the calculation of an operational window of a marine riser system needs thousands of calculations of a static equilibrium under the different speeds of the current and translations of the platform as presented in section 2 2 the static equilibrium is calculated using a dynamic integration process with artificial damping to reach the equilibrium quickly in a single calculation we can relax the limit of the integration error to obtain a bigger step size and stops until max δ q m 2 q m 2 q m 2 will satisfy the accuracy after a long interval of simulation time in fact only a few steps in addition we can get a fast convergence for the static calculation by starting from the equilibrium of its neighbour calculation therefore the sketch in fig 3 a gives a good choice for sequence of static calculations and for a recoil analysis with the dynamic calculation the lmrp is cut off from the bop at time t c u t for an emergency disconnection and the tensioned riser completes the recoil process in several seconds the integration before recoil and after recoil are fixed at the second order to obtain a larger step size by eliminating high order system noise with the numerical damping but it is fixed to the fourth order during the several seconds of recoil to keep the high frequency response of the riser which is important for a safety analysis of the riser as shown in fig 3 b an additional time interval t p r e t c u t is also included in the fourth order stage to ensure the fourth order at the very beginning of recoil 3 numerical results here we consider a real marine riser system of the x oil field in the south china sea with a water depth of 1547 m that is built with the composition shown in fig 4 and calculated with our homemade codes based on the method presented in section 2 other parameters such as the parameters of the platform tensioner auxiliary lines soil currents and waves are shown in appendix c 3 1 basic fmbd model in the basic fmbd model of the marine riser shown in fig 1 b the same as the one done in fem the auxiliary lines are considered as an added mass on the riser with their inner areas added to that of the riser to count up the total inner pressure effect and the distribution of the material on a riser joint is also considered as uniform a static analysis for operational window eigen analysis and recoil analysis are performed in the homemade codes and compared with the commercial fem software deepriser related research of a riser recoil analysis with deepriser can be found in lang et al 2009 calculation parameters for both the homemade codes and deepriser are set as follows the marine riser system is calculated with a top tension of 5800 kn the static analysis of the operational window uses the current data and limitations of the component shown in appendix c but waves are not considered according to standard engineering conventions the eigen frequencies and modes are calculated at the static equilibrium under the current of 0 05 probability with no offset the recoil analysis separates the riser from the well at the 46th second and the setting of antirecoil valve can be found in appendix c the effects of currents wave and platform offset are not considered in the recoil analysis of the example the length of the beam element for the marine riser is about 3 m for example a riser joint with a length of 22 86 m is evenly divided into eight beam elements the length of a beam element for a conductor is 1 m for the basic fmbd model the number of dofs of the system is 3470 and there are 17 constraints all the results are calculated on the same desktop computer intel core i7 7700k 4 2 ghz 4 cores as shown in fig 5 a b and c the results of the homemade codes agree well with deepriser with up to 93 coincidence covering the static eigen and recoil analyses and in fig 5 d the recoil analysis of deepriser takes a fixed step size of 0 05 before separation and 0 01 after separation for a stable calculation here a larger step size setting will cause a failure mainly because of its definition of a tj with contacts pairs however the homemade codes take a much larger step size with the definition of constraints with algebraic equations and specific strategy for the recoil analysis in fig 3 b here a 6 3 times speedup is gained in the homemade codes with a single core and another 2 1 times speedup is gained with the parallelisation of four cores resulting in 13 3 times speedup because parallelisation is not available for a recoil analysis in deepriser yet the lmrp displacement riser effective tension envelope top tension and tj stroke of the recoil analysis are compared in fig 5 c interestingly the lmrp displacement shows an acceptable elevation of the lmrp from the wellhead while the riser s effective tension envelope shows there is a dangerous effective compression of the marine riser during the recoil process negative value of effective tension represent compression in the riser both software give the same results indicating that a high tension may maintain the safety of the wellhead but may cause the collapse of the riser in an emergency disconnection a lower value of the top tension could avoid effective compression in the marine riser and it is calculated and compared with the extended fmbd model in the following subsection 3 2 extended fmbd model the basic fmbd model of a marine riser system could be improved in its accuracy by an extended modelling of the auxiliary lines and joint interface however the present paper will only focus on the auxiliary lines because the consideration of the differences of stiffness and mass at the joint interface is neither difficult for fem nor has an evident effect on the results more specifically only a recoil analysis is carefully studied and discussed with the extended modelling of the auxiliary lines as the calculations with homemade codes show that the basic model is good enough in accuracy for static and eigen analysis as shown in fig 6 a in the extended fmbd model each auxiliary line around the riser is modelled with a beam and is connected to the riser at each connection location six times the number of beam elements and more than four hundred fixed hinges are required in addition to what is found in the model in section 3 1 with its six auxiliary lines especially the fixed hinges between the auxiliary line nodes and riser nodes are located at the centres of the auxiliary line nodes and are eccentric to the riser nodes and this ensures the rotation of the riser nodes can influence the configuration of the auxiliary line nodes the calculated marine riser system is the same as that in section 3 1 except for the tension reduction from 5800 kn to 3850 kn and an additional consideration of the wave induced heave motion of the platform in the recoil analysis as described in section 3 1 the top tension is reduced to try to keep the lower bound values of the effective tension positive moreover ignoring the effects of waves in other directions and attitudes except for the heave direction keeps the riser recoil in a simple vertical line which helps us explain the interesting phenomenon found in the results of the extended model later for comparison the extended model uses the same initial data of the tensioning system and riser setup in the recoil analysis as the basic model the results will show us what will happen in the recoil analysis when these initial data are used for the real riser with stiffness of the auxiliary lines the results of the recoil analysis for the extended model are compared with those of the basic model and deepriser in fig 6 b where the basic model and deepriser agree with each other once again while the extended model gives different results for the results of extended model the lmrp elevation is larger but the lower bound of effective tension steps into the compressed state region again this phenomenon can be explained by a higher axial stiffness of the combination of the riser and auxiliary lines in the extended model which reduces the elongation and produces more vibration in the example as shown in table 3 appendix c and fig 4 the cross section areas of the auxiliary lines sum to 0 0286 m2 which is 1 38 times of riser 1 0 0208 m2 1 57 times of riser 2 0 0182 m2 and 1 83 times of riser 3 0 0156 m2 to illustrate this we calculate an artificially modified version of the basic model the welded basic model or welded model for short where the auxiliary lines are supposed to be welded to the main riser everywhere and where the associated stiffness is taken into account here the results are compared showing that the black dashed lines of the welded model agree well with the dark green dashed lines of the extended model meaning that the conclusion is confirmed we also calculated a deepriser model considering load sharing of the auxiliary lines namely the deepriser model with the deepriser option load sharing with choke kill line on and with the deepriser parameter load sharing gap set to zero this model is similar to the welded model in riser stiffness and the results are shown in fig 6 with light green dash lines once again the effective tension steps into the compressed state region when considering the stiffness of auxiliary lines in the recoil analysis however the results of the lmrp elevation and tj stroke of this model are different from those of the welded model it seems the initial value of the tj stroke in deepriser will not vary with considering the stiffness of the auxiliary lines by load sharing therefore there must be a default riser length increase in deepriser otherwise if the riser length and tensioner settings are unchanged a higher riser stiffness may lead to less riser elongation in static conditions which would lead to a higher initial tj stroke as shown in the results of the extended model and welded model to confirm the surmise a riser length increase is considered in the extended and welded models to compare with the deepriser model here 0 5 m of the riser length increase is employed as a proper value according to the difference of the lmrp elevations however our primary numerical tests show that the results do not agree well with those of the deepriser model after carefully checking on the setting of deepriser we find that in deepriser the prior static analysis for preparing a recoil analysis also considers the load sharing of auxiliary line and it leads to change of tensioner setting in the recoil analysis this is because the initial hp side pressure of the detailed hydro pneumatic tensioner in a recoil analysis is set by the balanced riser configuration of a prior static analysis therefore to form the welded and extended models to compared with the deepriser model the auxiliary line stiffness is also considered in the prior static analysis besides a riser increase of 0 5 m in the welded and extended models as shown in fig 6 the results of the extended model and welded model agree well with those of deepriser model in the top tension lmrp elevation and tj stroke time traces although some difference is found in the upper bound of the effective tension envelope the lower bound is actually concerned and is acceptable for a small difference in addition both the welded extended deepriser welded and extended models lead to a compressed state of riser in the recoil analysis with considering auxiliary line stiffness no matter the auxiliary line stiffness is considered or not in a prior static analysis for the recoil tensioner setting in general the results comparison in fig 6 can be divided into three groups ① models without considering auxiliary line stiffness ② models with considering auxiliary line stiffness in only the recoil process ③ models with a riser length increase and considering auxiliary line stiffness in both the prior static analysis and the recoil process and the situations in group ② is most likely to happen when an engineer uses the data calculated without considering auxiliary line stiffness in a riser recoil operation however the welded model only accounts for the vertical motion the motion of the marine riser is completely spatial when the other environmental impact factors are considered such as the platform offset all wave effects and the current and wave forces on the riser it is apparent that the welded model has a much larger bending stiffness for the riser than reality so a meaningful result is not expected therefore the extended fmbd model is required for a recoil analysis in 3d space especially where a buckling risk for the riser is concerned and where the traditional methods may produce an incorrect evaluation of whether there is bucking of the riser in the recoil process the results of lmrp elevation in fig 6 have lower values than the initial value indicating that there is contact danger between the lmrp and wellhead although this situation could be improved by setting the separation time at the lowest point of the heave motion of the platform we still set the separation time at the 46th second to keep it the same as in section 3 1 moreover it does not affect our conclusion regarding the notable influence of the stiffness of the auxiliary lines in the recoil analysis 4 conclusions in the present paper the flexibility of a marine riser system is investigated in fmbd with gebt which is important for a riser safety analysis because of its thousands of metres of length in deepwater settings however traditional methods with the rmbd use a simplified riser because of computation limits while methods with fem have difficulties modelling the movable constraints and detailing the marine riser therefore the combination of gebt and mbd in the modelling of a marine riser system in the current paper aims at an accurate and efficient algorithm for simultaneous consideration of a detailed riser flexibility and movable constraints to achieve this aim the parameterization of euler s rotation vector is chosen for both the beam node and rigid body to describe the 3d rotation with three parameters and a uniform formulation of beam element and rigid body is presented together with a uniform definition of the constraints and net loads to achieve a stable algorithm preconditioning of the static eigen and dynamic analyses are described with the same parameter η because of the different scales between the constraints and forces while different values of η are chosen for the eigen and dynamic cases then the numerical time integration with a variable step size is detailed because the static analysis shares the same algorithm in mbd with artificial damping to obtain an efficient algorithm more technical details for a larger scale analysis are applied including the parallelisation strategy utilisation of matrix sparsity and special strategies for the static analysis of operational windows and recoil analysis for verification of the present algorithm the basic model with a depth of 1547 m based on a real platform is built in homemade codes and the commercial software deepriser this is done by considering liquids loads hydro pneumatic tensioner and platform motion the numerical results show a coincidence up to 93 in all cases and a six times speedup is gained when using the homemade codes then the influence of the auxiliary lines in the recoil analysis based on an extended model is studied in the homemade codes where the pipelines are also modelled with beam elements and are constrained to the main riser with hundreds of joints the results show that ignoring the stiffness of the auxiliary lines in the basic model and deepriser leads to incorrect conclusions about whether there is compression of the marine riser in the recoil analysis the extended model is built to only consider vertical forces and motions to keep the riser recoil in the vertical direction and a welded model mimics the extended model yielding the same results however the extended model is recommended for the 3d simulation of a marine riser for safety analysis during recoil furthermore the presented fmbd method could be applied for a deeper detailing of the marine riser system such as the influence of drillstring contact on the riser which can be studied in future research credit authorship contribution statement cheng yang conceptualization methodology resources visualization writing original draft jianbin du conceptualization funding acquisition methodology project administration writing review editing zaibin cheng conceptualization software validation yi wu software validation chaowei li methodology declaration of competing interest the authors declared that they have no conflicts of interest acknowledgments the research is supported by nsfc 11772170 the key laboratory of spacecraft design optimization and dynamic simulation technologies beihang university ministry of education china under grant no 2019kf001 national basic research program of china 2015cb251203 and the project on electric driver seat technology for large passenger aircraft mj 2018 s 44 which are gratefully acknowledged by the authors appendix a some details in calculating the general force of bodies calculation of matrix g and h when φ approaches zero in rodrigues rotation formula the matrix g gives the transformation matrix of the attitude of the body with euler s rotation vector φ by 20 g cos φ i 3 3 sin φ φ screw φ 1 cos φ φ 2 φφ t wherein φ φ e φ is the rotation angle and e is the unit rotation axis and with the matrix h the rotation velocity and acceleration of body in local coordinate is calculated as 21 ω h t φ ε h t φ h t φ wherein 22 h sin φ φ i 3 3 1 cos φ φ 2 screw φ φ sin φ φ 3 φφ t however the coefficients sin φ φ 1 cos φ φ 2 and φ sin φ φ 3 in eq 20 and eq 22 cannot be calculated directly when φ approaches zero and the taylor expansion should be used instead 23 sin φ φ 1 1 6 φ 2 1 120 φ 4 1 5040 φ 6 o φ 8 1 cos φ φ 2 1 2 1 24 φ 2 1 720 φ 4 1 40320 φ 6 o φ 8 φ sin φ φ 3 1 6 1 120 φ 2 1 5040 φ 4 1 362880 φ 6 o φ 8 lagrange interpolation of displacement and attitude in the beam element the displacement r and attitude φ at length s of an element in the gebt are calculated with a lagrange interpolation 24 r s i 1 k n i s r i φ s i 1 k n i s φ i wherein k is the number of nodes in an element n i is the shape function at i t h node as shown in table 2 r i is the displacement at the i t h node and φ i is the attitude at the i t h node the interpolation 24 can be rewritten in matrix form as 25 r n d q e φ n a q e wherein q e r 1 t φ 1 t r k t φ k t t is the general coordinates n d n 1 i 3 3 0 3 3 n k i 3 3 0 3 3 and n a 0 3 3 n 1 i 3 3 0 3 3 n k i 3 3 and the derivatives of displacement and attitude at s are 26 d r d s d n d d s q e n d q e d φ d s d n a d s q e n a q e jacobian matrixes of a beam element the jacobian matrixes of a beam element in the presented method include the mass matrix gyroscopic matrix stiffness matrix and damping matrix the mass matrix is the derivative of the general inertia force with respect to the general acceleration and gyroscopic matrix is that with respect to the general velocity for a beam node with general coordinates q i r i t φ i t t the rotation velocity and rotation acceleration are ω i h i t φ i and ε i h i t φ i h i t φ i respectively wherein h i h φ i with the rigid body hypothesis presented in section 2 1 let m i be the equivalent mass and j i the equivalent rotation inertia matrix as shown in eq 8 then the general inertia force of the node is 27 f b i n e r m i r i h i j i ε i ω i j i ω i then the mass matrix and gyroscopic matrix for a beam node are respectively 28 m i f b i n e r q i m i i 3 3 0 0 h i j i h i t 29 c i g y r o f b i n e r q i 0 0 0 h i j i h i t φ i φ i h i h i t φ i j i h i t φ i φ i and they can be assembled into the system matrix directly node by node according to the general coordinates the stiffness matrix is the derivative of the general elastic force with respect to the general coordinates for a beam element with general coordinates q e r 1 t φ 1 t r k t φ k t t as shown in eq 11 the general elastic force of the beam element is 30 f b e l s t 0 l e a ε 11 ε 11 q e g a γ 12 γ 12 q e γ 13 γ 13 q e j 1 3 e j j j κ j κ j q e d s then the derivative eq 30 with respect to q e gives 31 k f b e l s t q e k 1 k 2 wherein k i contains only the i t h order derivatives of strain and 32 k 1 0 l e a ε 11 q e ε 11 q e g a γ 12 q e γ 12 q e γ 13 q e γ 13 q e j 1 3 e j j j κ j q e κ j q e d s k 2 0 l e a ε 11 2 ε 11 q e q e g a γ 12 2 γ 12 q e q e γ 13 2 γ 13 q e q e j 1 3 e j j j κ j 2 κ j q e q e d s the calculation of the stiffness matrix 32 relies on the derivations of the strains as presented in section 2 1 ε 11 d r d s 2 1 2 γ 12 d r d s y s γ 13 d r d s z s and κ κ 1 κ 2 κ 3 t h s t d φ d s or κ i d φ d s h i s wherein h s h 1 s h 2 s h 3 s then with eqs 25 and 26 the first order derivatives of the strains are calculated directly by 33 ε 11 q e n d t n d q e γ 12 q e n d t y n a t y t φ n d q e γ 13 q e n d t z n a t z t φ n d q e κ j q e n a t h j n a t h j t φ n a q e however the second order derivatives of the strains are more suitable to be calculated with numerical difference method except for the normal strain by 34 2 ε 11 q e q e n d t n d appendix b time discretisation with bdfs and the magnitude of coefficients bdfs use a polynomial curve to connect a series of points and calculate the curve slope at a given point for its velocity and the acceleration is the second order derivative in the time integration programme with bdfs the n t h order integration at m t h time step means that the n t h polynomial curve q t is used for connecting the candidate time step t m q m and its previous n known time steps t m 1 q m 1 t m 2 q m 2 t m n q m n that is 35 q t i 1 n t t m i t m t m i q m j 1 n i 0 n i j t t m i t m j t m i q m j then the first order derivation of eq 35 at t m gives the candidate velocity 36 q m d q t d t t t m p 1 n i 1 n i p t m t m i i 1 n t m t m i q m j 1 n p 0 n p j i 0 n i j i p t m t m i i 0 n i j t m j t m i q m j β 0 q m j 1 n β j q m j and the second order derivation gives the candidate acceleration the deduction for a second order derivation is trivial and the formula is long but we can always get the following abbreviate formulation for candidate acceleration 37 q m d 2 q t d t 2 t t m γ 0 q m j 1 n γ j q m j wherein β 0 β j γ 0 γ j are the functions of the time step sizes specifically 38 γ 0 2 j 1 n p 1 n p j i 1 n i p i j t m t m i i 1 n t m t m i it is obvious that γ 0 has the magnitude of o 1 h 2 when the fixed step size h is adopted for integration and we deduce that γ 0 also has the magnitude of o 1 h m 2 in the variable step size integration by noticing that the change ratio of the step size is controlled in a small scope by the neighboring steps to keep the stability of the time integration q m is the candidate coordinate at time t m and it can be initially predicted in the bdfs method with an extrapolation of the n t h order polynomial curve when the n t h order integration is adopted giving 39 q m j 1 n 1 i 0 n i j t t m i t m j t m i q m j j 1 n 1 α j q m j in general we have 40 q m j 1 n 1 α j q m j q m β 0 q m j 1 n β j q m j q m γ 0 q m j 1 n γ j q m j and γ 0 o 1 h m 2 later the candidate coordinate velocity and acceleration q m q m and q m could be modified iteratively by substituting them into the system equations and reducing the residuals with the iterative method appendix c parameters of the marine riser system to build the basic model and the extended model for simulation the following parameters of a real marine riser system are needed except for the composition shown in fig 4 including the general parameters in table 3 and soil layer composition in table 4 for calculating the operational window with static analyses the current speed distribution as shown in table 5 and the limits of components as shown in table 6 are needed for a recoil analysis a detailed composition of the tensioner as shown in fig 7 and table 7 is needed table 3 general parameters of marine riser system table 3 category item value unit sea environment density of seawater 1030 kg m3 density of mud 1300 kg m3 structure material density of steel 7850 kg m3 young s modulus of steel 206 84 gpa shear modulus of steel 82 74 gpa platform elevation of top of ufj relative to keel 37 4 m elevation of reference point above keel 0 m auxiliary lines serial number 1 2 3 4 5 6 od 152 4 152 4 127 63 5 63 5 66 675 mm id 101 6 101 6 101 6 50 8 50 8 50 8 mm tensioner suspension position points x 0 0 0 0 0 0 m y 2 25 2 25 2 265 2 265 2 265 2 265 m z 0 0 2 264 2 264 2 264 2 264 m table 4 soil layer composition table 4 parameters soft clay stiff clay depth m 0 1 2 4 5 6 7 8 11 undrained shear strength mpa 0 012 0 012 0 021 0 023 0 05 0 05 0 068 0 068 0 075 submerged unit weight kg m3 680 680 700 700 900 900 900 900 900 strain at half max stress 0 01 0 01 0 01 0 01 0 01 0 01 0 01 0 01 0 01 stiff clay sand 12 13 14 20 21 29 30 40 45 50 55 60 0 075 0 082 0 09 0 09 0 1 0 1 0 13 0 15 850 900 860 800 880 880 880 880 880 880 960 960 angle of internal friction 29 31 35 30 0 01 0 01 0 01 0 01 0 01 0 01 0 01 0 01 table 5 current speed distribution for calculation of operational window table 5 probability 90 80 70 60 50 40 30 20 15 10 depth in seawater speed speed speed speed speed speed speed speed speed speed m m s m s m s m s m s m s m s m s m s m s 0 0 017 0 037 0 059 0 084 0 115 0 151 0 199 0 266 0 314 0 381 99 0 015 0 031 0 05 0 071 0 096 0 127 0 167 0 223 0 263 0 32 154 0 013 0 027 0 044 0 062 0 085 0 112 0 147 0 197 0 232 0 282 193 0 012 0 025 0 041 0 058 0 079 0 104 0 137 0 184 0 216 0 263 303 0 01 0 021 0 033 0 047 0 064 0 085 0 111 0 149 0 176 0 213 479 0 006 0 013 0 02 0 029 0 039 0 051 0 068 0 09 0 107 0 129 603 0 005 0 011 0 017 0 024 0 033 0 044 0 058 0 077 0 091 0 11 759 0 004 0 008 0 014 0 019 0 026 0 035 0 046 0 061 0 072 0 088 1450 0 004 0 008 0 014 0 019 0 026 0 035 0 046 0 061 0 072 0 088 7 5 5 3 2 1 0 75 0 5 0 25 0 1 0 05 speed speed speed speed speed speed speed speed speed speed m s m s m s m s m s m s m s m s m s m s 0 37 0 428 0 495 0 58 0 647 0 761 0 809 0 876 0 99 1 142 0 312 0 36 0 416 0 487 0 543 0 639 0 679 0 736 0 832 0 959 0 275 0 317 0 366 0 429 0 478 0 563 0 598 0 648 0 733 0 845 0 254 0 295 0 342 0 4 0 446 0 525 0 558 0 604 0 683 0 788 0 208 0 24 0 277 0 325 0 362 0 426 0 453 0 491 0 554 0 639 0 127 0 146 0 168 0 197 0 22 0 259 0 275 0 298 0 337 0 388 0 107 0 124 0 144 0 168 0 188 0 221 0 234 0 254 0 287 0 331 0 084 0 098 0 114 0 133 0 149 0 175 0 186 0 201 0 228 0 263 0 084 0 098 0 114 0 133 0 149 0 175 0 186 0 201 0 228 0 263 table 6 limits of components for calculation of operational window table 6 restrictive conditions unit drilling standby survival riser mises stress mpa 369 440 8 551 ufj angle deg 4 15 15 lfj angle deg 4 9 10 conductor mises stress mpa 258 309 386 conductor bending moment kn m 3866 4616 5770 table 7 parameters of detailed tensioner for recoil analysis table 7 category item value unit accumulators and cylinders number of tensioner lines 6 hydraulic fluid specific gravity 1 08 isentropic exponent 1 4 tensioner length at zero stroke 25 5 m cylinder dynamic friction coefficient 0 05 cylinder friction characteristic velocity 0 1 m s diameter of tensioner ring 914 4 mm diameter of piston 560 mm diameter of rod 230 mm mass of tensioner ring 11000 kg mass of piston and rod 23930 kg volume of hp accumulator per tensioner cylinder 2 25 m3 volume of hp accumulator during recoil per tensioner cylinder 9 m3 volume of hp air oil accumulator per tensioner cylinder 2 m3 volume of lp accumulator 2 m3 volume of hydraulic fluid in hp accumulator at mid stroke 0 4 m3 pressure of lp side at mid stroke 0 5 mpa piping length m internal diameter mm num of bends friction factor from ① to ② 20 68 4 0 02 from ② to ③ 8 203 2 2 0 02 from ③ to ④ 24 5 200 2 0 02 from ④ to ⑤ 40 50 8 2 0 02 cv curve of antirecoil valve valve positon 0 0 193 0 204 0 222 0 239 0 25 0 284 0 313 cv 0 0 0 1 3 4 7 10 0 341 0 375 0 403 0 478 0 5 0 528 0 568 0 608 0 636 0 676 13 17 23 49 56 64 76 87 94 104 0 705 0 733 0 773 0 801 0 835 0 864 0 898 0 932 0 954 1 111 118 126 131 137 142 147 152 155 160 closure curve in recoil cylinder stroke 0 7 62 9 15 12 2 13 73 15 24 valve position 0 38 0 38 0 35 0 28 0 15 0 15 
21831,the flexibility of marine riser systems in deepwater settings is studied using flexible multibody dynamics with geometrically exact beam for static eigen and recoil analyses the aim is to form an accurate and efficient modelling method and the corresponding solution strategies algorithms for considering the spatial flexibility of the marine riser and movable constraints simultaneously where the former is usually oversimplified in rigid multibody dynamics based works and the latter is inefficient in the finite element method based works therefore a geometrically exact beam with the euler s rotation vector is adopted for the marine riser and its uniform formulation with rigid bodies is presented together with the preconditioning and solving strategies in static eigen and recoil cases some other technical details for speeding up the algorithm are also described such as parallelisation matrix sparsity and variable step size control to verify the present algorithm a model with a depth of 1547 m based on a real platform is built here considering the hydrostatic and hydrodynamic loads on riser hydro pneumatic tensioner and platform motion the numerical results are compared with the commercial software deepriser and up to 93 coincidence is obtained in all cases with six times speedup is achieved in addition an extended model with more details of the real system obtained by considering independent modelling of auxiliary lines is simulated in the recoil analysis with the presented algorithm and the results show that ignoring the stiffness of the auxiliary lines in the traditional method and commercial software may lead to incorrect conclusions regarding the tension safety of the marine riser keywords marine riser system flexible multibody dynamics geometrically exact beam auxiliary lines stiffness static eigen recoil analyses 1 introduction as shown in fig 1 a a marine riser system in deepwater drilling settings usually consists of a dynamically positioned platform hydro pneumatic tensioner subsystem heave compensational telescopic joint tj flex joints lower marine riser package lmrp blowout preventer bop wellhead conductor in the seabed and the main riser section which contains hundreds or thousands of metres of riser joints meanwhile buoyancy current and wave force on the riser should be considered together with drilling mud pressure inside the riser and wave induced motion of the platform for simulating a marine riser system in a deepwater setting the flexibility of the riser is essential because it can be up to several kilometres long and needs to be included in the problems of emergency disconnection vibration control fatigue analysis and so forth focus has been put on riser flexibility from the very beginning early research started from the differential equation of beams in 2d cases which fischer 1966 used for the design of floating vessel drilling risers and burke 1974 used on the static and dynamic analyses of a riser in deepwater settings in addition it still provides rough estimations for engineering design sparks 2007 dareing 2012 nowadays however more accurate results require the modelling of riser flexibility in 3d cases mainly because of 3d environmental conditions such as waves currents and wind all of which can come from different directions and the resulting spatial motion of vessel riser and conductor the finite element method fem was introduced into riser flexibility analyses for 3d cases for example gardner and kotch 1976 applied it with a bernoulli euler beam in the small angle theory and bernitsas et al 1985 applied it with coupling torsion and bending of thin walled circular beam for risers then large displacements are also taken into account with different theories including the geometrically exact beam theory gebt in fem neto et al 2014 absolute nodal coordinate formulation ancf chai and varyani 2006 rigid finite element method rfem adamiec wójcik et al 2015 and rigid finite segment method rfsm in multibody dynamics mbd adamiec wójcik and wojciech 2018 the applications of gebt in a marine riser neto et al 2014 uses accuracy and efficiency in flexible riser because it uses the accurate geometric nonlinearity to describe displacements in space and consumes only 6 degrees of freedom for a beam node whereas an ancf beam node consumes 12 degrees of freedom romero 2008 and rfem and rfsm adamiec wójcik et al 2015 adamiec wójcik and wojciech 2018 only provide simplified versions of 3d large displacements however applications based on mbd raman nair and baddour 2003 lee et al 2015 can take advantage of the natural definition and calculation efficiency of movable constraints i e the tj and flex joints whereas fem usually approximates them with contacts leading to much smaller integration step sizes for convergence however a deepwater drilling riser system usually contains both a long flexible riser that can move in space and that has movable constraints similar systems that require simultaneous consideration of the 3d large displacement of slender parts and movable constraints also widely exist in the industry flexible multibody dynamics fmbd with gebt is rather attractive and effort has been made to use this method by some authors including bauchau and bottasso 1999 ibrahimbegovic and mamouri 2000 romero 2008 liu et al 2018a 2018b 2018c and chen et al 2019 but corresponding applications for marine riser systems for drilling has been still sparse until now therefore the present paper applies a combination of gebt and mbd in the modelling of marine riser systems to investigate the flexibility of the riser and the influence of stiffness of auxiliary lines which is usually ignored in an engineering recoil analysis before the basic fmbd model of a marine riser system in the present paper is built as shown in fig 1 b where gebt is used on the modelling of the slender riser tj and conductor the tj is represented by two parallel beams and two cylindrical pairs on its ends are used regarding the boundary conditions the bottom of the conductor is linked to the seabed and the top of the riser is tensioned at the outer barrel of the tj by the tensioner the remainder of the current paper is organised as follows in section 2 a formulation and solving of the fmbd model of a marine riser system is presented including a uniform formulation of mbd and gebt formulation of loads on a riser preconditioning and solving of the system and the technical details needed for efficient algorithm then in section 3 a real marine riser system with a depth of 1547 m is calculated with the basic fmbd model and extended fmbd model where the former is compared with deepriser for verification and the latter analyses the influence of the independent modelling of auxiliary lines in a recoil analysis the conclusion is given in section 4 2 fmbd model introducing the gebt into mbd allows researchers to benefit from both the high accuracy for the flexible slender parts and modelling convenience of the movable constraints while keeping the solving ability for problems of large displacement large rotation and large deformation however the combination usually produces a system with more degrees of freedom than that with rigid multibody dynamics rmbd and one that is more complicated than that of fem because of the equations of constraints and the nonpositive character of the system matrix therefore correctly choosing the solving details in mbd and the corresponding adjustments for large scale problems are necessary for an efficient fmbd algorithm in the present paper lagrange equations of the first kind are adopted as the governing equations of the system compared with lagrange equations of the second kind although additional degrees of freedom dofs for constraint equations are necessary the cost is much smaller than the dofs of the flexible parts but a more uniform and simper presentation of the constraints for both the rigid and flexible parts can be gained attention should be paid to the preconditioning of the linearised system matrix with constraint equations which is essential when solving static eigen and dynamic problems this is detailed in section 2 3 for the formulation of rigid bodies and nodes of beam elements euler s rotation vector is chosen both to form a uniform description of the kinetic energy and constraint equations in addition euler s rotation vector has the advantage of nonsingularity when compared with the euler angle and has only three parameters for 3d rotation compared with the quaternions that have four parameters and one additional lagrange multiplier for its normalization constraint here the second characteristic is very important for large scale problems in addition the parallelisation strategy utilisation of matrix sparsity and specific solving strategies for a marine riser analysis are also discussed to form an efficient algorithm 2 1 uniform formulation of mbd and gebt the governing equations of an mbd system with lagrange equations of the first kind shabana 2013 are a set of differential algebraic equations daes 1 d d t l q q q l q q q φ q t λ f e φ q 0 wherein q is the dofs vector of rigid and flexible bodies with length n q λ is the dofs vector of constraint equations with length n λ and they are the variables that must be solved the lagrange function l q q is the difference between kinematic energy t and potential energy v φ q is the complete constraint equations and f e is the general force vector of the external loads we note the general forces of a rigid body and flexible body with f b which is produced by a differentiation of the lagrange function hence eq 1 can be rewritten as 2 f b f e φ q t λ 0 φ 0 to form the system governing equations f b f e and φ should be calculated for all the bodies loads and constraints in the system respectively and be assembled into eq 2 per the general coordinates where the loads or constraints are applied to in the following part we give a uniform formulation of f b and φ for the rigid bodies beam elements and their constraints used in the modelling of the marine riser system the spatial rotation of a rigid body or a beam node can be described by the euler s rotation vector φ φ e where φ is the rotation angle and e is the unit rotation axis rodrigues rotation formula shabana 2013 gives the 3 3 transformation matrix of attitude 3 g cos φ i 3 3 sin φ φ skew φ 1 cos φ φ 2 φφ t wherein i 3 3 is a 3 3 identity matrix and where the skew φ is the skew symmetric matrix of vector φ with the relationship skew ω g t g or ω axial g t g the angular velocity in the local coordinate system can be obtained and written compactly as 4 ω h t φ wherein 5 h sin φ φ i 3 3 1 cos φ φ 2 skew φ ϕ sin φ φ 3 φφ t the angular acceleration in local coordinate system is 6 ε ω h t φ h t φ the rotation general force of a rigid body is obtained by substituting eq 4 and eq 6 into eq 1 with the rotation kinematic energy t ω t j ω 2 wherein j diag j 1 j 2 j 3 is the rotation inertia matrix of a rigid body in the local principal axis coordinate system or we can simply calculate this by multiplying ω to the left of euler rotation equation saletan and cromer 1971 j ω ω j ω p to obtain the virtual work of moment p which balances the inertia moment of rotation and the result ϕ t h j ω ω j ω ϕ t h p indicates the rotation general force of a rigid body will be 7 f b r o t h j ε ω j ω the inertia force also includes the translation of inertia force with a mass m and displacement of origin of local coordinate system also inertia centre for rigid body in the frame of reference r r 1 r 2 r 3 t the total general inertia force of a rigid body is 8 f b i n e r m r h j ε ω j ω for simplification the inertia force for a beam element is calculated at its nodes by evenly dividing the inertia moment onto the nodes this means each node with its neighboring left half element if available and neighboring right half element if available is considered a rigid body in the calculation of the inertia force then the rotation inertia matrix j of a beam node is calculated with this rigid body and the origin of the local coordinate system of this rigid body is directly chosen at the node therefore eq 8 is also employed to calculate the general inertia force of the beam elements the general elastic force of a geometrically exact beam simo and vu quoc 1986 1988 is integrated at numerical integration points as shown in fig 2 by the total lagrangian framework the beam strain includes the normal strain ε 11 d r d s 2 1 2 in the axis direction engineering shear strains γ 12 d r d s y s γ 13 d r d s z s in the cross section and rotation strains κ κ 1 κ 2 κ 3 t h s t d φ d s along the three main axes the strain vector is 9 ε ε 11 γ 12 γ 13 κ 1 κ 2 κ 3 t with the same linear constitutive relation as used by neto et al 2014 with section area a elastic modulus e shear modulus g and section inertia moment in main direction j k k the beam stress is 10 σ d ε dia g e a g a g a e j 22 e j 11 e j 33 ε then the general elastic force of a beam element can be obtained by substituting 9 and 10 into eq 1 because the elastic energy v 0 l ε t σ 2 d s is only a function of displacement and attitude and we get 11 f b e l s t v q 0 l e a ε 11 ε 11 q g a γ 12 γ 12 q γ 13 γ 13 q j 1 3 e j j j κ j κ j q d s wherein the partial differential q is in fact q e with q e r 1 t φ 1 t r k t φ k t t as the general coordinates of the beam element when calculating eq 11 there is no explicit expression with the coordinates r i ϕ i put outside of the integration sign because of nonlinear expression of ε and numerical integration is usually applied for example the gauss integration other details such as the calculation of matrix g and h when φ approaches zero lagrange interpolation of displacement and attitude in beam element and calculation of mass matrix and stiffness matrix of a beam element are detailed in appendix a there is no difference between the definitions of constraints on a rigid body and a beam node because they use the same parameters usually in engineering a constraint is defined with the help of a so called maker which is a fixed coordinate frame at given a position and attitude on the body if r m i φ m i is the relative parameter of maker m i on body i r i φ i and r m j φ m j is the relative parameter of maker m j on body j r j φ j then the global position of maker m i is r i g φ i r m i and the global attitude matrix is g m i x m i y m i z m i g φ i g φ m i and it is similar to marker m j then the three kinds of constraints used in the riser system are obtained as table 1 is the inner product of two vectors 2 2 loads on the marine riser the loads on a marine riser mainly come from gravity buoyancy current wave and tensioner at the top and the friction force of the mud should be included in the recoil analysis in marine science gravity and buoyancy are usually considered together in marine science with the concept of effective gravity dareing 2012 which is the gravity of the structure minus that of displaced seawater and plus that of the fluid inside the structure if applicable because buoyancy is only valid for a subject with a closed surface in liquid when used for a slender structure in the sea a short section of the structure is surrounded with seawater and compensating pressure forces are added perpendicularly to the two cross sections outwards for the seawater outside inwards for the fluid inside it is worth mentioning that the effective gravity of a beam element is uniformly distributed along the length therefore the compensating pressure forces should also be uniformly distributed along the length rather than applied to the nodes at the ends directly then the general force of gravity and buoyancy is 12 f e g r a v b u o y 0 l m m d i s m i n l g r s q ρ s w a o u t ρ i n a i n l g x l g x 0 q d s wherein g 9 8 0 0 is the gravity acceleration vector x 0 and x l are the external normal vectors of the cross section at the ends m is the mass of beam element m d i s is the mass of displaced seawater m i n is the mass of fluid inside ρ s w is the density of seawater a o u t is the outer section area of the tubular beam ρ i n and a i n are the density and section area of liquid inside respectively the formula for a rigid body could be obtained from eq 12 by removing both the integration symbol and dividing it by l in engineering applications the inner surface of the riser is usually smooth and a i n and m i n could be obtained directly but m d i s should be calculated with the help of weight in seawater w s e a and weight in air w a i r where m d i s w s e a w a i r l a i n ρ s w in addition if the riser has additional buoyant foam attached to it m should be replaced with m m f o a m m d i s f o a m where m f o a m is the mass of the foam in the element length and m d i s f o a m is the mass of seawater displaced by the foam in the present paper the hydrodynamic forces of currents and waves are calculated using a morison empirical formula borgman 1958 where the perpendicular hydrodynamic force is a function of a transverse relative velocity and where the inertia force is a function of transverse relative acceleration let a s w and v s w be the acceleration and velocity vectors of seawater with the given current and wave a t r r s a s w r s a s w x s and v t r r s v s w r s v s w x s give the transverse relative acceleration and velocity then with the outer diameter d a coefficient of added mass c a m and coefficient of drag c d the morison force is 13 f m o r ρ s w c a m π d 2 4 a t r ρ s w c d d 2 v t r v t r and corresponding general force of current and wave is 14 f e c u r w a v e 0 l f m o r r q d s the tensioning force at the top of the riser is a group of nonlinear concentrated forces that act on the tensioner ring purple ring of the tensioner part in fig 1 in different directions in a static analysis it is usually simplified as a total upward constant force and an additional total stiffness is also considered for eigen analysis for the recoil analysis the detailed tensioner model presented by pestana et al 2016 for the direct acting tensioner is adopted in the present paper where the pressure loss of the gas flowing through pipelines is ignored the pressure loss of the oil flowing through pipelines is considered using the darcy weisbach formula the pressure loss of oil flowing through antirecoil valve is calculated with a definition formula of the flow coefficient cv and a controlled closure curve in recoil and the pressure change of the gas in the accumulators is considered an adiabatic process usually before the start of a recoil mode the antirecoil valve is fully opened and does not generate any pressure loss to the tensioner the parameters of the detailed tensioner used in the present paper are shown in appendix c table 7 here the pressure of the hp side is not given in the table which is in fact determined by the given total top tensioning force pressure of the lp side and the sizes of the tensioner therefore a prior static analysis is always executed before the recoil analysis to obtain the pressures of the hp side at the equilibrium status under the given top tensioning force when a recoil case is considered the lmrp is separated from the bop and the mud in the riser flows out from the bottom of the riser therefore the additional hydrostatic pressure of seawater over the newly cut truncation should be applied at the lower end of the lmrp after separation and the time varying friction forces of mud acting along the riser should be applied during recoil it should be noted that the area of a newly cut truncation is the total section area of the riser and the auxiliary lines not the section area of the lmrp the friction forces of mud acting on the riser are precalculated with the slug model presented by grytoyr et al 2011 and then applied onto the nodes of riser during recoil in which the mud column is considered a rigid body for velocity calculation and where the friction forces are proportional to the velocity squared the matlab code for calculating the mud friction forces by grønevik 2013 is directly adopted here 2 3 preconditioning and solving of the system equations the uniform formulations of the general force of bodies constraint equations and general force of external loads that is f b φ and f e in eq 2 were presented in the last section however the general force vector f f b f e and φ have distinct orders of magnitude and their jacobians stand in different rows and columns of the system matrix which finally lead to a condition number problem in the fmbd it is straightforward to precondition eq 2 by multiplying a coefficient η onto φ and its partial derivatives φ q t to balance the magnitude and the following equations are solved instead of eq 2 with λ λ η 15 f η φ q t λ 0 η φ 0 wherein f f q q q t φ φ q φ q t φ q t q however the value of η should be different for the cases of a dynamic solution and eigen solution in the fmbd because η is used to balance the jacobian matrix coefficient matrix of the linearised problem but the algebrisation and linearisation processes of eq 15 for the dynamic and eigen solutions are different for the former if q is chosen as the solving variable time discretisation is applied to q and q to get the nonlinear algebraic equations and then eq 15 is expanded at the pre estimated value of q for the latter letting q q 0 gives out the nonlinear equations and then eq 15 is expanded at the balance state the details of preconditioning the eigen problem has been presented in yang et al 2012 where the preconditioning coefficient is chosen as the maximum absolute value of the jacobian matrix of general forces that is 16 η f q for eigen problem for a dynamic problem it is related to the time discretisation and pre estimation formats for example if the n t h order backward differential formulas bdfs for stiff problems wanner and hairer 1996 are used for time discretisation and the same order polynomial extrapolations are used for pre estimation that is 17 q m j 1 n 1 α j q m j q m β 0 q m j 1 n β j q m j q m γ 0 q m j 1 n γ j q m j wherein q m is the pre estimated general coordinate at m t h time step t m q m and q m are the discretized velocity and acceleration with q m at t m and α j β 0 β j γ 0 and γ j are the coefficients only related to the time step size history see appendix b for deduction details in time integration of dynamic problem with bdfs method the inverse square of time step size multiplies the mass matrix as shown in appendix b and the order gap between jacobian of constraints and jacobian of general forces of bodies is largely amplified to prevent it γ 0 is chosen as the precondition coefficient of the dynamic problem that is 18 η γ 0 o 1 h m 2 for dynamic problem wherein h m is the current step size then for a dynamic problem the nonlinear equations 15 can be solved with a newton iteration by 19 f q γ 0 f q β 0 f q γ 0 φ q t γ 0 φ q 0 δ q m δ λ m f γ 0 φ q t λ m γ 0 φ at q m q m q m t m and q m δ q m t λ m δ λ m t t gives a better estimation of the solution at t m and the iteration stops until the error is satisfied the item f q is the mass matrix f q is the damping and gyroscopic matrix and f q is the stiffness matrix these matrixes are calculated at each rigid body and beam element and then assembled into the system matrix per their corresponding general coordinates the jacobian matrixes for the beam element are deduced in appendix a because the beam node and rigid body use the same inertia formula eq 3 8 these inertia matrixes are also applicable for a rigid body the jacobian matrixes of the external forces acting on the bodies are more suitable to be numerically calculated with the finite difference method the jacobian matrixes of constraints are trivial and their deductions are omitted here the jacobian matrix of the constraint force η φ q t λ q from eq 15 is ignored in 19 to obtain a stable algorithm because of an abrupt value change of λ in the iteration however it must be kept in the eigen problem to form a complete jacobian matrix for the correct calculation of eigen values and eigen modes the static problem of mbd is usually solved by the same process of a dynamic problem by adding artificial damping into the system that is using f q a f q b f q instead of f q in eq 19 wherein a and b are the rayleigh coefficients and therefore use the same preconditioning coefficient as shown in eq 18 although the direct iteration method is widely used in static fem problems and is very efficient it is not suitable for static mbd problems because of the nonlinearity of large rotations 2 4 technical details for an efficient algorithm the models for mbd with getb usually have much larger scales of calculation than those for rmbd when it comes to both modelling accuracy and object size therefore the technical details for medium and large scale problems should be implemented to form an efficient algorithm that has a high accuracy these details include the variable step size control parallelisation strategy utilisation of matrix sparsity fast convergence of static problem and integration adjustment for the recoil problem variable step size control is very important for a fast time integration using an implicit method the step size varies with the change of smoothness of the problem in the process where a larger step size is chosen for gentle response variations to speed up integration and a smaller step size for steep response variations to satisfy the integration error furthermore the implicit method itself has a much larger acceptable step size than explicit method because of the demand of stability and is widely used in problems of vibration and control the bdfs method presented with eq 17 is an implicit method and the intelligent control strategy of the step size adopted in the present paper is the same with that of the ode15i function in matlab https www mathworks com help matlab ref ode15i html which solves the ordinary differential equations with a variable step size by 1 5 order of the bdfs method the parallelisation and utilisation of matrix sparsity are indispensable for the speedup of medium and large scale problems parallelisation reduces the time consumption of the force vector and jacobian matrix in eq 19 by calculating f and f for different beam elements rigid bodies parallelly and the same is true for φ and φ q for different constraints parallelisation is also used on the matrix product of preconditioning in eq 19 however the parallelisable iteration method for solving linear equations is usually unsuitable for mbd because of the nonpositive character of system matrix in eq 19 therefore a direct method is adopted and the utilisation of sparsity could reduce the bandwidth of the jacobian matrix and save both the memory space and calculation time because the flexible deepwater marine riser system presented in the current paper is a medium scale problem with a number of dofs less than one hundred thousand the intel mkl pardiso package with shared memory parallelisation is used to reorder and solve the linear equations besides the general measures mentioned above specific strategies could be applied to speed up the time integration of the static and dynamic cases based on the characteristics of the problems for example the calculation of an operational window of a marine riser system needs thousands of calculations of a static equilibrium under the different speeds of the current and translations of the platform as presented in section 2 2 the static equilibrium is calculated using a dynamic integration process with artificial damping to reach the equilibrium quickly in a single calculation we can relax the limit of the integration error to obtain a bigger step size and stops until max δ q m 2 q m 2 q m 2 will satisfy the accuracy after a long interval of simulation time in fact only a few steps in addition we can get a fast convergence for the static calculation by starting from the equilibrium of its neighbour calculation therefore the sketch in fig 3 a gives a good choice for sequence of static calculations and for a recoil analysis with the dynamic calculation the lmrp is cut off from the bop at time t c u t for an emergency disconnection and the tensioned riser completes the recoil process in several seconds the integration before recoil and after recoil are fixed at the second order to obtain a larger step size by eliminating high order system noise with the numerical damping but it is fixed to the fourth order during the several seconds of recoil to keep the high frequency response of the riser which is important for a safety analysis of the riser as shown in fig 3 b an additional time interval t p r e t c u t is also included in the fourth order stage to ensure the fourth order at the very beginning of recoil 3 numerical results here we consider a real marine riser system of the x oil field in the south china sea with a water depth of 1547 m that is built with the composition shown in fig 4 and calculated with our homemade codes based on the method presented in section 2 other parameters such as the parameters of the platform tensioner auxiliary lines soil currents and waves are shown in appendix c 3 1 basic fmbd model in the basic fmbd model of the marine riser shown in fig 1 b the same as the one done in fem the auxiliary lines are considered as an added mass on the riser with their inner areas added to that of the riser to count up the total inner pressure effect and the distribution of the material on a riser joint is also considered as uniform a static analysis for operational window eigen analysis and recoil analysis are performed in the homemade codes and compared with the commercial fem software deepriser related research of a riser recoil analysis with deepriser can be found in lang et al 2009 calculation parameters for both the homemade codes and deepriser are set as follows the marine riser system is calculated with a top tension of 5800 kn the static analysis of the operational window uses the current data and limitations of the component shown in appendix c but waves are not considered according to standard engineering conventions the eigen frequencies and modes are calculated at the static equilibrium under the current of 0 05 probability with no offset the recoil analysis separates the riser from the well at the 46th second and the setting of antirecoil valve can be found in appendix c the effects of currents wave and platform offset are not considered in the recoil analysis of the example the length of the beam element for the marine riser is about 3 m for example a riser joint with a length of 22 86 m is evenly divided into eight beam elements the length of a beam element for a conductor is 1 m for the basic fmbd model the number of dofs of the system is 3470 and there are 17 constraints all the results are calculated on the same desktop computer intel core i7 7700k 4 2 ghz 4 cores as shown in fig 5 a b and c the results of the homemade codes agree well with deepriser with up to 93 coincidence covering the static eigen and recoil analyses and in fig 5 d the recoil analysis of deepriser takes a fixed step size of 0 05 before separation and 0 01 after separation for a stable calculation here a larger step size setting will cause a failure mainly because of its definition of a tj with contacts pairs however the homemade codes take a much larger step size with the definition of constraints with algebraic equations and specific strategy for the recoil analysis in fig 3 b here a 6 3 times speedup is gained in the homemade codes with a single core and another 2 1 times speedup is gained with the parallelisation of four cores resulting in 13 3 times speedup because parallelisation is not available for a recoil analysis in deepriser yet the lmrp displacement riser effective tension envelope top tension and tj stroke of the recoil analysis are compared in fig 5 c interestingly the lmrp displacement shows an acceptable elevation of the lmrp from the wellhead while the riser s effective tension envelope shows there is a dangerous effective compression of the marine riser during the recoil process negative value of effective tension represent compression in the riser both software give the same results indicating that a high tension may maintain the safety of the wellhead but may cause the collapse of the riser in an emergency disconnection a lower value of the top tension could avoid effective compression in the marine riser and it is calculated and compared with the extended fmbd model in the following subsection 3 2 extended fmbd model the basic fmbd model of a marine riser system could be improved in its accuracy by an extended modelling of the auxiliary lines and joint interface however the present paper will only focus on the auxiliary lines because the consideration of the differences of stiffness and mass at the joint interface is neither difficult for fem nor has an evident effect on the results more specifically only a recoil analysis is carefully studied and discussed with the extended modelling of the auxiliary lines as the calculations with homemade codes show that the basic model is good enough in accuracy for static and eigen analysis as shown in fig 6 a in the extended fmbd model each auxiliary line around the riser is modelled with a beam and is connected to the riser at each connection location six times the number of beam elements and more than four hundred fixed hinges are required in addition to what is found in the model in section 3 1 with its six auxiliary lines especially the fixed hinges between the auxiliary line nodes and riser nodes are located at the centres of the auxiliary line nodes and are eccentric to the riser nodes and this ensures the rotation of the riser nodes can influence the configuration of the auxiliary line nodes the calculated marine riser system is the same as that in section 3 1 except for the tension reduction from 5800 kn to 3850 kn and an additional consideration of the wave induced heave motion of the platform in the recoil analysis as described in section 3 1 the top tension is reduced to try to keep the lower bound values of the effective tension positive moreover ignoring the effects of waves in other directions and attitudes except for the heave direction keeps the riser recoil in a simple vertical line which helps us explain the interesting phenomenon found in the results of the extended model later for comparison the extended model uses the same initial data of the tensioning system and riser setup in the recoil analysis as the basic model the results will show us what will happen in the recoil analysis when these initial data are used for the real riser with stiffness of the auxiliary lines the results of the recoil analysis for the extended model are compared with those of the basic model and deepriser in fig 6 b where the basic model and deepriser agree with each other once again while the extended model gives different results for the results of extended model the lmrp elevation is larger but the lower bound of effective tension steps into the compressed state region again this phenomenon can be explained by a higher axial stiffness of the combination of the riser and auxiliary lines in the extended model which reduces the elongation and produces more vibration in the example as shown in table 3 appendix c and fig 4 the cross section areas of the auxiliary lines sum to 0 0286 m2 which is 1 38 times of riser 1 0 0208 m2 1 57 times of riser 2 0 0182 m2 and 1 83 times of riser 3 0 0156 m2 to illustrate this we calculate an artificially modified version of the basic model the welded basic model or welded model for short where the auxiliary lines are supposed to be welded to the main riser everywhere and where the associated stiffness is taken into account here the results are compared showing that the black dashed lines of the welded model agree well with the dark green dashed lines of the extended model meaning that the conclusion is confirmed we also calculated a deepriser model considering load sharing of the auxiliary lines namely the deepriser model with the deepriser option load sharing with choke kill line on and with the deepriser parameter load sharing gap set to zero this model is similar to the welded model in riser stiffness and the results are shown in fig 6 with light green dash lines once again the effective tension steps into the compressed state region when considering the stiffness of auxiliary lines in the recoil analysis however the results of the lmrp elevation and tj stroke of this model are different from those of the welded model it seems the initial value of the tj stroke in deepriser will not vary with considering the stiffness of the auxiliary lines by load sharing therefore there must be a default riser length increase in deepriser otherwise if the riser length and tensioner settings are unchanged a higher riser stiffness may lead to less riser elongation in static conditions which would lead to a higher initial tj stroke as shown in the results of the extended model and welded model to confirm the surmise a riser length increase is considered in the extended and welded models to compare with the deepriser model here 0 5 m of the riser length increase is employed as a proper value according to the difference of the lmrp elevations however our primary numerical tests show that the results do not agree well with those of the deepriser model after carefully checking on the setting of deepriser we find that in deepriser the prior static analysis for preparing a recoil analysis also considers the load sharing of auxiliary line and it leads to change of tensioner setting in the recoil analysis this is because the initial hp side pressure of the detailed hydro pneumatic tensioner in a recoil analysis is set by the balanced riser configuration of a prior static analysis therefore to form the welded and extended models to compared with the deepriser model the auxiliary line stiffness is also considered in the prior static analysis besides a riser increase of 0 5 m in the welded and extended models as shown in fig 6 the results of the extended model and welded model agree well with those of deepriser model in the top tension lmrp elevation and tj stroke time traces although some difference is found in the upper bound of the effective tension envelope the lower bound is actually concerned and is acceptable for a small difference in addition both the welded extended deepriser welded and extended models lead to a compressed state of riser in the recoil analysis with considering auxiliary line stiffness no matter the auxiliary line stiffness is considered or not in a prior static analysis for the recoil tensioner setting in general the results comparison in fig 6 can be divided into three groups ① models without considering auxiliary line stiffness ② models with considering auxiliary line stiffness in only the recoil process ③ models with a riser length increase and considering auxiliary line stiffness in both the prior static analysis and the recoil process and the situations in group ② is most likely to happen when an engineer uses the data calculated without considering auxiliary line stiffness in a riser recoil operation however the welded model only accounts for the vertical motion the motion of the marine riser is completely spatial when the other environmental impact factors are considered such as the platform offset all wave effects and the current and wave forces on the riser it is apparent that the welded model has a much larger bending stiffness for the riser than reality so a meaningful result is not expected therefore the extended fmbd model is required for a recoil analysis in 3d space especially where a buckling risk for the riser is concerned and where the traditional methods may produce an incorrect evaluation of whether there is bucking of the riser in the recoil process the results of lmrp elevation in fig 6 have lower values than the initial value indicating that there is contact danger between the lmrp and wellhead although this situation could be improved by setting the separation time at the lowest point of the heave motion of the platform we still set the separation time at the 46th second to keep it the same as in section 3 1 moreover it does not affect our conclusion regarding the notable influence of the stiffness of the auxiliary lines in the recoil analysis 4 conclusions in the present paper the flexibility of a marine riser system is investigated in fmbd with gebt which is important for a riser safety analysis because of its thousands of metres of length in deepwater settings however traditional methods with the rmbd use a simplified riser because of computation limits while methods with fem have difficulties modelling the movable constraints and detailing the marine riser therefore the combination of gebt and mbd in the modelling of a marine riser system in the current paper aims at an accurate and efficient algorithm for simultaneous consideration of a detailed riser flexibility and movable constraints to achieve this aim the parameterization of euler s rotation vector is chosen for both the beam node and rigid body to describe the 3d rotation with three parameters and a uniform formulation of beam element and rigid body is presented together with a uniform definition of the constraints and net loads to achieve a stable algorithm preconditioning of the static eigen and dynamic analyses are described with the same parameter η because of the different scales between the constraints and forces while different values of η are chosen for the eigen and dynamic cases then the numerical time integration with a variable step size is detailed because the static analysis shares the same algorithm in mbd with artificial damping to obtain an efficient algorithm more technical details for a larger scale analysis are applied including the parallelisation strategy utilisation of matrix sparsity and special strategies for the static analysis of operational windows and recoil analysis for verification of the present algorithm the basic model with a depth of 1547 m based on a real platform is built in homemade codes and the commercial software deepriser this is done by considering liquids loads hydro pneumatic tensioner and platform motion the numerical results show a coincidence up to 93 in all cases and a six times speedup is gained when using the homemade codes then the influence of the auxiliary lines in the recoil analysis based on an extended model is studied in the homemade codes where the pipelines are also modelled with beam elements and are constrained to the main riser with hundreds of joints the results show that ignoring the stiffness of the auxiliary lines in the basic model and deepriser leads to incorrect conclusions about whether there is compression of the marine riser in the recoil analysis the extended model is built to only consider vertical forces and motions to keep the riser recoil in the vertical direction and a welded model mimics the extended model yielding the same results however the extended model is recommended for the 3d simulation of a marine riser for safety analysis during recoil furthermore the presented fmbd method could be applied for a deeper detailing of the marine riser system such as the influence of drillstring contact on the riser which can be studied in future research credit authorship contribution statement cheng yang conceptualization methodology resources visualization writing original draft jianbin du conceptualization funding acquisition methodology project administration writing review editing zaibin cheng conceptualization software validation yi wu software validation chaowei li methodology declaration of competing interest the authors declared that they have no conflicts of interest acknowledgments the research is supported by nsfc 11772170 the key laboratory of spacecraft design optimization and dynamic simulation technologies beihang university ministry of education china under grant no 2019kf001 national basic research program of china 2015cb251203 and the project on electric driver seat technology for large passenger aircraft mj 2018 s 44 which are gratefully acknowledged by the authors appendix a some details in calculating the general force of bodies calculation of matrix g and h when φ approaches zero in rodrigues rotation formula the matrix g gives the transformation matrix of the attitude of the body with euler s rotation vector φ by 20 g cos φ i 3 3 sin φ φ screw φ 1 cos φ φ 2 φφ t wherein φ φ e φ is the rotation angle and e is the unit rotation axis and with the matrix h the rotation velocity and acceleration of body in local coordinate is calculated as 21 ω h t φ ε h t φ h t φ wherein 22 h sin φ φ i 3 3 1 cos φ φ 2 screw φ φ sin φ φ 3 φφ t however the coefficients sin φ φ 1 cos φ φ 2 and φ sin φ φ 3 in eq 20 and eq 22 cannot be calculated directly when φ approaches zero and the taylor expansion should be used instead 23 sin φ φ 1 1 6 φ 2 1 120 φ 4 1 5040 φ 6 o φ 8 1 cos φ φ 2 1 2 1 24 φ 2 1 720 φ 4 1 40320 φ 6 o φ 8 φ sin φ φ 3 1 6 1 120 φ 2 1 5040 φ 4 1 362880 φ 6 o φ 8 lagrange interpolation of displacement and attitude in the beam element the displacement r and attitude φ at length s of an element in the gebt are calculated with a lagrange interpolation 24 r s i 1 k n i s r i φ s i 1 k n i s φ i wherein k is the number of nodes in an element n i is the shape function at i t h node as shown in table 2 r i is the displacement at the i t h node and φ i is the attitude at the i t h node the interpolation 24 can be rewritten in matrix form as 25 r n d q e φ n a q e wherein q e r 1 t φ 1 t r k t φ k t t is the general coordinates n d n 1 i 3 3 0 3 3 n k i 3 3 0 3 3 and n a 0 3 3 n 1 i 3 3 0 3 3 n k i 3 3 and the derivatives of displacement and attitude at s are 26 d r d s d n d d s q e n d q e d φ d s d n a d s q e n a q e jacobian matrixes of a beam element the jacobian matrixes of a beam element in the presented method include the mass matrix gyroscopic matrix stiffness matrix and damping matrix the mass matrix is the derivative of the general inertia force with respect to the general acceleration and gyroscopic matrix is that with respect to the general velocity for a beam node with general coordinates q i r i t φ i t t the rotation velocity and rotation acceleration are ω i h i t φ i and ε i h i t φ i h i t φ i respectively wherein h i h φ i with the rigid body hypothesis presented in section 2 1 let m i be the equivalent mass and j i the equivalent rotation inertia matrix as shown in eq 8 then the general inertia force of the node is 27 f b i n e r m i r i h i j i ε i ω i j i ω i then the mass matrix and gyroscopic matrix for a beam node are respectively 28 m i f b i n e r q i m i i 3 3 0 0 h i j i h i t 29 c i g y r o f b i n e r q i 0 0 0 h i j i h i t φ i φ i h i h i t φ i j i h i t φ i φ i and they can be assembled into the system matrix directly node by node according to the general coordinates the stiffness matrix is the derivative of the general elastic force with respect to the general coordinates for a beam element with general coordinates q e r 1 t φ 1 t r k t φ k t t as shown in eq 11 the general elastic force of the beam element is 30 f b e l s t 0 l e a ε 11 ε 11 q e g a γ 12 γ 12 q e γ 13 γ 13 q e j 1 3 e j j j κ j κ j q e d s then the derivative eq 30 with respect to q e gives 31 k f b e l s t q e k 1 k 2 wherein k i contains only the i t h order derivatives of strain and 32 k 1 0 l e a ε 11 q e ε 11 q e g a γ 12 q e γ 12 q e γ 13 q e γ 13 q e j 1 3 e j j j κ j q e κ j q e d s k 2 0 l e a ε 11 2 ε 11 q e q e g a γ 12 2 γ 12 q e q e γ 13 2 γ 13 q e q e j 1 3 e j j j κ j 2 κ j q e q e d s the calculation of the stiffness matrix 32 relies on the derivations of the strains as presented in section 2 1 ε 11 d r d s 2 1 2 γ 12 d r d s y s γ 13 d r d s z s and κ κ 1 κ 2 κ 3 t h s t d φ d s or κ i d φ d s h i s wherein h s h 1 s h 2 s h 3 s then with eqs 25 and 26 the first order derivatives of the strains are calculated directly by 33 ε 11 q e n d t n d q e γ 12 q e n d t y n a t y t φ n d q e γ 13 q e n d t z n a t z t φ n d q e κ j q e n a t h j n a t h j t φ n a q e however the second order derivatives of the strains are more suitable to be calculated with numerical difference method except for the normal strain by 34 2 ε 11 q e q e n d t n d appendix b time discretisation with bdfs and the magnitude of coefficients bdfs use a polynomial curve to connect a series of points and calculate the curve slope at a given point for its velocity and the acceleration is the second order derivative in the time integration programme with bdfs the n t h order integration at m t h time step means that the n t h polynomial curve q t is used for connecting the candidate time step t m q m and its previous n known time steps t m 1 q m 1 t m 2 q m 2 t m n q m n that is 35 q t i 1 n t t m i t m t m i q m j 1 n i 0 n i j t t m i t m j t m i q m j then the first order derivation of eq 35 at t m gives the candidate velocity 36 q m d q t d t t t m p 1 n i 1 n i p t m t m i i 1 n t m t m i q m j 1 n p 0 n p j i 0 n i j i p t m t m i i 0 n i j t m j t m i q m j β 0 q m j 1 n β j q m j and the second order derivation gives the candidate acceleration the deduction for a second order derivation is trivial and the formula is long but we can always get the following abbreviate formulation for candidate acceleration 37 q m d 2 q t d t 2 t t m γ 0 q m j 1 n γ j q m j wherein β 0 β j γ 0 γ j are the functions of the time step sizes specifically 38 γ 0 2 j 1 n p 1 n p j i 1 n i p i j t m t m i i 1 n t m t m i it is obvious that γ 0 has the magnitude of o 1 h 2 when the fixed step size h is adopted for integration and we deduce that γ 0 also has the magnitude of o 1 h m 2 in the variable step size integration by noticing that the change ratio of the step size is controlled in a small scope by the neighboring steps to keep the stability of the time integration q m is the candidate coordinate at time t m and it can be initially predicted in the bdfs method with an extrapolation of the n t h order polynomial curve when the n t h order integration is adopted giving 39 q m j 1 n 1 i 0 n i j t t m i t m j t m i q m j j 1 n 1 α j q m j in general we have 40 q m j 1 n 1 α j q m j q m β 0 q m j 1 n β j q m j q m γ 0 q m j 1 n γ j q m j and γ 0 o 1 h m 2 later the candidate coordinate velocity and acceleration q m q m and q m could be modified iteratively by substituting them into the system equations and reducing the residuals with the iterative method appendix c parameters of the marine riser system to build the basic model and the extended model for simulation the following parameters of a real marine riser system are needed except for the composition shown in fig 4 including the general parameters in table 3 and soil layer composition in table 4 for calculating the operational window with static analyses the current speed distribution as shown in table 5 and the limits of components as shown in table 6 are needed for a recoil analysis a detailed composition of the tensioner as shown in fig 7 and table 7 is needed table 3 general parameters of marine riser system table 3 category item value unit sea environment density of seawater 1030 kg m3 density of mud 1300 kg m3 structure material density of steel 7850 kg m3 young s modulus of steel 206 84 gpa shear modulus of steel 82 74 gpa platform elevation of top of ufj relative to keel 37 4 m elevation of reference point above keel 0 m auxiliary lines serial number 1 2 3 4 5 6 od 152 4 152 4 127 63 5 63 5 66 675 mm id 101 6 101 6 101 6 50 8 50 8 50 8 mm tensioner suspension position points x 0 0 0 0 0 0 m y 2 25 2 25 2 265 2 265 2 265 2 265 m z 0 0 2 264 2 264 2 264 2 264 m table 4 soil layer composition table 4 parameters soft clay stiff clay depth m 0 1 2 4 5 6 7 8 11 undrained shear strength mpa 0 012 0 012 0 021 0 023 0 05 0 05 0 068 0 068 0 075 submerged unit weight kg m3 680 680 700 700 900 900 900 900 900 strain at half max stress 0 01 0 01 0 01 0 01 0 01 0 01 0 01 0 01 0 01 stiff clay sand 12 13 14 20 21 29 30 40 45 50 55 60 0 075 0 082 0 09 0 09 0 1 0 1 0 13 0 15 850 900 860 800 880 880 880 880 880 880 960 960 angle of internal friction 29 31 35 30 0 01 0 01 0 01 0 01 0 01 0 01 0 01 0 01 table 5 current speed distribution for calculation of operational window table 5 probability 90 80 70 60 50 40 30 20 15 10 depth in seawater speed speed speed speed speed speed speed speed speed speed m m s m s m s m s m s m s m s m s m s m s 0 0 017 0 037 0 059 0 084 0 115 0 151 0 199 0 266 0 314 0 381 99 0 015 0 031 0 05 0 071 0 096 0 127 0 167 0 223 0 263 0 32 154 0 013 0 027 0 044 0 062 0 085 0 112 0 147 0 197 0 232 0 282 193 0 012 0 025 0 041 0 058 0 079 0 104 0 137 0 184 0 216 0 263 303 0 01 0 021 0 033 0 047 0 064 0 085 0 111 0 149 0 176 0 213 479 0 006 0 013 0 02 0 029 0 039 0 051 0 068 0 09 0 107 0 129 603 0 005 0 011 0 017 0 024 0 033 0 044 0 058 0 077 0 091 0 11 759 0 004 0 008 0 014 0 019 0 026 0 035 0 046 0 061 0 072 0 088 1450 0 004 0 008 0 014 0 019 0 026 0 035 0 046 0 061 0 072 0 088 7 5 5 3 2 1 0 75 0 5 0 25 0 1 0 05 speed speed speed speed speed speed speed speed speed speed m s m s m s m s m s m s m s m s m s m s 0 37 0 428 0 495 0 58 0 647 0 761 0 809 0 876 0 99 1 142 0 312 0 36 0 416 0 487 0 543 0 639 0 679 0 736 0 832 0 959 0 275 0 317 0 366 0 429 0 478 0 563 0 598 0 648 0 733 0 845 0 254 0 295 0 342 0 4 0 446 0 525 0 558 0 604 0 683 0 788 0 208 0 24 0 277 0 325 0 362 0 426 0 453 0 491 0 554 0 639 0 127 0 146 0 168 0 197 0 22 0 259 0 275 0 298 0 337 0 388 0 107 0 124 0 144 0 168 0 188 0 221 0 234 0 254 0 287 0 331 0 084 0 098 0 114 0 133 0 149 0 175 0 186 0 201 0 228 0 263 0 084 0 098 0 114 0 133 0 149 0 175 0 186 0 201 0 228 0 263 table 6 limits of components for calculation of operational window table 6 restrictive conditions unit drilling standby survival riser mises stress mpa 369 440 8 551 ufj angle deg 4 15 15 lfj angle deg 4 9 10 conductor mises stress mpa 258 309 386 conductor bending moment kn m 3866 4616 5770 table 7 parameters of detailed tensioner for recoil analysis table 7 category item value unit accumulators and cylinders number of tensioner lines 6 hydraulic fluid specific gravity 1 08 isentropic exponent 1 4 tensioner length at zero stroke 25 5 m cylinder dynamic friction coefficient 0 05 cylinder friction characteristic velocity 0 1 m s diameter of tensioner ring 914 4 mm diameter of piston 560 mm diameter of rod 230 mm mass of tensioner ring 11000 kg mass of piston and rod 23930 kg volume of hp accumulator per tensioner cylinder 2 25 m3 volume of hp accumulator during recoil per tensioner cylinder 9 m3 volume of hp air oil accumulator per tensioner cylinder 2 m3 volume of lp accumulator 2 m3 volume of hydraulic fluid in hp accumulator at mid stroke 0 4 m3 pressure of lp side at mid stroke 0 5 mpa piping length m internal diameter mm num of bends friction factor from ① to ② 20 68 4 0 02 from ② to ③ 8 203 2 2 0 02 from ③ to ④ 24 5 200 2 0 02 from ④ to ⑤ 40 50 8 2 0 02 cv curve of antirecoil valve valve positon 0 0 193 0 204 0 222 0 239 0 25 0 284 0 313 cv 0 0 0 1 3 4 7 10 0 341 0 375 0 403 0 478 0 5 0 528 0 568 0 608 0 636 0 676 13 17 23 49 56 64 76 87 94 104 0 705 0 733 0 773 0 801 0 835 0 864 0 898 0 932 0 954 1 111 118 126 131 137 142 147 152 155 160 closure curve in recoil cylinder stroke 0 7 62 9 15 12 2 13 73 15 24 valve position 0 38 0 38 0 35 0 28 0 15 0 15 
21832,this article compares the accuracy of return value estimates from stationary and non stationary extreme value models when the data exhibits covariate dependence the non stationary covariate representation used is a penalised piecewise constant ppc model in which the data are partitioned into bins defined by covariates and the extreme value distribution is assumed to be homogeneous within each bin a generalised pareto model is assumed where the scale parameter can vary between bins but is penalised for the variance across bins and the shape parameter is assumed constant over all covariate bins the number and sizes of covariate bins must be defined by the user based on physical considerations numerical simulations are conducted to compare the performance of stationary and non stationary models for various case studies in terms of quality of estimation of the t year return value over the full covariate domain it is shown that a non stationary model can give improved estimates of return values provided that model assumptions are consistent with the data when the data exhibits non stationarity in the generalised pareto tail shape the use of non stationary model assuming a constant shape parameter can produce biases in return values in such cases a stationary model can give a more accurate estimate of return value over the full covariate domain as only the most extreme observations regardless of covariate are used to estimate tail shape in other cases the assumption of a stationary model will ignore key features of the data and be less reliable than a non stationary model for example if a relatively benign covariate interval exhibits a long or heavy tail extreme values from this interval may influence the t year return value for very large t however the sample of peaks over threshold with high threshold used to estimate a stationary model in this case may not include sufficient observations from this interval to estimate the return value adequately keywords covariate extreme generalised pareto metocean significant wave height non stationary 1 introduction accurate estimation of extreme events is important in offshore and coastal engineering under estimation of the magnitude of extreme events can lead to structural failures whilst over estimation can lead to overly conservative and expensive designs and inefficient allocation of limited resources return periods of extreme events are usually estimated by fitting a statistical model to observed or modelled data and extrapolating into the tail of the distribution the accuracy of estimated return values is dependent on numerous factors including a quality of historic data henceforth dataset b length of dataset c characteristics of the actual data generating distribution d misspecification of the statistical model and e method used to estimate the statistical model bias in metocean data obviously leads to bias in estimates of extremes random errors in metocean data lead to positive bias i e a tendency to estimate return values that are higher than the true return values since the distribution of random errors is convolved with the distribution of the variable forristall et al 1996 brooker et al 2004 shorter datasets lead to higher variance in estimates of extremes but can also increase bias since bias in parameter estimators for various distributions can vary with the number of observations similarly the shape of the tail of the distribution affects both the bias and variance of estimates of extreme values with estimates of longer tailed distributions having a higher variance for a given sample size biases in parameter estimates also vary with the shape of the tail see e g de zea bermudez and kotz 2010 kang and song 2017 model misspecification refers to differences between the true characteristics of the data and the data generating model responsible for it and the assumptions made in the statistical model at present the most commonly applied method for estimating extremes of metocean variables is the peaks over threshold pot method see e g coles 2001 jonathan and ewans 2013 the pot method makes the following key assumptions about the data 1 observations are independent and identically distributed iid given covariates and 2 exceedances of a sufficiently high threshold follow a generalised pareto gp distribution the gp distribution describes the asymptotic behaviour of independent threshold exceedances from a max stable data generating distribution as threshold level increases theory suggests that the closeness of the conditional distribution of peaks over threshold to the gp form improves the appropriateness of the gp distribution is therefore based on the threshold being sufficiently high that the asymptotic approximation is valid with too low a threshold leading to increased bias in the estimated extreme values due to the gp distribution not being an appropriate model the choice of threshold is a trade off between increased bias from setting the threshold low and increased variance from setting the threshold high so that there are fewer observations the rate of convergence of threshold exceedances from the data generating distribution to the gp distribution may however be slow that is a very large threshold might be required for the gp form to be considered a reasonable approximation making practical inference difficult for finite samples for this reason a number of pre asymptotic parametric distributional forms or penultimate approximations have been proposed beirlant et al 2012 gomes 2014 the idea being that the data generating distribution is in some sense closer to the penultimate approximation than to the asymptotic distribution for finite threshold however since the gp model is the most widely used at present it has been applied in the present work and the use of penultimate approximations is not pursued further here in addition a large literature on non parametric alternatives for estimation of distributional tails exists see e g hill 1975 dekkers et al 1989 regarding the assumption that observations are iid given covariates metocean variables typically exhibit serial correlation so the assumption of independence is not true if a model is fitted to all observations this is dealt with by declustering the data where only the largest observation in each storm are considered so that storm maxima can be considered approximately independent the criterion for what determines independent storms is usually defined in terms of a minimum separation in time a rigorous treatment of the correlation between successive extreme events can be made by plotting the extremogram davis and mikosch 2009 an analogue of the autocorrelation function for sequences of extreme events although care must be taken to first remove the seasonal signal from the data which introduces a longer range correlation an example of the use of the extremogram to define a declustering time scale was presented by mackay and johanning 2018 which showed that a time scale of around 5 days was sufficient for the datasets considered in that study alternatively declustering times can be defined based on more heuristic arguments about the average time scales for weather systems to pass over a site typically taken to be in the range of 2 5 days ewans and jonathan 2008 discuss a physically motivated approach to declustering time series of significant wave height h s based on the assumption that the peak severities of different storm events given covariates are statistically independent storm events are identified from time series of sea state h s a storm event corresponds to the time interval between the h s up crossing of some threshold level and the subsequent down crossing of the threshold in addition storm intervals separated by less than 24 h are merged the threshold can be defined e g in terms of a covariate dependent quantile of sea state h s the peak value of sea state h s during the storm interval then defines the storm peak h s values of storm peak h s for different storms are taken to be statistically independent the distributions of many metocean variables such as significant or individual wave heights wind speeds and storm surge exhibit dependence on other variables referred to as covariates for example many studies have considered the dependence of wind speeds or wave heights on the direction of origin of the storm and the time of year season fawcett and walshaw 2006 méndez et al 2008 ewans and jonathan 2008 randell et al 2015 jones et al 2016 wave heights and wind speeds are also dependent on large scale climatic indices such as the north atlantic oscillation nao woolf et al 2002 or the el niño southern oscillation enso bromirski et al 2005 moreover most studies tacitly assume that the distribution of metocean variables are stationary in time neglecting the effects of the changing climate which have been observed in some studies reguero et al 2019 cattrell et al 2019 in this study we focus on the effects of periodic covariates such as season and direction and defer consideration of longer term variations in climate to future work specifically we quantify differences in the performance of models which account for the covariate effects and those that do not referred to here as constant or stationary models obviously stationary models cannot produce estimates of seasonal or directional extremes so our interest here is in which model gives the more accurate estimates of return values for the full covariate domain typically referred to as annual omniseasonal or omnidirectional return values we use the term omnicovariate where necessary below for clarity the quality of historical data is not considered here but all the other factors listed above influence the comparison between stationary and non stationary models and therefore need to be considered there has been some debate in the literature about the circumstances in which non stationary models should be applied and whether stationary or non stationary models produce more accurate estimates of omnicovariate return values the motivation for using non stationary models is that their underpinning assumptions better reflect the characteristics of the data and our physical understanding non stationary models assume that the distributions of independent peaks over covariate dependent threshold conditional on covariates tend towards a gp distribution under this assumption now consider the highest threshold value on the covariate domain for which the gp distribution is a reasonable approximation for this threshold value the omnicovariate distribution is a convolution of the gp distributions over the covariate domain and therefore not a gp distribution itself if we now increase the threshold yet further we expect to eliminate the influence of all values of covariate except those contributing to the extreme tail of the omnicovariate distribution and that the resulting distribution of threshold exceedances would be closer to a gp distribution once more it may therefore be expected that a high threshold would be required for stationary models to give a similar level of performance as non stationary models however it is also reasonable to expect that the most information about the shape of the tail of the omnicovariate distribution is contained in the largest observations in applied extreme value analysis there is a maxim that ensuring a good fit to the bulk of the data does not guarantee a good fit to the tail it is therefore reasonable to ask whether modelling less extreme observations in a non stationary model reduces the bias and variance of return values model complexity is another consideration stationary models are simpler to implement and have fewer parameters to estimate whilst the complexity of non stationary models is not an argument against their use on its own practical considerations aside it may be expected that the larger number of parameters that need to be estimated for non stationary models would increase the variance of those estimates the need to estimate more parameters is traded off against two effects firstly due to the larger number of parameters non stationary models offer a more flexible hence potentially more accurate fit to the data secondly non stationary models are typically fitted to a larger proportion of the observations increasing the sample size from this discussion it is apparent that theoretical arguments alone cannot justify the use of a stationary or non stationary model exclusively from the practitioner s perspective the challenge is knowing which type of model gives the most accurate estimates of extreme values in a given situation many of the earlier studies on the use of non stationary models e g carter and challenor 1981 morton et al 1997 anderson et al 2001 compared their performance to stationary models in situations where the true return values were not known in these studies it is not possible to conclude which type of model is more accurate only that results differ jonathan et al 2008 presented a comparison of stationary and non stationary models using simulated data where the observations are drawn from two distinct distributions representing storms from two directions they demonstrate that in the cases they consider the non stationary models give lower bias in estimates of return values mackay et al 2010 argued that these results were not representative of real situations where the distribution of a variable will vary smoothly with direction season or other covariate rather than changing sharply at the boundary of two sectors mackay et al 2010 presented the results of simulations where the distribution of storm peak h s conditioned on season varied continuously through the year piecewise constant models were fitted where the data were divided into a number of discrete bins and independent fits were made in each bin it was shown that the piecewise constant models performed worse in estimating omniseasonal return values than the stationary models with higher bias and variance in all cases considered and with both bias and variance increasing with the number of bins used it was explained that the reason the non stationary models performed worse in these case studies was due to the independent estimates of the gp shape parameter in each bin as the number of bins increases the sample size in each bin decreases and the variance of the parameter estimates increases a high estimate of the gp shape parameter in one bin is not compensated for by a low estimate in another bin and therefore leads to a positive bias in the annual return values jonathan and ewans 2011 argued that the results in mackay et al 2010 were due to a fortuitous choice of extreme value threshold for the stationary model and that there was no way of knowing in practice where the correct threshold should be set jones et al 2016 extended the study of jonathan et al 2008 using more sophisticated covariate representations splines fourier series and gaussian processes and suggested that the performance of stationary models in estimating omnicovariate models is in general more variable than the performance of a non stationary model the purpose of the present study is to extend the results of jonathan et al 2008 and mackay et al 2010 in an attempt to provide further guidance on the relative performance of stationary and non stationary models in realistic situations we extend the results from mackay et al 2010 in two main ways firstly case studies are constructed where the threshold for both the stationary and non stationary models can be varied so that the effect of threshold choice can be examined secondly we consider a penalised piecewise constant ppc non stationary model ross et al 2018 2019 in this model the data are partitioned into bins defined by covariates and the gp scale parameter is allowed to vary between bins but the shape parameter is constant over all bins the likelihood function used to estimate the parameters is penalised for the variance in the scale parameter over all the bins with the roughness penalty selected using cross validation to maximise predictive likelihood more advanced non stationary models than the ppc model have been proposed which have the objective of providing optimally flexible descriptions of the systematic variability of extreme values with covariate e g zanini et al 2020 typically a regression approach underpins these models e g northrop et al 2016 a suitable set of basis functions for the covariate domain is defined and the value of each of the extreme value model parameters on the covariate domain is then defined as a linear combination of basis functions the basis coefficient vector is estimated statistically suitable bases for one dimensional covariate domains include splines and fourier series basis functions with compact support such as b splines are advantageous computationally ppc exploits a piecewise constant basis in one dimension there are numerous variants of spline parameterisations these include p splines penalised b splines eilers and marx 2010 for which squared differences of neighbouring basis coefficients are penalised to increase the smoothness of the representation and adaptive regression splines e g biller 2000 for which locations of spline basis knots are also estimated to optimise model fit useful bases for higher dimensional covariates include thin plate splines e g wood 2003 suitable kernels e g radial basis functions and voronoi tessellations e g bodin and sambridge 2009 bases for higher dimensional covariates can also be formed from tensor products of lower dimensional bases e g raghupathi et al 2016 higher dimensional bases formed from tensor products of penalised b splines admit efficient computation using generalised linear additive models currie et al 2016 the motivation for using the ppc model over more advanced forms of non stationary model is that is represents a good compromise between simplicity robustness and flexibility the ppc model represents a step up in complexity from binning the data and fitting independent models in each bin the non stationary model considered by mackay et al 2010 where the additional complexity of the roughness penalisation makes the model more robust to increasing uncertainties from dividing the data into bins the complexity of the ppc model is determined by the number of bins used rather than the number of covariates it can therefore be used for multidimensional covariate problems without modification making it very flexible as with previous studies the scope of the current study is necessarily limited to a relatively small number of case studies hence the conclusions drawn here may not be applicable universally the results presented apply to the ppc model and similar types of non stationary model however we have also attempted to draw more general conclusions that extend to other types of non stationary model in particular the conclusions about the effects of binning the data and assuming a piecewise constant distribution apply to other types of model that take this approach and the conclusions about the effects of assuming a stationary shape parameter are likely to be applicable to any non stationary model that makes this assumption moreover as discussed further in section 3 since the ppc model only considers the total level of variability between bins the particular choice of patterns of covariate dependence are not restrictive the paper is organised as follows a brief overview of the theory and model assumptions is presented in section 2 the design of the simulation case studies is described in section 3 in section 4 we examine the effect of non stationarity in the data on the shape of the tail of the omnicovariate distribution and the effect this has on quality of estimation from return values from a stationary fitted model the effect of partitioning the data into bins and fitting a piecewise constant non stationary model is considered in section 5 section 6 summarises the results of the simulation studies finally conclusions are given in section 7 2 theory and assumptions 2 1 return values from a non stationary distribution in the present study we consider estimation of the distribution of an arbitrary variable x where x could be interpreted as storm peak h s or another environmental variable showing dependence on covariates it is assumed that storm peaks are sufficiently separated in time that adjacent observations are independent further it is assumed that x follows some arbitrary distribution with parameters dependent on one or more covariates in the current study we consider the influence of a single covariate denoted t which could be interpreted as the time of year season or mean wave direction at the storm peak denote the cumulative distribution function cdf of x conditional on a particular choice of t as p s x x t for simplicity it is assumed that t t 0 360 it is further assumed that the occurrence rate of storm peaks ρ t t is dependent on t where the rate is defined as the number of storms per year per unit covariate the probability that a storm selected at random has associated covariate t is 1 p t t ρ t m where 2 m 0 360 ρ t d t is the expected number of storms per year the unconditional cdf of x for a storm selected at random denoted p r s is obtained by integrating the conditional cdf over the covariate domain weighted by occurrence 3 p r s x x 0 360 p s x x t p t t d t the t year return value x t is then the solution of 4 p r s x x t 1 t m 2 2 penalised piecewise constant ppc model consider a sample d x i i 1 n of n values of peaks over threshold for a random variable x further let t i i 1 n be the corresponding values of a covariate t on some domain t we assume a single covariate but extension to more complex covariate domains is straightforward as explained in ross et al 2018 we make inferences about extreme values of x given t for t t the piecewise constant model uses a particularly simple description of non stationarity with respect to covariates for each observation in the sample the value of covariate t i is used to allocate the observation to one and only one of n b i n covariate intervals or bins c k k 1 n b i n by means of an allocation vector a such that k a i and t c k for each k all observations in the set x i a i k with the same covariate interval c k are assumed to have common extreme value characteristics a non stationary gp model is then estimated using cross validated roughness penalised maximum likelihood estimation for covariate interval c k the extreme value threshold u k 0 is assumed to be a quantile of the empirical distribution of x in that interval with specified non exceedance probability ψ 0 1 with ψ constant across intervals and estimated by counting threshold exceedances are assumed to follow the gp distribution with shape ξ 0 5 and scale σ k 0 with cdf 5 f g p x ξ σ k u k 1 z k where 6 z k 1 ξ x u k σ k 1 ξ ξ 0 exp x u k σ k ξ 0 per covariate interval c k f g p is defined on x u k x k with x k u k σ k ξ when ξ 0 and otherwise the parameters u k σ k and ξ are the threshold scale and shape parameters respectively since estimation of the shape parameter is particularly problematic ξ is assumed constant but unknown across covariate intervals and the reasonableness of the assumption assessed by inspection of diagnostic plots parameters ξ σ k are estimated by maximising the predictive performance of a roughness penalised model optimally regulating the extent to which σ k varies across intervals using a cross validation procedure the sample gp likelihood l under the piecewise stationary model is 7 l k 1 n b i n x i u k i a i k 1 σ k 1 ξ σ k x i u k 1 ξ 1 where l u k σ k and ξ are functions of marginal extreme value threshold non exceedance probability ψ and ξ is constant across the n b i n intervals c k the negative log likelihood penalised for the roughness of σ k across intervals is then 8 ℓ log l λ σ 1 n b i n k 1 n b i n σ k σ 2 where ℓ is a function of both ψ and roughness coefficient λ σ and σ is the mean value of σ k over the bins 9 σ 1 n b i n j 1 n b i n σ j for given ψ and λ σ estimates for ξ and σ k are found by minimising ℓ the minimisation is conducted using a simplex search method lagarias et al 1998 the search is initialised using first guess of ξ ˆ 0 where the caret denotes an estimate of a parameter and the moment estimates of σ in each interval the optimisation is constrained to give ξ ˆ 0 5 and max x i a i k u k ˆ σ k ˆ ξ ˆ when ξ ˆ 0 a random 10 fold cross validation is then used to select the value λ ˆ σ of λ σ and corresponding ξ ˆ σ ˆ k which for each ψ maximises predictive performance in the ppc model if λ σ then the model has only one degree of freedom for σ whereas if λ σ 0 then the fitted model has n b i n degrees of freedom for σ for intermediate values the effective degrees of freedom for σ is at some intermediate value in a typical application the complete ppc modelling procedure is repeated for a number of bootstrap resamples of the original sample to capture sampling uncertainty moreover for each sample the extreme value model is evaluated for multiple thresholds with non exceedance probability ψ drawn at random from the interval i ψ 0 1 on which model performance is deemed reasonable from inspection of diagnostics however in the current study where the model is applied in a large number of monte carlo simulated datasets only the original sample is used moreover as we wish to study the effect of threshold level on the estimates the ppc model is fitted for several values of ψ and the results compared directly the method used to fit the ppc model is relatively simple it is conceivable that other methods such as markov chain monte carlo mcmc could potentially improve results however this would represent a significant step up in terms of complexity as mentioned above the motivation for using the ppc model is for its balance between simplicity and flexibility examples of non stationary models using mcmc can be found in e g hansen et al 2020 zanini et al 2020 once the ppc model has been estimated the omnicovariate distribution is obtained using the discretised form of 3 10 p ˆ r s x x 1 n t k 1 n b i n n k f g p x ξ ˆ σ k ˆ u k ˆ where n k is the number of observations in interval c k and 11 n t k 1 n b i n n k return values can then be estimated using 4 for consistency the stationary model used in this work is a special case of the ppc model with a single covariate bin and no roughness penalisation 2 3 assessment criteria the performance of the stationary and non stationary models are assessed in terms of the bias standard deviation std and root mean square error rmse of estimated model parameters and return values over n realisations of monte carlo simulated datasets let θ ˆ denote an estimator of either a model parameter or return value θ the expected value bias std and rmse of the estimator are defined as 12 e θ ˆ 1 n j 1 n θ ˆ j 13 bias θ ˆ e θ ˆ θ 14 std 2 θ ˆ 1 n j 1 n θ ˆ j e θ ˆ 2 15 rmse 2 θ ˆ 1 n j 1 n θ ˆ j θ 2 bias 2 θ ˆ std 2 θ ˆ where θ ˆ j is the estimate corresponding to the j th monte carlo simulated dataset henceforth referred to as a trial for brevity 3 design of case studies previous simulation studies comparing stationary and non stationary models have generated data from a gp distribution where the parameters depend on covariate values the limitation of this type of study is that the minimum threshold for which the stationary model can be applied is the maximum threshold value over all covariates since below this level the distribution is not defined for all covariate values to overcome this limitation a model is required for the distribution of all storm peak data not just the tails this could be achieved by using a two part model with a parametric distribution for the body of the distribution and a gp model for the tail the problem with this approach is that the choice of distribution for the body is arbitrary and it is difficult to ensure continuity of the density function on the boundary between body and tail in our simulations we have opted to simulate from the generalised extreme value distribution gev rather than the gp distribution avoiding the need for a two part model previous investigations details available from the authors on request with measured data also show that the gev distribution is a reasonable model for storm peak h s the gev is the asymptotic distribution of block maxima of fixed block size e g hourly daily or weekly maxima storm peak data can be considered block maxima in a sense where the block size is related to the method used for identifying storm peaks although the block size is not strictly constant however we are not using the gev to generate data to conduct a block maxima analysis instead we are using the gev to generate data for a non stationary pot analysis using the ppc model a pot analysis can be applied to data generated from any distribution the motivation for using the gev as the data generating distribution in the current study is that it has the convenient property that the tail converges to the gp distribution with the same shape parameter in the sense illustrated below the cdf of the gev can be written as 16 f g e v x exp z where z is defined in the same way as the for the gp distribution in 6 in the tail of the distribution z is small as z 0 we have exp z 1 z and f g e v x μ σ ξ f g p x μ σ ξ that is the gev and gp cdfs converge with common scale and shape parameters and gev location parameter μ equal to the gp threshold u as illustrated in fig 1 for the case μ 0 σ 1 ξ 0 it is well known that there is a relation between block maxima modelled using the gev distribution and threshold exceedances modelled using the gp distribution see e g coles 2001 however the argument above merely relates to the similarity of the functional forms of the gev and gp tails and is not the same as the argument associating the gp distribution for peaks over threshold when the gev is used for block maxima fitting a gp model to a dataset with gev as the data generating model will introduce some bias at lower threshold values due to the mismatch between the fitted model and data generating model see fig 1 the resulting bias and std of parameter and quantile estimates when fitting the gp distribution to gev data is examined in the appendix the bias in parameter and quantile estimates are slightly higher when a gp model is fitted to gev data than when a gp model is fitted to gp data however the std is slightly lower resulting in an rmse that is comparable the use of the gev distribution as the data generating model rather than the gp distribution will therefore not significantly influence the results for the ppc model the likelihood is penalised on the variance of the scale parameter the difference between the estimates of the scale parameters in adjacent bins is not considered explicitly only the total variance over all bins the complexity of the ppc model is therefore determined by the total number of covariate bins only and not the number of covariates used therefore the case studies considered here focus on a single covariate and the results can be expected to apply to cases with multiple covariates we now consider two sets of case studies in the first the gev parameters are assumed to vary linearly with covariate t and in the second the parameters are assumed to vary sinusoidally with t the parameters in the first set of case studies are defined as 17 μ a t 360 a 3 3 18 σ 1 b t 360 b 0 2 19 ξ 0 1 the first set of case studies is designed to illustrate the effect of fitting a stationary extreme value model to data from a non stationarity data generating distribution and is similar to the ppc fit in a specific covariate bin see section 4 the parameters in the second set of case studies are defined as 20 μ α cos 2 π t 360 21 σ 1 β cos 2 π t 360 22 ξ 0 1 γ cos 2 π t 360 where different choices of α β and γ are also considered as the ppc model does not directly account for the difference in parameter estimates between adjacent bins on the covariate domain it is mainly the level of variation between bins that influences model fit and not the pattern of variation the assumption of sinusoidal variation in model parameters is therefore not particularly restrictive however it will be shown in section 6 2 that the level of non stationarity of the data within a bin does influence model fit the second set of case studies is designed to be more representative of a real situation and are used to compare the performance of the stationary and non stationary models the gev parameters for each case are listed in table 1 the first case with α β γ 0 is included to illustrate the effect of increasing the number of bins on the estimated omnicovariate return values in absence of covariate effects and is discussed in section 5 the subsequent cases illustrate the effect of different patterns in the variation of the data generating distribution parameters and are discussed in section 6 for each case simulations are conducted as follows the sample size is fixed at 1440 observations this corresponds to a mean time between storm peaks of 5 days and a dataset length of 20 years if a year is assumed to last 360 days as defined in section 2 the occurrence rate is assumed to be constant with covariate so the values of the covariate t are simulated as uniformly distributed numbers in 0 360 after the values of t have been simulated gev parameters are defined for each storm peak conditional on t and a random value of x is generated the ppc model is fitted to the data using between 1 and 8 or sometimes 12 bins with bin edges spaced evenly over the covariate domain with the first bin centred at t 0 the threshold for the gp model in each bin is defined as the empirical quantile corresponding to a fixed non exceedance probability ψ where the levels are set at ψ 0 6 0 7 0 8 0 9 for the cases where the observations are partitioned into 8 bins this gives approximately n 72 54 36 18 threshold exceedances per bin respectively for each case 10 000 trials were performed the estimation of the optimal penalty via cross validation is the most time consuming step in estimating the ppc model to reduce computation the optimal penalty is estimated for only the first 100 trials subsequent trials use the median penalty from the first 100 trials the estimated optimal penalty showed very little variation over the first 100 trials justifying the use of the median value in the remaining trials examples of simulated datasets for four of the cases see eqs 20 22 and table 1 are shown in fig 2 together with the theoretical quantiles at non exceedance levels of ψ 0 6 0 9 0 99 0 999 and 0 9999 since we have defined the location parameter to be sinusoidally varying about zero there are some observations that are negative which is not representative of some environmental variables such as storm peak wave heights or wind speeds this could be rectified by adding an offset to μ which would have the effect of offsetting all the observations however the choice of offset would be arbitrary so has been left as zero the return values for each case calculated from 3 and 4 are shown in fig 3 in case 1 the location parameter varies with t whilst other parameters remain constant the result is an offset in the return value curves which grows with α the offset in the return values does not change much with return period in case 2 the scale parameter varies with t while other parameters are held constant the resulting distribution shown in fig 2 is pinched in the middle this pattern of variation is less representative of real situations but is included for illustration the resulting effect on return values grows with return period in case 3 both the scale and location parameters vary with t which is more representative of real situations finally in case 4 all the parameters are non stationary when γ 0 2 there is a change in the gradient of the return value curve that occurs at a return period of approximately 100 years for other values of γ the return value varies smoothly with return period in case 4 both the stationary and ppc models are misspecified since the ppc model assumes a constant shape parameter the assumption of a stationary shape parameter is commonly used in oceanographic applications e g davison and smith 1990 anderson et al 2001 it is therefore interesting to assess how well the ppc model performs in this situation the cases with γ 0 2 may be less realistic for metocean variables due to the positive shape parameter in some sectors however they are instructive to include as they illustrate some potentially important effects the performance of the stationary and non stationary models is assessed in terms of bias std and rmse in the 100 year and 1000 year return values as the size of the return values differs between cases we need to compare the relative size of the uncertainties in estimates to achieve this we have normalised the bias std and rmse by the size of the true return values this means that the normalised results are influenced by the arbitrary choice of the mean value of the location parameter μ for a larger mean value of μ the true return values would increase and the normalised bias std and rmse would be reduced however since any normalisation is somewhat arbitrary we have opted to use this convention the choice of normalisation used here does not influence the conclusions of the study in terms of which model performs better the choice of normalisation only influences the relative magnitude of the effects 4 fitting a stationary model to data from a non stationary data generating distribution here we examine the effect of non stationarity in the data generating model on the tail of the estimated omnicovariate distribution using a stationary fitted model so that the effect of non stationarity can be assessed in isolation before considering the effect of partitioning the data by covariate the true data generating model with a linear variation in parameters is described in 17 19 to illustrate the influence of non stationarity on the shape of tail of the distribution we apply a normalisation so that the tail shape can be considered without the influence of varying location and scale parameters suppose we wish to examine the shape of the tail above threshold level u corresponding to non exceedance probability ψ define threshold exceedances as y x u for x u the conditional distribution of threshold exceedances is 23 f y y p y y x u 1 1 p r s x y u 1 ψ the mean m and std s of the conditional distribution are given by 24 m 0 y f y y d y 25 s 2 0 y m 2 f y y d y where f y y d f y y d y is the probability density function of threshold exceedances fig 4 shows the tail distribution 1 f y y against the normalised quantity x u s where u is defined to correspond to a non exceedance probability of ψ 0 7 for various values of a and b from the upper left plot it is evident that for these values of ξ and ψ there is almost no change in the shape of the tail of the distribution for a linear variation in location however for the upper left plot where b 2 when the scale is also non stationary increase in a makes the distribution appear marginally shorter tailed the lower plots show that a non stationary scale parameter has a more significant effect making the distribution longer tailed with increasing b however the effect is reduced slightly when there is also an increase in location parameter a similar trends not shown were observed for other choices of ξ and ψ now we consider how non stationarity affects bias in estimates when fitting a stationary model for each value of a and b a simulation study was conducted as follows the sample size was fixed at n 500 observations for each simulation covariate values were generated as uniform random variables t 0 360 the parameters of the gev conditional on t were defined according to 17 and 18 and a stationary model the ppc model with one covariate bin was fitted at threshold levels corresponding to ψ 0 7 and 0 9 for each value of a and b 10 000 random trials were conducted fig 5 shows the mean of the estimated shape parameter ξ ˆ as a function of a and b for thresholds at ψ 0 7 and 0 9 note that only the mean ξ ˆ can be shown rather than bias since there is no true shape parameter when a b 0 since the data generating distribution is in fact a non stationary gev integrated over t and is not gev itself in the case a b 0 the true shape parameter is ξ 0 1 we observe a negative bias in ξ ˆ due to two effects the known bias in maximum likelihood estimators hosking and wallis 1987 and the fact that we are fitting a gp distribution to gev data see discussion in the appendix the trends in the mean ξ ˆ with a and b are similar to the results indicated in fig 4 when b 0 non stationarity in the location has little influence on the estimated shape parameter but when there is non stationarity in the scale then the estimated shape becomes more negative with increasing a it is also clear that non stationarity in the scale has more influence on the shape that non stationarity in the location fig 6 shows the bias in the estimated omnicovariate return value x p where the return value is defined to be the quantile at a non exceedance probability of 0 9999 corresponding to a return period around 20 times the length of the observations for the threshold at ψ 0 9 the non stationarity has relatively little effect on the bias in the return value since much of the non stationarity is removed by the high threshold and the gp model is a good fit for the tail of the distribution in fact for a less than approximately 1 5 the bias actually reduces with increasing b since the positive bias introduced by the non stationary scale is compensated by the negative bias which results from the parameter estimation method for the lower threshold the bias initially increases with b becomes more negative then decreases again the effect of non stationarity in the location parameter has a smaller effect overall the change in the bias with a and b is relatively small compared to the bias in the case of a stationary distribution at a b 0 5 fitting a non stationary model to data from a stationary data generating distribution if we are confident there is no non stationarity in the data then there is obviously no need to apply a non stationary model it is however instructive to consider the application of a non stationary model in this situation ppc and similar models are designed so that in application to stationary data a large roughness parameter λ σ would be estimated and the variability of estimated σ with covariate t would consequently be small corresponding to an approximately stationary gp fit however it is interesting to study the practical performance of ppc in this setting in particular the effect of choice of number of covariate bins and other characteristics of covariate binning since we know the data generating distribution is stationary any effects observed cannot be due to non stationary in the dataset we consider the simplest case of a fit to data from a constant distribution case 1 of table 1 with α 0 we consider three model types with increasing complexity in the first model independent fits to the data in each covariate bin are performed in the second model the shape parameter is assumed to be constant over all bins but the scale parameter fit is unconstrained i e ppc fit with λ σ 0 the third model is full ppc where the shape parameter is constant over all bins and the scale parameter per bin is chosen to maximise predictive performance note that in the case of a single covariate bin all models are equivalent the bias std and rmse in the 100 year omnicovariate return value x 100 are shown in fig 7 for fits using between 1 and 12 bins the results for the 1000 year return value are similar and are not shown here for the one bin stationary fitted model there is a negative bias in the estimate of x 100 which is a result of previously mentioned bias in the maximum likelihood estimators and fitting a gp model to gev data in the case of independent fits to each bin the bias increases with the number of bins used this effect was reported by mackay et al 2010 and is caused by the increased uncertainty in the shape parameter with a high estimate of ξ in one bin not being compensated for by a low estimate in another the std of estimates also increases with both the threshold non exceedance probability ψ and the number of bins used since sample size per bin reduces with both ψ and the number of bins the rmse for the fits with ψ 0 6 decreases slightly from its value in the 1 bin case to a minimum in the 3 bin case we attribute this to a balancing between the negative bias from parameter estimation and positive bias from increased binning resulting in a slightly lower rmse for all other threshold levels the rmse increases monotonically with the number of bins used for the ppc λ σ 0 case the performance of the fitted model is much more stable as a function of the number of bins used up to 8 bins for more than 8 bins there is a large increase in both the bias and std of the estimates for ppc optimal λ σ model the performance is very similar to the ppc λ σ 0 model up to 8 bins however when using more than 8 bins the performance of the full ppc model is much more stable due to the influence of the roughness penalty on σ for more than 10 bins there is some increase in the bias from the ppc model it is thought that this bias results from lack of convergence of the simple simplex type optimisation algorithm used for maximum likelihood inference nevertheless the bias is still very small compared with the other approaches even for 12 covariate bins the bias and std in parameter estimates from the independent fits per bin model are shown in fig 8 with the corresponding plots for full ppc model with optimal λ σ in fig 9 for the independent fits the bias and std increases with the number of bins used due to the reduced sample size in each bin for the full ppc model the results are again considerably more stable as a function of number of covariate bins there is a small reduction in the bias with increasing number of bins used this is likely due to the increased influence of the σ roughness penalty which acts to optimise the performance of the model the std in the estimates remains fairly constant with the number of bins used in the full ppc model for fits with 11 and 12 bins the std increases when using a threshold at ψ 0 6 but reduces for the threshold at ψ 0 9 again we attribute this effect at least in part to lack of convergence for large numbers of covariate bins of the simplex optimisation algorithm used in ppc we conclude from this study that the full ppc model provides a good representation of stationary data generating distributions with parameters considered at least when the number of covariate bins does not exceed 10 therefore for the studies reported in section 3 we focus on the fits using up to 8 bins we note that more sophisticated optimisation schemes exploiting likelihood slope and curvature characteristics davison 2003 raghupathi et al 2016 are available for more challenging applications 6 fitting a non stationary model to data from a non stationary data generating distribution in this section we consider the performance of stationary and non stationary fitted ppc models for the four cases with sinusoidal parameter variation described in section 3 eqs 20 22 and table 1 samples of which are illustrated in fig 2 for these case studies the true omnicovariate data generating distribution is an integral of gev distributions over the covariate domain and therefore not itself a gev distribution hence it is not possible to assess performance in terms of the parameter estimates from the fitted gp models instead we focus on the estimating omnicovariate return values for which the true values can be calculated from 4 as illustrated in fig 3 we consider the four cases from table 1 in turn 6 1 case 1 data from distribution with non stationary location parameter fig 10 shows the bias std and rmse of the estimated 100 year omnicovariate return value as a function of the number of covariate bins and threshold levels used in the full ppc model results for the 1000 year omnicovariate return value display similar trends and are not shown here the number of observations used for modelling is dependent only on the threshold non exceedance probability and not on the number of bins used however the observations used for fitting change depending on how the data are binned the fitted one bin stationary model shows a negative bias which reduces with increasing threshold level consistent with the results shown in fig 6 for the lower threshold levels the bias becomes slightly more negative as α increases and the amplitude of variation in location parameter grows in contrast at ψ 0 9 the bias and std does not vary much with α the reduction in bias with increasing threshold is due to two effects first as threshold increases there is less covariate variation in sample of threshold exceedances form modelling secondly the gp distribution provides a better fit to the gev distribution in the tail as discussed in theappendix for non stationary fits bias and std reduce initially as a function of increasing number of covariate bins up to 3 bins performance thereafter stabilises with std and rmse remaining approximately constant up to 8 bins the stability in performance of the ppc model with the number of bins used is due to the use of the σ roughness penalty as the number of bins used increases the optimal penalty also increases so that the model does not over fit the trend in bias with increasing number of bins for α 2 and 3 is somewhat more complex than might be anticipated this is due to the effect of the location of bin edges which is discussed further in section 6 2 in general the ppc model fitted using 5 8 bins using a threshold at ψ 0 6 or 0 7 gives the best performance in this case 6 2 case 2 data from distribution with non stationary scale parameter in a similar fashion to section 6 1 bias std and rmse in the 100 year omnicovariate return values for case 2 table 1 are shown in fig 11 for the fitted one bin stationary model bias is negative for β 0 25 but slightly positive for β 0 5 bias becomes more negative as the number of bins increases in general but there is an excursion in the bias for the 3 bin model most pronounced for β 0 5 and ψ 0 6 0 7 despite the increasingly negative bias with the number of bins used in the model the std and rmse remains approximately constant for more than 2 bins due to σ roughness penalisation for β 0 25 the performance of the stationary one bin and non stationary models are similar in terms of rmse for the case with β 0 5 the ppc model with n b i n 4 gives a small improvement in performance over the stationary model the excursion for three bin fits is due to the placement of the bin edges fig 12 shows the true tail distributions in each covariate bin for a threshold level at ψ 0 6 when the data is partitioned into 2 3 4 or 5 bins together with the omnicovariate distribution as reference the distributions in each bin have been normalised using the procedure described in section 4 in each case the first bin is centred at t 0 and all bins are of equal width for the two bin case the distribution in bin 1 is shorter tailed than the distribution in bin 2 as the ppc model assumes a constant gp shape parameter across bins the value of ξ ˆ will be an average over the shape for each bin for the three bin case the distribution in bins 2 and 3 has a longer tail than that in bin 1 since there is a larger change in the scale parameter in bins 2 and 3 than in bin 1 see fig 2 as discussed in section 4 the non stationarity in the shape parameter in bins 2 and 3 results in the distribution being longer tailed in these bins the estimated shape parameter over the three bins will be more influenced by the two longer tailed distributions in the lower sectors than the shorter tailed distribution in the higher sector in bin 1 for the cases with four and five bins there is less difference between the shapes of the distributions in each bin since the bins are smaller and the distribution in each bin is more homogeneous examination of the distribution of ξ ˆ showed that the estimates are indeed more positive for three bin than for other cases for higher threshold levels the size of the excursion is reduced since the sample of threshold exceedances is smaller an more homogeneous in practice where smooth variation of the data with covariate is expected it is not possible to define bins within which there is a homogeneous population this means that some bin placement effects are unavoidable however increasing the number of bins means that a piecewise constant covariate model is a better approximation to the true data generating distribution which should improve the performance of the ppc model optimisation of bins widths and locations for directional analysis of extreme conditions is discussed in ewans and jonathan 2008 to investigate the effect of bin placement further additional simulations were conducted with random placement of the first bin edge on 0 360 whilst keeping bin widths constant this procedure effectively eliminated the excursion discussed above but otherwise the characteristics of the results not shown are similar to those shown in fig 11 cases 3 and 4 discussed below utilise random bin placement for this reason it is possible to optimise the number of bins used and the placement of bin edges see e g zanini et al 2020 however this represents a significant step up from the ppc model in terms of complexity and has not been pursued further here 6 3 case 3 data from distribution with non stationary location and scale parameters the corresponding std and rmse in omnicovariate return value estimates for case 3 table 1 using random bin placement as described in section 6 2 are similar to those for case 2 with random bin placement and are therefore not shown here the bias for β 0 5 was found to be somewhat more negative than that for case 2 but of a similar magnitude between 0 and 10 the similarity in performance of ppc models for cases 2 and 3 agrees with results from section 4 the effect of non stationary scale is similar regardless of whether the location parameter is stationary or non stationary 6 4 case 4 data from distribution with non stationary location scale and shape parameters fig 13 shows the bias std and rmse in the 100 year omnicovariate return value estimates for the cases with γ 0 1 and 0 2 the data generating distribution in the benign covariate interval has a longer tail in than elsewhere see fig 3 results for γ 0 1 show a small negative bias of 2 4 for the one bin case and a small positive bias 2 4 for the ppc model with 3 or more bins bias for the two bin case is close to zero std is also relatively stable as a function of the number of bins used since std is larger than bias rmse is also relatively stable there is little difference between stationary and non stationary models in this case results for γ 0 2 show a small negative bias for the stationary model for non stationary models bias increases with both the number of bins and threshold level this behaviour is related to the model misspecification the ppc model estimates a constant shape parameter by maximising predictive likelihood over all bins the shape parameter estimate will therefore be influenced by the long tail for the benign sector resulting in a positive bias overall std is relatively stable with increasing number of bins used due to the large bias effect rmse is lowest for the stationary model and increases with the number of bins used corresponding results for the 1000 year omnicovariate return value are shown in fig 14 now the effect of the long tail in the benign sector is more pronounced see fig 3 results for γ 0 1 are similar to those in fig 13 but with slightly larger biases and stds for γ 0 2 the stationary model displays a large negative bias since it does not account for the effect of the longer tail in the benign sector which has a stronger influence on the 1000 year return value than the 100 year return value bias reduces with increasing number of bins used up to approximately four bins since ppc assumes a constant ξ this reduction in bias can only be explained by compensating optimal choices for bin scale parameters std is slightly lower for the stationary model than the non stationary models but due to the large negative bias in the stationary model rmse is lowest for the non stationary models using four or more bins rmse for ψ 0 9 is higher than the fits using the lower thresholds it is likely that this is because of lack of evidence in the sample of threshold exceedances to justify a large variation in the scale parameter to account for the longer tails in the benign sector fig 15 shows the bias std and rmse in the estimated omnicovariate 100 year return values for the cases with γ 0 1 and 0 2 in these cases the distribution in the benign covariate sector has a shorter tail trends in results are similar for both cases results from the fitted one bin stationary model indicate a negative bias between 2 and 12 depending on threshold level with the highest threshold giving the least biased results as expected bias becomes more negative with increasing number of bins used this effect is the opposite to that observed for the cases with negative γ the estimated shape parameter is lower for the non stationary models due to the influence of the shorter tails in the more benign sectors that do not contribute to the overall return values in contrast the stationary model is not influenced by the distribution in these benign sectors rmse is similar between the stationary and non stationary models and relatively constant as a function of the number of bins overall the performance of both models is poor due in part to model misspecification and in part to difficulty of estimating data generating distributions with positive shape parameters the results for the 1000 year return value not shown are similar but with larger bias and rmse in practice non stationarity in the tail shape can be assessed by examining diagnostic plots in each bin comparing the model to the data this can be assessed in terms of the fit of the model to the tail of the distribution or by plotting empirical and modelled return values a systematic variation in the fit of the model between bins with the model over predicting in some bins and under predicting in other bins can indicate that there is non stationarity in the tail shape in this case the use of more advanced non stationary models discussed in the introduction may be appropriate 7 conclusions this study compared the performance of stationary and non stationary extreme value models in estimating omnicovariate return values in the presence of covariate effects for samples of peaks over threshold the non stationary model considered was a penalised piecewise constant ppc gp model assuming a constant shape parameter but covariate dependence of scale and extreme value threshold the effects of linear trends in the location and scale parameters of the data generating gev model on the shape of the omnicovariate tail distribution were examined for the cases considered linear variation of the location parameter has only a small effect on the tail linear variation in the scale parameter of the data generating model results in the omnicovariate distribution having a longer tail further we examined the performance of a stationary gp fit to non stationary data generating distribution for the cases considered the change in bias due to a linear variation in location or scale was small relative to the bias for the case of a stationary data generating distribution the effect of fitting a non stationary piecewise constant model to data from a stationary data generating distribution was also investigated it was found that when independent gp models are fitted per covariate bin bias and variance of estimated return values increase with the number of bins used when the shape parameter is constrained to be constant across all bins and the values of scale per bin estimated freely it was shown that both bias and variance of estimates stabilise as a function of number of covariate bins for fits using more than 8 covariate bins bias and variance increased significantly with the number of bins used this effect was greatly reduced in the ppc model where the likelihood maximised to estimate the model parameters is penalised for the variance of estimated scale parameters over covariate bins with the roughness penalty estimated for optimal out of sample predictive performance further case studies involved datasets from data generating distributions with sinusoidal parameter variation estimated using a full ppc model for threshold exceedances for the cases considered results suggest that the ppc model performs better than a stationary model in estimating return values given non stationary location parameter in the data generating model and gives some improvement in performance given non stationary scale parameter in the data generating model care must be taken over the choice of the width and placement of covariate bins to ensure that the data is as homogeneous as possible within bin the choice of the number of bins and location of bin edges can influence model performance when the within bin data generating distribution is particularly inhomogeneous in violation of ppc model assumptions case studies with non stationary shape parameter in the data generating model showed mixed results here both stationary and non stationary ppc fitting models were misspecified and hence there was less expectation that the non stationary model would perform better clearly additional case studies need to be considered for which the non stationary model incorporates a non stationary representation for shape parameter should be made for more useful comparison with fits using a stationary gp in summary a non stationary extreme value model can give improved estimates of omnicovariate return values compared with stationary models provided that the characteristics of the data generating model and the model to be estimated are consistent however the relative performance of stationary and non stationary extreme value models in estimating an omnicovariate return value is problem specific either approach works reasonably well when the analysis is performed carefully when all that is needed from the analysis is an estimate of an omnicovariate return value a stationary fitted model may be sufficient however when a set of return values corresponding to multiple different partitions of the covariate domain is required in addition to the omnicovariate return value the non stationary model exploiting suitable covariate representations is likely to provide a more consistent and statistically efficient framework for inference credit authorship contribution statement ed mackay conceptualization methodology software formal analysis writing original draft philip jonathan conceptualization methodology software writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors acknowledge useful discussions with kevin ewans and colleagues at shell and lancaster university uk em was funded under epsrc united kingdom grant ep r007519 1 the ppc software developed during the part eu funded project ecsades ross et al 2019 is freely available from the authors and from https github com ecsades ecsades matlab appendix errors when fitting the gp distribution to gev data this study addresses the relative performance of stationary and non stationary extreme value models in the presence of covariate effects maximum likelihood estimation potentially penalised to ensure optimal parameter smoothness is used as discussed in the main text in conjunction with a gp distribution for exceedances of a high threshold it is instructive in addition to consider the performance of the maximum likelihood estimators for the gp parameters and extreme quantiles in a stationary case moreover in the current work the data generating distribution is the gev it is important also therefore to assess the bias and variance in parameter and quantile estimates for a gp fit to data generated from a gp distribution with a gp fit to data generated from a gev distribution there is a wide range of methods for estimating the parameters of the gp distribution differing in bias and variance characteristics with the performance depending on sample size and the value of the gp shape parameter see e g mackay et al 2011 kang and song 2017 the maximum likelihood ml estimators are asymptotically unbiased and efficient as the sample size tends to infinity the ml estimators achieve the cramer rao lower bound for the variance of an unbiased estimator however the ml estimates do not achieve this asymptotic property for small sample sizes and other methods can produce lower bias and variance a key step in the estimation of the ppc model is the penalisation of the likelihood function for the roughness of the gp scale parameter estimates which makes ml the most suitable computational framework for inference we therefore focus on the properties of ml estimators various methods have been proposed for calculating the ml estimators for the gp distribution e g grimshaw 1993 chaouche and bacro 2006 and the performance depends somewhat on the numerical algorithm used convergence of the algorithm is sometimes problematic and some methods can give results inconsistent with data in the sense that ξ ˆ 0 and max x u ˆ σ ˆ ξ ˆ in the ppc model parameter estimates are forced to be consistent with the data and the shape parameter is constrained to be ξ ˆ 0 5 as described in section 2 2 the asymptotic covariance matrix for the ml estimators of gp parameters is smith 1984 26 var σ ˆ ξ ˆ 1 n 2 σ 2 1 ξ σ 1 ξ σ 1 ξ 1 ξ 2 ξ 1 2 where n is the sample size this provides a lower bound for the variance of unbiased parameter estimates for the stationary model the second order bias in the ml estimators was derived by giles et al 2016 27 n bias σ ˆ σ 3 5 ξ 4 ξ 2 1 3 ξ o n 1 ξ 1 3 28 n bias ξ ˆ 3 4 ξ ξ 2 1 3 ξ o n 1 ξ 1 3 a simulation study was conducted to compare the bias and variance of gp fits to gp and gev data and to the theoretical values given above the aim was to investigate the influence of the threshold level at which the gp distribution is fitted to the gev to make a meaningful comparison the sample size must be consistent between the different threshold levels which requires generating more extreme gev values for fits using higher threshold values the approach taken is summarised as a set shape parameter ξ b set gev threshold non exceedance probability ψ c set number of gp samples n g p d define number of gev observations to generate as n g e v n g p 1 ψ 1 2 where is the floor function e generate n g p samples from gp distribution with u 0 and σ 1 and fit gp distribution to all samples and f generate n g e v samples from gev distribution with μ 0 and σ 1 and fit gp distribution to largest 1 ψ n g e v n g p samples for each value of ξ and ψ 100 000 trials were conducted as the gp distribution is fit to the gev data at different threshold levels the estimated scale parameter must be adjusted to allow consistent comparison a feature of the gp distribution is that if exceedances of threshold u 0 follow a gp distribution with parameters σ u 0 and ξ then for threshold u u 0 the exceedances are gp distributed with same ξ and scale parameter σ u σ u 0 ξ u u 0 see e g coles 2001 the parameter σ σ u ξ u is therefore threshold independent we therefore compare estimates of σ rather than σ note that since u 0 in this example the true value is σ 1 fig 16 shows the results of the simulation study for a sample size of n 50 results for n 200 yields similar results and are not reproduced here in this example the return value x p is defined as the quantile at a non exceedance probability of p 0 999 for the gp data the bias of parameter estimates for fits to gp data agree reasonably well with the theoretical values from 27 and 28 when ξ 0 1 but the theoretical values depart significantly from the simulations when ξ 0 1 due to the influence of singularity in the theoretical expressions when ξ 1 3 for the fits to the gev data the bias is larger than that for the fit to the gp data the bias reduces as the threshold increases and the tail of the gev converges to a gp distribution the std of estimates lower panel for the fit to gp data is slightly above that predicted by the asymptotic result for lower values of ξ std is closer to the asymptotic values this is because the estimated shape parameter is constrained to be greater than 0 5 restricting the range of values that the estimates can take the std of ξ ˆ for the fits to the gev data is slightly lower than that for the fit to the gp data the std for σ ˆ is lower for the fit to the gev data for ξ less than approximately 0 1 and higher than that for the fit to the gp data for larger values of ξ for the estimated return values there is an increase in absolute bias for fits to gev data there is a slight reduction in std for the fits to the gev data except for higher threshold case with ψ 0 9 and negative shape parameter in summary fitting a gp distribution to threshold exceedances from a gev data generating distribution results in a slight increase in the bias of parameter and quantile estimates relative to fitting to gp data with the bias decreasing as the threshold increases std of estimates in fits to gev data is generally slightly lower meaning that rmse in return value estimates is comparable to that for fits to gp data it therefore seems reasonable to use a gev model in the case studies in this work especially considering that the appropriate model for environmental data is not known beforehand 
21832,this article compares the accuracy of return value estimates from stationary and non stationary extreme value models when the data exhibits covariate dependence the non stationary covariate representation used is a penalised piecewise constant ppc model in which the data are partitioned into bins defined by covariates and the extreme value distribution is assumed to be homogeneous within each bin a generalised pareto model is assumed where the scale parameter can vary between bins but is penalised for the variance across bins and the shape parameter is assumed constant over all covariate bins the number and sizes of covariate bins must be defined by the user based on physical considerations numerical simulations are conducted to compare the performance of stationary and non stationary models for various case studies in terms of quality of estimation of the t year return value over the full covariate domain it is shown that a non stationary model can give improved estimates of return values provided that model assumptions are consistent with the data when the data exhibits non stationarity in the generalised pareto tail shape the use of non stationary model assuming a constant shape parameter can produce biases in return values in such cases a stationary model can give a more accurate estimate of return value over the full covariate domain as only the most extreme observations regardless of covariate are used to estimate tail shape in other cases the assumption of a stationary model will ignore key features of the data and be less reliable than a non stationary model for example if a relatively benign covariate interval exhibits a long or heavy tail extreme values from this interval may influence the t year return value for very large t however the sample of peaks over threshold with high threshold used to estimate a stationary model in this case may not include sufficient observations from this interval to estimate the return value adequately keywords covariate extreme generalised pareto metocean significant wave height non stationary 1 introduction accurate estimation of extreme events is important in offshore and coastal engineering under estimation of the magnitude of extreme events can lead to structural failures whilst over estimation can lead to overly conservative and expensive designs and inefficient allocation of limited resources return periods of extreme events are usually estimated by fitting a statistical model to observed or modelled data and extrapolating into the tail of the distribution the accuracy of estimated return values is dependent on numerous factors including a quality of historic data henceforth dataset b length of dataset c characteristics of the actual data generating distribution d misspecification of the statistical model and e method used to estimate the statistical model bias in metocean data obviously leads to bias in estimates of extremes random errors in metocean data lead to positive bias i e a tendency to estimate return values that are higher than the true return values since the distribution of random errors is convolved with the distribution of the variable forristall et al 1996 brooker et al 2004 shorter datasets lead to higher variance in estimates of extremes but can also increase bias since bias in parameter estimators for various distributions can vary with the number of observations similarly the shape of the tail of the distribution affects both the bias and variance of estimates of extreme values with estimates of longer tailed distributions having a higher variance for a given sample size biases in parameter estimates also vary with the shape of the tail see e g de zea bermudez and kotz 2010 kang and song 2017 model misspecification refers to differences between the true characteristics of the data and the data generating model responsible for it and the assumptions made in the statistical model at present the most commonly applied method for estimating extremes of metocean variables is the peaks over threshold pot method see e g coles 2001 jonathan and ewans 2013 the pot method makes the following key assumptions about the data 1 observations are independent and identically distributed iid given covariates and 2 exceedances of a sufficiently high threshold follow a generalised pareto gp distribution the gp distribution describes the asymptotic behaviour of independent threshold exceedances from a max stable data generating distribution as threshold level increases theory suggests that the closeness of the conditional distribution of peaks over threshold to the gp form improves the appropriateness of the gp distribution is therefore based on the threshold being sufficiently high that the asymptotic approximation is valid with too low a threshold leading to increased bias in the estimated extreme values due to the gp distribution not being an appropriate model the choice of threshold is a trade off between increased bias from setting the threshold low and increased variance from setting the threshold high so that there are fewer observations the rate of convergence of threshold exceedances from the data generating distribution to the gp distribution may however be slow that is a very large threshold might be required for the gp form to be considered a reasonable approximation making practical inference difficult for finite samples for this reason a number of pre asymptotic parametric distributional forms or penultimate approximations have been proposed beirlant et al 2012 gomes 2014 the idea being that the data generating distribution is in some sense closer to the penultimate approximation than to the asymptotic distribution for finite threshold however since the gp model is the most widely used at present it has been applied in the present work and the use of penultimate approximations is not pursued further here in addition a large literature on non parametric alternatives for estimation of distributional tails exists see e g hill 1975 dekkers et al 1989 regarding the assumption that observations are iid given covariates metocean variables typically exhibit serial correlation so the assumption of independence is not true if a model is fitted to all observations this is dealt with by declustering the data where only the largest observation in each storm are considered so that storm maxima can be considered approximately independent the criterion for what determines independent storms is usually defined in terms of a minimum separation in time a rigorous treatment of the correlation between successive extreme events can be made by plotting the extremogram davis and mikosch 2009 an analogue of the autocorrelation function for sequences of extreme events although care must be taken to first remove the seasonal signal from the data which introduces a longer range correlation an example of the use of the extremogram to define a declustering time scale was presented by mackay and johanning 2018 which showed that a time scale of around 5 days was sufficient for the datasets considered in that study alternatively declustering times can be defined based on more heuristic arguments about the average time scales for weather systems to pass over a site typically taken to be in the range of 2 5 days ewans and jonathan 2008 discuss a physically motivated approach to declustering time series of significant wave height h s based on the assumption that the peak severities of different storm events given covariates are statistically independent storm events are identified from time series of sea state h s a storm event corresponds to the time interval between the h s up crossing of some threshold level and the subsequent down crossing of the threshold in addition storm intervals separated by less than 24 h are merged the threshold can be defined e g in terms of a covariate dependent quantile of sea state h s the peak value of sea state h s during the storm interval then defines the storm peak h s values of storm peak h s for different storms are taken to be statistically independent the distributions of many metocean variables such as significant or individual wave heights wind speeds and storm surge exhibit dependence on other variables referred to as covariates for example many studies have considered the dependence of wind speeds or wave heights on the direction of origin of the storm and the time of year season fawcett and walshaw 2006 méndez et al 2008 ewans and jonathan 2008 randell et al 2015 jones et al 2016 wave heights and wind speeds are also dependent on large scale climatic indices such as the north atlantic oscillation nao woolf et al 2002 or the el niño southern oscillation enso bromirski et al 2005 moreover most studies tacitly assume that the distribution of metocean variables are stationary in time neglecting the effects of the changing climate which have been observed in some studies reguero et al 2019 cattrell et al 2019 in this study we focus on the effects of periodic covariates such as season and direction and defer consideration of longer term variations in climate to future work specifically we quantify differences in the performance of models which account for the covariate effects and those that do not referred to here as constant or stationary models obviously stationary models cannot produce estimates of seasonal or directional extremes so our interest here is in which model gives the more accurate estimates of return values for the full covariate domain typically referred to as annual omniseasonal or omnidirectional return values we use the term omnicovariate where necessary below for clarity the quality of historical data is not considered here but all the other factors listed above influence the comparison between stationary and non stationary models and therefore need to be considered there has been some debate in the literature about the circumstances in which non stationary models should be applied and whether stationary or non stationary models produce more accurate estimates of omnicovariate return values the motivation for using non stationary models is that their underpinning assumptions better reflect the characteristics of the data and our physical understanding non stationary models assume that the distributions of independent peaks over covariate dependent threshold conditional on covariates tend towards a gp distribution under this assumption now consider the highest threshold value on the covariate domain for which the gp distribution is a reasonable approximation for this threshold value the omnicovariate distribution is a convolution of the gp distributions over the covariate domain and therefore not a gp distribution itself if we now increase the threshold yet further we expect to eliminate the influence of all values of covariate except those contributing to the extreme tail of the omnicovariate distribution and that the resulting distribution of threshold exceedances would be closer to a gp distribution once more it may therefore be expected that a high threshold would be required for stationary models to give a similar level of performance as non stationary models however it is also reasonable to expect that the most information about the shape of the tail of the omnicovariate distribution is contained in the largest observations in applied extreme value analysis there is a maxim that ensuring a good fit to the bulk of the data does not guarantee a good fit to the tail it is therefore reasonable to ask whether modelling less extreme observations in a non stationary model reduces the bias and variance of return values model complexity is another consideration stationary models are simpler to implement and have fewer parameters to estimate whilst the complexity of non stationary models is not an argument against their use on its own practical considerations aside it may be expected that the larger number of parameters that need to be estimated for non stationary models would increase the variance of those estimates the need to estimate more parameters is traded off against two effects firstly due to the larger number of parameters non stationary models offer a more flexible hence potentially more accurate fit to the data secondly non stationary models are typically fitted to a larger proportion of the observations increasing the sample size from this discussion it is apparent that theoretical arguments alone cannot justify the use of a stationary or non stationary model exclusively from the practitioner s perspective the challenge is knowing which type of model gives the most accurate estimates of extreme values in a given situation many of the earlier studies on the use of non stationary models e g carter and challenor 1981 morton et al 1997 anderson et al 2001 compared their performance to stationary models in situations where the true return values were not known in these studies it is not possible to conclude which type of model is more accurate only that results differ jonathan et al 2008 presented a comparison of stationary and non stationary models using simulated data where the observations are drawn from two distinct distributions representing storms from two directions they demonstrate that in the cases they consider the non stationary models give lower bias in estimates of return values mackay et al 2010 argued that these results were not representative of real situations where the distribution of a variable will vary smoothly with direction season or other covariate rather than changing sharply at the boundary of two sectors mackay et al 2010 presented the results of simulations where the distribution of storm peak h s conditioned on season varied continuously through the year piecewise constant models were fitted where the data were divided into a number of discrete bins and independent fits were made in each bin it was shown that the piecewise constant models performed worse in estimating omniseasonal return values than the stationary models with higher bias and variance in all cases considered and with both bias and variance increasing with the number of bins used it was explained that the reason the non stationary models performed worse in these case studies was due to the independent estimates of the gp shape parameter in each bin as the number of bins increases the sample size in each bin decreases and the variance of the parameter estimates increases a high estimate of the gp shape parameter in one bin is not compensated for by a low estimate in another bin and therefore leads to a positive bias in the annual return values jonathan and ewans 2011 argued that the results in mackay et al 2010 were due to a fortuitous choice of extreme value threshold for the stationary model and that there was no way of knowing in practice where the correct threshold should be set jones et al 2016 extended the study of jonathan et al 2008 using more sophisticated covariate representations splines fourier series and gaussian processes and suggested that the performance of stationary models in estimating omnicovariate models is in general more variable than the performance of a non stationary model the purpose of the present study is to extend the results of jonathan et al 2008 and mackay et al 2010 in an attempt to provide further guidance on the relative performance of stationary and non stationary models in realistic situations we extend the results from mackay et al 2010 in two main ways firstly case studies are constructed where the threshold for both the stationary and non stationary models can be varied so that the effect of threshold choice can be examined secondly we consider a penalised piecewise constant ppc non stationary model ross et al 2018 2019 in this model the data are partitioned into bins defined by covariates and the gp scale parameter is allowed to vary between bins but the shape parameter is constant over all bins the likelihood function used to estimate the parameters is penalised for the variance in the scale parameter over all the bins with the roughness penalty selected using cross validation to maximise predictive likelihood more advanced non stationary models than the ppc model have been proposed which have the objective of providing optimally flexible descriptions of the systematic variability of extreme values with covariate e g zanini et al 2020 typically a regression approach underpins these models e g northrop et al 2016 a suitable set of basis functions for the covariate domain is defined and the value of each of the extreme value model parameters on the covariate domain is then defined as a linear combination of basis functions the basis coefficient vector is estimated statistically suitable bases for one dimensional covariate domains include splines and fourier series basis functions with compact support such as b splines are advantageous computationally ppc exploits a piecewise constant basis in one dimension there are numerous variants of spline parameterisations these include p splines penalised b splines eilers and marx 2010 for which squared differences of neighbouring basis coefficients are penalised to increase the smoothness of the representation and adaptive regression splines e g biller 2000 for which locations of spline basis knots are also estimated to optimise model fit useful bases for higher dimensional covariates include thin plate splines e g wood 2003 suitable kernels e g radial basis functions and voronoi tessellations e g bodin and sambridge 2009 bases for higher dimensional covariates can also be formed from tensor products of lower dimensional bases e g raghupathi et al 2016 higher dimensional bases formed from tensor products of penalised b splines admit efficient computation using generalised linear additive models currie et al 2016 the motivation for using the ppc model over more advanced forms of non stationary model is that is represents a good compromise between simplicity robustness and flexibility the ppc model represents a step up in complexity from binning the data and fitting independent models in each bin the non stationary model considered by mackay et al 2010 where the additional complexity of the roughness penalisation makes the model more robust to increasing uncertainties from dividing the data into bins the complexity of the ppc model is determined by the number of bins used rather than the number of covariates it can therefore be used for multidimensional covariate problems without modification making it very flexible as with previous studies the scope of the current study is necessarily limited to a relatively small number of case studies hence the conclusions drawn here may not be applicable universally the results presented apply to the ppc model and similar types of non stationary model however we have also attempted to draw more general conclusions that extend to other types of non stationary model in particular the conclusions about the effects of binning the data and assuming a piecewise constant distribution apply to other types of model that take this approach and the conclusions about the effects of assuming a stationary shape parameter are likely to be applicable to any non stationary model that makes this assumption moreover as discussed further in section 3 since the ppc model only considers the total level of variability between bins the particular choice of patterns of covariate dependence are not restrictive the paper is organised as follows a brief overview of the theory and model assumptions is presented in section 2 the design of the simulation case studies is described in section 3 in section 4 we examine the effect of non stationarity in the data on the shape of the tail of the omnicovariate distribution and the effect this has on quality of estimation from return values from a stationary fitted model the effect of partitioning the data into bins and fitting a piecewise constant non stationary model is considered in section 5 section 6 summarises the results of the simulation studies finally conclusions are given in section 7 2 theory and assumptions 2 1 return values from a non stationary distribution in the present study we consider estimation of the distribution of an arbitrary variable x where x could be interpreted as storm peak h s or another environmental variable showing dependence on covariates it is assumed that storm peaks are sufficiently separated in time that adjacent observations are independent further it is assumed that x follows some arbitrary distribution with parameters dependent on one or more covariates in the current study we consider the influence of a single covariate denoted t which could be interpreted as the time of year season or mean wave direction at the storm peak denote the cumulative distribution function cdf of x conditional on a particular choice of t as p s x x t for simplicity it is assumed that t t 0 360 it is further assumed that the occurrence rate of storm peaks ρ t t is dependent on t where the rate is defined as the number of storms per year per unit covariate the probability that a storm selected at random has associated covariate t is 1 p t t ρ t m where 2 m 0 360 ρ t d t is the expected number of storms per year the unconditional cdf of x for a storm selected at random denoted p r s is obtained by integrating the conditional cdf over the covariate domain weighted by occurrence 3 p r s x x 0 360 p s x x t p t t d t the t year return value x t is then the solution of 4 p r s x x t 1 t m 2 2 penalised piecewise constant ppc model consider a sample d x i i 1 n of n values of peaks over threshold for a random variable x further let t i i 1 n be the corresponding values of a covariate t on some domain t we assume a single covariate but extension to more complex covariate domains is straightforward as explained in ross et al 2018 we make inferences about extreme values of x given t for t t the piecewise constant model uses a particularly simple description of non stationarity with respect to covariates for each observation in the sample the value of covariate t i is used to allocate the observation to one and only one of n b i n covariate intervals or bins c k k 1 n b i n by means of an allocation vector a such that k a i and t c k for each k all observations in the set x i a i k with the same covariate interval c k are assumed to have common extreme value characteristics a non stationary gp model is then estimated using cross validated roughness penalised maximum likelihood estimation for covariate interval c k the extreme value threshold u k 0 is assumed to be a quantile of the empirical distribution of x in that interval with specified non exceedance probability ψ 0 1 with ψ constant across intervals and estimated by counting threshold exceedances are assumed to follow the gp distribution with shape ξ 0 5 and scale σ k 0 with cdf 5 f g p x ξ σ k u k 1 z k where 6 z k 1 ξ x u k σ k 1 ξ ξ 0 exp x u k σ k ξ 0 per covariate interval c k f g p is defined on x u k x k with x k u k σ k ξ when ξ 0 and otherwise the parameters u k σ k and ξ are the threshold scale and shape parameters respectively since estimation of the shape parameter is particularly problematic ξ is assumed constant but unknown across covariate intervals and the reasonableness of the assumption assessed by inspection of diagnostic plots parameters ξ σ k are estimated by maximising the predictive performance of a roughness penalised model optimally regulating the extent to which σ k varies across intervals using a cross validation procedure the sample gp likelihood l under the piecewise stationary model is 7 l k 1 n b i n x i u k i a i k 1 σ k 1 ξ σ k x i u k 1 ξ 1 where l u k σ k and ξ are functions of marginal extreme value threshold non exceedance probability ψ and ξ is constant across the n b i n intervals c k the negative log likelihood penalised for the roughness of σ k across intervals is then 8 ℓ log l λ σ 1 n b i n k 1 n b i n σ k σ 2 where ℓ is a function of both ψ and roughness coefficient λ σ and σ is the mean value of σ k over the bins 9 σ 1 n b i n j 1 n b i n σ j for given ψ and λ σ estimates for ξ and σ k are found by minimising ℓ the minimisation is conducted using a simplex search method lagarias et al 1998 the search is initialised using first guess of ξ ˆ 0 where the caret denotes an estimate of a parameter and the moment estimates of σ in each interval the optimisation is constrained to give ξ ˆ 0 5 and max x i a i k u k ˆ σ k ˆ ξ ˆ when ξ ˆ 0 a random 10 fold cross validation is then used to select the value λ ˆ σ of λ σ and corresponding ξ ˆ σ ˆ k which for each ψ maximises predictive performance in the ppc model if λ σ then the model has only one degree of freedom for σ whereas if λ σ 0 then the fitted model has n b i n degrees of freedom for σ for intermediate values the effective degrees of freedom for σ is at some intermediate value in a typical application the complete ppc modelling procedure is repeated for a number of bootstrap resamples of the original sample to capture sampling uncertainty moreover for each sample the extreme value model is evaluated for multiple thresholds with non exceedance probability ψ drawn at random from the interval i ψ 0 1 on which model performance is deemed reasonable from inspection of diagnostics however in the current study where the model is applied in a large number of monte carlo simulated datasets only the original sample is used moreover as we wish to study the effect of threshold level on the estimates the ppc model is fitted for several values of ψ and the results compared directly the method used to fit the ppc model is relatively simple it is conceivable that other methods such as markov chain monte carlo mcmc could potentially improve results however this would represent a significant step up in terms of complexity as mentioned above the motivation for using the ppc model is for its balance between simplicity and flexibility examples of non stationary models using mcmc can be found in e g hansen et al 2020 zanini et al 2020 once the ppc model has been estimated the omnicovariate distribution is obtained using the discretised form of 3 10 p ˆ r s x x 1 n t k 1 n b i n n k f g p x ξ ˆ σ k ˆ u k ˆ where n k is the number of observations in interval c k and 11 n t k 1 n b i n n k return values can then be estimated using 4 for consistency the stationary model used in this work is a special case of the ppc model with a single covariate bin and no roughness penalisation 2 3 assessment criteria the performance of the stationary and non stationary models are assessed in terms of the bias standard deviation std and root mean square error rmse of estimated model parameters and return values over n realisations of monte carlo simulated datasets let θ ˆ denote an estimator of either a model parameter or return value θ the expected value bias std and rmse of the estimator are defined as 12 e θ ˆ 1 n j 1 n θ ˆ j 13 bias θ ˆ e θ ˆ θ 14 std 2 θ ˆ 1 n j 1 n θ ˆ j e θ ˆ 2 15 rmse 2 θ ˆ 1 n j 1 n θ ˆ j θ 2 bias 2 θ ˆ std 2 θ ˆ where θ ˆ j is the estimate corresponding to the j th monte carlo simulated dataset henceforth referred to as a trial for brevity 3 design of case studies previous simulation studies comparing stationary and non stationary models have generated data from a gp distribution where the parameters depend on covariate values the limitation of this type of study is that the minimum threshold for which the stationary model can be applied is the maximum threshold value over all covariates since below this level the distribution is not defined for all covariate values to overcome this limitation a model is required for the distribution of all storm peak data not just the tails this could be achieved by using a two part model with a parametric distribution for the body of the distribution and a gp model for the tail the problem with this approach is that the choice of distribution for the body is arbitrary and it is difficult to ensure continuity of the density function on the boundary between body and tail in our simulations we have opted to simulate from the generalised extreme value distribution gev rather than the gp distribution avoiding the need for a two part model previous investigations details available from the authors on request with measured data also show that the gev distribution is a reasonable model for storm peak h s the gev is the asymptotic distribution of block maxima of fixed block size e g hourly daily or weekly maxima storm peak data can be considered block maxima in a sense where the block size is related to the method used for identifying storm peaks although the block size is not strictly constant however we are not using the gev to generate data to conduct a block maxima analysis instead we are using the gev to generate data for a non stationary pot analysis using the ppc model a pot analysis can be applied to data generated from any distribution the motivation for using the gev as the data generating distribution in the current study is that it has the convenient property that the tail converges to the gp distribution with the same shape parameter in the sense illustrated below the cdf of the gev can be written as 16 f g e v x exp z where z is defined in the same way as the for the gp distribution in 6 in the tail of the distribution z is small as z 0 we have exp z 1 z and f g e v x μ σ ξ f g p x μ σ ξ that is the gev and gp cdfs converge with common scale and shape parameters and gev location parameter μ equal to the gp threshold u as illustrated in fig 1 for the case μ 0 σ 1 ξ 0 it is well known that there is a relation between block maxima modelled using the gev distribution and threshold exceedances modelled using the gp distribution see e g coles 2001 however the argument above merely relates to the similarity of the functional forms of the gev and gp tails and is not the same as the argument associating the gp distribution for peaks over threshold when the gev is used for block maxima fitting a gp model to a dataset with gev as the data generating model will introduce some bias at lower threshold values due to the mismatch between the fitted model and data generating model see fig 1 the resulting bias and std of parameter and quantile estimates when fitting the gp distribution to gev data is examined in the appendix the bias in parameter and quantile estimates are slightly higher when a gp model is fitted to gev data than when a gp model is fitted to gp data however the std is slightly lower resulting in an rmse that is comparable the use of the gev distribution as the data generating model rather than the gp distribution will therefore not significantly influence the results for the ppc model the likelihood is penalised on the variance of the scale parameter the difference between the estimates of the scale parameters in adjacent bins is not considered explicitly only the total variance over all bins the complexity of the ppc model is therefore determined by the total number of covariate bins only and not the number of covariates used therefore the case studies considered here focus on a single covariate and the results can be expected to apply to cases with multiple covariates we now consider two sets of case studies in the first the gev parameters are assumed to vary linearly with covariate t and in the second the parameters are assumed to vary sinusoidally with t the parameters in the first set of case studies are defined as 17 μ a t 360 a 3 3 18 σ 1 b t 360 b 0 2 19 ξ 0 1 the first set of case studies is designed to illustrate the effect of fitting a stationary extreme value model to data from a non stationarity data generating distribution and is similar to the ppc fit in a specific covariate bin see section 4 the parameters in the second set of case studies are defined as 20 μ α cos 2 π t 360 21 σ 1 β cos 2 π t 360 22 ξ 0 1 γ cos 2 π t 360 where different choices of α β and γ are also considered as the ppc model does not directly account for the difference in parameter estimates between adjacent bins on the covariate domain it is mainly the level of variation between bins that influences model fit and not the pattern of variation the assumption of sinusoidal variation in model parameters is therefore not particularly restrictive however it will be shown in section 6 2 that the level of non stationarity of the data within a bin does influence model fit the second set of case studies is designed to be more representative of a real situation and are used to compare the performance of the stationary and non stationary models the gev parameters for each case are listed in table 1 the first case with α β γ 0 is included to illustrate the effect of increasing the number of bins on the estimated omnicovariate return values in absence of covariate effects and is discussed in section 5 the subsequent cases illustrate the effect of different patterns in the variation of the data generating distribution parameters and are discussed in section 6 for each case simulations are conducted as follows the sample size is fixed at 1440 observations this corresponds to a mean time between storm peaks of 5 days and a dataset length of 20 years if a year is assumed to last 360 days as defined in section 2 the occurrence rate is assumed to be constant with covariate so the values of the covariate t are simulated as uniformly distributed numbers in 0 360 after the values of t have been simulated gev parameters are defined for each storm peak conditional on t and a random value of x is generated the ppc model is fitted to the data using between 1 and 8 or sometimes 12 bins with bin edges spaced evenly over the covariate domain with the first bin centred at t 0 the threshold for the gp model in each bin is defined as the empirical quantile corresponding to a fixed non exceedance probability ψ where the levels are set at ψ 0 6 0 7 0 8 0 9 for the cases where the observations are partitioned into 8 bins this gives approximately n 72 54 36 18 threshold exceedances per bin respectively for each case 10 000 trials were performed the estimation of the optimal penalty via cross validation is the most time consuming step in estimating the ppc model to reduce computation the optimal penalty is estimated for only the first 100 trials subsequent trials use the median penalty from the first 100 trials the estimated optimal penalty showed very little variation over the first 100 trials justifying the use of the median value in the remaining trials examples of simulated datasets for four of the cases see eqs 20 22 and table 1 are shown in fig 2 together with the theoretical quantiles at non exceedance levels of ψ 0 6 0 9 0 99 0 999 and 0 9999 since we have defined the location parameter to be sinusoidally varying about zero there are some observations that are negative which is not representative of some environmental variables such as storm peak wave heights or wind speeds this could be rectified by adding an offset to μ which would have the effect of offsetting all the observations however the choice of offset would be arbitrary so has been left as zero the return values for each case calculated from 3 and 4 are shown in fig 3 in case 1 the location parameter varies with t whilst other parameters remain constant the result is an offset in the return value curves which grows with α the offset in the return values does not change much with return period in case 2 the scale parameter varies with t while other parameters are held constant the resulting distribution shown in fig 2 is pinched in the middle this pattern of variation is less representative of real situations but is included for illustration the resulting effect on return values grows with return period in case 3 both the scale and location parameters vary with t which is more representative of real situations finally in case 4 all the parameters are non stationary when γ 0 2 there is a change in the gradient of the return value curve that occurs at a return period of approximately 100 years for other values of γ the return value varies smoothly with return period in case 4 both the stationary and ppc models are misspecified since the ppc model assumes a constant shape parameter the assumption of a stationary shape parameter is commonly used in oceanographic applications e g davison and smith 1990 anderson et al 2001 it is therefore interesting to assess how well the ppc model performs in this situation the cases with γ 0 2 may be less realistic for metocean variables due to the positive shape parameter in some sectors however they are instructive to include as they illustrate some potentially important effects the performance of the stationary and non stationary models is assessed in terms of bias std and rmse in the 100 year and 1000 year return values as the size of the return values differs between cases we need to compare the relative size of the uncertainties in estimates to achieve this we have normalised the bias std and rmse by the size of the true return values this means that the normalised results are influenced by the arbitrary choice of the mean value of the location parameter μ for a larger mean value of μ the true return values would increase and the normalised bias std and rmse would be reduced however since any normalisation is somewhat arbitrary we have opted to use this convention the choice of normalisation used here does not influence the conclusions of the study in terms of which model performs better the choice of normalisation only influences the relative magnitude of the effects 4 fitting a stationary model to data from a non stationary data generating distribution here we examine the effect of non stationarity in the data generating model on the tail of the estimated omnicovariate distribution using a stationary fitted model so that the effect of non stationarity can be assessed in isolation before considering the effect of partitioning the data by covariate the true data generating model with a linear variation in parameters is described in 17 19 to illustrate the influence of non stationarity on the shape of tail of the distribution we apply a normalisation so that the tail shape can be considered without the influence of varying location and scale parameters suppose we wish to examine the shape of the tail above threshold level u corresponding to non exceedance probability ψ define threshold exceedances as y x u for x u the conditional distribution of threshold exceedances is 23 f y y p y y x u 1 1 p r s x y u 1 ψ the mean m and std s of the conditional distribution are given by 24 m 0 y f y y d y 25 s 2 0 y m 2 f y y d y where f y y d f y y d y is the probability density function of threshold exceedances fig 4 shows the tail distribution 1 f y y against the normalised quantity x u s where u is defined to correspond to a non exceedance probability of ψ 0 7 for various values of a and b from the upper left plot it is evident that for these values of ξ and ψ there is almost no change in the shape of the tail of the distribution for a linear variation in location however for the upper left plot where b 2 when the scale is also non stationary increase in a makes the distribution appear marginally shorter tailed the lower plots show that a non stationary scale parameter has a more significant effect making the distribution longer tailed with increasing b however the effect is reduced slightly when there is also an increase in location parameter a similar trends not shown were observed for other choices of ξ and ψ now we consider how non stationarity affects bias in estimates when fitting a stationary model for each value of a and b a simulation study was conducted as follows the sample size was fixed at n 500 observations for each simulation covariate values were generated as uniform random variables t 0 360 the parameters of the gev conditional on t were defined according to 17 and 18 and a stationary model the ppc model with one covariate bin was fitted at threshold levels corresponding to ψ 0 7 and 0 9 for each value of a and b 10 000 random trials were conducted fig 5 shows the mean of the estimated shape parameter ξ ˆ as a function of a and b for thresholds at ψ 0 7 and 0 9 note that only the mean ξ ˆ can be shown rather than bias since there is no true shape parameter when a b 0 since the data generating distribution is in fact a non stationary gev integrated over t and is not gev itself in the case a b 0 the true shape parameter is ξ 0 1 we observe a negative bias in ξ ˆ due to two effects the known bias in maximum likelihood estimators hosking and wallis 1987 and the fact that we are fitting a gp distribution to gev data see discussion in the appendix the trends in the mean ξ ˆ with a and b are similar to the results indicated in fig 4 when b 0 non stationarity in the location has little influence on the estimated shape parameter but when there is non stationarity in the scale then the estimated shape becomes more negative with increasing a it is also clear that non stationarity in the scale has more influence on the shape that non stationarity in the location fig 6 shows the bias in the estimated omnicovariate return value x p where the return value is defined to be the quantile at a non exceedance probability of 0 9999 corresponding to a return period around 20 times the length of the observations for the threshold at ψ 0 9 the non stationarity has relatively little effect on the bias in the return value since much of the non stationarity is removed by the high threshold and the gp model is a good fit for the tail of the distribution in fact for a less than approximately 1 5 the bias actually reduces with increasing b since the positive bias introduced by the non stationary scale is compensated by the negative bias which results from the parameter estimation method for the lower threshold the bias initially increases with b becomes more negative then decreases again the effect of non stationarity in the location parameter has a smaller effect overall the change in the bias with a and b is relatively small compared to the bias in the case of a stationary distribution at a b 0 5 fitting a non stationary model to data from a stationary data generating distribution if we are confident there is no non stationarity in the data then there is obviously no need to apply a non stationary model it is however instructive to consider the application of a non stationary model in this situation ppc and similar models are designed so that in application to stationary data a large roughness parameter λ σ would be estimated and the variability of estimated σ with covariate t would consequently be small corresponding to an approximately stationary gp fit however it is interesting to study the practical performance of ppc in this setting in particular the effect of choice of number of covariate bins and other characteristics of covariate binning since we know the data generating distribution is stationary any effects observed cannot be due to non stationary in the dataset we consider the simplest case of a fit to data from a constant distribution case 1 of table 1 with α 0 we consider three model types with increasing complexity in the first model independent fits to the data in each covariate bin are performed in the second model the shape parameter is assumed to be constant over all bins but the scale parameter fit is unconstrained i e ppc fit with λ σ 0 the third model is full ppc where the shape parameter is constant over all bins and the scale parameter per bin is chosen to maximise predictive performance note that in the case of a single covariate bin all models are equivalent the bias std and rmse in the 100 year omnicovariate return value x 100 are shown in fig 7 for fits using between 1 and 12 bins the results for the 1000 year return value are similar and are not shown here for the one bin stationary fitted model there is a negative bias in the estimate of x 100 which is a result of previously mentioned bias in the maximum likelihood estimators and fitting a gp model to gev data in the case of independent fits to each bin the bias increases with the number of bins used this effect was reported by mackay et al 2010 and is caused by the increased uncertainty in the shape parameter with a high estimate of ξ in one bin not being compensated for by a low estimate in another the std of estimates also increases with both the threshold non exceedance probability ψ and the number of bins used since sample size per bin reduces with both ψ and the number of bins the rmse for the fits with ψ 0 6 decreases slightly from its value in the 1 bin case to a minimum in the 3 bin case we attribute this to a balancing between the negative bias from parameter estimation and positive bias from increased binning resulting in a slightly lower rmse for all other threshold levels the rmse increases monotonically with the number of bins used for the ppc λ σ 0 case the performance of the fitted model is much more stable as a function of the number of bins used up to 8 bins for more than 8 bins there is a large increase in both the bias and std of the estimates for ppc optimal λ σ model the performance is very similar to the ppc λ σ 0 model up to 8 bins however when using more than 8 bins the performance of the full ppc model is much more stable due to the influence of the roughness penalty on σ for more than 10 bins there is some increase in the bias from the ppc model it is thought that this bias results from lack of convergence of the simple simplex type optimisation algorithm used for maximum likelihood inference nevertheless the bias is still very small compared with the other approaches even for 12 covariate bins the bias and std in parameter estimates from the independent fits per bin model are shown in fig 8 with the corresponding plots for full ppc model with optimal λ σ in fig 9 for the independent fits the bias and std increases with the number of bins used due to the reduced sample size in each bin for the full ppc model the results are again considerably more stable as a function of number of covariate bins there is a small reduction in the bias with increasing number of bins used this is likely due to the increased influence of the σ roughness penalty which acts to optimise the performance of the model the std in the estimates remains fairly constant with the number of bins used in the full ppc model for fits with 11 and 12 bins the std increases when using a threshold at ψ 0 6 but reduces for the threshold at ψ 0 9 again we attribute this effect at least in part to lack of convergence for large numbers of covariate bins of the simplex optimisation algorithm used in ppc we conclude from this study that the full ppc model provides a good representation of stationary data generating distributions with parameters considered at least when the number of covariate bins does not exceed 10 therefore for the studies reported in section 3 we focus on the fits using up to 8 bins we note that more sophisticated optimisation schemes exploiting likelihood slope and curvature characteristics davison 2003 raghupathi et al 2016 are available for more challenging applications 6 fitting a non stationary model to data from a non stationary data generating distribution in this section we consider the performance of stationary and non stationary fitted ppc models for the four cases with sinusoidal parameter variation described in section 3 eqs 20 22 and table 1 samples of which are illustrated in fig 2 for these case studies the true omnicovariate data generating distribution is an integral of gev distributions over the covariate domain and therefore not itself a gev distribution hence it is not possible to assess performance in terms of the parameter estimates from the fitted gp models instead we focus on the estimating omnicovariate return values for which the true values can be calculated from 4 as illustrated in fig 3 we consider the four cases from table 1 in turn 6 1 case 1 data from distribution with non stationary location parameter fig 10 shows the bias std and rmse of the estimated 100 year omnicovariate return value as a function of the number of covariate bins and threshold levels used in the full ppc model results for the 1000 year omnicovariate return value display similar trends and are not shown here the number of observations used for modelling is dependent only on the threshold non exceedance probability and not on the number of bins used however the observations used for fitting change depending on how the data are binned the fitted one bin stationary model shows a negative bias which reduces with increasing threshold level consistent with the results shown in fig 6 for the lower threshold levels the bias becomes slightly more negative as α increases and the amplitude of variation in location parameter grows in contrast at ψ 0 9 the bias and std does not vary much with α the reduction in bias with increasing threshold is due to two effects first as threshold increases there is less covariate variation in sample of threshold exceedances form modelling secondly the gp distribution provides a better fit to the gev distribution in the tail as discussed in theappendix for non stationary fits bias and std reduce initially as a function of increasing number of covariate bins up to 3 bins performance thereafter stabilises with std and rmse remaining approximately constant up to 8 bins the stability in performance of the ppc model with the number of bins used is due to the use of the σ roughness penalty as the number of bins used increases the optimal penalty also increases so that the model does not over fit the trend in bias with increasing number of bins for α 2 and 3 is somewhat more complex than might be anticipated this is due to the effect of the location of bin edges which is discussed further in section 6 2 in general the ppc model fitted using 5 8 bins using a threshold at ψ 0 6 or 0 7 gives the best performance in this case 6 2 case 2 data from distribution with non stationary scale parameter in a similar fashion to section 6 1 bias std and rmse in the 100 year omnicovariate return values for case 2 table 1 are shown in fig 11 for the fitted one bin stationary model bias is negative for β 0 25 but slightly positive for β 0 5 bias becomes more negative as the number of bins increases in general but there is an excursion in the bias for the 3 bin model most pronounced for β 0 5 and ψ 0 6 0 7 despite the increasingly negative bias with the number of bins used in the model the std and rmse remains approximately constant for more than 2 bins due to σ roughness penalisation for β 0 25 the performance of the stationary one bin and non stationary models are similar in terms of rmse for the case with β 0 5 the ppc model with n b i n 4 gives a small improvement in performance over the stationary model the excursion for three bin fits is due to the placement of the bin edges fig 12 shows the true tail distributions in each covariate bin for a threshold level at ψ 0 6 when the data is partitioned into 2 3 4 or 5 bins together with the omnicovariate distribution as reference the distributions in each bin have been normalised using the procedure described in section 4 in each case the first bin is centred at t 0 and all bins are of equal width for the two bin case the distribution in bin 1 is shorter tailed than the distribution in bin 2 as the ppc model assumes a constant gp shape parameter across bins the value of ξ ˆ will be an average over the shape for each bin for the three bin case the distribution in bins 2 and 3 has a longer tail than that in bin 1 since there is a larger change in the scale parameter in bins 2 and 3 than in bin 1 see fig 2 as discussed in section 4 the non stationarity in the shape parameter in bins 2 and 3 results in the distribution being longer tailed in these bins the estimated shape parameter over the three bins will be more influenced by the two longer tailed distributions in the lower sectors than the shorter tailed distribution in the higher sector in bin 1 for the cases with four and five bins there is less difference between the shapes of the distributions in each bin since the bins are smaller and the distribution in each bin is more homogeneous examination of the distribution of ξ ˆ showed that the estimates are indeed more positive for three bin than for other cases for higher threshold levels the size of the excursion is reduced since the sample of threshold exceedances is smaller an more homogeneous in practice where smooth variation of the data with covariate is expected it is not possible to define bins within which there is a homogeneous population this means that some bin placement effects are unavoidable however increasing the number of bins means that a piecewise constant covariate model is a better approximation to the true data generating distribution which should improve the performance of the ppc model optimisation of bins widths and locations for directional analysis of extreme conditions is discussed in ewans and jonathan 2008 to investigate the effect of bin placement further additional simulations were conducted with random placement of the first bin edge on 0 360 whilst keeping bin widths constant this procedure effectively eliminated the excursion discussed above but otherwise the characteristics of the results not shown are similar to those shown in fig 11 cases 3 and 4 discussed below utilise random bin placement for this reason it is possible to optimise the number of bins used and the placement of bin edges see e g zanini et al 2020 however this represents a significant step up from the ppc model in terms of complexity and has not been pursued further here 6 3 case 3 data from distribution with non stationary location and scale parameters the corresponding std and rmse in omnicovariate return value estimates for case 3 table 1 using random bin placement as described in section 6 2 are similar to those for case 2 with random bin placement and are therefore not shown here the bias for β 0 5 was found to be somewhat more negative than that for case 2 but of a similar magnitude between 0 and 10 the similarity in performance of ppc models for cases 2 and 3 agrees with results from section 4 the effect of non stationary scale is similar regardless of whether the location parameter is stationary or non stationary 6 4 case 4 data from distribution with non stationary location scale and shape parameters fig 13 shows the bias std and rmse in the 100 year omnicovariate return value estimates for the cases with γ 0 1 and 0 2 the data generating distribution in the benign covariate interval has a longer tail in than elsewhere see fig 3 results for γ 0 1 show a small negative bias of 2 4 for the one bin case and a small positive bias 2 4 for the ppc model with 3 or more bins bias for the two bin case is close to zero std is also relatively stable as a function of the number of bins used since std is larger than bias rmse is also relatively stable there is little difference between stationary and non stationary models in this case results for γ 0 2 show a small negative bias for the stationary model for non stationary models bias increases with both the number of bins and threshold level this behaviour is related to the model misspecification the ppc model estimates a constant shape parameter by maximising predictive likelihood over all bins the shape parameter estimate will therefore be influenced by the long tail for the benign sector resulting in a positive bias overall std is relatively stable with increasing number of bins used due to the large bias effect rmse is lowest for the stationary model and increases with the number of bins used corresponding results for the 1000 year omnicovariate return value are shown in fig 14 now the effect of the long tail in the benign sector is more pronounced see fig 3 results for γ 0 1 are similar to those in fig 13 but with slightly larger biases and stds for γ 0 2 the stationary model displays a large negative bias since it does not account for the effect of the longer tail in the benign sector which has a stronger influence on the 1000 year return value than the 100 year return value bias reduces with increasing number of bins used up to approximately four bins since ppc assumes a constant ξ this reduction in bias can only be explained by compensating optimal choices for bin scale parameters std is slightly lower for the stationary model than the non stationary models but due to the large negative bias in the stationary model rmse is lowest for the non stationary models using four or more bins rmse for ψ 0 9 is higher than the fits using the lower thresholds it is likely that this is because of lack of evidence in the sample of threshold exceedances to justify a large variation in the scale parameter to account for the longer tails in the benign sector fig 15 shows the bias std and rmse in the estimated omnicovariate 100 year return values for the cases with γ 0 1 and 0 2 in these cases the distribution in the benign covariate sector has a shorter tail trends in results are similar for both cases results from the fitted one bin stationary model indicate a negative bias between 2 and 12 depending on threshold level with the highest threshold giving the least biased results as expected bias becomes more negative with increasing number of bins used this effect is the opposite to that observed for the cases with negative γ the estimated shape parameter is lower for the non stationary models due to the influence of the shorter tails in the more benign sectors that do not contribute to the overall return values in contrast the stationary model is not influenced by the distribution in these benign sectors rmse is similar between the stationary and non stationary models and relatively constant as a function of the number of bins overall the performance of both models is poor due in part to model misspecification and in part to difficulty of estimating data generating distributions with positive shape parameters the results for the 1000 year return value not shown are similar but with larger bias and rmse in practice non stationarity in the tail shape can be assessed by examining diagnostic plots in each bin comparing the model to the data this can be assessed in terms of the fit of the model to the tail of the distribution or by plotting empirical and modelled return values a systematic variation in the fit of the model between bins with the model over predicting in some bins and under predicting in other bins can indicate that there is non stationarity in the tail shape in this case the use of more advanced non stationary models discussed in the introduction may be appropriate 7 conclusions this study compared the performance of stationary and non stationary extreme value models in estimating omnicovariate return values in the presence of covariate effects for samples of peaks over threshold the non stationary model considered was a penalised piecewise constant ppc gp model assuming a constant shape parameter but covariate dependence of scale and extreme value threshold the effects of linear trends in the location and scale parameters of the data generating gev model on the shape of the omnicovariate tail distribution were examined for the cases considered linear variation of the location parameter has only a small effect on the tail linear variation in the scale parameter of the data generating model results in the omnicovariate distribution having a longer tail further we examined the performance of a stationary gp fit to non stationary data generating distribution for the cases considered the change in bias due to a linear variation in location or scale was small relative to the bias for the case of a stationary data generating distribution the effect of fitting a non stationary piecewise constant model to data from a stationary data generating distribution was also investigated it was found that when independent gp models are fitted per covariate bin bias and variance of estimated return values increase with the number of bins used when the shape parameter is constrained to be constant across all bins and the values of scale per bin estimated freely it was shown that both bias and variance of estimates stabilise as a function of number of covariate bins for fits using more than 8 covariate bins bias and variance increased significantly with the number of bins used this effect was greatly reduced in the ppc model where the likelihood maximised to estimate the model parameters is penalised for the variance of estimated scale parameters over covariate bins with the roughness penalty estimated for optimal out of sample predictive performance further case studies involved datasets from data generating distributions with sinusoidal parameter variation estimated using a full ppc model for threshold exceedances for the cases considered results suggest that the ppc model performs better than a stationary model in estimating return values given non stationary location parameter in the data generating model and gives some improvement in performance given non stationary scale parameter in the data generating model care must be taken over the choice of the width and placement of covariate bins to ensure that the data is as homogeneous as possible within bin the choice of the number of bins and location of bin edges can influence model performance when the within bin data generating distribution is particularly inhomogeneous in violation of ppc model assumptions case studies with non stationary shape parameter in the data generating model showed mixed results here both stationary and non stationary ppc fitting models were misspecified and hence there was less expectation that the non stationary model would perform better clearly additional case studies need to be considered for which the non stationary model incorporates a non stationary representation for shape parameter should be made for more useful comparison with fits using a stationary gp in summary a non stationary extreme value model can give improved estimates of omnicovariate return values compared with stationary models provided that the characteristics of the data generating model and the model to be estimated are consistent however the relative performance of stationary and non stationary extreme value models in estimating an omnicovariate return value is problem specific either approach works reasonably well when the analysis is performed carefully when all that is needed from the analysis is an estimate of an omnicovariate return value a stationary fitted model may be sufficient however when a set of return values corresponding to multiple different partitions of the covariate domain is required in addition to the omnicovariate return value the non stationary model exploiting suitable covariate representations is likely to provide a more consistent and statistically efficient framework for inference credit authorship contribution statement ed mackay conceptualization methodology software formal analysis writing original draft philip jonathan conceptualization methodology software writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors acknowledge useful discussions with kevin ewans and colleagues at shell and lancaster university uk em was funded under epsrc united kingdom grant ep r007519 1 the ppc software developed during the part eu funded project ecsades ross et al 2019 is freely available from the authors and from https github com ecsades ecsades matlab appendix errors when fitting the gp distribution to gev data this study addresses the relative performance of stationary and non stationary extreme value models in the presence of covariate effects maximum likelihood estimation potentially penalised to ensure optimal parameter smoothness is used as discussed in the main text in conjunction with a gp distribution for exceedances of a high threshold it is instructive in addition to consider the performance of the maximum likelihood estimators for the gp parameters and extreme quantiles in a stationary case moreover in the current work the data generating distribution is the gev it is important also therefore to assess the bias and variance in parameter and quantile estimates for a gp fit to data generated from a gp distribution with a gp fit to data generated from a gev distribution there is a wide range of methods for estimating the parameters of the gp distribution differing in bias and variance characteristics with the performance depending on sample size and the value of the gp shape parameter see e g mackay et al 2011 kang and song 2017 the maximum likelihood ml estimators are asymptotically unbiased and efficient as the sample size tends to infinity the ml estimators achieve the cramer rao lower bound for the variance of an unbiased estimator however the ml estimates do not achieve this asymptotic property for small sample sizes and other methods can produce lower bias and variance a key step in the estimation of the ppc model is the penalisation of the likelihood function for the roughness of the gp scale parameter estimates which makes ml the most suitable computational framework for inference we therefore focus on the properties of ml estimators various methods have been proposed for calculating the ml estimators for the gp distribution e g grimshaw 1993 chaouche and bacro 2006 and the performance depends somewhat on the numerical algorithm used convergence of the algorithm is sometimes problematic and some methods can give results inconsistent with data in the sense that ξ ˆ 0 and max x u ˆ σ ˆ ξ ˆ in the ppc model parameter estimates are forced to be consistent with the data and the shape parameter is constrained to be ξ ˆ 0 5 as described in section 2 2 the asymptotic covariance matrix for the ml estimators of gp parameters is smith 1984 26 var σ ˆ ξ ˆ 1 n 2 σ 2 1 ξ σ 1 ξ σ 1 ξ 1 ξ 2 ξ 1 2 where n is the sample size this provides a lower bound for the variance of unbiased parameter estimates for the stationary model the second order bias in the ml estimators was derived by giles et al 2016 27 n bias σ ˆ σ 3 5 ξ 4 ξ 2 1 3 ξ o n 1 ξ 1 3 28 n bias ξ ˆ 3 4 ξ ξ 2 1 3 ξ o n 1 ξ 1 3 a simulation study was conducted to compare the bias and variance of gp fits to gp and gev data and to the theoretical values given above the aim was to investigate the influence of the threshold level at which the gp distribution is fitted to the gev to make a meaningful comparison the sample size must be consistent between the different threshold levels which requires generating more extreme gev values for fits using higher threshold values the approach taken is summarised as a set shape parameter ξ b set gev threshold non exceedance probability ψ c set number of gp samples n g p d define number of gev observations to generate as n g e v n g p 1 ψ 1 2 where is the floor function e generate n g p samples from gp distribution with u 0 and σ 1 and fit gp distribution to all samples and f generate n g e v samples from gev distribution with μ 0 and σ 1 and fit gp distribution to largest 1 ψ n g e v n g p samples for each value of ξ and ψ 100 000 trials were conducted as the gp distribution is fit to the gev data at different threshold levels the estimated scale parameter must be adjusted to allow consistent comparison a feature of the gp distribution is that if exceedances of threshold u 0 follow a gp distribution with parameters σ u 0 and ξ then for threshold u u 0 the exceedances are gp distributed with same ξ and scale parameter σ u σ u 0 ξ u u 0 see e g coles 2001 the parameter σ σ u ξ u is therefore threshold independent we therefore compare estimates of σ rather than σ note that since u 0 in this example the true value is σ 1 fig 16 shows the results of the simulation study for a sample size of n 50 results for n 200 yields similar results and are not reproduced here in this example the return value x p is defined as the quantile at a non exceedance probability of p 0 999 for the gp data the bias of parameter estimates for fits to gp data agree reasonably well with the theoretical values from 27 and 28 when ξ 0 1 but the theoretical values depart significantly from the simulations when ξ 0 1 due to the influence of singularity in the theoretical expressions when ξ 1 3 for the fits to the gev data the bias is larger than that for the fit to the gp data the bias reduces as the threshold increases and the tail of the gev converges to a gp distribution the std of estimates lower panel for the fit to gp data is slightly above that predicted by the asymptotic result for lower values of ξ std is closer to the asymptotic values this is because the estimated shape parameter is constrained to be greater than 0 5 restricting the range of values that the estimates can take the std of ξ ˆ for the fits to the gev data is slightly lower than that for the fit to the gp data the std for σ ˆ is lower for the fit to the gev data for ξ less than approximately 0 1 and higher than that for the fit to the gp data for larger values of ξ for the estimated return values there is an increase in absolute bias for fits to gev data there is a slight reduction in std for the fits to the gev data except for higher threshold case with ψ 0 9 and negative shape parameter in summary fitting a gp distribution to threshold exceedances from a gev data generating distribution results in a slight increase in the bias of parameter and quantile estimates relative to fitting to gp data with the bias decreasing as the threshold increases std of estimates in fits to gev data is generally slightly lower meaning that rmse in return value estimates is comparable to that for fits to gp data it therefore seems reasonable to use a gev model in the case studies in this work especially considering that the appropriate model for environmental data is not known beforehand 
21833,design and operationalization for search and rescue sar activities are unstructured and complex multi criteria decision making problems especially for maritime emergency scenario there is a lack of decision support methods based on intelligent algorithms to shorten the response time and to reduce the loss of life and property the primary purpose of this paper is to develop a three stage decision support method to optimize the type and number of resources when making sar schemes so as to formulate emergency response more efficiently and effectively first the main influential factors are identified in stage 1 including the particulars of environmental indices search objects and sar resources next in stage 2 important variables are defined for generating probability distribution maps identifying the search areas and evaluating the objective function in stage 3 two intelligent algorithms the differential evolution de and non dominated sorting genetic algorithm ii nsga ii are used to find appropriate sar schemes and help resources scheduling finally the feasibility and validity of the model are verified by a ship collision example from the simulation of the sar task assignment and decision preference analysis the proposed method can be used for further improvement of sar design and operationalization keywords search and rescue decision support multi objective optimization differential evolution maritime emergency response 1 introduction search and rescue sar in maritime emergencies may involve maritime craft aircrafts wreckage and lost or missing persons such as downed aircrews fishermen etc it comprises the search for and provision of aid to persons who are in danger of loss of life abi zeid et al 2011 the maritime sar center located in the ministry of transport of china and several sub centers manned by local maritime safety administrations are responsible for providing sar service moreover as volunteers thousands of non government aid participate in sar in maritime emergencies every year in china there are many maritime sar cases every month where tens of lives are lost ministry of transport 2019 when a maritime emergency occurs as shown in fig 1 sar operations need to be planned coordinated controlled and commanded in response to emergency from the perspective of search planners the main challenge is how to provide adequate support to make emergency response more efficiently it is a big challenge for decision makers to assign search tasks to multiple professional sub centers and make timely decisions about sar resources scheduling in situations where lives are at risk moreover search areas must be determined appropriately to ensure that each area is covered adequately which depends on contextual factors such as the locations the resources limitation as well as the number of search objects the maritime environment also influences the sar efficiency which means that some factors such as waves wind and currents should be taken into account the number of deaths in an emergency is expected to be minimized while the maritime environment is complex and the number and types of resources are limited so the question of how to dispatch sar resources in an efficient and safe way plays an important role in this research the particulars and performance of sar resources are essential constraints for sar operation such as the safety and capacity constraints in addition the environmental uncertainties make a difference on the sar operation xue et al 2019a all these constraints and uncertainties make the design and operationalization of sar a non linear decision making problem with limited resources therefore further research into designing a search plan and optimization for sar operation including determining search areas sar resources scheduling and decision making algorithms is urgently needed in order to improve the sar capability and to reduce the loss of life in maritime emergencies ai et al 2019 there are many studies for emergency response to maritime accidents especially using decision making method such as nuclear leak gomes et al 2014 and ship flooding crisis jasionowski 2011 abi zeid and frost 2005 presented sarplan to assist the canadian forces in searching for missing aircraft using search theory decision tree was used to analyze the ship accident in turkish search and rescue area erol and başar 2015 in china in order to mitigate the ship bridge collision risk some studies proposed a fuzzy logic based approach for ship bridge collision alert system which can be implemented in the decision support system for improving the shipping handling wu et al 2019 and wang et al 2019 took a resilience modulated risk model to the analysis of the eastern star accident and improved the system capability of emergency response in the search group on arctic marine technology and safety in aalto university the risk of oil spills in winter navigation in the gulf of finland was analyzed banda and osiris 2017 and lu et al 2019 proposed a bayesian network model to assess and prioritize actions to control the risk of operations moreover other studies focused on the development of tools for safety and risk informed decision making for supporting the implementation of safety management systems in the context of maritime traffic banda and goerlandt 2018 note that there are two main difficulties that need to be overcome in the theory and application development first in the design and operationalization of sar the previous literature has widely agreed that this decision is driven by the task characteristics sar resources risks and environment akbari et al 2018 however how to coordinate so many factors in a sar operation and develop the service oriented architecture soa based search planning decision support tools need further research second scientific research into search theory itself and the resulting development are looking for being more closely coupled with their application such as in the field of sar crews training plant and stanton 2016 and how the basic principles of sar are applied to forecasting the drift trajectory and search area wu et al 2018b when solving sar planning problems many search planning decision support systems dss were developed vidan et al 2016 the u s coast guard s search and rescue optimal planning system sarops started in october 2003 and the version 1 0 was deployed in early 2007 kratzke et al 2010 which is the successor to the computer assisted search planning casp system richardson and discenza 1980 as a new generation dss the advanced search planning tool aspt abi zeid et al 2019 is being developed by the canadian coast guard in order to replace the current canadian search and rescue program cansarp guard 1993 and there is a matrix summary of classical search planning tools in the work of frost and stone 2001 furthermore a software tool namely sargis guoxiang and maofeng 2010 was designed to provide supporting databases application modules and graphical user interface for sar in china the state oceanic administration organized the north china sea forecasting center the eastern china sea forecasting center the south china sea forecasting center the national marine environment forecasting center the national marine data and information service and researchers in shandong university of science and technology to develop the national maritime search and rescue support system nmsarss the system integrates multiple modules including drift prediction marine environmental forecasting visualization and collaborative service which greatly reduces the emergency response time and improves the prediction accuracy of maritime sar gao song 2019 the evaluation and selection of sar resources are more dependent on empirical judgment rather than intelligent algorithms at present and it is difficult to improve the reliability and efficiency of decision making therefore a better method should be further developed to overcome this drawback creating a clear multi objective decision support method to optimize the design and operationalization of sar schemes to address these problems we have developed a three stage decision support model to help the decision maker find an optimal sar scheme after maritime emergency happened in this paper the design and operationalization of sar is modeled as a multi objective optimization problem moop by considering the success rate of sar and the total cost which part we investigate in the sar planning problem is to answer how many and which types of sar resources should be implemented in a maritime emergency response and the nsga ii is used to find the pareto solutions set for optimizing the numbers and types of sar resources in order to obtain the compromise solution for final decision making the technique for order of preference by similarity to ideal solution topsis can be applicable to such moop after generating the pareto solutions set hwang and yoon 1981 as a classical multi attribute decision making method the topsis method is widely used to select the compromise solution owing to its concept of choosing the alternative by considering the shortest geometric distance from the positive ideal solution and the longest geometric distance from the negative ideal position wu et al 2018a this paper provides the following contributions 1 a three stage decision support method is developed for sar resources scheduling and tasks assignment 2 two intelligent algorithms were introduced to find the optimal solutions about the required types and number of the sar resources considering success rate and total costs 3 how to balance different objectives when dispatching sar resources under different decision preference is answered during the decision progress the remaining sections of this paper are organized as follows section 2 formulates the sar decision support model including integrating influencing factors describing important variables and designing the algorithm moreover the drift prediction and the method of determining search area are introduced in section 2 2 and 2 3 in section 2 4 the single objective optimization model with the de algorithm is outlined first based on single objective model the 2 4 2 part extends it to a multi objective model considering risk costs and direct costs an illustrative example about a ship collision and corresponding results are provided in section 3 to verify the decision support method a drift prediction and search area identification are obtained based on nmsarss section 4 makes a comparison of sar schemes by search effort analysis and tasks assignment simulation decision preference is also discussed in section 4 section 5 concludes the paper with a summary of the results we also introduce future work directions in this part 2 methodology a sar decision support model 2 1 framework of the proposed model as introduced and discussed in section 1 the emergency response to maritime sar incidents should be time sensitive and sar resources should be organized effectively and intelligently in this section a three stage decision support model is developed to meet these requirements the stage 1 combines parameters of the search object environment and sar resources for search planner to predict drift trajectory systematically which is discussed in section 2 2 next the sar principles and variables are analyzed in stage 2 important variables such as the probability of containment poc probability of detection pod and the probability of survival are described in this part to demonstrate the process of generating a probability distribution map and determining search areas moreover these variables are the basis of formulating objective functions in stage 3 in stage 3 there are two main intelligent algorithms introduced one is the de for a single objective model concerning about the success rate only the other is the nsga ii for a multi objective model taking the objective function of total cost into account to evaluate the loss of property the feasible sar schemes are obtained by de and nsga ii moreover the topsis is applied to select compromise solution from the pareto front at the end of stage 3 the final decision is made based on the comparison of sar schemes and decision preferences the framework of the proposed decision support method is shown in fig 2 2 2 stage 1 integration of influencing factors 2 2 1 the parameters of environment and search objects the sar resources scheduling and search tasks assignment are related to the environmental factors the nmsarss system integrates the influencing factors from its maritime environment the dynamic data of wave height wind current and temperature in a specified location can be provided and visualized quickly to predict the drifting routes as shown in fig 3 if we want to forecast the maritime environment and make a drift prediction for a search object at a given start location longitude 120 68 e latitude 35 83 n we can download these environmental data and see how they will change over time in fig 4 a the scenario with date location and types of search objects have been set for the drifting prediction the setting of wind field coefficient 0 08 0 05 0 02 0 015 0 01 depends on the types of objects such as a low power fishing boat life raft life ring upright life vest upright or life vest lying the maximum forecasting duration can be set 72 h a drift prediction example in 24 h is visualized in fig 4b the yellow arrows represent the sea current and the blue f marks the wind direction each set of particle s positions in time represents a search object s likely trajectory breivik and allen 2008 the monte carlo based stochastic drift simulation s output drift file containing the particles positions at each time step is an input to decision support system for search planning 2 2 2 sar resources the paradigm of decision result is a vector solution which includes the type and number of specific sar resources the paradigm can be expressed as 1 x x 1 x 2 x 3 x m where m indicates the total types of available sar resources and x i is the number of sar resource i participating in the sar operation if x i 0 it means that resource i is not used during a sar operation notation parameters i the serial number of sar resources type x i the number of resource i participating in sar b i the safe sea state levels of resource i c i the direct costs of resource i e u r o h g i the geographic coordinate of resource i v i the speed of resource i k n o t n m i h nc the number of particles in a grid cell nt the total number of particles a the containment area oa the overall area s t r a c k the track spacing m the types of resources n the number of people in water w sweep width 2 3 stage 2 sar principles and variables in order to develop a search plan and optimize the sar resources some basic sar principles and variables need to be defined and analyzed the process of considering these variables is also the preliminary work of generating a search plan such as generating probability distribution map and determining search area moreover the definition of objective functions in stage 3 are based on this stage notation variables dc the containment density dg the overall density r the density ratio t i the search time of resource i temp the average sea surface temperature c z search effort area effectively swept s i the assigned search area of resource i n m i 2 c coverage or search effort density poc the probability of containment pod the probability of detection pos the probability of success p survival t the probability of survival in water as a function of time 2 3 1 description of variables 2 3 1 1 the probability of containment when completing and optimizing a sar plan a prior probability density distribution on the search object location which is also named the probability of containment poc is the first probability map to derive in order to calculate the poc it is necessary to define the search area and transform it into a probability distribution map by dividing it into several cells agbissoh otote et al 2019 as shown in fig 5 the possible locations of a search object in 3 h are regarded as the movement of scatter points an initial search rectangle can be given and the color in the cells visualized using nmsarss indicate the gradient of the poc its value is decreasing from the inside out in this section a process of the poc calculation built in nmsarss is briefly introduced we use the map in fig 5 as an example the density ratio r should be considered to calculate the poc in a grid cell and it is determined by containment density dc and overall density dg the formula involved are as follows 2 d c n c a 3 d g n t o a 4 r d c d g where nc is the number of particles in a grid cell a is the containment area of a grid cell nt is the total number of particles and oa is the overall area such as the rectangle drawn with red line in fig 5 thus the poc value of a selected search area can be calculated according to equation 5 5 p o c c e l l 1 e r 2 3 1 2 the probability of detection the probability of detection pod is a variable associated with the sar units previous studies have shown that the p o d i can be modeled as an exponential function of coverage c frost 1999a in equation 6 6 p o d i 1 e c i where c i w s t r a c k as defined in modern search theory is a measure of how thoroughly an area was swept also equals the ratio of the area effectively swept divided by the physical size of the area where sweeping was done w is the sweep width and it has a strong relationship with the different detectors search environment and types of search objects w is obtained by statistical analysis of large numbers of experimental and real data wu and zhou 2015 s t r a c k is the track spacing and it depends on the search pattern and path the linear search koopman 1957 is a method for finding an element sequentially expanding square parallel glance as shown in fig 6 these search patterns are suitable for a large search area which needs to be covered evenly 2 3 1 3 the probability of survival when evaluating a search plan for emergency the search time of sar resources t i should be taken into account because the duration of a search plan has an impact on the probability of survival p s u r v i v a l t it is necessary to determine the final search area of each sar resource before calculating t i the process about how to assign the overall search area to different sar resources is explained in section 2 3 2 if a search area s i is swept by resource i completely the following equation should be satisfied v i t i s t r a c k x i s i so the search time of each sar unit can be obtained according to equation 7 7 t i s i x i v i s t r a c k moreover mccormack et al 2008 calculated the p s u r v i v a l t as a function of time and ambient temperature for people immersed in water temperature 0 25 c from his work the survival relationship can be expressed as equation 8 8 p s u r v i v a l t exp 0 349 t exp 0 094 t e m p t max t i 2 3 2 design and optimization for sar resources the key part of the proposed decision support method for sar plan is to optimize the type and number of sar resources the decision variables of the model are the x as explained in formula 1 next we need to design and optimize the details of search plan including the final area assignment and sar resources 2 3 2 1 task assignment for sar resources according to the previous description the initial search area has been divided into several grid cells of the same size this section introduces how to combine some of the cells and assign them to sar resources the goal of task assignment for sar resources is to maximize the probability of search success pos which is related to the poc and pod abi zeid and doyon 2003 the following steps are the guidelines to determine the search area of sar resource i step1 choose the first search rectangle based on probability distribution map the grid cell of highest poc is selected as the initial search area and deduce the p o s 1 max p o c c e l l p o d i step2 add one row or one column along each side of the search area to generate a new rectangle and recalculate the p o s 2 of the newly obtained area compare the value of the two search areas if p o s 2 p o s 1 the new rectangle is observed as the current search area otherwise the rectangle from the previous step is retained fig 7 shows the process briefly step3 repeat the above process until the pos no longer increases then the forming process is terminated and the current rectangle is assigned to the resource i as search task in other words the s i in equation 7 is obtained when making the next task assignment the overall area should subtract the s i then follow steps one to three to re determine search area for another resource the search areas do not overlap each other ai et al 2019 notation constraints n i the maximum available number of resource i b the sea state h max t e m p the maximum survival time of people in water at temp c u i the maximum number of people resource i can carry 2 3 2 2 constraint analysis this part discusses the mathematical constraints in the proposed decision support model instead of the constrains in search theory such as path constraints eagle and yee 1990 and simplicity constraints richardson and discenza 1980 according to the real environmental conditions and the sar resources particulars there are some inequality constraints for sar operation guo et al 2019 the following is a summary of four types of constraints in this model first the available number of resources is limited in different organizations for example there may be some resources occupied in other tasks when a new emergency occurs therefore sar resources scheduling cannot be more than the total number of each type of resources as shown in inequality 9 9 0 x i n i x i n i 1 2 m second the scale of sea state is divided into 9 levels as shown in appendix a the higher the level is the severer the maritime environment is in order to make sure that sar resources are working under a safe environment there is a maximal safety level that allows them carrying out sar tasks so when dispatching sar resources the decision makers should also take the sea state into account the maximum allowable sea state should be greater than the current sea state level 10 b i b b i 0 1 9 i 1 2 m third different vessels have different capacity and there is a limited capacity of a vessel so the rescue vessels should be dispatched to accommodate more people than the expected number of people in water as shown in inequality 11 11 n i 1 m u i fourth the search time should be smaller than the people s longest survival time in water at temp c 12 max t i h max t e m p i 1 2 m 2 4 stage 3 algorithm design and model solving as the basic search variables and planning method can be obtained by conducting the above two stages stage 3 manages to design intelligent algorithms to solve the single objective and the multi objective model for optimizing sar resources two objective functions are introduced to assess the sar schemes the p s u c c e s s describes the success rate of sar operation by considering the sar resources p s u r v i v a l t the pod and the poc together another objective function is the total cost of sar operation including the c r i s k and c d i r e c t the costs criteria have been widely used in risk analysis and decision making wu et al 2017 and here it represents the payment used for executing search tasks and the cost of life lost in maritime emergency first de is applied to solve the single objective model second since adding the cost criteria the nsga ii is applied to find the pareto solution for the multi objective model third the topsis method is introduced to find the compromise solution of optimizing the sar resources notation algorithm parameters the objective function cr the crossover probability np the size of population nm the iterations f the scale factor l 1 l 2 l 3 the penalty factors p s u c c e s s the success rate of sar operation c r i s k the economic loss of failing to find search objects c d i r e c t the total direct costs of resources executing search tasks f 1 the value of objective function derived from success rate f 2 the value of objective function derived from total cost 2 4 1 the single objective model according to the decision variables paradigm 1 the scale of solution space is 13 i 1 m max x i min x i 1 2 m the scale of solution space increases exponentially as m increases which means the problem of optimizing number and types of sar resources is an np hard problem as an intelligent optimization method de is reasonable to solve this kind of problem storn and price 1997 the standard algorithm framework is shown in fig 8 more details on de are given below step1 initialize the population according to the paradigm we encoded the decision variables in decimal integers the encoded form of an original individual is designed as x i 0 x i 1 0 x i 2 0 x i m 0 i 1 2 n p where np is the size of population the number in the parenthesis indicates which generation over the whole iterative process the upper boundary of every vector component depends on the available number of the corresponding resources step2 define the objective function f 1 it consists of the fitness and penalty functions given the constraints analysis in section 2 3 2 we applied the penalty function to address the constraint the fitness function is designed as 1 p s u c c e s s so that the lower a fitness value is the better success rate an individual has 14 f 1 1 p s u c c e s s i 1 m l 1 max b b i 0 l 2 max n i 1 m u i 0 l 3 max t i h max t e m p 0 15 p s u c c e s s i 1 m x i p o c i p o d i p s u r v i v a l t i 1 m x i the objective function derived from the success rate is defined and calculated by equations 14 and 15 which means not only searching out the people in water but also making sure they are alive the l 1 l 2 l 3 are three large numbers which represent penalty factors when the corresponding limit is exceeded step 3 mutation operation the individuals in the g th generation are marked as x i g x i 1 g x i 2 g x i m g i 1 2 n p we randomly choose three different individuals x p 1 g x p 2 g x p 3 g and use equation 16 to generate the mutant individual h i g 16 h i g x p 1 g f x p 2 g x p 3 g where f is the scale factor that controls the influence from differential vector recommended as 0 5 price et al 2006 step 4 crossover operation can enrich the diversity of population the specific method is explained as fig 9 and function 17 17 v i j h i j g r a n d 0 1 c r x i j g e l s e where c r 0 1 indicates the crossover probability recommended as 0 2 wan et al 2018 r a n d 0 1 is a random number obeying uniformly distribution between 0 1 step 5 selection and iteration by comparing the fitness values of x i g and v i g we can pick the preferable individual as x i g 1 so the selection process is as follows 18 x i g 1 v i g if f 1 v i g f 1 x i g x i g e l s e given the limits of iteration nm the de operation can proceed to steps 3 again or be stopped 2 4 2 use of an improved multi objective model for decision support the total costs during sar operation consists of the risk and direct costs in this paper risk costs are defined as the sum of economic loss of failing to find the search objects multiplied by the probability of that loss occurring we assume that k is the consequence of one person losing his or her life because sar crews fail to detect him or her for instance this is converted into the value of a statistical life vsl which is recommended as euro 3 4 million per person bickel and friedrich 2013 19 c r i s k 1 p o d n k p o d i 1 m x i p o d i i 1 m x i in general the mean of pod i will be improved when increasing the number of sar resources involved however the direct cost will increase as well because allocating resources excessively results in a redundant sar plan therefore the total costs should be considered as an economic indicator to assess the loss of property for a sar operation solution x x 1 x 2 x 3 x m the total costs are defined as 20 f 2 c r i s k c d i r e c t 1 p o d n k i 1 m x i c i t i where c d i r e c t is the total direct costs of executing a sar operation such as the flight costs of helicopters and shipping costs of vessels it is defined as a sum of direct costs estimation of all sar resources involving in a search plan according to the total costs equation 20 and the single objective model which is related to success rate and we build a multi objective optimization model which allows the decision makers to balance success rate and total costs and choose a satisfying solution to complete the sar operation hence the problem about the decision support for sar operation is treated as a typical multi objective optimization problem moop and its mathematical expression is described as min f 1 x f 2 x at present the multi objective evolutionary algorithms are usually applied to analyze and solve moop in this paper search planners are guided to coordinate the confliction between the objectives and find the pareto solutions nsga ii is one of the most influential and widely used multi objective genetic algorithms nsga ii is an improved algorithm which overcomes the high computational complexity of nsga and the lack of elite strategy srinivas and deb 1994 in this paper it is used to find pareto solutions for sar operation the overall process of nsga ii can be described as fig 10 deb et al 2000 there is no need to discuss the specific details about initialing population and genetic operation in this paper what we concerned about is the comparison process among various solutions nsga ii uses a fast non dominated ordering mechanism to reduce the computational complexity of ordering first define ϕ x f 1 x 2 f 2 x 2 and use x i x j to denote that the solution x i dominates the solution x j then the non dominated sorting principles are summarized below x i x j if and only if one of the following conditions is true 1 x i is a feasible solution and x j is an unfeasible solution 2 both x i and x j are unfeasible solutions and ϕ x i ϕ x j 3 both x i and x j are feasible solutions and f 1 x i f 1 x j f 2 x i f 2 x j those solutions x in the 1st rank generate a pareto front where solutions are non dominated which means that the algorithm cannot find a feasible solution better than x that is to say feasible solutions of higher success rate and less total costs than pareto solutions meanwhile do not exist 2 4 3 topsis method we select the compromise solution from the pareto front using topsis the main steps are explained as follows step 1 normalizing the decision matrix a a i j h 2 21 a i 1 f 1 x i a i 2 f 2 x i m i j a i j i 1 h a i j 2 i 1 2 h j 1 2 where h is the number of pareto solutions so the matrix μ m i j h 2 is the normalized matrix of a a i j h 2 step 2 calculating a weighted normalization decision matrix r r i j h 2 22 r i j m i j w j where w j is the weight value of the j th objective step 3 define the positive and the negative ideal solutions 23 f j min j r i j f j max j r i j i 1 2 h j 1 2 step 4 calculating the distance between the feasible solution and f j f j 24 d i j 1 2 r i j f j 2 d i j 1 2 r i j f j 2 step 5 calculating the proximity of the feasible solution to the ideal solution 25 d i d i d i d i i 1 2 h in the end we sort d i in descending order the solution of the maximum value of d i is the compromise solution in the pareto solution set zavadskas et al 2016 3 an illustrative example 3 1 description of the emergency scenario at 01 44 a m on october 30 2019 a ship collision happened 123 45 e 37 15 n between a fishing ship and a bulk ship whose detailed information is shown in table 1 eleven people were missing the weihai maritime rescue center received the alarm and launched an emergency response at 02 28 a m in order to verify the validity of the proposed decision support method this paper takes the collision event as an illustrative example to discuss how to apply this method in design and operationalization of sar the data of environmental factors such as wind current wave and temperature can be obtained from the cnmsarss as shown in fig 11 the drift prediction of people was simulated after confirming the alarm the blue line which starts from a green point and ends at a red one shows the main drift track within 24 h the grey points show the possible locations of people in water in addition the detailed information of the drift prediction is recorded every hour in table 2 once a maritime emergency happened the number and types of sar resources need to be urgently selected and optimized according to the detailed information of sar resources in table 3 there is a fishing vessel and one cargo vessel available near the place where the collision happened moreover other types of resources no 1 8 are affiliated with maritime safety administration and other professional rescue organization hence a feasible solution can be expressed as x x 1 x 2 x 10 the average sea surface temperature is 14 c according to the reference table in appendix a2 the longest expected survival time is h max 14 6 h based on the environmental indices in the first 6 h and the classification in appendix a1 we can evaluate the current sea state is b 2nd and the numbers of people in water is n 11 in order to compute coverage and pod some typical sar resources sweep width table are given in appendix b guard 1986 the visibility is 3 5 nautical miles in this case the linear search is applied in sar operation and the track spacing s t r a c k is 0 1 nautical miles the parameters applied in this case associated with de are depicted in table 4 the following section applied the proposed decision support method in search planning to find the optimal scheme of sar resources 3 2 results for decision support considering the expected survival time and the limited sar resources the search planner first designs and optimizes the plan from 01 44 to 07 44 as shown in fig 12 the search work is divided into three sub phases in three even intervals and the corresponding probability distribution maps are given then the search planners can begin to design and optimize the types and number of sar resources in this stage they are going to decide which kind of and how many resources should be assigned to the search area in order to maximize the success rate and minimize the total cost from the description of the emergency scenario above the sar resources scheduling schemes including the types and number solved by de can be obtained in three different time intervals respectively other results solved by genetic algorithm ga and particle swarm optimization pso are treated as a comparative experiment for the single objective model for every time interval we run the three algorithms ten times respectively under the same parameters settings as depicted in table 4 and then save the best evolutionary curves data of every algorithm the best evolutionary curve records the process of obtaining the relative minimum f 1 of 10 runs next three kinds of best evolutionary curves corresponding to the three algorithms are shown in fig 13 a b c under different time intervals with the evolutionary generations as abscissa and f 1 in equation 14 as the ordinate the number of evolutionary generations indicates the iteration in the process of running algorithms for example it can be seen from fig 13a that de can obtain the relative minimum f 1 0 192 at the 73rd generation the detailed results under different time intervals including the chromosome with the relative minimum f 1 and the corresponding sar schemes are shown in table 5 moreover table 5 shows the compromise solutions solved by nsga ii and topsis for the multi objective model in different intervals in different time intervals the search planer decides the sar schemes according to the environment and available resources a smaller f 1 indicates a higher success rate which means the corresponding search plan is better as can be seen from table 5 when discussing the single objective model the p s u c c e s s of sar schemes obtained by de and pso are significantly greater than that obtained by ga but in 05 44 07 44 pso cannot converge to find the optimal solution in a same time interval the sar schemes obtained by de use fewer resources to reach the same p s u c c e s s compared with schemes obtained by pso it can be seen from fig 13a b c that the objective value curves based on the de converge to an optimal solution at a fast speed that is because the de uses a simple mutation operation and one to one competitive survival strategy to reduce the complexity of a genetic operation in summary de for this decision support model is robust and has strong global convergence ability another significant performance variable is the probability of survival as part of the objective function as time goes by the probability of survival declined resulting in decrease of the p s u c c e s s it warns that the search plan should be designed and operated as soon as possible especially during the most likely survival time for a multi objective model we generate the pareto front using nsga ii and find the compromise solution using topsis in which the weights of both objectives are equal fig 13d e f show the results of multi objective model these solutions in the pareto front means that the algorithm cannot figure out which search plan is better the weights of the two objectives are set at 0 5 after generating the positive and negative solution of topsis we can find the compromise solution marked with data tips taking the solution from fig 13e as an example the results with more information about the relationship between the success rate and the total cost are shown in fig 14 from fig 14 it could be concluded that the success rate is improved when the decision maker increases the cost of allocating sar resources the growth process of the success rate can be divided into three phases based on the gradient change the decision can be made more accurately and easily when the search planner notices that the success rate improved greatly from point a to b however the success rate increases inefficiently in phase i and more costs would be required in phase iii if they want to obtain a same improvement as in phase ⅱ that is why a rational decision maker will prefer to choose point b as the final sar schemes when balancing two objectives which is also the compromise solution obtained by topsis in 03 44 05 44 4 discussion the analysis and reasoning surrounding the results is covered in this section we also make a comparison of search schemes from two aspects including search effort and search task assignment moreover the selection of the weights of the objectives to reflect the decision preference is discussed by analyzing some decision makers requirements 4 1 comparison of sar schemes an important thing which should be noted is that the search task assignment will influence the search efficiency every ship or aircraft has its own search time and search area in order to evaluate the search efficiency of each ship search effort is introduced in this part the search effort is defined as the distance traversed within the area of interest it maybe equivalently defined as the amount of time spent in the area of interest times the average speed of broom frost 1999b 26 z i 1 m x i w i v i t i if multiple vessels and aircrafts are used simultaneously then the effort should be multiplied by the number accordingly to demonstrate the decision support method of optimizing sar resources scheduling schemes we further make a comparison of search effort between the scheme x a obtained by de and the compromise scheme x b based on nsga ii and topsis in 05 44 07 44 as shown in table 6 according to table 6 after adding the total costs as another objective the search time of x b is longer than that of x a obviously the number of beihai rescue 116 no 3 and offshore patrol vessel no 6 become less or zero because of its expensive direct cost which leads the search effort of the offshore patrol vessel to decline and the task assignment for it to decrease as a result the other ships search time for people in water becomes longer it can be seen from table 6 that fishing ship no 9 cannot reach the search area on time and execute the sar task in scheme x b that is because the search area has been assigned to other available ships which are faster and closer to the search object the simulations of sar schemes x a and x b are visualized in fig 15 the overall search area is fully covered and divided as several search tasks for sar resources the number and types of resources have been included in the sar schemes obtained from the proposed model in addition our results confirm that the vessels near the accident are the useful supplements for sar because they may contribute more effectively than professional rescue units in a maritime accident in this case the sar effort of cargo vessel no 10 is more than that of offshore patrol vessel no 6 in scheme x b though the compromise scheme x b makes a concession because of the total costs it also adds the huaying ambulance boat no 4 and sar lifeboat no 7 which have a lower direct cost to make sure to finish the search work in time compared to the scheme x a of the single objective model 4 2 decision preference different decision makers may have different preferences to the two objectives xue et al 2019b thus we next set out to use different weights for the success rate and total costs in a pareto solution set the objective function values of compromise scheme in 03 44 05 44 under different weights are shown in table 7 if the decision makers are preferred to obtain a higher success rate they can set the weight of the total costs objective lower in contrast if the decision makers are cost sensitive they tend to choose a higher weight of total costs however we found that the results from nsga ii are not very sensitive to the weight changing from 0 1 to 0 9 within the weight interval 0 1 0 4 and 0 6 0 9 the compromise solutions of a pareto solution set do not change and the corresponding total costs do not increase even though the decision maker has a risk seeking tendency these data are consistent with the notion that the compromise solution is relatively stable which makes it easier for the decision maker to select the final sar scheme during an actual decision making process for sar in maritime emergency decision makers must artificially adjust and optimize the scheme to select vessels and aircrafts as soon as possible consider a scenario in which decision makers have some specific requirements such as 1 if three beihai rescue 116 must participate in the sar operation then the number encoding of x 3 is set to 3 when initializing the population and 2 if the sar task need coordination of two kinds of vessels then the encoding of them are linked to satisfy this requirement the proposed decision support method in this paper has enough flexible space to be applied in practice when other similar situations occur the decision support model and the corresponding solving algorithm allow users to set specific encoding and adjust the sar schemes based on individual preferences 5 conclusions the main contribution of this paper is to propose a decision support method to help design and operationalize the sar plan in a maritime emergency based on intelligent algorithms and topsis specifically the three stage framework including the factors integration identification of variables and model solving is described step by step in order to develop an intelligent algorithmic based decision support method for sar we investigate two important aspects 1 how to organize and optimize the number and types of sar resources to execute the search task 2 how to balance different objectives when allocating sar resources under different decision preferences the process of generating probability distribution maps and determining the search areas are also discussed before investigation of the sar resources optimization by introducing the poc pod probability of survival success rate and costs a single objective model and a multi objective model are both designed and discussed in this paper from the comparison of search efforts and simulations of search task assignments the proposed method can help to make the final search plan quickly and effectively the proposed decision support method can be applied to the maritime sar emergency response however one important future work of sar research may focus on extending our decision model to consider sar cooperation behavior defining uncertain environmental or human factors and enabling multi decision makers to work together in the future an agent based simulation framework could be developed to realize the intelligent interaction between resources and decision makers moreover this method should be applied to more maritime emergencies for further validation in the future credit authorship contribution statement weitao xiong conceptualization methodology software data curation visualization investigation validation writing original draft writing review editing p h a j m van gelder supervision methodology writing review editing kewei yang data curation software declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the work was supported by the national key r d program of china grant no 2017yfc1405005 national science foundation of china grant no 71901215 china scholarships council grant no 201803170203 appendix a table a1 classification of sea state table a1 levels the wave height the corresponding wind level wind speed m s 0 0 0 0 0 0 2 1 h 1 3 0 10 1 2 0 3 3 3 2 0 10 h 1 3 0 50 2 4 1 6 7 9 3 0 50 h 1 3 1 25 4 5 5 5 10 7 4 1 25 h 1 3 2 50 5 7 8 0 17 1 5 2 50 h 1 3 4 00 7 8 13 9 20 7 6 4 00 h 1 3 6 00 8 9 17 2 24 4 7 6 00 h 1 3 9 00 9 10 20 8 28 4 8 9 00 h 1 3 14 00 10 11 24 5 32 6 9 14 00 h 1 3 12 32 7 note national standard of the people s republic of china gbj 64 1 85 64 2 85 table a2 expected survival time in cold water table a2 water temperature exhaustion or unconsciousness in expected survival time 70 80 f 21 27 c 3 12 h 3 h indefinitely 60 70 f 16 21 c 2 7 h 2 40 h 50 60 f 10 16 c 1 2 h 1 6 h 40 50 f 4 10 c 30 60 min 1 3 h 32 5 40 f 0 4 c 15 30 min 30 90 min 32 f 0 c under 15 min under 15 45 min note us sar task force available http ussartf org cold water survival htm appendix b table b1 b2 and b3 summarize the sweep width nautical miles of sar resources table b1 vessels sweep width table b1 the search object visibility nmi 3 5 10 15 20 people in water 0 4 0 5 0 6 0 7 0 7 table b2 sweep width helicopters for altitudes 300 feet table b2 the search object visibility nmi 1 3 5 people in water 0 1 0 1 0 1 table b3 sweep width fixed wings aircrafts for altitudes 300 feet table b3 the search object visibility nmi 1 3 5 people in water 0 1 0 1 0 1 according to equations 9 and 10 the coverage and pod of sar resources can be calculated as shown below this is a necessary step to compute the success rate of whole sar operation table b4 the coverage and pod of sar sources table b4 sar resources coverage pod vessels 4 0 98 helicopters 1 0 63 fixed wings aircrafts 1 0 63 appendix c table c1 list of abbreviation table c1 abbreviation full name aspt advanced search planning tool cansarp canadian search and rescue program casp computer assisted search planning de differential evolution dss decision support systems ga genetic algorithm moop multi objective optimization problem nmsarss national maritime search and rescue support system nsga ii non dominated sorting genetic algorithm pso particle swarm optimization sar search and rescue sarops search and rescue optimal planning system soa service oriented architecture topsis technique for order preference by similarity to an ideal solution 
21833,design and operationalization for search and rescue sar activities are unstructured and complex multi criteria decision making problems especially for maritime emergency scenario there is a lack of decision support methods based on intelligent algorithms to shorten the response time and to reduce the loss of life and property the primary purpose of this paper is to develop a three stage decision support method to optimize the type and number of resources when making sar schemes so as to formulate emergency response more efficiently and effectively first the main influential factors are identified in stage 1 including the particulars of environmental indices search objects and sar resources next in stage 2 important variables are defined for generating probability distribution maps identifying the search areas and evaluating the objective function in stage 3 two intelligent algorithms the differential evolution de and non dominated sorting genetic algorithm ii nsga ii are used to find appropriate sar schemes and help resources scheduling finally the feasibility and validity of the model are verified by a ship collision example from the simulation of the sar task assignment and decision preference analysis the proposed method can be used for further improvement of sar design and operationalization keywords search and rescue decision support multi objective optimization differential evolution maritime emergency response 1 introduction search and rescue sar in maritime emergencies may involve maritime craft aircrafts wreckage and lost or missing persons such as downed aircrews fishermen etc it comprises the search for and provision of aid to persons who are in danger of loss of life abi zeid et al 2011 the maritime sar center located in the ministry of transport of china and several sub centers manned by local maritime safety administrations are responsible for providing sar service moreover as volunteers thousands of non government aid participate in sar in maritime emergencies every year in china there are many maritime sar cases every month where tens of lives are lost ministry of transport 2019 when a maritime emergency occurs as shown in fig 1 sar operations need to be planned coordinated controlled and commanded in response to emergency from the perspective of search planners the main challenge is how to provide adequate support to make emergency response more efficiently it is a big challenge for decision makers to assign search tasks to multiple professional sub centers and make timely decisions about sar resources scheduling in situations where lives are at risk moreover search areas must be determined appropriately to ensure that each area is covered adequately which depends on contextual factors such as the locations the resources limitation as well as the number of search objects the maritime environment also influences the sar efficiency which means that some factors such as waves wind and currents should be taken into account the number of deaths in an emergency is expected to be minimized while the maritime environment is complex and the number and types of resources are limited so the question of how to dispatch sar resources in an efficient and safe way plays an important role in this research the particulars and performance of sar resources are essential constraints for sar operation such as the safety and capacity constraints in addition the environmental uncertainties make a difference on the sar operation xue et al 2019a all these constraints and uncertainties make the design and operationalization of sar a non linear decision making problem with limited resources therefore further research into designing a search plan and optimization for sar operation including determining search areas sar resources scheduling and decision making algorithms is urgently needed in order to improve the sar capability and to reduce the loss of life in maritime emergencies ai et al 2019 there are many studies for emergency response to maritime accidents especially using decision making method such as nuclear leak gomes et al 2014 and ship flooding crisis jasionowski 2011 abi zeid and frost 2005 presented sarplan to assist the canadian forces in searching for missing aircraft using search theory decision tree was used to analyze the ship accident in turkish search and rescue area erol and başar 2015 in china in order to mitigate the ship bridge collision risk some studies proposed a fuzzy logic based approach for ship bridge collision alert system which can be implemented in the decision support system for improving the shipping handling wu et al 2019 and wang et al 2019 took a resilience modulated risk model to the analysis of the eastern star accident and improved the system capability of emergency response in the search group on arctic marine technology and safety in aalto university the risk of oil spills in winter navigation in the gulf of finland was analyzed banda and osiris 2017 and lu et al 2019 proposed a bayesian network model to assess and prioritize actions to control the risk of operations moreover other studies focused on the development of tools for safety and risk informed decision making for supporting the implementation of safety management systems in the context of maritime traffic banda and goerlandt 2018 note that there are two main difficulties that need to be overcome in the theory and application development first in the design and operationalization of sar the previous literature has widely agreed that this decision is driven by the task characteristics sar resources risks and environment akbari et al 2018 however how to coordinate so many factors in a sar operation and develop the service oriented architecture soa based search planning decision support tools need further research second scientific research into search theory itself and the resulting development are looking for being more closely coupled with their application such as in the field of sar crews training plant and stanton 2016 and how the basic principles of sar are applied to forecasting the drift trajectory and search area wu et al 2018b when solving sar planning problems many search planning decision support systems dss were developed vidan et al 2016 the u s coast guard s search and rescue optimal planning system sarops started in october 2003 and the version 1 0 was deployed in early 2007 kratzke et al 2010 which is the successor to the computer assisted search planning casp system richardson and discenza 1980 as a new generation dss the advanced search planning tool aspt abi zeid et al 2019 is being developed by the canadian coast guard in order to replace the current canadian search and rescue program cansarp guard 1993 and there is a matrix summary of classical search planning tools in the work of frost and stone 2001 furthermore a software tool namely sargis guoxiang and maofeng 2010 was designed to provide supporting databases application modules and graphical user interface for sar in china the state oceanic administration organized the north china sea forecasting center the eastern china sea forecasting center the south china sea forecasting center the national marine environment forecasting center the national marine data and information service and researchers in shandong university of science and technology to develop the national maritime search and rescue support system nmsarss the system integrates multiple modules including drift prediction marine environmental forecasting visualization and collaborative service which greatly reduces the emergency response time and improves the prediction accuracy of maritime sar gao song 2019 the evaluation and selection of sar resources are more dependent on empirical judgment rather than intelligent algorithms at present and it is difficult to improve the reliability and efficiency of decision making therefore a better method should be further developed to overcome this drawback creating a clear multi objective decision support method to optimize the design and operationalization of sar schemes to address these problems we have developed a three stage decision support model to help the decision maker find an optimal sar scheme after maritime emergency happened in this paper the design and operationalization of sar is modeled as a multi objective optimization problem moop by considering the success rate of sar and the total cost which part we investigate in the sar planning problem is to answer how many and which types of sar resources should be implemented in a maritime emergency response and the nsga ii is used to find the pareto solutions set for optimizing the numbers and types of sar resources in order to obtain the compromise solution for final decision making the technique for order of preference by similarity to ideal solution topsis can be applicable to such moop after generating the pareto solutions set hwang and yoon 1981 as a classical multi attribute decision making method the topsis method is widely used to select the compromise solution owing to its concept of choosing the alternative by considering the shortest geometric distance from the positive ideal solution and the longest geometric distance from the negative ideal position wu et al 2018a this paper provides the following contributions 1 a three stage decision support method is developed for sar resources scheduling and tasks assignment 2 two intelligent algorithms were introduced to find the optimal solutions about the required types and number of the sar resources considering success rate and total costs 3 how to balance different objectives when dispatching sar resources under different decision preference is answered during the decision progress the remaining sections of this paper are organized as follows section 2 formulates the sar decision support model including integrating influencing factors describing important variables and designing the algorithm moreover the drift prediction and the method of determining search area are introduced in section 2 2 and 2 3 in section 2 4 the single objective optimization model with the de algorithm is outlined first based on single objective model the 2 4 2 part extends it to a multi objective model considering risk costs and direct costs an illustrative example about a ship collision and corresponding results are provided in section 3 to verify the decision support method a drift prediction and search area identification are obtained based on nmsarss section 4 makes a comparison of sar schemes by search effort analysis and tasks assignment simulation decision preference is also discussed in section 4 section 5 concludes the paper with a summary of the results we also introduce future work directions in this part 2 methodology a sar decision support model 2 1 framework of the proposed model as introduced and discussed in section 1 the emergency response to maritime sar incidents should be time sensitive and sar resources should be organized effectively and intelligently in this section a three stage decision support model is developed to meet these requirements the stage 1 combines parameters of the search object environment and sar resources for search planner to predict drift trajectory systematically which is discussed in section 2 2 next the sar principles and variables are analyzed in stage 2 important variables such as the probability of containment poc probability of detection pod and the probability of survival are described in this part to demonstrate the process of generating a probability distribution map and determining search areas moreover these variables are the basis of formulating objective functions in stage 3 in stage 3 there are two main intelligent algorithms introduced one is the de for a single objective model concerning about the success rate only the other is the nsga ii for a multi objective model taking the objective function of total cost into account to evaluate the loss of property the feasible sar schemes are obtained by de and nsga ii moreover the topsis is applied to select compromise solution from the pareto front at the end of stage 3 the final decision is made based on the comparison of sar schemes and decision preferences the framework of the proposed decision support method is shown in fig 2 2 2 stage 1 integration of influencing factors 2 2 1 the parameters of environment and search objects the sar resources scheduling and search tasks assignment are related to the environmental factors the nmsarss system integrates the influencing factors from its maritime environment the dynamic data of wave height wind current and temperature in a specified location can be provided and visualized quickly to predict the drifting routes as shown in fig 3 if we want to forecast the maritime environment and make a drift prediction for a search object at a given start location longitude 120 68 e latitude 35 83 n we can download these environmental data and see how they will change over time in fig 4 a the scenario with date location and types of search objects have been set for the drifting prediction the setting of wind field coefficient 0 08 0 05 0 02 0 015 0 01 depends on the types of objects such as a low power fishing boat life raft life ring upright life vest upright or life vest lying the maximum forecasting duration can be set 72 h a drift prediction example in 24 h is visualized in fig 4b the yellow arrows represent the sea current and the blue f marks the wind direction each set of particle s positions in time represents a search object s likely trajectory breivik and allen 2008 the monte carlo based stochastic drift simulation s output drift file containing the particles positions at each time step is an input to decision support system for search planning 2 2 2 sar resources the paradigm of decision result is a vector solution which includes the type and number of specific sar resources the paradigm can be expressed as 1 x x 1 x 2 x 3 x m where m indicates the total types of available sar resources and x i is the number of sar resource i participating in the sar operation if x i 0 it means that resource i is not used during a sar operation notation parameters i the serial number of sar resources type x i the number of resource i participating in sar b i the safe sea state levels of resource i c i the direct costs of resource i e u r o h g i the geographic coordinate of resource i v i the speed of resource i k n o t n m i h nc the number of particles in a grid cell nt the total number of particles a the containment area oa the overall area s t r a c k the track spacing m the types of resources n the number of people in water w sweep width 2 3 stage 2 sar principles and variables in order to develop a search plan and optimize the sar resources some basic sar principles and variables need to be defined and analyzed the process of considering these variables is also the preliminary work of generating a search plan such as generating probability distribution map and determining search area moreover the definition of objective functions in stage 3 are based on this stage notation variables dc the containment density dg the overall density r the density ratio t i the search time of resource i temp the average sea surface temperature c z search effort area effectively swept s i the assigned search area of resource i n m i 2 c coverage or search effort density poc the probability of containment pod the probability of detection pos the probability of success p survival t the probability of survival in water as a function of time 2 3 1 description of variables 2 3 1 1 the probability of containment when completing and optimizing a sar plan a prior probability density distribution on the search object location which is also named the probability of containment poc is the first probability map to derive in order to calculate the poc it is necessary to define the search area and transform it into a probability distribution map by dividing it into several cells agbissoh otote et al 2019 as shown in fig 5 the possible locations of a search object in 3 h are regarded as the movement of scatter points an initial search rectangle can be given and the color in the cells visualized using nmsarss indicate the gradient of the poc its value is decreasing from the inside out in this section a process of the poc calculation built in nmsarss is briefly introduced we use the map in fig 5 as an example the density ratio r should be considered to calculate the poc in a grid cell and it is determined by containment density dc and overall density dg the formula involved are as follows 2 d c n c a 3 d g n t o a 4 r d c d g where nc is the number of particles in a grid cell a is the containment area of a grid cell nt is the total number of particles and oa is the overall area such as the rectangle drawn with red line in fig 5 thus the poc value of a selected search area can be calculated according to equation 5 5 p o c c e l l 1 e r 2 3 1 2 the probability of detection the probability of detection pod is a variable associated with the sar units previous studies have shown that the p o d i can be modeled as an exponential function of coverage c frost 1999a in equation 6 6 p o d i 1 e c i where c i w s t r a c k as defined in modern search theory is a measure of how thoroughly an area was swept also equals the ratio of the area effectively swept divided by the physical size of the area where sweeping was done w is the sweep width and it has a strong relationship with the different detectors search environment and types of search objects w is obtained by statistical analysis of large numbers of experimental and real data wu and zhou 2015 s t r a c k is the track spacing and it depends on the search pattern and path the linear search koopman 1957 is a method for finding an element sequentially expanding square parallel glance as shown in fig 6 these search patterns are suitable for a large search area which needs to be covered evenly 2 3 1 3 the probability of survival when evaluating a search plan for emergency the search time of sar resources t i should be taken into account because the duration of a search plan has an impact on the probability of survival p s u r v i v a l t it is necessary to determine the final search area of each sar resource before calculating t i the process about how to assign the overall search area to different sar resources is explained in section 2 3 2 if a search area s i is swept by resource i completely the following equation should be satisfied v i t i s t r a c k x i s i so the search time of each sar unit can be obtained according to equation 7 7 t i s i x i v i s t r a c k moreover mccormack et al 2008 calculated the p s u r v i v a l t as a function of time and ambient temperature for people immersed in water temperature 0 25 c from his work the survival relationship can be expressed as equation 8 8 p s u r v i v a l t exp 0 349 t exp 0 094 t e m p t max t i 2 3 2 design and optimization for sar resources the key part of the proposed decision support method for sar plan is to optimize the type and number of sar resources the decision variables of the model are the x as explained in formula 1 next we need to design and optimize the details of search plan including the final area assignment and sar resources 2 3 2 1 task assignment for sar resources according to the previous description the initial search area has been divided into several grid cells of the same size this section introduces how to combine some of the cells and assign them to sar resources the goal of task assignment for sar resources is to maximize the probability of search success pos which is related to the poc and pod abi zeid and doyon 2003 the following steps are the guidelines to determine the search area of sar resource i step1 choose the first search rectangle based on probability distribution map the grid cell of highest poc is selected as the initial search area and deduce the p o s 1 max p o c c e l l p o d i step2 add one row or one column along each side of the search area to generate a new rectangle and recalculate the p o s 2 of the newly obtained area compare the value of the two search areas if p o s 2 p o s 1 the new rectangle is observed as the current search area otherwise the rectangle from the previous step is retained fig 7 shows the process briefly step3 repeat the above process until the pos no longer increases then the forming process is terminated and the current rectangle is assigned to the resource i as search task in other words the s i in equation 7 is obtained when making the next task assignment the overall area should subtract the s i then follow steps one to three to re determine search area for another resource the search areas do not overlap each other ai et al 2019 notation constraints n i the maximum available number of resource i b the sea state h max t e m p the maximum survival time of people in water at temp c u i the maximum number of people resource i can carry 2 3 2 2 constraint analysis this part discusses the mathematical constraints in the proposed decision support model instead of the constrains in search theory such as path constraints eagle and yee 1990 and simplicity constraints richardson and discenza 1980 according to the real environmental conditions and the sar resources particulars there are some inequality constraints for sar operation guo et al 2019 the following is a summary of four types of constraints in this model first the available number of resources is limited in different organizations for example there may be some resources occupied in other tasks when a new emergency occurs therefore sar resources scheduling cannot be more than the total number of each type of resources as shown in inequality 9 9 0 x i n i x i n i 1 2 m second the scale of sea state is divided into 9 levels as shown in appendix a the higher the level is the severer the maritime environment is in order to make sure that sar resources are working under a safe environment there is a maximal safety level that allows them carrying out sar tasks so when dispatching sar resources the decision makers should also take the sea state into account the maximum allowable sea state should be greater than the current sea state level 10 b i b b i 0 1 9 i 1 2 m third different vessels have different capacity and there is a limited capacity of a vessel so the rescue vessels should be dispatched to accommodate more people than the expected number of people in water as shown in inequality 11 11 n i 1 m u i fourth the search time should be smaller than the people s longest survival time in water at temp c 12 max t i h max t e m p i 1 2 m 2 4 stage 3 algorithm design and model solving as the basic search variables and planning method can be obtained by conducting the above two stages stage 3 manages to design intelligent algorithms to solve the single objective and the multi objective model for optimizing sar resources two objective functions are introduced to assess the sar schemes the p s u c c e s s describes the success rate of sar operation by considering the sar resources p s u r v i v a l t the pod and the poc together another objective function is the total cost of sar operation including the c r i s k and c d i r e c t the costs criteria have been widely used in risk analysis and decision making wu et al 2017 and here it represents the payment used for executing search tasks and the cost of life lost in maritime emergency first de is applied to solve the single objective model second since adding the cost criteria the nsga ii is applied to find the pareto solution for the multi objective model third the topsis method is introduced to find the compromise solution of optimizing the sar resources notation algorithm parameters the objective function cr the crossover probability np the size of population nm the iterations f the scale factor l 1 l 2 l 3 the penalty factors p s u c c e s s the success rate of sar operation c r i s k the economic loss of failing to find search objects c d i r e c t the total direct costs of resources executing search tasks f 1 the value of objective function derived from success rate f 2 the value of objective function derived from total cost 2 4 1 the single objective model according to the decision variables paradigm 1 the scale of solution space is 13 i 1 m max x i min x i 1 2 m the scale of solution space increases exponentially as m increases which means the problem of optimizing number and types of sar resources is an np hard problem as an intelligent optimization method de is reasonable to solve this kind of problem storn and price 1997 the standard algorithm framework is shown in fig 8 more details on de are given below step1 initialize the population according to the paradigm we encoded the decision variables in decimal integers the encoded form of an original individual is designed as x i 0 x i 1 0 x i 2 0 x i m 0 i 1 2 n p where np is the size of population the number in the parenthesis indicates which generation over the whole iterative process the upper boundary of every vector component depends on the available number of the corresponding resources step2 define the objective function f 1 it consists of the fitness and penalty functions given the constraints analysis in section 2 3 2 we applied the penalty function to address the constraint the fitness function is designed as 1 p s u c c e s s so that the lower a fitness value is the better success rate an individual has 14 f 1 1 p s u c c e s s i 1 m l 1 max b b i 0 l 2 max n i 1 m u i 0 l 3 max t i h max t e m p 0 15 p s u c c e s s i 1 m x i p o c i p o d i p s u r v i v a l t i 1 m x i the objective function derived from the success rate is defined and calculated by equations 14 and 15 which means not only searching out the people in water but also making sure they are alive the l 1 l 2 l 3 are three large numbers which represent penalty factors when the corresponding limit is exceeded step 3 mutation operation the individuals in the g th generation are marked as x i g x i 1 g x i 2 g x i m g i 1 2 n p we randomly choose three different individuals x p 1 g x p 2 g x p 3 g and use equation 16 to generate the mutant individual h i g 16 h i g x p 1 g f x p 2 g x p 3 g where f is the scale factor that controls the influence from differential vector recommended as 0 5 price et al 2006 step 4 crossover operation can enrich the diversity of population the specific method is explained as fig 9 and function 17 17 v i j h i j g r a n d 0 1 c r x i j g e l s e where c r 0 1 indicates the crossover probability recommended as 0 2 wan et al 2018 r a n d 0 1 is a random number obeying uniformly distribution between 0 1 step 5 selection and iteration by comparing the fitness values of x i g and v i g we can pick the preferable individual as x i g 1 so the selection process is as follows 18 x i g 1 v i g if f 1 v i g f 1 x i g x i g e l s e given the limits of iteration nm the de operation can proceed to steps 3 again or be stopped 2 4 2 use of an improved multi objective model for decision support the total costs during sar operation consists of the risk and direct costs in this paper risk costs are defined as the sum of economic loss of failing to find the search objects multiplied by the probability of that loss occurring we assume that k is the consequence of one person losing his or her life because sar crews fail to detect him or her for instance this is converted into the value of a statistical life vsl which is recommended as euro 3 4 million per person bickel and friedrich 2013 19 c r i s k 1 p o d n k p o d i 1 m x i p o d i i 1 m x i in general the mean of pod i will be improved when increasing the number of sar resources involved however the direct cost will increase as well because allocating resources excessively results in a redundant sar plan therefore the total costs should be considered as an economic indicator to assess the loss of property for a sar operation solution x x 1 x 2 x 3 x m the total costs are defined as 20 f 2 c r i s k c d i r e c t 1 p o d n k i 1 m x i c i t i where c d i r e c t is the total direct costs of executing a sar operation such as the flight costs of helicopters and shipping costs of vessels it is defined as a sum of direct costs estimation of all sar resources involving in a search plan according to the total costs equation 20 and the single objective model which is related to success rate and we build a multi objective optimization model which allows the decision makers to balance success rate and total costs and choose a satisfying solution to complete the sar operation hence the problem about the decision support for sar operation is treated as a typical multi objective optimization problem moop and its mathematical expression is described as min f 1 x f 2 x at present the multi objective evolutionary algorithms are usually applied to analyze and solve moop in this paper search planners are guided to coordinate the confliction between the objectives and find the pareto solutions nsga ii is one of the most influential and widely used multi objective genetic algorithms nsga ii is an improved algorithm which overcomes the high computational complexity of nsga and the lack of elite strategy srinivas and deb 1994 in this paper it is used to find pareto solutions for sar operation the overall process of nsga ii can be described as fig 10 deb et al 2000 there is no need to discuss the specific details about initialing population and genetic operation in this paper what we concerned about is the comparison process among various solutions nsga ii uses a fast non dominated ordering mechanism to reduce the computational complexity of ordering first define ϕ x f 1 x 2 f 2 x 2 and use x i x j to denote that the solution x i dominates the solution x j then the non dominated sorting principles are summarized below x i x j if and only if one of the following conditions is true 1 x i is a feasible solution and x j is an unfeasible solution 2 both x i and x j are unfeasible solutions and ϕ x i ϕ x j 3 both x i and x j are feasible solutions and f 1 x i f 1 x j f 2 x i f 2 x j those solutions x in the 1st rank generate a pareto front where solutions are non dominated which means that the algorithm cannot find a feasible solution better than x that is to say feasible solutions of higher success rate and less total costs than pareto solutions meanwhile do not exist 2 4 3 topsis method we select the compromise solution from the pareto front using topsis the main steps are explained as follows step 1 normalizing the decision matrix a a i j h 2 21 a i 1 f 1 x i a i 2 f 2 x i m i j a i j i 1 h a i j 2 i 1 2 h j 1 2 where h is the number of pareto solutions so the matrix μ m i j h 2 is the normalized matrix of a a i j h 2 step 2 calculating a weighted normalization decision matrix r r i j h 2 22 r i j m i j w j where w j is the weight value of the j th objective step 3 define the positive and the negative ideal solutions 23 f j min j r i j f j max j r i j i 1 2 h j 1 2 step 4 calculating the distance between the feasible solution and f j f j 24 d i j 1 2 r i j f j 2 d i j 1 2 r i j f j 2 step 5 calculating the proximity of the feasible solution to the ideal solution 25 d i d i d i d i i 1 2 h in the end we sort d i in descending order the solution of the maximum value of d i is the compromise solution in the pareto solution set zavadskas et al 2016 3 an illustrative example 3 1 description of the emergency scenario at 01 44 a m on october 30 2019 a ship collision happened 123 45 e 37 15 n between a fishing ship and a bulk ship whose detailed information is shown in table 1 eleven people were missing the weihai maritime rescue center received the alarm and launched an emergency response at 02 28 a m in order to verify the validity of the proposed decision support method this paper takes the collision event as an illustrative example to discuss how to apply this method in design and operationalization of sar the data of environmental factors such as wind current wave and temperature can be obtained from the cnmsarss as shown in fig 11 the drift prediction of people was simulated after confirming the alarm the blue line which starts from a green point and ends at a red one shows the main drift track within 24 h the grey points show the possible locations of people in water in addition the detailed information of the drift prediction is recorded every hour in table 2 once a maritime emergency happened the number and types of sar resources need to be urgently selected and optimized according to the detailed information of sar resources in table 3 there is a fishing vessel and one cargo vessel available near the place where the collision happened moreover other types of resources no 1 8 are affiliated with maritime safety administration and other professional rescue organization hence a feasible solution can be expressed as x x 1 x 2 x 10 the average sea surface temperature is 14 c according to the reference table in appendix a2 the longest expected survival time is h max 14 6 h based on the environmental indices in the first 6 h and the classification in appendix a1 we can evaluate the current sea state is b 2nd and the numbers of people in water is n 11 in order to compute coverage and pod some typical sar resources sweep width table are given in appendix b guard 1986 the visibility is 3 5 nautical miles in this case the linear search is applied in sar operation and the track spacing s t r a c k is 0 1 nautical miles the parameters applied in this case associated with de are depicted in table 4 the following section applied the proposed decision support method in search planning to find the optimal scheme of sar resources 3 2 results for decision support considering the expected survival time and the limited sar resources the search planner first designs and optimizes the plan from 01 44 to 07 44 as shown in fig 12 the search work is divided into three sub phases in three even intervals and the corresponding probability distribution maps are given then the search planners can begin to design and optimize the types and number of sar resources in this stage they are going to decide which kind of and how many resources should be assigned to the search area in order to maximize the success rate and minimize the total cost from the description of the emergency scenario above the sar resources scheduling schemes including the types and number solved by de can be obtained in three different time intervals respectively other results solved by genetic algorithm ga and particle swarm optimization pso are treated as a comparative experiment for the single objective model for every time interval we run the three algorithms ten times respectively under the same parameters settings as depicted in table 4 and then save the best evolutionary curves data of every algorithm the best evolutionary curve records the process of obtaining the relative minimum f 1 of 10 runs next three kinds of best evolutionary curves corresponding to the three algorithms are shown in fig 13 a b c under different time intervals with the evolutionary generations as abscissa and f 1 in equation 14 as the ordinate the number of evolutionary generations indicates the iteration in the process of running algorithms for example it can be seen from fig 13a that de can obtain the relative minimum f 1 0 192 at the 73rd generation the detailed results under different time intervals including the chromosome with the relative minimum f 1 and the corresponding sar schemes are shown in table 5 moreover table 5 shows the compromise solutions solved by nsga ii and topsis for the multi objective model in different intervals in different time intervals the search planer decides the sar schemes according to the environment and available resources a smaller f 1 indicates a higher success rate which means the corresponding search plan is better as can be seen from table 5 when discussing the single objective model the p s u c c e s s of sar schemes obtained by de and pso are significantly greater than that obtained by ga but in 05 44 07 44 pso cannot converge to find the optimal solution in a same time interval the sar schemes obtained by de use fewer resources to reach the same p s u c c e s s compared with schemes obtained by pso it can be seen from fig 13a b c that the objective value curves based on the de converge to an optimal solution at a fast speed that is because the de uses a simple mutation operation and one to one competitive survival strategy to reduce the complexity of a genetic operation in summary de for this decision support model is robust and has strong global convergence ability another significant performance variable is the probability of survival as part of the objective function as time goes by the probability of survival declined resulting in decrease of the p s u c c e s s it warns that the search plan should be designed and operated as soon as possible especially during the most likely survival time for a multi objective model we generate the pareto front using nsga ii and find the compromise solution using topsis in which the weights of both objectives are equal fig 13d e f show the results of multi objective model these solutions in the pareto front means that the algorithm cannot figure out which search plan is better the weights of the two objectives are set at 0 5 after generating the positive and negative solution of topsis we can find the compromise solution marked with data tips taking the solution from fig 13e as an example the results with more information about the relationship between the success rate and the total cost are shown in fig 14 from fig 14 it could be concluded that the success rate is improved when the decision maker increases the cost of allocating sar resources the growth process of the success rate can be divided into three phases based on the gradient change the decision can be made more accurately and easily when the search planner notices that the success rate improved greatly from point a to b however the success rate increases inefficiently in phase i and more costs would be required in phase iii if they want to obtain a same improvement as in phase ⅱ that is why a rational decision maker will prefer to choose point b as the final sar schemes when balancing two objectives which is also the compromise solution obtained by topsis in 03 44 05 44 4 discussion the analysis and reasoning surrounding the results is covered in this section we also make a comparison of search schemes from two aspects including search effort and search task assignment moreover the selection of the weights of the objectives to reflect the decision preference is discussed by analyzing some decision makers requirements 4 1 comparison of sar schemes an important thing which should be noted is that the search task assignment will influence the search efficiency every ship or aircraft has its own search time and search area in order to evaluate the search efficiency of each ship search effort is introduced in this part the search effort is defined as the distance traversed within the area of interest it maybe equivalently defined as the amount of time spent in the area of interest times the average speed of broom frost 1999b 26 z i 1 m x i w i v i t i if multiple vessels and aircrafts are used simultaneously then the effort should be multiplied by the number accordingly to demonstrate the decision support method of optimizing sar resources scheduling schemes we further make a comparison of search effort between the scheme x a obtained by de and the compromise scheme x b based on nsga ii and topsis in 05 44 07 44 as shown in table 6 according to table 6 after adding the total costs as another objective the search time of x b is longer than that of x a obviously the number of beihai rescue 116 no 3 and offshore patrol vessel no 6 become less or zero because of its expensive direct cost which leads the search effort of the offshore patrol vessel to decline and the task assignment for it to decrease as a result the other ships search time for people in water becomes longer it can be seen from table 6 that fishing ship no 9 cannot reach the search area on time and execute the sar task in scheme x b that is because the search area has been assigned to other available ships which are faster and closer to the search object the simulations of sar schemes x a and x b are visualized in fig 15 the overall search area is fully covered and divided as several search tasks for sar resources the number and types of resources have been included in the sar schemes obtained from the proposed model in addition our results confirm that the vessels near the accident are the useful supplements for sar because they may contribute more effectively than professional rescue units in a maritime accident in this case the sar effort of cargo vessel no 10 is more than that of offshore patrol vessel no 6 in scheme x b though the compromise scheme x b makes a concession because of the total costs it also adds the huaying ambulance boat no 4 and sar lifeboat no 7 which have a lower direct cost to make sure to finish the search work in time compared to the scheme x a of the single objective model 4 2 decision preference different decision makers may have different preferences to the two objectives xue et al 2019b thus we next set out to use different weights for the success rate and total costs in a pareto solution set the objective function values of compromise scheme in 03 44 05 44 under different weights are shown in table 7 if the decision makers are preferred to obtain a higher success rate they can set the weight of the total costs objective lower in contrast if the decision makers are cost sensitive they tend to choose a higher weight of total costs however we found that the results from nsga ii are not very sensitive to the weight changing from 0 1 to 0 9 within the weight interval 0 1 0 4 and 0 6 0 9 the compromise solutions of a pareto solution set do not change and the corresponding total costs do not increase even though the decision maker has a risk seeking tendency these data are consistent with the notion that the compromise solution is relatively stable which makes it easier for the decision maker to select the final sar scheme during an actual decision making process for sar in maritime emergency decision makers must artificially adjust and optimize the scheme to select vessels and aircrafts as soon as possible consider a scenario in which decision makers have some specific requirements such as 1 if three beihai rescue 116 must participate in the sar operation then the number encoding of x 3 is set to 3 when initializing the population and 2 if the sar task need coordination of two kinds of vessels then the encoding of them are linked to satisfy this requirement the proposed decision support method in this paper has enough flexible space to be applied in practice when other similar situations occur the decision support model and the corresponding solving algorithm allow users to set specific encoding and adjust the sar schemes based on individual preferences 5 conclusions the main contribution of this paper is to propose a decision support method to help design and operationalize the sar plan in a maritime emergency based on intelligent algorithms and topsis specifically the three stage framework including the factors integration identification of variables and model solving is described step by step in order to develop an intelligent algorithmic based decision support method for sar we investigate two important aspects 1 how to organize and optimize the number and types of sar resources to execute the search task 2 how to balance different objectives when allocating sar resources under different decision preferences the process of generating probability distribution maps and determining the search areas are also discussed before investigation of the sar resources optimization by introducing the poc pod probability of survival success rate and costs a single objective model and a multi objective model are both designed and discussed in this paper from the comparison of search efforts and simulations of search task assignments the proposed method can help to make the final search plan quickly and effectively the proposed decision support method can be applied to the maritime sar emergency response however one important future work of sar research may focus on extending our decision model to consider sar cooperation behavior defining uncertain environmental or human factors and enabling multi decision makers to work together in the future an agent based simulation framework could be developed to realize the intelligent interaction between resources and decision makers moreover this method should be applied to more maritime emergencies for further validation in the future credit authorship contribution statement weitao xiong conceptualization methodology software data curation visualization investigation validation writing original draft writing review editing p h a j m van gelder supervision methodology writing review editing kewei yang data curation software declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the work was supported by the national key r d program of china grant no 2017yfc1405005 national science foundation of china grant no 71901215 china scholarships council grant no 201803170203 appendix a table a1 classification of sea state table a1 levels the wave height the corresponding wind level wind speed m s 0 0 0 0 0 0 2 1 h 1 3 0 10 1 2 0 3 3 3 2 0 10 h 1 3 0 50 2 4 1 6 7 9 3 0 50 h 1 3 1 25 4 5 5 5 10 7 4 1 25 h 1 3 2 50 5 7 8 0 17 1 5 2 50 h 1 3 4 00 7 8 13 9 20 7 6 4 00 h 1 3 6 00 8 9 17 2 24 4 7 6 00 h 1 3 9 00 9 10 20 8 28 4 8 9 00 h 1 3 14 00 10 11 24 5 32 6 9 14 00 h 1 3 12 32 7 note national standard of the people s republic of china gbj 64 1 85 64 2 85 table a2 expected survival time in cold water table a2 water temperature exhaustion or unconsciousness in expected survival time 70 80 f 21 27 c 3 12 h 3 h indefinitely 60 70 f 16 21 c 2 7 h 2 40 h 50 60 f 10 16 c 1 2 h 1 6 h 40 50 f 4 10 c 30 60 min 1 3 h 32 5 40 f 0 4 c 15 30 min 30 90 min 32 f 0 c under 15 min under 15 45 min note us sar task force available http ussartf org cold water survival htm appendix b table b1 b2 and b3 summarize the sweep width nautical miles of sar resources table b1 vessels sweep width table b1 the search object visibility nmi 3 5 10 15 20 people in water 0 4 0 5 0 6 0 7 0 7 table b2 sweep width helicopters for altitudes 300 feet table b2 the search object visibility nmi 1 3 5 people in water 0 1 0 1 0 1 table b3 sweep width fixed wings aircrafts for altitudes 300 feet table b3 the search object visibility nmi 1 3 5 people in water 0 1 0 1 0 1 according to equations 9 and 10 the coverage and pod of sar resources can be calculated as shown below this is a necessary step to compute the success rate of whole sar operation table b4 the coverage and pod of sar sources table b4 sar resources coverage pod vessels 4 0 98 helicopters 1 0 63 fixed wings aircrafts 1 0 63 appendix c table c1 list of abbreviation table c1 abbreviation full name aspt advanced search planning tool cansarp canadian search and rescue program casp computer assisted search planning de differential evolution dss decision support systems ga genetic algorithm moop multi objective optimization problem nmsarss national maritime search and rescue support system nsga ii non dominated sorting genetic algorithm pso particle swarm optimization sar search and rescue sarops search and rescue optimal planning system soa service oriented architecture topsis technique for order preference by similarity to an ideal solution 
21834,thin shell artificial reef ar structures with spatial internal volumes have demonstrated superior stock recruitment ability and material efficiency than many gravity based reef blocks and cementitious materials given the easy to tailor nature remains the most popular in reef constructions to date however under constant seawater immersion external sulfate attack esa introduces a major and uncertain reliability concern to this type of ar system due to the inherent material randomness this study is concerned with developing a novel stochastic modelling framework for assessing the esa under material uncertainty in this paper the main difficulty associated with the stochastic esa modelling is identified for the first time and a novel machine learning aided chemophysical modelling approach is proposed the performance of the developed framework is carefully examined through the analyses on two types of cementitious materials under esa keywords artificial reef chemophysical modelling external sulfate attack machine learning uncertainty quantification 1 introduction natural coral reefs primarily exploited for fishery tourism and coastal protection are of historical importance to many coastal regions around the world however this fundamental socio economic sector has become one of the first victims from the global climate changes where a large amount of natural reefs are losing stock recruitment capability and eventually dying hughes et al 2019 for the purposes of loss mitigation and habitat restoration a variety of artificial reefs ar constructed with diverse structural designs and materials have been widely implemented except for the early attempts of using gravity based solid blocks modern designs comprising thin walled shell structures with multiple cut openings become more appealing wang et al 2018 see fig 1 a the spatial internal volumes of these ar structures not only provide more shelters for sea lives but also contribute to material efficiency in large deployments for materials adopted in shell type ar structures concrete made of a variety of cementitious composites remains the most popular choice due to its easy to tailor nature accounting for 48 7 among all types of materials as reportedly utilised lima et al 2019 however under seawater submerged condition concrete may be subjected to diverse chemical attacks balestra et al 2019 wang et al 2019 yao and li 2019 causing material deterioration and loss of structural reliability one of the most detrimental durability issues is external sulfate attack esa yin et al 2017 yu et al 2018a which often leads to expansion cracking and even spalling of the surface concrete as shown in fig 1 b considering the thin walled feature of these ar structures the expansive cracking caused by esa may render the structure more prone to catastrophic failures than solid reef blocks although the esa under seawater and other sulfate enriched solution should be differentiated its origin can be described by the ingress of sulfate ion through the porous hydrated cement system santhanam et al 2006 as a very reactive media a variety of cement hydrates would participate in chemical reactions to retrieve the equilibrium as disturbed by ionic exchanges yu et al 2015 in specific leaching effect promotes the transformation of portlandite ch to gypsum which would be further interacted with monosulfates afm to precipitate the needle like ettringite aft according to experimental studies ragoug et al 2019 the gypsum is formed by through solution reaction on the pore surface causing the shrinkage of pore space while the needle like aft is precipitated by topochemical reactions and contributes directly to material expansion and cracking see fig 2 a a number of deterministic chemophysical approaches have also been reported for modelling the esa issue cefis and comi 2017 samson and marchand 2007 however except for the authors yu et al 2015 yu and zhang 2018a b few studies have distinguished the influence of these two different reaction mechanisms by adopting the authors method yu et al 2015 more accurate deterministic estimations on the elemental compositions under esa such as calcium and sulfur were achieved due to the better modelling of the expansive cracking process nonetheless the success in deterministic esa modelling does not guarantee the non deterministic counterpart that involves uncertainty quantifications firstly the chemophysical model represents a series of partial differential equations pdes and algebraic equations for ionic transportations and chemical reactions a closed form solution cannot be derived where only numerical approaches such as finite element method fem are feasible yu et al 2015 yu and zhang 2017 2018a b yu et al 2019a b under this circumstance the uncertainty quantification is forced to rely on the crude monte carlo simulation mcs of low efficiency yu et al 2018b thus the computational burden may often become intractable which inspires the surrogate modelling of using machine learning aided methods however also highlights the other research gaps in assessing esa under uncertainty according to fig 2 a the time dependent expansion appears to be a piecewise function due to the significant time required for the growth of gypsum and aft to cause stress build up expansion and cracking it becomes problematic when implementing the machine learning aided approaches for uncertainty quantification as the obtained stochastic responses at a given time frame may represent different status for instance three typical cases i e e1 e2 and e3 under the influence of material uncertainty are displayed in fig 2 b for illustrations e1 is shown to undergo the current expansion period while e2 is in the plateau of aft growth and e3 indicates the finish of the previous expansion to the authors knowledge it is difficult to address such a nonlinear relation by conventional surrogate modelling and few studies have investigated the esa involving uncertainty in this paper a novel machine learning strategy combined with the chemophysical modelling approach is developed focusing on the assessment of esa under uncertainty the mcs guided stochastic chemophysical modelling using fem and the inherent model instability are illustrated in section 2 targeting the unique data structures of the stochastic esa responses a self adaptive uncertainty quantification scheme based on the extended support vector regression xsvr feng et al 2019 is proposed in section 3 in section 4 the developed method is extensively examined by modelling the esa on two types of cements and comparing against other well established machine learning techniques finally some useful findings are reported in the conclusions of section 5 2 mcs guided stochastic chemophysical modelling of esa for real life concrete ar shells serving undersea the potential severity of esa may be subjected to the controls of competing aggressive agents and the associated uncertainty which however are out of the scope of this study herein only the pure esa is investigated and the accompanying cation of very limited aggressiveness i e sodium is concerned 2 1 preliminary similar to many other durability concerns esa is originated from the ingress of aggressive sulfate ions through the porous microstructure of cementitious materials to cause chemical reactions and further damages in numerical modelling such a process represents a reaction diffusion system formulated in partial differential equations pde by further incorporating uncertain factors the pde system evolves into a stochastic pde spde where a general mathematical form in space domain d and time domain t can be expressed using eq 1 1 u t div d grad u f u q u w u t where u d t ℜ indicates concentration vector containing diffusive agents mol m 3 of solution d is the diffusivity vector m 2 s the nonlinear operator f comprises the simultaneous reactions caused by the diffusive reactants q u is covariance w u is a vector of independent wiener processes and q u w u is the noise term describing the fluctuations in u d t due to uncertainty in general strict conditions for solving a spde are required allen 2012 as a promising approach hariharan and kannan 2014 wavelet method has been adopted to a few specific spde problems allen 2012 dan et al 2015 however a universal solution to a more general problem i e the deterioration modelling of cementitious materials has yet been reported so far herein uncertainty may lead to a covariance q u depending on the solution to u d t stundzia and lumsden 1996 while the nonlinear operator f may include the coupling effects other than chemical reactions for this case the spde bears the form of stochastic nernst planck np equation in eq 2 2 w s u s t w l u t div d w l grad u w l d z f r t u grad ψ d w l u grad ln γ d u ln γ u t grad t u d l grad r h w l r q u w u t where w s is volumetric solid content m 3 m 3 u s is reactant contents bound in solid products mol m 3 of solid matrix w l is volumetric liquid content m 3 m 3 z is valence numbers of diffusive reactants f is the faraday constant 96488 46 c mol r is the ideal gas constant 8 3143 j mol k t is thermodynamic temperature k ψ is electrical potential v γ is chemical activity coefficient calculated according to samson et al 1999 d l is diffusivity vector of liquid water m 2 s r h is relative humidity and r is source sink term according to eq 2 the solution to the stochastic np equation has to be achieved with solving other spdes and algebraic equations simultaneously due to the inclusions of other coupling effects such as electrical neutrality chemical activity and capillary advection to the authors knowledge no direct solution approach for such a complex problem has been developed where the non deterministic investigations under uncertainty have to rely on the crude mcs thus the strong yearning of improving computational efficiency inspires the implementation of diverse supporting approaches such as advanced sampling and machine learning method targeting the esa on thin shell ar structures a specialised machine learning aided technique is to be developed in section 3 herein the fundamentals of stochastic chemophysical modelling of esa are introduced first 2 2 stochastic multi species transportation modelling eq 2 represents a general form of stochastic np equation where specific modifications are required for the implementation in esa modelling firstly under the mcs guided scheme the noise term q u w u t can be eliminated yu et al 2019c moreover given the variety of chemical reactions under esa solving them simultaneously with ionic transportation is computationally intensive based on the authors yu et al 2015 yu and zhang 2018a b an operator splitting approach osa for reducing the computational effort is adopted through the osa the reaction related terms can also be omitted and solved independently using algebraic equations as illustrated in section 2 3 except for the reaction terms the corresponding advection term can also be omitted considering a constant submerged condition of ar structures in seawater in addition even though temperature variation in seawater may be common in engineering practices it is not considered here where an isothermal condition is assumed and the heat conduction term is omitted correspondingly furthermore as mentioned before cementitious materials under esa is prone to expansive cracking the impact of internal microcracks on the overall transportation mechanism should thus be considered which is achieved by including the so called cracking suction effect yu et al 2015 therefore the complete set of multi species transportation model for describing the esa under an isothermal and submerged condition is written in eq 3 which is to be solved with the poisson s equation of eq 4 for considering the electrical coupling effect and maintaining the system near the electroneutral state 3 w l u t div d w l grad u w l d z f r t u grad ψ d w l u grad ln γ d w l u grad c s i 0 4 div τ w l grad ψ f ε p w l z u 0 where τ is tortuosity vector ε p is the permittivity bearing the value of 7 08 10 10 c v m 2 and c s i indicates the cracking suction index calculated through yu et al 2015 for esa modelling to fully address the chemophysical degradation of hydrated cements under esa all the main ionic species comprising alkali ions and other diffusive reactants are considered including oh na k ca 2 so 4 2 al oh 4 and h 2 sio 4 2 2 3 chemical reaction modelling by implementing the osa yu et al 2015 the solution procedure to simultaneous chemical reactions is decoupled from ionic transportation according to yu et al 2019c within each run of mcs execution the evaluation of chemical reactions proceeds sequentially after solving the governing equations for ionic transportation it is achieved by following the theory of chemical thermodynamics where the chemical equilibrium model as given in eqs 5 6 is implemented 5 k s i 1 n 1 u i v s i γ i v s i j 1 n 2 u j v s j γ j v s j i 1 n 1 u i v s i j 1 n 2 u j v s j s 1 s 6 k p s s i u i ν i p s s γ i ν i p s s λ p s s δ p s s where k s is the equilibrium constant for the sth reaction denotes the chemical activity v s i and v s j are the stoichiometric coefficients for the ith product and the jth reactant in the sth reaction n 1 and n 2 denotes the total numbers of products and reactants from the sth reaction eq 6 is for evaluating the evolutions of solid solutions where the subscript p s s indicates solid p in solid solution s s k p s s is equilibrium constant of solid p in pure form δ p s s is molar fraction of solid p in solid solution λ p s s is activity coefficient which equals to 1 0 for ideal solid solution a variety of reactions can potentially occur under the esa which however is not necessarily essential in the numerical modelling considering the balance between accuracy and efficiency only the key heterogeneous reactions that may significantly contribute to the variations of microstructure and transportation mechanisms are included see table 1 as shown in the table the incongruent behaviour of the c s h gel is modelled as a solid solution with three phases in equilibrium yu et al 2015 yu and zhang 2017 i e jennite 1 5 cao sio 2 2 3 h 2 o calcium metasilicate cao sio 2 h 2 o and silica gel sio 2 in addition afm is also modelled as a solid solution where two types of afm phases are considered under the esa i e monosulfate and afm oh it is obvious that through the implementation of osa a total of eight degrees of freedom dofs from chemical reactions is separated from the modelling of ionic transportation which accounts for another seven dofs by doing so the computational effort of the mcs guided stochastic chemophysical modelling is significantly reduced 2 4 reaction induced damages and inherent model instability the observable damages of esa on hydrated cementitious materials generally consist of volumetric expansion and cracking these damages are resulted from the heterogeneous chemical reactions as analysed above and would lead to undesirable changes in both the mechanical and physical properties of ar structures herein as the first systematic stochastic study on the esa under uncertainty only the degradation in physical ionic diffusivity is concerned herein to do so the first step is to evaluate the change of material porosity which is easily calculated from the volumetric changes of all the heterogeneous reactions as listed in table 1 see eq 7 then the corresponding impact on the ionic diffusivity by porosity change is estimated through eq 8 samson and marchand 2007 7 ϕ ϕ 0 δ v s 8 h d ϕ ϕ e 4 3 ϕ v p e 4 3 ϕ 0 v p where ϕ is the real time porosity under chemical reactions ϕ 0 is the initial porosity δ v s indicates volumetric changes m 3 m 3 of heterogeneous reactions v p is volume fraction of cement paste in cementitious materials and h d ϕ is diffusivity multiplier as function of porosity with the porosity change being obtained the volumetric expansion under the esa can be estimated correspondingly as mentioned before the formations of the two expansive solid products i e gypsum and aft follow different mechanisms where only the precipitation of aft is potentially responsible for expansion and cracking of concrete therefore by adopting the empirical method as proposed by the authors yu et al 2015 the volumetric expansion and cracking due to the excessive amount of aft precipitation are estimated using the following equations 9 δ v s δ v s aft b ϕ v δ v s aft ϕ free v 10 ε δ v s v ε ε 3 for δ v s 0 11 c d k 1 ε th ε m for ε ε th where v indicates volume m 3 of the representative element volume rev b 1 is an empirical parameter accounting for the effective pore room to accommodate the needle like aft precipitations δ v s aft is volumetric growth m 3 of aft δ v s is volumetric expansion m 3 ε is volumetric strain ε is uniaxial strain assuming the material is isotropic macroscopically ε th is the threshold strain at which microcracking starts c d is a scalar value indicating crack density k and m are two tunning parameters according to the literature sarkar et al 2010 yu et al 2015 values of 0 36 0 16 2 3 and 2 10 4 are assigned to b k m and ε th respectively by obtaining the crack density the collective effects of porosity change and expansive cracking on ionic diffusivity can be estimated which is achieved by employing sarkar et al 2010 as follows 12 h d c c d 1 32 9 c d d p 13 d p 0 c d c dc 2 c dec c d c dec c dc 2 c d c dc c dc c d c dec c d c dec 14a h d h d ϕ h d c τ h d ϕ h d c τ h d ϕ h d c τ 14b d h d d 0 where h d c is diffusivity multiplier as function of crack density h d is overall diffusivity multiplier d 0 indicates material diffusivity at a sound state c dc 0 18 and c dec 0 56 are conduction percolation threshold and rigidity percolation threshold respectively yu et al 2015 according to the damage evaluation models as presented above the difficulty and inherent model instability in modelling esa under uncertainty can be clearly identified and explained as shown in eqs 9 11 only if the growth of aft overwhelms the available pore room for accommodation can the expansive strain occur in addition only if the expansive strain exceeds the threshold can cracks propagate even though these models are theoretically sound and reasonable they lead to piecewise developments of volumetric expansion as shown in fig 2 this phenomenon contributes to the non continuous development in material diffusivity which further influences the chemical reactions and the evaluated damages in the following sequences from the time domain by including uncertainty such as the inherent material uncertainty the present damage evaluation procedure designed for deterministic fem studies would generate even more issues due to the randomness within hydrate contents porosity and available pore volumes for accommodating solid precipitations as a result the stochastic analysis on esa aided with machine learning for reducing computational effort is not readily applicable which has yet been addressed in the literature 3 self adaptive uncertainty quantification based on machine learning focusing on effective and efficient stochastic esa analyses a self adaptive uncertainty quantification modelling framework based on the machine learning techniques is developed in this paper 3 1 self adaptive stochastic esa modelling framework according to the authors yu et al 2019c 2020 although the latest development in support vector regression svr i e the extended svr xsvr has demonstrated a significant superiority in the stochastic chemophysical modelling of diverse concrete durability issues xsvr along with other machine learning methods have yet been tested in the stochastic esa problems as mentioned before the fem based esa modelling is problematic due to the implemented damage evaluation method and the inherent finite element discretisation causing a piecewise response curve that could be difficult for performing machine learning aided stochastic analyses currently there is no universal solution to circumvent the difficulty in stochastic modelling of piecewise responses herein targeting the esa problem the authors propose an alternative approach consisting of modelling the stochastic responses of aft growth and shrinkage of free to fill pore volume considering the continuous developments of these two properties however the inherent model instability caused by the adopted damage model and finite element discretisation may still affect the overall responses in an implicit manner to better elaborate on this issue the cracking features from one numerical example in section 4 is displayed here in fig 3 fig 3 demonstrates two typical statistical distributions regarding the number of cracked elements n ce obtained from the mcs guided chemophysical fem modelling the mcs response of 10000 actual function evaluations n s is presented in fig 3 a where the data is not continuous in nature due to fem discretisation as to be presented in section 4 a direct surrogate model constructed through random sampling from such a data structure would suffer from low accuracy and instability regardless of the specific machine learning techniques as employed therefore in addition to the first proposition the authors also propose to divide the data into several clusters before constructing surrogate models in a cluster wise manner taking fig 3 a as an example three clusters may be roughly recognised by a visual observation over 10000 data points however detailed characteristics of clusters may vary depending on the number of samples as available for clustering see the case of n s 100 in fig 3 b this feature introduces another challenge in the stochastic esa analyses in order to resolve these difficulties a novel self adaptive stochastic esa modelling framework is developed in this paper as presented in fig 4 according to fig 4 there are five main steps to be carried out in order to fulfil an effective and efficient stochastic esa modelling where the first step is regarding the mcs guided chemophysical modelling which has been introduced in section 2 the mcs modelling with large number of function evaluations n s 10000 is taken as the reference in this paper a random subset with much fewer n s sampled from the full scale mcs would be generated for training the surrogate model based on machine learning in the following steps as demonstrated in the framework except for the last step of implementing the surrogate model the training of a self adaptive surrogate esa model is achieved by three consecutive procedures comprising unsupervised recognition supervised classification and cluster wise supervised regression which are illustrated in detail as follows 3 2 clustering with tune gaussian mixture model to the authors knowledge there are a variety of data driven techniques such as k means mixture model and neural network can be used to perform pattern recognition for clustering in view of both computational efficiency and accuracy the approach of fitting gaussian mixture models gmm using expectation maximisation em algorithm is adopted in the present study the combined em gmm is deemed as the fastest algorithm for learning mixture models which however may only ensure the convergence to a local optimum bishop 2006 even though this issue can be resolved through the variational bayesian gaussian mixture models it is not preferable due to that it may introduce many implicit biases to the cluster size and the covariance matrix structure on the contrary em maximises only the likelihood without bias and all types of structures for the covariance matrices can be examined and treated equally bishop 2006 in addition the concern about local optimum can be empirically resolved by repeating the process with different initiation points vanderplas 2016 moreover the model selection criteria such as the akaike information criterion aic and bayesian information criterion bic can be applied to further penalise for model complexity so as to determine the most suitable model and number of clusters burnham and anderson 2004 furthermore in the specific application to the study of esa a finite set of models with a presumable maximum cluster number can be generally assumed therefore the em gmm algorithm penalised by using aic and bic is adopted herein which is called the tune gaussian mixture model tgmm in general utilising tgmm for pattern recognition and clustering starts with the e step from the em algorithm to be more specific the e step is initialised based on the number of clusters k as presumed for fitting the data the gaussian mixture distribution p x as a combination of k gaussians with the weight π is written in eq 15 the corresponding posterior distribution of the responsibility that each gaussian has for each data point x i from x ℜ n is then calculated by eq 16 according to the expression of eq 16 a higher value of γ i k would be obtained if x i is very close to one gaussian k and relatively low values otherwise 15 p x k 1 k π k n x μ k σ k 16 γ i k π k n x i μ k σ k j 1 k π j n x i μ j σ j where n x μ σ denotes the multivariate gaussian with mean μ and covariance σ x contain the information of all the random input variables as well as n ce according to fig 4 after obtaining the posterior the m step is carried out by calculating m k and the corresponding weight for each cluster k in eq 17 then the new set of mean μ k new and covariance σ k new for cluster k is updated with the γ i k in eqs 18 19 bishop 2006 repeating the e and the m steps until the convergence criterion for the log likelihood i e eq 20 is met where the corresponding model cost can then be calculated through a cost function as given in eq 21 bishop 2006 17 m k i 1 n γ i k π k m k n 18 μ k new 1 m k i 1 n γ i k x i 19 σ k new 1 m k i 1 n γ i k x i μ k new t x i μ k new 20 ln p x π μ σ i 1 n ln k 1 k π k n x i μ k σ k 21 ℂ k i x i k μ k 2 in general the convergence of the em algorithm is always guaranteed while the cost also decreases monotonically with the increase of the presumed number of clusters as a result there is another essential step in implementing the tgmm i e model selection otherwise the clustering would always lead to the model with the maximum presumed cluster number which clearly is undesirable in terms of achieving an efficient stochastic analysis in the present study the model selection is achieved by penalising the model for increasing complexity by examining both the aic and bic values burnham and anderson 2004 moreover for models with comparable aic and bic values the corresponding gradients will be calculated to ensure an informed decision making herein the general principle is to favour the simpler model with fewer numbers of clusters to maintain computational efficiency more details on the model selection process are discussed by specific problems in the stochastic esa modelling presented in section 4 3 3 classification with support vector machine after clustering the training data through pattern recognition over the combined data x containing the information of n ce and all the random input variables classification based on only the random input variables are carried out it should be noticed that n ce is not a part of the input variables which is an intermediate response obtained through the mcs guided chemophysical modelling thus it should neither be included in the input nor the response and the response for the training of the classification model is the cluster information c obtained from the previous pattern recognition herein a doubly regularised support vector machine drsvm is implemented for classification in this paper li et al 2016 wang et al 2006 the general expression can be written as 22 min w γ λ 1 2 w 2 2 λ 2 w 1 i 1 m 1 c train i f ˆ c x i 23 1 c train i f ˆ c x i 0 1 c train i f ˆ c x i 0 1 c train i f ˆ c x i 1 c train i f ˆ c x i 0 where λ 1 and λ 2 are two tunning parameters controlling the balance between classification and feature selection γ ℜ denotes bias w ℜ n denotes the normal to the hyperplane created by f ˆ c indicating the classification model to be trained 1 and 2 represent the l 1 and l 2 norms and 1 c train i f ˆ c x i denotes the hinge loss function as defined in eq 23 in order to solve the optimisation problem as formulated in eq 22 dunbar et al 2010 proposed to utilise two non negative variables p q ℜ n to describe w ℜ n as w p q by doing so l 1 and l 2 norms can be decomposed and calculated in eqs 24 25 24 w 1 e n t p q e n 1 1 1 t ℜ n 25 w 2 2 p q 2 2 p 2 2 q 2 2 2 p t q p 2 2 q 2 2 the decompositions of l 1 and l 2 norms have been proven to be an effective approach in nonlinear optimisation fung and mangasarian 2004 furthermore the hinge loss function can also be replaced by a linear constraint to consider the noise in the training data where a non negative slack variable ξ ℜ m is adopted for error control by doing so the drsvm as expressed in eq 22 can be rewritten into eq 26 and solved as a quadratic programming problem using the lagrange method dunbar et al 2010 as a result the trained classification model is expressed using eq 27 26a min p q γ ξ λ 1 2 p 2 2 q 2 2 λ 2 e n t p q e m t ξ 26b s t d x train p q γ e m ξ e m p q 0 n ξ 0 m 27 f ˆ c x x t p q γ where 0 m 0 0 0 t ℜ m and d ℜ m m denotes a diagonal matrix containing the cluster labels from c train that are associated with the input data used for training the classification model f ˆ c x 3 4 cluster wise regression with xsvr the last major step in realising the self adaptive uncertainty quantification of the esa problem involves with constructing surrogate models based on the latest regression technique of xsvr feng et al 2019 herein with the obtained cluster information c the xsvr is applied to each respective cluster with the training data of all the random input variables that belong to the corresponding cluster moreover according to the specific features to be studied either the growth of aft δ v s aft or the shrinkage of free to fill pore volume ϕ free is chosen as the stochastic response for training the regression model due to that the xsvr is implemented in a cluster wise manner based on the clustering information as generated through tgmm and drsvm the uncertainty quantification achieved by such a self adaptive procedure is called the sa xsvr in the following context herein the key formulations regarding the xsvr are introduced where more detailed model derivations are referred to the authors feng et al 2019 yu et al 2019c 2020 the xsvr represents a direct extension of the previously applied drsvm from classification to regression unlike the ε insensitive support vector regression ϵ svr xsvr utilises a quadratic ϵ insensitive loss function to enhance the numerical stability moreover a mapping function φ x is introduced to map the raw variables from the low dimensional space ℜ n into a higher dimensional feature space f this mapping approach is particularly effective in the application to the nonlinear multiphysics problems yu et al 2020 leading to the development of kernelised xsvr algorithm in this paper the empirical kernel mapping is adopted and written in eq 28 schölkopf et al 1999 28a x i x i 1 x i 2 x i n t k ˆ x i φ x 1 t φ x i φ x 2 t φ x i φ x m t φ x i k x 1 x i k x 2 x i k x m x i 28b k x i x j φ x i t φ x j where k x i x j denotes kernel function k ˆ x i is the m degree empirical feature vector equalling to the number of training sets n indicates the number of different input variables after kernel mapping the empirical feature vector k ˆ x i is used to train the surrogate model provided with the training dataset x t r a i n and a kernel function of any acceptable types the original input is mapped into a kernel matrix k train ℜ m m in eq 29 and implemented to formulate the kernelised xsvr as expressed in eq 30 29 k train k x 1 x 1 k x 1 x 2 k x 1 x m k x 2 x 1 k x 2 x 2 k x 2 x m k x m x 1 k x m x 2 k x m x m 30a min p k q k γ ξ ξ ˆ λ 1 2 p k 2 2 q k 2 2 λ 2 e m t p k q k c 2 ξ t ξ ξ ˆ t ξ ˆ 30b s t k train p k q k γ e m y t r a i n ε e m ξ y train k train p k q k γ e m ε e m ξ ˆ p k q k ξ ξ ˆ 0 m where ξ ˆ is the redundant non negative constraint for slack variables e m 1 1 1 t ℜ m c 0 is a penalty constant y train ℜ is the actual responses from the training group ε is the tolerable deviation between y train and prediction from the surrogate regression model f ˆ r x t r a i n and p k q k ℜ m are two non negative variables defined by feng et al 2019 the subscript k here denotes a kernelised learning algorithm similar to the optimisation concept as adopted in the drsvm the kernelised xsvr can be rewritten in eq 31 and also be solved as a quadratic programming problem by incorporating a non negative lagrange multiplier u k ℜ 4 m using eq 32 31a min z k γ 1 2 z k t c ˆ k z k γ 2 λ 2 b k t z k 31b s t a ˆ k i 4 m 4 m z k ε i 4 m 4 m γ g ˆ k e ˆ k d ˆ k 0 4 m 32a min u k 1 2 u k t q k u k m k t u k 32b s t u k 0 4 m where i 4 m 4 m ℜ 4 m 4 m denotes an identity matrix the matrix expressions of c ˆ k g ˆ k a ˆ k b k e ˆ k d ˆ k and z k are presented in appendix a while q k and m k are calculated as follows 33a q k a ˆ k i 4 m 4 m c ˆ k 1 a ˆ k i 4 m 4 m t g ˆ k e ˆ k e ˆ k t g ˆ k q k ℜ 4 m 4 m 33b m k λ 2 a ˆ k i 4 m 4 m c ˆ k 1 b k ε e ˆ k d ˆ k since the obtained solution is based on optimisation whether the kernelised xsvr indicates a global or a local minimum is a subject requires examination according to feng et al 2019 the kernelised xsvr has been proven to always lead to the global minimum solution regardless of the specific kernel function as adopted by further assuming u k ℜ 4 m as a solution to eq 32 the variables z k γ and coefficient w can be written in eqs 34 36 and the surrogate regression model as obtained from the kernelised x svr algorithm is expressed by eq 37 34 z k c ˆ k 1 a ˆ k i 4 m 4 m t u k λ 2 b k 35 γ e ˆ k t g ˆ k u k 36 w p k q k z k 1 m z k m 1 2 m 37 f ˆ r x p k q k t k x e ˆ k t g ˆ k u k even though the convexity of the xsvr holds regardless of the kernel functions as adopted for mapping the performance of the surrogate model still depends on the kernel functions as chosen according to yu et al 2020 conventional kernels such as polynomial and gaussian kernels may suffer from low accuracy when modelling chemophysical problems due to the non orthonormal bases it can be resolved by implementing orthogonal kernels such as the gegenbauer polynomial feng et al 2019 tian and wang 2017 which has been successfully implemented in modelling the leaching of cementitious materials yu et al 2020 thus for the suggested ranges for hyperparameters one can refer to the authors feng et al 2019 yu et al 2020 to assess the goodness of parameter selections within the proposed range a 5 fold cross validation is adopted and bayesian optimisation method is implemented for the automatic selections of hyperparameters vapnik 2013 4 modelling the esa under uncertainty the main focus of the present study lies in addressing the difficulty in the machine learning aided uncertainty quantification of esa caused by the stepwise cracking propagation and fem discretisation in modelling thus other potential durability effects for ar structures such as chloride ingress and biogenic degradations are not concerned with the following analyses 4 1 problem descriptions and modelling setups 4 1 1 problem descriptions in order to create a comprehensive database for performing the stochastic analyses the design of samples is carried out with care herein the reported esa tests on cement pastes prepared with high water to cement ratio w c 0 6 are adopted maltais et al 2004 samson and marchand 2007 considering that a decent amount of material decay can be created on pastes within an acceptable timeframe choosing such an experimental dataset would not only benefit the following crude mcs guided chemophysical modelling but also eliminate the influence of interfacial transition zone to the overall expansive cracking process which is out of the specific scope of the present study in details the cement paste specimens prepared with two types of cementitious materials i e a canadian type 10 of ordinary portland cement and a canadian type 50 of sulfate resisting cement are adopted to construct the present stochastic analyses for the factory characterisations of chemical and mineralogical compositions of these two types of cements one may refer to the reported literature maltais et al 2004 samson and marchand 2007 herein the material properties for carrying out the chemophysical modelling comprising porosity ionic diffusivity initial solution and hydrate compositions are summarised in table 2 the material properties as presented in table 2 can be regarded as the mean values without considering material uncertainty and have been verified by hydration models yu et al 2015 in details porosity and ionic diffusivity were validated by empirical models as reported in jennings and tennis 1994 oh and jang 2004 the initial hydrate and pore solution compositions are examined by using the mass conservation law hewlett 2003 and an empirical method as introduced by yu and zhang 2017 respectively considering the constant submerged condition of ar structures in general only the full immersion tests as reported in the literature are considered according to maltais et al 2004 samson and marchand 2007 the thin shelled test samples were prepared as cylindrical disks of 70 mm in diameter and 10 mm in thickness the submission tests were carried out using a 30 l of solution tank filled with periodically renewed sodium sulfate na 2 so 4 solution to maintain a constant sulfate strength of 50 mmol l the test samples were sealed on the peripheral side and one circular face with silicon to ensure a one dimensional 1d attack scheme 4 1 2 modelling setups due to the utilisations of thin shelled cement paste samples prepared with high w c ratio the porous microstructure could generally lead to a decent amount of damages under the immersion in a high strength 50 mmol l sulfate solution within only a few months thus in the present study the maximum test duration in the numerical modelling is set as three months despite that samson and marchand 2007 have kept esa tests running for a period of 12 months given the purpose of the present study is to address the difficulty in stochastic esa modelling rather than to present a case study the shortened simulation time for the chemophysical fem modelling is considered to be beneficial towards the efficiency in terms of generating the crude mcs as reference as for the setups of fem along the ionic transportation path 1d linear element with resolution of 0 2 mm is adopted to discretise the governing equations by the galerkin method yu et al 2015 the time discretisation is performed by the implicit euler scheme and solved by the newton raphson method yu et al 2015 a time step of 30 minutes is adopted following the accuracy requirement as specified in the osa scheme yu and zhang 2017 for uncertainty quantification the reference of validations for the machine learning aided analyses comprises the crude mcs with n fun 10 4 of function evaluations so far no consensus on the determination of an exact n fun value has been reported where the chosen value represents the trade off between efficiency and accuracy considering the random nature of cementitious materials this study takes the material uncertainty into account as the major source of uncertainty in specific the hydrate composition is considered as independent random variables table 3 by doing so dependent random variables also present in the system such as porosity diffusivity and potential for cracking propagation it should be noted that the nature of this study does not lie in presenting a consensus on the statistical data on material uncertainty thus the distribution types and the coefficient of variation cov in table 3 are all presumed by ensuring the compatibility for actual cement systems the diverse distribution types are selected to eliminate any data sensitivity issue and maximise the complexity of stochastic modelling 4 2 clustering and classification according to the modelling framework as illustrated in section 3 the data structure of the stochastic esa responses should be analysed before the detailed surrogate models can be established herein by implementing the tgmm and drsvm the clustering and classification are performed first where the detailed data assessments are presented for type 10 and type 50 cements respectively as follows 4 2 1 type 10 cement due to the high afm aft ratio as shown in table 2 type 10 cement is generally low in sulfate resisting capability where more delayed aft formations and expansive cracking are expected through the crude mcs the stochastic response of the total solid sulfur element content in the system is compared against the reported experiment samson and marchand 2007 in fig 5 furthermore the detailed spatial distributions of sulfur bearing solid products such as afm aft and gypsum as obtained by the mcs are also demonstrated in fig 5 in fig 5 a the experiment fluctuates along the transportation path indicate the randomness of cementitious materials samson and marchand 2007 by utilising the proposed approach and the prescribed material uncertainty the mcs responses reasonably capture the trend further dissecting the mcs result the distributions of major sulfur bearing solid products are obtained in fig 5 b as shown in the figure the gypsum content is in accordance with the peaks of sulfur content in fig 5 a in addition the aft front also well corresponds to the front in fig 5 a it is called sulfate penetration front where expansive cracking is normally expected throughout the front for type 10 cement furthermore the deepest front is shown to be at around 5 mm with a resolution of 0 2 mm used in fem it equals to a maximum of 25 cracked elements corresponding to fig 3 by examining figs 3 and 5 the randomness in expansive cracking is shown to be associated with various combined effects depending on random inputs as listed in table 3 there are four main sources of material uncertainty thus the clustering and classification are not as straightforward as the preliminary demonstration by visual observation as exemplified in fig 3 following the modelling framework the problem appears to be multi dimensional including four random initial inputs one dependent random porosity and the n ce by using the tgmm clustering is performed with the numbers of training samples ranged from 50 to 500 a maximum of ten potential clusters are assumed and selected by aic and bic scores burnham and anderson 2004 for illustration the aic and bic scoring details for n s 100 and n s 500 are shown in figs 6 and 7 respectively in order to achieve an informed selection of clustering models four different categories of multivariate gaussians are adopted representing the combinations of two different forms and formats of covariance matrices in gaussians in figs 6 and 7 diagonal or full denotes whether an isotropic or anisotropic multivariate gaussian distribution is used moreover the shared indicates all the covariance matrices are restricted as the same while unshared indicates otherwise according to aic and bic scores it is obvious that the clustering achieved with unshared covariance matrices performs much better in term of the unshared category the choice of isotropic gaussians leads to a slightly better performance further examining the score gradients indicate a gentle development for a cluster size larger than four meaning the gain of accuracy is marginal with further increase in cluster size thus the ideal number of clusters should be around four by referencing back to the aic and bic scores the cluster size of four is chosen for n s 100 and five is chosen for n s 500 indicated by red outlines in figs 6 and 7 herein the clustering information for type 10 cement with different sizes of training sets is summarised in table 4 with the number of clusters determined it would be desirable to visualise the clustering details however due to the multi dimensional nature of the present problem it is difficult to showcase the obtained clusters under such a data structure herein for the purposes of visualisation and discussion of the obtained clusters three main dimensions including afm contents porosity and number of cracked elements n ce are chosen to visualise the clustering details in fig 8 it is demonstrated that the size of training samples affects the data structure and is thus crucial in determining the number of clusters moreover it is also shown that the number of cracked element n ce is dependent on the collective effects from the random hydrate contents and the corresponding randomness in porosity to extend the clustering from training set to the full spectrum uncertainty quantification a classification model is to be established following the modelling framework this is achieved by utilising the drsvm which is presented in section 4 3 4 2 2 type 50 cement based on the above discussion the clustering and classification for the esa on specimens prepared with type 50 cement under uncertainty are performed in a similar manner in general due to a much lower afm aft ratio table 2 type 50 cement possesses higher sulfate resistance as lesser delayed aft formation is expected this can be demonstrated from the crude mcs as shown in fig 9 by utilising the assumed material uncertainty the mcs guided stochastic responses capture the trend of experiment in fig 9 a again demonstrating the validity of the proposed method in addition by examining fig 9 b the improved sulfate resistance is demonstrated it is observed that a generally lower amount of delayed aft precipitation is formed along the path of sulfate penetration thus the potential cracking zone is limited to the area where the gypsum layer resides yu et al 2015 which significantly reduce the potential maximum number of cracking elements and alter the data structure following the developed framework a value of k 5 is selected for all the training sizes ranged from 50 to 500 exemplified using a case of n s 500 in fig 10 4 3 cluster wise regression for uncertainty quantification with the clustering and classification models obtained the establishment of a cluster wise regression model is followed as mentioned before the latest development in machine learning i e the xsvr yu et al 2019c 2020 is adopted as the core algorithm in the proposed sa xsvr herein the performance of sa xsvr in modelling stochastic esa problems under material uncertainty is examined in details where the ϵ svr the gaussian process regression gpr and the original xsvr are also implemented for comparisons to achieve a comprehensive study the sizes of training samples n s ranged from 50 to 500 are tested for all the adopted methods and a series of error metrics are applied see table 5 herein the stochastic analyses are presented in two parts including the stochastic modelling of the aft growth and the shrinkage of free to fill pore volume respectively 4 3 1 stochastic analysis on the accumulation of expansive aft the stochastic response regarding the aft growth for esa under material uncertainty is investigated first for the stochastic modelling of both type 10 and type 50 cements the performances of the proposed sa xsvr including efficiency and accuracy are preliminarily examined using the adopted error metrics and compared against other machine learning methods in figs 11 and 12 respectively according to fig 11 for the specific durability problem of esa on cementitious materials under marine conditions the conventional machine learning methods generally experience difficulties in surrogate modelling the stochastic response of the delayed aft growth including one of the latest developments in machine learning i e the xsvr all three previously reported methods suffer from relatively low r 2 for both the stochastic analyses on type 10 and type 50 cements moreover it is demonstrated that there are certain degrees of instability in the surrogate modelling from using ϵ svr gpr and xsvr where the values of r 2 fluctuate along the increases of training set n s as a result the highest r 2 values are often not the one with the largest training sets on the other hand the freshly developed sa xsvr directly targeting the esa issue is demonstrated to be a more stable and accurate surrogate method for both material applications the sa xsvr is more consistent with the increase of n s and reaches a much higher r 2 value at around 0 98 in fig 12 the rmse and re corresponding to the highest r 2 value from each algorithm are also compared which further demonstrate the overall better accuracy of the proposed sa xsvr method to fully demonstrate the superiority of a surrogate method using the above error metrics is not enough thus taking the modelling of type 10 cement as example detailed comparisons of the probability density functions pdf obtained from the utilised machine learning methods is presented in fig 13 moreover further accuracy comparisons based on the obtained cumulative distribution functions cdf are also presented for both types of cements in figs 14 and 15 respectively in fig 13 along with the demonstrations of the mcs and surrogate generated pdfs direct comparisons on the stochastic responses are also presented to be more specific the modelled results as showcased in fig 13 represent the corresponding sets with the highest r 2 from fig 11 according to fig 13 with the increases of r 2 values from the ϵ svr to the sa xsvr there is a visible trend of improvement in terms of the compliance of the surrogate generated pdf with the reference group correspondingly the compliance between surrogate estimations and mcs responses is also improved where the proposed sa xsvr is found to perform the best further examinations on the obtained cdfs in figs 14 and 15 reveal more details regarding the accuracy of the surrogate modelling for both types of cements the reported methods including ϵ svr gpr and xsvr all suffer from relatively poor accuracy on the other hand the proposed sa xsvr generates the cdfs of much improved accuracy especially for the case of type 50 cement the range of re distribution is much narrower than other reported machine learning methods overall by implementing the proposed sa xsvr the stochastic accumulation of the expansive aft can be estimated in a much more effective and efficient fashion 4 3 2 stochastic analysis on the shrinkage of free to fill pore volume in this section the stochastic response regarding the esa induced shrinkage of free to fill pore volume for accommodating the delayed precipitations of aft under uncertainty is investigated similar to section 4 3 1 for both cases of type 10 and type 50 cements the performances of the proposed sa xsvr are preliminarily examined through the adopted error metrics and compared against other machine learning methods in figs 16 and 17 from figs 16 and 17 the superior performance of the present sa xsvr method is again well demonstrated according to fig 16 despite that the accuracy of the sa xsvr may start lower than other reported methods for smaller training sizes n s 100 with a sufficient but still moderate amount of training data provided it is shown that the proposed approach is generally able to achieve a surrogate modelling with higher r 2 and lower error more detailed performance assessments of the proposed sa xsvr are achieved by examining the obtained pdf and cdf in details in figs 18 20 by showcasing the modelling of type 10 cement the obtained pdfs are compared in fig 18 the numerical results indicate the corresponding sets with the highest r 2 value as indicated in fig 16 herein in the stochastic analysis on the shrinkage of free to fill pore volumes all the adopted machine learning algorithms are shown to be able to achieve an effective surrogate modelling with decent agreement with the mcs reference especially for the proposed sa xsvr the agreement is exceptionally well where more detailed error analyses can be examined by figs 19 and 20 from figs 19 and 20 the surrogate modelling responses on both types of cements obtained through ϵ svr gpr and xsvr all suffer from a relatively wide range of re in the cdfs in comparison the present sa xsvr is demonstrated to perform very well in modelling the shrinkage of the free to fill pore volume with slim re ranges regardless of the types of materials such an improved accuracy in the surrogate generated cdfs is of fundamental significance in the reliability assessments of real life artificial reef structures made of cementitious materials as the probability of failure is normally very small within the designed service life 5 conclusions focusing on thin shell ars made of cementitious materials under external sulfate attack esa this study addresses the research gap in stochastic esa analyses considering inherent material uncertainty based on machine learning first the mechanism behind the underperformance of traditional machine learning aided stochastic esa analyses using chemophysical modelling is identified it is found to be resulted from the finite element method fem as adopted in describing the expansive cracking which causes a step wise feature in the time dependent response curves moreover the cracking of elements creates a discrete data structure due to the nature of finite element discretisation thus it is difficult to establish a unified surrogate model through conventional learning schemes to address this issue a novel self adaptive extended support vector regression sa xsvr framework is proposed and directly targets stochastic esa analyses the framework comprises three components including pattern recognition classification and regression in recognition an unsupervised method i e the tune gaussian mixture model tgmm is adopted for clustering in classification a doubly regularised support vector machine drsvm is utilised to construct the surrogate classification model based on clustering finally in regression the xsvr is adopted to construct surrogate regression models in a cluster wise manner the developed framework is applied to two types of cements i e type 10 and type 50 under the esa for both applications the proposed method is vigorously validated against the crude mcs by examining the stochastic responses of the aft accumulation and the shrinkage of free to fill pore volumes three common error metrics combined with pdf and cdf demonstrations are adopted to examine the accuracy and efficiency of the proposed method in addition three reported learning techniques i e ϵ svr gpr and the original xsvr are adopted to further highlight the advanced performance of the proposed sa xsvr overall the developed framework is demonstrated to be more effective and efficient for stochastic esa analyses under material uncertainty the improved accuracy is crucial for determining the small probability of failure in assessing the reliability of ar structures made of cementitious materials for future studies in view of that the undesirable step wise response feature is still rooted in the present framework focuses should be centralised on developing a more advanced mechanical model to describe the esa induced expansion and cracking propagation credit authorship contribution statement yuguo yu conceptualization methodology writing original draft wei gao writing original draft writing review editing supervision methodology arnaud castel writing review editing writing review editing airong liu software writing review editing xiaojun chen writing review editing data curation declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the research work presented in this paper has been supported by australian research council projects dp160103919 dp160104731 and ih150100006 appendix a matrices and vectors as involved in the optimisation problems the expressions of matrices c ˆ k g ˆ k and a ˆ k as well as the vectors b k e ˆ k d ˆ k and z k as involved in the optimisation problem of the kernelised x svr algorithm are given as follows a 1 c ˆ k λ 1 i m m λ 1 i m m c i m m c i m m a 2 g ˆ k 0 2 m 2 m 0 2 m m 0 2 m m 0 m 2 m i m m 0 m m 0 m 2 m 0 m m i m m a 3 a ˆ k 0 2 m m 0 2 m m 0 2 m 2 m k train k train 0 m 2 m k train k train 0 m 2 m a 4 b k e m e m 0 2 m e ˆ k 0 2 m e m e m d ˆ k 0 2 m y train y train z k p k q k ξ ξ ˆ 
21834,thin shell artificial reef ar structures with spatial internal volumes have demonstrated superior stock recruitment ability and material efficiency than many gravity based reef blocks and cementitious materials given the easy to tailor nature remains the most popular in reef constructions to date however under constant seawater immersion external sulfate attack esa introduces a major and uncertain reliability concern to this type of ar system due to the inherent material randomness this study is concerned with developing a novel stochastic modelling framework for assessing the esa under material uncertainty in this paper the main difficulty associated with the stochastic esa modelling is identified for the first time and a novel machine learning aided chemophysical modelling approach is proposed the performance of the developed framework is carefully examined through the analyses on two types of cementitious materials under esa keywords artificial reef chemophysical modelling external sulfate attack machine learning uncertainty quantification 1 introduction natural coral reefs primarily exploited for fishery tourism and coastal protection are of historical importance to many coastal regions around the world however this fundamental socio economic sector has become one of the first victims from the global climate changes where a large amount of natural reefs are losing stock recruitment capability and eventually dying hughes et al 2019 for the purposes of loss mitigation and habitat restoration a variety of artificial reefs ar constructed with diverse structural designs and materials have been widely implemented except for the early attempts of using gravity based solid blocks modern designs comprising thin walled shell structures with multiple cut openings become more appealing wang et al 2018 see fig 1 a the spatial internal volumes of these ar structures not only provide more shelters for sea lives but also contribute to material efficiency in large deployments for materials adopted in shell type ar structures concrete made of a variety of cementitious composites remains the most popular choice due to its easy to tailor nature accounting for 48 7 among all types of materials as reportedly utilised lima et al 2019 however under seawater submerged condition concrete may be subjected to diverse chemical attacks balestra et al 2019 wang et al 2019 yao and li 2019 causing material deterioration and loss of structural reliability one of the most detrimental durability issues is external sulfate attack esa yin et al 2017 yu et al 2018a which often leads to expansion cracking and even spalling of the surface concrete as shown in fig 1 b considering the thin walled feature of these ar structures the expansive cracking caused by esa may render the structure more prone to catastrophic failures than solid reef blocks although the esa under seawater and other sulfate enriched solution should be differentiated its origin can be described by the ingress of sulfate ion through the porous hydrated cement system santhanam et al 2006 as a very reactive media a variety of cement hydrates would participate in chemical reactions to retrieve the equilibrium as disturbed by ionic exchanges yu et al 2015 in specific leaching effect promotes the transformation of portlandite ch to gypsum which would be further interacted with monosulfates afm to precipitate the needle like ettringite aft according to experimental studies ragoug et al 2019 the gypsum is formed by through solution reaction on the pore surface causing the shrinkage of pore space while the needle like aft is precipitated by topochemical reactions and contributes directly to material expansion and cracking see fig 2 a a number of deterministic chemophysical approaches have also been reported for modelling the esa issue cefis and comi 2017 samson and marchand 2007 however except for the authors yu et al 2015 yu and zhang 2018a b few studies have distinguished the influence of these two different reaction mechanisms by adopting the authors method yu et al 2015 more accurate deterministic estimations on the elemental compositions under esa such as calcium and sulfur were achieved due to the better modelling of the expansive cracking process nonetheless the success in deterministic esa modelling does not guarantee the non deterministic counterpart that involves uncertainty quantifications firstly the chemophysical model represents a series of partial differential equations pdes and algebraic equations for ionic transportations and chemical reactions a closed form solution cannot be derived where only numerical approaches such as finite element method fem are feasible yu et al 2015 yu and zhang 2017 2018a b yu et al 2019a b under this circumstance the uncertainty quantification is forced to rely on the crude monte carlo simulation mcs of low efficiency yu et al 2018b thus the computational burden may often become intractable which inspires the surrogate modelling of using machine learning aided methods however also highlights the other research gaps in assessing esa under uncertainty according to fig 2 a the time dependent expansion appears to be a piecewise function due to the significant time required for the growth of gypsum and aft to cause stress build up expansion and cracking it becomes problematic when implementing the machine learning aided approaches for uncertainty quantification as the obtained stochastic responses at a given time frame may represent different status for instance three typical cases i e e1 e2 and e3 under the influence of material uncertainty are displayed in fig 2 b for illustrations e1 is shown to undergo the current expansion period while e2 is in the plateau of aft growth and e3 indicates the finish of the previous expansion to the authors knowledge it is difficult to address such a nonlinear relation by conventional surrogate modelling and few studies have investigated the esa involving uncertainty in this paper a novel machine learning strategy combined with the chemophysical modelling approach is developed focusing on the assessment of esa under uncertainty the mcs guided stochastic chemophysical modelling using fem and the inherent model instability are illustrated in section 2 targeting the unique data structures of the stochastic esa responses a self adaptive uncertainty quantification scheme based on the extended support vector regression xsvr feng et al 2019 is proposed in section 3 in section 4 the developed method is extensively examined by modelling the esa on two types of cements and comparing against other well established machine learning techniques finally some useful findings are reported in the conclusions of section 5 2 mcs guided stochastic chemophysical modelling of esa for real life concrete ar shells serving undersea the potential severity of esa may be subjected to the controls of competing aggressive agents and the associated uncertainty which however are out of the scope of this study herein only the pure esa is investigated and the accompanying cation of very limited aggressiveness i e sodium is concerned 2 1 preliminary similar to many other durability concerns esa is originated from the ingress of aggressive sulfate ions through the porous microstructure of cementitious materials to cause chemical reactions and further damages in numerical modelling such a process represents a reaction diffusion system formulated in partial differential equations pde by further incorporating uncertain factors the pde system evolves into a stochastic pde spde where a general mathematical form in space domain d and time domain t can be expressed using eq 1 1 u t div d grad u f u q u w u t where u d t ℜ indicates concentration vector containing diffusive agents mol m 3 of solution d is the diffusivity vector m 2 s the nonlinear operator f comprises the simultaneous reactions caused by the diffusive reactants q u is covariance w u is a vector of independent wiener processes and q u w u is the noise term describing the fluctuations in u d t due to uncertainty in general strict conditions for solving a spde are required allen 2012 as a promising approach hariharan and kannan 2014 wavelet method has been adopted to a few specific spde problems allen 2012 dan et al 2015 however a universal solution to a more general problem i e the deterioration modelling of cementitious materials has yet been reported so far herein uncertainty may lead to a covariance q u depending on the solution to u d t stundzia and lumsden 1996 while the nonlinear operator f may include the coupling effects other than chemical reactions for this case the spde bears the form of stochastic nernst planck np equation in eq 2 2 w s u s t w l u t div d w l grad u w l d z f r t u grad ψ d w l u grad ln γ d u ln γ u t grad t u d l grad r h w l r q u w u t where w s is volumetric solid content m 3 m 3 u s is reactant contents bound in solid products mol m 3 of solid matrix w l is volumetric liquid content m 3 m 3 z is valence numbers of diffusive reactants f is the faraday constant 96488 46 c mol r is the ideal gas constant 8 3143 j mol k t is thermodynamic temperature k ψ is electrical potential v γ is chemical activity coefficient calculated according to samson et al 1999 d l is diffusivity vector of liquid water m 2 s r h is relative humidity and r is source sink term according to eq 2 the solution to the stochastic np equation has to be achieved with solving other spdes and algebraic equations simultaneously due to the inclusions of other coupling effects such as electrical neutrality chemical activity and capillary advection to the authors knowledge no direct solution approach for such a complex problem has been developed where the non deterministic investigations under uncertainty have to rely on the crude mcs thus the strong yearning of improving computational efficiency inspires the implementation of diverse supporting approaches such as advanced sampling and machine learning method targeting the esa on thin shell ar structures a specialised machine learning aided technique is to be developed in section 3 herein the fundamentals of stochastic chemophysical modelling of esa are introduced first 2 2 stochastic multi species transportation modelling eq 2 represents a general form of stochastic np equation where specific modifications are required for the implementation in esa modelling firstly under the mcs guided scheme the noise term q u w u t can be eliminated yu et al 2019c moreover given the variety of chemical reactions under esa solving them simultaneously with ionic transportation is computationally intensive based on the authors yu et al 2015 yu and zhang 2018a b an operator splitting approach osa for reducing the computational effort is adopted through the osa the reaction related terms can also be omitted and solved independently using algebraic equations as illustrated in section 2 3 except for the reaction terms the corresponding advection term can also be omitted considering a constant submerged condition of ar structures in seawater in addition even though temperature variation in seawater may be common in engineering practices it is not considered here where an isothermal condition is assumed and the heat conduction term is omitted correspondingly furthermore as mentioned before cementitious materials under esa is prone to expansive cracking the impact of internal microcracks on the overall transportation mechanism should thus be considered which is achieved by including the so called cracking suction effect yu et al 2015 therefore the complete set of multi species transportation model for describing the esa under an isothermal and submerged condition is written in eq 3 which is to be solved with the poisson s equation of eq 4 for considering the electrical coupling effect and maintaining the system near the electroneutral state 3 w l u t div d w l grad u w l d z f r t u grad ψ d w l u grad ln γ d w l u grad c s i 0 4 div τ w l grad ψ f ε p w l z u 0 where τ is tortuosity vector ε p is the permittivity bearing the value of 7 08 10 10 c v m 2 and c s i indicates the cracking suction index calculated through yu et al 2015 for esa modelling to fully address the chemophysical degradation of hydrated cements under esa all the main ionic species comprising alkali ions and other diffusive reactants are considered including oh na k ca 2 so 4 2 al oh 4 and h 2 sio 4 2 2 3 chemical reaction modelling by implementing the osa yu et al 2015 the solution procedure to simultaneous chemical reactions is decoupled from ionic transportation according to yu et al 2019c within each run of mcs execution the evaluation of chemical reactions proceeds sequentially after solving the governing equations for ionic transportation it is achieved by following the theory of chemical thermodynamics where the chemical equilibrium model as given in eqs 5 6 is implemented 5 k s i 1 n 1 u i v s i γ i v s i j 1 n 2 u j v s j γ j v s j i 1 n 1 u i v s i j 1 n 2 u j v s j s 1 s 6 k p s s i u i ν i p s s γ i ν i p s s λ p s s δ p s s where k s is the equilibrium constant for the sth reaction denotes the chemical activity v s i and v s j are the stoichiometric coefficients for the ith product and the jth reactant in the sth reaction n 1 and n 2 denotes the total numbers of products and reactants from the sth reaction eq 6 is for evaluating the evolutions of solid solutions where the subscript p s s indicates solid p in solid solution s s k p s s is equilibrium constant of solid p in pure form δ p s s is molar fraction of solid p in solid solution λ p s s is activity coefficient which equals to 1 0 for ideal solid solution a variety of reactions can potentially occur under the esa which however is not necessarily essential in the numerical modelling considering the balance between accuracy and efficiency only the key heterogeneous reactions that may significantly contribute to the variations of microstructure and transportation mechanisms are included see table 1 as shown in the table the incongruent behaviour of the c s h gel is modelled as a solid solution with three phases in equilibrium yu et al 2015 yu and zhang 2017 i e jennite 1 5 cao sio 2 2 3 h 2 o calcium metasilicate cao sio 2 h 2 o and silica gel sio 2 in addition afm is also modelled as a solid solution where two types of afm phases are considered under the esa i e monosulfate and afm oh it is obvious that through the implementation of osa a total of eight degrees of freedom dofs from chemical reactions is separated from the modelling of ionic transportation which accounts for another seven dofs by doing so the computational effort of the mcs guided stochastic chemophysical modelling is significantly reduced 2 4 reaction induced damages and inherent model instability the observable damages of esa on hydrated cementitious materials generally consist of volumetric expansion and cracking these damages are resulted from the heterogeneous chemical reactions as analysed above and would lead to undesirable changes in both the mechanical and physical properties of ar structures herein as the first systematic stochastic study on the esa under uncertainty only the degradation in physical ionic diffusivity is concerned herein to do so the first step is to evaluate the change of material porosity which is easily calculated from the volumetric changes of all the heterogeneous reactions as listed in table 1 see eq 7 then the corresponding impact on the ionic diffusivity by porosity change is estimated through eq 8 samson and marchand 2007 7 ϕ ϕ 0 δ v s 8 h d ϕ ϕ e 4 3 ϕ v p e 4 3 ϕ 0 v p where ϕ is the real time porosity under chemical reactions ϕ 0 is the initial porosity δ v s indicates volumetric changes m 3 m 3 of heterogeneous reactions v p is volume fraction of cement paste in cementitious materials and h d ϕ is diffusivity multiplier as function of porosity with the porosity change being obtained the volumetric expansion under the esa can be estimated correspondingly as mentioned before the formations of the two expansive solid products i e gypsum and aft follow different mechanisms where only the precipitation of aft is potentially responsible for expansion and cracking of concrete therefore by adopting the empirical method as proposed by the authors yu et al 2015 the volumetric expansion and cracking due to the excessive amount of aft precipitation are estimated using the following equations 9 δ v s δ v s aft b ϕ v δ v s aft ϕ free v 10 ε δ v s v ε ε 3 for δ v s 0 11 c d k 1 ε th ε m for ε ε th where v indicates volume m 3 of the representative element volume rev b 1 is an empirical parameter accounting for the effective pore room to accommodate the needle like aft precipitations δ v s aft is volumetric growth m 3 of aft δ v s is volumetric expansion m 3 ε is volumetric strain ε is uniaxial strain assuming the material is isotropic macroscopically ε th is the threshold strain at which microcracking starts c d is a scalar value indicating crack density k and m are two tunning parameters according to the literature sarkar et al 2010 yu et al 2015 values of 0 36 0 16 2 3 and 2 10 4 are assigned to b k m and ε th respectively by obtaining the crack density the collective effects of porosity change and expansive cracking on ionic diffusivity can be estimated which is achieved by employing sarkar et al 2010 as follows 12 h d c c d 1 32 9 c d d p 13 d p 0 c d c dc 2 c dec c d c dec c dc 2 c d c dc c dc c d c dec c d c dec 14a h d h d ϕ h d c τ h d ϕ h d c τ h d ϕ h d c τ 14b d h d d 0 where h d c is diffusivity multiplier as function of crack density h d is overall diffusivity multiplier d 0 indicates material diffusivity at a sound state c dc 0 18 and c dec 0 56 are conduction percolation threshold and rigidity percolation threshold respectively yu et al 2015 according to the damage evaluation models as presented above the difficulty and inherent model instability in modelling esa under uncertainty can be clearly identified and explained as shown in eqs 9 11 only if the growth of aft overwhelms the available pore room for accommodation can the expansive strain occur in addition only if the expansive strain exceeds the threshold can cracks propagate even though these models are theoretically sound and reasonable they lead to piecewise developments of volumetric expansion as shown in fig 2 this phenomenon contributes to the non continuous development in material diffusivity which further influences the chemical reactions and the evaluated damages in the following sequences from the time domain by including uncertainty such as the inherent material uncertainty the present damage evaluation procedure designed for deterministic fem studies would generate even more issues due to the randomness within hydrate contents porosity and available pore volumes for accommodating solid precipitations as a result the stochastic analysis on esa aided with machine learning for reducing computational effort is not readily applicable which has yet been addressed in the literature 3 self adaptive uncertainty quantification based on machine learning focusing on effective and efficient stochastic esa analyses a self adaptive uncertainty quantification modelling framework based on the machine learning techniques is developed in this paper 3 1 self adaptive stochastic esa modelling framework according to the authors yu et al 2019c 2020 although the latest development in support vector regression svr i e the extended svr xsvr has demonstrated a significant superiority in the stochastic chemophysical modelling of diverse concrete durability issues xsvr along with other machine learning methods have yet been tested in the stochastic esa problems as mentioned before the fem based esa modelling is problematic due to the implemented damage evaluation method and the inherent finite element discretisation causing a piecewise response curve that could be difficult for performing machine learning aided stochastic analyses currently there is no universal solution to circumvent the difficulty in stochastic modelling of piecewise responses herein targeting the esa problem the authors propose an alternative approach consisting of modelling the stochastic responses of aft growth and shrinkage of free to fill pore volume considering the continuous developments of these two properties however the inherent model instability caused by the adopted damage model and finite element discretisation may still affect the overall responses in an implicit manner to better elaborate on this issue the cracking features from one numerical example in section 4 is displayed here in fig 3 fig 3 demonstrates two typical statistical distributions regarding the number of cracked elements n ce obtained from the mcs guided chemophysical fem modelling the mcs response of 10000 actual function evaluations n s is presented in fig 3 a where the data is not continuous in nature due to fem discretisation as to be presented in section 4 a direct surrogate model constructed through random sampling from such a data structure would suffer from low accuracy and instability regardless of the specific machine learning techniques as employed therefore in addition to the first proposition the authors also propose to divide the data into several clusters before constructing surrogate models in a cluster wise manner taking fig 3 a as an example three clusters may be roughly recognised by a visual observation over 10000 data points however detailed characteristics of clusters may vary depending on the number of samples as available for clustering see the case of n s 100 in fig 3 b this feature introduces another challenge in the stochastic esa analyses in order to resolve these difficulties a novel self adaptive stochastic esa modelling framework is developed in this paper as presented in fig 4 according to fig 4 there are five main steps to be carried out in order to fulfil an effective and efficient stochastic esa modelling where the first step is regarding the mcs guided chemophysical modelling which has been introduced in section 2 the mcs modelling with large number of function evaluations n s 10000 is taken as the reference in this paper a random subset with much fewer n s sampled from the full scale mcs would be generated for training the surrogate model based on machine learning in the following steps as demonstrated in the framework except for the last step of implementing the surrogate model the training of a self adaptive surrogate esa model is achieved by three consecutive procedures comprising unsupervised recognition supervised classification and cluster wise supervised regression which are illustrated in detail as follows 3 2 clustering with tune gaussian mixture model to the authors knowledge there are a variety of data driven techniques such as k means mixture model and neural network can be used to perform pattern recognition for clustering in view of both computational efficiency and accuracy the approach of fitting gaussian mixture models gmm using expectation maximisation em algorithm is adopted in the present study the combined em gmm is deemed as the fastest algorithm for learning mixture models which however may only ensure the convergence to a local optimum bishop 2006 even though this issue can be resolved through the variational bayesian gaussian mixture models it is not preferable due to that it may introduce many implicit biases to the cluster size and the covariance matrix structure on the contrary em maximises only the likelihood without bias and all types of structures for the covariance matrices can be examined and treated equally bishop 2006 in addition the concern about local optimum can be empirically resolved by repeating the process with different initiation points vanderplas 2016 moreover the model selection criteria such as the akaike information criterion aic and bayesian information criterion bic can be applied to further penalise for model complexity so as to determine the most suitable model and number of clusters burnham and anderson 2004 furthermore in the specific application to the study of esa a finite set of models with a presumable maximum cluster number can be generally assumed therefore the em gmm algorithm penalised by using aic and bic is adopted herein which is called the tune gaussian mixture model tgmm in general utilising tgmm for pattern recognition and clustering starts with the e step from the em algorithm to be more specific the e step is initialised based on the number of clusters k as presumed for fitting the data the gaussian mixture distribution p x as a combination of k gaussians with the weight π is written in eq 15 the corresponding posterior distribution of the responsibility that each gaussian has for each data point x i from x ℜ n is then calculated by eq 16 according to the expression of eq 16 a higher value of γ i k would be obtained if x i is very close to one gaussian k and relatively low values otherwise 15 p x k 1 k π k n x μ k σ k 16 γ i k π k n x i μ k σ k j 1 k π j n x i μ j σ j where n x μ σ denotes the multivariate gaussian with mean μ and covariance σ x contain the information of all the random input variables as well as n ce according to fig 4 after obtaining the posterior the m step is carried out by calculating m k and the corresponding weight for each cluster k in eq 17 then the new set of mean μ k new and covariance σ k new for cluster k is updated with the γ i k in eqs 18 19 bishop 2006 repeating the e and the m steps until the convergence criterion for the log likelihood i e eq 20 is met where the corresponding model cost can then be calculated through a cost function as given in eq 21 bishop 2006 17 m k i 1 n γ i k π k m k n 18 μ k new 1 m k i 1 n γ i k x i 19 σ k new 1 m k i 1 n γ i k x i μ k new t x i μ k new 20 ln p x π μ σ i 1 n ln k 1 k π k n x i μ k σ k 21 ℂ k i x i k μ k 2 in general the convergence of the em algorithm is always guaranteed while the cost also decreases monotonically with the increase of the presumed number of clusters as a result there is another essential step in implementing the tgmm i e model selection otherwise the clustering would always lead to the model with the maximum presumed cluster number which clearly is undesirable in terms of achieving an efficient stochastic analysis in the present study the model selection is achieved by penalising the model for increasing complexity by examining both the aic and bic values burnham and anderson 2004 moreover for models with comparable aic and bic values the corresponding gradients will be calculated to ensure an informed decision making herein the general principle is to favour the simpler model with fewer numbers of clusters to maintain computational efficiency more details on the model selection process are discussed by specific problems in the stochastic esa modelling presented in section 4 3 3 classification with support vector machine after clustering the training data through pattern recognition over the combined data x containing the information of n ce and all the random input variables classification based on only the random input variables are carried out it should be noticed that n ce is not a part of the input variables which is an intermediate response obtained through the mcs guided chemophysical modelling thus it should neither be included in the input nor the response and the response for the training of the classification model is the cluster information c obtained from the previous pattern recognition herein a doubly regularised support vector machine drsvm is implemented for classification in this paper li et al 2016 wang et al 2006 the general expression can be written as 22 min w γ λ 1 2 w 2 2 λ 2 w 1 i 1 m 1 c train i f ˆ c x i 23 1 c train i f ˆ c x i 0 1 c train i f ˆ c x i 0 1 c train i f ˆ c x i 1 c train i f ˆ c x i 0 where λ 1 and λ 2 are two tunning parameters controlling the balance between classification and feature selection γ ℜ denotes bias w ℜ n denotes the normal to the hyperplane created by f ˆ c indicating the classification model to be trained 1 and 2 represent the l 1 and l 2 norms and 1 c train i f ˆ c x i denotes the hinge loss function as defined in eq 23 in order to solve the optimisation problem as formulated in eq 22 dunbar et al 2010 proposed to utilise two non negative variables p q ℜ n to describe w ℜ n as w p q by doing so l 1 and l 2 norms can be decomposed and calculated in eqs 24 25 24 w 1 e n t p q e n 1 1 1 t ℜ n 25 w 2 2 p q 2 2 p 2 2 q 2 2 2 p t q p 2 2 q 2 2 the decompositions of l 1 and l 2 norms have been proven to be an effective approach in nonlinear optimisation fung and mangasarian 2004 furthermore the hinge loss function can also be replaced by a linear constraint to consider the noise in the training data where a non negative slack variable ξ ℜ m is adopted for error control by doing so the drsvm as expressed in eq 22 can be rewritten into eq 26 and solved as a quadratic programming problem using the lagrange method dunbar et al 2010 as a result the trained classification model is expressed using eq 27 26a min p q γ ξ λ 1 2 p 2 2 q 2 2 λ 2 e n t p q e m t ξ 26b s t d x train p q γ e m ξ e m p q 0 n ξ 0 m 27 f ˆ c x x t p q γ where 0 m 0 0 0 t ℜ m and d ℜ m m denotes a diagonal matrix containing the cluster labels from c train that are associated with the input data used for training the classification model f ˆ c x 3 4 cluster wise regression with xsvr the last major step in realising the self adaptive uncertainty quantification of the esa problem involves with constructing surrogate models based on the latest regression technique of xsvr feng et al 2019 herein with the obtained cluster information c the xsvr is applied to each respective cluster with the training data of all the random input variables that belong to the corresponding cluster moreover according to the specific features to be studied either the growth of aft δ v s aft or the shrinkage of free to fill pore volume ϕ free is chosen as the stochastic response for training the regression model due to that the xsvr is implemented in a cluster wise manner based on the clustering information as generated through tgmm and drsvm the uncertainty quantification achieved by such a self adaptive procedure is called the sa xsvr in the following context herein the key formulations regarding the xsvr are introduced where more detailed model derivations are referred to the authors feng et al 2019 yu et al 2019c 2020 the xsvr represents a direct extension of the previously applied drsvm from classification to regression unlike the ε insensitive support vector regression ϵ svr xsvr utilises a quadratic ϵ insensitive loss function to enhance the numerical stability moreover a mapping function φ x is introduced to map the raw variables from the low dimensional space ℜ n into a higher dimensional feature space f this mapping approach is particularly effective in the application to the nonlinear multiphysics problems yu et al 2020 leading to the development of kernelised xsvr algorithm in this paper the empirical kernel mapping is adopted and written in eq 28 schölkopf et al 1999 28a x i x i 1 x i 2 x i n t k ˆ x i φ x 1 t φ x i φ x 2 t φ x i φ x m t φ x i k x 1 x i k x 2 x i k x m x i 28b k x i x j φ x i t φ x j where k x i x j denotes kernel function k ˆ x i is the m degree empirical feature vector equalling to the number of training sets n indicates the number of different input variables after kernel mapping the empirical feature vector k ˆ x i is used to train the surrogate model provided with the training dataset x t r a i n and a kernel function of any acceptable types the original input is mapped into a kernel matrix k train ℜ m m in eq 29 and implemented to formulate the kernelised xsvr as expressed in eq 30 29 k train k x 1 x 1 k x 1 x 2 k x 1 x m k x 2 x 1 k x 2 x 2 k x 2 x m k x m x 1 k x m x 2 k x m x m 30a min p k q k γ ξ ξ ˆ λ 1 2 p k 2 2 q k 2 2 λ 2 e m t p k q k c 2 ξ t ξ ξ ˆ t ξ ˆ 30b s t k train p k q k γ e m y t r a i n ε e m ξ y train k train p k q k γ e m ε e m ξ ˆ p k q k ξ ξ ˆ 0 m where ξ ˆ is the redundant non negative constraint for slack variables e m 1 1 1 t ℜ m c 0 is a penalty constant y train ℜ is the actual responses from the training group ε is the tolerable deviation between y train and prediction from the surrogate regression model f ˆ r x t r a i n and p k q k ℜ m are two non negative variables defined by feng et al 2019 the subscript k here denotes a kernelised learning algorithm similar to the optimisation concept as adopted in the drsvm the kernelised xsvr can be rewritten in eq 31 and also be solved as a quadratic programming problem by incorporating a non negative lagrange multiplier u k ℜ 4 m using eq 32 31a min z k γ 1 2 z k t c ˆ k z k γ 2 λ 2 b k t z k 31b s t a ˆ k i 4 m 4 m z k ε i 4 m 4 m γ g ˆ k e ˆ k d ˆ k 0 4 m 32a min u k 1 2 u k t q k u k m k t u k 32b s t u k 0 4 m where i 4 m 4 m ℜ 4 m 4 m denotes an identity matrix the matrix expressions of c ˆ k g ˆ k a ˆ k b k e ˆ k d ˆ k and z k are presented in appendix a while q k and m k are calculated as follows 33a q k a ˆ k i 4 m 4 m c ˆ k 1 a ˆ k i 4 m 4 m t g ˆ k e ˆ k e ˆ k t g ˆ k q k ℜ 4 m 4 m 33b m k λ 2 a ˆ k i 4 m 4 m c ˆ k 1 b k ε e ˆ k d ˆ k since the obtained solution is based on optimisation whether the kernelised xsvr indicates a global or a local minimum is a subject requires examination according to feng et al 2019 the kernelised xsvr has been proven to always lead to the global minimum solution regardless of the specific kernel function as adopted by further assuming u k ℜ 4 m as a solution to eq 32 the variables z k γ and coefficient w can be written in eqs 34 36 and the surrogate regression model as obtained from the kernelised x svr algorithm is expressed by eq 37 34 z k c ˆ k 1 a ˆ k i 4 m 4 m t u k λ 2 b k 35 γ e ˆ k t g ˆ k u k 36 w p k q k z k 1 m z k m 1 2 m 37 f ˆ r x p k q k t k x e ˆ k t g ˆ k u k even though the convexity of the xsvr holds regardless of the kernel functions as adopted for mapping the performance of the surrogate model still depends on the kernel functions as chosen according to yu et al 2020 conventional kernels such as polynomial and gaussian kernels may suffer from low accuracy when modelling chemophysical problems due to the non orthonormal bases it can be resolved by implementing orthogonal kernels such as the gegenbauer polynomial feng et al 2019 tian and wang 2017 which has been successfully implemented in modelling the leaching of cementitious materials yu et al 2020 thus for the suggested ranges for hyperparameters one can refer to the authors feng et al 2019 yu et al 2020 to assess the goodness of parameter selections within the proposed range a 5 fold cross validation is adopted and bayesian optimisation method is implemented for the automatic selections of hyperparameters vapnik 2013 4 modelling the esa under uncertainty the main focus of the present study lies in addressing the difficulty in the machine learning aided uncertainty quantification of esa caused by the stepwise cracking propagation and fem discretisation in modelling thus other potential durability effects for ar structures such as chloride ingress and biogenic degradations are not concerned with the following analyses 4 1 problem descriptions and modelling setups 4 1 1 problem descriptions in order to create a comprehensive database for performing the stochastic analyses the design of samples is carried out with care herein the reported esa tests on cement pastes prepared with high water to cement ratio w c 0 6 are adopted maltais et al 2004 samson and marchand 2007 considering that a decent amount of material decay can be created on pastes within an acceptable timeframe choosing such an experimental dataset would not only benefit the following crude mcs guided chemophysical modelling but also eliminate the influence of interfacial transition zone to the overall expansive cracking process which is out of the specific scope of the present study in details the cement paste specimens prepared with two types of cementitious materials i e a canadian type 10 of ordinary portland cement and a canadian type 50 of sulfate resisting cement are adopted to construct the present stochastic analyses for the factory characterisations of chemical and mineralogical compositions of these two types of cements one may refer to the reported literature maltais et al 2004 samson and marchand 2007 herein the material properties for carrying out the chemophysical modelling comprising porosity ionic diffusivity initial solution and hydrate compositions are summarised in table 2 the material properties as presented in table 2 can be regarded as the mean values without considering material uncertainty and have been verified by hydration models yu et al 2015 in details porosity and ionic diffusivity were validated by empirical models as reported in jennings and tennis 1994 oh and jang 2004 the initial hydrate and pore solution compositions are examined by using the mass conservation law hewlett 2003 and an empirical method as introduced by yu and zhang 2017 respectively considering the constant submerged condition of ar structures in general only the full immersion tests as reported in the literature are considered according to maltais et al 2004 samson and marchand 2007 the thin shelled test samples were prepared as cylindrical disks of 70 mm in diameter and 10 mm in thickness the submission tests were carried out using a 30 l of solution tank filled with periodically renewed sodium sulfate na 2 so 4 solution to maintain a constant sulfate strength of 50 mmol l the test samples were sealed on the peripheral side and one circular face with silicon to ensure a one dimensional 1d attack scheme 4 1 2 modelling setups due to the utilisations of thin shelled cement paste samples prepared with high w c ratio the porous microstructure could generally lead to a decent amount of damages under the immersion in a high strength 50 mmol l sulfate solution within only a few months thus in the present study the maximum test duration in the numerical modelling is set as three months despite that samson and marchand 2007 have kept esa tests running for a period of 12 months given the purpose of the present study is to address the difficulty in stochastic esa modelling rather than to present a case study the shortened simulation time for the chemophysical fem modelling is considered to be beneficial towards the efficiency in terms of generating the crude mcs as reference as for the setups of fem along the ionic transportation path 1d linear element with resolution of 0 2 mm is adopted to discretise the governing equations by the galerkin method yu et al 2015 the time discretisation is performed by the implicit euler scheme and solved by the newton raphson method yu et al 2015 a time step of 30 minutes is adopted following the accuracy requirement as specified in the osa scheme yu and zhang 2017 for uncertainty quantification the reference of validations for the machine learning aided analyses comprises the crude mcs with n fun 10 4 of function evaluations so far no consensus on the determination of an exact n fun value has been reported where the chosen value represents the trade off between efficiency and accuracy considering the random nature of cementitious materials this study takes the material uncertainty into account as the major source of uncertainty in specific the hydrate composition is considered as independent random variables table 3 by doing so dependent random variables also present in the system such as porosity diffusivity and potential for cracking propagation it should be noted that the nature of this study does not lie in presenting a consensus on the statistical data on material uncertainty thus the distribution types and the coefficient of variation cov in table 3 are all presumed by ensuring the compatibility for actual cement systems the diverse distribution types are selected to eliminate any data sensitivity issue and maximise the complexity of stochastic modelling 4 2 clustering and classification according to the modelling framework as illustrated in section 3 the data structure of the stochastic esa responses should be analysed before the detailed surrogate models can be established herein by implementing the tgmm and drsvm the clustering and classification are performed first where the detailed data assessments are presented for type 10 and type 50 cements respectively as follows 4 2 1 type 10 cement due to the high afm aft ratio as shown in table 2 type 10 cement is generally low in sulfate resisting capability where more delayed aft formations and expansive cracking are expected through the crude mcs the stochastic response of the total solid sulfur element content in the system is compared against the reported experiment samson and marchand 2007 in fig 5 furthermore the detailed spatial distributions of sulfur bearing solid products such as afm aft and gypsum as obtained by the mcs are also demonstrated in fig 5 in fig 5 a the experiment fluctuates along the transportation path indicate the randomness of cementitious materials samson and marchand 2007 by utilising the proposed approach and the prescribed material uncertainty the mcs responses reasonably capture the trend further dissecting the mcs result the distributions of major sulfur bearing solid products are obtained in fig 5 b as shown in the figure the gypsum content is in accordance with the peaks of sulfur content in fig 5 a in addition the aft front also well corresponds to the front in fig 5 a it is called sulfate penetration front where expansive cracking is normally expected throughout the front for type 10 cement furthermore the deepest front is shown to be at around 5 mm with a resolution of 0 2 mm used in fem it equals to a maximum of 25 cracked elements corresponding to fig 3 by examining figs 3 and 5 the randomness in expansive cracking is shown to be associated with various combined effects depending on random inputs as listed in table 3 there are four main sources of material uncertainty thus the clustering and classification are not as straightforward as the preliminary demonstration by visual observation as exemplified in fig 3 following the modelling framework the problem appears to be multi dimensional including four random initial inputs one dependent random porosity and the n ce by using the tgmm clustering is performed with the numbers of training samples ranged from 50 to 500 a maximum of ten potential clusters are assumed and selected by aic and bic scores burnham and anderson 2004 for illustration the aic and bic scoring details for n s 100 and n s 500 are shown in figs 6 and 7 respectively in order to achieve an informed selection of clustering models four different categories of multivariate gaussians are adopted representing the combinations of two different forms and formats of covariance matrices in gaussians in figs 6 and 7 diagonal or full denotes whether an isotropic or anisotropic multivariate gaussian distribution is used moreover the shared indicates all the covariance matrices are restricted as the same while unshared indicates otherwise according to aic and bic scores it is obvious that the clustering achieved with unshared covariance matrices performs much better in term of the unshared category the choice of isotropic gaussians leads to a slightly better performance further examining the score gradients indicate a gentle development for a cluster size larger than four meaning the gain of accuracy is marginal with further increase in cluster size thus the ideal number of clusters should be around four by referencing back to the aic and bic scores the cluster size of four is chosen for n s 100 and five is chosen for n s 500 indicated by red outlines in figs 6 and 7 herein the clustering information for type 10 cement with different sizes of training sets is summarised in table 4 with the number of clusters determined it would be desirable to visualise the clustering details however due to the multi dimensional nature of the present problem it is difficult to showcase the obtained clusters under such a data structure herein for the purposes of visualisation and discussion of the obtained clusters three main dimensions including afm contents porosity and number of cracked elements n ce are chosen to visualise the clustering details in fig 8 it is demonstrated that the size of training samples affects the data structure and is thus crucial in determining the number of clusters moreover it is also shown that the number of cracked element n ce is dependent on the collective effects from the random hydrate contents and the corresponding randomness in porosity to extend the clustering from training set to the full spectrum uncertainty quantification a classification model is to be established following the modelling framework this is achieved by utilising the drsvm which is presented in section 4 3 4 2 2 type 50 cement based on the above discussion the clustering and classification for the esa on specimens prepared with type 50 cement under uncertainty are performed in a similar manner in general due to a much lower afm aft ratio table 2 type 50 cement possesses higher sulfate resistance as lesser delayed aft formation is expected this can be demonstrated from the crude mcs as shown in fig 9 by utilising the assumed material uncertainty the mcs guided stochastic responses capture the trend of experiment in fig 9 a again demonstrating the validity of the proposed method in addition by examining fig 9 b the improved sulfate resistance is demonstrated it is observed that a generally lower amount of delayed aft precipitation is formed along the path of sulfate penetration thus the potential cracking zone is limited to the area where the gypsum layer resides yu et al 2015 which significantly reduce the potential maximum number of cracking elements and alter the data structure following the developed framework a value of k 5 is selected for all the training sizes ranged from 50 to 500 exemplified using a case of n s 500 in fig 10 4 3 cluster wise regression for uncertainty quantification with the clustering and classification models obtained the establishment of a cluster wise regression model is followed as mentioned before the latest development in machine learning i e the xsvr yu et al 2019c 2020 is adopted as the core algorithm in the proposed sa xsvr herein the performance of sa xsvr in modelling stochastic esa problems under material uncertainty is examined in details where the ϵ svr the gaussian process regression gpr and the original xsvr are also implemented for comparisons to achieve a comprehensive study the sizes of training samples n s ranged from 50 to 500 are tested for all the adopted methods and a series of error metrics are applied see table 5 herein the stochastic analyses are presented in two parts including the stochastic modelling of the aft growth and the shrinkage of free to fill pore volume respectively 4 3 1 stochastic analysis on the accumulation of expansive aft the stochastic response regarding the aft growth for esa under material uncertainty is investigated first for the stochastic modelling of both type 10 and type 50 cements the performances of the proposed sa xsvr including efficiency and accuracy are preliminarily examined using the adopted error metrics and compared against other machine learning methods in figs 11 and 12 respectively according to fig 11 for the specific durability problem of esa on cementitious materials under marine conditions the conventional machine learning methods generally experience difficulties in surrogate modelling the stochastic response of the delayed aft growth including one of the latest developments in machine learning i e the xsvr all three previously reported methods suffer from relatively low r 2 for both the stochastic analyses on type 10 and type 50 cements moreover it is demonstrated that there are certain degrees of instability in the surrogate modelling from using ϵ svr gpr and xsvr where the values of r 2 fluctuate along the increases of training set n s as a result the highest r 2 values are often not the one with the largest training sets on the other hand the freshly developed sa xsvr directly targeting the esa issue is demonstrated to be a more stable and accurate surrogate method for both material applications the sa xsvr is more consistent with the increase of n s and reaches a much higher r 2 value at around 0 98 in fig 12 the rmse and re corresponding to the highest r 2 value from each algorithm are also compared which further demonstrate the overall better accuracy of the proposed sa xsvr method to fully demonstrate the superiority of a surrogate method using the above error metrics is not enough thus taking the modelling of type 10 cement as example detailed comparisons of the probability density functions pdf obtained from the utilised machine learning methods is presented in fig 13 moreover further accuracy comparisons based on the obtained cumulative distribution functions cdf are also presented for both types of cements in figs 14 and 15 respectively in fig 13 along with the demonstrations of the mcs and surrogate generated pdfs direct comparisons on the stochastic responses are also presented to be more specific the modelled results as showcased in fig 13 represent the corresponding sets with the highest r 2 from fig 11 according to fig 13 with the increases of r 2 values from the ϵ svr to the sa xsvr there is a visible trend of improvement in terms of the compliance of the surrogate generated pdf with the reference group correspondingly the compliance between surrogate estimations and mcs responses is also improved where the proposed sa xsvr is found to perform the best further examinations on the obtained cdfs in figs 14 and 15 reveal more details regarding the accuracy of the surrogate modelling for both types of cements the reported methods including ϵ svr gpr and xsvr all suffer from relatively poor accuracy on the other hand the proposed sa xsvr generates the cdfs of much improved accuracy especially for the case of type 50 cement the range of re distribution is much narrower than other reported machine learning methods overall by implementing the proposed sa xsvr the stochastic accumulation of the expansive aft can be estimated in a much more effective and efficient fashion 4 3 2 stochastic analysis on the shrinkage of free to fill pore volume in this section the stochastic response regarding the esa induced shrinkage of free to fill pore volume for accommodating the delayed precipitations of aft under uncertainty is investigated similar to section 4 3 1 for both cases of type 10 and type 50 cements the performances of the proposed sa xsvr are preliminarily examined through the adopted error metrics and compared against other machine learning methods in figs 16 and 17 from figs 16 and 17 the superior performance of the present sa xsvr method is again well demonstrated according to fig 16 despite that the accuracy of the sa xsvr may start lower than other reported methods for smaller training sizes n s 100 with a sufficient but still moderate amount of training data provided it is shown that the proposed approach is generally able to achieve a surrogate modelling with higher r 2 and lower error more detailed performance assessments of the proposed sa xsvr are achieved by examining the obtained pdf and cdf in details in figs 18 20 by showcasing the modelling of type 10 cement the obtained pdfs are compared in fig 18 the numerical results indicate the corresponding sets with the highest r 2 value as indicated in fig 16 herein in the stochastic analysis on the shrinkage of free to fill pore volumes all the adopted machine learning algorithms are shown to be able to achieve an effective surrogate modelling with decent agreement with the mcs reference especially for the proposed sa xsvr the agreement is exceptionally well where more detailed error analyses can be examined by figs 19 and 20 from figs 19 and 20 the surrogate modelling responses on both types of cements obtained through ϵ svr gpr and xsvr all suffer from a relatively wide range of re in the cdfs in comparison the present sa xsvr is demonstrated to perform very well in modelling the shrinkage of the free to fill pore volume with slim re ranges regardless of the types of materials such an improved accuracy in the surrogate generated cdfs is of fundamental significance in the reliability assessments of real life artificial reef structures made of cementitious materials as the probability of failure is normally very small within the designed service life 5 conclusions focusing on thin shell ars made of cementitious materials under external sulfate attack esa this study addresses the research gap in stochastic esa analyses considering inherent material uncertainty based on machine learning first the mechanism behind the underperformance of traditional machine learning aided stochastic esa analyses using chemophysical modelling is identified it is found to be resulted from the finite element method fem as adopted in describing the expansive cracking which causes a step wise feature in the time dependent response curves moreover the cracking of elements creates a discrete data structure due to the nature of finite element discretisation thus it is difficult to establish a unified surrogate model through conventional learning schemes to address this issue a novel self adaptive extended support vector regression sa xsvr framework is proposed and directly targets stochastic esa analyses the framework comprises three components including pattern recognition classification and regression in recognition an unsupervised method i e the tune gaussian mixture model tgmm is adopted for clustering in classification a doubly regularised support vector machine drsvm is utilised to construct the surrogate classification model based on clustering finally in regression the xsvr is adopted to construct surrogate regression models in a cluster wise manner the developed framework is applied to two types of cements i e type 10 and type 50 under the esa for both applications the proposed method is vigorously validated against the crude mcs by examining the stochastic responses of the aft accumulation and the shrinkage of free to fill pore volumes three common error metrics combined with pdf and cdf demonstrations are adopted to examine the accuracy and efficiency of the proposed method in addition three reported learning techniques i e ϵ svr gpr and the original xsvr are adopted to further highlight the advanced performance of the proposed sa xsvr overall the developed framework is demonstrated to be more effective and efficient for stochastic esa analyses under material uncertainty the improved accuracy is crucial for determining the small probability of failure in assessing the reliability of ar structures made of cementitious materials for future studies in view of that the undesirable step wise response feature is still rooted in the present framework focuses should be centralised on developing a more advanced mechanical model to describe the esa induced expansion and cracking propagation credit authorship contribution statement yuguo yu conceptualization methodology writing original draft wei gao writing original draft writing review editing supervision methodology arnaud castel writing review editing writing review editing airong liu software writing review editing xiaojun chen writing review editing data curation declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the research work presented in this paper has been supported by australian research council projects dp160103919 dp160104731 and ih150100006 appendix a matrices and vectors as involved in the optimisation problems the expressions of matrices c ˆ k g ˆ k and a ˆ k as well as the vectors b k e ˆ k d ˆ k and z k as involved in the optimisation problem of the kernelised x svr algorithm are given as follows a 1 c ˆ k λ 1 i m m λ 1 i m m c i m m c i m m a 2 g ˆ k 0 2 m 2 m 0 2 m m 0 2 m m 0 m 2 m i m m 0 m m 0 m 2 m 0 m m i m m a 3 a ˆ k 0 2 m m 0 2 m m 0 2 m 2 m k train k train 0 m 2 m k train k train 0 m 2 m a 4 b k e m e m 0 2 m e ˆ k 0 2 m e m e m d ˆ k 0 2 m y train y train z k p k q k ξ ξ ˆ 
