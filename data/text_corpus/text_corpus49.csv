index,text
245,despite the existence of several stochastic streamflow generators not much attention has been given to representing the impacts of large scale climate indices on seasonal to interannual streamflow variability by merging a formal predictor selection scheme with vine copulas we propose a generic approach to explicitly incorporate large scale climate indices in ensemble streamflow generation at single and multiple sites and in both short term prediction and long term projection modes the proposed framework is applied at three headwater streams in the oldman river basin in southern alberta canada the results demonstrate higher skills than existing models both in terms of representing intra and inter annual variability as well as accuracy and predictability of streamflow particularly during high flow seasons the proposed algorithm presents a globally relevant scheme for the stochastic streamflow generation where the impacts of large scale climate indices on streamflow variability across time and space are significant keywords streamflow variability large scale climate indices stochastic streamflow generation predictor selection vine copulas algorithm development 1 introduction streamflow has been often represented as a function of other hydroclimatic processes such as temperature precipitation and evapotranspiration at the catchment scale blöschl et al 2007 these variables are affected by large scale climate patterns merz et al 2014 steirou et al 2017 tan and gan 2017 which can consequently impact streamflow generation globally kisi et al 2019 konapala et al 2018 ward et al 2014 for instance various evidences show that large scale climate indices lscis most notably el niño southern oscillation enso interdecadal pacific oscillation ipo and pacific decadal oscillation pdo influence the streamflow in australia mcgowan et al 2009 murphy and timbal 2008 pui and sharma 2011 pui et al 2012 enso north atlantic oscillation nao and atlantic multidecadal oscillation amo affect european streamflow giuntoli et al 2013 steirou et al 2017 similarly enso pdo amo and nao impact streamflow in north america asong et al 2018 nazemi et al 2017 rajagopalan et al 2000 tamaddun et al 2017 2019 wu et al 2020 previous studies have made it clear that taking into account the effects of lscis directly on streamflow or indirectly through affected hydroclimate variables e g temperature and precipitation may improve the predictability of streamflow particularly at seasonal to interannual scales e g kiem et al 2021 kwon et al 2008 steinschneider et al 2019 wasko and sharma 2017 the indirect incorporation of lsci in the generation of streamflow is in fact very common in the context of process based models in which variables such as temperature and precipitation are the basis of simulating streamflow eisner et al 2017 shrestha et al 2013 su et al 2017 process based models however are deterministically formulated by implementing physically based and or conceptual equations without explicitly considering the distributional and or joint properties of observed data montanari and koutsoyiannis 2012 farmer and vogel 2016 past studies showed that although dependence between precipitation and lscis can be low lscis have more consistent impacts on streamflow and or temperature nalley et al 2016 nazemi et al 2017 in particular the direct dependencies between streamflow and lscis in coarser spatial and temporal scales are rather strong this has motivated a strain of modeling attempts to explicitly incorporate the effect of lscis in streamflow generation through stochastic approaches lee et al 2018a liu et al 2015 wang et al 2009 one way to represent the impact of lscis on stochastic streamflow generation is to transform the original data into a gaussian process that can be then described using multivariate joint distributions bennett et al 2014 papalexiou 2018 wang and robertson 2011 the simplest representation of such kind can be formed by assuming a symmetric and linear teleconnection between streamflow and lscis linear models such as autoregressive ar and its variants have been widely used for streamflow simulation matalas 1967 salas et al 1985 lee et al 2010 prairie et al 2008 however they are unable to adequately represent marginal streamflow distributions especially in the case of asymmetric nonlinear and multimodal conditions papalexiou 2018 papalexiou and serinaldi 2020 rajogopalan et al 2019 which is the case in many regions and or finer timescales fleming and dahlke 2014 hlinka et al 2014 khan et al 2006 konapala and mishra 2016 lee et al 2018b nonparametric resampling schemes can address some of the issues in linear models lall and sharma 1996 sharma and o neill 2002 however the generated flows may end up being too close to the reshuffling of historical sequences e g grantz et al 2005 lee et al 2010 over the last two decades copula based models see genest and favre 2007 nelsen 2007 have gained popularity in hydroclimatology aghakouchak 2014 nazemi and elshorbagy 2012 and have been applied in various contexts including stochastic streamflow generation salvadori and de michele 2004 worland et al 2019 zhang and singh 2019 copulas offer a generic solution to multivariate probabilistic sampling particularly with respect to quantifying the risk chen et al 2015 hao and singh 2013 serinaldi and kilsby 2017 application of copulas in streamflow generation can provide an opportunity for preserving dependence structures in time and space and or between streamflow and other relevant variables copula based stochastic streamflow generations have been used at the single site quite extensively and are able to capture nonlinear responses observed in streamflow time series bardossy and pegram 2009 hao and singh 2012 nazemi et al 2013 wang et al 2019 more recently methods based on multidimensional copulas in particular vine copulas have been used for multisite streamflow generation chen et al 2019 nazemi et al 2020 pereira et al 2017 pereira and veiga 2018 despite ongoing advances in copula based streamflow generations only a few incorporate climate related proxies in streamflow generation slater and villarini 2018 wang et al 2019 and none to the best of our knowledge explicitly incorporate the influence of multiple lscis in the procedure of stochastic streamflow generation here we propose a generic approach based on vine copulas to explicitly incorporate lscis as exogenous covariates in stochastic streamflow generation at the monthly scale both in prediction and projections modes and at single and multiple sites we hypothesize that the explicit representation of lscis improves both prediction and projection skills particularly in terms of representing seasonality and inter annual variability we recognize that this is a challenging problem first the proposed model should be able to capture both symmetric and asymmetric relationships between lscis and streamflow in time and space see e g hoerling et al 1997 in addition as the statistical dependence between streamflow and lscis can change in both time and space the proposed model should have a dynamic structure wang et al 2019 nguyen huy et al 2020 for this purpose we use vine copulas in conjunction with a formal predictor selection algorithm to identify the best common set of lscis for streamflow generation at a monthly scale we showcase the application of the proposed scheme for the prediction and projection of three mountainous headwaters in southern alberta canada to benchmark the performance of the proposed algorithm we compare the skills of our model with already existing reference algorithms at the single site we compare the performance of our proposed algorithm with a baseline copula model developed by nazemi et al 2013 and extended into multisite mode using regression models nazemi and wheater 2014 both single and multisite versions of this existing copula based algorithm were previously implemented in the same case study vine copulas were also used to represent asymmetric and nonlinear spatial relationships among multiple streams chen et al 2015 2019 nazemi et al 2020 these models provide a benchmark to discuss the added value of incorporating lscis in the process of streamflow generation the remainder of this paper is organized as the following section 2 presents the methodological basis of the proposed algorithm section 3 briefly introduces our case study section 4 discusses the model development experimental setup and benchmarking procedures section 5 presents the results compares the proposed model with existing reference models and discusses the added value of incorporating lscis in stochastic streamflow generation finally section 6 concludes the study 2 methodology the core of our proposed algorithm is a vine copula linked to a formal input selection scheme for selecting a set of lscis as exogenous covariates that influence the streamflow at the considered timescale monthly throughout this paper we show below that the proposed algorithm is generic and can be applied in both single and multisite settings and in both prediction and projection modes by prediction we refer to short term precisely one step ahead probabilistic estimates of streamflow conditioned to a known initial state prediction mode has relevance to real time applications such as flood forecasting or operational planning of water resource systems by projection in contrast we refer to long term estimates of streamflow conditioned to a range of possible initial states projection mode is relevant to long term planning and management of water resource systems as well as scenario analysis particularly under changing climate and land use conditions below we illustrate the elements of the proposed algorithm and its procedure 2 1 vine copulas consider a d dimensional copula function c 0 1 d 0 1 in which c is a multivariate cumulative distribution function cdf of a random vector u 1 u 2 ud defined on the unit hypercube 0 1 d where ui denotes uniform marginal distributions u 0 1 see joe 1997 the fundamental work of sklar 1959 shows that for any multivariate cdf such as f x 1 x 2 xd of d random variables x 1 x 2 xd there is a copula function c that can describe f x 1 x 2 xd using marginal cdfs f 1 x 1 f 2 x 2 fd xd as the following 1 f x 1 x 2 x d c f 1 x 1 f 2 x 2 f d x d denoting that the original d dimensional cdf is decomposed into 1 a dependence structure between uniform random variables and 2 marginal cdfs that are defined independently from the dependence structure if the marginal distribution functions are continuous then the copula function c is unique and the joint probability density function pdf can be calculated as joe 1997 2 f x 1 x d c f 1 x 1 f d x d j 1 d f j x j where c is the d dimensional copula pdf and fj xj is the marginal pdfs of the xj j 1 2 d vine copulas provide a generic approach to construct high dimensional joint distributions by using bivariate copulas as building blocks as any high dimensional copula can be decomposed into a product of d d 1 2 bivariate copulas ordered as a sequence of d 1 nested trees with nodes joined by edges see bedford and cooke 2001 joe 1997 aas et al 2009 czado 2010 here we focus on a particular form of vine copulas i e canonical vines hereafter c vines which are based on ordering the variables by importance considering a multivariate density function as a product of conditional densities a c vine can be constructed by 3 f x 1 x d f 1 x 1 j 2 d f j 1 j 1 x j x 1 x j 1 where 1 j 1 1 2 j 1 based on the sklar s theorem and eq 2 we have 4 f j 1 j 1 x j x 1 j 1 c j 1 j 1 j 2 f x j 1 x 1 j 2 f x j x 1 j 2 f x j x 1 j 2 where f and f denote conditional cdfs and pdfs respectively by applying eq 4 recursively the joint density in eq 3 can be expressed as bedford and cooke 2001 5 f x 1 x d k 1 d f k x k i 1 d 1 j 1 d i c i j i 1 i 1 f x i x 1 i 1 f x j i x 1 i 1 the c vine copula construction in eq 5 involves marginal conditional distributions in the form f x v which can be obtained recursively using the h function derivative joe 1997 2014 6 h f x v c x v j v j f x v j f v j v j f v j v j where v is a d dimensional multivariate space with vj is one arbitrary component chosen from v and v j denotes the vector v excluding vj and c x v j v j is the bivariate copula 2 2 selecting large scale climate indices to inform streamflow generation while c vine copulas provide a theoretical framework for conditioning streamflow generation to lscis several lscis may influence streamflow at a given lead time and the relevant lscis in one time step may be irrelevant in others as a result there is a need for a systematic and rather dynamic identification of appropriate lscis at a given lead time quilty et al 2016 2019 robertson and wang 2012 statistical methods such as principal component analysis barnston and ropelewski 1992 independent component analysis aires et al 2000 partial mutual information selection pmi sharma 2000 sharma and mehrotra 2014 sharma et al 2016 and partial correlation input selection pcis may et al 2008 2011 have been developed and already used in the literature for predictor selection here we use pcis an iterative forward looking input selection algorithm which chooses one lsci at a particular lag during each iteration brown et al 2012 quilty et al 2016 2019 tran et al 2016 the selected lsci is the one that has the largest dependence score with the streamflow at a given monthly time step similar to amir jabbari and nazemi 2019 we apply kendall s tau kendall 1976 to measure the dependence between lagged monthly lscis and monthly streamflow the kendall s tau is more compatible for selecting lscis as there can be strong nonlinear dependence between lscis and the monthly streamflow hlinka et al 2014 konapala et al 2018 the algorithm terminates when adding new predictors causes no improvement in the bayesian information criterion bic estimated from the predictand residuals as we apply the pcis for each month separately the selected monthly lscis can change for more details on pcis see may et al 2008 2011 and hatami et al 2019 2 3 proposed streamflow generation scheme by choosing the relevant lscis c vine copulas can be used to allow conditioning streamflow at each time step to previous values of streamflow as well relevant lscis as exogenous covariates previous studies showed that at coarser time scales e g weekly monthly the lag 1 dependence is sufficient for streamflow generation chen et al 2015 nazemi and wheater 2014 wang et al 2019 fig 1 summarizes the proposed streamflow generation scheme at single site considering the lag 1 temporal dependence no need to mention that this framework can be extended to consider more lags and or to cover finer timescales for the sake of simplicity we only consider one lsci in the schematic and formulations are given below in the case of more lsci this formulation can be extended to higher dimensions following the procedure explained in section 2 1 in single site setting the task is to generate streamflow at any site a fig 1a where c vines are used to decompose the dependence structure between monthly flows at time t i e q t a t 1 i e q t 1 a as well as a relevant lsci at time t τ i e l t τ where τ is the lag between the monthly flow at time t and the selected lsci following the eqs 3 and 4 the joint distribution between q t 1 a q t a and l t τ can be established as 7 f q t 1 a l t τ q t a f 1 q t 1 a f 2 1 l t τ q t 1 a f 3 1 2 q t a q t 1 a l t τ where the subscripts 1 2 and 3 correspond to q t 1 a l t τ and q t a respectively using eq 4 the conditional distributions in eq 7 can be estimated as 8 f 2 1 l t τ q t 1 a f l t τ q t 1 a f q t 1 a c 1 2 f 1 q t 1 a f 2 l t τ f 1 q t 1 a f 2 l t τ f 1 q t 1 a c 1 2 f 1 q t 1 a f 2 l t τ f 2 l t τ and 9 f 3 1 2 q t a q t 1 a l t τ f l t τ q t a q t 1 a f l t τ q t 1 a c 2 3 1 f l t τ q t 1 a f q t a q t 1 a f l t τ q t 1 a f q t a q t 1 a f l t τ q t 1 a c 2 3 1 f l t τ q t 1 a f q t a q t 1 a c 1 3 f 1 q t 1 a f 3 q t a f 3 q t a as a result the three dimensional joint density in eq 7 can be represented in terms of bivariate copulas c 1 2 c 1 3 and c 2 3 1 with densities c 1 2 c 1 3 and c 2 3 1 as the following 10 f q t 1 a l t τ q t a f 1 q t 1 a f 2 l t τ f 3 q t a c 1 2 f 1 q t 1 a f 2 l t τ c 1 3 f 1 q t 1 a f 3 q t a c 2 3 1 f l t τ q t 1 a f q t a q t 1 a the graphical representation of the proposed three dimensional c vine copula is depicted in fig 1b and includes two trees in the first tree t 1 the circled nodes 1 to 3 represent respectively the three pdfs of flows at time step t 1 lsci signal with lag τ i e l t τ and the flow at time step t the dependencies between nodes i e edges are modeled using bivariate copulas formed for each pair the edges in the first tree become nodes for the second level accordingly the conditional distribution function of f q t a q t 1 a l t τ can be obtained by recursive use of eq 6 as aas et al 2009 czado et al 2012 11 h f q t a q t 1 a l t τ c 2 3 1 f q t a q t 1 a f l t τ q t 1 a f l t τ q t 1 a note that f q t a q t 1 a h q t a q t 1 a c 3 1 f q t a f q t 1 a f q t 1 a and f l t τ q t 1 a h l t τ q t 1 a c 2 1 f l t τ f q t 1 a f q t 1 a as a result eq 11 can be rewritten as 12 h f q t a q t 1 a l t τ h h q t a q t 1 a h l t τ q t 1 a in the multisite setting the streamflow generation is based on selecting a primary site which is used as a reference to realize streamflow in other reaches hereafter secondary sites using the temporal and spatial dependencies within and between streamflow series see nazemi et al 2013 2020 nazemi and wheater 2014 these dependencies are usually not the same and require asymmetric copulas to be handled e g chen et al 2015 grimaldi and serinaldi 2006 fig 2 a shows the multisite streamflow generation schematically let q t b and q t 1 b be the monthly flows at time step t and t 1 at the secondary site b note that we consider a unique lsci at time t τ i e l t τ relevant to both stations this has a physical relevance as the effect of lscis on streamflow regime is manifested regionally bonsal and shabbar 2008 nalley et al 2016 nazemi et al 2017 the extension of the proposed vine copula to a multisite setting is shown schematically in fig 2b based on eqs 4 and 5 the joint dependence can be constructed as 13 f q t 1 b q t 1 a l t τ q t b f 1 f 2 f 3 f 4 c 1 2 c 1 3 c 1 4 c 2 3 1 c 2 4 1 c 34 21 where c 1 2 is c 1 2 f 1 q t 1 b f 2 q t 1 a and c 1 3 and c 1 4 being defined similarly c 34 21 is c 34 21 f l t τ q t 1 a q t 1 b f q b t q t 1 a q t 1 b see also eqs 5 and 10 having the joint distribution function from eq 13 the conditional cdf f q t b q t 1 b q t 1 a l t τ can be obtained recursively by applying the appropriate h function from eq 6 14 h f q t b q t 1 b q t 1 a l t τ h h h l t τ q t 1 b h q t 1 a q t 1 b h h q t b q t 1 b h q t 1 a q t 1 b we apply the inverse forms of h functions given in eqs 12 and 14 for streamflow generation at single and multiple sites respectively in the prediction mode and single site setting the aim is to obtain f q t a based on the known states of f q t 1 a and f l t τ this is achieved by estimating the inverse of h function given uniform random numbers of ε 15 q t a f 1 h 1 h 1 ε h l t τ q t 1 a q t 1 a the ensemble generation is obtained by synthesizing a large number of uniformly distributed random numbers 10 000 throughout this paper to realize corresponding scenarios of q t a this large number of realizations is needed to account for sampling uncertainty and to come up with confidence bounds roy and gupta 2020 the mean value of these realizations is considered as the best prediction at each time step similarly in the multisite mode the procedure of streamflow generation is also based on monte carlo simulations using the information available for q t 1 b q t 1 a and l t τ using the inverse h functions 16 q t b f 1 h 1 h 1 h 1 ε h h l t τ q t 1 b h q t 1 a q t 1 b h q t 1 a q t 1 b q t 1 b the process of ensemble generation in the projection mode is very similar to the prediction mode with the exception that the conditioning is not based on known antecedent quantities of streamflow for this purpose having the appropriate predictors i e lscis at each month flows can be generated at the following month conditioned to the given lscis 3 case study and data the oldman river and its tributaries form a large basin 27600 km2 and provide around 40 of the total annual streamflow in southern alberta canada alberta environment 2010 the oldman river originates from mountainous headwaters in the eastern slopes of the rocky mountains and joins the bow river before rolling downstream toward the province of saskatchewan the basin is located in a semi arid cold region where snowmelt from the rocky mountains headwaters is the main source of the annual water supply providing between 70 to 90 of the total annual flow volume nazemi et al 2017 the water resource system in this basin is complex including several sectors of water demand some with competing interests e g environmental vs irrigation water demands irrigation is the largest water consumption in the basin and includes 88 of the total water demand zandmoghaddam et al 2019 the basin is currently under pressure due to rapid socio environmental changes that intensify water demand and or loss e g inclined evapotranspiration due to warming this has created a regional water security concern where natural streamflow cannot meet additional water demand gober and wheater 2014 martz et al 2007 this concern will be even more in the future and under heightened climate variability and change as the natural streamflow is also under rapid changes zaerpour et al 2020 mainly due to the decline in the winter snowpack in the rocky mountains prowse et al 2006 we have chosen three headwater rivers upstream of the oldman reservoir namely the oldman river near waldron s corner crowsnest river near lundbreck and castle river near beaver mine together they provide 95 of the natural inflow to the oldman river nazemi et al 2017 fig 3 shows the map of the oldman river in which the three considered headwater streams are identified table 1 shows the key information about these streams for each stream we extract the monthly streamflow values for the period of 1967 to 2008 from the water survey of canada s hydrometric database hydat http www wsc ec gc ca we consider six large scale climate indices including pdo enso nao amo arctic oscillation ao and pacific north american pna as potential lscis for streamflow generation these lscis are the main ocean atmospheric patterns that affect hydroclimatological variables in canada bonsal et al 2006 coulibaly and burn 2004 nalley et al 2019 rasouli et al 2020 whitfield et al 2010 in brief the pdo is a large scale climate pattern of sea surface temperature fluctuations in the pacific with the periodicity of inter decadal to multi decadal scales mantua et al 1997 pdo data for the considered period is obtained from the joint institute for the study of the atmosphere and ocean university of washington http jiasao washington edu pdo pdo latest the nao is represented by the atmospheric pressure at sea level between the icelandic low and the azores high with a periodicity of around 3 6 years and is pronounced during the cold season the ao is characterized by atmospheric circulation patterns over the extra tropical northern hemisphere where sea level pressures over the polar vary in opposition to middle latitudes at around 45 n thompson and wallace 1998 nao and ao data are obtained from national center for environmental information http www ngdc noaa gov the amo is an atmospheric oceanic phenomenon with a periodicity of 50 70 years that arises from the variations in sea surface temperature in the atlantic ocean enfield et al 2001 amo data are obtained from earth system research laboratories center http www esrl noaa gov psd data correlation amon us long data the pna pattern features a sequence of high and low pressure anomalies stretching from the subtropical west pacific to the east coast of north america pna data are extracted from physical sciences laboratory https www psl noaa gov data correlation pna data enso represents large scale ocean atmosphere oscillations in the tropical pacific influencing climatic conditions around the globe including canada trenberth 1997 enso data is obtained from the climate prediction center http www cgd ucar edu cas catalog climind regarding enso we consider four indices including nino3 nino3 4 nino4 and nino1 2 anomalies selected using the predictor selection method based on the level of dependence to monthly flows 4 experimental setup and benchmarking approach we apply the proposed algorithm in the three considered streams in single and multisite settings and in both prediction and projection modes in the single site setting streamflow is generated at each site independently in the multisite setting however one stream should be chosen as the primary site from which streamflow is realized regionally in other locations here we follow the generation pathway used by nazemi and wheater 2014 they performed a rigorous experiment to choose the best pathway for streamflow generation and concluded that the oldman river near waldron s corner site a can be considered as the primary reach for the other two reaches most essentially the performance of our proposed algorithm is benchmarked with observed streamflow characteristics we also compare the performance of our proposed scheme at the single site with an existing reference model proposed by lee and salas 2011 and applied in the oldman river basin by nazemi and wheater 2014 the basic idea in such a model is to conditionally resample flow at the lead time of one month from the antecedent streamflow conditions without considering the effect of lscis similarly in the multisite setting we compare the performance of our proposed algorithm with the multisite algorithm based on vine copula developed by chen et al 2015 2019 and used in nazemi et al 2020 table 2 shows the notation used for proposed and reference algorithms in prediction and projection modes the first step in our proposed algorithm is to select a set of relevant lscis at each monthly timestep the pool of lscis from which relevant indices at each timestep are selected consists of the six named lscis up to 24 months lags we initially considered more lag times up to 48 months see nalley et al 2016 however inline with several other studies in western canada e g shabbar et al 1997 or globally chiew and mcmahon 2002 the analysis of kendall s tau rank dependence showed that the memory of lscis on streamflow in the three considered streams does not go beyond 24 months we then use the pcis algorithm to select the best set of lscis predictors at each month from a pool of options as the three streams are located in close proximity and are under the influence of similar lscis we aim at finding a common set of predictors at the three sites see hatami et al 2019 for the details of using the pcis algorithm for global input selection in multiple locations we also compare the results with pmi to ensure that a robust set of lsci is selected finally we select the structure of vine copulas by the maximal spanning tree algorithm as described by czado et al 2013 and considering empirical distributions in margins for both monthly streamflow and the selected lscis in this context a fast sequential estimation procedure for parameter estimation is suggested dissmann et al 2013 in brief the first bivariate copula families and parameters for the first tree are identified before calculating the weights for the second tree and so on we consider a set of well known parametric copula functions including gaussian student t clayton gumbel frank joe and their rotated forms for setting up the vine copulas see pereira and veiga 2018 for the formulation of these copulas the copula identification and parameterization are performed by applying the maximum log likelihood method and considering the bayesian information criteria bic akaike 1979 as the goodness of fit see also sadegh et al 2017 the modeling is implemented in the r platform by utilizing the vinecopula nagler et al 2019 schepsmeier et al 2016 cdvine brechmann and schepsmeier 2013 copula hofert et al 2014 cdvinecopulaconditional bevacqua 2017 and npred packages sharma and mehrotra 2014 the performance of the proposed algorithm is evaluated in the prediction and projection modes and compared with reference models using a set of metrics in the projection mode we generate 10 000 synthetic historical streamflow realizations with the same length as the observed data to demonstrate the performance of projected streamflow ensembles basic statistics including long term monthly mean value standard deviation skewness as well as the lag 1 temporal and lag 0 spatial dependencies are calculated to measure the relative improvement in the performance of the proposed method in representing persistence in the historical streamflow we adopt the interannual variability metric johnson and sharma 2011 taylor 2001 17 s 4 1 r 4 σ f 1 σ f 2 1 r 0 4 where r is the correlation coefficient between observed and simulated streamflow σ f is the ratio of the expected standard deviation of ensemble projections to the standard deviation of the observed streamflow and r 0 is the maximum theoretical correlation which is taken as 1 the range of s varies between 0 and 1 the score of 1 happens when both σ f and r are equal to one and there is no error in capturing the interannual variability the relative improvement in the performance of the proposed model in capturing the interannual variability over the reference model is given by the interannual variability skill score as wang and robertson 2011 18 s s proj s s ref 1 s ref 100 where s and s ref represent the interannual variability metric values for the proposed and the reference model respectively in the prediction mode we similarly generate 10 000 realizations of streamflow series with the same length as observed data in both single and multisite settings we calibrate and validate the predictions using the buffered leave one out cross validation method le rest et al 2014 roberts et al 2017 for prediction in each year the corresponding year and the two years ahead are excluded while the remaining years are used for predictor selection and model building procedures the predicted monthly flow is then compared with the observed data we present the overall skill of predictions as a skill score which is the relative improvement in error scores ess of the prediction over a set of reference predictions wang et al 2009 19 s s pred e s ref es e s ref 100 where es and esref represent the five considered error metrics calculated for the proposed and the reference model respectively each skill measure assesses different aspects of the forecast distribution a higher ss value indicates better performance in the proposed model with a score of 0 representing the same performance as the reference model the first skill score used is ssmae which is based on expected mean absolute error mae of the predicted mean mae measures the expected mean absolute error of predicted monthly streamflow ensemble and is defined as 20 mae t 1 t x sim x obs t where t is the total number of years the second score ssrmse is based on expected root mean squared error rmse of the predicted mean rmse assesses the standard deviation of predicted error applied to the ensemble mean 21 rmse t 1 t x sim x obs 2 t the third skill score ssrmsep is based on the expected root mean squared error in probability rmsep bennett et al 2014 wang and robertson 2011 of the predicted mean the rmsep measures predicted error on a probability scale giving the predicted events similar opportunity to contribute to the overall assessment of predicted skill 22 rmsep 1 t t 1 t f sim x t f obs x t 2 where f obs xt is the cdf of the historical streamflow data f sim x t is the cdf of the expected predicted streamflow rmsep is less sensitive to events with large errors wang and robertson 2011 the fourth score sscrps is based on the expected continuous ranked probability score crps harrigan et al 2018 kaune et al 2020 wang and robertson 2011 of the predicted mean unlike rmsep crps metric measures the error of the whole predicted probability distribution and can be sensitive to just a few usually very high flow events with large prediction errors 23 crps 1 t t 1 t f sim x t h x t x obs 2 d x where f sim xt is the predicted cdf and h xt x obs is the heaviside step function defined as wang and robertson 2011 24 h x t x obs 0 x t x obs 1 x t x obs finally the fifth score sskge is based on the expected kling gupta efficiency kge gupta et al 2009 the kge focuses on three criteria including correlation between observations and simulations the bias and the relative variability in the simulated and observed values expressed as 25 kge 1 r 1 2 σ sim σ obs 1 2 μ sim μ obs 1 2 where r is the correlation between observations and simulations σobs and σsim are the standard deviations of observed and simulated values μobs and μ sim are the means of observed and simulated values kge values close to 1 represent perfect model performance note that unlike the previous four scores where zero indicates the best performance kge 1 indicates the perfect agreement between simulations and observations to allow homogeneous comparison between all score we scale the kge to skill score as knoben et al 2019 26 s s kge kge kg e ref 1 kg e ref 100 apart from accuracy we also measure the reliability of predicted ensembles by assessing the statistical consistency of the predicted probability distributions and the associated observed events jolliffe and stephenson 2011 toth et al 2003 in this study we use histograms of probability integral transforms pit dawid 1984 gneiting et al 2007 to assess the average reliability of the prediction distributions the pit of the observed value is given as 27 pit f sim x obs where f sim denotes the cdf of simulated flow and f sim x obs therefore is the non exceedance probability of the observed streamflow based on the cdf of the simulated streamflow the pit values are then assigned to different histogram bins and the frequency of each bin is calculated the predictive distribution is said to be reliable if the pit values are distributed uniformly u shaped histograms usually indicate that the probabilistic predictions have under dispersion on the contrary if the pit histogram is hump shaped then the probabilistic predictions indicate over dispersion the deviation metric d quantifies the deviation from uniformity in pit this measure which is introduced by nipen and stull 2011 and later used in bourdin et al 2014 can be defined as 28 d 1 b i 1 b b i t 1 b 2 where b is the number of bins bi is the number of observations in bin i and t is the size of the dataset lower variability in bin frequency is indicative of a flatter pit sampling limitations however can cause a reliable prediction not generate a perfectly flat pit histogram leading to an expected value of the deviation e dp for perfect prediction brocker and smith 2007 pinson et al 2010 given by 29 e d p 1 b 1 t b note that apart from the given metrics we also look at the 95 confidence interval ci of generated ensembles both in terms of probabilistic characteristics as well projected predicted timeseries 5 results and discussions 5 1 influential large scale climate indices in upper oldman we initially analyze the dependence between lagged lscis and monthly streamflow in the three considered headwaters considering 1 to 24 months lags for six considered lscis a pool of 144 lscis is formed for each month from which the relevant lscis for a given month are chosen using the global version of pcis presented in hatami et al 2019 results are summarized in fig 4 each panel includes the results at one site in which rows are months ordered from october to september from the top to the bottom columns show the six considered lscis in each month the relevant lscis that are selected through the pcis algorithm are numbered and shaded numbers identify relevant lags and colors indicate values of kendall s tau dependencies between selected lscis and the corresponding monthly flows as it can be observed the dependencies between lsci and monthly streamflow are dynamic within a year and there can be months in which more than one lscis to which streamflow is significantly dependent this is particularly the case in high flow months e g april s flow in which the fully extended version of c vine mentioned in section 2 1 should be implemented among the considered lscis pdo is the most frequent lsci selected across different months and demonstrates the strongest dependence with spring and summer flows this empirical evidence is in line with previous findings showing that the pdo is the dominant lscis in this region e g fleming et al 2007 nazemi et al 2017 nao and enso are the other two key lscis which is in line with findings of gobena and gan 2006 for streamflow in southwestern canada additionally ao and pna are among lscis selected more than once these results are inline with previous findings over western canada e g bonsal and shabbar 2008 gobena and gan 2006 to address the uncertainty in the selected predictors we repeat the input selection using the pmi algorithm pmi uses mutual information as oppose to the kendall s tau used in the modified pcis used in this study results are presented in fig s1 in the supplement showing a considerable match between the results of pcis and pmi in 7 out of 12 months including all wet months the results of pmi and pcis are fully identical in months january april and august pmi chooses some lsci that pcis does not identify similarly in may pcis can identify a lsci that pmi has missed only in february which is a dry month pmi and pcis result into fully divergent sets of relevant lscis this intercomparison certifies the selection of relevant lscis 5 2 benchmarking the skill of the proposed model in the projection mode by identifying the influential lsci the proposed model can be setup and evaluated using the procedure explained in section 4 we first evaluate the results in the projection mode fig 5 shows the long term expected statistics of the first three moments of the monthly streamflow i e mean standard deviation and skewness obtained from 10 000 realizations with the same length as the historical data in each panel pink and gray lines depict the results obtained by the reference and proposed models respectively black lines indicate the observed values in general both the reference and proposed algorithms are able to preserve the observed statistics very well considering the single site generation and in site a relative errors res in projecting the expected long term monthly flow for both models is less than 5 across all months although reconstructing observed values of monthly standard deviation and skewness entails more error the res remain less than 10 for both the reference and proposed models except for the skewness of monthly flow in february reconstructed by the proposed model re 21 in site b both the reference and proposed models can represent the long term expected statistics of the three moments in all months with res less than 10 in both single and multisite settings except for the single site generation in months of august and september in which res in the representation of standard deviation and skewness obtained by the reference model is higher than 20 in site c the res in the representation of long term expected statistics of monthly flows by the reference and proposed models are less than 5 in the single site setting while in the multisite reconstruction of site b both proposed and reference models show similar performance the res in the representation of the long term expected mean and standard deviation of monthly flows are marginally higher for the reference model in site c yet remain below 10 the res in the multisite generation of long term expected skewness of monthly flows in site c is lower for the proposed model particularly in the low flow season we also assess the performance of our proposed algorithm in preserving the lag 1 temporal dependencies within each site as well as lag 0 spatial dependence between primary here site a and secondary sites here sites b and c fig 6 summarizes findings in the representation of a lag 1 temporal and b lag 0 spatial dependencies respectively black lines show the observed lag 1 and lag 0 dependencies pink and gray colors illustrate the results obtained by the reference and proposed models in single and multisite settings respectively in summary both reference and proposed algorithms are able to preserve temporal and spatial dependencies quite well in the single site setting the reference and proposed algorithms are able to preserve the lag 1 temporal dependencies with an absolute error of less than 0 1 in the multisite setting the proposed model can preserve lag 1 temporal dependence with absolute error values of less than 0 1 except few cases during the low flow season in site c the reference model however show better performance in capturing the temporal dependence in those months the interesting point is that the lag 1 dependence in site b is preserved better in multisite setting when streamflow in site b is conditioned based on the generated flow in site a this however is not the case in site c regarding the spatial dependencies both the reference and proposed models can capture the spatial dependencies with less than 0 1 absolute error in multisite scheme we assess the performance of the reference and proposed models in capturing the interdependencies between lagged lscis and the flows at a monthly scale fig 7 depicts the expected mae in representing observed interdependencies between influential lscis and monthly flows demonstrated in fig 4 panel a is related to the reference algorithm and panels b and c are related to the performance of the proposed algorithm in single and multisite settings respectively in general the expected mae in the representation of interdependencies for the reference model is high mae 0 25 this is due to the fact that the reference model does not explicitly represent the interdependencies between lagged lsci and monthly streamflow the performance of the proposed model in contrast is significantly better showing an expected mae of below 0 1 for high flow season and below 0 15 for low flow season in single site setting the expected mae in multisite setting is below 0 15 for both low and high flows the projected streamflow ensembles generated by proposed and reference models in both single and multisite settings are investigated in the form of hydrographs for the driest year 1977 78 a normal year i e near long term average 2008 2009 and the wettest year 1995 96 as illustrated in fig 8 for simulations during the entire data period see figs s2 to s6 in the supplement the pink and gray ensembles are the 95 ci obtained by the reference and proposed models respectively the black lines show the observed time series this figure clearly shows that proposed and reference models can capture the observed time series within the 95 cis having said that the seasonality can be much better represented by the proposed model than the reference model this can be supported by significantly narrower ci is in the case of the proposed model considering single site generation and in the dry year the expected reduction in the ci across the three sites is 45 and 54 9 during low and high flow seasons respectively the expected reduction in projected ci in the multisite setting is slightly lower and marks 40 6 and 51 3 in low and high flow seasons respectively in the normal year the expected reduction in the ci across the three sites in the single site setting are 30 6 and 22 during low and high flow seasons respectively in contrast to dry year the expected reduction in multisite setting is more than single site and marks 38 8 and 23 3 during low and high flow seasons respectively in the wet year and through single site generation the average reduction in the proposed model is 26 8 and 35 during low and high flow seasons respectively similar to the dry year the improvements in predictive uncertainty are slightly less in multisite setting and marks 21 3 and 28 8 reduction compared to the reference model during low and high flow seasons respectively we also analyze the performance of the proposed model in representing the extremes for this purpose we fit a generalized extreme value gev distribution to the annual low and high flows enabling the extrapolation beyond the range of observed data fig 9 shows the uncertainty bounds of ensembles of 10 000 gevs fitted to simulated annual low and high flows in single fig 9a and multisite settings fig 9b fig 9 includes two rows showing the ensembles of fitted gevs for the simulated annual low top rows and high flows bottom rows the dots are the observed extremes and the black lines are the fitted gev distribution to the observed extremes the skill is assessed by the percentage of reduction in ci of generated streamflow 10 000 realizations as the percentage of coverage poc which is the percentage of observed data falling within the ci of generated streamflow in general both reference and the proposed model demonstrate high skills in capturing the observed extremes within the ci in the single site setting and for the low flows the proposed and reference models are expected to capture 88 9 and 92 9 of extremes within their ci respectively regarding the high flows proposed and reference models show pocs of 95 2 and 96 0 respectively similar skills can be seen in the multisite setting where both pocs are 90 5 and 91 7 for proposed and reference models in representing the annual low flows respectively these pocs changes in the case of high flows to 94 0 and 96 4 respectively while the poc statistics are slightly better in the case of the reference models cis are substantially reduced providing less uncertainty in the projected flow in single site setting and for low flows the average range of ci of the proposed model is reduced by 11 7 compared to the reference model whereas this value is 3 1 in the multisite setting regarding the high flows the range of ci of the proposed model is much reduced compared to the reference model reaching to average of 15 5 and 31 1 reduction in single and multisite settings respectively finally we investigate the performance of the proposed model in representing of interannual variability fig 10 represents percentages of improvement in interannual skill scores at single panel a and multiple sites panel b the average improvement in this skill score in the single site setting is 10 2 across the three considered sites the lowest and highest improvements occur in site a and site b with average improvements of 3 6 and 14 1 respectively the proposed model demonstrates improvements in capturing interannual variability during high flow season with an average improvement of 16 0 in single site generation and across the three sites in the multisite setting the average improvement in the interannual variability skill score is slightly lower with expected improvements of 13 5 and 10 1 in sites b and c respectively having said that during the high flow season considering relevant lsci in the multisite generation results in more improvements compared with the single site generation the expected improvement in capturing the interannual variability during the high flow season is 23 5 in sites b and c 5 3 benchmarking the skill of the proposed model in the prediction mode in the prediction mode five skill scores including ssmae ssrmse ssrmsep sscrps and sskge are reported for single fig 11 a and multisite fig 11b settings at the three considered sites generally speaking predictions made by the proposed algorithm exhibit better skills than reference models the average improvement in single site predictions is ss mae 5 3 ss rmse 4 5 ss rmsep 5 8 ss crps 6 2 and ss kge 14 3 the range of changes in skill scores varies largely across different months and extends from 15 to 35 the greatest improvement in skill scores in the single site generation occurs in high flow season with average increases of ss mae 15 3 ss rmse 12 8 ss rmsep 13 4 ss crps 14 1 and ss kge 27 1 in the multisite prediction of streamflow in secondary sites b and c a similar pattern can be seen except few discrepancies in low flow season in general simulation made by the proposed model exhibits improvement in almost all months with average improvements of ss mae 5 4 ss rmse 3 1 ss rmsep 6 4 ss crps 4 9 and ss kge 15 4 the most encouraging result is the improvement in the skill scores during the high flow season reaching to ss mae 16 4 ss rmse 14 9 ss rmsep 14 1 ss crps 15 8 and ss kge 28 3 these skills vividly show the value gained by incorporating lscis into streamflow prediction particularly in high flow seasons in addition to the improvement in the prediction errors we assess the reliability of streamflow predictions fig 12 presents histograms of the pits related to the prediction using the proposed scheme against its reference counterparts in single fig 12a and multisite settings fig 12b each panel includes two rows showing the pit histograms for the low and high flow seasons winter and spring top and bottom rows respectively the gray and pink envelopes show the pit histograms obtained by 10 000 realizations generated by the proposed and reference schemes respectively the deviations from uniformity in pit histograms can be quantified using the measure of degree of uniformity d with lower values indicating better reliability perfectly reliable predictions produce a pit histogram with a uniform distribution the expected value of perfect prediction dp is 0 0267 regarding the low flow season although the predictions made by both the reference and proposed schemes exhibit slight overdispersion but in general both models show average deviation from pit uniformity close to the perfect prediction in low flow season and in single site the proposed model shows slightly better reliability in prediction of observed values with average d 0 0251 compared to d 0 0267 for the benchmark scheme similarly in multisite setting the average deviation related to pit of the proposed scheme is 0 0242 compared to 0 0262 for the benchmark scheme indicating slightly better reliability of the proposed scheme regarding the high flow season similar to low flow both models exhibit slight overdispersion in single site setting the average deviation of pit related to the proposed model is 0 0214 compared to 0 0262 for the benchmark model demonstrating slightly better reliability of the proposed model in multisite setting these values are 0 0163 and 0 0211 for the proposed and benchmark models respectively additionally to further assess the reliability of predictions in single and multisite settings particularly in terms of the extremes we evaluate the improvement in the 95 cis of the proposed scheme against its reference counterparts in single fig 13 a and multisite settings fig 13b each panel includes two rows showing the cis in the low and high flow seasons i e winter and spring top and bottom rows respectively the gray and pink envelopes show the 95 ci obtained by 10 000 realizations generated by the proposed and reference algorithms respectively black dots are the observed flow qsuantiles and the solid lines are the best predictions mean ensembles obtained either in single site panel a or multisite panel b settings for each flow season the uncertainty intervals during dry flows with percentile level of 0 1 or lower and wet conditions flows with percentile level of 0 9 or higher are separated from the rest of the flow conditions by the dashed red lines based on fig 13 empirical distributions of the observed high and low flows fall within 95 ci of both the reference and proposed models in all sites and through single and multisite settings having said that the proposed model can estimate the flow with much less uncertainty the average reduction in ci during the low flow season is 18 3 and 21 1 in single and multisite settings reductions in the uncertainty bounds during the high flow season however are much larger with expected reductions of 41 3 and 45 1 through the single and multisite settings respectively in the low flow season the highest reductions in the uncertainty bounds occur in the normal flow conditions between 0 1 to 0 9 percentile with expected reductions of 26 3 and 31 4 in ci through single and multisite settings respectively in the high flow season the largest improvements in the uncertainty bounds happen during dry conditions with average reductions of 68 3 and 64 5 in ci through single and multisite settings respectively 6 summary and conclusion skillful and reliable streamflow prediction and projection capabilities are of urgent needs for water resource planning and management here we aim at improving the skills of stochastic streamflow generation at single and multiple sites by explicit incorporation of lscis through a stochastic generator in particular we suggest a sampling scheme based on using c vine copulas in which the structure and parameters of the model change at each time step this is due to the variation of the influential lscis and the type of dependencies which can change across different timesteps here monthly to accommodate this we use a global input selection algorithm to pick the most significant lscis from a pool of potential predictors at each month and consider the selected lscis as influential predictors to which the streamflow is conditioned to showcase the application of this model in practice we demonstrate the use of the proposed algorithm at single and multiple sites and in both prediction and projection modes in three headwaters of the oldman river basin alberta canada the performance of the proposed algorithm is rigorously compared with the observed data as well as reference models that are essentially identical to proposed models yet do not use any information about lscis within stochastic streamflow generation the results of our study show that while simulation performances of the reference and proposed models are very similar in representing the first three moments of monthly streamflow as well as lag 1 temporal and lag 0 spatial dependencies within and between streamflow reaches the explicit consideration of lscis can significantly reduce simulation uncertainty and improve simulation skills in projection and predictive modes and in both single and multisite settings particularly in high flow seasons in projection mode and considering the single site setting the expected reductions in ci in the three sites are 34 1 and 37 1 in low and high flow seasons respectively in multisite setting these values are 33 6 and 34 5 in low and high flow seasons respectively the average improvement in the interannual variability skill score in single and multisite settings is over 10 across the three considered sites having said that during the high flow season considering relevant lsci in the multisite streamflow generation results in expected improvement in capturing the interannual variability by more than 23 in terms of observed annual extremes both reference and proposed models can capture the observed values within the range of ci with high poc values of greater than 90 in most cases the proposed model however demonstrates less uncertainty in capturing the observed extremes particularly regarding high flows for which the ranges of ci are decreased by 15 5 and 31 1 in single and multisite settings respectively in prediction mode both reference and proposed models show slight overdisperision in pit histograms in single and multisite settings but with expected deviation from uniformity close to perfect reliability value dp 0 0267 the expected deviations from the uniformity of pit histograms of the proposed model are 0 0233 and 0 0203 in single and multisite settings whereas these values are 0 0265 and 0 0237 for the benchmark model indicating slightly better reliability of the proposed model in terms of ci the average reduction during the low flow season is 18 3 and 21 1 in single and multisite settings reductions in the uncertainty bounds during the high flow season however are much larger with expected reductions of 41 3 and 45 1 through the single and multisite settings respectively within the high flow season the largest improvements in the uncertainty bounds happen during dry conditions with average reductions of 68 3 and 64 5 in ci through single and multisite settings respectively although the method is developed for lead time of one month the proposed algorithm is generic and can be applied in other basin our proposed method can be also extended by including more conditioning variables such as precipitation and or temperature whether observed or simulated this can provide a versatile and comprehensive model to inform water management models for better planning under current and future conditions we hope that our contribution here can inspire more efforts toward improved stochastic generation of streamflow in prediction and projection modes credit authorship contribution statement masoud zaerpour conceptualization methodology visualization investigation software writing original draft writing review editing simon michael papalexiou conceptualization methodology writing review editing ali nazemi conceptualization methodology writing review editing funding acquisition supervision declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement financial support for this study is provided by canada natural science and engineering research council through discovery grant program rgpin 2016 05470 as well as concordia university through various internal sources including strategic hire and excellence entrance award we appreciate shadi hatami of concordia university for her inputs on partial correlation input selection algorithm and its implementation to al cisneros and his lantern s of the soul supplementary materials supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2021 104037 appendix supplementary materials image application 1 
245,despite the existence of several stochastic streamflow generators not much attention has been given to representing the impacts of large scale climate indices on seasonal to interannual streamflow variability by merging a formal predictor selection scheme with vine copulas we propose a generic approach to explicitly incorporate large scale climate indices in ensemble streamflow generation at single and multiple sites and in both short term prediction and long term projection modes the proposed framework is applied at three headwater streams in the oldman river basin in southern alberta canada the results demonstrate higher skills than existing models both in terms of representing intra and inter annual variability as well as accuracy and predictability of streamflow particularly during high flow seasons the proposed algorithm presents a globally relevant scheme for the stochastic streamflow generation where the impacts of large scale climate indices on streamflow variability across time and space are significant keywords streamflow variability large scale climate indices stochastic streamflow generation predictor selection vine copulas algorithm development 1 introduction streamflow has been often represented as a function of other hydroclimatic processes such as temperature precipitation and evapotranspiration at the catchment scale blöschl et al 2007 these variables are affected by large scale climate patterns merz et al 2014 steirou et al 2017 tan and gan 2017 which can consequently impact streamflow generation globally kisi et al 2019 konapala et al 2018 ward et al 2014 for instance various evidences show that large scale climate indices lscis most notably el niño southern oscillation enso interdecadal pacific oscillation ipo and pacific decadal oscillation pdo influence the streamflow in australia mcgowan et al 2009 murphy and timbal 2008 pui and sharma 2011 pui et al 2012 enso north atlantic oscillation nao and atlantic multidecadal oscillation amo affect european streamflow giuntoli et al 2013 steirou et al 2017 similarly enso pdo amo and nao impact streamflow in north america asong et al 2018 nazemi et al 2017 rajagopalan et al 2000 tamaddun et al 2017 2019 wu et al 2020 previous studies have made it clear that taking into account the effects of lscis directly on streamflow or indirectly through affected hydroclimate variables e g temperature and precipitation may improve the predictability of streamflow particularly at seasonal to interannual scales e g kiem et al 2021 kwon et al 2008 steinschneider et al 2019 wasko and sharma 2017 the indirect incorporation of lsci in the generation of streamflow is in fact very common in the context of process based models in which variables such as temperature and precipitation are the basis of simulating streamflow eisner et al 2017 shrestha et al 2013 su et al 2017 process based models however are deterministically formulated by implementing physically based and or conceptual equations without explicitly considering the distributional and or joint properties of observed data montanari and koutsoyiannis 2012 farmer and vogel 2016 past studies showed that although dependence between precipitation and lscis can be low lscis have more consistent impacts on streamflow and or temperature nalley et al 2016 nazemi et al 2017 in particular the direct dependencies between streamflow and lscis in coarser spatial and temporal scales are rather strong this has motivated a strain of modeling attempts to explicitly incorporate the effect of lscis in streamflow generation through stochastic approaches lee et al 2018a liu et al 2015 wang et al 2009 one way to represent the impact of lscis on stochastic streamflow generation is to transform the original data into a gaussian process that can be then described using multivariate joint distributions bennett et al 2014 papalexiou 2018 wang and robertson 2011 the simplest representation of such kind can be formed by assuming a symmetric and linear teleconnection between streamflow and lscis linear models such as autoregressive ar and its variants have been widely used for streamflow simulation matalas 1967 salas et al 1985 lee et al 2010 prairie et al 2008 however they are unable to adequately represent marginal streamflow distributions especially in the case of asymmetric nonlinear and multimodal conditions papalexiou 2018 papalexiou and serinaldi 2020 rajogopalan et al 2019 which is the case in many regions and or finer timescales fleming and dahlke 2014 hlinka et al 2014 khan et al 2006 konapala and mishra 2016 lee et al 2018b nonparametric resampling schemes can address some of the issues in linear models lall and sharma 1996 sharma and o neill 2002 however the generated flows may end up being too close to the reshuffling of historical sequences e g grantz et al 2005 lee et al 2010 over the last two decades copula based models see genest and favre 2007 nelsen 2007 have gained popularity in hydroclimatology aghakouchak 2014 nazemi and elshorbagy 2012 and have been applied in various contexts including stochastic streamflow generation salvadori and de michele 2004 worland et al 2019 zhang and singh 2019 copulas offer a generic solution to multivariate probabilistic sampling particularly with respect to quantifying the risk chen et al 2015 hao and singh 2013 serinaldi and kilsby 2017 application of copulas in streamflow generation can provide an opportunity for preserving dependence structures in time and space and or between streamflow and other relevant variables copula based stochastic streamflow generations have been used at the single site quite extensively and are able to capture nonlinear responses observed in streamflow time series bardossy and pegram 2009 hao and singh 2012 nazemi et al 2013 wang et al 2019 more recently methods based on multidimensional copulas in particular vine copulas have been used for multisite streamflow generation chen et al 2019 nazemi et al 2020 pereira et al 2017 pereira and veiga 2018 despite ongoing advances in copula based streamflow generations only a few incorporate climate related proxies in streamflow generation slater and villarini 2018 wang et al 2019 and none to the best of our knowledge explicitly incorporate the influence of multiple lscis in the procedure of stochastic streamflow generation here we propose a generic approach based on vine copulas to explicitly incorporate lscis as exogenous covariates in stochastic streamflow generation at the monthly scale both in prediction and projections modes and at single and multiple sites we hypothesize that the explicit representation of lscis improves both prediction and projection skills particularly in terms of representing seasonality and inter annual variability we recognize that this is a challenging problem first the proposed model should be able to capture both symmetric and asymmetric relationships between lscis and streamflow in time and space see e g hoerling et al 1997 in addition as the statistical dependence between streamflow and lscis can change in both time and space the proposed model should have a dynamic structure wang et al 2019 nguyen huy et al 2020 for this purpose we use vine copulas in conjunction with a formal predictor selection algorithm to identify the best common set of lscis for streamflow generation at a monthly scale we showcase the application of the proposed scheme for the prediction and projection of three mountainous headwaters in southern alberta canada to benchmark the performance of the proposed algorithm we compare the skills of our model with already existing reference algorithms at the single site we compare the performance of our proposed algorithm with a baseline copula model developed by nazemi et al 2013 and extended into multisite mode using regression models nazemi and wheater 2014 both single and multisite versions of this existing copula based algorithm were previously implemented in the same case study vine copulas were also used to represent asymmetric and nonlinear spatial relationships among multiple streams chen et al 2015 2019 nazemi et al 2020 these models provide a benchmark to discuss the added value of incorporating lscis in the process of streamflow generation the remainder of this paper is organized as the following section 2 presents the methodological basis of the proposed algorithm section 3 briefly introduces our case study section 4 discusses the model development experimental setup and benchmarking procedures section 5 presents the results compares the proposed model with existing reference models and discusses the added value of incorporating lscis in stochastic streamflow generation finally section 6 concludes the study 2 methodology the core of our proposed algorithm is a vine copula linked to a formal input selection scheme for selecting a set of lscis as exogenous covariates that influence the streamflow at the considered timescale monthly throughout this paper we show below that the proposed algorithm is generic and can be applied in both single and multisite settings and in both prediction and projection modes by prediction we refer to short term precisely one step ahead probabilistic estimates of streamflow conditioned to a known initial state prediction mode has relevance to real time applications such as flood forecasting or operational planning of water resource systems by projection in contrast we refer to long term estimates of streamflow conditioned to a range of possible initial states projection mode is relevant to long term planning and management of water resource systems as well as scenario analysis particularly under changing climate and land use conditions below we illustrate the elements of the proposed algorithm and its procedure 2 1 vine copulas consider a d dimensional copula function c 0 1 d 0 1 in which c is a multivariate cumulative distribution function cdf of a random vector u 1 u 2 ud defined on the unit hypercube 0 1 d where ui denotes uniform marginal distributions u 0 1 see joe 1997 the fundamental work of sklar 1959 shows that for any multivariate cdf such as f x 1 x 2 xd of d random variables x 1 x 2 xd there is a copula function c that can describe f x 1 x 2 xd using marginal cdfs f 1 x 1 f 2 x 2 fd xd as the following 1 f x 1 x 2 x d c f 1 x 1 f 2 x 2 f d x d denoting that the original d dimensional cdf is decomposed into 1 a dependence structure between uniform random variables and 2 marginal cdfs that are defined independently from the dependence structure if the marginal distribution functions are continuous then the copula function c is unique and the joint probability density function pdf can be calculated as joe 1997 2 f x 1 x d c f 1 x 1 f d x d j 1 d f j x j where c is the d dimensional copula pdf and fj xj is the marginal pdfs of the xj j 1 2 d vine copulas provide a generic approach to construct high dimensional joint distributions by using bivariate copulas as building blocks as any high dimensional copula can be decomposed into a product of d d 1 2 bivariate copulas ordered as a sequence of d 1 nested trees with nodes joined by edges see bedford and cooke 2001 joe 1997 aas et al 2009 czado 2010 here we focus on a particular form of vine copulas i e canonical vines hereafter c vines which are based on ordering the variables by importance considering a multivariate density function as a product of conditional densities a c vine can be constructed by 3 f x 1 x d f 1 x 1 j 2 d f j 1 j 1 x j x 1 x j 1 where 1 j 1 1 2 j 1 based on the sklar s theorem and eq 2 we have 4 f j 1 j 1 x j x 1 j 1 c j 1 j 1 j 2 f x j 1 x 1 j 2 f x j x 1 j 2 f x j x 1 j 2 where f and f denote conditional cdfs and pdfs respectively by applying eq 4 recursively the joint density in eq 3 can be expressed as bedford and cooke 2001 5 f x 1 x d k 1 d f k x k i 1 d 1 j 1 d i c i j i 1 i 1 f x i x 1 i 1 f x j i x 1 i 1 the c vine copula construction in eq 5 involves marginal conditional distributions in the form f x v which can be obtained recursively using the h function derivative joe 1997 2014 6 h f x v c x v j v j f x v j f v j v j f v j v j where v is a d dimensional multivariate space with vj is one arbitrary component chosen from v and v j denotes the vector v excluding vj and c x v j v j is the bivariate copula 2 2 selecting large scale climate indices to inform streamflow generation while c vine copulas provide a theoretical framework for conditioning streamflow generation to lscis several lscis may influence streamflow at a given lead time and the relevant lscis in one time step may be irrelevant in others as a result there is a need for a systematic and rather dynamic identification of appropriate lscis at a given lead time quilty et al 2016 2019 robertson and wang 2012 statistical methods such as principal component analysis barnston and ropelewski 1992 independent component analysis aires et al 2000 partial mutual information selection pmi sharma 2000 sharma and mehrotra 2014 sharma et al 2016 and partial correlation input selection pcis may et al 2008 2011 have been developed and already used in the literature for predictor selection here we use pcis an iterative forward looking input selection algorithm which chooses one lsci at a particular lag during each iteration brown et al 2012 quilty et al 2016 2019 tran et al 2016 the selected lsci is the one that has the largest dependence score with the streamflow at a given monthly time step similar to amir jabbari and nazemi 2019 we apply kendall s tau kendall 1976 to measure the dependence between lagged monthly lscis and monthly streamflow the kendall s tau is more compatible for selecting lscis as there can be strong nonlinear dependence between lscis and the monthly streamflow hlinka et al 2014 konapala et al 2018 the algorithm terminates when adding new predictors causes no improvement in the bayesian information criterion bic estimated from the predictand residuals as we apply the pcis for each month separately the selected monthly lscis can change for more details on pcis see may et al 2008 2011 and hatami et al 2019 2 3 proposed streamflow generation scheme by choosing the relevant lscis c vine copulas can be used to allow conditioning streamflow at each time step to previous values of streamflow as well relevant lscis as exogenous covariates previous studies showed that at coarser time scales e g weekly monthly the lag 1 dependence is sufficient for streamflow generation chen et al 2015 nazemi and wheater 2014 wang et al 2019 fig 1 summarizes the proposed streamflow generation scheme at single site considering the lag 1 temporal dependence no need to mention that this framework can be extended to consider more lags and or to cover finer timescales for the sake of simplicity we only consider one lsci in the schematic and formulations are given below in the case of more lsci this formulation can be extended to higher dimensions following the procedure explained in section 2 1 in single site setting the task is to generate streamflow at any site a fig 1a where c vines are used to decompose the dependence structure between monthly flows at time t i e q t a t 1 i e q t 1 a as well as a relevant lsci at time t τ i e l t τ where τ is the lag between the monthly flow at time t and the selected lsci following the eqs 3 and 4 the joint distribution between q t 1 a q t a and l t τ can be established as 7 f q t 1 a l t τ q t a f 1 q t 1 a f 2 1 l t τ q t 1 a f 3 1 2 q t a q t 1 a l t τ where the subscripts 1 2 and 3 correspond to q t 1 a l t τ and q t a respectively using eq 4 the conditional distributions in eq 7 can be estimated as 8 f 2 1 l t τ q t 1 a f l t τ q t 1 a f q t 1 a c 1 2 f 1 q t 1 a f 2 l t τ f 1 q t 1 a f 2 l t τ f 1 q t 1 a c 1 2 f 1 q t 1 a f 2 l t τ f 2 l t τ and 9 f 3 1 2 q t a q t 1 a l t τ f l t τ q t a q t 1 a f l t τ q t 1 a c 2 3 1 f l t τ q t 1 a f q t a q t 1 a f l t τ q t 1 a f q t a q t 1 a f l t τ q t 1 a c 2 3 1 f l t τ q t 1 a f q t a q t 1 a c 1 3 f 1 q t 1 a f 3 q t a f 3 q t a as a result the three dimensional joint density in eq 7 can be represented in terms of bivariate copulas c 1 2 c 1 3 and c 2 3 1 with densities c 1 2 c 1 3 and c 2 3 1 as the following 10 f q t 1 a l t τ q t a f 1 q t 1 a f 2 l t τ f 3 q t a c 1 2 f 1 q t 1 a f 2 l t τ c 1 3 f 1 q t 1 a f 3 q t a c 2 3 1 f l t τ q t 1 a f q t a q t 1 a the graphical representation of the proposed three dimensional c vine copula is depicted in fig 1b and includes two trees in the first tree t 1 the circled nodes 1 to 3 represent respectively the three pdfs of flows at time step t 1 lsci signal with lag τ i e l t τ and the flow at time step t the dependencies between nodes i e edges are modeled using bivariate copulas formed for each pair the edges in the first tree become nodes for the second level accordingly the conditional distribution function of f q t a q t 1 a l t τ can be obtained by recursive use of eq 6 as aas et al 2009 czado et al 2012 11 h f q t a q t 1 a l t τ c 2 3 1 f q t a q t 1 a f l t τ q t 1 a f l t τ q t 1 a note that f q t a q t 1 a h q t a q t 1 a c 3 1 f q t a f q t 1 a f q t 1 a and f l t τ q t 1 a h l t τ q t 1 a c 2 1 f l t τ f q t 1 a f q t 1 a as a result eq 11 can be rewritten as 12 h f q t a q t 1 a l t τ h h q t a q t 1 a h l t τ q t 1 a in the multisite setting the streamflow generation is based on selecting a primary site which is used as a reference to realize streamflow in other reaches hereafter secondary sites using the temporal and spatial dependencies within and between streamflow series see nazemi et al 2013 2020 nazemi and wheater 2014 these dependencies are usually not the same and require asymmetric copulas to be handled e g chen et al 2015 grimaldi and serinaldi 2006 fig 2 a shows the multisite streamflow generation schematically let q t b and q t 1 b be the monthly flows at time step t and t 1 at the secondary site b note that we consider a unique lsci at time t τ i e l t τ relevant to both stations this has a physical relevance as the effect of lscis on streamflow regime is manifested regionally bonsal and shabbar 2008 nalley et al 2016 nazemi et al 2017 the extension of the proposed vine copula to a multisite setting is shown schematically in fig 2b based on eqs 4 and 5 the joint dependence can be constructed as 13 f q t 1 b q t 1 a l t τ q t b f 1 f 2 f 3 f 4 c 1 2 c 1 3 c 1 4 c 2 3 1 c 2 4 1 c 34 21 where c 1 2 is c 1 2 f 1 q t 1 b f 2 q t 1 a and c 1 3 and c 1 4 being defined similarly c 34 21 is c 34 21 f l t τ q t 1 a q t 1 b f q b t q t 1 a q t 1 b see also eqs 5 and 10 having the joint distribution function from eq 13 the conditional cdf f q t b q t 1 b q t 1 a l t τ can be obtained recursively by applying the appropriate h function from eq 6 14 h f q t b q t 1 b q t 1 a l t τ h h h l t τ q t 1 b h q t 1 a q t 1 b h h q t b q t 1 b h q t 1 a q t 1 b we apply the inverse forms of h functions given in eqs 12 and 14 for streamflow generation at single and multiple sites respectively in the prediction mode and single site setting the aim is to obtain f q t a based on the known states of f q t 1 a and f l t τ this is achieved by estimating the inverse of h function given uniform random numbers of ε 15 q t a f 1 h 1 h 1 ε h l t τ q t 1 a q t 1 a the ensemble generation is obtained by synthesizing a large number of uniformly distributed random numbers 10 000 throughout this paper to realize corresponding scenarios of q t a this large number of realizations is needed to account for sampling uncertainty and to come up with confidence bounds roy and gupta 2020 the mean value of these realizations is considered as the best prediction at each time step similarly in the multisite mode the procedure of streamflow generation is also based on monte carlo simulations using the information available for q t 1 b q t 1 a and l t τ using the inverse h functions 16 q t b f 1 h 1 h 1 h 1 ε h h l t τ q t 1 b h q t 1 a q t 1 b h q t 1 a q t 1 b q t 1 b the process of ensemble generation in the projection mode is very similar to the prediction mode with the exception that the conditioning is not based on known antecedent quantities of streamflow for this purpose having the appropriate predictors i e lscis at each month flows can be generated at the following month conditioned to the given lscis 3 case study and data the oldman river and its tributaries form a large basin 27600 km2 and provide around 40 of the total annual streamflow in southern alberta canada alberta environment 2010 the oldman river originates from mountainous headwaters in the eastern slopes of the rocky mountains and joins the bow river before rolling downstream toward the province of saskatchewan the basin is located in a semi arid cold region where snowmelt from the rocky mountains headwaters is the main source of the annual water supply providing between 70 to 90 of the total annual flow volume nazemi et al 2017 the water resource system in this basin is complex including several sectors of water demand some with competing interests e g environmental vs irrigation water demands irrigation is the largest water consumption in the basin and includes 88 of the total water demand zandmoghaddam et al 2019 the basin is currently under pressure due to rapid socio environmental changes that intensify water demand and or loss e g inclined evapotranspiration due to warming this has created a regional water security concern where natural streamflow cannot meet additional water demand gober and wheater 2014 martz et al 2007 this concern will be even more in the future and under heightened climate variability and change as the natural streamflow is also under rapid changes zaerpour et al 2020 mainly due to the decline in the winter snowpack in the rocky mountains prowse et al 2006 we have chosen three headwater rivers upstream of the oldman reservoir namely the oldman river near waldron s corner crowsnest river near lundbreck and castle river near beaver mine together they provide 95 of the natural inflow to the oldman river nazemi et al 2017 fig 3 shows the map of the oldman river in which the three considered headwater streams are identified table 1 shows the key information about these streams for each stream we extract the monthly streamflow values for the period of 1967 to 2008 from the water survey of canada s hydrometric database hydat http www wsc ec gc ca we consider six large scale climate indices including pdo enso nao amo arctic oscillation ao and pacific north american pna as potential lscis for streamflow generation these lscis are the main ocean atmospheric patterns that affect hydroclimatological variables in canada bonsal et al 2006 coulibaly and burn 2004 nalley et al 2019 rasouli et al 2020 whitfield et al 2010 in brief the pdo is a large scale climate pattern of sea surface temperature fluctuations in the pacific with the periodicity of inter decadal to multi decadal scales mantua et al 1997 pdo data for the considered period is obtained from the joint institute for the study of the atmosphere and ocean university of washington http jiasao washington edu pdo pdo latest the nao is represented by the atmospheric pressure at sea level between the icelandic low and the azores high with a periodicity of around 3 6 years and is pronounced during the cold season the ao is characterized by atmospheric circulation patterns over the extra tropical northern hemisphere where sea level pressures over the polar vary in opposition to middle latitudes at around 45 n thompson and wallace 1998 nao and ao data are obtained from national center for environmental information http www ngdc noaa gov the amo is an atmospheric oceanic phenomenon with a periodicity of 50 70 years that arises from the variations in sea surface temperature in the atlantic ocean enfield et al 2001 amo data are obtained from earth system research laboratories center http www esrl noaa gov psd data correlation amon us long data the pna pattern features a sequence of high and low pressure anomalies stretching from the subtropical west pacific to the east coast of north america pna data are extracted from physical sciences laboratory https www psl noaa gov data correlation pna data enso represents large scale ocean atmosphere oscillations in the tropical pacific influencing climatic conditions around the globe including canada trenberth 1997 enso data is obtained from the climate prediction center http www cgd ucar edu cas catalog climind regarding enso we consider four indices including nino3 nino3 4 nino4 and nino1 2 anomalies selected using the predictor selection method based on the level of dependence to monthly flows 4 experimental setup and benchmarking approach we apply the proposed algorithm in the three considered streams in single and multisite settings and in both prediction and projection modes in the single site setting streamflow is generated at each site independently in the multisite setting however one stream should be chosen as the primary site from which streamflow is realized regionally in other locations here we follow the generation pathway used by nazemi and wheater 2014 they performed a rigorous experiment to choose the best pathway for streamflow generation and concluded that the oldman river near waldron s corner site a can be considered as the primary reach for the other two reaches most essentially the performance of our proposed algorithm is benchmarked with observed streamflow characteristics we also compare the performance of our proposed scheme at the single site with an existing reference model proposed by lee and salas 2011 and applied in the oldman river basin by nazemi and wheater 2014 the basic idea in such a model is to conditionally resample flow at the lead time of one month from the antecedent streamflow conditions without considering the effect of lscis similarly in the multisite setting we compare the performance of our proposed algorithm with the multisite algorithm based on vine copula developed by chen et al 2015 2019 and used in nazemi et al 2020 table 2 shows the notation used for proposed and reference algorithms in prediction and projection modes the first step in our proposed algorithm is to select a set of relevant lscis at each monthly timestep the pool of lscis from which relevant indices at each timestep are selected consists of the six named lscis up to 24 months lags we initially considered more lag times up to 48 months see nalley et al 2016 however inline with several other studies in western canada e g shabbar et al 1997 or globally chiew and mcmahon 2002 the analysis of kendall s tau rank dependence showed that the memory of lscis on streamflow in the three considered streams does not go beyond 24 months we then use the pcis algorithm to select the best set of lscis predictors at each month from a pool of options as the three streams are located in close proximity and are under the influence of similar lscis we aim at finding a common set of predictors at the three sites see hatami et al 2019 for the details of using the pcis algorithm for global input selection in multiple locations we also compare the results with pmi to ensure that a robust set of lsci is selected finally we select the structure of vine copulas by the maximal spanning tree algorithm as described by czado et al 2013 and considering empirical distributions in margins for both monthly streamflow and the selected lscis in this context a fast sequential estimation procedure for parameter estimation is suggested dissmann et al 2013 in brief the first bivariate copula families and parameters for the first tree are identified before calculating the weights for the second tree and so on we consider a set of well known parametric copula functions including gaussian student t clayton gumbel frank joe and their rotated forms for setting up the vine copulas see pereira and veiga 2018 for the formulation of these copulas the copula identification and parameterization are performed by applying the maximum log likelihood method and considering the bayesian information criteria bic akaike 1979 as the goodness of fit see also sadegh et al 2017 the modeling is implemented in the r platform by utilizing the vinecopula nagler et al 2019 schepsmeier et al 2016 cdvine brechmann and schepsmeier 2013 copula hofert et al 2014 cdvinecopulaconditional bevacqua 2017 and npred packages sharma and mehrotra 2014 the performance of the proposed algorithm is evaluated in the prediction and projection modes and compared with reference models using a set of metrics in the projection mode we generate 10 000 synthetic historical streamflow realizations with the same length as the observed data to demonstrate the performance of projected streamflow ensembles basic statistics including long term monthly mean value standard deviation skewness as well as the lag 1 temporal and lag 0 spatial dependencies are calculated to measure the relative improvement in the performance of the proposed method in representing persistence in the historical streamflow we adopt the interannual variability metric johnson and sharma 2011 taylor 2001 17 s 4 1 r 4 σ f 1 σ f 2 1 r 0 4 where r is the correlation coefficient between observed and simulated streamflow σ f is the ratio of the expected standard deviation of ensemble projections to the standard deviation of the observed streamflow and r 0 is the maximum theoretical correlation which is taken as 1 the range of s varies between 0 and 1 the score of 1 happens when both σ f and r are equal to one and there is no error in capturing the interannual variability the relative improvement in the performance of the proposed model in capturing the interannual variability over the reference model is given by the interannual variability skill score as wang and robertson 2011 18 s s proj s s ref 1 s ref 100 where s and s ref represent the interannual variability metric values for the proposed and the reference model respectively in the prediction mode we similarly generate 10 000 realizations of streamflow series with the same length as observed data in both single and multisite settings we calibrate and validate the predictions using the buffered leave one out cross validation method le rest et al 2014 roberts et al 2017 for prediction in each year the corresponding year and the two years ahead are excluded while the remaining years are used for predictor selection and model building procedures the predicted monthly flow is then compared with the observed data we present the overall skill of predictions as a skill score which is the relative improvement in error scores ess of the prediction over a set of reference predictions wang et al 2009 19 s s pred e s ref es e s ref 100 where es and esref represent the five considered error metrics calculated for the proposed and the reference model respectively each skill measure assesses different aspects of the forecast distribution a higher ss value indicates better performance in the proposed model with a score of 0 representing the same performance as the reference model the first skill score used is ssmae which is based on expected mean absolute error mae of the predicted mean mae measures the expected mean absolute error of predicted monthly streamflow ensemble and is defined as 20 mae t 1 t x sim x obs t where t is the total number of years the second score ssrmse is based on expected root mean squared error rmse of the predicted mean rmse assesses the standard deviation of predicted error applied to the ensemble mean 21 rmse t 1 t x sim x obs 2 t the third skill score ssrmsep is based on the expected root mean squared error in probability rmsep bennett et al 2014 wang and robertson 2011 of the predicted mean the rmsep measures predicted error on a probability scale giving the predicted events similar opportunity to contribute to the overall assessment of predicted skill 22 rmsep 1 t t 1 t f sim x t f obs x t 2 where f obs xt is the cdf of the historical streamflow data f sim x t is the cdf of the expected predicted streamflow rmsep is less sensitive to events with large errors wang and robertson 2011 the fourth score sscrps is based on the expected continuous ranked probability score crps harrigan et al 2018 kaune et al 2020 wang and robertson 2011 of the predicted mean unlike rmsep crps metric measures the error of the whole predicted probability distribution and can be sensitive to just a few usually very high flow events with large prediction errors 23 crps 1 t t 1 t f sim x t h x t x obs 2 d x where f sim xt is the predicted cdf and h xt x obs is the heaviside step function defined as wang and robertson 2011 24 h x t x obs 0 x t x obs 1 x t x obs finally the fifth score sskge is based on the expected kling gupta efficiency kge gupta et al 2009 the kge focuses on three criteria including correlation between observations and simulations the bias and the relative variability in the simulated and observed values expressed as 25 kge 1 r 1 2 σ sim σ obs 1 2 μ sim μ obs 1 2 where r is the correlation between observations and simulations σobs and σsim are the standard deviations of observed and simulated values μobs and μ sim are the means of observed and simulated values kge values close to 1 represent perfect model performance note that unlike the previous four scores where zero indicates the best performance kge 1 indicates the perfect agreement between simulations and observations to allow homogeneous comparison between all score we scale the kge to skill score as knoben et al 2019 26 s s kge kge kg e ref 1 kg e ref 100 apart from accuracy we also measure the reliability of predicted ensembles by assessing the statistical consistency of the predicted probability distributions and the associated observed events jolliffe and stephenson 2011 toth et al 2003 in this study we use histograms of probability integral transforms pit dawid 1984 gneiting et al 2007 to assess the average reliability of the prediction distributions the pit of the observed value is given as 27 pit f sim x obs where f sim denotes the cdf of simulated flow and f sim x obs therefore is the non exceedance probability of the observed streamflow based on the cdf of the simulated streamflow the pit values are then assigned to different histogram bins and the frequency of each bin is calculated the predictive distribution is said to be reliable if the pit values are distributed uniformly u shaped histograms usually indicate that the probabilistic predictions have under dispersion on the contrary if the pit histogram is hump shaped then the probabilistic predictions indicate over dispersion the deviation metric d quantifies the deviation from uniformity in pit this measure which is introduced by nipen and stull 2011 and later used in bourdin et al 2014 can be defined as 28 d 1 b i 1 b b i t 1 b 2 where b is the number of bins bi is the number of observations in bin i and t is the size of the dataset lower variability in bin frequency is indicative of a flatter pit sampling limitations however can cause a reliable prediction not generate a perfectly flat pit histogram leading to an expected value of the deviation e dp for perfect prediction brocker and smith 2007 pinson et al 2010 given by 29 e d p 1 b 1 t b note that apart from the given metrics we also look at the 95 confidence interval ci of generated ensembles both in terms of probabilistic characteristics as well projected predicted timeseries 5 results and discussions 5 1 influential large scale climate indices in upper oldman we initially analyze the dependence between lagged lscis and monthly streamflow in the three considered headwaters considering 1 to 24 months lags for six considered lscis a pool of 144 lscis is formed for each month from which the relevant lscis for a given month are chosen using the global version of pcis presented in hatami et al 2019 results are summarized in fig 4 each panel includes the results at one site in which rows are months ordered from october to september from the top to the bottom columns show the six considered lscis in each month the relevant lscis that are selected through the pcis algorithm are numbered and shaded numbers identify relevant lags and colors indicate values of kendall s tau dependencies between selected lscis and the corresponding monthly flows as it can be observed the dependencies between lsci and monthly streamflow are dynamic within a year and there can be months in which more than one lscis to which streamflow is significantly dependent this is particularly the case in high flow months e g april s flow in which the fully extended version of c vine mentioned in section 2 1 should be implemented among the considered lscis pdo is the most frequent lsci selected across different months and demonstrates the strongest dependence with spring and summer flows this empirical evidence is in line with previous findings showing that the pdo is the dominant lscis in this region e g fleming et al 2007 nazemi et al 2017 nao and enso are the other two key lscis which is in line with findings of gobena and gan 2006 for streamflow in southwestern canada additionally ao and pna are among lscis selected more than once these results are inline with previous findings over western canada e g bonsal and shabbar 2008 gobena and gan 2006 to address the uncertainty in the selected predictors we repeat the input selection using the pmi algorithm pmi uses mutual information as oppose to the kendall s tau used in the modified pcis used in this study results are presented in fig s1 in the supplement showing a considerable match between the results of pcis and pmi in 7 out of 12 months including all wet months the results of pmi and pcis are fully identical in months january april and august pmi chooses some lsci that pcis does not identify similarly in may pcis can identify a lsci that pmi has missed only in february which is a dry month pmi and pcis result into fully divergent sets of relevant lscis this intercomparison certifies the selection of relevant lscis 5 2 benchmarking the skill of the proposed model in the projection mode by identifying the influential lsci the proposed model can be setup and evaluated using the procedure explained in section 4 we first evaluate the results in the projection mode fig 5 shows the long term expected statistics of the first three moments of the monthly streamflow i e mean standard deviation and skewness obtained from 10 000 realizations with the same length as the historical data in each panel pink and gray lines depict the results obtained by the reference and proposed models respectively black lines indicate the observed values in general both the reference and proposed algorithms are able to preserve the observed statistics very well considering the single site generation and in site a relative errors res in projecting the expected long term monthly flow for both models is less than 5 across all months although reconstructing observed values of monthly standard deviation and skewness entails more error the res remain less than 10 for both the reference and proposed models except for the skewness of monthly flow in february reconstructed by the proposed model re 21 in site b both the reference and proposed models can represent the long term expected statistics of the three moments in all months with res less than 10 in both single and multisite settings except for the single site generation in months of august and september in which res in the representation of standard deviation and skewness obtained by the reference model is higher than 20 in site c the res in the representation of long term expected statistics of monthly flows by the reference and proposed models are less than 5 in the single site setting while in the multisite reconstruction of site b both proposed and reference models show similar performance the res in the representation of the long term expected mean and standard deviation of monthly flows are marginally higher for the reference model in site c yet remain below 10 the res in the multisite generation of long term expected skewness of monthly flows in site c is lower for the proposed model particularly in the low flow season we also assess the performance of our proposed algorithm in preserving the lag 1 temporal dependencies within each site as well as lag 0 spatial dependence between primary here site a and secondary sites here sites b and c fig 6 summarizes findings in the representation of a lag 1 temporal and b lag 0 spatial dependencies respectively black lines show the observed lag 1 and lag 0 dependencies pink and gray colors illustrate the results obtained by the reference and proposed models in single and multisite settings respectively in summary both reference and proposed algorithms are able to preserve temporal and spatial dependencies quite well in the single site setting the reference and proposed algorithms are able to preserve the lag 1 temporal dependencies with an absolute error of less than 0 1 in the multisite setting the proposed model can preserve lag 1 temporal dependence with absolute error values of less than 0 1 except few cases during the low flow season in site c the reference model however show better performance in capturing the temporal dependence in those months the interesting point is that the lag 1 dependence in site b is preserved better in multisite setting when streamflow in site b is conditioned based on the generated flow in site a this however is not the case in site c regarding the spatial dependencies both the reference and proposed models can capture the spatial dependencies with less than 0 1 absolute error in multisite scheme we assess the performance of the reference and proposed models in capturing the interdependencies between lagged lscis and the flows at a monthly scale fig 7 depicts the expected mae in representing observed interdependencies between influential lscis and monthly flows demonstrated in fig 4 panel a is related to the reference algorithm and panels b and c are related to the performance of the proposed algorithm in single and multisite settings respectively in general the expected mae in the representation of interdependencies for the reference model is high mae 0 25 this is due to the fact that the reference model does not explicitly represent the interdependencies between lagged lsci and monthly streamflow the performance of the proposed model in contrast is significantly better showing an expected mae of below 0 1 for high flow season and below 0 15 for low flow season in single site setting the expected mae in multisite setting is below 0 15 for both low and high flows the projected streamflow ensembles generated by proposed and reference models in both single and multisite settings are investigated in the form of hydrographs for the driest year 1977 78 a normal year i e near long term average 2008 2009 and the wettest year 1995 96 as illustrated in fig 8 for simulations during the entire data period see figs s2 to s6 in the supplement the pink and gray ensembles are the 95 ci obtained by the reference and proposed models respectively the black lines show the observed time series this figure clearly shows that proposed and reference models can capture the observed time series within the 95 cis having said that the seasonality can be much better represented by the proposed model than the reference model this can be supported by significantly narrower ci is in the case of the proposed model considering single site generation and in the dry year the expected reduction in the ci across the three sites is 45 and 54 9 during low and high flow seasons respectively the expected reduction in projected ci in the multisite setting is slightly lower and marks 40 6 and 51 3 in low and high flow seasons respectively in the normal year the expected reduction in the ci across the three sites in the single site setting are 30 6 and 22 during low and high flow seasons respectively in contrast to dry year the expected reduction in multisite setting is more than single site and marks 38 8 and 23 3 during low and high flow seasons respectively in the wet year and through single site generation the average reduction in the proposed model is 26 8 and 35 during low and high flow seasons respectively similar to the dry year the improvements in predictive uncertainty are slightly less in multisite setting and marks 21 3 and 28 8 reduction compared to the reference model during low and high flow seasons respectively we also analyze the performance of the proposed model in representing the extremes for this purpose we fit a generalized extreme value gev distribution to the annual low and high flows enabling the extrapolation beyond the range of observed data fig 9 shows the uncertainty bounds of ensembles of 10 000 gevs fitted to simulated annual low and high flows in single fig 9a and multisite settings fig 9b fig 9 includes two rows showing the ensembles of fitted gevs for the simulated annual low top rows and high flows bottom rows the dots are the observed extremes and the black lines are the fitted gev distribution to the observed extremes the skill is assessed by the percentage of reduction in ci of generated streamflow 10 000 realizations as the percentage of coverage poc which is the percentage of observed data falling within the ci of generated streamflow in general both reference and the proposed model demonstrate high skills in capturing the observed extremes within the ci in the single site setting and for the low flows the proposed and reference models are expected to capture 88 9 and 92 9 of extremes within their ci respectively regarding the high flows proposed and reference models show pocs of 95 2 and 96 0 respectively similar skills can be seen in the multisite setting where both pocs are 90 5 and 91 7 for proposed and reference models in representing the annual low flows respectively these pocs changes in the case of high flows to 94 0 and 96 4 respectively while the poc statistics are slightly better in the case of the reference models cis are substantially reduced providing less uncertainty in the projected flow in single site setting and for low flows the average range of ci of the proposed model is reduced by 11 7 compared to the reference model whereas this value is 3 1 in the multisite setting regarding the high flows the range of ci of the proposed model is much reduced compared to the reference model reaching to average of 15 5 and 31 1 reduction in single and multisite settings respectively finally we investigate the performance of the proposed model in representing of interannual variability fig 10 represents percentages of improvement in interannual skill scores at single panel a and multiple sites panel b the average improvement in this skill score in the single site setting is 10 2 across the three considered sites the lowest and highest improvements occur in site a and site b with average improvements of 3 6 and 14 1 respectively the proposed model demonstrates improvements in capturing interannual variability during high flow season with an average improvement of 16 0 in single site generation and across the three sites in the multisite setting the average improvement in the interannual variability skill score is slightly lower with expected improvements of 13 5 and 10 1 in sites b and c respectively having said that during the high flow season considering relevant lsci in the multisite generation results in more improvements compared with the single site generation the expected improvement in capturing the interannual variability during the high flow season is 23 5 in sites b and c 5 3 benchmarking the skill of the proposed model in the prediction mode in the prediction mode five skill scores including ssmae ssrmse ssrmsep sscrps and sskge are reported for single fig 11 a and multisite fig 11b settings at the three considered sites generally speaking predictions made by the proposed algorithm exhibit better skills than reference models the average improvement in single site predictions is ss mae 5 3 ss rmse 4 5 ss rmsep 5 8 ss crps 6 2 and ss kge 14 3 the range of changes in skill scores varies largely across different months and extends from 15 to 35 the greatest improvement in skill scores in the single site generation occurs in high flow season with average increases of ss mae 15 3 ss rmse 12 8 ss rmsep 13 4 ss crps 14 1 and ss kge 27 1 in the multisite prediction of streamflow in secondary sites b and c a similar pattern can be seen except few discrepancies in low flow season in general simulation made by the proposed model exhibits improvement in almost all months with average improvements of ss mae 5 4 ss rmse 3 1 ss rmsep 6 4 ss crps 4 9 and ss kge 15 4 the most encouraging result is the improvement in the skill scores during the high flow season reaching to ss mae 16 4 ss rmse 14 9 ss rmsep 14 1 ss crps 15 8 and ss kge 28 3 these skills vividly show the value gained by incorporating lscis into streamflow prediction particularly in high flow seasons in addition to the improvement in the prediction errors we assess the reliability of streamflow predictions fig 12 presents histograms of the pits related to the prediction using the proposed scheme against its reference counterparts in single fig 12a and multisite settings fig 12b each panel includes two rows showing the pit histograms for the low and high flow seasons winter and spring top and bottom rows respectively the gray and pink envelopes show the pit histograms obtained by 10 000 realizations generated by the proposed and reference schemes respectively the deviations from uniformity in pit histograms can be quantified using the measure of degree of uniformity d with lower values indicating better reliability perfectly reliable predictions produce a pit histogram with a uniform distribution the expected value of perfect prediction dp is 0 0267 regarding the low flow season although the predictions made by both the reference and proposed schemes exhibit slight overdispersion but in general both models show average deviation from pit uniformity close to the perfect prediction in low flow season and in single site the proposed model shows slightly better reliability in prediction of observed values with average d 0 0251 compared to d 0 0267 for the benchmark scheme similarly in multisite setting the average deviation related to pit of the proposed scheme is 0 0242 compared to 0 0262 for the benchmark scheme indicating slightly better reliability of the proposed scheme regarding the high flow season similar to low flow both models exhibit slight overdispersion in single site setting the average deviation of pit related to the proposed model is 0 0214 compared to 0 0262 for the benchmark model demonstrating slightly better reliability of the proposed model in multisite setting these values are 0 0163 and 0 0211 for the proposed and benchmark models respectively additionally to further assess the reliability of predictions in single and multisite settings particularly in terms of the extremes we evaluate the improvement in the 95 cis of the proposed scheme against its reference counterparts in single fig 13 a and multisite settings fig 13b each panel includes two rows showing the cis in the low and high flow seasons i e winter and spring top and bottom rows respectively the gray and pink envelopes show the 95 ci obtained by 10 000 realizations generated by the proposed and reference algorithms respectively black dots are the observed flow qsuantiles and the solid lines are the best predictions mean ensembles obtained either in single site panel a or multisite panel b settings for each flow season the uncertainty intervals during dry flows with percentile level of 0 1 or lower and wet conditions flows with percentile level of 0 9 or higher are separated from the rest of the flow conditions by the dashed red lines based on fig 13 empirical distributions of the observed high and low flows fall within 95 ci of both the reference and proposed models in all sites and through single and multisite settings having said that the proposed model can estimate the flow with much less uncertainty the average reduction in ci during the low flow season is 18 3 and 21 1 in single and multisite settings reductions in the uncertainty bounds during the high flow season however are much larger with expected reductions of 41 3 and 45 1 through the single and multisite settings respectively in the low flow season the highest reductions in the uncertainty bounds occur in the normal flow conditions between 0 1 to 0 9 percentile with expected reductions of 26 3 and 31 4 in ci through single and multisite settings respectively in the high flow season the largest improvements in the uncertainty bounds happen during dry conditions with average reductions of 68 3 and 64 5 in ci through single and multisite settings respectively 6 summary and conclusion skillful and reliable streamflow prediction and projection capabilities are of urgent needs for water resource planning and management here we aim at improving the skills of stochastic streamflow generation at single and multiple sites by explicit incorporation of lscis through a stochastic generator in particular we suggest a sampling scheme based on using c vine copulas in which the structure and parameters of the model change at each time step this is due to the variation of the influential lscis and the type of dependencies which can change across different timesteps here monthly to accommodate this we use a global input selection algorithm to pick the most significant lscis from a pool of potential predictors at each month and consider the selected lscis as influential predictors to which the streamflow is conditioned to showcase the application of this model in practice we demonstrate the use of the proposed algorithm at single and multiple sites and in both prediction and projection modes in three headwaters of the oldman river basin alberta canada the performance of the proposed algorithm is rigorously compared with the observed data as well as reference models that are essentially identical to proposed models yet do not use any information about lscis within stochastic streamflow generation the results of our study show that while simulation performances of the reference and proposed models are very similar in representing the first three moments of monthly streamflow as well as lag 1 temporal and lag 0 spatial dependencies within and between streamflow reaches the explicit consideration of lscis can significantly reduce simulation uncertainty and improve simulation skills in projection and predictive modes and in both single and multisite settings particularly in high flow seasons in projection mode and considering the single site setting the expected reductions in ci in the three sites are 34 1 and 37 1 in low and high flow seasons respectively in multisite setting these values are 33 6 and 34 5 in low and high flow seasons respectively the average improvement in the interannual variability skill score in single and multisite settings is over 10 across the three considered sites having said that during the high flow season considering relevant lsci in the multisite streamflow generation results in expected improvement in capturing the interannual variability by more than 23 in terms of observed annual extremes both reference and proposed models can capture the observed values within the range of ci with high poc values of greater than 90 in most cases the proposed model however demonstrates less uncertainty in capturing the observed extremes particularly regarding high flows for which the ranges of ci are decreased by 15 5 and 31 1 in single and multisite settings respectively in prediction mode both reference and proposed models show slight overdisperision in pit histograms in single and multisite settings but with expected deviation from uniformity close to perfect reliability value dp 0 0267 the expected deviations from the uniformity of pit histograms of the proposed model are 0 0233 and 0 0203 in single and multisite settings whereas these values are 0 0265 and 0 0237 for the benchmark model indicating slightly better reliability of the proposed model in terms of ci the average reduction during the low flow season is 18 3 and 21 1 in single and multisite settings reductions in the uncertainty bounds during the high flow season however are much larger with expected reductions of 41 3 and 45 1 through the single and multisite settings respectively within the high flow season the largest improvements in the uncertainty bounds happen during dry conditions with average reductions of 68 3 and 64 5 in ci through single and multisite settings respectively although the method is developed for lead time of one month the proposed algorithm is generic and can be applied in other basin our proposed method can be also extended by including more conditioning variables such as precipitation and or temperature whether observed or simulated this can provide a versatile and comprehensive model to inform water management models for better planning under current and future conditions we hope that our contribution here can inspire more efforts toward improved stochastic generation of streamflow in prediction and projection modes credit authorship contribution statement masoud zaerpour conceptualization methodology visualization investigation software writing original draft writing review editing simon michael papalexiou conceptualization methodology writing review editing ali nazemi conceptualization methodology writing review editing funding acquisition supervision declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement financial support for this study is provided by canada natural science and engineering research council through discovery grant program rgpin 2016 05470 as well as concordia university through various internal sources including strategic hire and excellence entrance award we appreciate shadi hatami of concordia university for her inputs on partial correlation input selection algorithm and its implementation to al cisneros and his lantern s of the soul supplementary materials supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2021 104037 appendix supplementary materials image application 1 
246,multiphase flow simulation is well known to be computationally demanding and modeling large scale engineered subsurface systems entails significant additional numerical challenges these challenges arise from a the presence of small scale discrete features like shafts tunnels waste packages and barriers b the need to accurately represent both the waste form processes at the small spatial scale of the repository and the large scale transport processes throughout heterogeneous geological formations c the strong contrast in material properties such as porosity and permeability as well as the nonlinear constitutive relations for multiphase flow numerical solution entails discretization of the coupled system of nonlinear governing equations and solving a linear system of equations at each newton raphson iteration practical problems require a very large number of unknowns that must be solved efficiently using iterative methods in parallel on high performance computers the unique challenges noted above can lead to an ill conditioned jacobian matrix and non convergence with newton s method due to discontinuous nonlinearity in constitutive models moreover practical applications can require numerous monte carlo simulations to explore uncertainly in material properties geological heterogeneity failure scenarios or other factors governmental regulatory agencies can mandate these as part of performance assessments hence there is a need for flexible robust and computationally efficient methods for multiphase flow in large scale engineered subsurface systems we apply the open source simulator pflotran to the practical problem of performance assessment of the us doe waste isolation pilot plant wipp site the simulator employs a finite volume discretization and uses the petsc parallel framework we evaluate the performance of several preconditioners for the iterative solution of the linearized jacobian system these range from stabilized biconjugate gradient with block jacobi preconditioning bcgs to methods adopted from reservoir modeling such as the constrained pressure residual cpr two stage preconditioner and flexible generalized residual solver fgmres we also implement within petsc the general purpose nonlinear solver newton trust region dogleg cauchy ntrdc which truncates the newton update or modifies the update with a cauchy solution that is within the quadratic model trust region of the objective function we demonstrate the effectiveness of each method for a series of test problems with increasing difficulty we find that the ntrdc and fgmres cpr abf fca preconditioners generally perform best for the test problem having the extreme nonlinear processes achieving a 50x speed up compared with bcgs the most ill conditioned and extreme nonlinear simulations do not converge with bcgs as one may expect but they do complete the simulation with ntrdc and fca we also investigate the strong scalability of each method and demonstrate the impact of node packing upon parallel performance on modern processor architectures keywords preconditioner nonlinear porous media multiphase flow trust region subsurface system 1 introduction simulation of multiphase porous flow is required to address a wide range of practical engineering problems such as nuclear waste disposal enhanced oil recovery contaminant remediation geothermal engineering and carbon sequestration understanding and predicting underground phenomena can help mankind address world issues like climate change clean water and renewable energy numerical solution of the strongly coupled nonlinear pdes presents significant numerical challenges to the commonly used stabilized biconjugate gradient bicgstab with incomplete lu factorization ilu preconditioner the flow and reactive transport simulation typically uses the finite volume method fvm to conserve mass accurately for reaction calculations and the newton raphson nr method is widely used to solve nonlinear equations a linear system of equations namely the linear jacobian matrix must be solved at each nr iteration the nature of immiscible multiphase physics and challenges of applications to large scale heterogeneous geological domains produces an ill conditioned matrix for the linear solver due to the highly nonlinear nature of capillary pressure and relative permeability constitutive relations as well as other mechanistic models like rock compressibility and fracturing as a function of liquid pressure and large contrasts in grid cell volumes and neighboring permeability of discretized rock formations and engineered features the waste isolation pilot plant wipp the nation s only active nuclear waste repository in carlsbad new mexico national research council 1996 presents the complex simulation domain and process models appendix a our simulator pflotran hammond et al 2012 lichtner et al 2015 is a massively parallel subsurface flow and reactive transport code for potential use in future performance assessment pa calculations for the wipp helton et al 1997 appendix b the goal is to implement 3d simulations of immiscible isothermal 2 phase flow in the wipp in a parallel computing environment models constructed for the development and testing of the new capabilities are used here to demonstrate advancements in linear and nonlinear solvers that improve performance the wipp pa requires 1800 realizations of the 10 000 year 3d simulation models to complete the full uncertainty quantification the desired run time for 1800 simulations is 3 weeks on the wipp high performance computing cluster and we demonstrate results for 3 simulations exhibiting a wide range of numerical difficulty as a part of the pa process the default solvers were tested on a wide variety of realizations in all cases the run times were either too long or the simulations did not finish within the 48 hour time limit using 64 cores this very practical need to successfully meet regulatory pa for the nation s only active nuclear waste repository was the main motivation for the development and testing of newly implemented linear and nonlinear solvers our simulator already has the capability to simulate immiscible multiphase flow but the challenge arises from a modeling cubic meter to cubic kilometer volumes in the same domain b high grid cell aspect ratios forced by stratigraphy of the natural surroundings of the wipp c the extreme material properties e g permeability and porosity of bedded salt rock formation and d large contrast in material properties due to open excavation next to the formation these considerations introduce difficulty in solving the linear system of equations by causing the jacobian matrix to be ill conditioned resulting in failures of the conventional parallel iterative krylov solver an effective solver requires proper scaling of solution vectors for each fluid phase preconditioners tailored to the governing equations and interprocessor communication that maximize parallel performance in this work we integrate several well established numerical methods to improve computational performance and solver robustness to enable simulation of challenging real world subsurface engineered systems in particular to address the ill conditioned matrix and breakdown of the krylov solver the constrained pressure residual cpr preconditioner wallis et al 1985 was tested with two different decoupling methods alternate block factorization abf bank et al 1989 and quasi implicit pressure and explicit saturation impes lacroix et al 2001 the flexible generalized minimal residual method fgmres saad 1993 linear solver was used and the performance was compared to the widely used bicgstab linear solver and ilu preconditioner saad 2003 the integration of these well established preconditioners was the key to improving computational performance and solver robustness the nonlinearity in the wipp model is also very difficult to solve due to discontinuous nonlinear constitutive relationships such as salt creep rock fracturing and borehole intrusions these constitutive relations have abrupt changes and piecewise continuous features for which analytical derivatives do not exist these features are to be expected for repository systems which contain discrete layered engineered and natural media with fitted curves from experimental data to improve solution of the nonlinear optimization problem we apply the newton trust region dogleg cauchy ntrdc method to enhance convergence of the newton raphson iterations powell 1970 the efficient linear and nonlinear solvers are compared to bicgstab block jacobi ilu bcgs ilu we also study the parallel performance and strong scalability of the efficient solvers in summary the main contributions of our paper are to emphasize that successful simulation of multiphase flow for large scale practical repository applications requires combinations of special linear and nonlinear solution approaches beyond the standard algorithms that have been successfully applied in the past section 2 presents the governing equations nonlinear capillary pressure and relative permeability relationships and newton raphson linearized matrix section 3 summarizes the derivation and implementation of cpr amg two stage preconditioners with two different decouplers alternate block factorization abf and quasi implicit pressure explicit saturation impes the section also includes the method and algorithm for ntrdc all the significant improvements made with the efficient solvers are demonstrated and analyzed in section 4 and the paper concludes with section 5 2 isothermal immiscible two phase porous media flow model 2 1 governing equations the wipp model simulates two phases namely brine liquid and hydrogen gas phases and it assumes isothermal conditions the model assumes further that brine cannot evaporate into hydrogen gas and that hydrogen gas does not dissolve into the brine therefore the resulting immiscible model assumes that both phases exist in all simulation grid cells these assumptions are a part of the wipp pa conceptual model caporuscio et al 2003 it uses the generalized flux form of darcy s law lichtner et al 2015 in eq 1 and 2 s is saturation ϕ is porosity ρ kg m 3 is density k m 2 is intrinsic permeability μ pa s is viscosity k r is relative permeability and q kg m 3 s 1 is the source sink term each phase is denoted by subscripts g for hydrogen and w for brine the governing equations are 1 t ϕ ρ w s w ρ w k k r w μ w p w ρ w g z ˆ q w 2 t ϕ ρ g s g ρ g k k r g μ g p g ρ g g z ˆ q g to couple these two mass balance equations there are saturation and capillary pressure constraints to complete the system of equations 3 s g s w 1 4 p c p g p w 2 2 relative permeability and capillary pressure for two phase flow in two phase immiscible flow modeling additional constitutive relations expressing capillary pressure and relative permeability as a function of saturation are required in the wipp model we use 12 different models for relative permeability and capillary pressure functions frederick et al 2020 the list includes versions of brooks corey linear power and a wipp specific model that represents excavated empty and filled waste area these functions were assigned to different materials in the simulation domain which leads to more nonlinearity in the simulations 2 3 linearized system of equations for each newton iteration the jacobian matrix from the newton raphson method is composed of the derivatives of the discrete form of the governing equations eqs 1 and 2 with respect to the primary dependent variables each grid cell has two unknowns p w and s g and generates a 2 2 jacobian block j as 5 f w p w f w s g f g p w f g s g n δ p w δ s g f w f g n where f w and f g are residuals of the two unknowns as one can imagine increasing the number of grid cells will quickly increase the size of the system of equations according to the code profiler valgrind nethercote and seward 2003 approximately 31 of computation time is spent on solving the linear system 18 is spent calculating the residuals and 51 is spent on computing jacobian for simulating the test cases introduced in section 4 for bcgs this research will focus on the 31 of computation time related to the performance of the linear system solver to improve the overall computation time it will also focus on an improved nonlinear solver for more accurate solution updates to reduce the overall number of nonlinear iterations which in turn will reduce the number of linear solver iterations jacobian and residual calculations the linear solution is declared convergent when the 2 norm of the residual is less than the default value of 10 5 times the 2 norm of the initial residual this is the default relative 2 norm convergence criterion in portable extensible toolkit for scientific computations petsc 2 4 newton raphson iteration for nonlinear system of equations the fully coupled system of equations eqs 1 and 2 in our simulator is discretized by backward euler for the time derivative and standard second order finite volume method for spatial derivatives the unknown variables are gas saturation s g and water pressure p w day et al 2012 in a grid cell we apply newton s method to solve the resulting system of coupled nonlinear algebraic equations where a system of k nonlinear equations by finding zeroes in a continuously differentiable function f r k r k described as 6 j f x n δ x f x n and x n 1 x n δ x with unknowns or solution update δ x iteration number n the residuals f and jacobian matrix j f either direct or iterative krylov linear solvers saad 2003 like bicgstab or generalized minimal residual method gmres with preconditioning are used to solve for δ x in eq 6 typically newton iteration is stopped and convergence is declared when the 2 norm of the residual drops below absolute or relative tolerances or the infinity norm of the residual drops below absolute tolerance or the infinity norm solution update drops below relative tolerances 7 f x n 2 tol absolute f x n 2 f x 0 2 tol relative or f x n inf tol absolute x n x n 1 inf tol update 2 5 time stepping algorithms the time step size in our simulator is dynamically adjusted to the criteria described in this section after the solution has converged for a completed time step the next time step size δ t is determined by several factors if the solution has not converged then our simulator repeats the current time step with half of the time step size δ t 2 i e time step cut time step size can be governed by the number of newton iterations required to converge a maximum pressure or saturation change or a maximum time step size which can change over the course of the simulation all of which can be user defined for the examples below we employ the default settings for cases where the solution converges if the nonlinear iteration number 5 the next time step size is twice the size of the completed one because the current time step facilitated convergence within a small number of iterations if the nonlinear iteration number 10 the next time step size is the same as the completed time step if the nonlinear iteration number is 6 9 then the time step size is increased by a factor of 1 8 1 6 1 4 and 1 2 respectively the next time step size is further adjusted with pressure and saturation governors for example to preserve the accuracy of the simulation the default maximum pressure change for a time step for a single cell throughout the domain is 500 000 pa if the current time step size had a change in pressure larger than 500 000 pa then the next time step will be adjusted to be smaller than what was determined in the first step by the number of newton iterations the adjustment could make the next time step size smaller than the completed time step there are three cases when the current time step is repeated with the time step size cut by half if the current time step did not reach user defined convergence tolerances after iterating for the maximum number of nonlinear iterations if the resulting solution update is outside of physical boundaries e g negative gas pressure negative saturation or saturation greater than 1 0 if the linear solver does not converge properly due to krylov subspace breakdown saad 2003 non convergence oscillations or divergence the time step cut relaxes the ill conditioned matrix and reduces nonlinearity for convergence in some challenging cases the time step can be cut repeatedly however if a user specified number of consecutive time step cuts occur 40 for these simulations the simulation stops and reports failure the effect of time step cuts in computation time is discussed in the numerical experiments in sections 4 2 1 and 4 2 2 3 the need for efficient solvers major contrasts in permeability and high grid cell aspect ratios generate ill conditioned matrices which are systems of equations that are difficult to solve with conventional iterative krylov solvers such as gmres and bicgstab with ilu preconditioning the performance of these iterative solvers depends heavily on the effectiveness of the preconditioner ineffective preconditioning can result in krylov solver failure due to breakdown or from reaching the maximum number of iterations which ultimately produces small time steps and excessive run times the nonlinearity of the governing equations also may affect the conditioning of the jacobian matrix and may lead to convergence problems when using standard newton s method we address each of these computational challenges section 3 1 presents the constrained pressure residual cpr preconditioner section 3 3 presents the derivation of the newton trust region dogleg cauchy method and shows how it is applied in solving the nonlinear governing equations 3 1 constrained pressure residual preconditioner for linear solver one possible approach to address krylov solver failure is to implement cpr 2 stage type preconditioner wallis et al 1985 in the first stage the matrix corresponding to the pressure equation is extracted and solved while in the second stage the resulting residuals are corrected using coupling terms in the global system two different methods were applied to decouple the pressure matrix from the global matrix quasi implicit pressure explicit saturation impes lacroix et al 2001 and alternate block factorization abf bank et al 1989 ponting et al 2008 bui et al 2017 stüben et al 2007 and cusini et al 2015 apply the two stage combinative preconditioner cpr amg a well known preconditioner within the two phase porous media modeling community that is assisted with the algebraic multigrid amg the algorithms and methods from these papers were used as references for the developments in this study fgmres saad 1993 is used for the numerical experiments as it allows larger flexibility in the choice of solution subspace than gmres and bicgstab and it is designed to accommodate changing preconditioners for each linear solver iteration which means it can use any iterative method or multi level method as its preconditioner in this study we use multi level amg solver lacroix et al 2003 in the cpr preconditioner the algorithm to solve the system of linear equations a u b in these experiments is listed in section 3 1 2 3 1 1 qimpes and abf decoupler to explain the algorithm further the jacobian in eq 5 can be written as 8 a a p p a p s a s p a s s the restriction matrices r p and r s are defined as follows 9 r p u δ p r p t δ p δ p 0 r s u δ s r s t δ s 0 δ s where u δ p δ s where to simplify the notation δ p is the same as δ p w and δ s is δ s g in eq 5 two different decoupling methods are proposed the quasi impes formulation and abf method a decoupling matrix m is applied as 10 m a u m r the quasi impes decoupling matrix lacroix et al 2001 is defined as 11 m q i i d p s d s s 1 0 i and the abf decoupling matrix bank et al 1989 is defined as 12 m a b f λ 1 0 0 λ 1 d s s d p s d s p d p p where d is a block diagonal matrix of a e g d p p diag a p p and λ d p p d s s d p s d s p this decoupler implementation also applies modified column scaling on eq 12 to enhance the effectiveness of amg for smoothing the pressure residuals d ponting personal communication appendix c shows the scaling equation for these parallel numerical experiments boomeramg as part of the hypre falgout et al 2006 package was used as the amg solver for the cpr preconditioner while fgmres from petsc was used for the tailored linear solver runs for amg solver we used the default settings from hypre hmis for coarsening de sterck et al 2006 extended i interpolation for prolongation de sterck et al 2008 and v 1 1 cycle the results are compared to the block jacobi parallel preconditioner ilu 0 as a preconditioner for each block and bicgstab as the linear solver bcgs from petsc 3 1 2 cpr algorithm cpr preconditioner solves the decoupled pressure system with an amg solver that is extremely effective stüben et al 2007 the second stage uses the ilu preconditioner applied to the global matrix with a pressure corrected matrix cpr amg is used to solve the linear system for every newton iteration the algorithm as implemented for the numerical experiments is listed below 3 2 nonlinear optimization problem as noted earlier the extreme nonlinear nature of two phase flow in an engineered repository can present challenges for convergence these challenges can be addressed by revisiting the jacobian system used for newton raphson in eq 6 the residual function f represents the discrete approximation of the two governing equations eq 1 and 2 the goal is to solve the nonlinear system f x 0 where the linear system solution δ x is obtained by using a nonlinear solver for x x 0 δ x such that f x 0 which would satisfy the nonlinear convergence criteria the approach begins with an initial guess x 0 to approximate the function f by solving a linearized system of equations then using the zero of this linear model as the second guess the next iteration n 1 is computed as discussed in section 2 4 the process is repeated until the satisfactory convergence criteria are met eq 7 the optimization problems and convergence criteria eq 7 commonly use the 2 norm as the measurement which is defined for f x in discrete form as 13 f x 2 i 1 n f i x 2 1 2 using a practical iterative algorithm the aim is to make the residual as small as possible within a reasonable number of iterations therefore we can restate the problem as the following unconstrained minimization quadratic model 14 f x 1 2 f x 2 2 r n r finding the minimum of the function achieves the global unconstrained optimization mathworks 2020 dennis jr and schnabel 1996 15 min x r f x 1 2 f x t f x the goal of newton trust region dogleg cauchy is to solve the optimization function eq 15 which will minimize the residual f x section 3 3 explains how it works to minimize the objective function 3 3 newton trust region dogleg cauchy nonlinear solver the nonlinear solver newton trust region dogleg cauchy ntrdc is a general purpose optimization solver and it is not specific to multiphase flow models for complicated problems like this case of a large scale subsurface system newton raphson may not be robust enough to update solutions properly without a sufficiently close initial guess to the solution i e a small time step heath 2018 while the ntrdc method is more complex and expensive we hypothesize that its enhanced robustness can solve extreme nonlinear problems and achieve convergence using larger time step sizes in the simulations thereby reducing the overall computation time to explain the ntrdc methods we define the following a the newton raphson newton linearized system yields a solution update p b the current solution at nonlinear iteration n is x n c f x n is the objective function in eq 15 at iteration n after solving the newton linear system ntrdc introduces a global model function to optimize with the solution update in order to determine the proper size of the trust region tr which controls the maximum step size allowed for the newton solution update detailed derivations and proofs are found in the text by dennis jr and schnabel 1996 the newton solution update is written as 16 m n x n p f x n j x n p where f is the residual and j is jacobian in eq 5 replacing m n with f in eq 15 the model function or the tr sub problem is found as 17 m n p f n g n t p 1 2 p t b n p s t p δ n where n is the iteration number p is the newton solution update or search direction and step δ n is the trust region radius f 1 2 f t f the objective function in eq 15 g n j t f and b n j t j which is an approximation to the hessian matrix the ntrdc method defines ρ as the ratio of actual improvement numerator the objective function and the predicted improvement denominator the trust region model the ratio is used to determine the trust region size for the next iteration and whether p is the improvement direction and step for x n 1 18 ρ n f x n f x n p n m n 0 m n p n if ρ η 3 the trust region expands by a factor t 2 and if ρ η 2 the trust region shrinks by factor t 1 if ρ is in between η 2 and η 3 the trust region size is unchanged for the next iteration δ n is the trust region at iteration n typical values for these input parameters are 0 η 1 η 2 η 2 0 25 η 3 0 75 shrinking factor t 1 0 25 and expanding factor t 2 2 0 ye 2014 hauser 2005b if ρ η 1 then the tr sub problem has found the adequate model function within the trust region and the solution update is checked for convergence if the newton solution update is outside the trust region the cauchy point cp method attempts to improve the solution update in the ntrdc solver hauser 2005a cp takes the steepest descent direction hauser 2005b the cp solution update is written as 19 s n f n 2 f n t b n f n f n if neither cp nor newton solution update fits within the trust region then both solutions are combined to form a quadratic equation to solve for the ratio τ 20 δ s n c y τ s n n t s n c y 2 here c y and n t are the cauchy and newton solution update respectively the ratio τ and 1 τ multiply the newton and cp solution respectively to form the final solution update again if this solution satisfies ρ η 1 then the convergence of the time step is checked the combination of cauchy and newton solution update can still break down the inner iterations of trust region method when the trust region shrinks below 10 12 of the solution norm x at this point our simulator will cut the time step to relax the problem the procedure of the algorithm is further described using an example test optimization problem in appendix d 3 3 1 scaling solution vector for two phase flow proper scaling of solution update vector and jacobian is helpful for any advanced nonlinear solvers that measure l2 norm to determine whether the newton solution update direction and step size are suitable for optimization for ntrdc the entire inner iteration must be properly scaled in order to truly take advantage of the robustness of the algorithm in our simulator the pressure unknown variable is in the units of pa and the saturation unknown variable is dimensionless between 0 and 1 this means the solution update vector has two discrete range of values for example a new pressure update can arbitrarily range between 0 and 1000 pa while a new saturation update can range between 0 and 0 1 in the simulation domain for this hypothetical nonlinear iteration the l2 norm is defined as 21 x 2 x 1 p 2 x 1 s 2 x 2 p 2 x 2 s 2 x n p 2 x n s 2 in the given example it is observed that pressure values in the solution update vector can completely diminish saturation values measuring an improper l2 norm for the nonlinear solvers therefore the two quantities must be scaled properly by measuring the infinity norm of each quantity eq 22 and then normalizing them such that both quantities in the solution range between 0 and 1 22 p x p max x i p and s x s max x i s all of the pressure values in the solution update are scaled by p the saturation values are scaled by s and the jacobian is column scaled by the stride vector p s p s the effects of scaling is discussed in the 4 2 3 3 3 2 ntrdc algorithm as noted above the ntrdc algorithm is more complex than a simple newton raphson iteration but we expect it to be more robust which overall enhances the efficiency of the solver the additional computational expense of the algorithm is mainly due to residual evaluations in the inner iterations that are needed if the solution is not within the trust region if the trust region criteria are satisfied after calculating the linear system the algorithm is almost identical to newton raphson except for the evaluation of ρ the algorithm is listed below 4 numerical experiments we used the wipp 3d simulation domain for the numerical experiments with three different test cases easy mid and hard case these represent three hypothetical future scenarios where additional information is included in section 4 1 the computational enhancement from the fgmres cpr qimpes fcq and fgmres cpr abf fca preconditioners is compared to block jacobi ilu preconditioner in section 4 2 1 then the effectiveness of newton trust region dogleg cauchy ntrdc compared to newton using bicgstab block jacobi ilu 0 bcgs is demonstrated in section 4 2 2 the combination of tailored linear and new nonlinear solvers that mostly improve the computation time are discussed in section 4 2 4 finally the strong scalability of the combinations of the new linear and nonlinear solver results are shown in section 4 3 3 appendix e displays all tabulated data for the figures in the numerical experiments section 4 1 description of numerical simulation case study there are several key engineered features 650 meters below the surface in the 3d wipp simulation domain which are shown in fig 1 these include excavated shafts and hallways to deliver and deposit nuclear wastes and the borehole during hypothetical unexpected intrusion scenarios three test case scenarios occur on the same domain with different numerical difficulty the easy case simulates an undisturbed wipp nuclear waste repository without disruption from closure through the 10 000 year simulation period there are material changes throughout the domain as the run of mine salt closures reconsolidate and the disturbed salt rock above and below the excavated regions self heals returning the porosity and permeability to near the intact salt values the mid case expands the easy case to include a hypothetical human intrusion event where the borehole is drilled into the repository at 1000 years after the simulation begins fig 1 illustrates a borehole as a skinny brown line in the repository after the intrusion in this scenario the time step size changes from months or years to minutes to simulate the transient flow process through an intruding borehole that has very high permeability compared to the salt rock formation such scenarios cause a dramatic perturbation in time spatial and permeability scales the hard case includes two separate events one borehole intrusion into the repository at 350 years and a second borehole intrusion through the repository into an over pressured brine reservoir at 1000 years this event can potentially flood the dry waste repository the simulation domain spans approximately 30 km in the x 28 km in the y and 1 km in the z direction and the domain is expanded in x and y direction from the nuclear waste repository figs 2 and 3 the domain is also non uniformly discretized in 33 layers in the z direction to represent 11 formations table pa 3 of u s department of energy carlsbad field office 2014 the wipp 2 phase flow model consists of a multi scale problem that spans over 10 orders of magnitude in permeability 5 orders of magnitude in the spatial dimensions and requires simulation for 10 000 years the domain has a total of 460 020 grid cells or 920 040 unknowns and consists of 41 different rock soil types with 12 different capillary pressure and relative permeability relations national research council 1996 frederick et al 2020 the simulation also includes gas source terms generated from the degradation of plastics and rubbers corrosion of steel drums and radiolysis of water more nonlinearities are introduced by the interbed fracture model and salt creep closure model day et al 2012 in this section all simulation results at 10 000 years for 20 selected grid cells in important regions are verified and confirmed against each other within 0 5 of relative error convergence tolerances and all the other parameters in the simulation inputs are the same in all simulations we next report results comparing block jacobi ilu 0 and newton raphson against the tailored preconditioner and the new nonlinear solver presented above in section 3 4 2 improvements of the efficient solvers compared to the bcgs solvers this section demonstrates how the different combinations of the efficient solvers outperformed the bcgs solver in the number of linear and nonlinear iterations and the overall wall clock computation time section 4 2 4 shows the performance of the best combination of the efficient solvers versus the bcgs solver 4 2 1 bcgs vs tailored linear solvers and preconditioners we applied two different methods to decouple the pressure matrix from the global matrix quasi impes and abf the nonlinear solver used in this section is newton raphson newton the linear solvers used are bcgs with block jacobi preconditioning and ilu 0 in each block bcgs and fgmres with cpr quasi impes fcq and cpr abf preconditioner fca for the easy case fig 4a shows the wall clock computation time using 2 compute nodes 32 cores on sandia national labs snl s skybridge high performance computing hpc cluster that hosts intel xeon processor e5 2670 2 60 ghz 20m cache sandia national laboratories 2021 the bcgs takes 42 h to complete the 10 000 year simulation of the undisturbed scenario the easy case while the tailored fgmres cpr qimpes fcq takes 12 h 3 5x speedup and fgmres cpr abf fca takes 1 2 h 35x speedup fig 4a demonstrates that bcgs had 28 million linear iterations and 76 thousand nonlinear iterations while fca had 125 times fewer linear iterations and five times fewer nonlinear iterations although each iteration for fca is more computationally expensive it is more robust and effective and results in fewer iterations the main difference in computation time between bcgs and fca fcq solvers emerges from the large number of linear solver failures for bcgs as shown in fig 4b when the time step cut is triggered the current time step is halved and retried for a convergence the linear solver failure occurs when it diverges or oscillates or when the krylov space breaks down the time step is intentionally cut when the linear solver fails when the nonlinear solution update is non physical or when the maximum nonlinear iteration count is reached the figure illustrates the disproportionate number of bcgs solver failures fca eliminated linear solver failures in the simulation which is the dominant cause of bcgs solver time step cuts and leads to smaller δ t per time step and longer overall computation time stüben et al 2007 discusses that abf may result in strongly nonsymmetric matrix and also it may lead to oversolving the jacobian system however the modified column scaling corrects this issue in this paper and works in favor of amg by preserving the shape of the original a p p and a more aggressive row scaling is performed than qimpes fcq performance was consistently worse than fca throughout these experiments qimpes is a more conservative decoupling strategy that preserves most of the original structure of the coupled system which evidently did not benefit the numerical experiments the strength of decoupling of the matrix is explained in more detail in stuben s paper all the results in the next sections illustrate the effectiveness of the fca linear preconditioner combination and exclude fcq results because fcq performed consistently worse than fca for 15 unique samples from the 1800 realizations table 1 shows the performance of bcgs and fca for the mid case it required twice as many cores to complete the simulation in a similar computation time bcgs took 35 7 h and fca solver and preconditioner combination took 1 2 h 30x speedup table 2 presents the results for the hard case where only fca completed the simulation on 32 cores fca took 41 h to complete compared to 1 2 h for the easy case demonstrating increased difficulty of the hard case bcgs could not solve this case in 4 days with 128 cores and there is no valid data point to present in the table section 4 2 3 demonstrates how a more advanced nonlinear solver and solution scaling can further reduce the hard case run time 4 2 2 newton vs ntrdc nonlinear solver section 4 2 1 demonstrated how the tailored preconditioner linear solver combinations could improve the overall computation performance while using the newton solver here we compare newton and ntrdc solver performance using bcgs ntrdc uses auto scaling for inner iterations in the comparison because it is the more robust way to utilize the ntrdc algorithm the original linear system is not scaled it is the same for newton and ntrdc auto scaling is described in section 3 3 1 and its effectiveness is demonstrated in section 4 2 3 fig 5a demonstrates the superior performance of ntrdc relative to newton for the mid case where ntrdc is 7x faster even with a 40 growth in nonlinear iterations newton which applies a full step update in the search direction struggled to converge often generating an ill conditioned system of equations for bcgs that resulted in failure see fig 5b ntrdc s truncated step update in a more optimal search direction eliminated such failures in bcgs and greatly improved performance most of the linear solver failures were due to bcgs reaching the maximum number of iterations i e the default 10 000 such failures are computationally expensive as for each bcgs failure that resulted in a time step cut there were one to three nonlinear iterations and 1000 to 9000 linear iterations wasted in addition the time step was cut in half slowing simulation progress although ntrdc required more nonlinear iterations ntrdc averaged 36 linear iterations per nonlinear iteration compared to 712 for newton also the linear systems produced by ntrdc were more robust and solvable reducing the number of wasted linear iterations due to time step cut from 21 756 005 newton to 941 029 ntrdc fig 5b illustrates 4x more time step cuts for ntrdc relative to newton due to the nonlinear solver reaching the maximum nonlinear iteration count however since each ntrdc iteration required far fewer linear iterations 36 versus 712 for newton each ntrdc iteration was much faster greatly reducing the overall runtime as mentioned in section 2 3 the linear solver takes 31 the residual calculation takes 18 and the jacobian takes 51 of total the computation time for the newton solver ntrdc has the computational effort distribution of 23 21 and 56 for the linear solver residual calculation and jacobian calculation respectively the changes in effort ratio for ntrdc corresponds to the number of iterations where the number of linear iterations for the linear solver has been reduced and the inner iteration residual calculation for nonlinear solve has increased also the jacobian has an increased computational effort ratio because the linear solver computational effort has been reduced significantly 4 2 3 effects of scaling solution vector this analysis highlights the importance of scaling the solution update vector for pressure to be in the range of saturation as discussed previously in section 3 3 1 and step 5 3 of the ntrdc algorithm we hypothesized that if the solution update vector is not properly scaled and normalized between two unknown variables pressure and saturation then the pressure dominates the optimization criterion which might result in a low accuracy solution for saturation thus if the two unknown quantities are scaled properly it should be possible to minimize both the saturation and pressure residuals using the ntrdc algorithm with an enhancement in computational performance fig 6 demonstrates that auto scaling capability can dramatically improve the efficiency of the ntrdc solver both cases use fca preconditioner linear solver combinations fig 6a shows the performance of newton ntrdc noscale and ntrdc for the mid case using 32 cores on skybridge it is shown that compared to newton ntrdc noscale reduced the overall computation time by 11 but ntrdc on the other hand achieved a 48 reduction in computation time the easy case which is not shown does not have a major reduction in computation time using ntrdc noscale because the simulation does not include as many sudden changes in the dynamic nonlinear processes and fca is powerful enough to resolve the resulting ill conditioned matrix the impact of nonlinear solver scaling is maximized for the hard case which has the most nonlinearity in the simulation fig 6b shows that ntrdc has about 3 times the overall computation time reduction compared to ntrdc noscale and newton furthermore ntrdc without scaling optimizes for pressure more than saturation section 3 3 1 therefore it degrades in performance compared to newton in this case section 4 2 4 will present the best improvements that can be obtained with combinations of ntrdc and fca 4 2 4 best improvements with tailored linear and new nonlinear solvers fig 7 shows the two best cases with the best improvements in computation time compared with bcgs unfortunately it was not possible to assess the hard case because the simulation did not complete after two days with 64 cores or 128 cores for bcgs the combinations of cpr abf preconditioner for gmres and ntrdc nonlinear solver resulted in the best computation time with 32 cores for the easy case 42 h for bcgs and newton reduced to 0 9 h for the efficient solvers which is 47 times faster interestingly gmres performed slightly better than fgmres approximately 5 the number of linear iterations decreased from 28 443 067 to 107 950 approximately 263 times and the number of nonlinear iterations decreased from 76 874 to 9 787 recall that each iteration of bcgs linear solve and nonlinear iterations are computationally much cheaper than the efficient solvers but the efficient solvers robustness and accuracy on each iteration enable them to outperform overall the mid case required more computational resources for bcgs so the comparison was made using 64 cores in this case the efficient solvers of fgmres and ntrdc outperformed the traditional solvers by 40 times with computation time reduction from 35 7 h to 0 9 h the number of linear iterations dropped from 51 864 686 to 348 019 approximately 149 times and the number of nonlinear iterations dropped from 72 837 to 12 647 4 3 parallel scalability on the efficient solvers this section illustrates the parallel scalability of the efficient solvers fgrmes cpr abf preconditioner linear solver combination and ntrdc we also explain some defects found during the numerical experiments due to parallel domain decomposition of the unstructured grid and node packing effect in the modern processor architectures 4 3 1 parallel domain decomposition impacts on computational performance bcgs was tested on many different easy case realizations the results were similar in that it was too slow and would often reach the 48 hour desired time limit for pa with 64 cores which is about 15 000 unknowns per core using decomposition of the domain another downside of the bcgs solver was the inconsistent parallel computation time this behavior would hinder the users ability to reliably estimate wall time and the required number of cores when running simulations on shared clusters the inconsistency is caused by how the domain is decomposed and the block jacobi preconditioner drops coefficients that are off process instead of factoring them this elimination of factored coefficients degrades preconditioner performance in other words the coefficients that are at the boundaries of the decomposed domain do not have information from the neighboring decomposed domains when solving the linear system most of the time the loss of this information has minimal impacts upon convergence but not for simulation domains that are as complex as the wipp domain for example if the domain decomposition occurs between the boundary of an excavated zone very high permeability and intact salt formation very low permeability it is very important to know the solution update from the neighboring but separately calculated grid cells in each linear iteration the bcgs inconsistencies are clearly illustrated in fig 8 compared to fca fca shows consistent improvements in computation time over 32 64 and 128 cores or about 30 000 15 000 and 7 500 unknowns per core respectively however bcgs using 64 cores outperforms 128 cores and using 64 cores is 4 times faster than 32 cores it should ideally improve the performance only by 2 times the number of nonlinear iterations required to complete the 10 000 year simulation provide a more in depth explanation bcgs has a large fluctuating number of nonlinear iterations but fca shows the uniformly consistent number of nonlinear iterations ideally the number of nonlinear iterations should not change for a given simulation regardless of how the linear system was solved i e the number of cores domain decomposition and types of solvers would not matter bcgs showed erratic behavior while fca showed nearly consistent newton iteration numbers 15468 15443 and 15609 for 32 64 and 128 cores respectively 4 3 2 scalability performance on modern processor architectures before discussing the scalability analysis of fca with newton and ntrdc the physical architecture of the skybridge cluster must be understood properly skybridge cluster nodes have intel xeon processor e5 2670 2 60 ghz 20mb cache the processor has two sockets four cores per socket and two threads per core that sum to 16 cpus connected with infiniband sandia national laboratories 2021 the cluster has the multi threading option turned off so in effect petsc can utilize all 16 cpus in a node however when all 16 cpus are used there is a bottleneck effect in accessing cache and memory for each of the cpus the maximum number of memory channels in the node is 4 for skybridge some parallel tools take advantage of multi threading for better performance but our simulator uses petsc mode that does not take advantage of multi threading fig 9 illustrates the efficiency of the processors when they are relieved from the bottleneck all of the cases use 16 cpus in total and runs the mid case using fca newton solver but each case uses a different number of nodes and a different number of cores per node when using one node and all 16 cores this simulation takes close to 15 000 s but when two cores per node and eight nodes are used the simulation takes about 10 000 s that is close to a 30 difference in computation time even though the same number of cpus are used of course there can be more communication time loss across nodes when using message passing interface mpi as seen between 8 n 2 cpn case and 16 n 1 cpn case but that seems to be a small difference compared to the bottleneck access to cache and memory within the node the results are specific to the skybridge cluster but multi threaded nodes are a common parallel computer architecture one must take into account the physical architecture of the machine before studying the scalability of a parallel code 4 3 3 scalability of combined tailored linear and new nonlinear solvers the mid case was run on different numbers of cores and cores per node to demonstrate the strong scalability of the newly implemented algorithms fca preconditioner linear solver combination with newton and ntrdc the strong scaling of fca newton and fca ntrdc follows the trend of ideal strong scaling in fig 10 also it should be noted that bcgs newton is shown with a single available data point in the figure with 64 cores the scalability trend has a kink after 8 cores because of the node packing effect mentioned in section 4 3 2 the trend of fully packed nodes 16 32 and 64 cores is still close and parallel to the ideal trend line there are four data points with the hollow circle and hollow triangle that represents reduced node packing effect with 4 cores per node utilization and you can see that it is much closer to the ideal for strong scalability running this domain on 128 cores would not be efficient because an insufficient number of unknowns would be distributed to the processors to maximize efficiency i e there would be too small of a problem to solve per processor fig 11 is the more detailed plot of strong scaling in terms of efficiency where the ideal case reduces the computation time exactly by the number of cores ntrdc had higher efficiency than newton for 8 cores but slightly worse efficiency for 16 and 32 as ntrdc may require more parallel communications than newton all of the combinations of non packed node cases performed better than packed nodes and stayed above 50 efficiency for both ntrdc and newton the same kink from 8 cores to 16 cores is seen here in dramatic fashion by reducing the packing by a half newton data points at eight cores per node hover between 50 to 75 efficiency 5 conclusion numerical simulation of multiphase flow continues to be one of the most computationally challenging problems in subsurface modeling due to the highly nonlinear governing equations and the need for fine spatial discretization simulated in a parallel high performance computing hpc environment to overcome these challenges we present and evaluate two new preconditioners a constrained pressure residual cpr ponting et al 2008 with alternate block factorization abf bank et al 1989 and quasi implicit pressure explicit saturation qimpes decouplers cusini et al 2015 and b a new nonlinear solver newton trust region dogleg cauchy ntrdc dennis jr and schnabel 1996 we demonstrate the performance of these methods for a large number of numerical experiments of isothermal immiscible flow of the waste isolation pilot plant wipp models national research council 1996 wipp is a real world example of a large scale engineered subsurface system that adds additional practical complexities such as 41 different soil types with 12 different relative permeability and capillary pressure functions the simulations also include nonlinear processes such as gas generation from the degradation of bio materials steel drum corrosion and radiolysis of water fracture models for clay and anhydrite interbeds in the bedded salt formation a creep closure model for excavated rooms in salt formations and human borehole intrusion events simulated by material property changes in a grid cell over time these features create an ill conditioned jacobian and extremely nonlinear system of equations three test scenarios occur in the same domain the easy case simulates an undisturbed wipp nuclear repository without disruption from closure through the 10 000 year simulation period the mid case simulates a hypothetical human intrusion event where a borehole is drilled into the repository in search of natural resources 1000 years after the closure the hard case includes two separate events where one borehole intrusion occurs into the repository at 350 years and a second borehole intrusion through the repository into an over pressured brine reservoir both mid and hard cases continue the simulation for 10 000 years after the closure our results show that the bcgs linear solver saad 2003 and preconditioner with newton raphson nr nonlinear solver failed to complete simulations due to repeated krylov solver breakdowns and oscillatory non convergence for most of the 1800 realizations required to complete for uncertainty quantification of the wipp performance assessment pa the pa is desired to be done in 3 weeks with the high performance computing cluster at wipp we hypothesized that a more efficient preconditioner can reduce the number of solver breakdowns and a new nonlinear solver can resolve the non convergence oscillatory behavior of the newton solver these efficient solvers must be parallelized and scalable in order to improve computational efficiency in large scale simulations like the wipp the tailored linear and the new nonlinear solvers are proven to be much more efficient than bcgs in this paper the paper demonstrates the combinations of flexible generalized minimal residual fgmres linear solver saad 1993 cpr abf preconditioner and ntrdc nonlinear solver can give up to 47 times speed up compared to bcgs on the first trial of running 1800 realizations with fgmres cpr abf preconditioner with the default newton solver approximately 97 of the simulations were completed the other 3 of the simulations are expected to complete with the ntrdc solver once the solver becomes official through a quality assurance process these were never attempted with bcgs solver for the wipp 3d models the bcgs solver is inefficient because the block jacobi preconditioner drops coefficients that are off process instead of factoring them when the domain is decomposed for running in parallel the coefficients that are at the boundaries of the decomposed domain do not have the information from the neighboring domain when solving the linear system which leads to inaccuracy especially in discrete and heterogeneous domains the algebraic multigrid amg method stüben et al 2007 used in cpr preconditioner does not have such a loss of information using the same nr nonlinear solver cpr amg preconditioned linear solver was up to 35 times faster than bcgs the extreme nonlinearity of the real world models leads to nr non convergence oscillations which prohibit the solver to find the global minimum of the system of nonlinear equations the new nonlinear solver ntrdc was able to provide a more robust solution to the nonlinear optimization problem using bcgs linear solver ntrdc was 7 times faster than nr nonlinear solver while ntrdc resolved the nonlinearity and provided relaxed matrices nr caused many linear solver failures because it generated ill conditioned jacobian for the linear solver the strong scaling efficiency for the combined methods fgmres cpr abf and ntrdc was between 50 to 75 efficient which was similar to newton and better than bcgs future work will apply these methods for non isothermal miscible multiphase flow in porous media that incorporate primary variable switching and multiple phase states due to miscibility examples would be to simulate an underground repository for high level radioactive waste or a geothermal power plant site we expect these robust methods to be powerful for nonisothermal miscible applications with high degrees of nonlinearity credit authorship contribution statement heeho d park conceptualization methodology software validation investigation writing original draft formal analysis visualization glenn e hammond conceptualization software supervision writing review editing albert j valocchi supervision writing review editing conceptualization tara laforce resources visualization declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the authors would like to thank paolo orsini sebastien loisel and daniel stone from pflotran ogs development team for setting up the framework of cpr preconditioner in pflotran they would like to thank michael nole alex salazar emily stein chris camphouse paul shoemaker and tito bonano in the wipp pa team and geologic disposal safety assessment gdsa team at sandia national labs for the support of this work and the doctoral study program in university programs at sandia this research was funded by the us department of energy office of environmental management doe em under the waste isolation pilot plant program and the us department of energy office of nuclear energy doe ne under the spent fuel and waste science and technology sfwst campaign this paper describes objective technical results and analysis any subjective views or opinions that might be expressed in the paper do not necessarily represent the views of the u s department of energy or the u s government sandia national laboratories is a multimission laboratory managed and operated by national technology engineering solutions of sandia llc a wholly owned subsidiary of honeywell international inc for the u s department of energy s national nuclear security administration under contract de na0003525 this research is funded by wipp programs administered by the office of environmental management em of the u s department of energy sand2021 11975j appendix a background on wipp the waste isolation pilot plant is constructed in southeastern new mexico and operated by the u s department of energy doe office of environmental management for the deep geologic disposal of defense generated transuranic waste from doe sites around the country national research council 1996 wipp is located 650 meters below the surface in a bedded salt rock formation called the salado formation which mostly consists of layers of halite and thin layers of mixed clay and anhydrite typically clay and anhydrite are both considered low permeability 10 17 to 10 19 m 2 but relative to the salado formation and on the time scale of 10 000 years they are considered higher permeability compared to that of halite 10 21 to 10 24 m 2 the pa simulations include the culebra dolomite layer where accessible groundwater flows having a permeability of 10 14 m 2 the pa calculations helton et al 1997 estimate the probability and consequence of potential radionuclide releases from the land withdrawal boundary defined by the land withdrawal act of 1992 law 1992 which is a 4 4 mile area that contains the repository therefore the 3d simulation domain scale spans from decimeters a borehole to a few kilometers a schematic of wipp is in fig a 12 there can be extreme contrasts in porosity permeability and volume over short distances a borehole grid cell is defined with permeability on the order of 10 11 m 2 whereas permeability of the adjacent salado halite grid cell may be 10 24 m 2 in summary the wipp 2 phase flow model consists of a multi scale problem that spans over 10 orders of magnitude in permeability 5 orders of magnitude in the spatial dimensions and requires simulation for 10 000 years table pa 3 of u s department of energy carlsbad field office 2014 also some scenarios require a borehole intrusion simulation by human activity possibly in search of natural resources near or through the repository human intrusion will introduce instantaneous changes in material properties for some grid cells during the simulation which might require a dramatic reduction of time step size down to minutes for numerical stability appendix b background on pflotran pflotran is an open source and community driven code developed by doe that solves a nonlinear system of partial differential equations pde s governing multiphase flow and multicomponent reactive transport in porous materials using the finite volume method the code is designed to run in massively parallel computing architectures as well as workstations and laptops the code is developed and used by an international research community of subsurface scientists hammond et al 2012 lichtner et al 2015 it can handle many subsurface processes involving flow and transport in porous media including multiphase isothermal non isothermal miscible immiscible flow co2 two phase flow black oil model richards flow and thermal hydrologic flow and multicomponent reactive transport including aqueous complexation sorption and mineral precipitation and dissolution our simulator also has the capability to run miscible two phase co2 two phase black oil richards and thermal hydrologic models the flow and reactive transport equations are solved using the newton raphson method and fully implicit temporal discretization parallelization is achieved through domain decomposition using the petsc library balay et al 2019 our simulator scales well on distributed memory high performance computing architectures hammond et al 2014 it has been run on up to 2 18 processor cores with problem sizes up to 2 billion degrees of freedom hammond et al 2014 it uses the metis parmetis karypis 2011 libraries for unstructured grids also it can read and write hdf5 folk et al 2011 files as well as ascii files for general use appendix c alternate block factorization scaling in section 3 1 1 we introduced the abf decoupling matrix eq c 1 describes how the abf matrix is modified or scaled to improve convergence based on personal communication with david ponting c 1 m a b f c 1 0 0 c 2 λ 1 0 0 λ 1 d s s d p s d s p d p p where c 1 abs d p p abs d s p c 2 abs d p s abs d s s λ d p p d s s d p s d s p the scaling is applied to preserve the long wave shape of diffusion residuals which is very effective for solving by the amg solver we credit david ponting and daniel stone for initially implementing the method in pflotran appendix d ntrdc algorithm demonstration fig d 13 demonstrates the ntrdc methods the contour in the figure is the branin optimization test function commonly used to test optimization algorithms with unknowns x and y coordinates bingham 2013 this function is particularly difficult for newton solver because of multiple minima and saddle points in the v shaped lower channel the starting point was randomly selected in the figure a thinner white line in the figure illustrates the original newton step and direction solution update after the first iteration however it is over estimated such that ρ η 2 and the trust region shrinks in the next inner iteration the newton step size is truncated to the shrunk trust region size but this time it does not satisfy the condition s n n t δ in the algorithm listed in section 3 3 2 the second solution cauchy step and direction in eq 19 is calculated but also does not satisfy the condition s n c y δ finally the dogleg method combines the two solution step and direction vectors with calculated ratio τ from eq 20 the combined solution finds one of minima of the contour function in just 2 iterations in this example the improvement over ntrdc over newton is demonstrated in section 4 2 2 appendix e tabulated data for figures see tables e 3 e 10 balay et al 2019 balay s abhyankar s adams m brown j brune p buschelman k dalcin l dener a eijkhout v gropp w petsc users manual 2019 argonne national laboratory s balay s abhyankar m adams j brown p brune k buschelman l dalcin a dener v eijkhout w gropp et al petsc users manual bank et al 1989 bank r e chan t f coughran w smith r k the alternate block factorization procedure for systems of partial differential equations bit numer math 29 4 1989 938 954 r e bank t f chan w coughran r k smith the alternate block factorization procedure for systems of partial differential equations bit numerical mathematics 29 4 1989 938 954 bingham 2013 bingham d branin function 2013 simon fraser university url https www sfu ca ssurjano branin html d bingham branin function 2013 https www sfu ca ssurjano branin html bui et al 2017 bui q m elman h c moulton j d algebraic multigrid preconditioners for multiphase flow in porous media siam j sci comput 39 5 2017 s662 s680 q m bui h c elman j d moulton algebraic multigrid preconditioners for multiphase flow in porous media siam journal on scientific computing 39 5 2017 s662 s680 caporuscio et al 2003 caporuscio f gibbons j li c oswald e waste isolation pilot plant salado flow conceptual models final peer review report 2003 u s department of energy carlsbad area office url https www wipp energy gov library cra cra 2014 others caporuscio gibbons oswald 2003 salado flow conceptual models erms526879 pdf f caporuscio j gibbons c li e oswald waste isolation pilot plant salado flow conceptual models final peer review report 2003 https www wipp energy gov library cra cra 2014 
246,multiphase flow simulation is well known to be computationally demanding and modeling large scale engineered subsurface systems entails significant additional numerical challenges these challenges arise from a the presence of small scale discrete features like shafts tunnels waste packages and barriers b the need to accurately represent both the waste form processes at the small spatial scale of the repository and the large scale transport processes throughout heterogeneous geological formations c the strong contrast in material properties such as porosity and permeability as well as the nonlinear constitutive relations for multiphase flow numerical solution entails discretization of the coupled system of nonlinear governing equations and solving a linear system of equations at each newton raphson iteration practical problems require a very large number of unknowns that must be solved efficiently using iterative methods in parallel on high performance computers the unique challenges noted above can lead to an ill conditioned jacobian matrix and non convergence with newton s method due to discontinuous nonlinearity in constitutive models moreover practical applications can require numerous monte carlo simulations to explore uncertainly in material properties geological heterogeneity failure scenarios or other factors governmental regulatory agencies can mandate these as part of performance assessments hence there is a need for flexible robust and computationally efficient methods for multiphase flow in large scale engineered subsurface systems we apply the open source simulator pflotran to the practical problem of performance assessment of the us doe waste isolation pilot plant wipp site the simulator employs a finite volume discretization and uses the petsc parallel framework we evaluate the performance of several preconditioners for the iterative solution of the linearized jacobian system these range from stabilized biconjugate gradient with block jacobi preconditioning bcgs to methods adopted from reservoir modeling such as the constrained pressure residual cpr two stage preconditioner and flexible generalized residual solver fgmres we also implement within petsc the general purpose nonlinear solver newton trust region dogleg cauchy ntrdc which truncates the newton update or modifies the update with a cauchy solution that is within the quadratic model trust region of the objective function we demonstrate the effectiveness of each method for a series of test problems with increasing difficulty we find that the ntrdc and fgmres cpr abf fca preconditioners generally perform best for the test problem having the extreme nonlinear processes achieving a 50x speed up compared with bcgs the most ill conditioned and extreme nonlinear simulations do not converge with bcgs as one may expect but they do complete the simulation with ntrdc and fca we also investigate the strong scalability of each method and demonstrate the impact of node packing upon parallel performance on modern processor architectures keywords preconditioner nonlinear porous media multiphase flow trust region subsurface system 1 introduction simulation of multiphase porous flow is required to address a wide range of practical engineering problems such as nuclear waste disposal enhanced oil recovery contaminant remediation geothermal engineering and carbon sequestration understanding and predicting underground phenomena can help mankind address world issues like climate change clean water and renewable energy numerical solution of the strongly coupled nonlinear pdes presents significant numerical challenges to the commonly used stabilized biconjugate gradient bicgstab with incomplete lu factorization ilu preconditioner the flow and reactive transport simulation typically uses the finite volume method fvm to conserve mass accurately for reaction calculations and the newton raphson nr method is widely used to solve nonlinear equations a linear system of equations namely the linear jacobian matrix must be solved at each nr iteration the nature of immiscible multiphase physics and challenges of applications to large scale heterogeneous geological domains produces an ill conditioned matrix for the linear solver due to the highly nonlinear nature of capillary pressure and relative permeability constitutive relations as well as other mechanistic models like rock compressibility and fracturing as a function of liquid pressure and large contrasts in grid cell volumes and neighboring permeability of discretized rock formations and engineered features the waste isolation pilot plant wipp the nation s only active nuclear waste repository in carlsbad new mexico national research council 1996 presents the complex simulation domain and process models appendix a our simulator pflotran hammond et al 2012 lichtner et al 2015 is a massively parallel subsurface flow and reactive transport code for potential use in future performance assessment pa calculations for the wipp helton et al 1997 appendix b the goal is to implement 3d simulations of immiscible isothermal 2 phase flow in the wipp in a parallel computing environment models constructed for the development and testing of the new capabilities are used here to demonstrate advancements in linear and nonlinear solvers that improve performance the wipp pa requires 1800 realizations of the 10 000 year 3d simulation models to complete the full uncertainty quantification the desired run time for 1800 simulations is 3 weeks on the wipp high performance computing cluster and we demonstrate results for 3 simulations exhibiting a wide range of numerical difficulty as a part of the pa process the default solvers were tested on a wide variety of realizations in all cases the run times were either too long or the simulations did not finish within the 48 hour time limit using 64 cores this very practical need to successfully meet regulatory pa for the nation s only active nuclear waste repository was the main motivation for the development and testing of newly implemented linear and nonlinear solvers our simulator already has the capability to simulate immiscible multiphase flow but the challenge arises from a modeling cubic meter to cubic kilometer volumes in the same domain b high grid cell aspect ratios forced by stratigraphy of the natural surroundings of the wipp c the extreme material properties e g permeability and porosity of bedded salt rock formation and d large contrast in material properties due to open excavation next to the formation these considerations introduce difficulty in solving the linear system of equations by causing the jacobian matrix to be ill conditioned resulting in failures of the conventional parallel iterative krylov solver an effective solver requires proper scaling of solution vectors for each fluid phase preconditioners tailored to the governing equations and interprocessor communication that maximize parallel performance in this work we integrate several well established numerical methods to improve computational performance and solver robustness to enable simulation of challenging real world subsurface engineered systems in particular to address the ill conditioned matrix and breakdown of the krylov solver the constrained pressure residual cpr preconditioner wallis et al 1985 was tested with two different decoupling methods alternate block factorization abf bank et al 1989 and quasi implicit pressure and explicit saturation impes lacroix et al 2001 the flexible generalized minimal residual method fgmres saad 1993 linear solver was used and the performance was compared to the widely used bicgstab linear solver and ilu preconditioner saad 2003 the integration of these well established preconditioners was the key to improving computational performance and solver robustness the nonlinearity in the wipp model is also very difficult to solve due to discontinuous nonlinear constitutive relationships such as salt creep rock fracturing and borehole intrusions these constitutive relations have abrupt changes and piecewise continuous features for which analytical derivatives do not exist these features are to be expected for repository systems which contain discrete layered engineered and natural media with fitted curves from experimental data to improve solution of the nonlinear optimization problem we apply the newton trust region dogleg cauchy ntrdc method to enhance convergence of the newton raphson iterations powell 1970 the efficient linear and nonlinear solvers are compared to bicgstab block jacobi ilu bcgs ilu we also study the parallel performance and strong scalability of the efficient solvers in summary the main contributions of our paper are to emphasize that successful simulation of multiphase flow for large scale practical repository applications requires combinations of special linear and nonlinear solution approaches beyond the standard algorithms that have been successfully applied in the past section 2 presents the governing equations nonlinear capillary pressure and relative permeability relationships and newton raphson linearized matrix section 3 summarizes the derivation and implementation of cpr amg two stage preconditioners with two different decouplers alternate block factorization abf and quasi implicit pressure explicit saturation impes the section also includes the method and algorithm for ntrdc all the significant improvements made with the efficient solvers are demonstrated and analyzed in section 4 and the paper concludes with section 5 2 isothermal immiscible two phase porous media flow model 2 1 governing equations the wipp model simulates two phases namely brine liquid and hydrogen gas phases and it assumes isothermal conditions the model assumes further that brine cannot evaporate into hydrogen gas and that hydrogen gas does not dissolve into the brine therefore the resulting immiscible model assumes that both phases exist in all simulation grid cells these assumptions are a part of the wipp pa conceptual model caporuscio et al 2003 it uses the generalized flux form of darcy s law lichtner et al 2015 in eq 1 and 2 s is saturation ϕ is porosity ρ kg m 3 is density k m 2 is intrinsic permeability μ pa s is viscosity k r is relative permeability and q kg m 3 s 1 is the source sink term each phase is denoted by subscripts g for hydrogen and w for brine the governing equations are 1 t ϕ ρ w s w ρ w k k r w μ w p w ρ w g z ˆ q w 2 t ϕ ρ g s g ρ g k k r g μ g p g ρ g g z ˆ q g to couple these two mass balance equations there are saturation and capillary pressure constraints to complete the system of equations 3 s g s w 1 4 p c p g p w 2 2 relative permeability and capillary pressure for two phase flow in two phase immiscible flow modeling additional constitutive relations expressing capillary pressure and relative permeability as a function of saturation are required in the wipp model we use 12 different models for relative permeability and capillary pressure functions frederick et al 2020 the list includes versions of brooks corey linear power and a wipp specific model that represents excavated empty and filled waste area these functions were assigned to different materials in the simulation domain which leads to more nonlinearity in the simulations 2 3 linearized system of equations for each newton iteration the jacobian matrix from the newton raphson method is composed of the derivatives of the discrete form of the governing equations eqs 1 and 2 with respect to the primary dependent variables each grid cell has two unknowns p w and s g and generates a 2 2 jacobian block j as 5 f w p w f w s g f g p w f g s g n δ p w δ s g f w f g n where f w and f g are residuals of the two unknowns as one can imagine increasing the number of grid cells will quickly increase the size of the system of equations according to the code profiler valgrind nethercote and seward 2003 approximately 31 of computation time is spent on solving the linear system 18 is spent calculating the residuals and 51 is spent on computing jacobian for simulating the test cases introduced in section 4 for bcgs this research will focus on the 31 of computation time related to the performance of the linear system solver to improve the overall computation time it will also focus on an improved nonlinear solver for more accurate solution updates to reduce the overall number of nonlinear iterations which in turn will reduce the number of linear solver iterations jacobian and residual calculations the linear solution is declared convergent when the 2 norm of the residual is less than the default value of 10 5 times the 2 norm of the initial residual this is the default relative 2 norm convergence criterion in portable extensible toolkit for scientific computations petsc 2 4 newton raphson iteration for nonlinear system of equations the fully coupled system of equations eqs 1 and 2 in our simulator is discretized by backward euler for the time derivative and standard second order finite volume method for spatial derivatives the unknown variables are gas saturation s g and water pressure p w day et al 2012 in a grid cell we apply newton s method to solve the resulting system of coupled nonlinear algebraic equations where a system of k nonlinear equations by finding zeroes in a continuously differentiable function f r k r k described as 6 j f x n δ x f x n and x n 1 x n δ x with unknowns or solution update δ x iteration number n the residuals f and jacobian matrix j f either direct or iterative krylov linear solvers saad 2003 like bicgstab or generalized minimal residual method gmres with preconditioning are used to solve for δ x in eq 6 typically newton iteration is stopped and convergence is declared when the 2 norm of the residual drops below absolute or relative tolerances or the infinity norm of the residual drops below absolute tolerance or the infinity norm solution update drops below relative tolerances 7 f x n 2 tol absolute f x n 2 f x 0 2 tol relative or f x n inf tol absolute x n x n 1 inf tol update 2 5 time stepping algorithms the time step size in our simulator is dynamically adjusted to the criteria described in this section after the solution has converged for a completed time step the next time step size δ t is determined by several factors if the solution has not converged then our simulator repeats the current time step with half of the time step size δ t 2 i e time step cut time step size can be governed by the number of newton iterations required to converge a maximum pressure or saturation change or a maximum time step size which can change over the course of the simulation all of which can be user defined for the examples below we employ the default settings for cases where the solution converges if the nonlinear iteration number 5 the next time step size is twice the size of the completed one because the current time step facilitated convergence within a small number of iterations if the nonlinear iteration number 10 the next time step size is the same as the completed time step if the nonlinear iteration number is 6 9 then the time step size is increased by a factor of 1 8 1 6 1 4 and 1 2 respectively the next time step size is further adjusted with pressure and saturation governors for example to preserve the accuracy of the simulation the default maximum pressure change for a time step for a single cell throughout the domain is 500 000 pa if the current time step size had a change in pressure larger than 500 000 pa then the next time step will be adjusted to be smaller than what was determined in the first step by the number of newton iterations the adjustment could make the next time step size smaller than the completed time step there are three cases when the current time step is repeated with the time step size cut by half if the current time step did not reach user defined convergence tolerances after iterating for the maximum number of nonlinear iterations if the resulting solution update is outside of physical boundaries e g negative gas pressure negative saturation or saturation greater than 1 0 if the linear solver does not converge properly due to krylov subspace breakdown saad 2003 non convergence oscillations or divergence the time step cut relaxes the ill conditioned matrix and reduces nonlinearity for convergence in some challenging cases the time step can be cut repeatedly however if a user specified number of consecutive time step cuts occur 40 for these simulations the simulation stops and reports failure the effect of time step cuts in computation time is discussed in the numerical experiments in sections 4 2 1 and 4 2 2 3 the need for efficient solvers major contrasts in permeability and high grid cell aspect ratios generate ill conditioned matrices which are systems of equations that are difficult to solve with conventional iterative krylov solvers such as gmres and bicgstab with ilu preconditioning the performance of these iterative solvers depends heavily on the effectiveness of the preconditioner ineffective preconditioning can result in krylov solver failure due to breakdown or from reaching the maximum number of iterations which ultimately produces small time steps and excessive run times the nonlinearity of the governing equations also may affect the conditioning of the jacobian matrix and may lead to convergence problems when using standard newton s method we address each of these computational challenges section 3 1 presents the constrained pressure residual cpr preconditioner section 3 3 presents the derivation of the newton trust region dogleg cauchy method and shows how it is applied in solving the nonlinear governing equations 3 1 constrained pressure residual preconditioner for linear solver one possible approach to address krylov solver failure is to implement cpr 2 stage type preconditioner wallis et al 1985 in the first stage the matrix corresponding to the pressure equation is extracted and solved while in the second stage the resulting residuals are corrected using coupling terms in the global system two different methods were applied to decouple the pressure matrix from the global matrix quasi implicit pressure explicit saturation impes lacroix et al 2001 and alternate block factorization abf bank et al 1989 ponting et al 2008 bui et al 2017 stüben et al 2007 and cusini et al 2015 apply the two stage combinative preconditioner cpr amg a well known preconditioner within the two phase porous media modeling community that is assisted with the algebraic multigrid amg the algorithms and methods from these papers were used as references for the developments in this study fgmres saad 1993 is used for the numerical experiments as it allows larger flexibility in the choice of solution subspace than gmres and bicgstab and it is designed to accommodate changing preconditioners for each linear solver iteration which means it can use any iterative method or multi level method as its preconditioner in this study we use multi level amg solver lacroix et al 2003 in the cpr preconditioner the algorithm to solve the system of linear equations a u b in these experiments is listed in section 3 1 2 3 1 1 qimpes and abf decoupler to explain the algorithm further the jacobian in eq 5 can be written as 8 a a p p a p s a s p a s s the restriction matrices r p and r s are defined as follows 9 r p u δ p r p t δ p δ p 0 r s u δ s r s t δ s 0 δ s where u δ p δ s where to simplify the notation δ p is the same as δ p w and δ s is δ s g in eq 5 two different decoupling methods are proposed the quasi impes formulation and abf method a decoupling matrix m is applied as 10 m a u m r the quasi impes decoupling matrix lacroix et al 2001 is defined as 11 m q i i d p s d s s 1 0 i and the abf decoupling matrix bank et al 1989 is defined as 12 m a b f λ 1 0 0 λ 1 d s s d p s d s p d p p where d is a block diagonal matrix of a e g d p p diag a p p and λ d p p d s s d p s d s p this decoupler implementation also applies modified column scaling on eq 12 to enhance the effectiveness of amg for smoothing the pressure residuals d ponting personal communication appendix c shows the scaling equation for these parallel numerical experiments boomeramg as part of the hypre falgout et al 2006 package was used as the amg solver for the cpr preconditioner while fgmres from petsc was used for the tailored linear solver runs for amg solver we used the default settings from hypre hmis for coarsening de sterck et al 2006 extended i interpolation for prolongation de sterck et al 2008 and v 1 1 cycle the results are compared to the block jacobi parallel preconditioner ilu 0 as a preconditioner for each block and bicgstab as the linear solver bcgs from petsc 3 1 2 cpr algorithm cpr preconditioner solves the decoupled pressure system with an amg solver that is extremely effective stüben et al 2007 the second stage uses the ilu preconditioner applied to the global matrix with a pressure corrected matrix cpr amg is used to solve the linear system for every newton iteration the algorithm as implemented for the numerical experiments is listed below 3 2 nonlinear optimization problem as noted earlier the extreme nonlinear nature of two phase flow in an engineered repository can present challenges for convergence these challenges can be addressed by revisiting the jacobian system used for newton raphson in eq 6 the residual function f represents the discrete approximation of the two governing equations eq 1 and 2 the goal is to solve the nonlinear system f x 0 where the linear system solution δ x is obtained by using a nonlinear solver for x x 0 δ x such that f x 0 which would satisfy the nonlinear convergence criteria the approach begins with an initial guess x 0 to approximate the function f by solving a linearized system of equations then using the zero of this linear model as the second guess the next iteration n 1 is computed as discussed in section 2 4 the process is repeated until the satisfactory convergence criteria are met eq 7 the optimization problems and convergence criteria eq 7 commonly use the 2 norm as the measurement which is defined for f x in discrete form as 13 f x 2 i 1 n f i x 2 1 2 using a practical iterative algorithm the aim is to make the residual as small as possible within a reasonable number of iterations therefore we can restate the problem as the following unconstrained minimization quadratic model 14 f x 1 2 f x 2 2 r n r finding the minimum of the function achieves the global unconstrained optimization mathworks 2020 dennis jr and schnabel 1996 15 min x r f x 1 2 f x t f x the goal of newton trust region dogleg cauchy is to solve the optimization function eq 15 which will minimize the residual f x section 3 3 explains how it works to minimize the objective function 3 3 newton trust region dogleg cauchy nonlinear solver the nonlinear solver newton trust region dogleg cauchy ntrdc is a general purpose optimization solver and it is not specific to multiphase flow models for complicated problems like this case of a large scale subsurface system newton raphson may not be robust enough to update solutions properly without a sufficiently close initial guess to the solution i e a small time step heath 2018 while the ntrdc method is more complex and expensive we hypothesize that its enhanced robustness can solve extreme nonlinear problems and achieve convergence using larger time step sizes in the simulations thereby reducing the overall computation time to explain the ntrdc methods we define the following a the newton raphson newton linearized system yields a solution update p b the current solution at nonlinear iteration n is x n c f x n is the objective function in eq 15 at iteration n after solving the newton linear system ntrdc introduces a global model function to optimize with the solution update in order to determine the proper size of the trust region tr which controls the maximum step size allowed for the newton solution update detailed derivations and proofs are found in the text by dennis jr and schnabel 1996 the newton solution update is written as 16 m n x n p f x n j x n p where f is the residual and j is jacobian in eq 5 replacing m n with f in eq 15 the model function or the tr sub problem is found as 17 m n p f n g n t p 1 2 p t b n p s t p δ n where n is the iteration number p is the newton solution update or search direction and step δ n is the trust region radius f 1 2 f t f the objective function in eq 15 g n j t f and b n j t j which is an approximation to the hessian matrix the ntrdc method defines ρ as the ratio of actual improvement numerator the objective function and the predicted improvement denominator the trust region model the ratio is used to determine the trust region size for the next iteration and whether p is the improvement direction and step for x n 1 18 ρ n f x n f x n p n m n 0 m n p n if ρ η 3 the trust region expands by a factor t 2 and if ρ η 2 the trust region shrinks by factor t 1 if ρ is in between η 2 and η 3 the trust region size is unchanged for the next iteration δ n is the trust region at iteration n typical values for these input parameters are 0 η 1 η 2 η 2 0 25 η 3 0 75 shrinking factor t 1 0 25 and expanding factor t 2 2 0 ye 2014 hauser 2005b if ρ η 1 then the tr sub problem has found the adequate model function within the trust region and the solution update is checked for convergence if the newton solution update is outside the trust region the cauchy point cp method attempts to improve the solution update in the ntrdc solver hauser 2005a cp takes the steepest descent direction hauser 2005b the cp solution update is written as 19 s n f n 2 f n t b n f n f n if neither cp nor newton solution update fits within the trust region then both solutions are combined to form a quadratic equation to solve for the ratio τ 20 δ s n c y τ s n n t s n c y 2 here c y and n t are the cauchy and newton solution update respectively the ratio τ and 1 τ multiply the newton and cp solution respectively to form the final solution update again if this solution satisfies ρ η 1 then the convergence of the time step is checked the combination of cauchy and newton solution update can still break down the inner iterations of trust region method when the trust region shrinks below 10 12 of the solution norm x at this point our simulator will cut the time step to relax the problem the procedure of the algorithm is further described using an example test optimization problem in appendix d 3 3 1 scaling solution vector for two phase flow proper scaling of solution update vector and jacobian is helpful for any advanced nonlinear solvers that measure l2 norm to determine whether the newton solution update direction and step size are suitable for optimization for ntrdc the entire inner iteration must be properly scaled in order to truly take advantage of the robustness of the algorithm in our simulator the pressure unknown variable is in the units of pa and the saturation unknown variable is dimensionless between 0 and 1 this means the solution update vector has two discrete range of values for example a new pressure update can arbitrarily range between 0 and 1000 pa while a new saturation update can range between 0 and 0 1 in the simulation domain for this hypothetical nonlinear iteration the l2 norm is defined as 21 x 2 x 1 p 2 x 1 s 2 x 2 p 2 x 2 s 2 x n p 2 x n s 2 in the given example it is observed that pressure values in the solution update vector can completely diminish saturation values measuring an improper l2 norm for the nonlinear solvers therefore the two quantities must be scaled properly by measuring the infinity norm of each quantity eq 22 and then normalizing them such that both quantities in the solution range between 0 and 1 22 p x p max x i p and s x s max x i s all of the pressure values in the solution update are scaled by p the saturation values are scaled by s and the jacobian is column scaled by the stride vector p s p s the effects of scaling is discussed in the 4 2 3 3 3 2 ntrdc algorithm as noted above the ntrdc algorithm is more complex than a simple newton raphson iteration but we expect it to be more robust which overall enhances the efficiency of the solver the additional computational expense of the algorithm is mainly due to residual evaluations in the inner iterations that are needed if the solution is not within the trust region if the trust region criteria are satisfied after calculating the linear system the algorithm is almost identical to newton raphson except for the evaluation of ρ the algorithm is listed below 4 numerical experiments we used the wipp 3d simulation domain for the numerical experiments with three different test cases easy mid and hard case these represent three hypothetical future scenarios where additional information is included in section 4 1 the computational enhancement from the fgmres cpr qimpes fcq and fgmres cpr abf fca preconditioners is compared to block jacobi ilu preconditioner in section 4 2 1 then the effectiveness of newton trust region dogleg cauchy ntrdc compared to newton using bicgstab block jacobi ilu 0 bcgs is demonstrated in section 4 2 2 the combination of tailored linear and new nonlinear solvers that mostly improve the computation time are discussed in section 4 2 4 finally the strong scalability of the combinations of the new linear and nonlinear solver results are shown in section 4 3 3 appendix e displays all tabulated data for the figures in the numerical experiments section 4 1 description of numerical simulation case study there are several key engineered features 650 meters below the surface in the 3d wipp simulation domain which are shown in fig 1 these include excavated shafts and hallways to deliver and deposit nuclear wastes and the borehole during hypothetical unexpected intrusion scenarios three test case scenarios occur on the same domain with different numerical difficulty the easy case simulates an undisturbed wipp nuclear waste repository without disruption from closure through the 10 000 year simulation period there are material changes throughout the domain as the run of mine salt closures reconsolidate and the disturbed salt rock above and below the excavated regions self heals returning the porosity and permeability to near the intact salt values the mid case expands the easy case to include a hypothetical human intrusion event where the borehole is drilled into the repository at 1000 years after the simulation begins fig 1 illustrates a borehole as a skinny brown line in the repository after the intrusion in this scenario the time step size changes from months or years to minutes to simulate the transient flow process through an intruding borehole that has very high permeability compared to the salt rock formation such scenarios cause a dramatic perturbation in time spatial and permeability scales the hard case includes two separate events one borehole intrusion into the repository at 350 years and a second borehole intrusion through the repository into an over pressured brine reservoir at 1000 years this event can potentially flood the dry waste repository the simulation domain spans approximately 30 km in the x 28 km in the y and 1 km in the z direction and the domain is expanded in x and y direction from the nuclear waste repository figs 2 and 3 the domain is also non uniformly discretized in 33 layers in the z direction to represent 11 formations table pa 3 of u s department of energy carlsbad field office 2014 the wipp 2 phase flow model consists of a multi scale problem that spans over 10 orders of magnitude in permeability 5 orders of magnitude in the spatial dimensions and requires simulation for 10 000 years the domain has a total of 460 020 grid cells or 920 040 unknowns and consists of 41 different rock soil types with 12 different capillary pressure and relative permeability relations national research council 1996 frederick et al 2020 the simulation also includes gas source terms generated from the degradation of plastics and rubbers corrosion of steel drums and radiolysis of water more nonlinearities are introduced by the interbed fracture model and salt creep closure model day et al 2012 in this section all simulation results at 10 000 years for 20 selected grid cells in important regions are verified and confirmed against each other within 0 5 of relative error convergence tolerances and all the other parameters in the simulation inputs are the same in all simulations we next report results comparing block jacobi ilu 0 and newton raphson against the tailored preconditioner and the new nonlinear solver presented above in section 3 4 2 improvements of the efficient solvers compared to the bcgs solvers this section demonstrates how the different combinations of the efficient solvers outperformed the bcgs solver in the number of linear and nonlinear iterations and the overall wall clock computation time section 4 2 4 shows the performance of the best combination of the efficient solvers versus the bcgs solver 4 2 1 bcgs vs tailored linear solvers and preconditioners we applied two different methods to decouple the pressure matrix from the global matrix quasi impes and abf the nonlinear solver used in this section is newton raphson newton the linear solvers used are bcgs with block jacobi preconditioning and ilu 0 in each block bcgs and fgmres with cpr quasi impes fcq and cpr abf preconditioner fca for the easy case fig 4a shows the wall clock computation time using 2 compute nodes 32 cores on sandia national labs snl s skybridge high performance computing hpc cluster that hosts intel xeon processor e5 2670 2 60 ghz 20m cache sandia national laboratories 2021 the bcgs takes 42 h to complete the 10 000 year simulation of the undisturbed scenario the easy case while the tailored fgmres cpr qimpes fcq takes 12 h 3 5x speedup and fgmres cpr abf fca takes 1 2 h 35x speedup fig 4a demonstrates that bcgs had 28 million linear iterations and 76 thousand nonlinear iterations while fca had 125 times fewer linear iterations and five times fewer nonlinear iterations although each iteration for fca is more computationally expensive it is more robust and effective and results in fewer iterations the main difference in computation time between bcgs and fca fcq solvers emerges from the large number of linear solver failures for bcgs as shown in fig 4b when the time step cut is triggered the current time step is halved and retried for a convergence the linear solver failure occurs when it diverges or oscillates or when the krylov space breaks down the time step is intentionally cut when the linear solver fails when the nonlinear solution update is non physical or when the maximum nonlinear iteration count is reached the figure illustrates the disproportionate number of bcgs solver failures fca eliminated linear solver failures in the simulation which is the dominant cause of bcgs solver time step cuts and leads to smaller δ t per time step and longer overall computation time stüben et al 2007 discusses that abf may result in strongly nonsymmetric matrix and also it may lead to oversolving the jacobian system however the modified column scaling corrects this issue in this paper and works in favor of amg by preserving the shape of the original a p p and a more aggressive row scaling is performed than qimpes fcq performance was consistently worse than fca throughout these experiments qimpes is a more conservative decoupling strategy that preserves most of the original structure of the coupled system which evidently did not benefit the numerical experiments the strength of decoupling of the matrix is explained in more detail in stuben s paper all the results in the next sections illustrate the effectiveness of the fca linear preconditioner combination and exclude fcq results because fcq performed consistently worse than fca for 15 unique samples from the 1800 realizations table 1 shows the performance of bcgs and fca for the mid case it required twice as many cores to complete the simulation in a similar computation time bcgs took 35 7 h and fca solver and preconditioner combination took 1 2 h 30x speedup table 2 presents the results for the hard case where only fca completed the simulation on 32 cores fca took 41 h to complete compared to 1 2 h for the easy case demonstrating increased difficulty of the hard case bcgs could not solve this case in 4 days with 128 cores and there is no valid data point to present in the table section 4 2 3 demonstrates how a more advanced nonlinear solver and solution scaling can further reduce the hard case run time 4 2 2 newton vs ntrdc nonlinear solver section 4 2 1 demonstrated how the tailored preconditioner linear solver combinations could improve the overall computation performance while using the newton solver here we compare newton and ntrdc solver performance using bcgs ntrdc uses auto scaling for inner iterations in the comparison because it is the more robust way to utilize the ntrdc algorithm the original linear system is not scaled it is the same for newton and ntrdc auto scaling is described in section 3 3 1 and its effectiveness is demonstrated in section 4 2 3 fig 5a demonstrates the superior performance of ntrdc relative to newton for the mid case where ntrdc is 7x faster even with a 40 growth in nonlinear iterations newton which applies a full step update in the search direction struggled to converge often generating an ill conditioned system of equations for bcgs that resulted in failure see fig 5b ntrdc s truncated step update in a more optimal search direction eliminated such failures in bcgs and greatly improved performance most of the linear solver failures were due to bcgs reaching the maximum number of iterations i e the default 10 000 such failures are computationally expensive as for each bcgs failure that resulted in a time step cut there were one to three nonlinear iterations and 1000 to 9000 linear iterations wasted in addition the time step was cut in half slowing simulation progress although ntrdc required more nonlinear iterations ntrdc averaged 36 linear iterations per nonlinear iteration compared to 712 for newton also the linear systems produced by ntrdc were more robust and solvable reducing the number of wasted linear iterations due to time step cut from 21 756 005 newton to 941 029 ntrdc fig 5b illustrates 4x more time step cuts for ntrdc relative to newton due to the nonlinear solver reaching the maximum nonlinear iteration count however since each ntrdc iteration required far fewer linear iterations 36 versus 712 for newton each ntrdc iteration was much faster greatly reducing the overall runtime as mentioned in section 2 3 the linear solver takes 31 the residual calculation takes 18 and the jacobian takes 51 of total the computation time for the newton solver ntrdc has the computational effort distribution of 23 21 and 56 for the linear solver residual calculation and jacobian calculation respectively the changes in effort ratio for ntrdc corresponds to the number of iterations where the number of linear iterations for the linear solver has been reduced and the inner iteration residual calculation for nonlinear solve has increased also the jacobian has an increased computational effort ratio because the linear solver computational effort has been reduced significantly 4 2 3 effects of scaling solution vector this analysis highlights the importance of scaling the solution update vector for pressure to be in the range of saturation as discussed previously in section 3 3 1 and step 5 3 of the ntrdc algorithm we hypothesized that if the solution update vector is not properly scaled and normalized between two unknown variables pressure and saturation then the pressure dominates the optimization criterion which might result in a low accuracy solution for saturation thus if the two unknown quantities are scaled properly it should be possible to minimize both the saturation and pressure residuals using the ntrdc algorithm with an enhancement in computational performance fig 6 demonstrates that auto scaling capability can dramatically improve the efficiency of the ntrdc solver both cases use fca preconditioner linear solver combinations fig 6a shows the performance of newton ntrdc noscale and ntrdc for the mid case using 32 cores on skybridge it is shown that compared to newton ntrdc noscale reduced the overall computation time by 11 but ntrdc on the other hand achieved a 48 reduction in computation time the easy case which is not shown does not have a major reduction in computation time using ntrdc noscale because the simulation does not include as many sudden changes in the dynamic nonlinear processes and fca is powerful enough to resolve the resulting ill conditioned matrix the impact of nonlinear solver scaling is maximized for the hard case which has the most nonlinearity in the simulation fig 6b shows that ntrdc has about 3 times the overall computation time reduction compared to ntrdc noscale and newton furthermore ntrdc without scaling optimizes for pressure more than saturation section 3 3 1 therefore it degrades in performance compared to newton in this case section 4 2 4 will present the best improvements that can be obtained with combinations of ntrdc and fca 4 2 4 best improvements with tailored linear and new nonlinear solvers fig 7 shows the two best cases with the best improvements in computation time compared with bcgs unfortunately it was not possible to assess the hard case because the simulation did not complete after two days with 64 cores or 128 cores for bcgs the combinations of cpr abf preconditioner for gmres and ntrdc nonlinear solver resulted in the best computation time with 32 cores for the easy case 42 h for bcgs and newton reduced to 0 9 h for the efficient solvers which is 47 times faster interestingly gmres performed slightly better than fgmres approximately 5 the number of linear iterations decreased from 28 443 067 to 107 950 approximately 263 times and the number of nonlinear iterations decreased from 76 874 to 9 787 recall that each iteration of bcgs linear solve and nonlinear iterations are computationally much cheaper than the efficient solvers but the efficient solvers robustness and accuracy on each iteration enable them to outperform overall the mid case required more computational resources for bcgs so the comparison was made using 64 cores in this case the efficient solvers of fgmres and ntrdc outperformed the traditional solvers by 40 times with computation time reduction from 35 7 h to 0 9 h the number of linear iterations dropped from 51 864 686 to 348 019 approximately 149 times and the number of nonlinear iterations dropped from 72 837 to 12 647 4 3 parallel scalability on the efficient solvers this section illustrates the parallel scalability of the efficient solvers fgrmes cpr abf preconditioner linear solver combination and ntrdc we also explain some defects found during the numerical experiments due to parallel domain decomposition of the unstructured grid and node packing effect in the modern processor architectures 4 3 1 parallel domain decomposition impacts on computational performance bcgs was tested on many different easy case realizations the results were similar in that it was too slow and would often reach the 48 hour desired time limit for pa with 64 cores which is about 15 000 unknowns per core using decomposition of the domain another downside of the bcgs solver was the inconsistent parallel computation time this behavior would hinder the users ability to reliably estimate wall time and the required number of cores when running simulations on shared clusters the inconsistency is caused by how the domain is decomposed and the block jacobi preconditioner drops coefficients that are off process instead of factoring them this elimination of factored coefficients degrades preconditioner performance in other words the coefficients that are at the boundaries of the decomposed domain do not have information from the neighboring decomposed domains when solving the linear system most of the time the loss of this information has minimal impacts upon convergence but not for simulation domains that are as complex as the wipp domain for example if the domain decomposition occurs between the boundary of an excavated zone very high permeability and intact salt formation very low permeability it is very important to know the solution update from the neighboring but separately calculated grid cells in each linear iteration the bcgs inconsistencies are clearly illustrated in fig 8 compared to fca fca shows consistent improvements in computation time over 32 64 and 128 cores or about 30 000 15 000 and 7 500 unknowns per core respectively however bcgs using 64 cores outperforms 128 cores and using 64 cores is 4 times faster than 32 cores it should ideally improve the performance only by 2 times the number of nonlinear iterations required to complete the 10 000 year simulation provide a more in depth explanation bcgs has a large fluctuating number of nonlinear iterations but fca shows the uniformly consistent number of nonlinear iterations ideally the number of nonlinear iterations should not change for a given simulation regardless of how the linear system was solved i e the number of cores domain decomposition and types of solvers would not matter bcgs showed erratic behavior while fca showed nearly consistent newton iteration numbers 15468 15443 and 15609 for 32 64 and 128 cores respectively 4 3 2 scalability performance on modern processor architectures before discussing the scalability analysis of fca with newton and ntrdc the physical architecture of the skybridge cluster must be understood properly skybridge cluster nodes have intel xeon processor e5 2670 2 60 ghz 20mb cache the processor has two sockets four cores per socket and two threads per core that sum to 16 cpus connected with infiniband sandia national laboratories 2021 the cluster has the multi threading option turned off so in effect petsc can utilize all 16 cpus in a node however when all 16 cpus are used there is a bottleneck effect in accessing cache and memory for each of the cpus the maximum number of memory channels in the node is 4 for skybridge some parallel tools take advantage of multi threading for better performance but our simulator uses petsc mode that does not take advantage of multi threading fig 9 illustrates the efficiency of the processors when they are relieved from the bottleneck all of the cases use 16 cpus in total and runs the mid case using fca newton solver but each case uses a different number of nodes and a different number of cores per node when using one node and all 16 cores this simulation takes close to 15 000 s but when two cores per node and eight nodes are used the simulation takes about 10 000 s that is close to a 30 difference in computation time even though the same number of cpus are used of course there can be more communication time loss across nodes when using message passing interface mpi as seen between 8 n 2 cpn case and 16 n 1 cpn case but that seems to be a small difference compared to the bottleneck access to cache and memory within the node the results are specific to the skybridge cluster but multi threaded nodes are a common parallel computer architecture one must take into account the physical architecture of the machine before studying the scalability of a parallel code 4 3 3 scalability of combined tailored linear and new nonlinear solvers the mid case was run on different numbers of cores and cores per node to demonstrate the strong scalability of the newly implemented algorithms fca preconditioner linear solver combination with newton and ntrdc the strong scaling of fca newton and fca ntrdc follows the trend of ideal strong scaling in fig 10 also it should be noted that bcgs newton is shown with a single available data point in the figure with 64 cores the scalability trend has a kink after 8 cores because of the node packing effect mentioned in section 4 3 2 the trend of fully packed nodes 16 32 and 64 cores is still close and parallel to the ideal trend line there are four data points with the hollow circle and hollow triangle that represents reduced node packing effect with 4 cores per node utilization and you can see that it is much closer to the ideal for strong scalability running this domain on 128 cores would not be efficient because an insufficient number of unknowns would be distributed to the processors to maximize efficiency i e there would be too small of a problem to solve per processor fig 11 is the more detailed plot of strong scaling in terms of efficiency where the ideal case reduces the computation time exactly by the number of cores ntrdc had higher efficiency than newton for 8 cores but slightly worse efficiency for 16 and 32 as ntrdc may require more parallel communications than newton all of the combinations of non packed node cases performed better than packed nodes and stayed above 50 efficiency for both ntrdc and newton the same kink from 8 cores to 16 cores is seen here in dramatic fashion by reducing the packing by a half newton data points at eight cores per node hover between 50 to 75 efficiency 5 conclusion numerical simulation of multiphase flow continues to be one of the most computationally challenging problems in subsurface modeling due to the highly nonlinear governing equations and the need for fine spatial discretization simulated in a parallel high performance computing hpc environment to overcome these challenges we present and evaluate two new preconditioners a constrained pressure residual cpr ponting et al 2008 with alternate block factorization abf bank et al 1989 and quasi implicit pressure explicit saturation qimpes decouplers cusini et al 2015 and b a new nonlinear solver newton trust region dogleg cauchy ntrdc dennis jr and schnabel 1996 we demonstrate the performance of these methods for a large number of numerical experiments of isothermal immiscible flow of the waste isolation pilot plant wipp models national research council 1996 wipp is a real world example of a large scale engineered subsurface system that adds additional practical complexities such as 41 different soil types with 12 different relative permeability and capillary pressure functions the simulations also include nonlinear processes such as gas generation from the degradation of bio materials steel drum corrosion and radiolysis of water fracture models for clay and anhydrite interbeds in the bedded salt formation a creep closure model for excavated rooms in salt formations and human borehole intrusion events simulated by material property changes in a grid cell over time these features create an ill conditioned jacobian and extremely nonlinear system of equations three test scenarios occur in the same domain the easy case simulates an undisturbed wipp nuclear repository without disruption from closure through the 10 000 year simulation period the mid case simulates a hypothetical human intrusion event where a borehole is drilled into the repository in search of natural resources 1000 years after the closure the hard case includes two separate events where one borehole intrusion occurs into the repository at 350 years and a second borehole intrusion through the repository into an over pressured brine reservoir both mid and hard cases continue the simulation for 10 000 years after the closure our results show that the bcgs linear solver saad 2003 and preconditioner with newton raphson nr nonlinear solver failed to complete simulations due to repeated krylov solver breakdowns and oscillatory non convergence for most of the 1800 realizations required to complete for uncertainty quantification of the wipp performance assessment pa the pa is desired to be done in 3 weeks with the high performance computing cluster at wipp we hypothesized that a more efficient preconditioner can reduce the number of solver breakdowns and a new nonlinear solver can resolve the non convergence oscillatory behavior of the newton solver these efficient solvers must be parallelized and scalable in order to improve computational efficiency in large scale simulations like the wipp the tailored linear and the new nonlinear solvers are proven to be much more efficient than bcgs in this paper the paper demonstrates the combinations of flexible generalized minimal residual fgmres linear solver saad 1993 cpr abf preconditioner and ntrdc nonlinear solver can give up to 47 times speed up compared to bcgs on the first trial of running 1800 realizations with fgmres cpr abf preconditioner with the default newton solver approximately 97 of the simulations were completed the other 3 of the simulations are expected to complete with the ntrdc solver once the solver becomes official through a quality assurance process these were never attempted with bcgs solver for the wipp 3d models the bcgs solver is inefficient because the block jacobi preconditioner drops coefficients that are off process instead of factoring them when the domain is decomposed for running in parallel the coefficients that are at the boundaries of the decomposed domain do not have the information from the neighboring domain when solving the linear system which leads to inaccuracy especially in discrete and heterogeneous domains the algebraic multigrid amg method stüben et al 2007 used in cpr preconditioner does not have such a loss of information using the same nr nonlinear solver cpr amg preconditioned linear solver was up to 35 times faster than bcgs the extreme nonlinearity of the real world models leads to nr non convergence oscillations which prohibit the solver to find the global minimum of the system of nonlinear equations the new nonlinear solver ntrdc was able to provide a more robust solution to the nonlinear optimization problem using bcgs linear solver ntrdc was 7 times faster than nr nonlinear solver while ntrdc resolved the nonlinearity and provided relaxed matrices nr caused many linear solver failures because it generated ill conditioned jacobian for the linear solver the strong scaling efficiency for the combined methods fgmres cpr abf and ntrdc was between 50 to 75 efficient which was similar to newton and better than bcgs future work will apply these methods for non isothermal miscible multiphase flow in porous media that incorporate primary variable switching and multiple phase states due to miscibility examples would be to simulate an underground repository for high level radioactive waste or a geothermal power plant site we expect these robust methods to be powerful for nonisothermal miscible applications with high degrees of nonlinearity credit authorship contribution statement heeho d park conceptualization methodology software validation investigation writing original draft formal analysis visualization glenn e hammond conceptualization software supervision writing review editing albert j valocchi supervision writing review editing conceptualization tara laforce resources visualization declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the authors would like to thank paolo orsini sebastien loisel and daniel stone from pflotran ogs development team for setting up the framework of cpr preconditioner in pflotran they would like to thank michael nole alex salazar emily stein chris camphouse paul shoemaker and tito bonano in the wipp pa team and geologic disposal safety assessment gdsa team at sandia national labs for the support of this work and the doctoral study program in university programs at sandia this research was funded by the us department of energy office of environmental management doe em under the waste isolation pilot plant program and the us department of energy office of nuclear energy doe ne under the spent fuel and waste science and technology sfwst campaign this paper describes objective technical results and analysis any subjective views or opinions that might be expressed in the paper do not necessarily represent the views of the u s department of energy or the u s government sandia national laboratories is a multimission laboratory managed and operated by national technology engineering solutions of sandia llc a wholly owned subsidiary of honeywell international inc for the u s department of energy s national nuclear security administration under contract de na0003525 this research is funded by wipp programs administered by the office of environmental management em of the u s department of energy sand2021 11975j appendix a background on wipp the waste isolation pilot plant is constructed in southeastern new mexico and operated by the u s department of energy doe office of environmental management for the deep geologic disposal of defense generated transuranic waste from doe sites around the country national research council 1996 wipp is located 650 meters below the surface in a bedded salt rock formation called the salado formation which mostly consists of layers of halite and thin layers of mixed clay and anhydrite typically clay and anhydrite are both considered low permeability 10 17 to 10 19 m 2 but relative to the salado formation and on the time scale of 10 000 years they are considered higher permeability compared to that of halite 10 21 to 10 24 m 2 the pa simulations include the culebra dolomite layer where accessible groundwater flows having a permeability of 10 14 m 2 the pa calculations helton et al 1997 estimate the probability and consequence of potential radionuclide releases from the land withdrawal boundary defined by the land withdrawal act of 1992 law 1992 which is a 4 4 mile area that contains the repository therefore the 3d simulation domain scale spans from decimeters a borehole to a few kilometers a schematic of wipp is in fig a 12 there can be extreme contrasts in porosity permeability and volume over short distances a borehole grid cell is defined with permeability on the order of 10 11 m 2 whereas permeability of the adjacent salado halite grid cell may be 10 24 m 2 in summary the wipp 2 phase flow model consists of a multi scale problem that spans over 10 orders of magnitude in permeability 5 orders of magnitude in the spatial dimensions and requires simulation for 10 000 years table pa 3 of u s department of energy carlsbad field office 2014 also some scenarios require a borehole intrusion simulation by human activity possibly in search of natural resources near or through the repository human intrusion will introduce instantaneous changes in material properties for some grid cells during the simulation which might require a dramatic reduction of time step size down to minutes for numerical stability appendix b background on pflotran pflotran is an open source and community driven code developed by doe that solves a nonlinear system of partial differential equations pde s governing multiphase flow and multicomponent reactive transport in porous materials using the finite volume method the code is designed to run in massively parallel computing architectures as well as workstations and laptops the code is developed and used by an international research community of subsurface scientists hammond et al 2012 lichtner et al 2015 it can handle many subsurface processes involving flow and transport in porous media including multiphase isothermal non isothermal miscible immiscible flow co2 two phase flow black oil model richards flow and thermal hydrologic flow and multicomponent reactive transport including aqueous complexation sorption and mineral precipitation and dissolution our simulator also has the capability to run miscible two phase co2 two phase black oil richards and thermal hydrologic models the flow and reactive transport equations are solved using the newton raphson method and fully implicit temporal discretization parallelization is achieved through domain decomposition using the petsc library balay et al 2019 our simulator scales well on distributed memory high performance computing architectures hammond et al 2014 it has been run on up to 2 18 processor cores with problem sizes up to 2 billion degrees of freedom hammond et al 2014 it uses the metis parmetis karypis 2011 libraries for unstructured grids also it can read and write hdf5 folk et al 2011 files as well as ascii files for general use appendix c alternate block factorization scaling in section 3 1 1 we introduced the abf decoupling matrix eq c 1 describes how the abf matrix is modified or scaled to improve convergence based on personal communication with david ponting c 1 m a b f c 1 0 0 c 2 λ 1 0 0 λ 1 d s s d p s d s p d p p where c 1 abs d p p abs d s p c 2 abs d p s abs d s s λ d p p d s s d p s d s p the scaling is applied to preserve the long wave shape of diffusion residuals which is very effective for solving by the amg solver we credit david ponting and daniel stone for initially implementing the method in pflotran appendix d ntrdc algorithm demonstration fig d 13 demonstrates the ntrdc methods the contour in the figure is the branin optimization test function commonly used to test optimization algorithms with unknowns x and y coordinates bingham 2013 this function is particularly difficult for newton solver because of multiple minima and saddle points in the v shaped lower channel the starting point was randomly selected in the figure a thinner white line in the figure illustrates the original newton step and direction solution update after the first iteration however it is over estimated such that ρ η 2 and the trust region shrinks in the next inner iteration the newton step size is truncated to the shrunk trust region size but this time it does not satisfy the condition s n n t δ in the algorithm listed in section 3 3 2 the second solution cauchy step and direction in eq 19 is calculated but also does not satisfy the condition s n c y δ finally the dogleg method combines the two solution step and direction vectors with calculated ratio τ from eq 20 the combined solution finds one of minima of the contour function in just 2 iterations in this example the improvement over ntrdc over newton is demonstrated in section 4 2 2 appendix e tabulated data for figures see tables e 3 e 10 balay et al 2019 balay s abhyankar s adams m brown j brune p buschelman k dalcin l dener a eijkhout v gropp w petsc users manual 2019 argonne national laboratory s balay s abhyankar m adams j brown p brune k buschelman l dalcin a dener v eijkhout w gropp et al petsc users manual bank et al 1989 bank r e chan t f coughran w smith r k the alternate block factorization procedure for systems of partial differential equations bit numer math 29 4 1989 938 954 r e bank t f chan w coughran r k smith the alternate block factorization procedure for systems of partial differential equations bit numerical mathematics 29 4 1989 938 954 bingham 2013 bingham d branin function 2013 simon fraser university url https www sfu ca ssurjano branin html d bingham branin function 2013 https www sfu ca ssurjano branin html bui et al 2017 bui q m elman h c moulton j d algebraic multigrid preconditioners for multiphase flow in porous media siam j sci comput 39 5 2017 s662 s680 q m bui h c elman j d moulton algebraic multigrid preconditioners for multiphase flow in porous media siam journal on scientific computing 39 5 2017 s662 s680 caporuscio et al 2003 caporuscio f gibbons j li c oswald e waste isolation pilot plant salado flow conceptual models final peer review report 2003 u s department of energy carlsbad area office url https www wipp energy gov library cra cra 2014 others caporuscio gibbons oswald 2003 salado flow conceptual models erms526879 pdf f caporuscio j gibbons c li e oswald waste isolation pilot plant salado flow conceptual models final peer review report 2003 https www wipp energy gov library cra cra 2014 
247,this study examines multi scale aspects of radar rainfall r r conditional error defined as the difference between a given r r value and the conditional average of corresponding reference observations to decompose the systematic and random error components the authors adopted the second order separation method while previous studies relied on addressing the first order moment i e conditional bias only the authors applied a non parametric kernel regression approach to characterize the conditional mean and standard deviation and thus to derive a distribution of random component standardized error this empirical study is based on data from two rain gauge networks consisting of 66 and 115 gauges and two r r estimates the iowa flood center and multi radar multi sensor products over the iowa domain in the united states the authors explored the effect of multiple temporal 1 24 h and spatial 0 5 32 km scales on the conditional error structure they found that the error distribution gradually approaches the gaussian distribution with longer temporal scale while the error feature regarding spatial scale appears to be almost scale invariant keywords conditional error rainfall radar qpe scale 1 introduction precipitation is a main driver of hydrologic cycles and its space time variability at relevant hydrologic scales is a crucial factor to model or predict hydrologic responses and manage potential risks e g woods and sivapalan 1999 paschalis et al 2014 in the united states weather radar has been used as a primary instrument to monitor the quantitative and dynamic features of precipitation fields since the emergence of the weather surveillance radar 1988 doppler wsr 88d network known as nexrad in the early 1990s crum and alberty 1993 radar based quantitative precipitation estimation qpe has been extensively applied for operational hydrologic applications e g fulton et al 1998 krajewski et al 2017 in recent decades however many challenging issues associated with qpe uncertainties still remain e g see the discussions in krajewski and smith 2002 villarini and krajewski 2010a numerous studies have documented a variety of aspects and perspectives regarding quantification and modeling of radar rainfall r r uncertainties e g ciach et al 2007 aghakouchak et al 2010 mandapaka and germann 2010 villarini et al 2014 kirstetter et al 2015 neuper and ehret 2019 these approaches typically depart from a choice of the r r error definition e g multiplicative or additive and characterize the subsequent statistical structure of errors e g villarini and krajewski 2010b a typical application of the characterized error structure is to represent the r r uncertainty in the form of ensembles for hydrologic prediction e g mandapaka and germann 2010 kirstetter et al 2015 in this study we employ the additive term of r r error represented by the difference between radar based estimates and reference e g rain gauge observations and describe the error structure conditioned on rainfall magnitude e g ciach et al 2000 this conditional error is defined using the conditional average of true reference rainfall observations for a given r r value this study was inspired by the recent investigation ciach and gebremichael 2020 hereafter cg2020 that proposed a further refinement of the conditional error this new method is called the second order separation which is in contrast with many prior approaches e g ciach et al 2007 habib et al 2008 seo et al 2018 that accounted for the first order moment i e conditional mean bias only to derive the random error feature in the framework of cg2020 they derived the r r standardized error random component by estimating conditional mean and standard deviation together despite their pioneering effort the work of cg2020 has several limitations and remains preliminary the two main limitations are 1 the analysis was performed at fixed temporal and spatial scales i e hourly and 3 3 km2 which do not correspond to any major widely available rainfall product and 2 the standardized error was characterized by the marginal distribution only and no conditional property was investigated therefore our objective is to extend the analysis framework to the inspection of multi scale in time and space aspects as well as to examine conditional features of the random component the standardized error our investigation is empirical and is based on data collected from two rain gauge networks operated by the iowa flood center ifc krajewski et al 2017 and the u s national oceanic and atmospheric administration noaa our experiment uses two r r products generated separately by the ifc and noaa which process the same data from several wsr 88d radars covering the iowa domain see fig 1 but with different qpe algorithms zhang et al 2016 seo and krajewski 2020 for spatial scale investigation of this study we rely on a single rain gauge to represent areal reference rainfall at a given spatial scale this experiment setup was due to the inherent gauge configuration e g density of the two networks and may lead to poor representation of point area approximation e g kitchen and blackall 1992 morrissey et al 1995 ciach and krajewski 1999 habib et al 2004 at the larger end of spatial scale range used in this study this paper is structured as follows in section 2 we briefly discuss the data sources used for our multi scale analysis including the r r products and rain gauge observations section 3 describes the theoretical background of our analysis framework and the setup for our multi scale experiment in section 4 we present our analysis results on the structure of conditional error components and their scale dependent property in section 5 we discuss the main findings and limitations of this study with prospective research work 2 experimental domain and data we determined the domain of this study to enable an investigation of differences in the structure of conditional error derived from multiple radar qpe products e g different processing and estimation algorithms the iowa domain shown in fig 1 is qualified for this requirement because the ifc has provided a nexrad based rainfall composite product over the entire domain for more than ten years which allows a comparison with the multi radar multi sensor mrms qpe product zhang et al 2016 for ground reference the ifc also has operated a dense rain gauge network consisting of 66 dual tipping bucket platforms that complements the u s national network of noaa cooperative observer program coop lawrimore et al 2020 gauges in the domain this iowa domain has been used for many recent hydrologic studies e g krajewski et al 2020 quintero et al 2020 seo et al 2020 2021 ghimire et al 2021 because of the presence of high quality hydrologic datasets e g rainfall and streamflow 2 1 r r products the radar based rainfall estimates the ifc has generated hereafter the ifc product have a legacy in the hydro nexrad system krajewski et al 2011 seo et al 2011 krajewski et al 2013 which offered flexible options in the product resolution and processing algorithm selection the ifc s estimation algorithm has evolved over time to improve the accuracy of its estimates in accordance with the enhancements in the nexrad radar s capability i e high resolution and dual polarization upgrades the ifc product is a composite of data from seven nexrad radars that illuminate the iowa domain as shown in fig 1 its current algorithm seo et al 2020 seo and krajewski 2020 uses the specific attenuation method e g ryzhkov et al 2014 wang et al 2019 based on polarimetric observations and environmental variables that help identify the altitude of the melting layer while the ifc product is generated in real time with 500 m and 5 min resolutions we reproduced an hourly basis product for the period of 2016 to 2018 to avoid data missed in real time operation we did not include winter month products in the analysis of this study and limited the analysis period to wet months april to october only the estimation of radar based precipitation for the winter season is challenging and winter precipitation estimates generally contain large uncertainties e g souverijns et al 2017 mrms is also a composite precipitation product of nexrad radars incorporating satellite lightning and numerical weather prediction model analyses zhang et al 2016 mrms has been one of the official u s precipitation products and its qpe product suite includes rainfall rate accumulation and precipitation type with an approximately 1 km resolution in this study we collected an hourly gauge corrected product for the period of 2016 to 2018 and compared its error features with those of the ifc product while mrms s main qpe algorithm also has been updated to the specific attenuation method zhang et al 2020 it has not been applied retrospectively therefore in this study we used mrms rainfall estimates derived using multiple relationships between radar reflectivity z and rain rate r as documented in zhang et al 2016 2 2 rain gauge data as ground reference for r r error characterization we acquired rain gauge data from the ifc and noaa national weather service coop networks the local ifc network consists of 66 rain gauges most of which have been deployed in eastern iowa centered on the iowa city municipal airport and within the turkey river basin near the northeast corner of iowa as shown in fig 1 the network features dual tipping bucket gauges at each site which help detect gauge malfunctioning and reduce the effect of small scale rainfall spatial variability e g ciach 2003 the tip resolution is 0 254 mm and each gauge transfers observations i e the accumulated number of tips every 15 minutes a detailed quality control procedure using observations from the dual tipping bucket gauges is documented in seo et al 2015 we also collected data from the coop hourly precipitation data hpd network version 2 0 from the noaa national centers for environmental information ncei as demonstrated in fig 1 the coop gauges are well distributed over the iowa domain the gauges in the hourly coop network are equipped with the fischer porter f p weighing bucket sensors e g lawrimore et al 2020 which can measure precipitation to 0 254 mm in both networks there were some gauge locations where the precipitation records were discontinuous with many missing hours these locations vary year to year and the quality of data from the locations was suspicious based on our preliminary quality check as part of data quality control we applied a rainfall threshold i e 1 mm h to calculate probability of rainfall detection pod and excluded all locations where pod calculated based on the number of rainfall hours for the wet months was abnormally small 2 this quality assurance procedure was performed on an annual basis because the quality of gauge records may change over time fig 2 illustrates spatial correlation estimated from each network we observe from fig 2 that the correlation structures from different networks look quite similar and that the correlation distance is very close e g 34 3 vs 32 5 km based on a spatial correlation value of 0 4 this provides us with confidence regarding data quality and also allows us to assemble the gauge data from the different networks that cover different spatial areas to evaluate the radar based composite products note we did not use data from another u s national network of hourly rain gauges the hydrometeorological automated data system hads e g kim et al 2009 also shown in fig 1 this is because the mrms product includes a bias correction using the hourly hads observations zhang et al 2016 3 methodology in this section we define the r r error and provide theoretical background and formulas to derive empirical distributions of the r r conditional error quantification of the errors presented in this study is based primarily on the method proposed by cg2020 one difference between this study and cg2020 is the choice of conditioning variable we decompose the r r systematic and random components conditioned on the r r value itself whereas cg2020 investigated those error components for given reference e g gauge rainfall values refer to ciach et al 2007 for more discussion on this but one may argue that conditioning on r r is more pragmatic from the operational point of view it allows asking the question given the value i got from my operational product where should i expect the true reference rainfall to be the conditional characterization of r r errors can be represented by one of the two ways 1 r g h 1 r r e 1 2 r r h 2 r g e 2 where r denotes a rainfall value measured or estimated by gauge g and radar r h indicates an argument function e is a random error component and is represented as a function of corresponding rainfall values rr or rg in these equations and other formulas below we use a simplified notation omitting reference to location and time as integration over those should be clear from the context as we discussed above we apply eq 1 for most of our analyses while cg2020 used eq 2 to explore the r r error structure for a given reference rain rate our approach is practically useful because it enables further applications e g ensemble qpe generation of resulting error features once the r r estimates are given e g villarini et al 2009 kirstetter et al 2015 more discussion on probabilistic and explanatory approaches each of which are represented by eqs 1 and 2 is provided in cg2020 using eq 2 we also compare the error features of the ifc and mrms products with those presented in cg2020 this will demonstrate discrepancies caused by different r r estimation algorithms and experimental setups in this study we employ an additive term of the r r error e g ciach and krajewski 1999 aghakouchak et al 2012 and the general form of additive representation of eq 1 is 3 r g e s y s r r e r a n r r where esys and eran denote systematic distortion and random error functions conditioned on the r r estimates rr respectively the conditional systematic distortion cavg is defined as conditional expectation e for a given r r value 4 c avg r r e r g r r the mean of the random component eran is regarded as zero and many previous studies e g ciach et al 2007 aghakouchak et al 2012 have attempted to separate the two error components considering the first order moment i e the mean only in this study we apply the second order separation approach cg2020 by introducing a conditional standard deviation term cstd to describe the r r error structure 5 r g c a v g r r c s t d r r e s t d r r 6 c std 2 r r e r g c avg r r 2 r r where estd is known as standardized error and can be defined using eq 5 after the conditional standard deviation in eq 6 is estimated 7 e s t d r r r g c a v g r r c s t d r r the standardized error is a key element of the entire characterization procedure and the derivation of its empirical distribution requires the estimation of the two conditional functions cavg and cstd included in eq 7 to estimate the conditional mean and standard deviation functions in eqs 4 and 6 we use a non parametric gaussian kernel regression method e g takeda et al 2007 and parameterize the conditional tendencies using power law and polynomial functions 8 c r r a r r b 9 c r r c 0 c 1 r r c 2 r r 2 c n r r n where c rr represents both of the conditional mean and standard deviation functions and the order of polynomial function n varies from 2 to 4 we apply both eqs 8 and 9 to fit the conditional tendencies and then select the best fitting functions we examine these conditional features regarding different r r estimation algorithms ifc and mrms with various product spatial 0 5 32 km and temporal 1 24 h resolutions 4 results this section describes the empirical estimation of the conditional r r error structure and distribution using r g pairs collected for the three year experimental data period we defined a rainfall threshold and excluded the r g pairs rr 0 5 mm h to avoid numerical issues e g instability and divergence when deriving terms and parameters in eqs 3 to 9 this resulted in a total of more than 80 000 hourly r g samples for both of the ifc and mrms products 4 1 comparison with cg2020 although our main analyses rely on the probabilistic approach formulated in eq 1 we also performed the explanatory analysis based on eq 2 for a quick evaluation and comparison with the results reported in cg2020 this allows us to understand algorithm and geography dependent r r error structure the procedures to obtain corresponding results to cg2020 can be achieved by switching subscripts g and r in eqs 3 8 for a fair comparison we applied the same rainfall threshold rg 2 mm h used in cg2020 when retrieving r g samples from our dataset fig 3 shows the conditional mean and standard deviation functions estimated for the ifc and mrms products we also compared their conditional tendencies with those of cg2020 the circular dots in fig 3 indicate the mean and standard deviation values estimated using the gaussian kernel regression method at each gauge rainfall rg interval we observe that the conditional mean of cg2020 tends to be closer to the one to one line this does not necessarily mean that the r r estimates in cg2020 are more accurate than ifc and mrms because the accuracy of r r should be assessed regarding given r r values the r r estimates in cg2020 were generated using the unique nexrad z r relationship fulton et al 1998 and the locations of the rain gauges used in the study were within a good range from the radar for a quantitative evaluation e g 100 km in table 1 we present and compare the estimated parameters of the power law function in eq 8 using the conditional error structures shown in fig 3 we derived the empirical distributions of standardized error for both products and present their probability density functions pdf in fig 4 the standardized error of cg2020 shown in fig 4 is not an experimental distribution but a parameterized one with the shifted asymmetric laplace model as shown in fig 4 the standardized error features of the ifc and mrms products do not agree with those of cg2020 in terms of two main aspects 1 the modes of ifc and mrms are much closer to zero than that of cg2020 and 2 ifc and mrms distributions are wider with decreased skewness these can be attributed to different estimation algorithms and application areas i e iowa vs oklahoma the reduced random variability for ifc and mrms may expose improvements in the estimation algorithm using polarimetric capabilities 4 2 conditional error characterization we analyzed the error structure of the ifc and mrms products conditioned on their r r values as defined in eq 1 this analysis used hourly r g samples rr 0 5 mm h retrieved at their original spatial resolutions 0 5 and 1 0 km respectively we will examine the effect of different space and time scales on the error features in the next section in fig 5 we provide the conditional mean and standard deviation functions parameterized for the ifc and mrms products the parameterization procedure using the gaussian kernel regression is exactly the same as the one applied in fig 3 but considered the two functions provided in eqs 8 and 9 to acquire the result shown in fig 5 we first estimated parameters of the power law and three polynomial 2nd 4th functions and then selected the best fitting function the polynomial function can complement the power law one particularly when the tendency at very small or large rainfall ranges does not tend to align with an overall pattern defined by the power law fig 5 reveals that the selected polynomial function represents conditional tendencies better than the power law in some cases e g conditional standard deviation with mrms the parameters of the selected functions in fig 5 are presented in tables 2 and 3 together with those estimated for various temporal scales that are explored in this study in fig 6 we provide the empirical distributions of standardized error for the ifc and mrms products by decomposing their conditional mean and standard deviation components described in fig 5 the pdfs shown in fig 6 are not conditional distributions but marginal ones for the entire range of r r values as in cg2020 the two distributions presented in fig 5 look slightly different 1 the mode of mrms is closer to zero and 2 the distribution of ifc is more positively skewed the difference in their spatial resolution i e 0 5 vs 1 0 km did not likely affect the observed differences given this hourly scale analysis in other words the effect of gauge representativeness error e g kitchen and blackall 1992 ciach and krajewski 1999 seo and krajewski 2011 should not be significant at the hourly and spatial scales of the two products we will come back to this point overall the random error feature of mrms seems slightly better than that of ifc we speculate that this is the result of the bias correction included in mrms whereas ifc remains radar only to investigate conditional structure of standardized error a sufficient number of samples should be ensured to construct individual probability distributions at each r r value because ifc contained the greater number of r g samples for the data period we estimated ifc s standardized error distributions conditioned on several rainfall intervals in fig 7 as an example fig 7 a shows gradual changes of the probability distributions as the magnitude of rainfall interval varies the mode gradually approaches zero with decreasing skewness as the r r value increases this tendency implies that random variability in the r r estimation tends to decrease at a greater value of rainfall e g heavy rain with convective storms however we discover an unexpected tendency at a range of standardized error between 2 and 1 with the highest rainfall values we speculate that this observed feature bump in the pdf might be caused by the relatively small number of r g samples and wider interval at that rainfall range as shown in fig 7 b 4 3 multi scale investigation we extended the error characterization analysis into a variety of time and space scales that are frequently employed in many hydrologic and meteorological applications the time and space scales at which the error structure is explored are 1 to 24 1 3 6 12 and 24 h and 0 5 to 32 0 5 1 2 4 8 16 and 32 km respectively to generate a dataset for this multi scale analysis we first performed temporal aggregation using hourly r g samples with the original product spatial resolutions 0 5 and 1 km we avoided time overlapping when generating longer time span data to keep the r g samples independent which significantly reduced the number of samples particularly for longer time resolutions e g 24 h fig 8 shows conditional mean functions parameterized for ifc and mrms with respect to temporal scale in table 2 we present the parameter values of conditional mean functions shown in fig 8 the systematic tendency of conditional mean observed in fig 8 looks consistent with the result reported in seo et al 2018 and longer time aggregation leads to an improved r r estimation with decreased conditional errors for both products as a result of a comparison between the two products mrms shows smaller conditional errors at all temporal scales used in the analysis and its conditional tendency almost disappears at temporal scales longer than 6 h a common aspect detected from both products is that the conditional error decreases significantly when temporal scale increases from 1 to 3 h in other words both hourly r r products seem to contain a substantial magnitude of conditional errors although mrms was corrected for bias using rain gauge observations for conditional standard deviation we do not show the estimation result because no systematic tendency was observed concerning temporal scale instead we provide the estimated parameters in table 3 the distributions of standardized error for each different temporal scale derived using the parametrized conditional mean and standard deviation functions listed in tables 2 and 3 are illustrated in fig 9 as shown in fig 9 the effect of temporal scale on this random error component is prominent especially for the mrms product the mode of distribution approaches zero with decreasing skewness as temporal scale increases we note that a conditional examination of standardized error as shown in fig 7 a was not conducted with this temporal scale analysis due to the reduction in the sample size for longer scale data for spatial aggregation of the r r products we took a simple average of r r rainfall values from the corresponding grids located within a coarser grid we then paired the r r average values with gauge observations to generate r g samples at hourly scale only there is no spatial averaging of rain gauge observations for larger spatial resolutions because it is rare that multiple rain gauges are located within a r r product grid this may cause a gauge representativeness issue e g kitchen and blackall 1992 when comparing gauge vs radar i e point vs area rainfall particularly at larger spatial resolutions we also ignored the related effect of the position of the gauge within a given spatial grid see krajewski et al 2000 gebremichael and krajewski 2004 in fig 10 we provide two descriptive statistics representing r g agreement regarding spatial scale to briefly inspect the effect of the gauge representativeness error 1 multiplicative bias is defined as a ratio r g to reveal a systematic tendency of r r estimates e g over or under estimation and 2 mean absolute error mae is an arithmetic average of the absolute error r g implying average magnitude of errors regardless of their direction from the two metrics in fig 10 we observe that the errors of ifc and mrms tend to sharply increase from 16 and 8 km scales respectively the increased errors observed at larger scales were likely contributed by the gauge representativeness error however it is hard to quantify its contribution without data from a dense rain gauge network e g seo and krajewski 2011 because the r g difference e g mae in fig 10 consists of each individual radar and gauge errors if their covariance is negligible ciach and krajewski 1999 therefore the conditional error analysis provided here regarding spatial scale might be inaccurate at scales where the effect of gauge representativeness error is considerable e g 8 32 km fig 11 shows the structure of conditional mean of ifc and mrms for different spatial scales based on the comparison with fig 8 the r r conditional mean seems to be more sensitive to a product temporal resolution than spatial one there is little recognizable variation within 0 5 4 km scales at which the gauge representativeness error could be negligible in fig 11 we discover that the conditional means of ifc at 16 32 km and mrms at 8 32 km scales look separated from a group of those at smaller scales this tendency is somewhat consistent with what we observed from fig 10 because of this unknown effect of gauge representativeness we do not provide the estimated parameters of the conditional mean and standard deviation regarding spatial scale we also derived the distributions of the standardized error regarding presented spatial scale as illustrated in fig 12 the error distributions do not reveal systematic behavior possibly due to the error contribution from the gauge representativeness and the distributions at 0 5 4 km scales appear to be almost scale invariant we note that an investigation of the r r conditional error at large spatial scales requires a dense rain gauge network that includes a sufficient number of gauges within r r product grids e g 32 km to minimize the gauge representativeness error and ensure data samples for analysis 5 conclusions and discussion this study examines several interesting aspects associated with the characterization of r r conditional error for two products used operationally in hydrologic forecasting to explore the effect of various temporal and spatial scales on this error we quantified the conditional error structure of r r estimates with respect to multiple scales by applying the second order separation method proposed by cg2020 our analyses presented in this study are based on the additive term of r r error e g ciach and krajewski 1999 and the key element of this conditional approach is to derive an empirical distribution of r r standardized error defined in eq 7 compared to cg2020 our study has a number of novel aspects 1 investigation of the scale dependent time 1 24 h and space 0 5 32 km conditional error structure 2 use of multiple r r products ifc and mrms to explore algorithm dependent error features 3 use of multiple parameterization power law and polynomial functions to better capture the conditional structures and 4 inspection of the conditional structure of standardized error distributions however we note that our analysis results presented in this study might be specific to the study region and period the demonstrated conditional tendencies describe an average behavior over the specific time and space domains that integrate radar sampling conditions and precipitation features it is obvious that the standardized error distributions shown in fig 4 depend on r r processing and estimation algorithms for a fair comparison in fig 4 we followed cg2020 s explanatory approach i e conditioning on reference gauge values and derived the empirical distributions of standardized error for the ifc and mrms products while cg2020 parameterized the error distribution using a three parameter modified laplace model the distributions derived from ifc and mrms demonstrate some improvement e g reduced skewness in the random error structure against the one in cg2020 this improvement may imply a benefit from nexrad s polarimetric capability 1 the basis of the ifc s core qpe algorithm is the specific attenuation method seo et al 2020 seo and krajewski 2020 and 2 the mrms qpe algorithm uses multiple rainfall estimators zhang et al 2016 applied according to the result of precipitation classification that utilizes polarimetric observations although the distributions of ifc and mrms are quite close to each other the slight difference between them seems to arise from a gauge correction included in the mrms estimation procedure the systematic behavior of the r r standardized error regarding temporal scale shown in fig 8 depended on how well the conditional mean and standard deviation were captured this was the reason why we used four different i e power law and 2nd 4th order polynomial functions to describe the conditional property of the r r error in some cases it was hard to capture the conditional tendency using the power law function only as discussed in section 4 2 and inadequate fitting may lead to the corruption of systematic tendency observed in figs 8 and 9 the analysis result regarding spatial scale presented in fig 12 does not show a systematic tendency because we were not able to separate the effect of rain gauge representativeness error particularly for larger spatial scales e g 16 and 32 km rigorous examination at these large scales requires a high density network of rain gauges to cover the r r product spatial resolution as well as to minimize the effect of the representativeness error it is clear that the behavior of r r standardized error varies with the magnitude of rainfall as demonstrated in fig 7 a the result presented in fig 7 a does not provide a full description of the error feature because a range of rainfall was limited up to 20 mm h due to the shortage of data samples although we used a large number of rain gauges see fig 1 for the three year 2016 2018 period the conditional structure of standardized error should be further refined by expanding the data period for practical applications in which there is no limit on the magnitude of rainfall in the future we will also characterize or parameterize a group of distributions such as the ones shown in fig 7 a using an appropriate probability distribution our r r error characterization based on the probabilistic approach conditioning on the r r values enables many useful applications of r r estimates the error structure identified in this study can be used to generate a precipitation ensemble for operational streamflow prediction once the r r product is ready in real time the most challenging factor in the ensemble generation would be to incorporate the space time correlation structure of the r r error e g germann et al 2009 villarini et al 2014 into the ensemble procedure we think that this conditional characterization could partially describe the space time correlated error structure because it is apparent that the rainfall magnitude itself on which the error is conditioned is also correlated in space and time another possible avenue to apply the result of this study is frequency analysis or estimation e g eldardiry et al 2015 for a given rainfall duration e g 24 h this approach would be particularly beneficial if the structure of r r error is well defined for the regions where rain gauges are scarce and the frequency estimation is provided based on the interpolation of data from sparse networks of gauges e g noaa atlas 14 merkel et al 2017 credit authorship contribution statement bong chul seo conceptualization methodology software data curation investigation writing original draft witold f krajewski conceptualization methodology funding acquisition writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this study was supported by the iowa flood center at the university of iowa w f krajewski also acknowledges partial support by the rose and joseph summers endowment 
247,this study examines multi scale aspects of radar rainfall r r conditional error defined as the difference between a given r r value and the conditional average of corresponding reference observations to decompose the systematic and random error components the authors adopted the second order separation method while previous studies relied on addressing the first order moment i e conditional bias only the authors applied a non parametric kernel regression approach to characterize the conditional mean and standard deviation and thus to derive a distribution of random component standardized error this empirical study is based on data from two rain gauge networks consisting of 66 and 115 gauges and two r r estimates the iowa flood center and multi radar multi sensor products over the iowa domain in the united states the authors explored the effect of multiple temporal 1 24 h and spatial 0 5 32 km scales on the conditional error structure they found that the error distribution gradually approaches the gaussian distribution with longer temporal scale while the error feature regarding spatial scale appears to be almost scale invariant keywords conditional error rainfall radar qpe scale 1 introduction precipitation is a main driver of hydrologic cycles and its space time variability at relevant hydrologic scales is a crucial factor to model or predict hydrologic responses and manage potential risks e g woods and sivapalan 1999 paschalis et al 2014 in the united states weather radar has been used as a primary instrument to monitor the quantitative and dynamic features of precipitation fields since the emergence of the weather surveillance radar 1988 doppler wsr 88d network known as nexrad in the early 1990s crum and alberty 1993 radar based quantitative precipitation estimation qpe has been extensively applied for operational hydrologic applications e g fulton et al 1998 krajewski et al 2017 in recent decades however many challenging issues associated with qpe uncertainties still remain e g see the discussions in krajewski and smith 2002 villarini and krajewski 2010a numerous studies have documented a variety of aspects and perspectives regarding quantification and modeling of radar rainfall r r uncertainties e g ciach et al 2007 aghakouchak et al 2010 mandapaka and germann 2010 villarini et al 2014 kirstetter et al 2015 neuper and ehret 2019 these approaches typically depart from a choice of the r r error definition e g multiplicative or additive and characterize the subsequent statistical structure of errors e g villarini and krajewski 2010b a typical application of the characterized error structure is to represent the r r uncertainty in the form of ensembles for hydrologic prediction e g mandapaka and germann 2010 kirstetter et al 2015 in this study we employ the additive term of r r error represented by the difference between radar based estimates and reference e g rain gauge observations and describe the error structure conditioned on rainfall magnitude e g ciach et al 2000 this conditional error is defined using the conditional average of true reference rainfall observations for a given r r value this study was inspired by the recent investigation ciach and gebremichael 2020 hereafter cg2020 that proposed a further refinement of the conditional error this new method is called the second order separation which is in contrast with many prior approaches e g ciach et al 2007 habib et al 2008 seo et al 2018 that accounted for the first order moment i e conditional mean bias only to derive the random error feature in the framework of cg2020 they derived the r r standardized error random component by estimating conditional mean and standard deviation together despite their pioneering effort the work of cg2020 has several limitations and remains preliminary the two main limitations are 1 the analysis was performed at fixed temporal and spatial scales i e hourly and 3 3 km2 which do not correspond to any major widely available rainfall product and 2 the standardized error was characterized by the marginal distribution only and no conditional property was investigated therefore our objective is to extend the analysis framework to the inspection of multi scale in time and space aspects as well as to examine conditional features of the random component the standardized error our investigation is empirical and is based on data collected from two rain gauge networks operated by the iowa flood center ifc krajewski et al 2017 and the u s national oceanic and atmospheric administration noaa our experiment uses two r r products generated separately by the ifc and noaa which process the same data from several wsr 88d radars covering the iowa domain see fig 1 but with different qpe algorithms zhang et al 2016 seo and krajewski 2020 for spatial scale investigation of this study we rely on a single rain gauge to represent areal reference rainfall at a given spatial scale this experiment setup was due to the inherent gauge configuration e g density of the two networks and may lead to poor representation of point area approximation e g kitchen and blackall 1992 morrissey et al 1995 ciach and krajewski 1999 habib et al 2004 at the larger end of spatial scale range used in this study this paper is structured as follows in section 2 we briefly discuss the data sources used for our multi scale analysis including the r r products and rain gauge observations section 3 describes the theoretical background of our analysis framework and the setup for our multi scale experiment in section 4 we present our analysis results on the structure of conditional error components and their scale dependent property in section 5 we discuss the main findings and limitations of this study with prospective research work 2 experimental domain and data we determined the domain of this study to enable an investigation of differences in the structure of conditional error derived from multiple radar qpe products e g different processing and estimation algorithms the iowa domain shown in fig 1 is qualified for this requirement because the ifc has provided a nexrad based rainfall composite product over the entire domain for more than ten years which allows a comparison with the multi radar multi sensor mrms qpe product zhang et al 2016 for ground reference the ifc also has operated a dense rain gauge network consisting of 66 dual tipping bucket platforms that complements the u s national network of noaa cooperative observer program coop lawrimore et al 2020 gauges in the domain this iowa domain has been used for many recent hydrologic studies e g krajewski et al 2020 quintero et al 2020 seo et al 2020 2021 ghimire et al 2021 because of the presence of high quality hydrologic datasets e g rainfall and streamflow 2 1 r r products the radar based rainfall estimates the ifc has generated hereafter the ifc product have a legacy in the hydro nexrad system krajewski et al 2011 seo et al 2011 krajewski et al 2013 which offered flexible options in the product resolution and processing algorithm selection the ifc s estimation algorithm has evolved over time to improve the accuracy of its estimates in accordance with the enhancements in the nexrad radar s capability i e high resolution and dual polarization upgrades the ifc product is a composite of data from seven nexrad radars that illuminate the iowa domain as shown in fig 1 its current algorithm seo et al 2020 seo and krajewski 2020 uses the specific attenuation method e g ryzhkov et al 2014 wang et al 2019 based on polarimetric observations and environmental variables that help identify the altitude of the melting layer while the ifc product is generated in real time with 500 m and 5 min resolutions we reproduced an hourly basis product for the period of 2016 to 2018 to avoid data missed in real time operation we did not include winter month products in the analysis of this study and limited the analysis period to wet months april to october only the estimation of radar based precipitation for the winter season is challenging and winter precipitation estimates generally contain large uncertainties e g souverijns et al 2017 mrms is also a composite precipitation product of nexrad radars incorporating satellite lightning and numerical weather prediction model analyses zhang et al 2016 mrms has been one of the official u s precipitation products and its qpe product suite includes rainfall rate accumulation and precipitation type with an approximately 1 km resolution in this study we collected an hourly gauge corrected product for the period of 2016 to 2018 and compared its error features with those of the ifc product while mrms s main qpe algorithm also has been updated to the specific attenuation method zhang et al 2020 it has not been applied retrospectively therefore in this study we used mrms rainfall estimates derived using multiple relationships between radar reflectivity z and rain rate r as documented in zhang et al 2016 2 2 rain gauge data as ground reference for r r error characterization we acquired rain gauge data from the ifc and noaa national weather service coop networks the local ifc network consists of 66 rain gauges most of which have been deployed in eastern iowa centered on the iowa city municipal airport and within the turkey river basin near the northeast corner of iowa as shown in fig 1 the network features dual tipping bucket gauges at each site which help detect gauge malfunctioning and reduce the effect of small scale rainfall spatial variability e g ciach 2003 the tip resolution is 0 254 mm and each gauge transfers observations i e the accumulated number of tips every 15 minutes a detailed quality control procedure using observations from the dual tipping bucket gauges is documented in seo et al 2015 we also collected data from the coop hourly precipitation data hpd network version 2 0 from the noaa national centers for environmental information ncei as demonstrated in fig 1 the coop gauges are well distributed over the iowa domain the gauges in the hourly coop network are equipped with the fischer porter f p weighing bucket sensors e g lawrimore et al 2020 which can measure precipitation to 0 254 mm in both networks there were some gauge locations where the precipitation records were discontinuous with many missing hours these locations vary year to year and the quality of data from the locations was suspicious based on our preliminary quality check as part of data quality control we applied a rainfall threshold i e 1 mm h to calculate probability of rainfall detection pod and excluded all locations where pod calculated based on the number of rainfall hours for the wet months was abnormally small 2 this quality assurance procedure was performed on an annual basis because the quality of gauge records may change over time fig 2 illustrates spatial correlation estimated from each network we observe from fig 2 that the correlation structures from different networks look quite similar and that the correlation distance is very close e g 34 3 vs 32 5 km based on a spatial correlation value of 0 4 this provides us with confidence regarding data quality and also allows us to assemble the gauge data from the different networks that cover different spatial areas to evaluate the radar based composite products note we did not use data from another u s national network of hourly rain gauges the hydrometeorological automated data system hads e g kim et al 2009 also shown in fig 1 this is because the mrms product includes a bias correction using the hourly hads observations zhang et al 2016 3 methodology in this section we define the r r error and provide theoretical background and formulas to derive empirical distributions of the r r conditional error quantification of the errors presented in this study is based primarily on the method proposed by cg2020 one difference between this study and cg2020 is the choice of conditioning variable we decompose the r r systematic and random components conditioned on the r r value itself whereas cg2020 investigated those error components for given reference e g gauge rainfall values refer to ciach et al 2007 for more discussion on this but one may argue that conditioning on r r is more pragmatic from the operational point of view it allows asking the question given the value i got from my operational product where should i expect the true reference rainfall to be the conditional characterization of r r errors can be represented by one of the two ways 1 r g h 1 r r e 1 2 r r h 2 r g e 2 where r denotes a rainfall value measured or estimated by gauge g and radar r h indicates an argument function e is a random error component and is represented as a function of corresponding rainfall values rr or rg in these equations and other formulas below we use a simplified notation omitting reference to location and time as integration over those should be clear from the context as we discussed above we apply eq 1 for most of our analyses while cg2020 used eq 2 to explore the r r error structure for a given reference rain rate our approach is practically useful because it enables further applications e g ensemble qpe generation of resulting error features once the r r estimates are given e g villarini et al 2009 kirstetter et al 2015 more discussion on probabilistic and explanatory approaches each of which are represented by eqs 1 and 2 is provided in cg2020 using eq 2 we also compare the error features of the ifc and mrms products with those presented in cg2020 this will demonstrate discrepancies caused by different r r estimation algorithms and experimental setups in this study we employ an additive term of the r r error e g ciach and krajewski 1999 aghakouchak et al 2012 and the general form of additive representation of eq 1 is 3 r g e s y s r r e r a n r r where esys and eran denote systematic distortion and random error functions conditioned on the r r estimates rr respectively the conditional systematic distortion cavg is defined as conditional expectation e for a given r r value 4 c avg r r e r g r r the mean of the random component eran is regarded as zero and many previous studies e g ciach et al 2007 aghakouchak et al 2012 have attempted to separate the two error components considering the first order moment i e the mean only in this study we apply the second order separation approach cg2020 by introducing a conditional standard deviation term cstd to describe the r r error structure 5 r g c a v g r r c s t d r r e s t d r r 6 c std 2 r r e r g c avg r r 2 r r where estd is known as standardized error and can be defined using eq 5 after the conditional standard deviation in eq 6 is estimated 7 e s t d r r r g c a v g r r c s t d r r the standardized error is a key element of the entire characterization procedure and the derivation of its empirical distribution requires the estimation of the two conditional functions cavg and cstd included in eq 7 to estimate the conditional mean and standard deviation functions in eqs 4 and 6 we use a non parametric gaussian kernel regression method e g takeda et al 2007 and parameterize the conditional tendencies using power law and polynomial functions 8 c r r a r r b 9 c r r c 0 c 1 r r c 2 r r 2 c n r r n where c rr represents both of the conditional mean and standard deviation functions and the order of polynomial function n varies from 2 to 4 we apply both eqs 8 and 9 to fit the conditional tendencies and then select the best fitting functions we examine these conditional features regarding different r r estimation algorithms ifc and mrms with various product spatial 0 5 32 km and temporal 1 24 h resolutions 4 results this section describes the empirical estimation of the conditional r r error structure and distribution using r g pairs collected for the three year experimental data period we defined a rainfall threshold and excluded the r g pairs rr 0 5 mm h to avoid numerical issues e g instability and divergence when deriving terms and parameters in eqs 3 to 9 this resulted in a total of more than 80 000 hourly r g samples for both of the ifc and mrms products 4 1 comparison with cg2020 although our main analyses rely on the probabilistic approach formulated in eq 1 we also performed the explanatory analysis based on eq 2 for a quick evaluation and comparison with the results reported in cg2020 this allows us to understand algorithm and geography dependent r r error structure the procedures to obtain corresponding results to cg2020 can be achieved by switching subscripts g and r in eqs 3 8 for a fair comparison we applied the same rainfall threshold rg 2 mm h used in cg2020 when retrieving r g samples from our dataset fig 3 shows the conditional mean and standard deviation functions estimated for the ifc and mrms products we also compared their conditional tendencies with those of cg2020 the circular dots in fig 3 indicate the mean and standard deviation values estimated using the gaussian kernel regression method at each gauge rainfall rg interval we observe that the conditional mean of cg2020 tends to be closer to the one to one line this does not necessarily mean that the r r estimates in cg2020 are more accurate than ifc and mrms because the accuracy of r r should be assessed regarding given r r values the r r estimates in cg2020 were generated using the unique nexrad z r relationship fulton et al 1998 and the locations of the rain gauges used in the study were within a good range from the radar for a quantitative evaluation e g 100 km in table 1 we present and compare the estimated parameters of the power law function in eq 8 using the conditional error structures shown in fig 3 we derived the empirical distributions of standardized error for both products and present their probability density functions pdf in fig 4 the standardized error of cg2020 shown in fig 4 is not an experimental distribution but a parameterized one with the shifted asymmetric laplace model as shown in fig 4 the standardized error features of the ifc and mrms products do not agree with those of cg2020 in terms of two main aspects 1 the modes of ifc and mrms are much closer to zero than that of cg2020 and 2 ifc and mrms distributions are wider with decreased skewness these can be attributed to different estimation algorithms and application areas i e iowa vs oklahoma the reduced random variability for ifc and mrms may expose improvements in the estimation algorithm using polarimetric capabilities 4 2 conditional error characterization we analyzed the error structure of the ifc and mrms products conditioned on their r r values as defined in eq 1 this analysis used hourly r g samples rr 0 5 mm h retrieved at their original spatial resolutions 0 5 and 1 0 km respectively we will examine the effect of different space and time scales on the error features in the next section in fig 5 we provide the conditional mean and standard deviation functions parameterized for the ifc and mrms products the parameterization procedure using the gaussian kernel regression is exactly the same as the one applied in fig 3 but considered the two functions provided in eqs 8 and 9 to acquire the result shown in fig 5 we first estimated parameters of the power law and three polynomial 2nd 4th functions and then selected the best fitting function the polynomial function can complement the power law one particularly when the tendency at very small or large rainfall ranges does not tend to align with an overall pattern defined by the power law fig 5 reveals that the selected polynomial function represents conditional tendencies better than the power law in some cases e g conditional standard deviation with mrms the parameters of the selected functions in fig 5 are presented in tables 2 and 3 together with those estimated for various temporal scales that are explored in this study in fig 6 we provide the empirical distributions of standardized error for the ifc and mrms products by decomposing their conditional mean and standard deviation components described in fig 5 the pdfs shown in fig 6 are not conditional distributions but marginal ones for the entire range of r r values as in cg2020 the two distributions presented in fig 5 look slightly different 1 the mode of mrms is closer to zero and 2 the distribution of ifc is more positively skewed the difference in their spatial resolution i e 0 5 vs 1 0 km did not likely affect the observed differences given this hourly scale analysis in other words the effect of gauge representativeness error e g kitchen and blackall 1992 ciach and krajewski 1999 seo and krajewski 2011 should not be significant at the hourly and spatial scales of the two products we will come back to this point overall the random error feature of mrms seems slightly better than that of ifc we speculate that this is the result of the bias correction included in mrms whereas ifc remains radar only to investigate conditional structure of standardized error a sufficient number of samples should be ensured to construct individual probability distributions at each r r value because ifc contained the greater number of r g samples for the data period we estimated ifc s standardized error distributions conditioned on several rainfall intervals in fig 7 as an example fig 7 a shows gradual changes of the probability distributions as the magnitude of rainfall interval varies the mode gradually approaches zero with decreasing skewness as the r r value increases this tendency implies that random variability in the r r estimation tends to decrease at a greater value of rainfall e g heavy rain with convective storms however we discover an unexpected tendency at a range of standardized error between 2 and 1 with the highest rainfall values we speculate that this observed feature bump in the pdf might be caused by the relatively small number of r g samples and wider interval at that rainfall range as shown in fig 7 b 4 3 multi scale investigation we extended the error characterization analysis into a variety of time and space scales that are frequently employed in many hydrologic and meteorological applications the time and space scales at which the error structure is explored are 1 to 24 1 3 6 12 and 24 h and 0 5 to 32 0 5 1 2 4 8 16 and 32 km respectively to generate a dataset for this multi scale analysis we first performed temporal aggregation using hourly r g samples with the original product spatial resolutions 0 5 and 1 km we avoided time overlapping when generating longer time span data to keep the r g samples independent which significantly reduced the number of samples particularly for longer time resolutions e g 24 h fig 8 shows conditional mean functions parameterized for ifc and mrms with respect to temporal scale in table 2 we present the parameter values of conditional mean functions shown in fig 8 the systematic tendency of conditional mean observed in fig 8 looks consistent with the result reported in seo et al 2018 and longer time aggregation leads to an improved r r estimation with decreased conditional errors for both products as a result of a comparison between the two products mrms shows smaller conditional errors at all temporal scales used in the analysis and its conditional tendency almost disappears at temporal scales longer than 6 h a common aspect detected from both products is that the conditional error decreases significantly when temporal scale increases from 1 to 3 h in other words both hourly r r products seem to contain a substantial magnitude of conditional errors although mrms was corrected for bias using rain gauge observations for conditional standard deviation we do not show the estimation result because no systematic tendency was observed concerning temporal scale instead we provide the estimated parameters in table 3 the distributions of standardized error for each different temporal scale derived using the parametrized conditional mean and standard deviation functions listed in tables 2 and 3 are illustrated in fig 9 as shown in fig 9 the effect of temporal scale on this random error component is prominent especially for the mrms product the mode of distribution approaches zero with decreasing skewness as temporal scale increases we note that a conditional examination of standardized error as shown in fig 7 a was not conducted with this temporal scale analysis due to the reduction in the sample size for longer scale data for spatial aggregation of the r r products we took a simple average of r r rainfall values from the corresponding grids located within a coarser grid we then paired the r r average values with gauge observations to generate r g samples at hourly scale only there is no spatial averaging of rain gauge observations for larger spatial resolutions because it is rare that multiple rain gauges are located within a r r product grid this may cause a gauge representativeness issue e g kitchen and blackall 1992 when comparing gauge vs radar i e point vs area rainfall particularly at larger spatial resolutions we also ignored the related effect of the position of the gauge within a given spatial grid see krajewski et al 2000 gebremichael and krajewski 2004 in fig 10 we provide two descriptive statistics representing r g agreement regarding spatial scale to briefly inspect the effect of the gauge representativeness error 1 multiplicative bias is defined as a ratio r g to reveal a systematic tendency of r r estimates e g over or under estimation and 2 mean absolute error mae is an arithmetic average of the absolute error r g implying average magnitude of errors regardless of their direction from the two metrics in fig 10 we observe that the errors of ifc and mrms tend to sharply increase from 16 and 8 km scales respectively the increased errors observed at larger scales were likely contributed by the gauge representativeness error however it is hard to quantify its contribution without data from a dense rain gauge network e g seo and krajewski 2011 because the r g difference e g mae in fig 10 consists of each individual radar and gauge errors if their covariance is negligible ciach and krajewski 1999 therefore the conditional error analysis provided here regarding spatial scale might be inaccurate at scales where the effect of gauge representativeness error is considerable e g 8 32 km fig 11 shows the structure of conditional mean of ifc and mrms for different spatial scales based on the comparison with fig 8 the r r conditional mean seems to be more sensitive to a product temporal resolution than spatial one there is little recognizable variation within 0 5 4 km scales at which the gauge representativeness error could be negligible in fig 11 we discover that the conditional means of ifc at 16 32 km and mrms at 8 32 km scales look separated from a group of those at smaller scales this tendency is somewhat consistent with what we observed from fig 10 because of this unknown effect of gauge representativeness we do not provide the estimated parameters of the conditional mean and standard deviation regarding spatial scale we also derived the distributions of the standardized error regarding presented spatial scale as illustrated in fig 12 the error distributions do not reveal systematic behavior possibly due to the error contribution from the gauge representativeness and the distributions at 0 5 4 km scales appear to be almost scale invariant we note that an investigation of the r r conditional error at large spatial scales requires a dense rain gauge network that includes a sufficient number of gauges within r r product grids e g 32 km to minimize the gauge representativeness error and ensure data samples for analysis 5 conclusions and discussion this study examines several interesting aspects associated with the characterization of r r conditional error for two products used operationally in hydrologic forecasting to explore the effect of various temporal and spatial scales on this error we quantified the conditional error structure of r r estimates with respect to multiple scales by applying the second order separation method proposed by cg2020 our analyses presented in this study are based on the additive term of r r error e g ciach and krajewski 1999 and the key element of this conditional approach is to derive an empirical distribution of r r standardized error defined in eq 7 compared to cg2020 our study has a number of novel aspects 1 investigation of the scale dependent time 1 24 h and space 0 5 32 km conditional error structure 2 use of multiple r r products ifc and mrms to explore algorithm dependent error features 3 use of multiple parameterization power law and polynomial functions to better capture the conditional structures and 4 inspection of the conditional structure of standardized error distributions however we note that our analysis results presented in this study might be specific to the study region and period the demonstrated conditional tendencies describe an average behavior over the specific time and space domains that integrate radar sampling conditions and precipitation features it is obvious that the standardized error distributions shown in fig 4 depend on r r processing and estimation algorithms for a fair comparison in fig 4 we followed cg2020 s explanatory approach i e conditioning on reference gauge values and derived the empirical distributions of standardized error for the ifc and mrms products while cg2020 parameterized the error distribution using a three parameter modified laplace model the distributions derived from ifc and mrms demonstrate some improvement e g reduced skewness in the random error structure against the one in cg2020 this improvement may imply a benefit from nexrad s polarimetric capability 1 the basis of the ifc s core qpe algorithm is the specific attenuation method seo et al 2020 seo and krajewski 2020 and 2 the mrms qpe algorithm uses multiple rainfall estimators zhang et al 2016 applied according to the result of precipitation classification that utilizes polarimetric observations although the distributions of ifc and mrms are quite close to each other the slight difference between them seems to arise from a gauge correction included in the mrms estimation procedure the systematic behavior of the r r standardized error regarding temporal scale shown in fig 8 depended on how well the conditional mean and standard deviation were captured this was the reason why we used four different i e power law and 2nd 4th order polynomial functions to describe the conditional property of the r r error in some cases it was hard to capture the conditional tendency using the power law function only as discussed in section 4 2 and inadequate fitting may lead to the corruption of systematic tendency observed in figs 8 and 9 the analysis result regarding spatial scale presented in fig 12 does not show a systematic tendency because we were not able to separate the effect of rain gauge representativeness error particularly for larger spatial scales e g 16 and 32 km rigorous examination at these large scales requires a high density network of rain gauges to cover the r r product spatial resolution as well as to minimize the effect of the representativeness error it is clear that the behavior of r r standardized error varies with the magnitude of rainfall as demonstrated in fig 7 a the result presented in fig 7 a does not provide a full description of the error feature because a range of rainfall was limited up to 20 mm h due to the shortage of data samples although we used a large number of rain gauges see fig 1 for the three year 2016 2018 period the conditional structure of standardized error should be further refined by expanding the data period for practical applications in which there is no limit on the magnitude of rainfall in the future we will also characterize or parameterize a group of distributions such as the ones shown in fig 7 a using an appropriate probability distribution our r r error characterization based on the probabilistic approach conditioning on the r r values enables many useful applications of r r estimates the error structure identified in this study can be used to generate a precipitation ensemble for operational streamflow prediction once the r r product is ready in real time the most challenging factor in the ensemble generation would be to incorporate the space time correlation structure of the r r error e g germann et al 2009 villarini et al 2014 into the ensemble procedure we think that this conditional characterization could partially describe the space time correlated error structure because it is apparent that the rainfall magnitude itself on which the error is conditioned is also correlated in space and time another possible avenue to apply the result of this study is frequency analysis or estimation e g eldardiry et al 2015 for a given rainfall duration e g 24 h this approach would be particularly beneficial if the structure of r r error is well defined for the regions where rain gauges are scarce and the frequency estimation is provided based on the interpolation of data from sparse networks of gauges e g noaa atlas 14 merkel et al 2017 credit authorship contribution statement bong chul seo conceptualization methodology software data curation investigation writing original draft witold f krajewski conceptualization methodology funding acquisition writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this study was supported by the iowa flood center at the university of iowa w f krajewski also acknowledges partial support by the rose and joseph summers endowment 
248,fracture surface topography exhibits long range spatial correlations resulting in a heterogeneous aperture field this leads to the formation within fracture planes of preferential flow channels controlling flow and transport processes by means of a 3 d heat transport model coupled with a 2 d fracture flow model based on the lubrification approximation i e local cubic law we investigate how the statistical parameters determining spatial aperture variations in individual fractures control the heat exchange at the fluid rock interface and heat transport by flow ensemble statistics over fracture realizations provide insights into the main hydraulic and geometrical parameters controlling the hydraulic and thermal behaviour of rough fractures similarly to the rough fracture s hydraulic behaviour we find that its heat transport behaviour deviates from the conventional parallel plate fracture model with increasing fracture closure and or decreasing correlation length we demonstrate that the advancement of the thermal front is typically slower in rough fractures compared to smooth fractures having the same mechanical aperture in contrast with previous studies that neglect temporal and spatial temperature variations in the rock matrix we find that the thermal behavior of a rough walled fracture can under field relevant conditions be predicted from a parallel plate model with an aperture equal to the rough fracture s effective hydraulic aperture this greatly simplifies the prediction of possible reservoir thermal behavior when using field measurable quantities and hydrological modeling keywords fracture roughness heat exchange flow channeling 1 introduction heat transport in fractured media is often considered in hydrogeological studies for instance when inferring hydraulic parameters by fitting heat transfer equations to thermal data heat carried by groundwater serves as a tracer that can be used to quantify flow through fractures ge 1998 read et al 2013 to characterize fracture network connectivity silliman and robinson 1989 klepikova et al 2011 2014 and to constrain regional scale flow patterns anderson 2005 saar 2010 understanding heat transport in fractured media is a prerequisite for studying hydrothermal flows fairley 2009 malkovsky and magri 2016 moreover characterizing heat transport processes in the subsurface is essential for numerous industrial applications for instance heat transfer is critical when assessing heat storage in the ground de la bernardie et al 2019 lanahan and tabares velasco 2017 and near field thermal effects in the context of radioactive waste disposal zhang et al 2017 knowledge of thermal transport is also necessary to maximize the efficiency and sustainability of geothermal systems kolditz and clauser 1998 martinez et al 2014 shortall et al 2015 guo et al 2016 vik et al 2018 patterson and driesner 2020 heat transport in fractured media has predominantly been addressed using simplified conceptual fracture models for example most fracture network models used for geothermal investigations assume a 1 d linear flow geometry pruess and doughty 2010 or consider fractures as parallel plate systems with a constant aperture gringarten et al 1975 kolditz and clauser 1998 kocabas 2005 jung and pruess 2012 zhou et al 2017 vik et al 2018 hydrothermal studies generally represent faults as tabular bodies of internally homogeneous properties or as 2 d discontinuities that juxtapose hydrogeologic units of differing properties e g malkovsky and magri 2016 while it is common practice to neglect fracture heterogeneity the consequences of such simplifications in terms of predictability remain poorly understood klepikova et al 2016 de la bernardie et al 2019 the fracturing process itself as well as post fracturing processes such as geological stress and strain chemical dissolution precipitation and erosion may result in complex fracture wall surface geometries at the scale of a single fracture fracture wall roughness exhibits long range spatial correlations that induce a heterogeneous aperture field brown 1987 johns et al 1993 candela et al 2012 thus promoting strongly heterogeneous flow path distributions and preferential flow channels within fracture planes e g tsang and tsang 1987 méheust and schmittbuhl 2000 2001 numerous studies have shown that fracture roughness has a remarkably strong impact on fluid flow and as a consequence on solute and particle transport through single fractures studies of flow and solute transport in rough walled fractures include both theoretical and numerical studies e g boutt et al 2006 cardenas et al 2009 ge 1997 méheust and schmittbuhl 2001 2003 wang and cardenas 2014 2017 yang et al 2019 yoon and kang 2021 as well as laboratory experiments e g plouraboué et al 2000 méheust and schmittbuhl 2000 detwiler et al 2000 boschan et al 2007 2008 ishibashi et al 2015 more recently flow channeling in fractured media has been recognized as a critical control on heat transport as well e g geiger and emmanuel 2010 luo et al 2016 based on numerical simulations of flow and heat transport neuville et al 2010b found that the heat exchange between the rock and the fluid is either enhanced or decreased in rough fractures compared to smooth fractures with equivalent mechanical apertures depending on the fracture s morphology and aspect ratio they concluded that because of the presence of larger flow velocities leading to reduced transit times in the channeled areas the heat exchange is generally less efficient in fractures with variable apertures compared to smooth fractures the so called parallel plate with the same hydraulic aperture the authors applied their modeling approach to the geothermal reservoir of soultz sous forêts france leading to predictions of decreased thermal exchanges in rough fractures compared to smooth ones having identical hydraulic transmissivities neuville et al 2010a in their modelling studies neuville et al 2010b 2010a 2011 neglected temporal and spatial temperature variations within the rock matrix neuville et al 2013 moved beyond the assumption of constant matrix temperatures and demonstrated that the hydrothermal behavior within a fracture is heavily influenced by fracture matrix heat exchange processes in their study the navier stokes equations and advection diffusion equations were solved in a simple 3 d model of a fracture consisting of flat parallel walls perturbed by a single sharp asperity neuville et al 2013 more recently using a 3 d numerical model in which the flow in the fracture is described by the reynolds equation fox et al 2015 offered practical understanding of the effects that fracture aperture variations have on heat production in geothermal reservoirs notably fox et al 2015 demonstrated that the thermal exchanges between the fluid flowing within a rough walled fracture and the surrounding matrix are reduced in comparison to planar surfaces because flow channeling reduces the contact area over which heat conductive transfer takes place they conclude that as a consequence fracture aperture variations generally have a negative impact on the thermal performance of geothermal reservoirs more recently from theoretical and numerical analyses of heat transfer in geometrically simple 3 d conceptual models of fractures klepikova et al 2016 demonstrated that flow channeling locally enhances heat diffusion rates because a channel cylindrical conduit is more efficient in exchanging heat between a fracture and the matrix than a planar fracture of equivalent surface regardless of the different underlying assumptions these results reveal that two opposing effects related to flow channeling impact fracture matrix heat exchange at the fracture scale on the one hand heat transfer is locally enhanced by increasing the dimensionality of the diffusive flux klepikova et al 2016 on the other hand heat transfer is reduced by decreasing the effective contact area fox et al 2015 guo et al 2016 consequently it is necessary to jointly quantify the underlying mechanisms using high fidelity physics and modelling in order to understand which of those opposing effects is dominant and under which conditions the results of recent field experiments also call for the development of numerical modelling that considers flow channeling within individual fractures heat tracer experiments have been conducted recently for example at the experimental site of ploemeur france h observatory network read et al 2013 klepikova et al 2016 de la bernardie et al 2019 in a field site in altona ny hawkins et al 2017 and at the grimsel test site gts switzerland doetsch et al 2018 these in situ experiments have demonstrated that due to the signature of fracture heterogeneity on heat transfer processes predictions based on the classical parallel plate fracture model differ significantly from field observations in terms of the first arrival time the maximum amplitude and the tailing of the thermal breakthrough we present a numerical study in which the fracture is described in two dimensions 2 d and the impermeable rock matrix in three dimensions 3 d the flow in the fracture is described by the reynolds equation that is assuming that the lubrication approximation and hence the local cubic law is valid while heat transport is described by the advection diffusion equation in 2 d in the fracture plane and in 3 d in the matrix this formulation allows us to investigate heat transport along the fracture plane and in the matrix as well as heat exchange between them while allowing for much faster numerical simulations than with a 3 d discretization of the fracture we simulate 20 different rough topographies with a hurst exponent ζ 0 8 d m with aperture closures γ d m varying from 0 001 to 0 6 over a wide range of mean fracture apertures d m and with four different values of the mismatch scale l l c 1 2 4 16 more precisely we investigate how these properties impact heat exchange at the fluid rock interface and heat transport along a fracture in terms of the mean behavior among 20 fractures having such a geometry we address how flow heterogeneity within the fracture affects the macroscopic properties i e the hydraulic transmissivity the velocity of the thermal front and its width ultimately governing the efficiency of the fluid mass and heat transport through the fracture compared to previous studies considering simplified fracture geometry gringarten et al 1975 kocabas 2005 pruess and doughty 2010 jung and pruess 2012 neuville et al 2013 klepikova et al 2016 zhou et al 2017 and or a simplified heat transfer model neuville et al 2010b 2010a 2011 the developed numerical model of flow and heat transport considers simultaneously heat conduction in the matrix and in the fracture as well as heat advection coupled to flow channeling in the fracture transient alteration of the fracture s geometry due to thermo hydro mechanical chemical thmc coupling tsang 1991 taron and elsworth 2009 pandey et al 2014 guo et al 2016 salimzadeh and nick 2019 patterson and driesner 2020 are not considered as such effects are mainly relevant for very sharp temperature contrasts and typically act over time scales of months or years compared to recent works investigating heat transfer within rock samples luo et al 2017 chen and zhao 2020 our modelling results allow evaluating the impact of the statistical geometrical properties of a single geological fracture on heat transfer and to determine the key controlling parameters ultimately this work aims at providing improved parameterizations and guidance for effective low dimensional fracture models the presented results also advance our understanding of how fracture heterogeneity control the efficiency of diffusive exchange processes at the fracture scale this paper is organized as follows section 2 describes the physical conceptualization and the implemented numerical approach the numerical results are presented in section 3 where we first describe the results for a given aperture field realizations and then present the general trends that are observed when considering large sets of synthetic fracture fields the relevance of our models to practical configurations and applications is discussed in section 4 finally section 5 concludes with a summary of the most important findings and outlines possible future developments 2 methods to investigate the sensitivity of the hydrothermal behavior of a rough walled fracture embedded in a homogeneous rock matrix to statistical fracture aperture properties we develop a numerical model of flow and heat transport we first present the self affine aperture fracture model then we develop a numerical model of flow and heat transport using the finite element based software comsol multiphysics comsol 2018 as detailed in the following 2 1 roughness of fracture aperture we consider fractures whose projection on the mean fracture plane is square and of lateral length l experimental studies carried out on cores from natural joints brown et al 1986 have shown that the two fracture walls are self affine but are matched that is they display identical topography fluctuations at scales larger than a critical length scale l c l this scale also denoted mismatch scale is the upper limit to the self affinity of the aperture field it is the only characteristic scale smaller than l available to describe the aperture geometry and is a property related to the regimes of faulting e g strike slip normal and the history of the fracture erosion dissolution precipitation processes which is independent of the fracture size l de dreuzy et al 2012 using an algorithm adapted from méheust and schmittbuhl 2003 we generate fracture aperture fields d fr x y with periodic boundary conditions that are self affine up to l c and have a mechanical aperture d m i e the distance between the mean planes of the fracture walls which are parallel to each other if the walls are nowhere in contact then the mechanical aperture is the average value of d fr x y and standard deviation γ of the aperture field over the entire fracture in our model the hurst or roughness exponent has been chosen constant at ζ 0 8 a value observed for many natural and artificial fracture surfaces schmittbuhl et al 1993 bouchaud 1997 renard et al 2013 each fracture is characterized by the fracture closure γ d m which expresses the vertical extent of roughness relative to the fracture wall separation to keep a simple boundary geometry of the domain we prevent contact between fracture surfaces by only considering relatively small fracture closures consequently the mechanical aperture is also the mean aperture due to stochastic variations the mean aperture of the realizations deviates slightly from the value specified when generating the field here and below d m refers to the actual mechanical apertures of each fracture realization and the size of the fracture is fixed to l 10 m the grid size is 1024 1024 using different seeds of the random generator of the white noise used in the algorithm it is possible to generate multiple independent self affine aperture realizations with the same underlying statistical parameters an example of a fracture aperture profile is shown in fig 1 b 2 2 hydrothermal modelling 2 2 1 hydraulic flow flow within a fracture is modeled as a steady state flow where viscous forces dominate inertial effects that is at a low reynolds number furthermore we apply the lubrication approximation according to which fracture walls have small local slopes zimmerman and yeo 2000 under these assumptions out of plane i e along z components of fluid flow become negligible and the velocity field is dominated by in plane components consequently the hydraulic flow through the fracture q defined as the integral over the local fracture aperture of the fluid velocity u can be related to local apertures through a local cubic law similar to the law used for smooth parallel plate fractures méheust and schmittbuhl 2001 zimmerman and yeo 2000 1 q d fr 3 x y 12 η p where p is the local pressure pa and η is the fluid s dynamic viscosity pa s p and q only depend on the two spatial coordinates that define the fracture s mean plane which will be simply denoted fracture plane in the following for quasi parallel flows the reynolds number is generally defined as r e u charact ρ f l z 2 η l h méheust and schmittbuhl 2001 neuville et al 2011 where ρ f is the fluid density kg m 3 l z and l h denote estimates of the vertical and horizontal scales of variation of the velocities m u charact is a characteristic velocity which we choose equal to the maximum velocity within a parallel plate fracture geometry of aperture d m that is u m p d m 2 8 η as estimated from the classical cubic law in the particular case of a rough fracture one can consider l z d m and l h l c so r e can be expressed as 2 r e u m ρ f d m 2 η l c for the flow to remain linear in the fracture the reynolds numbers should be low that is r e 1 oron and berkowitz 1998 brush and thomson 2003 lee et al 2015 furthermore we assume the fluid to be incompressible u 0 which implies that q is also conservative q 0 plouraboué et al 2000 inserting the local cubic law eq 1 in this conservation law yields the reynolds equation 3 d fr 3 x y p 0 as boundary conditions we impose the pressure at the inlet x 0 and outlet x l of the fracture resulting in a macroscopic pressure gradient p and consider periodic boundary conditions at y 0 and y l the aperture field of the fracture also has periodic boundary conditions by construction i e apertures at x l are identical to those at x 0 although this condition is artificial it is more appropriate than assigning no flow boundaries which could restrict the flow thus limiting the sensitivity to fracture properties e g odling 1992 oron and berkowitz 1998 méheust and schmittbuhl 2001 the surrounding rock matrix is assumed to be impermeable an example of the hydraulic flow computed inside a fracture s aperture field a profile of which is shown in fig 1b is shown in fig 1a as black arrows the permeability of a rough fracture depends both on the mean aperture and the geometry of the rock walls the hydraulic aperture for a rough fracture is classically defined as the aperture of the parallel plate i e smooth fracture of identical transmissivity it can thus be computed from the measured flux q and imposed pressure gradient δ p along the fracture tsang 1992 as 4 d h 12 η q δ p 1 3 in the field the hydraulic aperture can be assessed based on hydraulic transmissivity measurement obtained for example by pumping tests while mechanical aperture can be assessed from roughness analysis on core material 2 2 2 heat transport the 3 d finite element modeling tool comsol multiphysics allows for straightforward coupling of fluid flow and heat transport we consider conductive and advective heat transport in the fracture and heat conduction in the surrounding rock matrix moreover our modelling approach allows avoiding 3 d discretization of a thin fracture domain which would lead to very large computational times when considering realistic ratios of the size of the matrix along z to the mean fracture aperture fig 1a presents a 3 d sketch of our model with a fracture located within the x y plane that is surrounded by the impermeable rock matrix at z 0 we assume that fluid at a constant pressure and temperature of t inj enters the fracture initially at temperature t rock at the left model boundary x 0 and flows in response to the imposed pressure gradient from left to right the rock temperature at the outer boundaries as well as the temperature at the fracture outlet are t rock the injected fluid temperature is here warmer t inj t rock than the initial rock temperature a scenario typically encountered during hydrological investigations in hydro carbon recovery or in nuclear waste leakage in the following we shall consider the relative temperature deviation from the host rock s initial temperature defined in eq 10 so the results for the injection of a colder fluid t inj t rock a scenario typical of geothermal systems would be exactly identical several studies have found that heat conduction in the matrix in the direction parallel to the fracture has only a minor effect on the temperature distribution e g jung and pruess 2012 we do consider horizontal conductive heat transport in the matrix but assume that the conductive heat flux through the boundaries at x y 0 and at x y l can be neglected and hence we do not extend the rock matrix in these directions beyond the fracture length the thickness of the rock matrix layer d 2 m was chosen sufficiently large not to influence the temperature field within the fracture during the simulation time the boundary conditions for temperature at the lateral boundaries of the fracture are periodic similarly to those for the aperture field and local flux field we neglect natural convection by temperature induced bouyancy effects that is we consider that the fluid s density is independent of its temperature the heat transport in the system can then be described as follows in the impermeable rock matrix 5 ρ r c p r t t q r h q h where the conductive heat flux is given by 6 q r h k r t in the fracture 7 d fr ρ f c p f t t d fr ρ f c p f u t t t q fr h q h where the conductive heat flux is given by 8 q fr h d fr k f t t here t denotes the gradient operator restricted to the fracture s tangential plane ρ the density kg m 3 c p the heat capacity j kgk k the thermal conductivity w mk u the local fluid velocity field m s and the f fr and r subscripts denote the fluid fracture and rock respectively the transport is characterized by the dimensionless thermal péclet number p e which is the ratio between the characteristic times of heat diffusion conduction and advection in the fracture e g ge 1998 gossler et al 2019 9 p e u m d m ρ f c p f k r the numerical model relies on a 2 d discretization of the fracture plane and the 3 d discretization of the matrix domain to capture with sufficient accuracy the relative variations of temperature we imposed a finer mesh size around the fracture 0 02 m while the coarser elements 0 2 m are located near the outer boundaries of the rock matrix we verified that refining the element size by a factor of 2 did not influence the resulting temperature field significantly 0 1 since we ignore thermo hydro mechanical chemical thmc processes in this study see introduction and discussion all physical properties are assumed to be time invariant for all the the computations done in this study the pressure gradient was chosen such that the péclet number is p e 51 such a high péclet number implies that heat advection in the fracture is much faster than heat conduction in the matrix the simulations are done at low reynolds numbers the maximum reynolds number being r e 0 8 for fracture apertures as high as d m 23 mm which is compatible with the lubrication approximation and hence the local cubic law note that while the definition of péclet and reynolds numbers varies between studies the range of velocities and apertures studied herein is similar to those reported in previous works e g neuville et al 2013 2011 here and below thermal parameters are selected to represent granite the host formations of most deep geothermal projects table 1 3 results 3 1 hydraulic behaviour in fig 2 a we present the ratio of hydraulic to mechanical apertures d h d m as a function of the fracture closure γ d m for 20 families of fractures a family of fractures refers here to a set of fractures generated with the same random seed but with different fracture closure γ d m and or values for the mismatch scale l c investigated in section 3 2 5 in fig 2a each curve represents a family of fractures with the same rock walls but their separation d m differs the behavior of a parallel plate model corresponds to the horizontal dashed line d h d m 1 for closure γ d m 0 1 the hydraulic behaviour is close to the parallel plate model for different fracture families fluid flow tends to be channelized and different hydraulic behaviors can be observed for similar values of the fracture closure γ d m in agreement with previous studies e g méheust and schmittbuhl 2001 2003 we find that the hydraulic behavior of rough fractures deviate monotonically from the ideal parallel plate model as fracture closure is increased depending on the geometry as determined by the random seed the deviation from the parallel plate model can be positive which implies that the fracture is more conducive to flow than a parallel plate with identical mean separation flow enhancing behavior méheust and schmittbuhl 2000 if the deviation is negative then the fracture is characterized by flow inhibiting behavior similar to previous studies méheust and schmittbuhl 2001 neuville et al 2010b we see that for most cases 75 of fracture families the effective hydraulic transmissivity at the scale of the fracture is reduced in order to better understand the origin of these differences in hydraulic behaviour we plot in figs 2b d some of the investigated aperture fields d fr x y d m for the highest closure considered and in figs 2e g the corresponding maps of local fluxes 2 d velocities normalized by their mean value q q in fig 2b we see a large channel oriented parallel to the applied pressure gradient from left to right which constitutes a configuration favorable to flow as seen in fig 2e this fracture morphology corresponds to the largest ratio d h d m in fig 2a fig 2c shows the map of the ratio d fr x y d m for one of the fracture families considered in fig 2a this family demonstrates a moderate flow inhibiting behavior family a red markers in this case the fracture aperture field is characterized by a main tortuous channel with smaller flow obstacles fig 2f in fig 2d a barrier is seen across the whole fracture that is perpendicular to the applied pressure gradient as shown in fig 2g this results in a strong flow inhibiting behaviour of the fracture lowest ratio d h d m in fig 2a in general the resulting local fluxes in fractures vary over several orders of magnitude with a ratio of the local flux to the mean local flux q q reaching 12 in some geometrical configurations additional simulations with other mismatch scales l c indicate in agreement with méheust and schmittbuhl 2003 that the mismatch scale also has a critical impact on the flow channeling we find that the mean hydraulic behavior of rough walled fractures generally converges to the parallel plate estimate when the ratio l l c increases these results are discussed later in relation to the resulting thermal behaviour section 3 2 5 3 2 thermal behaviour 3 2 1 thermal front definition fig 3 presents snapshots of the temperature field simulated using a parallel plate fracture model and a rough fracture model here and throughout the paper see table 2 we consider as leading example the self affine fracture from family a shown on fig 2c at closure γ d m 0 46 family a is considered as a representative example since this fracture family demonstrates moderate flow inhibiting behaviour as most natural rock fractures méheust and schmittbuhl 2001 we calculate the non dimensional temperature anomaly as 10 δ t x y z t t x y z t t rock t inj t rock the temperature distribution in the rough walled fracture is highly heterogeneous fig 3b and the temperature evolution over time may differ considerably even for points located close to each other fig 3e initially the thermal anomaly propagates along preferential large aperture channels and reaches for instance points b d and c of fig 3b whose temperature evolution is shown in fig 3e at these points the rate of change in temperature slows down after the first few tenths of seconds a similar trend albeit less pronounced is observed for a flat fracture point a in fig 3a on the contrary the temperature at the points in regions of the fracture of low local aperture has a slower dynamic point e overall the variation of the temperature field over time and space is complex as shown in fig 3 c and d thermal plumes advance approximately 0 1 m into the rock matrix these observations confirm that considering the thickness of the rock matrix layer d 2 m is sufficient to eliminate the boundary effect to evaluate how the temperature field is linked to the pressure gradient we use here the concept of a thermal front the thermal front s velocity is an essential parameter for a geothermal reservoir as the cold front arrival associated with the re injection of fluids causes a decrease of the temperature of the produced fluid thus determining the longevity and economic prospect of the system e g nottebohm et al 2012 furthermore a widely used concept to characterize hydraulic properties from heat tracer tests is based on measuring thermal velocities that are generally derived from thermal breakthrough curves using predefined values between injection and initial temperatures gossler et al 2019 we define the thermal front as the set of locations at which δ t 1 2 black lines in fig 3 for both the parallel plate and heterogeneous fractures considered above we find as expected that heat loss at the fracture walls creates a thermal front black solid line in fig 3 that is delayed relative to the fluid front blue solid line in fig 3 e g bodvarsson 1972 thus for the parallel plate fracture when the fluid front is approaching the outlet of the fracture the thermal front travels a normalized distance x pp l equal to 0 3 fig 3a in order to characterize how the thermal behavior evolves on average we consider the evolution of the thermal front with time fig 3b presents an example of the thermal front advancement for the fracture family a γ d m 0 46 black dashed lines the front spreading pathway varies with the local fluid velocity due to the roughness of the fracture aperture as the thermal front grows heat fingers are developing along preferential flow paths within the fracture plane mainly in the middle region of the fracture this causes deviations of the thermal front position from its average x rough l in the following we characterize the advancement and the evolution of thermal fronts in rough fractures and determine geometrical parameters of individual fractures controlling this advancement this is achieved by studying the hydrothermal behavior of models with different fracture aperture patterns furthermore we compare the results with the reference case of a fracture modeled with two parallel plates separated by a constant aperture d m i e no self affine spatial variations the key geometric characteristic of the studied fractures are presented in table 2 3 2 2 influence of fracture closure we now investigate how the roughness amplitude influences the thermal behaviour of fractures from family a fig 2c which exhibits a moderate flow inhibiting behavior red markers in fig 2a to do so we vary the fracture closure γ d m by varying the fracture s mechanical aperture d m while keeping the same standard deviation for their height distributions γ test 1 table 2 examples of fracture apertures generated on a 1024 1024 grid are shown in fig 4 a for small fracture closure γ d m 0 02 fig 4a top spatial variations of the aperture are negligible in comparison to the mean aperture as the fracture is closed γ d m 0 21 0 40 and 0 59 fig 4a from top to bottom relative fluctuations of the aperture increase as a consequence of increased closure flow channeling becomes more important this is shown in fig 4b where maps of local fluxes normalized by the mean local flux are shown in fig 4b for high closure cases the flow tends to avoid regions of small local apertures and consequently is localized in a large aperture channel along the flow direction this channel can be seen across almost the whole fracture in fig 4a bottom maps the aperture of this channel is relatively small in the vicinity of the inlet and in the vicinity of the outlet leading to the flow inhibiting behavior displayed in fig 2 finally fig 4c presents the simulated temperature fields after 1000 s of injection for different values of fracture closure as discussed above one of the important characteristics of the geometry of the mixing zone where 0 δ t 1 is the position and shape of the thermal front black dashed lines in fig 4 the shape of the thermal front within the fracture is strongly correlated with the hydraulic flow for small values of fracture closure the thermal front is almost straight and transverse to the mean flow direction however the thermal front becomes less smooth as γ d m increases the pattern of temperature distribution becomes complex with slow zones forming in regions of low local fracture apertures and thermal fingers developing along preferential flow channels thus for γ d m 0 59 when the most advanced thermal finger passes half of the fracture s length the most delayed region of the thermal front are still in the vicinity of the fracture inlet fig 4c bottom hence the width of the thermal front parallel to the flow direction increases as the fracture is closed in order to quantify the variability of thermal front velocities we use the results of test 1 table 2 and observe how the mean position of the thermal front x rough evolves with time the standard deviation of the front position which quantifies its width along the mean flow direction is plotted in fig 5 against the position x rough at positions y 0 0 01 10 m when the roughness amplitude increases the standard deviation of the front position increases implying that the thermal channeling effect is more pronounced as expected we also note that the velocity of the thermal front decreases as the fracture closure increases the latter is related to the hydraulic behaviour of the fracture which tends to inhibit the hydraulic flow as shown in fig 2a we further verified this observation by comparing the results of hydrothermal simulations in rough walled fractures with simulations in parallel plate fractures of identical mechanical aperture test 2 table 2 fig 6 presents the evolution in time of the average velocity of the thermal front v rough x rough t relative to the v pp x pp t that is the thermal front velocity in a fracture modeled with a constant aperture d m for a small fracture closure γ d m 0 05 which corresponds to a nearly smooth aperture field a parallel plate model reproduces a similar thermal profile and the ratio v rough v pp is close to 1 as the fracture closure is increased γ d m 0 18 0 32 0 46 and 0 60 fig 6 the velocity of thermal front becomes slower compared to thermal front velocity in parallel plate fractures with identical mechanical aperture v rough v pp for all simulations higher ratios of thermal front velocities v rough v pp are observed close to the fracture inlet fig 6 also demonstrates that for small fracture closure γ d m 0 05 v rough v pp meaning that the thermal front advances faster in rough fractures compared to what would be expected with a parallel plate fracture of uniform aperture d m however after a short transient regime the thermal front velocities in rough and in smooth fractures become equal v rough v pp as the thermal tracer enters the fracture heat diffuses first across the fracture and afterwards away into the matrix once the rock temperature starts to evolve in time and in space the ratio v rough v pp stabilizes larger fracture apertures imply that heat needs more time to diffuse across the fracture aperture to verify this we evaluate through tests 3 4 table 2 the thermal front velocity within the fractures in family a with different mechanical apertures but the same fracture closures γ d m fig 6 for a fracture with an aperture d m 15 mm the thermal front velocity ratio becomes quasi steady after t 500 s when the thermal front has travelled a mean normalized distance x rough l equal to 0 17 for a fracture with the same closure γ d m 0 05 but smaller aperture d m 1 5 mm the thermal front velocity ratio becomes quasi steady after t 200 s and for fractures with small apertures d m 1 mm the thermal front velocity ratio stabilizes already after t 100 s when the thermal front has travelled a mean normalized distance x rough l of less than 0 1 we further evaluated through tests 5 6 table 2 that for more closed fractures asterisk in fig 6 with large apertures d m 13 23 mm the thermal front velocity ratio becomes quasi steady after t 400 700 s respectively 3 2 3 influence of the fracture hydraulic aperture for the same cases as in fig 6 figure 7 presents the ratio v rough v pp versus the ratio between the hydraulic and mechanical apertures d h d m for two different times for a very short duration t 30 s when the regime is still transitory grey markers and for a longer duration t 700 s black markers when the thermal front velocity ratio becomes quasi steady for a short duration we can observe that for a rough aperture the thermal front advances systematically faster in comparison to what we expect from the hydraulic behavior all the grey points are above the 1 1 line our results also demonstrate that this effect is more pronounced for fractures with large apertures circle markers referring to small apertures are closer to the 1 1 line than square markers and asterisk referring to larger apertures the demonstrated faster propagation of the thermal signal in rough fractures v rough when compared to that in smooth fractures v pp agrees with the work of neuville et al 2010b who attributes this effect to a decrease in heat exchange efficiency in rough fractures due to the reduction of transit times in the channeled areas of a rough walled fracture however once the hosting rock temperature starts to evolve a process which was not accounted for in the model of neuville et al 2010b the thermal front velocity in rough fractures slows down fig 6 and for t 400 s we obtain fig 7 a perfect correlation between the ratio v rough v pp and the ratio between the hydraulic and mechanical apertures d h d m black triangle markers this implies that once heat has diffused along the out of plane direction over the fracture s aperture the thermal behavior of a rough walled fracture is determined by its effective hydraulic transmissivity for the morphology investigated here family a fractures the permeability is reduced and thus the advancement of the thermal front is slower in rough fractures compared to what would be expected with a model of flat fractures having the same mechanical aperture d m v rough v pp 1 we now leave the specifics of family a and consider all fracture families in fig 2a for different closure values test 7 table 2 statistical thermal results presented in fig 8 confirm the near perfect correlation between the ratio v rough v pp and the ratio between the hydraulic and mechanical apertures d h d m this means that once stabilized such that heat has diffused along the out of plane direction over the fracture s aperture the mean position in time of the thermal front within a rough walled fracture is determined by the hydraulic aperture d h for different fracture families with the same fracture closure γ d m we find that the flow inhibiting behavior is favored statistically méheust and schmittbuhl 2001 note that these flow enhancing or flow inhibiting behaviors of individual fractures are related to the fractures hydraulic anisotropy as a flow enhancing fracture becomes flow inhibiting and vice versa when the flow direction is rotated by 90 see discussion in méheust and schmittbuhl 2001 hence as the fracture due to the roughness of its walls is either less or more permeable than a flat parallel plate of identical mechanical aperture the efficiency in transferring heat is also highly variable 0 1 v rough v pp 1 6 from one fracture family to another but ratios smaller than 1 are favored statistically 3 2 4 conductive heat flux while previous studies characterized the heat exchange efficiency of rough fractures through the use of temperature metrics neuville et al 2010a 2013 fox et al 2015 in this study we provide some insights into diffusive exchange processes at the fracture scale using the same data tests 1 2 table 2 we now calculate the total conductive heat flux between the fracture and the rock matrix for family a for different values of the fracture closure γ d m 0 05 0 18 0 32 0 46 0 60 our results presented in fig 9 demonstrate that for all cases investigated here the conductive heat flux is greater for the equivalent parallel plate fracture i e the parallel plate with an aperture equal to the rough fracture s mechanical aperture d m q rough h q pp h moreover the ratio q rough h q pp h converges in time to a plateau fig 9 inset interestingly fig 9 showing the plateau value versus the ratio of the hydraulic to mechanical apertures d h d m demonstrates that the conductive heat flux between the rough fracture and the surrounding rock can be predicted from the equivalent parallel plate model overall for fracture family a both the heat flux along the fracture and the conductive heat fluxes between the fracture and the embedding rock decrease when the roughness amplitude increases this analysis suggests that a major cause of the observed slowing down of the thermal front in rough fractures compared to smooth fractures having the same mechanical aperture fig 7 is related to the flow inhibiting behavior of fractures from family a rather than to an increase in the efficiency of the conductive heat exchange between the fluid and the rock matrix finally this result confirms that for the hydrothermal conditions studied here when the influence of heat advection in the fracture dominates the influence of conduction in the matrix the hydraulic aperture governs the fracture s thermal behaviour as we shall discuss in section 4 such conditions are actually relevant for most applications heat tracer testing and geothermal systems 3 2 5 influence of the mismatch scale correlation length finally we investigate how the mismatch scale i e correlation length influences the thermal behaviour of the fracture to do so we modify not only the fracture closure γ d m but also the ratio l l c while keeping the same numerical seed for the generation of the rough topographies and the same standard deviation for their height distributions test 8 table 2 fig 10 presents the simulated temperature fields after 1000 s of injection for different mismatch scales l l c 2 fig 10a l l c 4 fig 10b l l c 16 fig 10c and for different values of fracture closure γ d m 0 02 0 25 0 48 and 0 60 from top to bottom similarly to l l c 1 fig 4 the thermal channeling effect is all the more pronounced as the fracture is more closed fig 10 for l l c 2 fig 10a the thermal front displays large scale distortions representing a large fraction of its total width furthermore we observe the refinement of filaments and global flattening of thermal fronts i e decrease in their roughness amplitude with increasing l l c fig 10 from left to right which is determined by how the decrease of the mismatch scale impacts the advecting velocity field reducing the typical scale of its heterogeneities and the 2 d velocity contrast between preferential flow paths and regions of lower velocities despite the changes in the q field with increasing l l c we find similarly to what is observed for l c l black markers in fig 8 that the fluid flow scaling translates into that of heat transport indeed red markers in fig 8 show strong correlation between the ratio v rough v pp and the ratio between the hydraulic and mechanical apertures d h d m for l l c 2 4 and 16 this result confirms that the hydraulic behaviour allows predicting thermal channeling effects and the related thermal behavior for geological rough fractures for various values of the fracture closure and various values of the ratio l l c 4 discussion we find that the thermal behavior of horizontal rough walled fractures can be predicted from their hydraulic behavior as demonstrated for a wide range of thermal péclet numbers 6 p e 200 of course this finding is expected to be all the more valid for larger péclet numbers p e 200 for lower péclet values slight deviations from this general finding was observed for p e 5 considering lower péclet numbers would be very demanding in terms of computational resources but it would also not be so relevant for applications indeed péclet numbers typically take values in the range of 10 7 000 in fractured geothermal systems under production horne and rodriguez 1983 geiger and emmanuel 2010 neuville et al 2010a and the péclet numbers of thermal tracer tests range between 2 and 70 000 klepikova et al 2016 hawkins et al 2017 de la bernardie et al 2019 as summarized in table 3 this suggests a broad applicability of our inferred relationships between heat transport and hydraulic behavior interestingly similar conclusions have also been previously achieved for solute transport applying the equivalent aperture size calculated based on the equivalent permeability of the system provides an acceptable prediction of solute transport nick et al 2011 in our model fracture flow and heat transport are described in 2 d this allows us to solve transport both in the fracture and in the rock matrix with an extent of the matrix along the direction normal to the fracture plane that is sufficiently large to avoid boundary effects this wouldn t be possible if we had to discretize the fracture aperture in a 3 d mesh within the fracture to account for fracture flow and transport in 3 d or it would be so demanding on computer resources that we wouldn t be able to consider ensemble statistics of fractures with identical geometrical parameters due to this choice however 3 d flow effects cannot be accounted for in our model a number of studies have addressed such effects and concluded that they might be significant these include nonlinearities in the flow induced either by local tortuosity and roughness or by inertial effects e g ge 1997 brush and thomson 2003 wang et al 2015 2020 he et al 2021 the latter are not relevant to our study since we consider creeping flow however solving the flow from the reynolds equation i e the traditional local cubic law which cannot account for tortuosity effects in the third dimension can result in overestimation of the transmissivity or hydraulic aperture nevertheless in laminar flow regimes contributions of these effects do not impact flow and heat transport significantly e g brush and thomson 2003 lee et al 2015 the fracture s dimensionality may also impact heat transport through it we consider a horizontal fracture and thus do not need to account for buoyancy effects resulting from the temperature dependence of the fluid s density which in a subvertical fracture may lead to convective fluid circulation patterson et al 2018 note that some studies solving advection diffusion equations for heat transport in three dimensions report the emergence of 3 d effects thus the presence of recirculation and stagnant zones related to highly variable morphology of the fluid rock interface may locally within the asperity modify the heat exchange andrade et al 2004 neuville et al 2013 another possible thermal effect was studied by klepikova et al 2016 and de la bernardie et al 2019 and emerges when large local slopes angle between the orientation of the local fracture wall and that of the fracture plane exist and the heat flux in between the matrix and the fracture occurring dominantly in the direction perpendicular to the fracture walls is oriented locally at a large angle with respect to the fracture plane this effect was shown to have a significant impact on the scaling of heat recovery in both space and time and the findings were supported by field experiments performed on the fractured rock site of ploemeur where high aperture channels around a few cm participate to the transport of heat klepikova et al 2016 de la bernardie et al 2019 still we do not expect that acounting for 3 d effects of fracture geometry would have a significant impact on our results in contrast to ploemeur field site fractures walls in this study are assumed to have small local slopes this is essentially what the lubrication approximation means the impact of the spatial resolution of the aperture roughness has also been investigated additional simulations reported in the appendix demonstrate that using slightly lower spatial resolution i e downsampling the aperture field by a factor up to 8 does not significantly modify the results these results are in general agreement with the studies of méheust and schmittbuhl 2001 and neuville et al 2011 the former demonstrated that the fourier modes of the aperture field corresponding to the largest scales control flow channeling in the fracture plane for the most part and therefore the fracture s hydraulic aperture while the latter confirmed this finding and further demonstrated that it also holds for heat transport in rough fractures we do not consider the thermal stress acting on preferential flow paths which reduces the effective compressive stress along these paths and thereby further exacerbates flow channeling thus impacting heat exchange processes guo et al 2016 salimzadeh et al 2018 patterson and driesner 2020 as demonstrated by recent works of guo et al 2016 patterson and driesner 2020 the magnitude of the changes due to thermo mechanical effects is comparable with the magnitude of the initial aperture variation we have chosen to ignore such thermal hydraulic mechanical chemical thmc couplings for two reasons first we sought to quantify the impact of fracture surface roughness on heat transport and exchange with the rock matrix and wanted to discriminate these purely geometrical effects from those related to thermo mechanical couplings second since thmc processes i arise in enhanced engineered geothermal systems egs when strong contrasts in temperature exist and ii are relatively slow effects on reservoir performance are noticeable over a time scale of months years pandey et al 2014 guo et al 2016 salimzadeh et al 2018 consequently such effects are expected to be negligible within the typical duration of heat tracer tests which is our prime target application introducing thmc couplings in the model is expected to produce stronger channeling and consequently a larger deviation of the fracture s hydraulic behavior from that of the smooth fracture parallel plate of aperture equal to the rough fracture s mean aperture since this would result in a change in fracture surface topography we would still expect the main finding of the present study to hold namely that heat transport behavior can be predicted from the hydraulic behavior of the fracture 5 conclusions we have investigated numerically the influence of the statistical properties of the aperture field and upscaled hydraulic behavior on heat transport in rough rock fractures with realistic geometries the flow regime was assumed to be laminar and at low reynolds number and the gradient of the aperture field to be small lubrication approximation so that flow in the fracture could be modeled by the reynolds equation in the 2 d fracture plane we considered a regime where heat transport in the fracture is moderately dominant with respect to heat conduction in the rock matrix p e 51 a configuration which is relevant for practical situations at geothermal sites and for forced hydraulic conditions usually adopted during field heat tracer tests we analyzed 20 rough topographies with a hurst exponent ζ 0 8 with aperture closures γ d m varying from 0 001 to 0 6 over a wide range of mean fracture apertures and with four different values of the mismatch scale l l c 1 2 4 16 at fixed fracture closure the deviation from the parallel plate model increases as l c is decreased when closing the fracture the deviation of the hydraulic and thermal behaviors from the equivalent parallel plate model increases in general the thermal behaviour is highly variable among a population of fractures with identical geometrical parameters in comparison to a fracture of uniform aperture equal to the rough fracture s mechanical aperture 75 of the considered fracture aperture fields exhibit a slower displacement of the thermal front along the fracture and less thermal exchange between the fracture and the surrounding rock our main finding is that under the considered conditions thermal behaviour of rough walled rock fractures only depends on the hydraulic properties a similar conclusion was reached by neuville et al 2010b who found that the hydraulic aperture is a better parameter than closure to assess the thermal exchange efficiency of rough fractures in stark contrast to neuville et al 2010b we found that the heat transport along rough walled fractures was similarly efficient as a parallel plate i e smooth fracture of identical hydraulic transmissivity the thermal fronts in rough fractures are initially slightly more advanced at identical times than thermal fronts in flat fractures with equivalent permeabilities but this holds only for a very short time i e the time needed for heat to diffuse along the out of plane direction over the fracture s aperture depending on the mean fracture aperture this transition period lasts for tens to a few hundreds of seconds during which the thermal front travels a mean normalized distance x rough l equal to 0 1 by accounting for fracture matrix heat exchange by transverse diffusion a process which was neglected by neuville et al 2010b the thermal behavior of a rough walled fracture is found to be controlled by its hydraulic aperture and boundary conditions this striking novel finding results from an improved description of the coupled flow and heat transport the practical implication of our finding is that thermal exchanges at the scale of a single fracture is controlled by the effective hydraulic transmissivity provided that thermal properties of the host rock are known this implies that 1 heat tracer tests are reliable for inferring effective fracture transmissivity and 2 the geothermal efficiency can be computed at field sites using hydraulic characterization alone furthermore as long as the considered time scale doesn t allow for significant thm c coupling to take place it follows that the temporal evolution of the geothermal efficiency can be predicted over significantly large time scales using well known low dimensional hydraulic parameterizations in terms of effective hydraulic properties future work could address non stokes flow conditions i e laminar but non linear flow in the fracture another interesting prospect is the overall large scale heat transport behavior in a fractured geological formation it is expected to depend on the combined effects of both the local scale heterogeneity of individual fractures and the heterogeneity at the scale of a discrete fracture network dfn consisting of multiple intersecting fractures the study of coupled flow and heat transport in such dfns will be the topic of future work credit authorship contribution statement maria klepikova conceptualization methodology software writing original draft writing review editing funding acquisition yves méheust conceptualization methodology software writing review editing clément roques methodology software niklas linde conceptualization writing review editing supervision declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments maria klepikova acknowledges the equal opportunity postdoctoral funding from the faculty of geosciences and environment university of lausanne the project has received funding from the european union s horizon 2020 research and innovation programme under the marie sklodowska curie grant agreement no 838508 maria klepikova is grateful to olivier bour for valuable suggestions appendix impacts of the spatial resolution on the hydraulic and thermal results we use the same finite difference numerical scheme as described previously to compute the flow and temperature fields in downsampled apertures that is the aperture field d fr x y is only considered at every n th point of the grid where n 2 4 8 in fig 11 a the relative difference between hydraulic aperture d h for the downsampled apertures and the hydraulic aperture with full resolution of the geometric aperture d h ref is evaluated all computed hydraulic apertures are closer than 0 4 per cent to the full resolution d h ref in general our results reveal that d h is overestimated with respect to d h ref when decreasing the spatial resolution the relative difference between the average velocity of the thermal front calculated inside the downsampled apertures and apertures with full resolution is shown on fig 11b for the case of the largest closure considered γ d m 0 6 here the precision of the approximation is better than 0 6 per cent notation symbol list c p heat capacity d fr x y fracture aperture field d m mechanical aperture d h hydraulic aperture d thickness of the rock matrix k thermal conductivity l fracture length l c mismatch scale correlation length p local pressure p e péclet number p e u m d m ρ f c p f k r q local flux or 2d velocity i e the integral over the local fracture aperture of u q flux through the fracture q h conductive heat flux between fracture and rock matrix over the the entire fracture walls areas r e reynolds number r e d m u m η t time t x y z t temperature δ t x y z t non dimensional temperature anomaly δ t x y z t t x y z t t rock t inj t rock t inj injection temperature t rock initial temperature u three dimensional fluid velocity in the fracture u m maximum velocity within a parallel flat wall fracture v average velocity of the thermal front x distance along applied pressure gradient x mean position of the thermal front y distance normal to applied pressure gradient z out of fracture plane distance γ standard deviation of the aperture field η dynamic viscosity ρ density ζ the hurst or roughness exponent subscripts f fluid fr fracture r rock rough rough fracture pp parallel plate fracture 
248,fracture surface topography exhibits long range spatial correlations resulting in a heterogeneous aperture field this leads to the formation within fracture planes of preferential flow channels controlling flow and transport processes by means of a 3 d heat transport model coupled with a 2 d fracture flow model based on the lubrification approximation i e local cubic law we investigate how the statistical parameters determining spatial aperture variations in individual fractures control the heat exchange at the fluid rock interface and heat transport by flow ensemble statistics over fracture realizations provide insights into the main hydraulic and geometrical parameters controlling the hydraulic and thermal behaviour of rough fractures similarly to the rough fracture s hydraulic behaviour we find that its heat transport behaviour deviates from the conventional parallel plate fracture model with increasing fracture closure and or decreasing correlation length we demonstrate that the advancement of the thermal front is typically slower in rough fractures compared to smooth fractures having the same mechanical aperture in contrast with previous studies that neglect temporal and spatial temperature variations in the rock matrix we find that the thermal behavior of a rough walled fracture can under field relevant conditions be predicted from a parallel plate model with an aperture equal to the rough fracture s effective hydraulic aperture this greatly simplifies the prediction of possible reservoir thermal behavior when using field measurable quantities and hydrological modeling keywords fracture roughness heat exchange flow channeling 1 introduction heat transport in fractured media is often considered in hydrogeological studies for instance when inferring hydraulic parameters by fitting heat transfer equations to thermal data heat carried by groundwater serves as a tracer that can be used to quantify flow through fractures ge 1998 read et al 2013 to characterize fracture network connectivity silliman and robinson 1989 klepikova et al 2011 2014 and to constrain regional scale flow patterns anderson 2005 saar 2010 understanding heat transport in fractured media is a prerequisite for studying hydrothermal flows fairley 2009 malkovsky and magri 2016 moreover characterizing heat transport processes in the subsurface is essential for numerous industrial applications for instance heat transfer is critical when assessing heat storage in the ground de la bernardie et al 2019 lanahan and tabares velasco 2017 and near field thermal effects in the context of radioactive waste disposal zhang et al 2017 knowledge of thermal transport is also necessary to maximize the efficiency and sustainability of geothermal systems kolditz and clauser 1998 martinez et al 2014 shortall et al 2015 guo et al 2016 vik et al 2018 patterson and driesner 2020 heat transport in fractured media has predominantly been addressed using simplified conceptual fracture models for example most fracture network models used for geothermal investigations assume a 1 d linear flow geometry pruess and doughty 2010 or consider fractures as parallel plate systems with a constant aperture gringarten et al 1975 kolditz and clauser 1998 kocabas 2005 jung and pruess 2012 zhou et al 2017 vik et al 2018 hydrothermal studies generally represent faults as tabular bodies of internally homogeneous properties or as 2 d discontinuities that juxtapose hydrogeologic units of differing properties e g malkovsky and magri 2016 while it is common practice to neglect fracture heterogeneity the consequences of such simplifications in terms of predictability remain poorly understood klepikova et al 2016 de la bernardie et al 2019 the fracturing process itself as well as post fracturing processes such as geological stress and strain chemical dissolution precipitation and erosion may result in complex fracture wall surface geometries at the scale of a single fracture fracture wall roughness exhibits long range spatial correlations that induce a heterogeneous aperture field brown 1987 johns et al 1993 candela et al 2012 thus promoting strongly heterogeneous flow path distributions and preferential flow channels within fracture planes e g tsang and tsang 1987 méheust and schmittbuhl 2000 2001 numerous studies have shown that fracture roughness has a remarkably strong impact on fluid flow and as a consequence on solute and particle transport through single fractures studies of flow and solute transport in rough walled fractures include both theoretical and numerical studies e g boutt et al 2006 cardenas et al 2009 ge 1997 méheust and schmittbuhl 2001 2003 wang and cardenas 2014 2017 yang et al 2019 yoon and kang 2021 as well as laboratory experiments e g plouraboué et al 2000 méheust and schmittbuhl 2000 detwiler et al 2000 boschan et al 2007 2008 ishibashi et al 2015 more recently flow channeling in fractured media has been recognized as a critical control on heat transport as well e g geiger and emmanuel 2010 luo et al 2016 based on numerical simulations of flow and heat transport neuville et al 2010b found that the heat exchange between the rock and the fluid is either enhanced or decreased in rough fractures compared to smooth fractures with equivalent mechanical apertures depending on the fracture s morphology and aspect ratio they concluded that because of the presence of larger flow velocities leading to reduced transit times in the channeled areas the heat exchange is generally less efficient in fractures with variable apertures compared to smooth fractures the so called parallel plate with the same hydraulic aperture the authors applied their modeling approach to the geothermal reservoir of soultz sous forêts france leading to predictions of decreased thermal exchanges in rough fractures compared to smooth ones having identical hydraulic transmissivities neuville et al 2010a in their modelling studies neuville et al 2010b 2010a 2011 neglected temporal and spatial temperature variations within the rock matrix neuville et al 2013 moved beyond the assumption of constant matrix temperatures and demonstrated that the hydrothermal behavior within a fracture is heavily influenced by fracture matrix heat exchange processes in their study the navier stokes equations and advection diffusion equations were solved in a simple 3 d model of a fracture consisting of flat parallel walls perturbed by a single sharp asperity neuville et al 2013 more recently using a 3 d numerical model in which the flow in the fracture is described by the reynolds equation fox et al 2015 offered practical understanding of the effects that fracture aperture variations have on heat production in geothermal reservoirs notably fox et al 2015 demonstrated that the thermal exchanges between the fluid flowing within a rough walled fracture and the surrounding matrix are reduced in comparison to planar surfaces because flow channeling reduces the contact area over which heat conductive transfer takes place they conclude that as a consequence fracture aperture variations generally have a negative impact on the thermal performance of geothermal reservoirs more recently from theoretical and numerical analyses of heat transfer in geometrically simple 3 d conceptual models of fractures klepikova et al 2016 demonstrated that flow channeling locally enhances heat diffusion rates because a channel cylindrical conduit is more efficient in exchanging heat between a fracture and the matrix than a planar fracture of equivalent surface regardless of the different underlying assumptions these results reveal that two opposing effects related to flow channeling impact fracture matrix heat exchange at the fracture scale on the one hand heat transfer is locally enhanced by increasing the dimensionality of the diffusive flux klepikova et al 2016 on the other hand heat transfer is reduced by decreasing the effective contact area fox et al 2015 guo et al 2016 consequently it is necessary to jointly quantify the underlying mechanisms using high fidelity physics and modelling in order to understand which of those opposing effects is dominant and under which conditions the results of recent field experiments also call for the development of numerical modelling that considers flow channeling within individual fractures heat tracer experiments have been conducted recently for example at the experimental site of ploemeur france h observatory network read et al 2013 klepikova et al 2016 de la bernardie et al 2019 in a field site in altona ny hawkins et al 2017 and at the grimsel test site gts switzerland doetsch et al 2018 these in situ experiments have demonstrated that due to the signature of fracture heterogeneity on heat transfer processes predictions based on the classical parallel plate fracture model differ significantly from field observations in terms of the first arrival time the maximum amplitude and the tailing of the thermal breakthrough we present a numerical study in which the fracture is described in two dimensions 2 d and the impermeable rock matrix in three dimensions 3 d the flow in the fracture is described by the reynolds equation that is assuming that the lubrication approximation and hence the local cubic law is valid while heat transport is described by the advection diffusion equation in 2 d in the fracture plane and in 3 d in the matrix this formulation allows us to investigate heat transport along the fracture plane and in the matrix as well as heat exchange between them while allowing for much faster numerical simulations than with a 3 d discretization of the fracture we simulate 20 different rough topographies with a hurst exponent ζ 0 8 d m with aperture closures γ d m varying from 0 001 to 0 6 over a wide range of mean fracture apertures d m and with four different values of the mismatch scale l l c 1 2 4 16 more precisely we investigate how these properties impact heat exchange at the fluid rock interface and heat transport along a fracture in terms of the mean behavior among 20 fractures having such a geometry we address how flow heterogeneity within the fracture affects the macroscopic properties i e the hydraulic transmissivity the velocity of the thermal front and its width ultimately governing the efficiency of the fluid mass and heat transport through the fracture compared to previous studies considering simplified fracture geometry gringarten et al 1975 kocabas 2005 pruess and doughty 2010 jung and pruess 2012 neuville et al 2013 klepikova et al 2016 zhou et al 2017 and or a simplified heat transfer model neuville et al 2010b 2010a 2011 the developed numerical model of flow and heat transport considers simultaneously heat conduction in the matrix and in the fracture as well as heat advection coupled to flow channeling in the fracture transient alteration of the fracture s geometry due to thermo hydro mechanical chemical thmc coupling tsang 1991 taron and elsworth 2009 pandey et al 2014 guo et al 2016 salimzadeh and nick 2019 patterson and driesner 2020 are not considered as such effects are mainly relevant for very sharp temperature contrasts and typically act over time scales of months or years compared to recent works investigating heat transfer within rock samples luo et al 2017 chen and zhao 2020 our modelling results allow evaluating the impact of the statistical geometrical properties of a single geological fracture on heat transfer and to determine the key controlling parameters ultimately this work aims at providing improved parameterizations and guidance for effective low dimensional fracture models the presented results also advance our understanding of how fracture heterogeneity control the efficiency of diffusive exchange processes at the fracture scale this paper is organized as follows section 2 describes the physical conceptualization and the implemented numerical approach the numerical results are presented in section 3 where we first describe the results for a given aperture field realizations and then present the general trends that are observed when considering large sets of synthetic fracture fields the relevance of our models to practical configurations and applications is discussed in section 4 finally section 5 concludes with a summary of the most important findings and outlines possible future developments 2 methods to investigate the sensitivity of the hydrothermal behavior of a rough walled fracture embedded in a homogeneous rock matrix to statistical fracture aperture properties we develop a numerical model of flow and heat transport we first present the self affine aperture fracture model then we develop a numerical model of flow and heat transport using the finite element based software comsol multiphysics comsol 2018 as detailed in the following 2 1 roughness of fracture aperture we consider fractures whose projection on the mean fracture plane is square and of lateral length l experimental studies carried out on cores from natural joints brown et al 1986 have shown that the two fracture walls are self affine but are matched that is they display identical topography fluctuations at scales larger than a critical length scale l c l this scale also denoted mismatch scale is the upper limit to the self affinity of the aperture field it is the only characteristic scale smaller than l available to describe the aperture geometry and is a property related to the regimes of faulting e g strike slip normal and the history of the fracture erosion dissolution precipitation processes which is independent of the fracture size l de dreuzy et al 2012 using an algorithm adapted from méheust and schmittbuhl 2003 we generate fracture aperture fields d fr x y with periodic boundary conditions that are self affine up to l c and have a mechanical aperture d m i e the distance between the mean planes of the fracture walls which are parallel to each other if the walls are nowhere in contact then the mechanical aperture is the average value of d fr x y and standard deviation γ of the aperture field over the entire fracture in our model the hurst or roughness exponent has been chosen constant at ζ 0 8 a value observed for many natural and artificial fracture surfaces schmittbuhl et al 1993 bouchaud 1997 renard et al 2013 each fracture is characterized by the fracture closure γ d m which expresses the vertical extent of roughness relative to the fracture wall separation to keep a simple boundary geometry of the domain we prevent contact between fracture surfaces by only considering relatively small fracture closures consequently the mechanical aperture is also the mean aperture due to stochastic variations the mean aperture of the realizations deviates slightly from the value specified when generating the field here and below d m refers to the actual mechanical apertures of each fracture realization and the size of the fracture is fixed to l 10 m the grid size is 1024 1024 using different seeds of the random generator of the white noise used in the algorithm it is possible to generate multiple independent self affine aperture realizations with the same underlying statistical parameters an example of a fracture aperture profile is shown in fig 1 b 2 2 hydrothermal modelling 2 2 1 hydraulic flow flow within a fracture is modeled as a steady state flow where viscous forces dominate inertial effects that is at a low reynolds number furthermore we apply the lubrication approximation according to which fracture walls have small local slopes zimmerman and yeo 2000 under these assumptions out of plane i e along z components of fluid flow become negligible and the velocity field is dominated by in plane components consequently the hydraulic flow through the fracture q defined as the integral over the local fracture aperture of the fluid velocity u can be related to local apertures through a local cubic law similar to the law used for smooth parallel plate fractures méheust and schmittbuhl 2001 zimmerman and yeo 2000 1 q d fr 3 x y 12 η p where p is the local pressure pa and η is the fluid s dynamic viscosity pa s p and q only depend on the two spatial coordinates that define the fracture s mean plane which will be simply denoted fracture plane in the following for quasi parallel flows the reynolds number is generally defined as r e u charact ρ f l z 2 η l h méheust and schmittbuhl 2001 neuville et al 2011 where ρ f is the fluid density kg m 3 l z and l h denote estimates of the vertical and horizontal scales of variation of the velocities m u charact is a characteristic velocity which we choose equal to the maximum velocity within a parallel plate fracture geometry of aperture d m that is u m p d m 2 8 η as estimated from the classical cubic law in the particular case of a rough fracture one can consider l z d m and l h l c so r e can be expressed as 2 r e u m ρ f d m 2 η l c for the flow to remain linear in the fracture the reynolds numbers should be low that is r e 1 oron and berkowitz 1998 brush and thomson 2003 lee et al 2015 furthermore we assume the fluid to be incompressible u 0 which implies that q is also conservative q 0 plouraboué et al 2000 inserting the local cubic law eq 1 in this conservation law yields the reynolds equation 3 d fr 3 x y p 0 as boundary conditions we impose the pressure at the inlet x 0 and outlet x l of the fracture resulting in a macroscopic pressure gradient p and consider periodic boundary conditions at y 0 and y l the aperture field of the fracture also has periodic boundary conditions by construction i e apertures at x l are identical to those at x 0 although this condition is artificial it is more appropriate than assigning no flow boundaries which could restrict the flow thus limiting the sensitivity to fracture properties e g odling 1992 oron and berkowitz 1998 méheust and schmittbuhl 2001 the surrounding rock matrix is assumed to be impermeable an example of the hydraulic flow computed inside a fracture s aperture field a profile of which is shown in fig 1b is shown in fig 1a as black arrows the permeability of a rough fracture depends both on the mean aperture and the geometry of the rock walls the hydraulic aperture for a rough fracture is classically defined as the aperture of the parallel plate i e smooth fracture of identical transmissivity it can thus be computed from the measured flux q and imposed pressure gradient δ p along the fracture tsang 1992 as 4 d h 12 η q δ p 1 3 in the field the hydraulic aperture can be assessed based on hydraulic transmissivity measurement obtained for example by pumping tests while mechanical aperture can be assessed from roughness analysis on core material 2 2 2 heat transport the 3 d finite element modeling tool comsol multiphysics allows for straightforward coupling of fluid flow and heat transport we consider conductive and advective heat transport in the fracture and heat conduction in the surrounding rock matrix moreover our modelling approach allows avoiding 3 d discretization of a thin fracture domain which would lead to very large computational times when considering realistic ratios of the size of the matrix along z to the mean fracture aperture fig 1a presents a 3 d sketch of our model with a fracture located within the x y plane that is surrounded by the impermeable rock matrix at z 0 we assume that fluid at a constant pressure and temperature of t inj enters the fracture initially at temperature t rock at the left model boundary x 0 and flows in response to the imposed pressure gradient from left to right the rock temperature at the outer boundaries as well as the temperature at the fracture outlet are t rock the injected fluid temperature is here warmer t inj t rock than the initial rock temperature a scenario typically encountered during hydrological investigations in hydro carbon recovery or in nuclear waste leakage in the following we shall consider the relative temperature deviation from the host rock s initial temperature defined in eq 10 so the results for the injection of a colder fluid t inj t rock a scenario typical of geothermal systems would be exactly identical several studies have found that heat conduction in the matrix in the direction parallel to the fracture has only a minor effect on the temperature distribution e g jung and pruess 2012 we do consider horizontal conductive heat transport in the matrix but assume that the conductive heat flux through the boundaries at x y 0 and at x y l can be neglected and hence we do not extend the rock matrix in these directions beyond the fracture length the thickness of the rock matrix layer d 2 m was chosen sufficiently large not to influence the temperature field within the fracture during the simulation time the boundary conditions for temperature at the lateral boundaries of the fracture are periodic similarly to those for the aperture field and local flux field we neglect natural convection by temperature induced bouyancy effects that is we consider that the fluid s density is independent of its temperature the heat transport in the system can then be described as follows in the impermeable rock matrix 5 ρ r c p r t t q r h q h where the conductive heat flux is given by 6 q r h k r t in the fracture 7 d fr ρ f c p f t t d fr ρ f c p f u t t t q fr h q h where the conductive heat flux is given by 8 q fr h d fr k f t t here t denotes the gradient operator restricted to the fracture s tangential plane ρ the density kg m 3 c p the heat capacity j kgk k the thermal conductivity w mk u the local fluid velocity field m s and the f fr and r subscripts denote the fluid fracture and rock respectively the transport is characterized by the dimensionless thermal péclet number p e which is the ratio between the characteristic times of heat diffusion conduction and advection in the fracture e g ge 1998 gossler et al 2019 9 p e u m d m ρ f c p f k r the numerical model relies on a 2 d discretization of the fracture plane and the 3 d discretization of the matrix domain to capture with sufficient accuracy the relative variations of temperature we imposed a finer mesh size around the fracture 0 02 m while the coarser elements 0 2 m are located near the outer boundaries of the rock matrix we verified that refining the element size by a factor of 2 did not influence the resulting temperature field significantly 0 1 since we ignore thermo hydro mechanical chemical thmc processes in this study see introduction and discussion all physical properties are assumed to be time invariant for all the the computations done in this study the pressure gradient was chosen such that the péclet number is p e 51 such a high péclet number implies that heat advection in the fracture is much faster than heat conduction in the matrix the simulations are done at low reynolds numbers the maximum reynolds number being r e 0 8 for fracture apertures as high as d m 23 mm which is compatible with the lubrication approximation and hence the local cubic law note that while the definition of péclet and reynolds numbers varies between studies the range of velocities and apertures studied herein is similar to those reported in previous works e g neuville et al 2013 2011 here and below thermal parameters are selected to represent granite the host formations of most deep geothermal projects table 1 3 results 3 1 hydraulic behaviour in fig 2 a we present the ratio of hydraulic to mechanical apertures d h d m as a function of the fracture closure γ d m for 20 families of fractures a family of fractures refers here to a set of fractures generated with the same random seed but with different fracture closure γ d m and or values for the mismatch scale l c investigated in section 3 2 5 in fig 2a each curve represents a family of fractures with the same rock walls but their separation d m differs the behavior of a parallel plate model corresponds to the horizontal dashed line d h d m 1 for closure γ d m 0 1 the hydraulic behaviour is close to the parallel plate model for different fracture families fluid flow tends to be channelized and different hydraulic behaviors can be observed for similar values of the fracture closure γ d m in agreement with previous studies e g méheust and schmittbuhl 2001 2003 we find that the hydraulic behavior of rough fractures deviate monotonically from the ideal parallel plate model as fracture closure is increased depending on the geometry as determined by the random seed the deviation from the parallel plate model can be positive which implies that the fracture is more conducive to flow than a parallel plate with identical mean separation flow enhancing behavior méheust and schmittbuhl 2000 if the deviation is negative then the fracture is characterized by flow inhibiting behavior similar to previous studies méheust and schmittbuhl 2001 neuville et al 2010b we see that for most cases 75 of fracture families the effective hydraulic transmissivity at the scale of the fracture is reduced in order to better understand the origin of these differences in hydraulic behaviour we plot in figs 2b d some of the investigated aperture fields d fr x y d m for the highest closure considered and in figs 2e g the corresponding maps of local fluxes 2 d velocities normalized by their mean value q q in fig 2b we see a large channel oriented parallel to the applied pressure gradient from left to right which constitutes a configuration favorable to flow as seen in fig 2e this fracture morphology corresponds to the largest ratio d h d m in fig 2a fig 2c shows the map of the ratio d fr x y d m for one of the fracture families considered in fig 2a this family demonstrates a moderate flow inhibiting behavior family a red markers in this case the fracture aperture field is characterized by a main tortuous channel with smaller flow obstacles fig 2f in fig 2d a barrier is seen across the whole fracture that is perpendicular to the applied pressure gradient as shown in fig 2g this results in a strong flow inhibiting behaviour of the fracture lowest ratio d h d m in fig 2a in general the resulting local fluxes in fractures vary over several orders of magnitude with a ratio of the local flux to the mean local flux q q reaching 12 in some geometrical configurations additional simulations with other mismatch scales l c indicate in agreement with méheust and schmittbuhl 2003 that the mismatch scale also has a critical impact on the flow channeling we find that the mean hydraulic behavior of rough walled fractures generally converges to the parallel plate estimate when the ratio l l c increases these results are discussed later in relation to the resulting thermal behaviour section 3 2 5 3 2 thermal behaviour 3 2 1 thermal front definition fig 3 presents snapshots of the temperature field simulated using a parallel plate fracture model and a rough fracture model here and throughout the paper see table 2 we consider as leading example the self affine fracture from family a shown on fig 2c at closure γ d m 0 46 family a is considered as a representative example since this fracture family demonstrates moderate flow inhibiting behaviour as most natural rock fractures méheust and schmittbuhl 2001 we calculate the non dimensional temperature anomaly as 10 δ t x y z t t x y z t t rock t inj t rock the temperature distribution in the rough walled fracture is highly heterogeneous fig 3b and the temperature evolution over time may differ considerably even for points located close to each other fig 3e initially the thermal anomaly propagates along preferential large aperture channels and reaches for instance points b d and c of fig 3b whose temperature evolution is shown in fig 3e at these points the rate of change in temperature slows down after the first few tenths of seconds a similar trend albeit less pronounced is observed for a flat fracture point a in fig 3a on the contrary the temperature at the points in regions of the fracture of low local aperture has a slower dynamic point e overall the variation of the temperature field over time and space is complex as shown in fig 3 c and d thermal plumes advance approximately 0 1 m into the rock matrix these observations confirm that considering the thickness of the rock matrix layer d 2 m is sufficient to eliminate the boundary effect to evaluate how the temperature field is linked to the pressure gradient we use here the concept of a thermal front the thermal front s velocity is an essential parameter for a geothermal reservoir as the cold front arrival associated with the re injection of fluids causes a decrease of the temperature of the produced fluid thus determining the longevity and economic prospect of the system e g nottebohm et al 2012 furthermore a widely used concept to characterize hydraulic properties from heat tracer tests is based on measuring thermal velocities that are generally derived from thermal breakthrough curves using predefined values between injection and initial temperatures gossler et al 2019 we define the thermal front as the set of locations at which δ t 1 2 black lines in fig 3 for both the parallel plate and heterogeneous fractures considered above we find as expected that heat loss at the fracture walls creates a thermal front black solid line in fig 3 that is delayed relative to the fluid front blue solid line in fig 3 e g bodvarsson 1972 thus for the parallel plate fracture when the fluid front is approaching the outlet of the fracture the thermal front travels a normalized distance x pp l equal to 0 3 fig 3a in order to characterize how the thermal behavior evolves on average we consider the evolution of the thermal front with time fig 3b presents an example of the thermal front advancement for the fracture family a γ d m 0 46 black dashed lines the front spreading pathway varies with the local fluid velocity due to the roughness of the fracture aperture as the thermal front grows heat fingers are developing along preferential flow paths within the fracture plane mainly in the middle region of the fracture this causes deviations of the thermal front position from its average x rough l in the following we characterize the advancement and the evolution of thermal fronts in rough fractures and determine geometrical parameters of individual fractures controlling this advancement this is achieved by studying the hydrothermal behavior of models with different fracture aperture patterns furthermore we compare the results with the reference case of a fracture modeled with two parallel plates separated by a constant aperture d m i e no self affine spatial variations the key geometric characteristic of the studied fractures are presented in table 2 3 2 2 influence of fracture closure we now investigate how the roughness amplitude influences the thermal behaviour of fractures from family a fig 2c which exhibits a moderate flow inhibiting behavior red markers in fig 2a to do so we vary the fracture closure γ d m by varying the fracture s mechanical aperture d m while keeping the same standard deviation for their height distributions γ test 1 table 2 examples of fracture apertures generated on a 1024 1024 grid are shown in fig 4 a for small fracture closure γ d m 0 02 fig 4a top spatial variations of the aperture are negligible in comparison to the mean aperture as the fracture is closed γ d m 0 21 0 40 and 0 59 fig 4a from top to bottom relative fluctuations of the aperture increase as a consequence of increased closure flow channeling becomes more important this is shown in fig 4b where maps of local fluxes normalized by the mean local flux are shown in fig 4b for high closure cases the flow tends to avoid regions of small local apertures and consequently is localized in a large aperture channel along the flow direction this channel can be seen across almost the whole fracture in fig 4a bottom maps the aperture of this channel is relatively small in the vicinity of the inlet and in the vicinity of the outlet leading to the flow inhibiting behavior displayed in fig 2 finally fig 4c presents the simulated temperature fields after 1000 s of injection for different values of fracture closure as discussed above one of the important characteristics of the geometry of the mixing zone where 0 δ t 1 is the position and shape of the thermal front black dashed lines in fig 4 the shape of the thermal front within the fracture is strongly correlated with the hydraulic flow for small values of fracture closure the thermal front is almost straight and transverse to the mean flow direction however the thermal front becomes less smooth as γ d m increases the pattern of temperature distribution becomes complex with slow zones forming in regions of low local fracture apertures and thermal fingers developing along preferential flow channels thus for γ d m 0 59 when the most advanced thermal finger passes half of the fracture s length the most delayed region of the thermal front are still in the vicinity of the fracture inlet fig 4c bottom hence the width of the thermal front parallel to the flow direction increases as the fracture is closed in order to quantify the variability of thermal front velocities we use the results of test 1 table 2 and observe how the mean position of the thermal front x rough evolves with time the standard deviation of the front position which quantifies its width along the mean flow direction is plotted in fig 5 against the position x rough at positions y 0 0 01 10 m when the roughness amplitude increases the standard deviation of the front position increases implying that the thermal channeling effect is more pronounced as expected we also note that the velocity of the thermal front decreases as the fracture closure increases the latter is related to the hydraulic behaviour of the fracture which tends to inhibit the hydraulic flow as shown in fig 2a we further verified this observation by comparing the results of hydrothermal simulations in rough walled fractures with simulations in parallel plate fractures of identical mechanical aperture test 2 table 2 fig 6 presents the evolution in time of the average velocity of the thermal front v rough x rough t relative to the v pp x pp t that is the thermal front velocity in a fracture modeled with a constant aperture d m for a small fracture closure γ d m 0 05 which corresponds to a nearly smooth aperture field a parallel plate model reproduces a similar thermal profile and the ratio v rough v pp is close to 1 as the fracture closure is increased γ d m 0 18 0 32 0 46 and 0 60 fig 6 the velocity of thermal front becomes slower compared to thermal front velocity in parallel plate fractures with identical mechanical aperture v rough v pp for all simulations higher ratios of thermal front velocities v rough v pp are observed close to the fracture inlet fig 6 also demonstrates that for small fracture closure γ d m 0 05 v rough v pp meaning that the thermal front advances faster in rough fractures compared to what would be expected with a parallel plate fracture of uniform aperture d m however after a short transient regime the thermal front velocities in rough and in smooth fractures become equal v rough v pp as the thermal tracer enters the fracture heat diffuses first across the fracture and afterwards away into the matrix once the rock temperature starts to evolve in time and in space the ratio v rough v pp stabilizes larger fracture apertures imply that heat needs more time to diffuse across the fracture aperture to verify this we evaluate through tests 3 4 table 2 the thermal front velocity within the fractures in family a with different mechanical apertures but the same fracture closures γ d m fig 6 for a fracture with an aperture d m 15 mm the thermal front velocity ratio becomes quasi steady after t 500 s when the thermal front has travelled a mean normalized distance x rough l equal to 0 17 for a fracture with the same closure γ d m 0 05 but smaller aperture d m 1 5 mm the thermal front velocity ratio becomes quasi steady after t 200 s and for fractures with small apertures d m 1 mm the thermal front velocity ratio stabilizes already after t 100 s when the thermal front has travelled a mean normalized distance x rough l of less than 0 1 we further evaluated through tests 5 6 table 2 that for more closed fractures asterisk in fig 6 with large apertures d m 13 23 mm the thermal front velocity ratio becomes quasi steady after t 400 700 s respectively 3 2 3 influence of the fracture hydraulic aperture for the same cases as in fig 6 figure 7 presents the ratio v rough v pp versus the ratio between the hydraulic and mechanical apertures d h d m for two different times for a very short duration t 30 s when the regime is still transitory grey markers and for a longer duration t 700 s black markers when the thermal front velocity ratio becomes quasi steady for a short duration we can observe that for a rough aperture the thermal front advances systematically faster in comparison to what we expect from the hydraulic behavior all the grey points are above the 1 1 line our results also demonstrate that this effect is more pronounced for fractures with large apertures circle markers referring to small apertures are closer to the 1 1 line than square markers and asterisk referring to larger apertures the demonstrated faster propagation of the thermal signal in rough fractures v rough when compared to that in smooth fractures v pp agrees with the work of neuville et al 2010b who attributes this effect to a decrease in heat exchange efficiency in rough fractures due to the reduction of transit times in the channeled areas of a rough walled fracture however once the hosting rock temperature starts to evolve a process which was not accounted for in the model of neuville et al 2010b the thermal front velocity in rough fractures slows down fig 6 and for t 400 s we obtain fig 7 a perfect correlation between the ratio v rough v pp and the ratio between the hydraulic and mechanical apertures d h d m black triangle markers this implies that once heat has diffused along the out of plane direction over the fracture s aperture the thermal behavior of a rough walled fracture is determined by its effective hydraulic transmissivity for the morphology investigated here family a fractures the permeability is reduced and thus the advancement of the thermal front is slower in rough fractures compared to what would be expected with a model of flat fractures having the same mechanical aperture d m v rough v pp 1 we now leave the specifics of family a and consider all fracture families in fig 2a for different closure values test 7 table 2 statistical thermal results presented in fig 8 confirm the near perfect correlation between the ratio v rough v pp and the ratio between the hydraulic and mechanical apertures d h d m this means that once stabilized such that heat has diffused along the out of plane direction over the fracture s aperture the mean position in time of the thermal front within a rough walled fracture is determined by the hydraulic aperture d h for different fracture families with the same fracture closure γ d m we find that the flow inhibiting behavior is favored statistically méheust and schmittbuhl 2001 note that these flow enhancing or flow inhibiting behaviors of individual fractures are related to the fractures hydraulic anisotropy as a flow enhancing fracture becomes flow inhibiting and vice versa when the flow direction is rotated by 90 see discussion in méheust and schmittbuhl 2001 hence as the fracture due to the roughness of its walls is either less or more permeable than a flat parallel plate of identical mechanical aperture the efficiency in transferring heat is also highly variable 0 1 v rough v pp 1 6 from one fracture family to another but ratios smaller than 1 are favored statistically 3 2 4 conductive heat flux while previous studies characterized the heat exchange efficiency of rough fractures through the use of temperature metrics neuville et al 2010a 2013 fox et al 2015 in this study we provide some insights into diffusive exchange processes at the fracture scale using the same data tests 1 2 table 2 we now calculate the total conductive heat flux between the fracture and the rock matrix for family a for different values of the fracture closure γ d m 0 05 0 18 0 32 0 46 0 60 our results presented in fig 9 demonstrate that for all cases investigated here the conductive heat flux is greater for the equivalent parallel plate fracture i e the parallel plate with an aperture equal to the rough fracture s mechanical aperture d m q rough h q pp h moreover the ratio q rough h q pp h converges in time to a plateau fig 9 inset interestingly fig 9 showing the plateau value versus the ratio of the hydraulic to mechanical apertures d h d m demonstrates that the conductive heat flux between the rough fracture and the surrounding rock can be predicted from the equivalent parallel plate model overall for fracture family a both the heat flux along the fracture and the conductive heat fluxes between the fracture and the embedding rock decrease when the roughness amplitude increases this analysis suggests that a major cause of the observed slowing down of the thermal front in rough fractures compared to smooth fractures having the same mechanical aperture fig 7 is related to the flow inhibiting behavior of fractures from family a rather than to an increase in the efficiency of the conductive heat exchange between the fluid and the rock matrix finally this result confirms that for the hydrothermal conditions studied here when the influence of heat advection in the fracture dominates the influence of conduction in the matrix the hydraulic aperture governs the fracture s thermal behaviour as we shall discuss in section 4 such conditions are actually relevant for most applications heat tracer testing and geothermal systems 3 2 5 influence of the mismatch scale correlation length finally we investigate how the mismatch scale i e correlation length influences the thermal behaviour of the fracture to do so we modify not only the fracture closure γ d m but also the ratio l l c while keeping the same numerical seed for the generation of the rough topographies and the same standard deviation for their height distributions test 8 table 2 fig 10 presents the simulated temperature fields after 1000 s of injection for different mismatch scales l l c 2 fig 10a l l c 4 fig 10b l l c 16 fig 10c and for different values of fracture closure γ d m 0 02 0 25 0 48 and 0 60 from top to bottom similarly to l l c 1 fig 4 the thermal channeling effect is all the more pronounced as the fracture is more closed fig 10 for l l c 2 fig 10a the thermal front displays large scale distortions representing a large fraction of its total width furthermore we observe the refinement of filaments and global flattening of thermal fronts i e decrease in their roughness amplitude with increasing l l c fig 10 from left to right which is determined by how the decrease of the mismatch scale impacts the advecting velocity field reducing the typical scale of its heterogeneities and the 2 d velocity contrast between preferential flow paths and regions of lower velocities despite the changes in the q field with increasing l l c we find similarly to what is observed for l c l black markers in fig 8 that the fluid flow scaling translates into that of heat transport indeed red markers in fig 8 show strong correlation between the ratio v rough v pp and the ratio between the hydraulic and mechanical apertures d h d m for l l c 2 4 and 16 this result confirms that the hydraulic behaviour allows predicting thermal channeling effects and the related thermal behavior for geological rough fractures for various values of the fracture closure and various values of the ratio l l c 4 discussion we find that the thermal behavior of horizontal rough walled fractures can be predicted from their hydraulic behavior as demonstrated for a wide range of thermal péclet numbers 6 p e 200 of course this finding is expected to be all the more valid for larger péclet numbers p e 200 for lower péclet values slight deviations from this general finding was observed for p e 5 considering lower péclet numbers would be very demanding in terms of computational resources but it would also not be so relevant for applications indeed péclet numbers typically take values in the range of 10 7 000 in fractured geothermal systems under production horne and rodriguez 1983 geiger and emmanuel 2010 neuville et al 2010a and the péclet numbers of thermal tracer tests range between 2 and 70 000 klepikova et al 2016 hawkins et al 2017 de la bernardie et al 2019 as summarized in table 3 this suggests a broad applicability of our inferred relationships between heat transport and hydraulic behavior interestingly similar conclusions have also been previously achieved for solute transport applying the equivalent aperture size calculated based on the equivalent permeability of the system provides an acceptable prediction of solute transport nick et al 2011 in our model fracture flow and heat transport are described in 2 d this allows us to solve transport both in the fracture and in the rock matrix with an extent of the matrix along the direction normal to the fracture plane that is sufficiently large to avoid boundary effects this wouldn t be possible if we had to discretize the fracture aperture in a 3 d mesh within the fracture to account for fracture flow and transport in 3 d or it would be so demanding on computer resources that we wouldn t be able to consider ensemble statistics of fractures with identical geometrical parameters due to this choice however 3 d flow effects cannot be accounted for in our model a number of studies have addressed such effects and concluded that they might be significant these include nonlinearities in the flow induced either by local tortuosity and roughness or by inertial effects e g ge 1997 brush and thomson 2003 wang et al 2015 2020 he et al 2021 the latter are not relevant to our study since we consider creeping flow however solving the flow from the reynolds equation i e the traditional local cubic law which cannot account for tortuosity effects in the third dimension can result in overestimation of the transmissivity or hydraulic aperture nevertheless in laminar flow regimes contributions of these effects do not impact flow and heat transport significantly e g brush and thomson 2003 lee et al 2015 the fracture s dimensionality may also impact heat transport through it we consider a horizontal fracture and thus do not need to account for buoyancy effects resulting from the temperature dependence of the fluid s density which in a subvertical fracture may lead to convective fluid circulation patterson et al 2018 note that some studies solving advection diffusion equations for heat transport in three dimensions report the emergence of 3 d effects thus the presence of recirculation and stagnant zones related to highly variable morphology of the fluid rock interface may locally within the asperity modify the heat exchange andrade et al 2004 neuville et al 2013 another possible thermal effect was studied by klepikova et al 2016 and de la bernardie et al 2019 and emerges when large local slopes angle between the orientation of the local fracture wall and that of the fracture plane exist and the heat flux in between the matrix and the fracture occurring dominantly in the direction perpendicular to the fracture walls is oriented locally at a large angle with respect to the fracture plane this effect was shown to have a significant impact on the scaling of heat recovery in both space and time and the findings were supported by field experiments performed on the fractured rock site of ploemeur where high aperture channels around a few cm participate to the transport of heat klepikova et al 2016 de la bernardie et al 2019 still we do not expect that acounting for 3 d effects of fracture geometry would have a significant impact on our results in contrast to ploemeur field site fractures walls in this study are assumed to have small local slopes this is essentially what the lubrication approximation means the impact of the spatial resolution of the aperture roughness has also been investigated additional simulations reported in the appendix demonstrate that using slightly lower spatial resolution i e downsampling the aperture field by a factor up to 8 does not significantly modify the results these results are in general agreement with the studies of méheust and schmittbuhl 2001 and neuville et al 2011 the former demonstrated that the fourier modes of the aperture field corresponding to the largest scales control flow channeling in the fracture plane for the most part and therefore the fracture s hydraulic aperture while the latter confirmed this finding and further demonstrated that it also holds for heat transport in rough fractures we do not consider the thermal stress acting on preferential flow paths which reduces the effective compressive stress along these paths and thereby further exacerbates flow channeling thus impacting heat exchange processes guo et al 2016 salimzadeh et al 2018 patterson and driesner 2020 as demonstrated by recent works of guo et al 2016 patterson and driesner 2020 the magnitude of the changes due to thermo mechanical effects is comparable with the magnitude of the initial aperture variation we have chosen to ignore such thermal hydraulic mechanical chemical thmc couplings for two reasons first we sought to quantify the impact of fracture surface roughness on heat transport and exchange with the rock matrix and wanted to discriminate these purely geometrical effects from those related to thermo mechanical couplings second since thmc processes i arise in enhanced engineered geothermal systems egs when strong contrasts in temperature exist and ii are relatively slow effects on reservoir performance are noticeable over a time scale of months years pandey et al 2014 guo et al 2016 salimzadeh et al 2018 consequently such effects are expected to be negligible within the typical duration of heat tracer tests which is our prime target application introducing thmc couplings in the model is expected to produce stronger channeling and consequently a larger deviation of the fracture s hydraulic behavior from that of the smooth fracture parallel plate of aperture equal to the rough fracture s mean aperture since this would result in a change in fracture surface topography we would still expect the main finding of the present study to hold namely that heat transport behavior can be predicted from the hydraulic behavior of the fracture 5 conclusions we have investigated numerically the influence of the statistical properties of the aperture field and upscaled hydraulic behavior on heat transport in rough rock fractures with realistic geometries the flow regime was assumed to be laminar and at low reynolds number and the gradient of the aperture field to be small lubrication approximation so that flow in the fracture could be modeled by the reynolds equation in the 2 d fracture plane we considered a regime where heat transport in the fracture is moderately dominant with respect to heat conduction in the rock matrix p e 51 a configuration which is relevant for practical situations at geothermal sites and for forced hydraulic conditions usually adopted during field heat tracer tests we analyzed 20 rough topographies with a hurst exponent ζ 0 8 with aperture closures γ d m varying from 0 001 to 0 6 over a wide range of mean fracture apertures and with four different values of the mismatch scale l l c 1 2 4 16 at fixed fracture closure the deviation from the parallel plate model increases as l c is decreased when closing the fracture the deviation of the hydraulic and thermal behaviors from the equivalent parallel plate model increases in general the thermal behaviour is highly variable among a population of fractures with identical geometrical parameters in comparison to a fracture of uniform aperture equal to the rough fracture s mechanical aperture 75 of the considered fracture aperture fields exhibit a slower displacement of the thermal front along the fracture and less thermal exchange between the fracture and the surrounding rock our main finding is that under the considered conditions thermal behaviour of rough walled rock fractures only depends on the hydraulic properties a similar conclusion was reached by neuville et al 2010b who found that the hydraulic aperture is a better parameter than closure to assess the thermal exchange efficiency of rough fractures in stark contrast to neuville et al 2010b we found that the heat transport along rough walled fractures was similarly efficient as a parallel plate i e smooth fracture of identical hydraulic transmissivity the thermal fronts in rough fractures are initially slightly more advanced at identical times than thermal fronts in flat fractures with equivalent permeabilities but this holds only for a very short time i e the time needed for heat to diffuse along the out of plane direction over the fracture s aperture depending on the mean fracture aperture this transition period lasts for tens to a few hundreds of seconds during which the thermal front travels a mean normalized distance x rough l equal to 0 1 by accounting for fracture matrix heat exchange by transverse diffusion a process which was neglected by neuville et al 2010b the thermal behavior of a rough walled fracture is found to be controlled by its hydraulic aperture and boundary conditions this striking novel finding results from an improved description of the coupled flow and heat transport the practical implication of our finding is that thermal exchanges at the scale of a single fracture is controlled by the effective hydraulic transmissivity provided that thermal properties of the host rock are known this implies that 1 heat tracer tests are reliable for inferring effective fracture transmissivity and 2 the geothermal efficiency can be computed at field sites using hydraulic characterization alone furthermore as long as the considered time scale doesn t allow for significant thm c coupling to take place it follows that the temporal evolution of the geothermal efficiency can be predicted over significantly large time scales using well known low dimensional hydraulic parameterizations in terms of effective hydraulic properties future work could address non stokes flow conditions i e laminar but non linear flow in the fracture another interesting prospect is the overall large scale heat transport behavior in a fractured geological formation it is expected to depend on the combined effects of both the local scale heterogeneity of individual fractures and the heterogeneity at the scale of a discrete fracture network dfn consisting of multiple intersecting fractures the study of coupled flow and heat transport in such dfns will be the topic of future work credit authorship contribution statement maria klepikova conceptualization methodology software writing original draft writing review editing funding acquisition yves méheust conceptualization methodology software writing review editing clément roques methodology software niklas linde conceptualization writing review editing supervision declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments maria klepikova acknowledges the equal opportunity postdoctoral funding from the faculty of geosciences and environment university of lausanne the project has received funding from the european union s horizon 2020 research and innovation programme under the marie sklodowska curie grant agreement no 838508 maria klepikova is grateful to olivier bour for valuable suggestions appendix impacts of the spatial resolution on the hydraulic and thermal results we use the same finite difference numerical scheme as described previously to compute the flow and temperature fields in downsampled apertures that is the aperture field d fr x y is only considered at every n th point of the grid where n 2 4 8 in fig 11 a the relative difference between hydraulic aperture d h for the downsampled apertures and the hydraulic aperture with full resolution of the geometric aperture d h ref is evaluated all computed hydraulic apertures are closer than 0 4 per cent to the full resolution d h ref in general our results reveal that d h is overestimated with respect to d h ref when decreasing the spatial resolution the relative difference between the average velocity of the thermal front calculated inside the downsampled apertures and apertures with full resolution is shown on fig 11b for the case of the largest closure considered γ d m 0 6 here the precision of the approximation is better than 0 6 per cent notation symbol list c p heat capacity d fr x y fracture aperture field d m mechanical aperture d h hydraulic aperture d thickness of the rock matrix k thermal conductivity l fracture length l c mismatch scale correlation length p local pressure p e péclet number p e u m d m ρ f c p f k r q local flux or 2d velocity i e the integral over the local fracture aperture of u q flux through the fracture q h conductive heat flux between fracture and rock matrix over the the entire fracture walls areas r e reynolds number r e d m u m η t time t x y z t temperature δ t x y z t non dimensional temperature anomaly δ t x y z t t x y z t t rock t inj t rock t inj injection temperature t rock initial temperature u three dimensional fluid velocity in the fracture u m maximum velocity within a parallel flat wall fracture v average velocity of the thermal front x distance along applied pressure gradient x mean position of the thermal front y distance normal to applied pressure gradient z out of fracture plane distance γ standard deviation of the aperture field η dynamic viscosity ρ density ζ the hurst or roughness exponent subscripts f fluid fr fracture r rock rough rough fracture pp parallel plate fracture 
249,when a smoothed particle hydrodynamics sph method a lagrangian and meshfree numerical scheme is used to solve the advection dispersion equation ade sph solutions may not be accurate when particles are irregularly distributed i e the distance between two neighbor particles varies irregularly in a simulation domain particle irregularity may be caused by nonuniform groundwater flow in a heterogeneous field of hydraulic conductivity this study explores for the first time whether the finite particle method fpm can provide more accurate ade solutions than sph does for irregularly distributed particles fpm is similar to sph in theory but uses a modified kernel gradient to construct a sph approximation of solute concentration gradients performance of sph and fpm with irregularly distributed particles is evaluated by using two numerical cases the first case considers only diffusive transport and has an analytical solution for the evaluation the second case considers both advection and dispersion and uses a numerical solution as a reference for the evaluation for each of the two cases several numerical experiments are conducted using multiple sets of irregularly distributed particles with different levels of particle irregularity due to different levels of heterogeneity of hydraulic conductivity numerical results indicate that for the numerical experiments of this study fpm outperforms sph to yield more accurate ade solutions however fpm solutions are still subject to numerical errors and the errors increase when the level of heterogeneity of hydraulic conductivity increases further improvement of fpm is warranted in a future study keywords advection dispersion equation finite particle method fpm smoothed particle hydrodynamic particle irregularity heterogeneous flow field 1 introduction while considerable advancements have been made for numerically solving the advection dispersion equation ade in porous media there are still a number of open questions to obtain accurate numerical solutions of solute transport in a heterogeneous aquifer with non uniform velocity boso et al 2013 de barros et al 2015 herrera et al 2017 konikow 2011 radu et al 2011 werth et al 2006 sole mari and fernàndez garcia 2018 zhou et al 2020 radu et al 2011 one of the questions is the numerical dispersion that overestimates solute mixing which occurs often in traditional grid based methods such as finite difference and finite element methods radu et al 2011 zheng and wang 1999 different from the grid based methods lagrangian approaches theoretically can eliminate numerical dispersion by handling the ade s advection term using particle tracking approaches one of the lagrangian approaches is the smoothed particle hydrodynamics sph method that uses particle movement to take account of the advection term e g tartakovsky et al 2007a b 2009 herrera et al 2009 2010 2017 de barros et al 2015 benson et al 2017 avesani et al 2017 zhu and fox 2001 alvarado rodríguez et al 2019 boso et al 2013 compared sph with four grid based and lagrangian methods for simulating nonreactive and reactive solute transport in heterogeneous porous media and showed the potential of sph to reduce the impacts of numerical dispersion on ade numerical solutions however herrera and beckie 2013 found that standard sph methods are highly sensitive to spatial distributions of particles and that sph simulation accuracy is heavily influenced by the particle irregularity problem discussed in detail by monaghan 2005 when using sph to solve the ade the ade s dispersion term is approximated by a kernel integral interpolation over a support domain and accuracy of the approximation depends on spatial distribution of the particles used in sph liu and liu 2010 monaghan 2005 the particle spatial distribution in turn depends on groundwater flow fields if groundwater seepage velocity s magnitude and direction are constant over a model domain when the particles move with groundwater flow the relative particle positions do not change over space if the particles are uniformly distributed in space i e the distance between two particles is a constant over the model domain accurate sph solutions can be obtained for a groundwater flow in a heterogeneous field of hydraulic conductivity groundwater seepage velocity s magnitude and direction vary in space and time this leads to more particles in locations where pathlines converge and less particles in space where the pathlines diverge in other words particles are irregularly distributed in space and such an irregular particle distribution may cause large numerical errors in sph based ade solutions tartakovsky 2010 when simulation time increases the particle distribution becomes more irregular and the corresponding numerical errors cumulate over time to cause significant numerical errors and instability several methods have been developed to improve sph for irregular particle distributions one method uses a remeshing technique to periodically remesh irregularly distributed particles on a uniform grid and to interpolate particle properties e g solute concentration to new particle locations zimmermann 2001 chaniotis et al 2003 avesani et al 2015 developed a modified sph method based on the moving least squares weighted essentially non oscillatory mls weno reconstruction of concentrations the method applies mls weno to moving particles for accurate reproduction of concentration gradients avesani et al 2015 showed that in comparison with a standard sph method the modified sph method is less sensitive to particle spatial distributions due to a better estimation of spatially varying concentration fluxes however the modified sph method is computationally expensive due to intermediate steps involved in mls weno reconstruction of concentrations when using comparable particle spacings and support domains the modified sph method requires up to two orders of magnitude more cpu time than a standard sph does although the computational time of the modified sph can be reduced by using the ader weno sph method developed by avesani et al 2021 where ader stands for arbitrary derivatives ader weno sph has not been applied to groundwater solute transport problems alvarado rodríguez et al 2019 presented a consistent sph method which resolves the particle irregularity problem by adjusting the number of neighbor particles in a support domain based on the relation between the support domain size and the number of particles derived by zhu et al 2015 and sigalotti et al 2016 using a large number of neighbor particles requires a large amount of computational memory in addition adjusting the number of neighbor particles is not straightforward because the degree of particle irregularity changes in space and time this study is focused on the finite particle method fpm that was developed by liu et al 2005 2006 based on the corrective sph method of chen et al 1999 and chen and beraum 2000 fpm constructs improved sph approximations of a function and the function s derivatives the function can be solute concentrations in the context of solving ade fpm is in theory similar to the modified sph method developed by zhang and batra 2004 and batra and zhang 2004 in the field of solid mechanics in the field of computational fluid dynamics xu and deng 2016 used fpm to model transient free surface flows of viscous and viscoelastic fluids and zhang et al 2019 coupled fpm with a particle shifting technique to model particulate flows with thermal convection other applications of fpm can be found in huang et al 2018 montanino et al 2017 zhang and batra 2008 and zhang et al 2018 fpm has not been used for solving ade and this study is the first attempt in the groundwater modeling community to explore whether fpm can resolve the problem of irregularly distributed particles for solving groundwater flow and solute transport in heterogeneous porous media the irregular particle distributions considered in this study are caused by nonuniform groundwater due to heterogeneous hydraulic conductivity and the distance between two neighbor particles is irregular when the distance is constant over a simulation domain the corresponding particle distribution is called a uniform particle distribution we demonstrate from a theoretical perspective that sph approximations are accurate for uniform particle distributions but may not be the case for irregular particle distributions fpm improves sph by using a modified kernel gradient to construct the sph approximation of concentration gradients the performance of fpm for solving ade in heterogeneous flow fields with irregularly distributed particles is evaluated by using two numerical cases the first case considers only diffusive transport and has an analytical solution that can be used to evaluate accuracy of sph and fpm solutions the second case considers both advection and dispersion and uses a numerical solution as a reference to evaluate accuracy of sph and fpm solutions for each of the two cases several numerical experiments are conducted using multiple irregularly distributed particles with different levels of particle irregularity 2 mathematical model and numerical schemes the ade is given as 1 c t v c d c where c is the solute concentration d is a tensor of dispersion coefficient and v is a vector of seepage velocity in lagrangian coordinates this equation is written as herrera et al 2009 2 d x d t v 3 d c d t d c where x is the position of a particle the substantial derivative d c d t c t v c x represents time rate change of solute concentration along the pathline of a particle 2 1 smoothed particle hydrodynamics and particle irregularity problems in sph eqs 2 and 3 are approximated by using a finite number of particles to carry the solute concentration eq 2 governs movement of the particles and eq 3 defines the diffusive transport component the solution of eq 3 for the particles is evaluated using a kernel interpolation approximation the approximations of concentration c x and its spatial derivative c x at position x are written as liu and liu 2010 4 c x ω c x w x x h d x 5 c x ω c x w x x h d x where denotes the approximations w is a kernel function h is the smoothing length that defines a support domain of the kernel function x prime is the location of a neighbor particle of the particle at x within the support domain ω is the support domain whose radius is in general several times of h and the gradient w x x h w x x h x of w is evaluated at x information of all the particles inside the support domain is used to determine the information of the particle at x liu and liu 2003 according to liu and liu 2010 the kernel function w should be sufficiently smooth and satisfies the following properties 1 positive property w x x 0 2 unity property ω w x x h d x 1 3 symmetric property ω x x w x x h d x 0 4 delta function property lim h 0 w x x h δ x x 5 compact condition w x x 0 for x x kh with k being an integer and 6 monotonical property that w monotonically decreases with h for illustrative purpose the commonly used cubic b spline function is chosen as the kernel function monaghan 2005 in this study 6 w w 0 2 3 q 2 q 3 2 0 q 1 2 q 3 6 1 q 2 0 q 2 where q x x h is the relative distance between two particles at locations x and x w 0 is a normalization factor with w 0 1 h for a one dimensional space w 0 15 7 π h 2 for a two dimensional space and w 0 3 2 π h 3 for a three dimensional space other kernel functions e g gaussian function and wendland function dehnen and aly 2012 can also be used for fpm and the discussion and conclusions of this study based on the cubic b spline function are expected to be valid for other kernel functions the integrals in eqs 4 and 5 around particle i at position x i within the support domain can be approximated as a summation over all neighbor particles j at position x j these approximations are called particle approximations and they are 7 c x i c i ω c x w x x h d x j 1 n b m j ρ j c x j w i j j 1 n b m j ρ j c j w i j 8 c x i c i ω c x w x x h d x j 1 n b m j ρ j c x j j w i j j 1 n b m j ρ j c x j i w i j j 1 n b m j ρ j c j i w i j where ci c x i cj c x j nb is the number of particles within the support domain of particle i wij w x i x j h i wij is the gradient of the kernel function for particle i and satisfies i wij j wij and mj and ρ j are the mass and density of particle j respectively if we denote m j ρ j v j to represent the volume associated with particle j the particle number density nj can be defined as n j 1 v j i e the number of particles per unit volume following tartakovsky and meakin 2005 we have n j k w x j x k h where x k is the location of neighbor particles of particle j eq 8 has a potential problem that for a constant c in space ci should be zero theoretically but j 1 n b m j ρ j i w i j 0 may not be numerically satisfied to resolve this problem monaghan 2005 expressed ci as 9 c i 1 φ i φ i c i c i φ i where φ i φ x i can be any differentiable function with eq 9 ci can be approximated as monaghan 2005 10 c i 1 φ i j n b m j ρ j φ j c j c i i w i j choosing ф i 1 leads to 11 c i j n b m j ρ j c j c i i w i j eq 11 is commonly used in sph since it improves particle approximation accuracy liu and liu 2006 for this reason this study used eq 11 rather than eq 8 numerical accuracy of sph depends on the two particle approximations defined in eqs 7 and 11 for c and c respectively accuracy of eq 7 is closely related to the unity and symmetric properties of the kernel function and accuracy of eq 11 depends on the anti symmetric property of the kernel s first order derivatives to show these we apply the taylor series expansion to cj c x j at particle location x i xi yi zi and retain the first order derivatives i e 12 c j c i x j α x i α c i x i α c i x j x i c i x i y j y i c i y i z j z i c i z i where α is a dimension index for the x y and z directions to analyze the error of the particle approximation given in eq 7 we substitute eq 12 to eq 7 and have 13 c i j 1 n b m j ρ j c j w i j c i j 1 n b m j ρ j w i j c x i α x i α j 1 n b m j ρ j x j α x i α w i j if the unity property j 1 n b m j ρ j w i j 1 and the symmetric property j 1 n b m j ρ j x j α x i α w i j 0 are satisfied then eq 13 becomes ci ci with a second order accuracy in terms of r x j x i where r is the distance between particles i and j whether the unity and symmetric properties of a kernel function are satisfied depends on spatial distribution of the particles this is illustrated in fig 1 with uniform and irregular particle distributions for a one dimensional problem for the uniform distribution any two neighbor particles have the same distance of δx 0 125 m based on the kernel function defined in eq 6 set h 2δx the radius of the support domain ω as 2 h 4δx and w0 1 h for the one dimensional problem fig 1 a1 show functions w i j w 0 and xj xi wij as a function of x j x i h for a number of uniformly distributed particles fig 1 a2 is similar to fig 1 a1 but for irregularly distributed particles the irregular particle distribution is generated by adding random noises to the uniformly distributed particles the random noises follow a uniform distribution within the range of 0 δx in the two figures the dashed lines represent analytical values of w i j w 0 and xj xi wij based on eq 6 the area in blue corresponds to j 1 n b m j ρ j w i j and the area in fuchsia corresponds to j 1 n b m j ρ j x j α x i α w i j in fig 1 a1 for the uniformly distributed particles with h 2δx and nb 7 for the support domain of 2h 2h the blue area estimated via j 1 n b m j ρ j w i j j 1 n b 1 n j w i j equals 1 0000 indicating that the unit property is satisfied the number nj of particles per unit volume is evaluated as n j k w x j x k h by following tartakovsky and meakin 2005 this is the case for the entire manuscript the fuchsia area estimated via j 1 n b m j ρ j x j α x i α w i j j 1 n b 1 n j x j α x i α w i j equals 0 0000 indicating that the symmetric property is satisfied in fig 1 a2 for the irregularly distributed particles with h 2δx and nb 8 for the support domain of 2h 2h similar to the estimation for the uniformly distributed particles we have j 1 n b m j ρ j w i j 1 0457 and j 1 n b m j ρ j x j α x i α w i j 0 0067 indicating that the unity and symmetric properties of the kernel are not satisfied this is not surprising if one examines the distributions of the particles and the shapes of w i j w 0 and xj xi wij based on the particle distributions because the particles are irregularly distributed the shapes of the w i j w 0 and xj xi wij functions cannot be accurately approximated this is the reason that for irregularly distributed particles the sph based approximation of a function may not be accurate to have an accurate particle approximation of the first order derivatives of concentration the first order derivatives of the kernel need to satisfy the so called anti symmetric property chen and beraun 2000 taking the first order derivatives of ci in the x direction as an example based on eq 11 of the derivative and eq 12 of the taylor series expansion of cj we have 14 c i x i j 1 n b m j ρ j c j c i w i j x i c i x i j 1 n b m j ρ j x j i w i j x i c i y i j 1 n b m j ρ j y j i w i j x i c i z i j 1 n b m j ρ j z j i w i j x i where xji xj xi yji yj yi and zji zj zi the anti symmetric property for the first order derivative of the kernel in the x direction is as follows j 1 n b m j ρ j x j i w i j x i 1 j 1 n b m j ρ j y j i w i j x i 0 and j 1 n b m j ρ j z j i w i j x i 0 if the three equations are satisfied then c i x i c x i x i with a second order accuracy in terms of r x j x i where r is the distance between particles i and j liu and liu 2010 the anti symmetric property may not be satisfied for irregularly distributed particle and this is demonstrated in figs 1 a3 and 1 a4 for function x j x i w i x w 0 which is x j i w i j x i normalized by w 0 because the x axis is multiplied by w 0 1 h the two figures are plotted using the same kernel function and particle distribution used for plotting figs 1 a1 and 1 a2 for this one dimensional problem we only examine whether j 1 n b m j ρ j x j i w i j x i 1 is satisfied i e whether the areas in red in figs 1 a3 and 1 a4 equal 1 for fig 1 a3 with the uniformly distributed particles j 1 n b m j ρ j x j i w i j x i 0 9999 which is close to 1 for fig 1 a4 with the irregularly distributed particles this term becomes 0 8060 indicating that the anti symmetric property is not satisfied the two figures demonstrate that for irregularly distributed particles the sph based approximations of the first order derivatives of concentration may not be accurate 2 2 solving ade by using sph the numerical errors of sph caused by irregularly distributed particles affects accuracy of the sph solutions of the ade based on the sph solution of the heat conduction equation cleary and monaghan 1999 espanol and revenga 2003 derived the sph integral approximation of dispersive fluxes according to alvarado rodríguez et al 2019 and herrera et al 2009 the sph approximation of eq 3 for particle i is given as 15 d c i d t 1 2 j 1 n b 1 n i j c j c i x j x i x j x i 2 i w i j α β d α β i d α β j γ x j x i α x j x i β x j x i 2 δ α β where nij 2ninj ni nj is the harmonic mean of ni and nj that are the particle density number at positions x i and x j respectively г 4 and 5 in a two and three dimensional space respectively espanol and revenga 2003 yildiz et al 2009 α and β are the dimension indices ranging from 1 to 3 i e from x to z d α β i is a component of the dispersion coefficient tensor related to particle i and δαβ is the kronecker delta function to obtain the solution of c in time this study solves eq 15 numerically using the algorithm of predictor corrector leapfrog time integration which is computationally efficient and does not require a large amount of computer memory alvarado rodríguez et al 2019 in the predictor step the particle position and associated solute concentration of the particle are advanced in a half time step and the results are used to evaluate time derivative of particle position i e seepage velocity v and the derivative of concentration at the half time step afterward in the prediction step the particle position and solute concentration are advanced for the full time step using the derivatives calculated at the half time step since the derivation of eq 15 involves the particle approximations of ci and its first order derivatives accuracy of eq 15 depends on particle distributions as discussed above for an irregular particle distribution sph accuracy improvement is needed by using the fpm method 2 3 finite particle method fpm and fpm solution of ade in comparison with sph fpm is less sensitive to spatial distribution of particles this is achieved by correcting the first order derivatives of the kernel function involved in eq 15 following xu and deng 2016 we first multiply the both sides of eq 12 by the three first order derivatives w i j x i w i j y i and w i j z i and then integrate over the support domain subsequently replacing the integrations with the particle approximations leads to 16 j 1 n b m j ρ j c j w i j x i j 1 n b m j ρ j c i w i j x i c i x i j 1 n b m j ρ j x j i w i j x i c i y i j 1 n b m j ρ j y j i w i j x i c i z i j 1 n b m j ρ j z j i w i j x i j 1 n b m j ρ j c j w i j y i j 1 n b m j ρ j c i w i j y i c i x i j 1 n b m j ρ j x j i w i j y i c i y i j 1 n b m j ρ j y j i w i j y i c i z i j 1 n b m j ρ j z j i w i j y i j 1 n b m j ρ j c j w i j z i j 1 n b m j ρ j c i w i j z i c i x i j 1 n b m j ρ j x j i w i j z i c i y i j 1 n b m j ρ j y j i w i j z i c i z i j 1 n b m j ρ j z j i w i j z i moving the first term on the right hand side of eq 16 to the left hand side gives 17 c i x i j 1 n b m j ρ j c j c i w i j x i c i x i j 1 n b m j ρ j x j i w i j x i c i y i j 1 n b m j ρ j y j i w i j x i c i z i j 1 n b m j ρ j z j i w i j x i c i y i j 1 n b m j ρ j c j c i w i j y i c i x i j 1 n b m j ρ j x j i w i j y i c i y i j 1 n b m j ρ j y j i w i j y i c i z i j 1 n b m j ρ j z j i w i j y i c i z i j 1 n b m j ρ j c j c i w i j z i c i x i j 1 n b m j ρ j x j i w i j z i c i y i j 1 n b m j ρ j y j i w i j z i c i z i j 1 n b m j ρ j z j i w i j z i writing eq 17 in the matrix form we have the particle approximations as 18 c i x i c i y i c i z i j 1 n b m j ρ j c j c i w i j x i j 1 n b m j ρ j c j c i w i j y i j 1 n b m j ρ j c j c i w i j z i j 1 n b m j ρ j x j i w i j x i j 1 n b m j ρ j y j i w i j x i j 1 n b m j ρ j z j i w i j x i j 1 n b m j ρ j x j i w i j y i j 1 n b m j ρ j y j i w i j y i j 1 n b m j ρ j z j i w i j y i j 1 n b m j ρ j x j i w i j z i j 1 n b m j ρ j y j i w i j z i j 1 n b m j ρ j z j i w i j z i c i x i c i y i c i z i this leads to the expression of the three first order derivatives of ci as 19 c i x i c i y i c i z i m 1 j 1 n b m j ρ j c j c i w i j x i j 1 n b m j ρ j c j c i w i j y i j 1 n b m j ρ j c j c i w i j z i j 1 n b m j ρ j x j i w i j x i j 1 n b m j ρ j y j i w i j x i j 1 n b m j ρ j z j i w i j x i j 1 n b m j ρ j x j i w i j y i j 1 n b m j ρ j y j i w i j y i j 1 n b m j ρ j z j i w i j y i j 1 n b m j ρ j x j i w i j z i j 1 n b m j ρ j y j i w i j z i j 1 n b m j ρ j z j i w i j z i 1 j 1 n b m j ρ j c j c i w i j x i j 1 n b m j ρ j c j c i w i j y i j 1 n b m j ρ j c j c i w i j z i where 20 m j m j ρ j x j i w i j x i j m j ρ j y j i w i j x i j m j ρ j z j i w i j x i j m j ρ j x j i w i j y i j m j ρ j y j i w i j y i j m j ρ j z j i w i j y i j m j ρ j x j i w i j z i j m j ρ j y j i w i j z i j m j ρ j z j i w i j z i if the anti symmetric property of the kernel function discussed in section 2 1 is satisfied matrix m is the identity matrix and the particle approximations of the first order derivatives of ci in eq 19 have the second order accuracy in terms of r x j x i the anti symmetric property however may not hold for irregularly distributed particles as discussed in section 2 1 to resolve the problem related to the anti symmetric property fpm assumes that there exists a modified kernel gradient m w i j x i using the modified kernel gradient and based on eq 11 we can write the particle approximation of the first order derivatives of ci as 21 c i x i j 1 n b m j ρ j c j c i m w i j x i based on eqs 19 and 21 setting c i x i c i x i gives 22 j 1 n b m j ρ j c j c i m w i j x i j 1 n b m j ρ j c j c i m w i j y i j 1 n b m j ρ j c j c i m w i j z i j 1 n b m j ρ j x j i w i j x i j 1 n b m j ρ j y j i w i j x i j 1 n b m j ρ j z j i w i j x i j 1 n b m j ρ j x j i w i j y i j 1 n b m j ρ j y j i w i j y i j 1 n b m j ρ j z j i w i j y i j 1 n b m j ρ j x j i w i j z i j 1 n b m j ρ j y j i w i j z i j 1 n b m j ρ j z j i w i j z i 1 j 1 n b m j ρ j c j c i w i j x i j 1 n b m j ρ j c j c i w i j y i j 1 n b m j ρ j c j c i w i j z i this leads to the modified kernel gradients as 23 m w i j x i m w i j y i m w i j z i j 1 n b m j ρ j x j i w i j x i j 1 n b m j ρ j y j i w i j x i j 1 n b m j ρ j z j i w i j x i j 1 n b m j ρ j x j i w i j y i j 1 n b m j ρ j y j i w i j y i j 1 n b m j ρ j z j i w i j y i j 1 n b m j ρ j x j i w i j z i j 1 n b m j ρ j y j i w i j z i j 1 n b m j ρ j z j i w i j z i 1 w i j x i w i j y i w i j z i m 1 w i j x i w i j y i w i j z i expressing eq 23 in a matrix form gives the modified kernel gradient as 24 i m w i j m 1 i w i j with this modified kernel gradient in fpm the gradient of concentration is approximated by using eq 21 rather than eq 11 accordingly eq 15 becomes 25 d c i d t 1 2 j 1 n b 1 n i j c j c i x j x i x j x i 2 i m w i j α β d α β i d α β j γ x j x i α x j x i β x j x i 2 δ α β and it provides the fpm solution of the ade the only difference between eqs 15 and 25 is the use of the modified kernel gradient corrected by using matrix m this is an advantage of fpm because eq 25 of fpm can be implemented in a straightforward way based on eq 15 of the sph implementation when m is a unity matrix the fpm implementation is identical to the sph implementation fig 1 b illustrates why fpm can yield more accurate results than sph for irregularly distributed particles fig 1 b uses the same particles used for plotting fig 1 a4 but uses the modified first order derivative of the kernel function i e x j x i w i x m w 0 without the modified derivative j 1 n b m j ρ j x j i w i j x i 0 8060 fig 1 a4 with the modified kernel derivative j 1 n b m j ρ j x j i m w i j x i 1 0000 fig 1 b shows that the modified kernel derivative produces an extra area in the right half of the function to compensate the missing area in the left half of the function the extent of this compensation depends on the level of irregularity of particle distributions for extremely irregular particles fpm results may still be inaccurate as shown in the numerical examples discussed in section 3 the modified kernel gradient i m w i j can be obtained by using eq 24 or solving a system of equation m i m w i j i w i j the latter approach is used in this study either approach makes fpm more computationally expensive than sph more importantly if the matrix is ill conditioned or even singular for cases with highly irregular particles fpm may suffer from numerical instability or early termination liu and liu 2010 this occurs in the numerical experiments in section 3 of this study when fpm is used to simulate a solute transport problem with both diffusion and advection dispersion in a heterogeneous field of hydraulic conductivity to resolve this problem we adopt the decoupled fpm method developed by zhang and liu 2018 the method uses matrix m 26 m j m j ρ j x j i w i j x i 0 0 0 j m j ρ j y j i w i j y i 0 0 0 j m j ρ j z j i w i j z i which contains only the diagonal elements of m using m assumes that the diagonal elements of m dominate over the off diagonal terms e g j n b m j ρ j x j i w i j x i dominates over the other two terms j n b m j ρ j y j i w i j x i and j n b m j ρ j z j i w i j x i in the first row of matrix m evaluating the inverse of the diagonal matrix m or solving m i m w i j i w i j is straightforward and computationally efficient in this study if m i m w i j i w i j cannot be solved for a particle due to ill conditioned m associated with the particle the decoupled fpm is used for the particle by solving m i m w i j i w i j 2 4 particle approximation errors with irregular particle distributions in this section we briefly describe the approximation errors of sph with irregular particle distributions according to monaghan 2005 meshfree approximations in sph based on the kernel interpolant integration has two sources of errors the smoothing error caused by the integral interpolation and the discretizing error caused by the summation of discrete particle in the integral the smoothing error depends on the shape and smoothing length of the kernel function using a small h value can reduce the smoothing error monaghan 2005 zhu et al 2015 the discretizing error depends on the particle number and particle distribution for uniformly distributed particle generally speaking using small h leads to small discretizing error for irregularly distributed particle using a small h value may result in a small number of neighbor particles in the corresponding support domain and thus increases discretizing error zhu et al 2015 sigalotti et al 2016 herrera and beckie 2013 to simultaneously reduce smoothing and discretizing errors requires decreasing the smoothing length and simultaneously increasing the number of particles so that the number of particles per support domain does not change on an average sense for all particles zhu et al 2015 this however is difficult to achieve in practice for an irregular particle distribution because it is difficult to estimate the number of particles within a support domain for irregularly distributed particles herrera et al 2009 for the same reason it is also difficult to use a variable smoothing length as a potential alternative solution to ensure a stable average number of neighbor particles the tradeoff between the smoothing error and the discretizing error must be considered when determining h values when using sph or fpm to solve ade with heterogeneous hydraulic conductivity the particle approximation errors increase with time because the degree of particle irregularity increases with time generally speaking the initial particle distribution is always uniform therefore at an early time the particle distribution is still uniform and the smoothing error is larger than the discretizing error with time increasing the discretizing error increases because the particle distribution becomes more and more irregular due to non uniform flow in a heterogeneous field of hydraulic conductivity therefore when solving the ade using sph and fpm the sph and fpm solutions may be similar in an early time but fpm outperforms sph when time increases this is illustrated in the numerical examples discussed below 3 numerical investigation the performance of sph and fpm is investigated by using two numerical cases of two dimensional solute transport modeling with the focus on evaluating the effects of irregular particle distributions on accuracy of the sph and fpm solutions to simulate the particle distributions driven by real flow velocity the irregular particle distributions are generated by advection process in heterogeneous hydraulic conductivity and multiple variance values of log hydraulic conductivity are used for each case to quantify the degree of irregular particle distributions the unity index q is used for particle i its unity index is defined as zhu et al 2015 27 q i j 1 n b m j ρ j w i j which according to eq 7 is the sph approximation of a constant c i 1 for particle i with a complete support domain that is not intercepted by a boundary of the simulation domain if the support domain has sufficient uniformly distributed neighbor particles then q i equals 1 otherwise q i follows a peaked distribution around 1 zhu et al 2015 after q i is evaluated for all the particles the standard deviation of q i is estimated and used to quantify the degree of particle irregularity a larger standard deviation indicates a higher degree of particle irregularity to analyze the errors of sph and fpm solutions the l2 norm of the errors relative to a reference solution an analytical solution or a numerical solution of high accuracy is calculated as 28 erro r l 2 c n c 0 c r c 0 2 n where cn c 0 is the numerical solution of normalized concentration c 0 being an initial concentration given by sph or fpm and cr c 0 is the reference solution of normalized concentration and n is the total number of particles in addition to the two numerical cases presented below an additional numerical case is considered in this study to validate our numerical codes of sph and fpm description of the additional case is given in text s1 and figs s1 s5 of the supplementary information file 3 1 diffusive transport with irregular particle distributions 3 1 1 model setup and particle generation this numerical case considers a diffusive transport problem with zero seepage velocity in a two dimensional confined domain with the size of 40 m 15 m the initial condition of the diffusive transport has an instantaneous gaussian plume the center of the gaussian plume with the maximum concentration c 0 is placed at location x 0 y 0 for the two dimensional diffusion problem the analytical solution of the normalized solute concentration c c0 is zimmermann et al 2001 alvarado rodríguez et al 2019 herrera and beckie 2013 29 c x t c 0 w 2 c 4 exp x x 0 2 a 1 y y 0 2 a 2 4 x x 0 y y 0 a 3 8 t 2 c 2 4 w 2 t c 3 2 w 4 wherea 1 2tdyy w 2 a 2 2tdxx w 2 a 3 tdxy c 2 d x x d y y d x y 2 c 3 dxx dyy c 4 4 t 2 c 2 2 t w 2 c 3 w 4 the initial plume width is set as w 1 m and the components of molecular diffusion coefficient are set as dxx dyy 1 10 6 m2 s dxy 0 the analytical solution is used to compare performance of sph and fpm with uniformly and irregularly distributed particles in terms of solving the diffusion equation for the sph and fpm simulations the total simulation time is set as t 5 000δt 500 h with the uniform time step δt 360 s this time step satisfies the relation of δ t ε h 2 d x x d y y that has been used in literature herrera and beckie 2013 herrera et al 2009 alvarado rodríguez et al 2019 where ε 0 01 is an empirical coefficient used in this study the radius of the supporting domain of sph and fpm is set as 2 h and the smoothing length h is set as h 1 5δx 0 1875 m where δx 0 125 m is used for the modflow simulation discussed below for generating three particle distributions for the diffusion transport problem we conduct three numerical experiments with the plume center placed at locations 10 m 7 m 20 m 7 m and 30 m 7 m and each experiment has its own particle distribution generated by solving an advective transport problem in the same domain of diffusive transport using the predictor corrector leapfrog time integration method discussed in section 2 2 for the advective transport problem a field of heterogeneous hydraulic conductivity is generated by using the gstat geostatistical package pebesma 2004 it is assumed that the natural logarithm of the hydraulic conductivity lnk follows the normal distribution with the mean of lnk 1 and the variance of σ ln k 2 0 1 an exponential covariance function with the correlation length of ilnk 1 m is used to generate the heterogeneous hydraulic conductivity field the flow direction is from a constant head boundary at the left side to the constant head boundary at the right side of the simulation domain the top and bottom boundaries are set as no flow boundaries the groundwater flow is assumed to be steady and modflow 2000 harbaugh 2000 is used to solve the flow problem the modflow grid has 38 400 uniform blocks and each block has the size of 0 125 m 0 125 m seepage velocity is evaluated by using a constant porosity of 0 3 over the simulation domain based on the modflow simulated flow field three sets of particle distributions one uniformly and two irregularly distributed are generated in the first set of particle distribution pd1 particles are uniformly placed at the modflow block centers the second set of particle distribution pd2 is generated by first using pd1 as the initial particle distribution and then simulating advective transport of the particles using the predictor corrector leapfrog time integration method with a constant timestep of δt 1800 s at the simulation time of t 500δt 250 h the particle distribution is saved as pd2 due to the non uniform seepage velocity in the heterogeneous field of hydraulic conductivity the particle positions of pd2 are irregular with more particles in locations where pathlines converge and less particles in locations where the pathlines diverge the third set of particle distribution pd3 is generated in a same way of generating pd2 but using the particle distribution at time t 1 000δt 500 h it should be noted that the computed flow field used for generating pd1 pd3 is not completely divergent free since divergence of the flow field for an incompressible fluid is related to the temporal change of the fluid mass over a given volume the generated particle distributions are affected by the divergence this is a main drawback of the particle methods for simulating solute transport and it also occurs for high order finite volume and finite element methods the three particle distributions are illustrated in fig 2 for an area of the size of 10 m 10 m centered at the plume centers of 10 m 7 m 20 m 7 m and 30 m 7 m fig 2 shows that pd3 is more irregular than pd2 and pd1 is uniform this is reflected in the standard deviation values of the unity index q and they are 0 0201 0 0628 and 0 072 for pd1 pd3 respectively the non zero value of 0 0201 of pd1 is caused by incomplete support domains for particles whose distances to the boundaries are less than the radius of the support domains 3 1 2 results for σ ln k 2 0 1 fig 3 plots the three concentration profiles of the normalized concentration at simulation time t 300 h along the cross section of y 7 m based on the analytical solution and the sph and fpm solutions obtained using the three particle distributions the normalized concentrations are based on the discrete particles for the irregular distributed particles since there are no particles exactly located at the line of y 7 m the particles within the range of lines of y 6 95 m and y 7 05 m are used to compare the two numerical solutions with the analytical solution for particle distribution pd1 fig 3 a since the particles are uniformly distributed the sph and fpm solutions are identical and they coincide with the analytical solution for particle distributions pd2 and pd3 with irregular particle distributions the sph solutions are larger than the analytical solutions at the centers the concentration profiles as shown in figs 3 b and 3 c the errors of the sph solution are significantly larger than those of the fpm solution indicating that fpm outperforms sph for the irregular particle distributions the comparion of sph fpm and deoupeld fpm fpm d solutions are shown in fig s6 of the supplementary information file and the figure illustrates that the fpm and fpm d solutions are visually identical examining the two dimensional plumes also indicates that fpm outperforms sph fig 4 a plots the two dimensional plumes at the simulation time of t 300 h given by the analytical solution and the sph and fpm soltuions obtained usin pd2 fig 4 b does the same for pd3 both figs 4 a and 3 b show that sph overestimates the maximum concentrantion at the plume centers the overestimations are 0 0192 for pd2 and 0 0411 for pd3 which are 6 and 13 of the analytical solution of 0 3164 this kind of overestimation was also reported in alvarado rodríguez et al 2019 and avesani et al 2015 while the overestimations also occur for fpm they are substnatially smaller being 2 and 1 of the analytical solution for pd2 and pd3 respectively the better performance of fpm is attributed to using the modified kernel gradient in eq 25 for fpm rather than the original kernel gradient in eq 15 for sph in sph the first order derivatives of concentration in space are usually underestimated because the integrations of x i x j i x w i j over the support domain are usually less than 1 e g 0 8060 in fig 1 a4 for a limited number of irregularly distributed particles as a result the time rate change of concentration in eq 25 is underestimated and this leads to the overestimation of the normalized concetration at the plume centers fig 3 and an underestimation away from the plume centers an illustration of the underestimation problem is given in fig s3 of the supplementary information file the problems of overestimation and underestimation are alleviated by fpm because of the use of the modified kernel gradients fig s4 of supplementary information file plots the convergence rates based on the relative l2 norm of the numerical solutions the figure indicates that fpm converges to the reference solution at a faster rate than sph does the high accuracy of fpm is attributed to the correction made to kernel gradient as discussed before the correction however makes fpm computationally more expensive than sph take as an example the diffusive transport problem with the irregular particle distribution of pd3 with σ ln k 2 0 1 for a computer with intel r core tm i7 8550 u cpu of 1 99 ghz the cpu time averaged over 10 repeated runs is 18 719 s and 19 678 s for sph and fpm respectively 3 1 3 results for different σ ln k 2 values in addition to σ ln k 2 0 1 this study also considers four additional fields of heterogeneous hydraulic conductivity with σ ln k 2 0 05 0 2 0 4 and 0 8 to further evaluate sph and fpm performance with an increasing level of particle irregularity caused by the increasing level of heterogeneity for each σ ln k 2 value a particle distribution is generated in the same way of generating pd3 by solving the advective transport problem and saving the particle distribution at time t 1 000δt 500 h for the particle distributions of the five σ ln k 2 values the standard deviation of the unit index qi is evaluated and the standard deviation values are plotted in fig 5 the figure also plots the standard deviation values for multiple particle distributions generated at different dimensionless times t ut i lnk for each σ ln k 2 value where u is the mean seepage velocity in x direction the figure shows that for each σ ln k 2 value the irregularity of the particle distributions increases with time due to particle movement when σ ln k 2 increases heterogeneity of hydraulic conductivity increases and irregularity of the particle distributions increases accordingly fig s7 of the supplementary information file illustrates three particle distributions generated for each σ ln k 2 value the particle irregularity for σ ln k 2 0 8 is already large enough as indicated by fig s9 that plots the standard deviation of the unit index qi for σ ln k 2 0 2 0 4 0 8 2 0 4 0 and 8 0 the five particle distributions generated for time t 1 000δt 500 h are used for the third experiment discussed above with the initial plume center placed at 30 m 7 m and the accuracy of sph and fpm is evaluated for the maximum normalized concentration at the plume center and over the entire simulation domain if a particle distribution has high irregularity its matrix m may be ill conditioned or even singular and the system equation of m i m w i j i w i j cannot be solved to resolve this problem the decoupled fpm discussed in section 2 3 is used after the diffusive transport problem is solved by using sph and fpm coupled or decoupled accuracy of the sph and fpm numerical solutions is evaluated by evaluating the difference of the maximum normalized concentration i e cmax n c 0 cmax a c 0 or equivalently cmax n cmax a c 0 between the numerical solutions and the analytical solution of eq 29 the differences are positive for both sph and fpm indicating the overestimation problem discussed above the differences are plotted with time in fig 6 a for the five σ ln k 2 values the figure shows that while accuracy of both sph and fpm deteriorates when σ ln k 2 increases fpm is more accurate than sph for each σ ln k 2 over the entire simulation time accuracy of sph and fpm is also evaluated for the entire domain by using the l2 norm of numerical error defined in eq 28 and the variation of the l2 norm with time is plotted in fig 6 b for the five σ ln k 2 values the figure shows that while the l2 norms of both sph and fpm increases when σ ln k 2 increases the l2 norm of fpm is smaller than that of sph for each σ ln k 2 over the entire simulation time fig 6 shows that the temporal patterns of the sph and fpm solutions are similar it is further noted that the temporal pattern of fig 6 a for the difference of maximum normalized concentration is different from the temporal pattern of fig 6 b for the l2 norm especially for large σ ln k 2 values taking σ ln k 2 0 8 as an example the difference increases until about 200 h and then decreases where the l2 norm keeps increasing this is due to the problem of overestimating the normalized concentration at the plume center as discussed in section 3 1 2 the overestimation may accumulate over time until the plume spreads out over the simulation domain for the l2 norm calculated for the entire domain it accounts for both overestimation and underestimation over the simulation domain and thus increases with time 3 2 advection and dispersion in heterogeneous flow field 3 2 1 model setup this numerical case is generally similar to the first numerical case described above and the differences between the two cases are described here different from the first numerical case this numerical case considers both advection and dispersion and is thus more realistic in this numerical case the flow problem is the same as that of the first numerical case described in section 3 1 the initial condition of this numerical case has an instantaneous concentration c 0 that is a constant over a square with the size of l 3 m centered at the location of 10 m 7 m the total simulation time is set as t 1 200δt with a uniform time step of δt 1800 s we consider three variance values of log hydraulic conductivity i e σ ln k 2 0 1 0 4 and 0 8 for each σ ln k 2 the initial particle distribution at t 0 is uniform and the particles are placed at the center of the modflow blocks when the particles move with the flow the particle distribution becomes more and more irregular for each σ ln k 2 the smoothing length is set as h 1 5δx 0 1875 m when implementing sph and fpm since there is no analytical solution for this numerical case to evaluate the performance of sph and fpm a reference solution is obtained by using sph with a smaller particle spacing of δx δy 0 03125 m and smoothing length of h 0 3 m these values are chosen based on the general principles discussed in section 2 4 that accurate sph solutions can be obtained by using a smoothing length h to have a large number of particles within a support domain zhu et al 2015 herrera et al 2009 figs s10 and s11 of the supplementary information file show the effects of δx and h respectively on the sph based reference solution and text s2 of the supplementary information file explains the reason of choosing δx 0 03125 m and h 0 3 m to obtain the reference solution because the particles used for the reference solution are different from those used for implementing sph and fpm in this numerical case to compare the fpm and sph solutions with the reference solution the three solutions are interpolated to the modflow grid that has uniform blocks each has the size of δx δy 0 125 m the interpolation is conducted by using the matlab scatter interpolation function scatteredinterpolant with the natural option for the function the interpolation is only used for comparing the sph and fpm solutions with the reference solution not for implementing the sph and fpm methods 3 2 2 results fig 7 plots the pathlines of the particles that are placed along the line of x 10 m within the square of the initial plume for heterogeneous fields of hydraulic conductivity with three σ ln k 2 values the pathlines for the case of σ ln k 2 0 1 are significantly more uniform than those for the cases of σ ln k 2 0 4 and σ ln k 2 0 8 for the latter two cases the converging flow zones have more particles than the diverging flow zones fig 8 plots the concentration plumes of sph fpm and reference solutions at dimensionless time t 11 21 10 95 and 10 64 for three σ ln k 2 values of 0 1 0 4 and 0 8 respectively in figs 8 a1 a3 for σ ln k 2 0 1 comparing with the reference solution sph overestimates the maximum normalized concentration by about 3 the reference solution is 0 4728 and the sph solution is 0 4872 whereas fpm overestimate the maximum normalized solution by 0 2 the fpm solution is 0 4729 for σ ln k 2 0 4 the reference solution is 0 4248 and the sph and fpm solutions are 0 4723 and 0 4333 respectively the overestimation increases to 11 and 2 for sph and fpm respectively for σ ln k 2 0 8 the reference solution is 0 3788 and the sph and fpm solutions are 0 4191 and 0 3859 respectively the overestimation remains as 11 and 2 for sph and fpm respectively these results indicate that the fpm solutions are more accurate than the sph solutions for all the three levels of particle irregularity figs 9 a 9 c plot the profiles of the reference sph and fpm solutions along y 7 5 m at different simulation dimensionless times for three σ ln k 2 values of 0 1 0 4 and 0 8 the profiles along y 7 5 m are chosen because while the initial plume center is located at 10 m 7 m the solute plume moves roughly along the line of y 7 5 due to non uniform velocity as shown in figs 8 c1 8 c3 fig 9 a shows that for σ ln k 2 0 1 the sph and fpm solutions are visually identical to the reference solution at early time of t 5 61 when time increases the sph solution deviates from the reference solutions but the fpm solution is still visually identical to the reference solution this is not surprising given that the initial particle distribution is uniform and that irregularity of the particle distribution is relatively small for σ ln k 2 0 1 when σ ln k 2 increases to 0 4 and 0 8 figs 9 b and 9 c show that the sph and fpm solutions become less accurate the sph solutions are different from the reference solutions even at the early time of t 5 48 for σ ln k 2 0 4 and t 5 32 for σ ln k 2 0 8 the fpm solutions are still accurate at the early time but becomes different from the reference solutions when time increases figs 9 a 9 b and 9 c consistently indicate that the fpm solutions are more accurate than the sph solutions for all the three simulation times and for all the three σ ln k 2 values that generate different levels of particle irregularity figs 10 a1 10 c1 plot the differences of maximum normalized concentration calculated over the entire simulation period for σ ln k 2 0 1 0 4 and 0 8 respectively the fpm solutions are more accurate than the sph solutions because fpm simulates more dispersion with the modified kernel gradients and thus alleviates the sph underestimation of time rate change of concentration the results are consistent with those of the diffusive transport problem discussed in section 3 1 2 figs 10 a2 10 c2 do the same for the l2 norm calculated for the entire simulation domain these figures lead to a conclusion that fpm is more accurate than sph over the entire simulation period for all the three σ ln k 2 values that generate different levels of particle irregularity it is noted that the l2 norm of fpm is larger than that of sph in the early simulation time t 140 h which may be resulted from the effect the numerical error introduced by the correction matrix in fpm the numerical error may be relatively large at early time when accumulative discretizing error caused by irregular particle distribution is small with small change in concentration an in depth understanding of the numerical error is warranted in a future study figs 9 and 10 indicate that accuracy of sph and fpm solutions is affected by simulation time and heterogeneity of hydraulic conductivity as discussed in section 3 1 3 for fig 5 the both factors are sources of particle irregularity for heterogenous hydraulic conductivity because uniformly distributed particles become more irregular over time and when heterogeneity of hydraulic conductivity increases when particle irregularity increases the errors of the sph based ade solutions accumulate over time and become large for larger σ ln k 2 values the fpm based solutions are more accurate than the sph based solutions because of using the modified kernel gradient given in eq 24 although the fpm based solutions also become less accurate when time increases and or hydraulic conductivity becomes more heterogeneous 4 conclusions to the best of our knowledge this study is the first attempt of using fpm to solve ade for groundwater solute transport modeling fpm is developed as an improvement of sph to obtain more accurate numerical solutions when particle distributions become irregular in groundwater modeling initially uniformly distributed particles become irregularly distributed when the particles move with groundwater flow in a heterogeneous field of hydraulic conductivity with more less particles flowing to locations where flow pathlines converge diverge for the irregularly distributed particles we demonstrate that the unity and symmetric properties of a sph kernel function and the anti symmetric property of the kernel gradient may not be satisfied the unsatisfied anti symmetric property directly affects accuracy of sph based solutions the accuracy is improved by fpm by using the modified kernel gradient that is based on the sph kernel function and its first order derivatives this makes the implementation of fpm straightforward based on existing sph codes accuracy of the sph and fpm based ade solutions is examined in two numerical cases with irregularly distributed particles the first case considers diffusion only and has an analytical solution that is used as a reference solution to evaluate accuracy of the sph and fpm solutions the second case considers both advection and dispersion and uses a numerical solution obtained using sph with a large number of particles and a large smoothing length to reduce computational errors as a reference solution accuracy of the sph and fpm solutions is evaluated with respect to the difference of maximum normalized concentration calculated at the plume center and the l2 norm calculated for the simulation domain for the limited number of numerical experiments analyzed in this study fpm outperforms sph to yield more accurate solutions in particular the overestimation problem of sph at the plume centers is substantially alleviated by using fpm which is attributed to using the modified kernel gradient in fpm the numerical errors of the sph and fpm solutions increase if particle distributions become more irregular when simulation time increases and or hydraulic conductivity becomes more heterogeneous it is found in this study that the fpm solutions are substantially more accurate than the sph solutions indicating that fpm should be used when simulation time is long and or heterogeneity of hydraulic conductivity is relatively high for highly heterogenous hydraulic conductivity a further improvement of fpm is needed which is however beyond the scope of this study this study uses a fixed smoothing length and thus the same number of particles for all numerical simulations impacts of the smoothing length and the number of particles on fpm solutions is warranted in a future study in addition this study only compares fpm with the standard sph and a comparison between fpm and improved sph methods is also warranted in a future study credit authorship contribution statement tian jiao conceptualization investigation methodology writing original draft ming ye conceptualization methodology supervision writing review editing menggui jin conceptualization supervision writing review editing jing yang writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments we thank the three anonymous reviewers for their constructive and insightful suggestions that have improved the content of the manuscript this study was supported by the national natural science foundation of china no 41877192 the first author was supported by the fundamental research funds for national universities to the china university of geosciences university of geosciences wuhan for her research at the florida state university the second author was supported by nsf grant ear 1552329 supplementary materials supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2021 104043 appendix supplementary materials image application 1 
249,when a smoothed particle hydrodynamics sph method a lagrangian and meshfree numerical scheme is used to solve the advection dispersion equation ade sph solutions may not be accurate when particles are irregularly distributed i e the distance between two neighbor particles varies irregularly in a simulation domain particle irregularity may be caused by nonuniform groundwater flow in a heterogeneous field of hydraulic conductivity this study explores for the first time whether the finite particle method fpm can provide more accurate ade solutions than sph does for irregularly distributed particles fpm is similar to sph in theory but uses a modified kernel gradient to construct a sph approximation of solute concentration gradients performance of sph and fpm with irregularly distributed particles is evaluated by using two numerical cases the first case considers only diffusive transport and has an analytical solution for the evaluation the second case considers both advection and dispersion and uses a numerical solution as a reference for the evaluation for each of the two cases several numerical experiments are conducted using multiple sets of irregularly distributed particles with different levels of particle irregularity due to different levels of heterogeneity of hydraulic conductivity numerical results indicate that for the numerical experiments of this study fpm outperforms sph to yield more accurate ade solutions however fpm solutions are still subject to numerical errors and the errors increase when the level of heterogeneity of hydraulic conductivity increases further improvement of fpm is warranted in a future study keywords advection dispersion equation finite particle method fpm smoothed particle hydrodynamic particle irregularity heterogeneous flow field 1 introduction while considerable advancements have been made for numerically solving the advection dispersion equation ade in porous media there are still a number of open questions to obtain accurate numerical solutions of solute transport in a heterogeneous aquifer with non uniform velocity boso et al 2013 de barros et al 2015 herrera et al 2017 konikow 2011 radu et al 2011 werth et al 2006 sole mari and fernàndez garcia 2018 zhou et al 2020 radu et al 2011 one of the questions is the numerical dispersion that overestimates solute mixing which occurs often in traditional grid based methods such as finite difference and finite element methods radu et al 2011 zheng and wang 1999 different from the grid based methods lagrangian approaches theoretically can eliminate numerical dispersion by handling the ade s advection term using particle tracking approaches one of the lagrangian approaches is the smoothed particle hydrodynamics sph method that uses particle movement to take account of the advection term e g tartakovsky et al 2007a b 2009 herrera et al 2009 2010 2017 de barros et al 2015 benson et al 2017 avesani et al 2017 zhu and fox 2001 alvarado rodríguez et al 2019 boso et al 2013 compared sph with four grid based and lagrangian methods for simulating nonreactive and reactive solute transport in heterogeneous porous media and showed the potential of sph to reduce the impacts of numerical dispersion on ade numerical solutions however herrera and beckie 2013 found that standard sph methods are highly sensitive to spatial distributions of particles and that sph simulation accuracy is heavily influenced by the particle irregularity problem discussed in detail by monaghan 2005 when using sph to solve the ade the ade s dispersion term is approximated by a kernel integral interpolation over a support domain and accuracy of the approximation depends on spatial distribution of the particles used in sph liu and liu 2010 monaghan 2005 the particle spatial distribution in turn depends on groundwater flow fields if groundwater seepage velocity s magnitude and direction are constant over a model domain when the particles move with groundwater flow the relative particle positions do not change over space if the particles are uniformly distributed in space i e the distance between two particles is a constant over the model domain accurate sph solutions can be obtained for a groundwater flow in a heterogeneous field of hydraulic conductivity groundwater seepage velocity s magnitude and direction vary in space and time this leads to more particles in locations where pathlines converge and less particles in space where the pathlines diverge in other words particles are irregularly distributed in space and such an irregular particle distribution may cause large numerical errors in sph based ade solutions tartakovsky 2010 when simulation time increases the particle distribution becomes more irregular and the corresponding numerical errors cumulate over time to cause significant numerical errors and instability several methods have been developed to improve sph for irregular particle distributions one method uses a remeshing technique to periodically remesh irregularly distributed particles on a uniform grid and to interpolate particle properties e g solute concentration to new particle locations zimmermann 2001 chaniotis et al 2003 avesani et al 2015 developed a modified sph method based on the moving least squares weighted essentially non oscillatory mls weno reconstruction of concentrations the method applies mls weno to moving particles for accurate reproduction of concentration gradients avesani et al 2015 showed that in comparison with a standard sph method the modified sph method is less sensitive to particle spatial distributions due to a better estimation of spatially varying concentration fluxes however the modified sph method is computationally expensive due to intermediate steps involved in mls weno reconstruction of concentrations when using comparable particle spacings and support domains the modified sph method requires up to two orders of magnitude more cpu time than a standard sph does although the computational time of the modified sph can be reduced by using the ader weno sph method developed by avesani et al 2021 where ader stands for arbitrary derivatives ader weno sph has not been applied to groundwater solute transport problems alvarado rodríguez et al 2019 presented a consistent sph method which resolves the particle irregularity problem by adjusting the number of neighbor particles in a support domain based on the relation between the support domain size and the number of particles derived by zhu et al 2015 and sigalotti et al 2016 using a large number of neighbor particles requires a large amount of computational memory in addition adjusting the number of neighbor particles is not straightforward because the degree of particle irregularity changes in space and time this study is focused on the finite particle method fpm that was developed by liu et al 2005 2006 based on the corrective sph method of chen et al 1999 and chen and beraum 2000 fpm constructs improved sph approximations of a function and the function s derivatives the function can be solute concentrations in the context of solving ade fpm is in theory similar to the modified sph method developed by zhang and batra 2004 and batra and zhang 2004 in the field of solid mechanics in the field of computational fluid dynamics xu and deng 2016 used fpm to model transient free surface flows of viscous and viscoelastic fluids and zhang et al 2019 coupled fpm with a particle shifting technique to model particulate flows with thermal convection other applications of fpm can be found in huang et al 2018 montanino et al 2017 zhang and batra 2008 and zhang et al 2018 fpm has not been used for solving ade and this study is the first attempt in the groundwater modeling community to explore whether fpm can resolve the problem of irregularly distributed particles for solving groundwater flow and solute transport in heterogeneous porous media the irregular particle distributions considered in this study are caused by nonuniform groundwater due to heterogeneous hydraulic conductivity and the distance between two neighbor particles is irregular when the distance is constant over a simulation domain the corresponding particle distribution is called a uniform particle distribution we demonstrate from a theoretical perspective that sph approximations are accurate for uniform particle distributions but may not be the case for irregular particle distributions fpm improves sph by using a modified kernel gradient to construct the sph approximation of concentration gradients the performance of fpm for solving ade in heterogeneous flow fields with irregularly distributed particles is evaluated by using two numerical cases the first case considers only diffusive transport and has an analytical solution that can be used to evaluate accuracy of sph and fpm solutions the second case considers both advection and dispersion and uses a numerical solution as a reference to evaluate accuracy of sph and fpm solutions for each of the two cases several numerical experiments are conducted using multiple irregularly distributed particles with different levels of particle irregularity 2 mathematical model and numerical schemes the ade is given as 1 c t v c d c where c is the solute concentration d is a tensor of dispersion coefficient and v is a vector of seepage velocity in lagrangian coordinates this equation is written as herrera et al 2009 2 d x d t v 3 d c d t d c where x is the position of a particle the substantial derivative d c d t c t v c x represents time rate change of solute concentration along the pathline of a particle 2 1 smoothed particle hydrodynamics and particle irregularity problems in sph eqs 2 and 3 are approximated by using a finite number of particles to carry the solute concentration eq 2 governs movement of the particles and eq 3 defines the diffusive transport component the solution of eq 3 for the particles is evaluated using a kernel interpolation approximation the approximations of concentration c x and its spatial derivative c x at position x are written as liu and liu 2010 4 c x ω c x w x x h d x 5 c x ω c x w x x h d x where denotes the approximations w is a kernel function h is the smoothing length that defines a support domain of the kernel function x prime is the location of a neighbor particle of the particle at x within the support domain ω is the support domain whose radius is in general several times of h and the gradient w x x h w x x h x of w is evaluated at x information of all the particles inside the support domain is used to determine the information of the particle at x liu and liu 2003 according to liu and liu 2010 the kernel function w should be sufficiently smooth and satisfies the following properties 1 positive property w x x 0 2 unity property ω w x x h d x 1 3 symmetric property ω x x w x x h d x 0 4 delta function property lim h 0 w x x h δ x x 5 compact condition w x x 0 for x x kh with k being an integer and 6 monotonical property that w monotonically decreases with h for illustrative purpose the commonly used cubic b spline function is chosen as the kernel function monaghan 2005 in this study 6 w w 0 2 3 q 2 q 3 2 0 q 1 2 q 3 6 1 q 2 0 q 2 where q x x h is the relative distance between two particles at locations x and x w 0 is a normalization factor with w 0 1 h for a one dimensional space w 0 15 7 π h 2 for a two dimensional space and w 0 3 2 π h 3 for a three dimensional space other kernel functions e g gaussian function and wendland function dehnen and aly 2012 can also be used for fpm and the discussion and conclusions of this study based on the cubic b spline function are expected to be valid for other kernel functions the integrals in eqs 4 and 5 around particle i at position x i within the support domain can be approximated as a summation over all neighbor particles j at position x j these approximations are called particle approximations and they are 7 c x i c i ω c x w x x h d x j 1 n b m j ρ j c x j w i j j 1 n b m j ρ j c j w i j 8 c x i c i ω c x w x x h d x j 1 n b m j ρ j c x j j w i j j 1 n b m j ρ j c x j i w i j j 1 n b m j ρ j c j i w i j where ci c x i cj c x j nb is the number of particles within the support domain of particle i wij w x i x j h i wij is the gradient of the kernel function for particle i and satisfies i wij j wij and mj and ρ j are the mass and density of particle j respectively if we denote m j ρ j v j to represent the volume associated with particle j the particle number density nj can be defined as n j 1 v j i e the number of particles per unit volume following tartakovsky and meakin 2005 we have n j k w x j x k h where x k is the location of neighbor particles of particle j eq 8 has a potential problem that for a constant c in space ci should be zero theoretically but j 1 n b m j ρ j i w i j 0 may not be numerically satisfied to resolve this problem monaghan 2005 expressed ci as 9 c i 1 φ i φ i c i c i φ i where φ i φ x i can be any differentiable function with eq 9 ci can be approximated as monaghan 2005 10 c i 1 φ i j n b m j ρ j φ j c j c i i w i j choosing ф i 1 leads to 11 c i j n b m j ρ j c j c i i w i j eq 11 is commonly used in sph since it improves particle approximation accuracy liu and liu 2006 for this reason this study used eq 11 rather than eq 8 numerical accuracy of sph depends on the two particle approximations defined in eqs 7 and 11 for c and c respectively accuracy of eq 7 is closely related to the unity and symmetric properties of the kernel function and accuracy of eq 11 depends on the anti symmetric property of the kernel s first order derivatives to show these we apply the taylor series expansion to cj c x j at particle location x i xi yi zi and retain the first order derivatives i e 12 c j c i x j α x i α c i x i α c i x j x i c i x i y j y i c i y i z j z i c i z i where α is a dimension index for the x y and z directions to analyze the error of the particle approximation given in eq 7 we substitute eq 12 to eq 7 and have 13 c i j 1 n b m j ρ j c j w i j c i j 1 n b m j ρ j w i j c x i α x i α j 1 n b m j ρ j x j α x i α w i j if the unity property j 1 n b m j ρ j w i j 1 and the symmetric property j 1 n b m j ρ j x j α x i α w i j 0 are satisfied then eq 13 becomes ci ci with a second order accuracy in terms of r x j x i where r is the distance between particles i and j whether the unity and symmetric properties of a kernel function are satisfied depends on spatial distribution of the particles this is illustrated in fig 1 with uniform and irregular particle distributions for a one dimensional problem for the uniform distribution any two neighbor particles have the same distance of δx 0 125 m based on the kernel function defined in eq 6 set h 2δx the radius of the support domain ω as 2 h 4δx and w0 1 h for the one dimensional problem fig 1 a1 show functions w i j w 0 and xj xi wij as a function of x j x i h for a number of uniformly distributed particles fig 1 a2 is similar to fig 1 a1 but for irregularly distributed particles the irregular particle distribution is generated by adding random noises to the uniformly distributed particles the random noises follow a uniform distribution within the range of 0 δx in the two figures the dashed lines represent analytical values of w i j w 0 and xj xi wij based on eq 6 the area in blue corresponds to j 1 n b m j ρ j w i j and the area in fuchsia corresponds to j 1 n b m j ρ j x j α x i α w i j in fig 1 a1 for the uniformly distributed particles with h 2δx and nb 7 for the support domain of 2h 2h the blue area estimated via j 1 n b m j ρ j w i j j 1 n b 1 n j w i j equals 1 0000 indicating that the unit property is satisfied the number nj of particles per unit volume is evaluated as n j k w x j x k h by following tartakovsky and meakin 2005 this is the case for the entire manuscript the fuchsia area estimated via j 1 n b m j ρ j x j α x i α w i j j 1 n b 1 n j x j α x i α w i j equals 0 0000 indicating that the symmetric property is satisfied in fig 1 a2 for the irregularly distributed particles with h 2δx and nb 8 for the support domain of 2h 2h similar to the estimation for the uniformly distributed particles we have j 1 n b m j ρ j w i j 1 0457 and j 1 n b m j ρ j x j α x i α w i j 0 0067 indicating that the unity and symmetric properties of the kernel are not satisfied this is not surprising if one examines the distributions of the particles and the shapes of w i j w 0 and xj xi wij based on the particle distributions because the particles are irregularly distributed the shapes of the w i j w 0 and xj xi wij functions cannot be accurately approximated this is the reason that for irregularly distributed particles the sph based approximation of a function may not be accurate to have an accurate particle approximation of the first order derivatives of concentration the first order derivatives of the kernel need to satisfy the so called anti symmetric property chen and beraun 2000 taking the first order derivatives of ci in the x direction as an example based on eq 11 of the derivative and eq 12 of the taylor series expansion of cj we have 14 c i x i j 1 n b m j ρ j c j c i w i j x i c i x i j 1 n b m j ρ j x j i w i j x i c i y i j 1 n b m j ρ j y j i w i j x i c i z i j 1 n b m j ρ j z j i w i j x i where xji xj xi yji yj yi and zji zj zi the anti symmetric property for the first order derivative of the kernel in the x direction is as follows j 1 n b m j ρ j x j i w i j x i 1 j 1 n b m j ρ j y j i w i j x i 0 and j 1 n b m j ρ j z j i w i j x i 0 if the three equations are satisfied then c i x i c x i x i with a second order accuracy in terms of r x j x i where r is the distance between particles i and j liu and liu 2010 the anti symmetric property may not be satisfied for irregularly distributed particle and this is demonstrated in figs 1 a3 and 1 a4 for function x j x i w i x w 0 which is x j i w i j x i normalized by w 0 because the x axis is multiplied by w 0 1 h the two figures are plotted using the same kernel function and particle distribution used for plotting figs 1 a1 and 1 a2 for this one dimensional problem we only examine whether j 1 n b m j ρ j x j i w i j x i 1 is satisfied i e whether the areas in red in figs 1 a3 and 1 a4 equal 1 for fig 1 a3 with the uniformly distributed particles j 1 n b m j ρ j x j i w i j x i 0 9999 which is close to 1 for fig 1 a4 with the irregularly distributed particles this term becomes 0 8060 indicating that the anti symmetric property is not satisfied the two figures demonstrate that for irregularly distributed particles the sph based approximations of the first order derivatives of concentration may not be accurate 2 2 solving ade by using sph the numerical errors of sph caused by irregularly distributed particles affects accuracy of the sph solutions of the ade based on the sph solution of the heat conduction equation cleary and monaghan 1999 espanol and revenga 2003 derived the sph integral approximation of dispersive fluxes according to alvarado rodríguez et al 2019 and herrera et al 2009 the sph approximation of eq 3 for particle i is given as 15 d c i d t 1 2 j 1 n b 1 n i j c j c i x j x i x j x i 2 i w i j α β d α β i d α β j γ x j x i α x j x i β x j x i 2 δ α β where nij 2ninj ni nj is the harmonic mean of ni and nj that are the particle density number at positions x i and x j respectively г 4 and 5 in a two and three dimensional space respectively espanol and revenga 2003 yildiz et al 2009 α and β are the dimension indices ranging from 1 to 3 i e from x to z d α β i is a component of the dispersion coefficient tensor related to particle i and δαβ is the kronecker delta function to obtain the solution of c in time this study solves eq 15 numerically using the algorithm of predictor corrector leapfrog time integration which is computationally efficient and does not require a large amount of computer memory alvarado rodríguez et al 2019 in the predictor step the particle position and associated solute concentration of the particle are advanced in a half time step and the results are used to evaluate time derivative of particle position i e seepage velocity v and the derivative of concentration at the half time step afterward in the prediction step the particle position and solute concentration are advanced for the full time step using the derivatives calculated at the half time step since the derivation of eq 15 involves the particle approximations of ci and its first order derivatives accuracy of eq 15 depends on particle distributions as discussed above for an irregular particle distribution sph accuracy improvement is needed by using the fpm method 2 3 finite particle method fpm and fpm solution of ade in comparison with sph fpm is less sensitive to spatial distribution of particles this is achieved by correcting the first order derivatives of the kernel function involved in eq 15 following xu and deng 2016 we first multiply the both sides of eq 12 by the three first order derivatives w i j x i w i j y i and w i j z i and then integrate over the support domain subsequently replacing the integrations with the particle approximations leads to 16 j 1 n b m j ρ j c j w i j x i j 1 n b m j ρ j c i w i j x i c i x i j 1 n b m j ρ j x j i w i j x i c i y i j 1 n b m j ρ j y j i w i j x i c i z i j 1 n b m j ρ j z j i w i j x i j 1 n b m j ρ j c j w i j y i j 1 n b m j ρ j c i w i j y i c i x i j 1 n b m j ρ j x j i w i j y i c i y i j 1 n b m j ρ j y j i w i j y i c i z i j 1 n b m j ρ j z j i w i j y i j 1 n b m j ρ j c j w i j z i j 1 n b m j ρ j c i w i j z i c i x i j 1 n b m j ρ j x j i w i j z i c i y i j 1 n b m j ρ j y j i w i j z i c i z i j 1 n b m j ρ j z j i w i j z i moving the first term on the right hand side of eq 16 to the left hand side gives 17 c i x i j 1 n b m j ρ j c j c i w i j x i c i x i j 1 n b m j ρ j x j i w i j x i c i y i j 1 n b m j ρ j y j i w i j x i c i z i j 1 n b m j ρ j z j i w i j x i c i y i j 1 n b m j ρ j c j c i w i j y i c i x i j 1 n b m j ρ j x j i w i j y i c i y i j 1 n b m j ρ j y j i w i j y i c i z i j 1 n b m j ρ j z j i w i j y i c i z i j 1 n b m j ρ j c j c i w i j z i c i x i j 1 n b m j ρ j x j i w i j z i c i y i j 1 n b m j ρ j y j i w i j z i c i z i j 1 n b m j ρ j z j i w i j z i writing eq 17 in the matrix form we have the particle approximations as 18 c i x i c i y i c i z i j 1 n b m j ρ j c j c i w i j x i j 1 n b m j ρ j c j c i w i j y i j 1 n b m j ρ j c j c i w i j z i j 1 n b m j ρ j x j i w i j x i j 1 n b m j ρ j y j i w i j x i j 1 n b m j ρ j z j i w i j x i j 1 n b m j ρ j x j i w i j y i j 1 n b m j ρ j y j i w i j y i j 1 n b m j ρ j z j i w i j y i j 1 n b m j ρ j x j i w i j z i j 1 n b m j ρ j y j i w i j z i j 1 n b m j ρ j z j i w i j z i c i x i c i y i c i z i this leads to the expression of the three first order derivatives of ci as 19 c i x i c i y i c i z i m 1 j 1 n b m j ρ j c j c i w i j x i j 1 n b m j ρ j c j c i w i j y i j 1 n b m j ρ j c j c i w i j z i j 1 n b m j ρ j x j i w i j x i j 1 n b m j ρ j y j i w i j x i j 1 n b m j ρ j z j i w i j x i j 1 n b m j ρ j x j i w i j y i j 1 n b m j ρ j y j i w i j y i j 1 n b m j ρ j z j i w i j y i j 1 n b m j ρ j x j i w i j z i j 1 n b m j ρ j y j i w i j z i j 1 n b m j ρ j z j i w i j z i 1 j 1 n b m j ρ j c j c i w i j x i j 1 n b m j ρ j c j c i w i j y i j 1 n b m j ρ j c j c i w i j z i where 20 m j m j ρ j x j i w i j x i j m j ρ j y j i w i j x i j m j ρ j z j i w i j x i j m j ρ j x j i w i j y i j m j ρ j y j i w i j y i j m j ρ j z j i w i j y i j m j ρ j x j i w i j z i j m j ρ j y j i w i j z i j m j ρ j z j i w i j z i if the anti symmetric property of the kernel function discussed in section 2 1 is satisfied matrix m is the identity matrix and the particle approximations of the first order derivatives of ci in eq 19 have the second order accuracy in terms of r x j x i the anti symmetric property however may not hold for irregularly distributed particles as discussed in section 2 1 to resolve the problem related to the anti symmetric property fpm assumes that there exists a modified kernel gradient m w i j x i using the modified kernel gradient and based on eq 11 we can write the particle approximation of the first order derivatives of ci as 21 c i x i j 1 n b m j ρ j c j c i m w i j x i based on eqs 19 and 21 setting c i x i c i x i gives 22 j 1 n b m j ρ j c j c i m w i j x i j 1 n b m j ρ j c j c i m w i j y i j 1 n b m j ρ j c j c i m w i j z i j 1 n b m j ρ j x j i w i j x i j 1 n b m j ρ j y j i w i j x i j 1 n b m j ρ j z j i w i j x i j 1 n b m j ρ j x j i w i j y i j 1 n b m j ρ j y j i w i j y i j 1 n b m j ρ j z j i w i j y i j 1 n b m j ρ j x j i w i j z i j 1 n b m j ρ j y j i w i j z i j 1 n b m j ρ j z j i w i j z i 1 j 1 n b m j ρ j c j c i w i j x i j 1 n b m j ρ j c j c i w i j y i j 1 n b m j ρ j c j c i w i j z i this leads to the modified kernel gradients as 23 m w i j x i m w i j y i m w i j z i j 1 n b m j ρ j x j i w i j x i j 1 n b m j ρ j y j i w i j x i j 1 n b m j ρ j z j i w i j x i j 1 n b m j ρ j x j i w i j y i j 1 n b m j ρ j y j i w i j y i j 1 n b m j ρ j z j i w i j y i j 1 n b m j ρ j x j i w i j z i j 1 n b m j ρ j y j i w i j z i j 1 n b m j ρ j z j i w i j z i 1 w i j x i w i j y i w i j z i m 1 w i j x i w i j y i w i j z i expressing eq 23 in a matrix form gives the modified kernel gradient as 24 i m w i j m 1 i w i j with this modified kernel gradient in fpm the gradient of concentration is approximated by using eq 21 rather than eq 11 accordingly eq 15 becomes 25 d c i d t 1 2 j 1 n b 1 n i j c j c i x j x i x j x i 2 i m w i j α β d α β i d α β j γ x j x i α x j x i β x j x i 2 δ α β and it provides the fpm solution of the ade the only difference between eqs 15 and 25 is the use of the modified kernel gradient corrected by using matrix m this is an advantage of fpm because eq 25 of fpm can be implemented in a straightforward way based on eq 15 of the sph implementation when m is a unity matrix the fpm implementation is identical to the sph implementation fig 1 b illustrates why fpm can yield more accurate results than sph for irregularly distributed particles fig 1 b uses the same particles used for plotting fig 1 a4 but uses the modified first order derivative of the kernel function i e x j x i w i x m w 0 without the modified derivative j 1 n b m j ρ j x j i w i j x i 0 8060 fig 1 a4 with the modified kernel derivative j 1 n b m j ρ j x j i m w i j x i 1 0000 fig 1 b shows that the modified kernel derivative produces an extra area in the right half of the function to compensate the missing area in the left half of the function the extent of this compensation depends on the level of irregularity of particle distributions for extremely irregular particles fpm results may still be inaccurate as shown in the numerical examples discussed in section 3 the modified kernel gradient i m w i j can be obtained by using eq 24 or solving a system of equation m i m w i j i w i j the latter approach is used in this study either approach makes fpm more computationally expensive than sph more importantly if the matrix is ill conditioned or even singular for cases with highly irregular particles fpm may suffer from numerical instability or early termination liu and liu 2010 this occurs in the numerical experiments in section 3 of this study when fpm is used to simulate a solute transport problem with both diffusion and advection dispersion in a heterogeneous field of hydraulic conductivity to resolve this problem we adopt the decoupled fpm method developed by zhang and liu 2018 the method uses matrix m 26 m j m j ρ j x j i w i j x i 0 0 0 j m j ρ j y j i w i j y i 0 0 0 j m j ρ j z j i w i j z i which contains only the diagonal elements of m using m assumes that the diagonal elements of m dominate over the off diagonal terms e g j n b m j ρ j x j i w i j x i dominates over the other two terms j n b m j ρ j y j i w i j x i and j n b m j ρ j z j i w i j x i in the first row of matrix m evaluating the inverse of the diagonal matrix m or solving m i m w i j i w i j is straightforward and computationally efficient in this study if m i m w i j i w i j cannot be solved for a particle due to ill conditioned m associated with the particle the decoupled fpm is used for the particle by solving m i m w i j i w i j 2 4 particle approximation errors with irregular particle distributions in this section we briefly describe the approximation errors of sph with irregular particle distributions according to monaghan 2005 meshfree approximations in sph based on the kernel interpolant integration has two sources of errors the smoothing error caused by the integral interpolation and the discretizing error caused by the summation of discrete particle in the integral the smoothing error depends on the shape and smoothing length of the kernel function using a small h value can reduce the smoothing error monaghan 2005 zhu et al 2015 the discretizing error depends on the particle number and particle distribution for uniformly distributed particle generally speaking using small h leads to small discretizing error for irregularly distributed particle using a small h value may result in a small number of neighbor particles in the corresponding support domain and thus increases discretizing error zhu et al 2015 sigalotti et al 2016 herrera and beckie 2013 to simultaneously reduce smoothing and discretizing errors requires decreasing the smoothing length and simultaneously increasing the number of particles so that the number of particles per support domain does not change on an average sense for all particles zhu et al 2015 this however is difficult to achieve in practice for an irregular particle distribution because it is difficult to estimate the number of particles within a support domain for irregularly distributed particles herrera et al 2009 for the same reason it is also difficult to use a variable smoothing length as a potential alternative solution to ensure a stable average number of neighbor particles the tradeoff between the smoothing error and the discretizing error must be considered when determining h values when using sph or fpm to solve ade with heterogeneous hydraulic conductivity the particle approximation errors increase with time because the degree of particle irregularity increases with time generally speaking the initial particle distribution is always uniform therefore at an early time the particle distribution is still uniform and the smoothing error is larger than the discretizing error with time increasing the discretizing error increases because the particle distribution becomes more and more irregular due to non uniform flow in a heterogeneous field of hydraulic conductivity therefore when solving the ade using sph and fpm the sph and fpm solutions may be similar in an early time but fpm outperforms sph when time increases this is illustrated in the numerical examples discussed below 3 numerical investigation the performance of sph and fpm is investigated by using two numerical cases of two dimensional solute transport modeling with the focus on evaluating the effects of irregular particle distributions on accuracy of the sph and fpm solutions to simulate the particle distributions driven by real flow velocity the irregular particle distributions are generated by advection process in heterogeneous hydraulic conductivity and multiple variance values of log hydraulic conductivity are used for each case to quantify the degree of irregular particle distributions the unity index q is used for particle i its unity index is defined as zhu et al 2015 27 q i j 1 n b m j ρ j w i j which according to eq 7 is the sph approximation of a constant c i 1 for particle i with a complete support domain that is not intercepted by a boundary of the simulation domain if the support domain has sufficient uniformly distributed neighbor particles then q i equals 1 otherwise q i follows a peaked distribution around 1 zhu et al 2015 after q i is evaluated for all the particles the standard deviation of q i is estimated and used to quantify the degree of particle irregularity a larger standard deviation indicates a higher degree of particle irregularity to analyze the errors of sph and fpm solutions the l2 norm of the errors relative to a reference solution an analytical solution or a numerical solution of high accuracy is calculated as 28 erro r l 2 c n c 0 c r c 0 2 n where cn c 0 is the numerical solution of normalized concentration c 0 being an initial concentration given by sph or fpm and cr c 0 is the reference solution of normalized concentration and n is the total number of particles in addition to the two numerical cases presented below an additional numerical case is considered in this study to validate our numerical codes of sph and fpm description of the additional case is given in text s1 and figs s1 s5 of the supplementary information file 3 1 diffusive transport with irregular particle distributions 3 1 1 model setup and particle generation this numerical case considers a diffusive transport problem with zero seepage velocity in a two dimensional confined domain with the size of 40 m 15 m the initial condition of the diffusive transport has an instantaneous gaussian plume the center of the gaussian plume with the maximum concentration c 0 is placed at location x 0 y 0 for the two dimensional diffusion problem the analytical solution of the normalized solute concentration c c0 is zimmermann et al 2001 alvarado rodríguez et al 2019 herrera and beckie 2013 29 c x t c 0 w 2 c 4 exp x x 0 2 a 1 y y 0 2 a 2 4 x x 0 y y 0 a 3 8 t 2 c 2 4 w 2 t c 3 2 w 4 wherea 1 2tdyy w 2 a 2 2tdxx w 2 a 3 tdxy c 2 d x x d y y d x y 2 c 3 dxx dyy c 4 4 t 2 c 2 2 t w 2 c 3 w 4 the initial plume width is set as w 1 m and the components of molecular diffusion coefficient are set as dxx dyy 1 10 6 m2 s dxy 0 the analytical solution is used to compare performance of sph and fpm with uniformly and irregularly distributed particles in terms of solving the diffusion equation for the sph and fpm simulations the total simulation time is set as t 5 000δt 500 h with the uniform time step δt 360 s this time step satisfies the relation of δ t ε h 2 d x x d y y that has been used in literature herrera and beckie 2013 herrera et al 2009 alvarado rodríguez et al 2019 where ε 0 01 is an empirical coefficient used in this study the radius of the supporting domain of sph and fpm is set as 2 h and the smoothing length h is set as h 1 5δx 0 1875 m where δx 0 125 m is used for the modflow simulation discussed below for generating three particle distributions for the diffusion transport problem we conduct three numerical experiments with the plume center placed at locations 10 m 7 m 20 m 7 m and 30 m 7 m and each experiment has its own particle distribution generated by solving an advective transport problem in the same domain of diffusive transport using the predictor corrector leapfrog time integration method discussed in section 2 2 for the advective transport problem a field of heterogeneous hydraulic conductivity is generated by using the gstat geostatistical package pebesma 2004 it is assumed that the natural logarithm of the hydraulic conductivity lnk follows the normal distribution with the mean of lnk 1 and the variance of σ ln k 2 0 1 an exponential covariance function with the correlation length of ilnk 1 m is used to generate the heterogeneous hydraulic conductivity field the flow direction is from a constant head boundary at the left side to the constant head boundary at the right side of the simulation domain the top and bottom boundaries are set as no flow boundaries the groundwater flow is assumed to be steady and modflow 2000 harbaugh 2000 is used to solve the flow problem the modflow grid has 38 400 uniform blocks and each block has the size of 0 125 m 0 125 m seepage velocity is evaluated by using a constant porosity of 0 3 over the simulation domain based on the modflow simulated flow field three sets of particle distributions one uniformly and two irregularly distributed are generated in the first set of particle distribution pd1 particles are uniformly placed at the modflow block centers the second set of particle distribution pd2 is generated by first using pd1 as the initial particle distribution and then simulating advective transport of the particles using the predictor corrector leapfrog time integration method with a constant timestep of δt 1800 s at the simulation time of t 500δt 250 h the particle distribution is saved as pd2 due to the non uniform seepage velocity in the heterogeneous field of hydraulic conductivity the particle positions of pd2 are irregular with more particles in locations where pathlines converge and less particles in locations where the pathlines diverge the third set of particle distribution pd3 is generated in a same way of generating pd2 but using the particle distribution at time t 1 000δt 500 h it should be noted that the computed flow field used for generating pd1 pd3 is not completely divergent free since divergence of the flow field for an incompressible fluid is related to the temporal change of the fluid mass over a given volume the generated particle distributions are affected by the divergence this is a main drawback of the particle methods for simulating solute transport and it also occurs for high order finite volume and finite element methods the three particle distributions are illustrated in fig 2 for an area of the size of 10 m 10 m centered at the plume centers of 10 m 7 m 20 m 7 m and 30 m 7 m fig 2 shows that pd3 is more irregular than pd2 and pd1 is uniform this is reflected in the standard deviation values of the unity index q and they are 0 0201 0 0628 and 0 072 for pd1 pd3 respectively the non zero value of 0 0201 of pd1 is caused by incomplete support domains for particles whose distances to the boundaries are less than the radius of the support domains 3 1 2 results for σ ln k 2 0 1 fig 3 plots the three concentration profiles of the normalized concentration at simulation time t 300 h along the cross section of y 7 m based on the analytical solution and the sph and fpm solutions obtained using the three particle distributions the normalized concentrations are based on the discrete particles for the irregular distributed particles since there are no particles exactly located at the line of y 7 m the particles within the range of lines of y 6 95 m and y 7 05 m are used to compare the two numerical solutions with the analytical solution for particle distribution pd1 fig 3 a since the particles are uniformly distributed the sph and fpm solutions are identical and they coincide with the analytical solution for particle distributions pd2 and pd3 with irregular particle distributions the sph solutions are larger than the analytical solutions at the centers the concentration profiles as shown in figs 3 b and 3 c the errors of the sph solution are significantly larger than those of the fpm solution indicating that fpm outperforms sph for the irregular particle distributions the comparion of sph fpm and deoupeld fpm fpm d solutions are shown in fig s6 of the supplementary information file and the figure illustrates that the fpm and fpm d solutions are visually identical examining the two dimensional plumes also indicates that fpm outperforms sph fig 4 a plots the two dimensional plumes at the simulation time of t 300 h given by the analytical solution and the sph and fpm soltuions obtained usin pd2 fig 4 b does the same for pd3 both figs 4 a and 3 b show that sph overestimates the maximum concentrantion at the plume centers the overestimations are 0 0192 for pd2 and 0 0411 for pd3 which are 6 and 13 of the analytical solution of 0 3164 this kind of overestimation was also reported in alvarado rodríguez et al 2019 and avesani et al 2015 while the overestimations also occur for fpm they are substnatially smaller being 2 and 1 of the analytical solution for pd2 and pd3 respectively the better performance of fpm is attributed to using the modified kernel gradient in eq 25 for fpm rather than the original kernel gradient in eq 15 for sph in sph the first order derivatives of concentration in space are usually underestimated because the integrations of x i x j i x w i j over the support domain are usually less than 1 e g 0 8060 in fig 1 a4 for a limited number of irregularly distributed particles as a result the time rate change of concentration in eq 25 is underestimated and this leads to the overestimation of the normalized concetration at the plume centers fig 3 and an underestimation away from the plume centers an illustration of the underestimation problem is given in fig s3 of the supplementary information file the problems of overestimation and underestimation are alleviated by fpm because of the use of the modified kernel gradients fig s4 of supplementary information file plots the convergence rates based on the relative l2 norm of the numerical solutions the figure indicates that fpm converges to the reference solution at a faster rate than sph does the high accuracy of fpm is attributed to the correction made to kernel gradient as discussed before the correction however makes fpm computationally more expensive than sph take as an example the diffusive transport problem with the irregular particle distribution of pd3 with σ ln k 2 0 1 for a computer with intel r core tm i7 8550 u cpu of 1 99 ghz the cpu time averaged over 10 repeated runs is 18 719 s and 19 678 s for sph and fpm respectively 3 1 3 results for different σ ln k 2 values in addition to σ ln k 2 0 1 this study also considers four additional fields of heterogeneous hydraulic conductivity with σ ln k 2 0 05 0 2 0 4 and 0 8 to further evaluate sph and fpm performance with an increasing level of particle irregularity caused by the increasing level of heterogeneity for each σ ln k 2 value a particle distribution is generated in the same way of generating pd3 by solving the advective transport problem and saving the particle distribution at time t 1 000δt 500 h for the particle distributions of the five σ ln k 2 values the standard deviation of the unit index qi is evaluated and the standard deviation values are plotted in fig 5 the figure also plots the standard deviation values for multiple particle distributions generated at different dimensionless times t ut i lnk for each σ ln k 2 value where u is the mean seepage velocity in x direction the figure shows that for each σ ln k 2 value the irregularity of the particle distributions increases with time due to particle movement when σ ln k 2 increases heterogeneity of hydraulic conductivity increases and irregularity of the particle distributions increases accordingly fig s7 of the supplementary information file illustrates three particle distributions generated for each σ ln k 2 value the particle irregularity for σ ln k 2 0 8 is already large enough as indicated by fig s9 that plots the standard deviation of the unit index qi for σ ln k 2 0 2 0 4 0 8 2 0 4 0 and 8 0 the five particle distributions generated for time t 1 000δt 500 h are used for the third experiment discussed above with the initial plume center placed at 30 m 7 m and the accuracy of sph and fpm is evaluated for the maximum normalized concentration at the plume center and over the entire simulation domain if a particle distribution has high irregularity its matrix m may be ill conditioned or even singular and the system equation of m i m w i j i w i j cannot be solved to resolve this problem the decoupled fpm discussed in section 2 3 is used after the diffusive transport problem is solved by using sph and fpm coupled or decoupled accuracy of the sph and fpm numerical solutions is evaluated by evaluating the difference of the maximum normalized concentration i e cmax n c 0 cmax a c 0 or equivalently cmax n cmax a c 0 between the numerical solutions and the analytical solution of eq 29 the differences are positive for both sph and fpm indicating the overestimation problem discussed above the differences are plotted with time in fig 6 a for the five σ ln k 2 values the figure shows that while accuracy of both sph and fpm deteriorates when σ ln k 2 increases fpm is more accurate than sph for each σ ln k 2 over the entire simulation time accuracy of sph and fpm is also evaluated for the entire domain by using the l2 norm of numerical error defined in eq 28 and the variation of the l2 norm with time is plotted in fig 6 b for the five σ ln k 2 values the figure shows that while the l2 norms of both sph and fpm increases when σ ln k 2 increases the l2 norm of fpm is smaller than that of sph for each σ ln k 2 over the entire simulation time fig 6 shows that the temporal patterns of the sph and fpm solutions are similar it is further noted that the temporal pattern of fig 6 a for the difference of maximum normalized concentration is different from the temporal pattern of fig 6 b for the l2 norm especially for large σ ln k 2 values taking σ ln k 2 0 8 as an example the difference increases until about 200 h and then decreases where the l2 norm keeps increasing this is due to the problem of overestimating the normalized concentration at the plume center as discussed in section 3 1 2 the overestimation may accumulate over time until the plume spreads out over the simulation domain for the l2 norm calculated for the entire domain it accounts for both overestimation and underestimation over the simulation domain and thus increases with time 3 2 advection and dispersion in heterogeneous flow field 3 2 1 model setup this numerical case is generally similar to the first numerical case described above and the differences between the two cases are described here different from the first numerical case this numerical case considers both advection and dispersion and is thus more realistic in this numerical case the flow problem is the same as that of the first numerical case described in section 3 1 the initial condition of this numerical case has an instantaneous concentration c 0 that is a constant over a square with the size of l 3 m centered at the location of 10 m 7 m the total simulation time is set as t 1 200δt with a uniform time step of δt 1800 s we consider three variance values of log hydraulic conductivity i e σ ln k 2 0 1 0 4 and 0 8 for each σ ln k 2 the initial particle distribution at t 0 is uniform and the particles are placed at the center of the modflow blocks when the particles move with the flow the particle distribution becomes more and more irregular for each σ ln k 2 the smoothing length is set as h 1 5δx 0 1875 m when implementing sph and fpm since there is no analytical solution for this numerical case to evaluate the performance of sph and fpm a reference solution is obtained by using sph with a smaller particle spacing of δx δy 0 03125 m and smoothing length of h 0 3 m these values are chosen based on the general principles discussed in section 2 4 that accurate sph solutions can be obtained by using a smoothing length h to have a large number of particles within a support domain zhu et al 2015 herrera et al 2009 figs s10 and s11 of the supplementary information file show the effects of δx and h respectively on the sph based reference solution and text s2 of the supplementary information file explains the reason of choosing δx 0 03125 m and h 0 3 m to obtain the reference solution because the particles used for the reference solution are different from those used for implementing sph and fpm in this numerical case to compare the fpm and sph solutions with the reference solution the three solutions are interpolated to the modflow grid that has uniform blocks each has the size of δx δy 0 125 m the interpolation is conducted by using the matlab scatter interpolation function scatteredinterpolant with the natural option for the function the interpolation is only used for comparing the sph and fpm solutions with the reference solution not for implementing the sph and fpm methods 3 2 2 results fig 7 plots the pathlines of the particles that are placed along the line of x 10 m within the square of the initial plume for heterogeneous fields of hydraulic conductivity with three σ ln k 2 values the pathlines for the case of σ ln k 2 0 1 are significantly more uniform than those for the cases of σ ln k 2 0 4 and σ ln k 2 0 8 for the latter two cases the converging flow zones have more particles than the diverging flow zones fig 8 plots the concentration plumes of sph fpm and reference solutions at dimensionless time t 11 21 10 95 and 10 64 for three σ ln k 2 values of 0 1 0 4 and 0 8 respectively in figs 8 a1 a3 for σ ln k 2 0 1 comparing with the reference solution sph overestimates the maximum normalized concentration by about 3 the reference solution is 0 4728 and the sph solution is 0 4872 whereas fpm overestimate the maximum normalized solution by 0 2 the fpm solution is 0 4729 for σ ln k 2 0 4 the reference solution is 0 4248 and the sph and fpm solutions are 0 4723 and 0 4333 respectively the overestimation increases to 11 and 2 for sph and fpm respectively for σ ln k 2 0 8 the reference solution is 0 3788 and the sph and fpm solutions are 0 4191 and 0 3859 respectively the overestimation remains as 11 and 2 for sph and fpm respectively these results indicate that the fpm solutions are more accurate than the sph solutions for all the three levels of particle irregularity figs 9 a 9 c plot the profiles of the reference sph and fpm solutions along y 7 5 m at different simulation dimensionless times for three σ ln k 2 values of 0 1 0 4 and 0 8 the profiles along y 7 5 m are chosen because while the initial plume center is located at 10 m 7 m the solute plume moves roughly along the line of y 7 5 due to non uniform velocity as shown in figs 8 c1 8 c3 fig 9 a shows that for σ ln k 2 0 1 the sph and fpm solutions are visually identical to the reference solution at early time of t 5 61 when time increases the sph solution deviates from the reference solutions but the fpm solution is still visually identical to the reference solution this is not surprising given that the initial particle distribution is uniform and that irregularity of the particle distribution is relatively small for σ ln k 2 0 1 when σ ln k 2 increases to 0 4 and 0 8 figs 9 b and 9 c show that the sph and fpm solutions become less accurate the sph solutions are different from the reference solutions even at the early time of t 5 48 for σ ln k 2 0 4 and t 5 32 for σ ln k 2 0 8 the fpm solutions are still accurate at the early time but becomes different from the reference solutions when time increases figs 9 a 9 b and 9 c consistently indicate that the fpm solutions are more accurate than the sph solutions for all the three simulation times and for all the three σ ln k 2 values that generate different levels of particle irregularity figs 10 a1 10 c1 plot the differences of maximum normalized concentration calculated over the entire simulation period for σ ln k 2 0 1 0 4 and 0 8 respectively the fpm solutions are more accurate than the sph solutions because fpm simulates more dispersion with the modified kernel gradients and thus alleviates the sph underestimation of time rate change of concentration the results are consistent with those of the diffusive transport problem discussed in section 3 1 2 figs 10 a2 10 c2 do the same for the l2 norm calculated for the entire simulation domain these figures lead to a conclusion that fpm is more accurate than sph over the entire simulation period for all the three σ ln k 2 values that generate different levels of particle irregularity it is noted that the l2 norm of fpm is larger than that of sph in the early simulation time t 140 h which may be resulted from the effect the numerical error introduced by the correction matrix in fpm the numerical error may be relatively large at early time when accumulative discretizing error caused by irregular particle distribution is small with small change in concentration an in depth understanding of the numerical error is warranted in a future study figs 9 and 10 indicate that accuracy of sph and fpm solutions is affected by simulation time and heterogeneity of hydraulic conductivity as discussed in section 3 1 3 for fig 5 the both factors are sources of particle irregularity for heterogenous hydraulic conductivity because uniformly distributed particles become more irregular over time and when heterogeneity of hydraulic conductivity increases when particle irregularity increases the errors of the sph based ade solutions accumulate over time and become large for larger σ ln k 2 values the fpm based solutions are more accurate than the sph based solutions because of using the modified kernel gradient given in eq 24 although the fpm based solutions also become less accurate when time increases and or hydraulic conductivity becomes more heterogeneous 4 conclusions to the best of our knowledge this study is the first attempt of using fpm to solve ade for groundwater solute transport modeling fpm is developed as an improvement of sph to obtain more accurate numerical solutions when particle distributions become irregular in groundwater modeling initially uniformly distributed particles become irregularly distributed when the particles move with groundwater flow in a heterogeneous field of hydraulic conductivity with more less particles flowing to locations where flow pathlines converge diverge for the irregularly distributed particles we demonstrate that the unity and symmetric properties of a sph kernel function and the anti symmetric property of the kernel gradient may not be satisfied the unsatisfied anti symmetric property directly affects accuracy of sph based solutions the accuracy is improved by fpm by using the modified kernel gradient that is based on the sph kernel function and its first order derivatives this makes the implementation of fpm straightforward based on existing sph codes accuracy of the sph and fpm based ade solutions is examined in two numerical cases with irregularly distributed particles the first case considers diffusion only and has an analytical solution that is used as a reference solution to evaluate accuracy of the sph and fpm solutions the second case considers both advection and dispersion and uses a numerical solution obtained using sph with a large number of particles and a large smoothing length to reduce computational errors as a reference solution accuracy of the sph and fpm solutions is evaluated with respect to the difference of maximum normalized concentration calculated at the plume center and the l2 norm calculated for the simulation domain for the limited number of numerical experiments analyzed in this study fpm outperforms sph to yield more accurate solutions in particular the overestimation problem of sph at the plume centers is substantially alleviated by using fpm which is attributed to using the modified kernel gradient in fpm the numerical errors of the sph and fpm solutions increase if particle distributions become more irregular when simulation time increases and or hydraulic conductivity becomes more heterogeneous it is found in this study that the fpm solutions are substantially more accurate than the sph solutions indicating that fpm should be used when simulation time is long and or heterogeneity of hydraulic conductivity is relatively high for highly heterogenous hydraulic conductivity a further improvement of fpm is needed which is however beyond the scope of this study this study uses a fixed smoothing length and thus the same number of particles for all numerical simulations impacts of the smoothing length and the number of particles on fpm solutions is warranted in a future study in addition this study only compares fpm with the standard sph and a comparison between fpm and improved sph methods is also warranted in a future study credit authorship contribution statement tian jiao conceptualization investigation methodology writing original draft ming ye conceptualization methodology supervision writing review editing menggui jin conceptualization supervision writing review editing jing yang writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments we thank the three anonymous reviewers for their constructive and insightful suggestions that have improved the content of the manuscript this study was supported by the national natural science foundation of china no 41877192 the first author was supported by the fundamental research funds for national universities to the china university of geosciences university of geosciences wuhan for her research at the florida state university the second author was supported by nsf grant ear 1552329 supplementary materials supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2021 104043 appendix supplementary materials image application 1 
