index,text
555,the advection dispersion equation for scalar transport is essential for the numerical modeling of many fluid dynamics problems however solutions from numerical schemes always suffer from numerical diffusion and or oscillation in this study we develop an intra cell advection tracking icat scheme to minimize numerical diffusion and preserve monotonicity for advection dispersion modeling the key idea is to introduce queues in each discretized cell and using a sequential transport rule and a flow distribution mechanism to track the scalar transport in these queues temporally and spatially the capability and limitations of icat are first investigated through three test cases compared with the results obtained from other numerical schemes the results from icat show substantially reduced numerical diffusion and agree better with analytical solutions we also employ icat to simulate the transport process of a conservative tracer in a fracture with a highly heterogeneous aperture distribution discrete flow channels in the fracture are better discerned by icat than by other numerical schemes indicating the suitability of icat for modeling tracer transport in channelized flow fields keywords advection dispersion numerical diffusion intra cell queue tracer transport flow channeling 1 introduction advection dispersion is a fundamental physical phenomenon describing the transport of certain scalar s such as heat or mass in physical systems including atmosphere ocean and porous media it has long been recognized as an essential process for many fluid dynamics problems in meteorology oceanography biology hydrology and geology with a broad range of applications in climate change contaminant transport subsurface reservoir characterization and so on patankar 1980 tang et al 1981 marshall et al 2006 pontrelli and de monte 2007 versteeg and malalasekera 2007 kumar et al 2010 radu et al 2011 hawkins et al 2017 accurate analyses of these advection dispersion processes are important for the understanding and prediction of the behavior of both natural and artificial systems as analytical solutions are only applicable to a limited number of idealized scenarios much effort has been devoted to developing numerical schemes to simulate the advection dispersion process in arbitrary flow fields a core challenge encountered in these numerical schemes is the approximation of advection terms at the interfaces between discretized cells the central difference scheme cds and the upstream difference scheme uds are two commonly used numerical schemes patankar and spalding 1972 raithby 1976 huh et al 1986 11 both have some limitations and the inappropriate use of them may lead to instability and inaccuracy rood 1987 arampatzis and assimacopoulos 1994 brasseur and jacob 2017 cds assumes that the scalar is linearly distributed between neighboring cells and therefore the scalar value at the interface between two cells can be calculated by linearly interpolating between the scalar values at the centers of the two neighboring cells the application of cds requires that the peclet number a dimensionless number defined as the ratio between advection and dispersion effects is no greater than two uds assumes that the scalar distribution is uniform in each cell and the scalar value at the interface between two cells equals the scalar value in the center of the upstream cell uds can tolerate higher peclet numbers but suffers from numerical diffusion caused by truncation error and flow field to grid skewness leonard 1979 arampatzis and assimacopoulos 1994 many numerical schemes have been developed to address the stability problem in cds and the accuracy problem in uds however efforts to correct one error in a scheme usually lead to the exacerbation of the other error rood 1987 the leapfrog scheme courant et al 1928 is developed based on cds and is second order accurate in time compared with uds leapfrog has less drastic numerical diffusion but always shows artificial oscillations the lax wendroff scheme lax 1954 lax and wendroff 1960 introduces a diffusion term to stabilize cds under high peclet number conditions and as a side effect leads to numerical diffusion and oscillation the quadratic upstream interpolation for convective kinematics quick scheme and quickest scheme leonard 1979 employ a quadratic interpolation method which involves more neighboring cells to estimate the advection term at a cell interface the results from quick and quickest are less numerically diffusive but may have oscillations with unphysically high and negative values brasseur and jacob 2017 the flux corrected transport fct scheme preserves monotonicity in the solution by combining the accurate high order but dispersive scheme with the low order monotonic but diffusive scheme however although considerably less diffusive than the results from uds the results from fct still contain scale dependent diffusion van leer 1977 roe 1986 brasseur and jacob 2017 the multidimensional positive definite advection transport algorithm mpdata uses an iterative approach based on uds to reduce numerical diffusion but the solution is not free from oscillations under shock type initial conditions smolarkiewicz 1984 an alternative approach to limiting numerical diffusion is to employ purely lagrangian approaches such as smooth particle hydrodynamics sph monaghan 2012 alvarado rodríguez et al 2019 and random walk particle tracking rwpt tompson and gelhar 1990 benson et al 2017 or hybrid lagrangian eulerian approaches such as particle in cell harlow et al 1964 morris et al 2015 such schemes have distinct advantages for limiting numerical diffusion however compared with purely eulerian approaches sph is computationally demanding and particle tracking methods may develop spurious concentration fluctuations kothe and rider 1995 boso et al 2013 in the present study a new scheme named intra cell advection tracking icat is developed in the uds framework to minimize numerical diffusion as well as to preserve stability and monotonicity for advection dispersion modeling the key idea is to track scalar transport in each discretized cell temporally and spatially by introducing queues to connect the inflow and outflow faces of the cell we present this method in both 1 d and multi dimensional flow problems in section 3 three test cases are performed to demonstrate the capability of icat and also investigate its limitations the first test case examines the transport of a conservative tracer in a one dimensional 1 d model with a pulse type injection where the effects of the courant number and the peclet number are analyzed the second test case assumes a uniform but skew to grid flow field in a two dimensional 2 d model and studies the transport behavior of a square wave the effect of the angle between the flow field and the grid is analyzed the third case investigates the transport of a pollutant cloud in a three dimensional 3 d model and the results are compared with both an analytical solution and numerical results from other schemes besides the three test cases a more realistic model involving the transport of a conservative tracer in a fracture is developed to study the potential application of icat to identify discrete flow channels in subsurface reservoirs 2 the formulation of icat 2 1 numerical diffusion in advection dispersion modeling consider the advection and dispersion of a scalar ϕ in a 1 d model fig 1 a the governing equation for scalar ϕ is the continuity equation for this scalar 1 ϕ t v x ϕ x x d ϕ x where t is time vx is the flow velocity d is the hydrodynamic dispersion coefficient using the finite volume method the computational domain is discretized into non overlapping cells consider cell i in fig 1 a and assuming d is constant eq 1 can be expressed as 2 ϕ i t δ t ϕ i t u δ t δ x ϕ i 1 2 t ϕ i 1 2 t d δ t δ x 2 ϕ i 1 t 2 ϕ i t ϕ i 1 t where δt is the time step and δx is the cell length the subscript i 1 2 and i 1 2 denote the interfaces between the three cells to calculate φ i t δt we need to use the scalar values at the upstream and downstream faces of cell i i e φ i 1 2 t and φ i 1 2 t in uds and cds the two terms are estimated as 3 uds ϕ i 1 2 t ϕ i 1 t ϕ i 1 2 t ϕ i t cds ϕ i 1 2 t ϕ i 1 t ϕ i t 2 ϕ i 1 2 t ϕ i t ϕ i 1 t 2 to illustrate the mechanism of numerical diffusion generation from uds we assume that d 0 and only consider the advection process in the 1 d model in fig 1 as shown in fig 1 b at t 0 the scalar value is ϕ 0 in cell i 1 and zero in cells i and i 1 the time step δt is calculated to satisfy the courant friedrichs lewy cfl condition that the courant number c vx δt δx should be equal to or less than one meaning that the transport distance of the scalar should not be larger than one cell length in each time step for a 1 d problem the courant number happens to be the fraction of the cell length that the scalar front travels in δt since the courant number has a remarkable effect on numerical diffusion we examine two different courant number conditions c 1 and c 0 5 in fig 1 b and c respectively for c 1 the scalar moves exactly one cell length in each time step and the estimates of φ i 1 2 and φ i 1 2 from uds are correct in each time step as a result the numerical result from uds is the same as the analytical solution however for c 0 5 the result from uds shows drastic numerical diffusion fig 1 c and d show the analytical solution and the numerical result from uds respectively when c 0 5 at t 0 δt the scalar values at the upstream and downstream faces of cell i are estimated to be 0 5ϕ 0 fig 1 d by uds while the exact values are ϕ 0 and 0 according to the analytical solution in fig 1 c the incorrect estimates of φ i 1 2 and φ i 1 2 are caused by the truncation error leonard 1979 leonard 1994 and further induce the numerical diffusion at t 0 2δt in fig 1 d in general the smaller the courant number the more drastic the numerical diffusion from uds 2 2 icat scheme for 1 d advection dispersion problem as indicated in fig 1 d when the courant number is smaller than one the scalar front should move less than one cell length in each time step and form a nonuniform scalar distribution within the cells however uds smears the scalar into a single value in the entire cell and the scalar front instantaneously moves to the downstream interface of the cell causing the incorrect estimate of the scalar values at the cell interfaces a key to reduce such numerical diffusion is therefore to track the scalar transport within each cell in the proposed intra cell advection tracking icat scheme we introduce a queue in each cell at least in 1 d as discussed herein connecting the upstream interface to the downstream interface and use a sequential transport rule to track the scalar transport along the queue for a cell e the queue contains ne queue cells with 4 n e v e q e δ t where ve is the volume of cell e qe is the flow rate in cell e and is the rounding up operator for the 1 d model in fig 1 ne also equals 1 ce where ce is the courant number of cell e as icat is still a variant of uds it also requires that the courant number for each cell is not greater than one implying that ne is no less than one each queue cell has two attributes the volume which is a constant and the scalar value that evolves with the advection dispersion process the queue volume the sum of all the queue cell volumes equals the volume of the host cell and the scalar value of the host cell when such upscaled quantity is needed can be calculated as the average of the scalar values in the queue cells weighted by the queue cell volumes consider a general scenario where the 1 d computational domain is discretized into cells with different volumes and ve qe δt is not necessarily an integer for every cell fig 2 a in order to minimize the scalar mixing effect we use the following method to calculate queue cell volumes in each cell except for the first queue cell closest to the upstream interface all the other queue cells have a volume of qe δt so that the scalar front moves one queue cell length exactly one queue cell length without dispersion and approximately one queue cell length in advection dominated flow in each time step in these queue cells the volume of the first queue cell is therefore v e 1 v e n e 1 q e δ t since ne is the smallest integer greater than ve qe δt v e 1 is smaller than qe δt the fluid entering cell e would fill the first queue cell in entirety and partially fill the second queue cells fig 2 a the proportion of the scalar transporting to the first queue cell αe is the ratio between the volume of the first queue cell and the volume of the fluid entering the cell in one time step i e αe ve n e 1 qe δt qe δt in the queue the scalar is essentially represented by a piecewise constant function and transports downstream along the queue accordingly fig 2 a note that numerical diffusion still takes place in icat because the second queue cell smears a fraction of the scalar entering the host cell with the scalar transporting from the first queue cell however it is less drastic than that in uds since scalar smearing only occurs in a queue cell the physical dispersion in icat is handled in the same way as that in uds that the physical dispersion only occurs between neighboring cells and there is no physical dispersion among the queue cells in each time step we first calculate the physical dispersion term and update the scalar values in queues and then apply the sequential transport rule to calculate the advection term we present two simple examples to illustrate how icat works in 1 d fig 2 b and c since numerical diffusion is caused by the incorrect estimation of advection terms the physical dispersion is ignored in fig 2 b ve qe δt 2 is the same for all the cells ce 0 5 and therefore each cell has a queue with two queue cells in each time step the scalar travels exactly one queue cell length and no numerical diffusion is generated in fig 2 c ve qe δt is 2 2 7 and 1 3 for the three cells respectively as a result the queue for cell i 1 has two queue cells with equal volume the queue for cell i has three queue cells with αi 0 7 and the queue for cell i 1 has two queue cells with αi 1 0 3 according to the sequential transport rule we calculate the scalar distribution after δt 2δt and 3δt as shown in fig 2 c numerical diffusion occurs but is less drastic than that in uds fig 2 d vs fig 1 d since the numerical diffusion in icat is caused by scalar smearing in the second queue cell a potential mitigation to the numerical diffusion in icat is to further reduce the volume of the second queue cell and inevitably the volumes of all queue cells which can be achieved by decreasing the time step in section 3 a 1 d problem is analyzed to investigate the effect of time step on numerical diffusion in different schemes 2 3 icat scheme for multidimensional advection dispersion problems in a multidimensional problem 2 d or 3 d numerical diffusion originates from not only the truncation error but also the skewness between the flow field and the underlying numerical grid for a multidimensional model with a skew to grid flow field a cell may have multiple inflow faces i e the interfaces to upstream cells and multiple outflow faces i e the interfaces to downstream cells scalars entering a cell through different inflow faces are smeared if they flow into the same queue cell therefore an intra cell queue which is essentially a 1 d construct and the associated sequential transport rule cannot accurately track the scalar transport in this scenario we develop the following scheme to extend icat to multidimensional flow problems general strategy we attach a queue to each inflow face of a given cell to separately track the scalar entering the cell in different directions note that an outflow face of a given cell is an inflow face of a downstream cell therefore there exists one and only one queue on each inter cell interface and we use this queue to track scalar concentration advances in the cell that is downstream to the interface fig 3 a as multiple inflow and outflow faces are involved we need to determine the allocation of fluid from each intra cell queue to the multiple outflow faces of the same cell in a cell with m in inflow faces and m out outflow faces there are m in m out possible combinations of inflow outflow pairs we introduce a heuristic flow distribution mechanism to allocate the fluid flow among these pairs by prioritizing the pairs that imply flow directions that are more consistent with the overall flow velocity of this cell the length and volume of the queues take a cell with two inflow faces and two outflow faces as an example fig 3 a each inflow face owns a queue and the length of the queue is determined by the following equations similar to eq 4 5 v i f q i f v i g 1 g m in q i g 6 n i f v i f q i f δ t where v i f and n i f are the total volume and the length of the queue attached to the f th inflow face of cell i respectively vi is the volume of cell i q i f is the flow rate at the inflow face m in is the number of inflow faces in cell i note that in a 1 d scenario there is only one queue in each cell and the queue volume equals the volume of the cell in this 2 d case the cell has two queues and the volume of the cell is distributed between the two queues in proportion to flow rates through the two corresponding interfaces the volumes of queue cells in each queue are then calculated in the same way as that introduced for 1 d within each queue the scalar transport can be simplified to a 1 d problem and be tracked using the sequential transport rule note that eqs 5 and 6 dictates that the two queues in cell i in fig 3 a have the same length ni 1 ni 2 the heuristic flow distribution mechanism for flow allocation for cell i we first approximate the overall flow velocity vector of cell i v i as 7 v i f 1 m in m out v i f 2 where v i f is the flow velocity at the f th face in cell i as mentioned before the inflow outflow pair that implies a flow direction that is most consistent with the overall flow velocity has the highest priority in the fluid allocation for each inflow outflow pair we calculate the angle between the implied flow direction and the overall flow velocity as the priority factor for this pair 8 θ i a b cos 1 v i v i a v i b v i v i a v i b where v i a and v i b denote the flow velocities at the inflow face a and the outflow face b respectively we compute θ for all the pairs and sort the pairs in an ascending order of θ the allocation proceeds going down the list for each inflow outflow pair we allocate the smaller of 1 the remaining inflow rate to be allocated on the inflow face in the pair and 2 the remaining outflow rate on the outflow face in the pair to the inflow outflow pair we then subtract the allocated amount from both the inflow and outflow rates on the two faces before processing the next pair in the sorted list when an inflow face is depleted or an outflow face is full we remove all the pairs that involve the inflow face or outflow face from the list due to the mass conservation condition in each cell this process completes when all the inflow faces are depleted and all the outflow faces are filled we use an example to show the fluid allocation in the 2 d cell in fig 3 a assume that the flow field is uniform v i 1 v i 3 v i 2 v i 4 with v i 1 v i 2 and that cell is square meaning that qi 1 qi 3 qi 2 qi 4 according to the above flow distribution mechanism the priority factors of the four inflow outflow pairs are sorted as θi 1 4 θi 2 3 0 θi 1 3 θi 2 4 and the flow rate for each pair is allocated as shown in fig 3 b based on the flow allocation results the scalar value at face 4 is ϕ i 1 n i 1 since face 3 receives fluid scalar from both face 1 qi 1 qi 4 and face 2 qi 2 the scalar value at face 3 is therefore equal to w ϕ i 1 n i 1 1 w ϕ i 2 n i 2 where w qi 1 qi 4 qi 3 an example of icat in a 2 d advection problem we use a 2 d model to demonstrate tracking of a scalar in icat fig 4 at t 0 the scalar value is ϕ 0 in cell i and zero in other cells according to the flow distribution mechanism the scalar from the queue attached on face 1 will flow to face 4 and that from the queue attached on face 2 will flow to face 3 according to the sequential transport rule the scalar moves one queue cell length in each queue in one time step after four time steps the scalar in cell i flows to cell l for this scenario with the angle between the flow field and the grid equal to 45 no numerical diffusion is caused by the flow field to grid skewness numerical diffusion would still occur if the angle is neither 0 nor 45 but be much less drastic than the uds counterpart an example is given in section 3 to analyze the numerical diffusion under different angles limiting queue length to reduce memory use for real world fluid dynamics problems the velocity field is often highly heterogeneous the advection process is typically dominated by domains with high flow velocity and fine mesh resolutions are often used in these domains since the cfl condition requires that the courant number for each cell should not be greater than 1 the time step is dictated by cells within these high velocity fine mesh domains as a result the queue lengths of cells outside these dominating domains may be quite large demanding excessive memory use and undermining the efficiency of icat considering that the numerical diffusion away from these dominating domains has limited effect on the overall advection process we can improve the efficiency of icat without significant sacrifice in accuracy by using an upper limit n max to limit the queue length in general the greater the n max the less drastic the numerical diffusion a 2 d model that involves tracer transport in a fracture under a heterogeneous flow field is used to study the effect of n max in the discussions section comparison with adaptive mesh refinement amr method the tracking of tracers in icat via queues is similar to the amr method fig 2 in fact many previous studies have tried to reduce numerical diffusion by using a refined mesh smith and hutton 1982 huseby et al 2013 however since different flow velocities require different mesh resolutions in the same domain it might be difficult to create a mesh exactly adaptive to the flow field for some realistic scenarios with highly heterogenous flow fields an advantage of icat is that the queue length in a cell only depends on the flow velocity in this cell and there is no compatibility issue between neighboring cells in addition mesh refinement can only reduce numerical diffusion caused by truncation error while icat can reduce numerical diffusion from both truncation error and flow field to grid skewness 3 validation of icat against analytical solutions in this section we use three test cases to analyze the capability and limitations of icat the results from icat are compared with both analytical solutions and numerical results from other commonly used schemes including uds fct with the superbee flux limiter fct superbee and quick 3 1 transport of a conservative tracer in a 1 d model in this test case a conservative tracer is injected into a 1 d model and we use different numerical schemes to model the tracer transport fig 5 a since the tracer is conservative we only consider advection and dispersion of the tracer in the 1 d model also note that molecular diffusion is ignored since it is generally several magnitudes smaller than dispersion coefficient we use the first norm l 1 second norm l 2 and infinity norm l 3 to quantify the numerical errors from each scheme gross et al 1999 9 l 1 i 1 n ϕ i ϕ i i 1 n ϕ i 10 l 2 i 1 n ϕ i ϕ i 2 0 5 i 1 n ϕ i 2 0 5 11 l 3 max ϕ i ϕ i max ϕ i where n is the number of cells in the 1 d model φ i and ϕ i are the numerical results and analytical solutions in cell i respectively first we study the tracer transport in an advection only scenario d 0 fig 5 b shows the evolution of relative concentration φ φ0 at point a obtained from the analytical solution and different numerical schemes since the courant number c is smaller than 1 numerical diffusion is significant in the results from uds for icat the results are the same as the analytical solution if 1 c is an integer and show slight numerical diffusion otherwise with the increase of 1 c the evolutions of the peak relative concentration and the three error indices differ among the three numerical schemes fig 5 c for icat the peak relative concentration stabilizes at 1 0 when 1 c is larger than 3 while for uds and fct superbee the peak relative concentrations decrease from 1 to 0 42 and 0 88 respectively when 1 c increases from 1 to 10 to describe the evolution of the three error indices with the increase of 1 c for each error index we define the lower bound error to be the smallest error and the upper bound error to be the greatest error when 1 c varies between two consecutive integers as shown in fig 5 c the change in the lower and upper bound errors when 1 c increases from 1 to 10 is similar for l 1 l 2 and l 3 the lower bound error from uds and fct superbee gradually increases and the upper bound error from icat gradually decreases and tend to converge to zero the numerical error in icat is smaller than that in uds for any 1 c values and smaller than that in fct superbee if 1 c is larger than two according to the results in fig 5 c the numerical diffusion in icat caused by nonintegral 1 c can be reduced by reducing the time step second physical dispersion is considered and the effect of the peclet number p e v x δ x d is analyzed fig 5 d the analytical solution is derived by assuming a semi infinite domain with zero initial concentration and applying the first type inlet condition at the injection boundary van genuchten et al 2013 12 ϕ x t ϕ 0 a x t 0 t t 0 ϕ 0 a x t a x t t 0 t t 0 13 a x t 1 2 erfc x v x t 4 dt 1 2 exp v x x d erfc x v x t 4 dt where t 0 is the injection time under a relatively high peclet number condition p e 5 advection dominates the tracer transport process the results from uds suffer significant numerical diffusion while that from fct superbee and icat agree well with the analytical solution for a relatively low peclet number condition p e 0 5 physical dispersion becomes more dominant and the results from the three numerical schemes are almost the same and all coincide with the analytical solution uds is suitable for the low peclet number condition while icat and fct superbee can be used for both high and low peclet number conditions 3 2 transport of a square wave in a 2 d model the transport of a square wave in a 2 d model is a common test case to evaluate numerical schemes for advection gross et al 1999 in this case a non uniform initial concentration is assumed in a uniform flow velocity field fig 6 a without physical dispersion the square wave should move along the flow direction without changing its shape gross et al 1999 examined the performance of different numerical schemes on this square wave transport problem assuming that the angle between flow velocity and mesh grid β is 45 in the subsequent analysis we apply uds fct superbee and icat to simulate the transport of the square wave under various β values fig 6 b compares the relative concentration φ φ0 along the diagonal direction o o after 50 s of transport under different β values due to the mesh symmetry we only study the scenarios with 0 β 45 when β 45 the results from icat are identical to the analytical solution as alluded to by the example in fig 4 however when 0 β 45 numerical diffusion still occurs and the results from icat are slightly less accurate than that from fct superbee but significantly more accurate than that from uds table 1 further compares the results from icat fct superbee uds and that from other schemes reported by gross et al 1999 for the scenario with β 45 among the compared numerical schemes leapfrog quick quickest and mpdata suffer from oscillations with unphysical values greater than 1 or less than 0 and uds and eulerian lagrangian method elm suffer from significant numerical diffusion fct superbee provides a better solution with less drastic numerical diffusion while the results from icat are the same as the analytical solution 3 3 transport of a pollutant cloud in a 3 d model arampatzis and assimacopoulos 1994 investigated the transport of a pollutant cloud in a 3 d model with a flow field aligning with or forming an angle from the model grid the model mesh was chosen intentionally coarse in order to study numerical errors from different numerical schemes including uds and quick the results indicate that although quick delivers better performance than uds numerical diffusion from truncation error and flow field to grid skewness still leads to significant discrepancies between the numerical results and the analytical solution in this test case we simulate the same problem using icat to examine if the numerical diffusion could be reduced for this relatively complex problem involving both dispersion and advection in a 3 d model the model is similar to that in fig 6 a but in 3 d fig 7 three scenarios are analyzed in the first scenario the flow field aligns with the grid lines vx 1 5 m s vy vz 0 m s and the streamline passing through the center of the pollutant cloud aligns with the x axis as denoted by a 1 a 2 in the second scenario the flow field forms an angle of 45 with the grid lines vx vy 1 061 m s vz 0 m s and the streamline is denoted by b 1 b 2 in the last scenario the flow field forms an angle of 22 5 with the grid lines vx 1 38 m s vy 0 57 m s vz 0 m s and the streamline is denoted by c 1 c 2 note that the total velocity magnitude v is 1 5 m s for all the three scenarios following arampatzis and assimacopoulos 1994 the hydrodynamic dispersion coefficient d is 0 0015 m2 s and the time step δt is 0 01 s we compare the numerical solutions with the following analytical solution from arampatzis and assimacopoulos 1994 note that an alternative analytical solution in park and zhan 2001 has a different form but yields nearly identical results for this particular problem 14 ϕ x y z t ϕ 0 8 erf l x 2 x 2 dt erf l x 2 x 2 dt erf l y 2 y 2 dt erf l y 2 y 2 dt erf l z 2 z 2 dt erf l z 2 z 2 dt where x y and z are the coordinates of a point in the model lx ly and lz are the lengths of the pollutant cloud in the x y and z directions respectively x y and z are the distances between the point and the original pollutant cloud center in the x y and z directions respectively and are calculated as x x x 0 vxt y y y 0 vyt and z z z 0 vzt x 0 y 0 and z 0 are the original coordinates of the pollutant cloud center the comparison in fig 7 indicates that the results from uds show significant numerical diffusion while the results from quick show less drastic numerical diffusion but exhibit negative values icat substantially reduces the numerical diffusion especially when the angle between the flow field and the grid 0 or 45 due to the coarse mesh and the nonintegral 1 c there still exists numerical diffusion in the results from icat for the two scenarios with the angle 0 and 45 however the numerical diffusion from icat can be further reduced in the two scenarios with mesh refinement for the scenario with the angle 22 5 numerical diffusion from icat is more drastic than that for the other two scenarios among the compared numerical schemes icat shows the best performance in this test case 4 application of icat to identify flow channels along a fracture in rock the above analyses assume uniform flow fields however for real world advection dispersion problems such as tracer transport in fractured rock the flow field is often highly heterogeneous due to nonuniform spatial aperture distribution and preferential flow channels may form within the fracture brown et al 1998 fu et al 2016 guo et al 2016 in fact tracer tests have been used to identify flow channels along fracture networks in subsurface reservoirs dverstorp et al 1992 guo et al 2016 in this section we investigate the performance of different numerical schemes for such heterogeneous flow fields specifically we simulate the transport of a conservative tracer in a fracture with a randomly generated aperture distribution and seek to identify preferential flow channels between injection and production wells through the analysis of the tracer breakthrough curve at the production well we model a fracture with a circular shape as shown in fig 8 a the radius of the fracture is 15 m and the distance between the injection well and the production well is 10 m we use a spatially auto correlated aperture distribution with the correlation length 1 m mean aperture 0 1 mm and standard deviation 0 17 mm guo et al 2016 the fracture is represented by a thin layer of porous medium with an equivalent porosity guo et al 2016 the permeability of the fracture is calculated according to the cubic law witherspoon et al 1980 to simulate tracer transport we first inject water into the fracture to obtain a heterogeneous but steady flow field the circulation rate is 0 1 l min with the water dynamic viscosity 0 5 mpa s afterward we inject a conservative tracer with a concentration of φ0 through the injection well for 1000s and calculate the tracer concentration at the production well the hydrodynamic dispersion coefficient of the fracture is assumed to be constant and equals 1 10 7 m2 s benson et al 2017 zhou et al 2018 seven preferential flow channels connecting the injection and production wells are visually identified in the flow field fig 8 b since the flow distances and velocities are different among these channels tracers are likely to arrive at the production well at different times leading to multiple local peaks on the breakthrough curve at the production well therefore we can associate the preferential flow channels with the arrival times and magnitudes of the peaks on the breakthrough curve fig 8 c compares the tracer breakthrough curves at the production well obtained from icat fct superbee and uds six peaks are resolved on the breakthrough curve from icat by further analyzing the tracer transport process simulated by icat we find that flow channels 1 2 3 4 and 7 each correspond to a peak on the breakthrough curve and flow channels 5 and 6 correspond to the same peak since tracer transport times along channels 5 and 6 are similar because the numerical diffusion is more drastic in the results from fct superbee and uds than that from icat some peaks are smeared by these two schemes for fct superbee the peaks for channels 3 and 4 merge into the same peak at 3 2 h while for uds the four peaks for channels 1 2 3 and 4 merge into one broad peak compared with fct superbee and uds icat reduces the numerical diffusion in the simulation results and the preferential flow channels in the flow field can be better resolved from the breakthrough curve note that we use a constant and low hydraulic dispersion coefficient in this example to demonstrate the capability of icat in reducing numerical diffusion thereby enabling the identification of flow channels from tracer recovery in systems with much higher hydraulic dispersion coefficients the ability to identify flow channels from tracer breakthrough curves is lost due to physical dispersion which cannot be addressed by improving numerical schemes 5 discussion we discuss the effect of the queue length limit n max and the application of icat to unstructured mesh in this section as mentioned before we can use an upper limit n max of queue length to avoid excessive memory use for the heterogeneous flow field in fig 8 b tracer mainly flows through the seven preferential flow channels and numerical diffusion in locations outside these channels is expected to have little impact on the overall tracer transport process to quantitatively investigate the effect of the upper limit we repeat the simulation using different n max values in icat when n max 1 all the queues have only one queue cell meaning that the flow sequential rule is ineffective the numerical diffusion is significant as shown by the relatively diffusive tracer distribution in fig 9 a and only three peaks are resolved on the corresponding breakthrough curve with an increase in the upper limit the numerical diffusion decreases and when n max exceeds ten six peaks are resolved on the breakthrough curve of course computation time increases with the increase in the upper limit compared with the simulation with n max 1 the computation time for simulations with n max 5 10 and 100 increases by 8 2 9 2 and 250 respectively considering the simulation results in fig 9 the optimal value of n max is 10 the trade off between accuracy and efficiency in icat depends on the flow field for the heterogeneous flow field in fig 8 b an upper limit of 10 appears to be adequate for icat to provide a reasonable result of the tracer transport process without much increase in the computation time an advantage of icat is that it can be applied to any mesh and is amenable to implementation in parallel computing environments since the scheme is applied on a cell by cell basis it requires no knowledge of the global structure of the grid the estimation of advection terms at an interface only needs the scalar values in the cell that is upstream to this interface therefore icat is applicable to unstructured meshes and can be readily implemented in parallel in advection dispersion schemes such as fct and quick estimating the advection terms requires scalar values in more than one layer of neighboring cells that is upstream to the interface this makes it difficult to implement such schemes in parallel codes relying on spatial domain partitioning because multiple layers of cells must be communicated between partitions we examine the applicability of icat on different meshes by repeating the simulation in fig 8 with two different mesh geometries the first mesh is generated by counterclockwise rotating the structured mesh in fig 8 a by 30 fig 10 a and the second mesh is an unstructured mesh with triangular cells fig 10 b the velocity fields for the two mesh geometries are visually very similar to the baseline as that shown in fig 8 b and therefore not repeated here for both the two meshes the breakthrough curve from icat resolves more peaks and provides better flow channel identification results than the breakthrough curves from uds and fct superbee note that only the results from uds and icat are compared in fig 10 d since fct superbee is not applicable to unstructured mesh in our implementation 6 conclusion an accurate and robust advection dispersion scheme is essential for the modeling of many fluid dynamics problems we propose a new scheme named intra cell advection tracking icat to minimize numerical diffusion as well as preserve stability and monotonicity for advection dispersion simulation with the new scheme numerical diffusion caused by truncation error and flow field to grid skewness can be substantially reduced or even eliminated the efficacy of the new scheme is demonstrated through several numerical examples involving 1 d 2 d and 3 d flow various underlying mesh configurations as well as comparison with other schemes the transport process of a conservative tracer in a fracture with random aperture distribution is simulated with icat discrete flow channels in the fracture are correctly identified from the tracer breakthrough curve for both structured and unstructured mesh scenarios compared with some other numerical schemes icat is less sensitive to mesh geometry since the numerical diffusion caused by flow field to grid skewness is reduced one limitation of the method is that it depends upon a static velocity field to enable a consistent first in first out ordering within the queues if the flow field is evolving either due to changes in boundary conditions or changes in spatial distributions in fluid properties such as in the case of multiple phases with different viscosities additional errors will be incurred as the mapping from the inlet to outlet flow will either become irrelevant or need to be updated dynamically in the case of the latter advection errors similar to those introduced by remapping from lagrangian to eulerian grids will be introduced for single phase flow in relatively static flow networks such errors will be minimal declaration of competing interest the authors declare that there is no conflict of interest acknowledgments this research was performed in support of the egs collab project taking place in part at the sanford underground research facility in lead south dakota the assistance of the sanford underground research facility and its personnel in providing physical access and general logistical and technical support is acknowledged support from the egs collab team is gratefully acknowledged this work was supported by u s department of energy geothermal technologies office and performed under the auspices of the u s department of energy by lawrence livermore national laboratory under contract de ac52 07na27344 this document is llnl report llnl jrnl 766472 draft 
555,the advection dispersion equation for scalar transport is essential for the numerical modeling of many fluid dynamics problems however solutions from numerical schemes always suffer from numerical diffusion and or oscillation in this study we develop an intra cell advection tracking icat scheme to minimize numerical diffusion and preserve monotonicity for advection dispersion modeling the key idea is to introduce queues in each discretized cell and using a sequential transport rule and a flow distribution mechanism to track the scalar transport in these queues temporally and spatially the capability and limitations of icat are first investigated through three test cases compared with the results obtained from other numerical schemes the results from icat show substantially reduced numerical diffusion and agree better with analytical solutions we also employ icat to simulate the transport process of a conservative tracer in a fracture with a highly heterogeneous aperture distribution discrete flow channels in the fracture are better discerned by icat than by other numerical schemes indicating the suitability of icat for modeling tracer transport in channelized flow fields keywords advection dispersion numerical diffusion intra cell queue tracer transport flow channeling 1 introduction advection dispersion is a fundamental physical phenomenon describing the transport of certain scalar s such as heat or mass in physical systems including atmosphere ocean and porous media it has long been recognized as an essential process for many fluid dynamics problems in meteorology oceanography biology hydrology and geology with a broad range of applications in climate change contaminant transport subsurface reservoir characterization and so on patankar 1980 tang et al 1981 marshall et al 2006 pontrelli and de monte 2007 versteeg and malalasekera 2007 kumar et al 2010 radu et al 2011 hawkins et al 2017 accurate analyses of these advection dispersion processes are important for the understanding and prediction of the behavior of both natural and artificial systems as analytical solutions are only applicable to a limited number of idealized scenarios much effort has been devoted to developing numerical schemes to simulate the advection dispersion process in arbitrary flow fields a core challenge encountered in these numerical schemes is the approximation of advection terms at the interfaces between discretized cells the central difference scheme cds and the upstream difference scheme uds are two commonly used numerical schemes patankar and spalding 1972 raithby 1976 huh et al 1986 11 both have some limitations and the inappropriate use of them may lead to instability and inaccuracy rood 1987 arampatzis and assimacopoulos 1994 brasseur and jacob 2017 cds assumes that the scalar is linearly distributed between neighboring cells and therefore the scalar value at the interface between two cells can be calculated by linearly interpolating between the scalar values at the centers of the two neighboring cells the application of cds requires that the peclet number a dimensionless number defined as the ratio between advection and dispersion effects is no greater than two uds assumes that the scalar distribution is uniform in each cell and the scalar value at the interface between two cells equals the scalar value in the center of the upstream cell uds can tolerate higher peclet numbers but suffers from numerical diffusion caused by truncation error and flow field to grid skewness leonard 1979 arampatzis and assimacopoulos 1994 many numerical schemes have been developed to address the stability problem in cds and the accuracy problem in uds however efforts to correct one error in a scheme usually lead to the exacerbation of the other error rood 1987 the leapfrog scheme courant et al 1928 is developed based on cds and is second order accurate in time compared with uds leapfrog has less drastic numerical diffusion but always shows artificial oscillations the lax wendroff scheme lax 1954 lax and wendroff 1960 introduces a diffusion term to stabilize cds under high peclet number conditions and as a side effect leads to numerical diffusion and oscillation the quadratic upstream interpolation for convective kinematics quick scheme and quickest scheme leonard 1979 employ a quadratic interpolation method which involves more neighboring cells to estimate the advection term at a cell interface the results from quick and quickest are less numerically diffusive but may have oscillations with unphysically high and negative values brasseur and jacob 2017 the flux corrected transport fct scheme preserves monotonicity in the solution by combining the accurate high order but dispersive scheme with the low order monotonic but diffusive scheme however although considerably less diffusive than the results from uds the results from fct still contain scale dependent diffusion van leer 1977 roe 1986 brasseur and jacob 2017 the multidimensional positive definite advection transport algorithm mpdata uses an iterative approach based on uds to reduce numerical diffusion but the solution is not free from oscillations under shock type initial conditions smolarkiewicz 1984 an alternative approach to limiting numerical diffusion is to employ purely lagrangian approaches such as smooth particle hydrodynamics sph monaghan 2012 alvarado rodríguez et al 2019 and random walk particle tracking rwpt tompson and gelhar 1990 benson et al 2017 or hybrid lagrangian eulerian approaches such as particle in cell harlow et al 1964 morris et al 2015 such schemes have distinct advantages for limiting numerical diffusion however compared with purely eulerian approaches sph is computationally demanding and particle tracking methods may develop spurious concentration fluctuations kothe and rider 1995 boso et al 2013 in the present study a new scheme named intra cell advection tracking icat is developed in the uds framework to minimize numerical diffusion as well as to preserve stability and monotonicity for advection dispersion modeling the key idea is to track scalar transport in each discretized cell temporally and spatially by introducing queues to connect the inflow and outflow faces of the cell we present this method in both 1 d and multi dimensional flow problems in section 3 three test cases are performed to demonstrate the capability of icat and also investigate its limitations the first test case examines the transport of a conservative tracer in a one dimensional 1 d model with a pulse type injection where the effects of the courant number and the peclet number are analyzed the second test case assumes a uniform but skew to grid flow field in a two dimensional 2 d model and studies the transport behavior of a square wave the effect of the angle between the flow field and the grid is analyzed the third case investigates the transport of a pollutant cloud in a three dimensional 3 d model and the results are compared with both an analytical solution and numerical results from other schemes besides the three test cases a more realistic model involving the transport of a conservative tracer in a fracture is developed to study the potential application of icat to identify discrete flow channels in subsurface reservoirs 2 the formulation of icat 2 1 numerical diffusion in advection dispersion modeling consider the advection and dispersion of a scalar ϕ in a 1 d model fig 1 a the governing equation for scalar ϕ is the continuity equation for this scalar 1 ϕ t v x ϕ x x d ϕ x where t is time vx is the flow velocity d is the hydrodynamic dispersion coefficient using the finite volume method the computational domain is discretized into non overlapping cells consider cell i in fig 1 a and assuming d is constant eq 1 can be expressed as 2 ϕ i t δ t ϕ i t u δ t δ x ϕ i 1 2 t ϕ i 1 2 t d δ t δ x 2 ϕ i 1 t 2 ϕ i t ϕ i 1 t where δt is the time step and δx is the cell length the subscript i 1 2 and i 1 2 denote the interfaces between the three cells to calculate φ i t δt we need to use the scalar values at the upstream and downstream faces of cell i i e φ i 1 2 t and φ i 1 2 t in uds and cds the two terms are estimated as 3 uds ϕ i 1 2 t ϕ i 1 t ϕ i 1 2 t ϕ i t cds ϕ i 1 2 t ϕ i 1 t ϕ i t 2 ϕ i 1 2 t ϕ i t ϕ i 1 t 2 to illustrate the mechanism of numerical diffusion generation from uds we assume that d 0 and only consider the advection process in the 1 d model in fig 1 as shown in fig 1 b at t 0 the scalar value is ϕ 0 in cell i 1 and zero in cells i and i 1 the time step δt is calculated to satisfy the courant friedrichs lewy cfl condition that the courant number c vx δt δx should be equal to or less than one meaning that the transport distance of the scalar should not be larger than one cell length in each time step for a 1 d problem the courant number happens to be the fraction of the cell length that the scalar front travels in δt since the courant number has a remarkable effect on numerical diffusion we examine two different courant number conditions c 1 and c 0 5 in fig 1 b and c respectively for c 1 the scalar moves exactly one cell length in each time step and the estimates of φ i 1 2 and φ i 1 2 from uds are correct in each time step as a result the numerical result from uds is the same as the analytical solution however for c 0 5 the result from uds shows drastic numerical diffusion fig 1 c and d show the analytical solution and the numerical result from uds respectively when c 0 5 at t 0 δt the scalar values at the upstream and downstream faces of cell i are estimated to be 0 5ϕ 0 fig 1 d by uds while the exact values are ϕ 0 and 0 according to the analytical solution in fig 1 c the incorrect estimates of φ i 1 2 and φ i 1 2 are caused by the truncation error leonard 1979 leonard 1994 and further induce the numerical diffusion at t 0 2δt in fig 1 d in general the smaller the courant number the more drastic the numerical diffusion from uds 2 2 icat scheme for 1 d advection dispersion problem as indicated in fig 1 d when the courant number is smaller than one the scalar front should move less than one cell length in each time step and form a nonuniform scalar distribution within the cells however uds smears the scalar into a single value in the entire cell and the scalar front instantaneously moves to the downstream interface of the cell causing the incorrect estimate of the scalar values at the cell interfaces a key to reduce such numerical diffusion is therefore to track the scalar transport within each cell in the proposed intra cell advection tracking icat scheme we introduce a queue in each cell at least in 1 d as discussed herein connecting the upstream interface to the downstream interface and use a sequential transport rule to track the scalar transport along the queue for a cell e the queue contains ne queue cells with 4 n e v e q e δ t where ve is the volume of cell e qe is the flow rate in cell e and is the rounding up operator for the 1 d model in fig 1 ne also equals 1 ce where ce is the courant number of cell e as icat is still a variant of uds it also requires that the courant number for each cell is not greater than one implying that ne is no less than one each queue cell has two attributes the volume which is a constant and the scalar value that evolves with the advection dispersion process the queue volume the sum of all the queue cell volumes equals the volume of the host cell and the scalar value of the host cell when such upscaled quantity is needed can be calculated as the average of the scalar values in the queue cells weighted by the queue cell volumes consider a general scenario where the 1 d computational domain is discretized into cells with different volumes and ve qe δt is not necessarily an integer for every cell fig 2 a in order to minimize the scalar mixing effect we use the following method to calculate queue cell volumes in each cell except for the first queue cell closest to the upstream interface all the other queue cells have a volume of qe δt so that the scalar front moves one queue cell length exactly one queue cell length without dispersion and approximately one queue cell length in advection dominated flow in each time step in these queue cells the volume of the first queue cell is therefore v e 1 v e n e 1 q e δ t since ne is the smallest integer greater than ve qe δt v e 1 is smaller than qe δt the fluid entering cell e would fill the first queue cell in entirety and partially fill the second queue cells fig 2 a the proportion of the scalar transporting to the first queue cell αe is the ratio between the volume of the first queue cell and the volume of the fluid entering the cell in one time step i e αe ve n e 1 qe δt qe δt in the queue the scalar is essentially represented by a piecewise constant function and transports downstream along the queue accordingly fig 2 a note that numerical diffusion still takes place in icat because the second queue cell smears a fraction of the scalar entering the host cell with the scalar transporting from the first queue cell however it is less drastic than that in uds since scalar smearing only occurs in a queue cell the physical dispersion in icat is handled in the same way as that in uds that the physical dispersion only occurs between neighboring cells and there is no physical dispersion among the queue cells in each time step we first calculate the physical dispersion term and update the scalar values in queues and then apply the sequential transport rule to calculate the advection term we present two simple examples to illustrate how icat works in 1 d fig 2 b and c since numerical diffusion is caused by the incorrect estimation of advection terms the physical dispersion is ignored in fig 2 b ve qe δt 2 is the same for all the cells ce 0 5 and therefore each cell has a queue with two queue cells in each time step the scalar travels exactly one queue cell length and no numerical diffusion is generated in fig 2 c ve qe δt is 2 2 7 and 1 3 for the three cells respectively as a result the queue for cell i 1 has two queue cells with equal volume the queue for cell i has three queue cells with αi 0 7 and the queue for cell i 1 has two queue cells with αi 1 0 3 according to the sequential transport rule we calculate the scalar distribution after δt 2δt and 3δt as shown in fig 2 c numerical diffusion occurs but is less drastic than that in uds fig 2 d vs fig 1 d since the numerical diffusion in icat is caused by scalar smearing in the second queue cell a potential mitigation to the numerical diffusion in icat is to further reduce the volume of the second queue cell and inevitably the volumes of all queue cells which can be achieved by decreasing the time step in section 3 a 1 d problem is analyzed to investigate the effect of time step on numerical diffusion in different schemes 2 3 icat scheme for multidimensional advection dispersion problems in a multidimensional problem 2 d or 3 d numerical diffusion originates from not only the truncation error but also the skewness between the flow field and the underlying numerical grid for a multidimensional model with a skew to grid flow field a cell may have multiple inflow faces i e the interfaces to upstream cells and multiple outflow faces i e the interfaces to downstream cells scalars entering a cell through different inflow faces are smeared if they flow into the same queue cell therefore an intra cell queue which is essentially a 1 d construct and the associated sequential transport rule cannot accurately track the scalar transport in this scenario we develop the following scheme to extend icat to multidimensional flow problems general strategy we attach a queue to each inflow face of a given cell to separately track the scalar entering the cell in different directions note that an outflow face of a given cell is an inflow face of a downstream cell therefore there exists one and only one queue on each inter cell interface and we use this queue to track scalar concentration advances in the cell that is downstream to the interface fig 3 a as multiple inflow and outflow faces are involved we need to determine the allocation of fluid from each intra cell queue to the multiple outflow faces of the same cell in a cell with m in inflow faces and m out outflow faces there are m in m out possible combinations of inflow outflow pairs we introduce a heuristic flow distribution mechanism to allocate the fluid flow among these pairs by prioritizing the pairs that imply flow directions that are more consistent with the overall flow velocity of this cell the length and volume of the queues take a cell with two inflow faces and two outflow faces as an example fig 3 a each inflow face owns a queue and the length of the queue is determined by the following equations similar to eq 4 5 v i f q i f v i g 1 g m in q i g 6 n i f v i f q i f δ t where v i f and n i f are the total volume and the length of the queue attached to the f th inflow face of cell i respectively vi is the volume of cell i q i f is the flow rate at the inflow face m in is the number of inflow faces in cell i note that in a 1 d scenario there is only one queue in each cell and the queue volume equals the volume of the cell in this 2 d case the cell has two queues and the volume of the cell is distributed between the two queues in proportion to flow rates through the two corresponding interfaces the volumes of queue cells in each queue are then calculated in the same way as that introduced for 1 d within each queue the scalar transport can be simplified to a 1 d problem and be tracked using the sequential transport rule note that eqs 5 and 6 dictates that the two queues in cell i in fig 3 a have the same length ni 1 ni 2 the heuristic flow distribution mechanism for flow allocation for cell i we first approximate the overall flow velocity vector of cell i v i as 7 v i f 1 m in m out v i f 2 where v i f is the flow velocity at the f th face in cell i as mentioned before the inflow outflow pair that implies a flow direction that is most consistent with the overall flow velocity has the highest priority in the fluid allocation for each inflow outflow pair we calculate the angle between the implied flow direction and the overall flow velocity as the priority factor for this pair 8 θ i a b cos 1 v i v i a v i b v i v i a v i b where v i a and v i b denote the flow velocities at the inflow face a and the outflow face b respectively we compute θ for all the pairs and sort the pairs in an ascending order of θ the allocation proceeds going down the list for each inflow outflow pair we allocate the smaller of 1 the remaining inflow rate to be allocated on the inflow face in the pair and 2 the remaining outflow rate on the outflow face in the pair to the inflow outflow pair we then subtract the allocated amount from both the inflow and outflow rates on the two faces before processing the next pair in the sorted list when an inflow face is depleted or an outflow face is full we remove all the pairs that involve the inflow face or outflow face from the list due to the mass conservation condition in each cell this process completes when all the inflow faces are depleted and all the outflow faces are filled we use an example to show the fluid allocation in the 2 d cell in fig 3 a assume that the flow field is uniform v i 1 v i 3 v i 2 v i 4 with v i 1 v i 2 and that cell is square meaning that qi 1 qi 3 qi 2 qi 4 according to the above flow distribution mechanism the priority factors of the four inflow outflow pairs are sorted as θi 1 4 θi 2 3 0 θi 1 3 θi 2 4 and the flow rate for each pair is allocated as shown in fig 3 b based on the flow allocation results the scalar value at face 4 is ϕ i 1 n i 1 since face 3 receives fluid scalar from both face 1 qi 1 qi 4 and face 2 qi 2 the scalar value at face 3 is therefore equal to w ϕ i 1 n i 1 1 w ϕ i 2 n i 2 where w qi 1 qi 4 qi 3 an example of icat in a 2 d advection problem we use a 2 d model to demonstrate tracking of a scalar in icat fig 4 at t 0 the scalar value is ϕ 0 in cell i and zero in other cells according to the flow distribution mechanism the scalar from the queue attached on face 1 will flow to face 4 and that from the queue attached on face 2 will flow to face 3 according to the sequential transport rule the scalar moves one queue cell length in each queue in one time step after four time steps the scalar in cell i flows to cell l for this scenario with the angle between the flow field and the grid equal to 45 no numerical diffusion is caused by the flow field to grid skewness numerical diffusion would still occur if the angle is neither 0 nor 45 but be much less drastic than the uds counterpart an example is given in section 3 to analyze the numerical diffusion under different angles limiting queue length to reduce memory use for real world fluid dynamics problems the velocity field is often highly heterogeneous the advection process is typically dominated by domains with high flow velocity and fine mesh resolutions are often used in these domains since the cfl condition requires that the courant number for each cell should not be greater than 1 the time step is dictated by cells within these high velocity fine mesh domains as a result the queue lengths of cells outside these dominating domains may be quite large demanding excessive memory use and undermining the efficiency of icat considering that the numerical diffusion away from these dominating domains has limited effect on the overall advection process we can improve the efficiency of icat without significant sacrifice in accuracy by using an upper limit n max to limit the queue length in general the greater the n max the less drastic the numerical diffusion a 2 d model that involves tracer transport in a fracture under a heterogeneous flow field is used to study the effect of n max in the discussions section comparison with adaptive mesh refinement amr method the tracking of tracers in icat via queues is similar to the amr method fig 2 in fact many previous studies have tried to reduce numerical diffusion by using a refined mesh smith and hutton 1982 huseby et al 2013 however since different flow velocities require different mesh resolutions in the same domain it might be difficult to create a mesh exactly adaptive to the flow field for some realistic scenarios with highly heterogenous flow fields an advantage of icat is that the queue length in a cell only depends on the flow velocity in this cell and there is no compatibility issue between neighboring cells in addition mesh refinement can only reduce numerical diffusion caused by truncation error while icat can reduce numerical diffusion from both truncation error and flow field to grid skewness 3 validation of icat against analytical solutions in this section we use three test cases to analyze the capability and limitations of icat the results from icat are compared with both analytical solutions and numerical results from other commonly used schemes including uds fct with the superbee flux limiter fct superbee and quick 3 1 transport of a conservative tracer in a 1 d model in this test case a conservative tracer is injected into a 1 d model and we use different numerical schemes to model the tracer transport fig 5 a since the tracer is conservative we only consider advection and dispersion of the tracer in the 1 d model also note that molecular diffusion is ignored since it is generally several magnitudes smaller than dispersion coefficient we use the first norm l 1 second norm l 2 and infinity norm l 3 to quantify the numerical errors from each scheme gross et al 1999 9 l 1 i 1 n ϕ i ϕ i i 1 n ϕ i 10 l 2 i 1 n ϕ i ϕ i 2 0 5 i 1 n ϕ i 2 0 5 11 l 3 max ϕ i ϕ i max ϕ i where n is the number of cells in the 1 d model φ i and ϕ i are the numerical results and analytical solutions in cell i respectively first we study the tracer transport in an advection only scenario d 0 fig 5 b shows the evolution of relative concentration φ φ0 at point a obtained from the analytical solution and different numerical schemes since the courant number c is smaller than 1 numerical diffusion is significant in the results from uds for icat the results are the same as the analytical solution if 1 c is an integer and show slight numerical diffusion otherwise with the increase of 1 c the evolutions of the peak relative concentration and the three error indices differ among the three numerical schemes fig 5 c for icat the peak relative concentration stabilizes at 1 0 when 1 c is larger than 3 while for uds and fct superbee the peak relative concentrations decrease from 1 to 0 42 and 0 88 respectively when 1 c increases from 1 to 10 to describe the evolution of the three error indices with the increase of 1 c for each error index we define the lower bound error to be the smallest error and the upper bound error to be the greatest error when 1 c varies between two consecutive integers as shown in fig 5 c the change in the lower and upper bound errors when 1 c increases from 1 to 10 is similar for l 1 l 2 and l 3 the lower bound error from uds and fct superbee gradually increases and the upper bound error from icat gradually decreases and tend to converge to zero the numerical error in icat is smaller than that in uds for any 1 c values and smaller than that in fct superbee if 1 c is larger than two according to the results in fig 5 c the numerical diffusion in icat caused by nonintegral 1 c can be reduced by reducing the time step second physical dispersion is considered and the effect of the peclet number p e v x δ x d is analyzed fig 5 d the analytical solution is derived by assuming a semi infinite domain with zero initial concentration and applying the first type inlet condition at the injection boundary van genuchten et al 2013 12 ϕ x t ϕ 0 a x t 0 t t 0 ϕ 0 a x t a x t t 0 t t 0 13 a x t 1 2 erfc x v x t 4 dt 1 2 exp v x x d erfc x v x t 4 dt where t 0 is the injection time under a relatively high peclet number condition p e 5 advection dominates the tracer transport process the results from uds suffer significant numerical diffusion while that from fct superbee and icat agree well with the analytical solution for a relatively low peclet number condition p e 0 5 physical dispersion becomes more dominant and the results from the three numerical schemes are almost the same and all coincide with the analytical solution uds is suitable for the low peclet number condition while icat and fct superbee can be used for both high and low peclet number conditions 3 2 transport of a square wave in a 2 d model the transport of a square wave in a 2 d model is a common test case to evaluate numerical schemes for advection gross et al 1999 in this case a non uniform initial concentration is assumed in a uniform flow velocity field fig 6 a without physical dispersion the square wave should move along the flow direction without changing its shape gross et al 1999 examined the performance of different numerical schemes on this square wave transport problem assuming that the angle between flow velocity and mesh grid β is 45 in the subsequent analysis we apply uds fct superbee and icat to simulate the transport of the square wave under various β values fig 6 b compares the relative concentration φ φ0 along the diagonal direction o o after 50 s of transport under different β values due to the mesh symmetry we only study the scenarios with 0 β 45 when β 45 the results from icat are identical to the analytical solution as alluded to by the example in fig 4 however when 0 β 45 numerical diffusion still occurs and the results from icat are slightly less accurate than that from fct superbee but significantly more accurate than that from uds table 1 further compares the results from icat fct superbee uds and that from other schemes reported by gross et al 1999 for the scenario with β 45 among the compared numerical schemes leapfrog quick quickest and mpdata suffer from oscillations with unphysical values greater than 1 or less than 0 and uds and eulerian lagrangian method elm suffer from significant numerical diffusion fct superbee provides a better solution with less drastic numerical diffusion while the results from icat are the same as the analytical solution 3 3 transport of a pollutant cloud in a 3 d model arampatzis and assimacopoulos 1994 investigated the transport of a pollutant cloud in a 3 d model with a flow field aligning with or forming an angle from the model grid the model mesh was chosen intentionally coarse in order to study numerical errors from different numerical schemes including uds and quick the results indicate that although quick delivers better performance than uds numerical diffusion from truncation error and flow field to grid skewness still leads to significant discrepancies between the numerical results and the analytical solution in this test case we simulate the same problem using icat to examine if the numerical diffusion could be reduced for this relatively complex problem involving both dispersion and advection in a 3 d model the model is similar to that in fig 6 a but in 3 d fig 7 three scenarios are analyzed in the first scenario the flow field aligns with the grid lines vx 1 5 m s vy vz 0 m s and the streamline passing through the center of the pollutant cloud aligns with the x axis as denoted by a 1 a 2 in the second scenario the flow field forms an angle of 45 with the grid lines vx vy 1 061 m s vz 0 m s and the streamline is denoted by b 1 b 2 in the last scenario the flow field forms an angle of 22 5 with the grid lines vx 1 38 m s vy 0 57 m s vz 0 m s and the streamline is denoted by c 1 c 2 note that the total velocity magnitude v is 1 5 m s for all the three scenarios following arampatzis and assimacopoulos 1994 the hydrodynamic dispersion coefficient d is 0 0015 m2 s and the time step δt is 0 01 s we compare the numerical solutions with the following analytical solution from arampatzis and assimacopoulos 1994 note that an alternative analytical solution in park and zhan 2001 has a different form but yields nearly identical results for this particular problem 14 ϕ x y z t ϕ 0 8 erf l x 2 x 2 dt erf l x 2 x 2 dt erf l y 2 y 2 dt erf l y 2 y 2 dt erf l z 2 z 2 dt erf l z 2 z 2 dt where x y and z are the coordinates of a point in the model lx ly and lz are the lengths of the pollutant cloud in the x y and z directions respectively x y and z are the distances between the point and the original pollutant cloud center in the x y and z directions respectively and are calculated as x x x 0 vxt y y y 0 vyt and z z z 0 vzt x 0 y 0 and z 0 are the original coordinates of the pollutant cloud center the comparison in fig 7 indicates that the results from uds show significant numerical diffusion while the results from quick show less drastic numerical diffusion but exhibit negative values icat substantially reduces the numerical diffusion especially when the angle between the flow field and the grid 0 or 45 due to the coarse mesh and the nonintegral 1 c there still exists numerical diffusion in the results from icat for the two scenarios with the angle 0 and 45 however the numerical diffusion from icat can be further reduced in the two scenarios with mesh refinement for the scenario with the angle 22 5 numerical diffusion from icat is more drastic than that for the other two scenarios among the compared numerical schemes icat shows the best performance in this test case 4 application of icat to identify flow channels along a fracture in rock the above analyses assume uniform flow fields however for real world advection dispersion problems such as tracer transport in fractured rock the flow field is often highly heterogeneous due to nonuniform spatial aperture distribution and preferential flow channels may form within the fracture brown et al 1998 fu et al 2016 guo et al 2016 in fact tracer tests have been used to identify flow channels along fracture networks in subsurface reservoirs dverstorp et al 1992 guo et al 2016 in this section we investigate the performance of different numerical schemes for such heterogeneous flow fields specifically we simulate the transport of a conservative tracer in a fracture with a randomly generated aperture distribution and seek to identify preferential flow channels between injection and production wells through the analysis of the tracer breakthrough curve at the production well we model a fracture with a circular shape as shown in fig 8 a the radius of the fracture is 15 m and the distance between the injection well and the production well is 10 m we use a spatially auto correlated aperture distribution with the correlation length 1 m mean aperture 0 1 mm and standard deviation 0 17 mm guo et al 2016 the fracture is represented by a thin layer of porous medium with an equivalent porosity guo et al 2016 the permeability of the fracture is calculated according to the cubic law witherspoon et al 1980 to simulate tracer transport we first inject water into the fracture to obtain a heterogeneous but steady flow field the circulation rate is 0 1 l min with the water dynamic viscosity 0 5 mpa s afterward we inject a conservative tracer with a concentration of φ0 through the injection well for 1000s and calculate the tracer concentration at the production well the hydrodynamic dispersion coefficient of the fracture is assumed to be constant and equals 1 10 7 m2 s benson et al 2017 zhou et al 2018 seven preferential flow channels connecting the injection and production wells are visually identified in the flow field fig 8 b since the flow distances and velocities are different among these channels tracers are likely to arrive at the production well at different times leading to multiple local peaks on the breakthrough curve at the production well therefore we can associate the preferential flow channels with the arrival times and magnitudes of the peaks on the breakthrough curve fig 8 c compares the tracer breakthrough curves at the production well obtained from icat fct superbee and uds six peaks are resolved on the breakthrough curve from icat by further analyzing the tracer transport process simulated by icat we find that flow channels 1 2 3 4 and 7 each correspond to a peak on the breakthrough curve and flow channels 5 and 6 correspond to the same peak since tracer transport times along channels 5 and 6 are similar because the numerical diffusion is more drastic in the results from fct superbee and uds than that from icat some peaks are smeared by these two schemes for fct superbee the peaks for channels 3 and 4 merge into the same peak at 3 2 h while for uds the four peaks for channels 1 2 3 and 4 merge into one broad peak compared with fct superbee and uds icat reduces the numerical diffusion in the simulation results and the preferential flow channels in the flow field can be better resolved from the breakthrough curve note that we use a constant and low hydraulic dispersion coefficient in this example to demonstrate the capability of icat in reducing numerical diffusion thereby enabling the identification of flow channels from tracer recovery in systems with much higher hydraulic dispersion coefficients the ability to identify flow channels from tracer breakthrough curves is lost due to physical dispersion which cannot be addressed by improving numerical schemes 5 discussion we discuss the effect of the queue length limit n max and the application of icat to unstructured mesh in this section as mentioned before we can use an upper limit n max of queue length to avoid excessive memory use for the heterogeneous flow field in fig 8 b tracer mainly flows through the seven preferential flow channels and numerical diffusion in locations outside these channels is expected to have little impact on the overall tracer transport process to quantitatively investigate the effect of the upper limit we repeat the simulation using different n max values in icat when n max 1 all the queues have only one queue cell meaning that the flow sequential rule is ineffective the numerical diffusion is significant as shown by the relatively diffusive tracer distribution in fig 9 a and only three peaks are resolved on the corresponding breakthrough curve with an increase in the upper limit the numerical diffusion decreases and when n max exceeds ten six peaks are resolved on the breakthrough curve of course computation time increases with the increase in the upper limit compared with the simulation with n max 1 the computation time for simulations with n max 5 10 and 100 increases by 8 2 9 2 and 250 respectively considering the simulation results in fig 9 the optimal value of n max is 10 the trade off between accuracy and efficiency in icat depends on the flow field for the heterogeneous flow field in fig 8 b an upper limit of 10 appears to be adequate for icat to provide a reasonable result of the tracer transport process without much increase in the computation time an advantage of icat is that it can be applied to any mesh and is amenable to implementation in parallel computing environments since the scheme is applied on a cell by cell basis it requires no knowledge of the global structure of the grid the estimation of advection terms at an interface only needs the scalar values in the cell that is upstream to this interface therefore icat is applicable to unstructured meshes and can be readily implemented in parallel in advection dispersion schemes such as fct and quick estimating the advection terms requires scalar values in more than one layer of neighboring cells that is upstream to the interface this makes it difficult to implement such schemes in parallel codes relying on spatial domain partitioning because multiple layers of cells must be communicated between partitions we examine the applicability of icat on different meshes by repeating the simulation in fig 8 with two different mesh geometries the first mesh is generated by counterclockwise rotating the structured mesh in fig 8 a by 30 fig 10 a and the second mesh is an unstructured mesh with triangular cells fig 10 b the velocity fields for the two mesh geometries are visually very similar to the baseline as that shown in fig 8 b and therefore not repeated here for both the two meshes the breakthrough curve from icat resolves more peaks and provides better flow channel identification results than the breakthrough curves from uds and fct superbee note that only the results from uds and icat are compared in fig 10 d since fct superbee is not applicable to unstructured mesh in our implementation 6 conclusion an accurate and robust advection dispersion scheme is essential for the modeling of many fluid dynamics problems we propose a new scheme named intra cell advection tracking icat to minimize numerical diffusion as well as preserve stability and monotonicity for advection dispersion simulation with the new scheme numerical diffusion caused by truncation error and flow field to grid skewness can be substantially reduced or even eliminated the efficacy of the new scheme is demonstrated through several numerical examples involving 1 d 2 d and 3 d flow various underlying mesh configurations as well as comparison with other schemes the transport process of a conservative tracer in a fracture with random aperture distribution is simulated with icat discrete flow channels in the fracture are correctly identified from the tracer breakthrough curve for both structured and unstructured mesh scenarios compared with some other numerical schemes icat is less sensitive to mesh geometry since the numerical diffusion caused by flow field to grid skewness is reduced one limitation of the method is that it depends upon a static velocity field to enable a consistent first in first out ordering within the queues if the flow field is evolving either due to changes in boundary conditions or changes in spatial distributions in fluid properties such as in the case of multiple phases with different viscosities additional errors will be incurred as the mapping from the inlet to outlet flow will either become irrelevant or need to be updated dynamically in the case of the latter advection errors similar to those introduced by remapping from lagrangian to eulerian grids will be introduced for single phase flow in relatively static flow networks such errors will be minimal declaration of competing interest the authors declare that there is no conflict of interest acknowledgments this research was performed in support of the egs collab project taking place in part at the sanford underground research facility in lead south dakota the assistance of the sanford underground research facility and its personnel in providing physical access and general logistical and technical support is acknowledged support from the egs collab team is gratefully acknowledged this work was supported by u s department of energy geothermal technologies office and performed under the auspices of the u s department of energy by lawrence livermore national laboratory under contract de ac52 07na27344 this document is llnl report llnl jrnl 766472 draft 
556,identifying time lag between two hydrogeological time series for planning and management of water resources has a long history and is of continuing research interest many hydrogeological studies in the past have used visual inspection and cross correlogram techniques for quantifying the time lag cross correlogram techniques if not done under the transfer function framework could lead to ambiguous results in order to conduct cross correlogram analysis under the transfer function framework careful pre processing steps have to be undertaken which are often ignored in practice in this paper we propose a new approach to compare two sets of hydrogeological time series data using the visibility graph algorithm and show the advantages of using the new approach over the traditional approach application of the new approach is demonstrated by assessing the lags between rainfall and water level fluctuation in lake okeechobee florida we also present simulation studies to better understand the performance of the method for different sample sizes different underlying models and in the presence of missing values keywords time series visibility graph algorithm cross correlogram water levels precipitation 1 introduction long term planning of water resources often requires an understanding of time lag between a precipitation event and corresponding water level or flow response in a lake stream or an aquifer considering precipitation events are the primary source of recharge for groundwater and surface water resources it is critical from an operational standpoint to quantify the time lag and responses due to precipitation in these water bodies especially for lakes whose levels are artificially controlled for flood management and other ecological reasons although existence of time lag between precipitation events and water level responses is supported by empirical data quantitative assessments of time lags are typically done by visual inspection on a graphical plot e g westoff et al 2010 or using cross correlation techniques e g levanon et al 2016 cross correlation method although useful in many cases could lead to ambiguous results if not done under the transfer function framework if cross correlation is done under the transfer function framework certain assumptions such as joint bivariate stationarity of the two time series have to be met wei 2006 box et al 2008 the method also requires diagnostic checking for model adequacy which is rarely done in practice in this paper we present a non parametric method to quantify the time lag using a simple adaptation of the visibility graph algorithm vga this algorithm converts a time series into a graph although originally developed by physicists lacasa et al 2008 lacasa and luque 2010 nuñez et al 2012 it has found wide applications outside the physics literature in our adaptation we consider one of the time series e g water levels as a reference time series and create time shifted copies of the other time series of interest e g precipitation the time series original copies and the reference are then converted to graphs and their corresponding adjacency matrices calculated using vga and then compared the vga method is described in detail in section 2 1 1 data selection to illustrate the vga based approach we compiled long term hydrological time series data which include water level data of a surface water reservoir and corresponding precipitation data from nearby stations the criteria used for data selection was that the time series should have quality continuous data without gaps and real world environmental or anthropogenic significance the surface water reservoir lake selected for analysis is lake okeechobee which is the second largest natural freshwater lake within the contiguous united states covering approximately 730 square miles the primary inflows into the lake are kissimmee river and taylor creek located north of the lake fisheating creek located west of the lake and the primary outflows are the everglades caloosahatchee river and the st lucie river fig 1 water flowing in and out of the lake is controlled by human decisions which includes determining the time and frequency of opening and closing of numerous gates and locks the south florida water management district sfwmd and the us army corps of engineers usace jointly operate the lake s water control structures to achieve water levels in lake okeechobee that balance water supply flood protection and environmental health audobon florida naturalist magazine fall 2005a audobon florida naturalist magazine writer 2005b the comprehensive everglades restoration plan cerp identifies ideal lake okeechobee water level range to be between 12 feet ngvd at the end of dry season and 15 feet ngvd at the end of wet season any rise in lake level exceeding 18 5 feet ngvd may compromise the structural integrity of the herbert hoover dike surrounding the lake hence as lake levels approach 17 feet ngvd large freshwater discharges are made to the st lucie estuary to the east and caloosahatchee estuary to the west disrupting the natural salinity patterns and water chemistry of these estuaries and impacting its flora and fauna lake levels going below 12 feet ngvd can cause water shortages especially during drought years 2 method let us denote the two hydrogeological time series that we are interested in namely precipitation and water levels by p t and wl t or simply p and wl respectively in order to find the time lag between the two time series as a first step we fix one of the series say wl and obtain time shifted copies of the other series p τ 1 p τ κ the key step in our methodology is the conversion of all the above time series into graphs based on the visibility graph algorithm graphs are mathematical constructs that are used to study relationships among various objects in graph models the objects of interest are modeled as nodes or vertices and the relationships among the objects are modeled using edges or lines connecting the vertices visibility graph algorithm vga lacasa et al 2008 lacasa and luque 2010 nuñez et al 2012 is a method that extends usefulness of the techniques and focus of mathematical graph theory to characterize time series it has been shown that the visibility graph inherits several properties of the time series and its study reveals nontrivial information about the time series itself vga has become very popular e g xu et al 2008 marwan et al 2009 donner et al 2010 luque et al 2009 ahmadlou et al 2010 gao et al 2015 ahmadlou et al 2012 elsner et al 2009 zhu et al 2014 yang et al 2009 donges et al 2012 donner and donges 2012 zhang 2017 and has found wide applications outside the physics literature as evidenced by the 700 citations of the original paper the applications have ranged from health applications related to alzheimer s disease ahmadlou et al 2010 autism disorders ahmadlou et al 2012 and sleep studies zhu et al 2014 to geophysical studies donner and donges 2012 such as hurricanes elsner et al 2009 to financial applications yang et al 2009 however to the best of our knowledge our paper is the first paper in which vga has been used for time lag detection fig 2 top panel illustrates how the visibility algorithm works the time series plotted in the upper panel is an approximate sine series specifically a sine series with gaussian white noise added the values at 24 time points are plotted as vertical bars one may imagine these vertical bars as for example buildings along a straight line in a city landscape i e a city block each node in the associated visibility graph shown in the bottom panel corresponds to each time point in the series so the graph in fig 2 has 24 nodes we draw a link or an edge between a pair of nodes say ti and tj if the visual line of sight from the top of the building vertical bar situated at ti towards the top of the building bar at tj is not blocked by any intermediate buildings that is if we were to draw a line from the top of the vertical bar at ti to the top of the vertical bar at tj it should not intersect any intermediate vertical bars visibility lines corresponding to the edges in the graph are plotted as dotted lines in the figure in the upper panel for example there is no edge between t 2 and t 4 since the line of sight not shown between the top points of the vertical bars at these two time points is blocked by the vertical bar at t 3 on the other hand there is an edge between t 1 and t 3 since the corresponding visibility line shown as a dotted line does not intersect the vertical bar at t 2 more formally the following visibility criteria can be established two arbitrary data values tq yq and ts ys will have visibility and consequently will become two connected nodes of the associated graph if any other data tr yr placed between them fulfills y r y s y q y s t s t r t s t q this simple intuitive idea has been proven useful practically because of certain features exhibited by the graphs generated by this algorithm first of all they are connected since each node is connected to at least its neighbors secondly there is no directionality between the edges so that the graph obtained is undirected in addition the visibility graph is invariant under rescaling of the horizontal and vertical axes and under horizontal and vertical translations in other words the graph is invariant under affine transformations of the original time series data in mathematical notation any graph with n nodes could be represented by its n n adjacency matrix a which consists of 0 s and 1 s the i j th element of a is 1 if there is an edge connecting the ith and the jth node 0 otherwise two graphs g 1 and g 2 can be be compared by the metric distance a g 1 a g 2 2 between their corresponding adjacency matrices a g 1 and a g 1 here 2 called the frobenius norm of a matrix is the square root of the sum of the squares of the elements of the matrix that is the square root of the trace of the product of the matrix with itself in mathematical notation if d denotes the matrix a g 1 a g 2 and dij the ijth element of d with i j 1 t then a g 1 a g 2 2 d 2 trace d d t i 1 t j 1 t d i j 2 our proposed method to assess the time lag between the two hydrogeological time series p and wl using the visibility graph approach is as follows convert the wl time series into a visibility graph and obtain its corresponding adjacency matrix awl consider time shifted copies of the p time series p τ 1 p τ κ each shifted in time by a lag from the set τ 1 τ κ convert these time shifted copies of p into their visibility graphs and obtain the corresponding adjacency matrices a p τ 1 a p τ κ we determine the copy a p τ s for which the frobenius norm a w l a p τ s 2 is minimized the time lag between the two original hydrogeological series is then taken as τs we further illustrate our method using the plots in fig 3 the time series in the top panel ts a is a series of 50 values approximately based on a sine function that is a sine series with some white noise added t s a t 100 sin 2 π f t w t where f 80 1000 w t n 0 25 2 the time series ts b plotted in the middle panel of fig 2 is derived from ts a as follows t s b t 1 3 t s a t 2 e t where e t n 0 5 2 that is ts b is derived by shifting ts a to the right by two units by reducing the amplitude to one third that of ts a and adding some white noise in other words ts a and ts b have roughly the same shape although their amplitudes are different and one is shifted by two time units relative to the other as seen in the figure one may think of ts a and ts b as two time series one affecting the other since ts b is shifted to the left physically we would think of ts b affecting ts a e g ts b as precipitation and ts a as water levels physically water levels and precipitation never take negative values so if one really wants to think of ts a and ts b as water levels and precipitation one could think of them as mean subtracted and scaled appropriately we considered time shifted copies of ts b with time shifts from the following set 0 1 2 20 vga was applied and adjacency matrices for the corresponding graphs were obtained distance measure based on the frobenius norm for the time shifted copies of ts b compared to the reference ts a are plotted in the bottom panel of fig 2 the distance measure is minimized at 2 which was the lag that we set a priori thus in this illustrative example the lag was correctly identified by the method that we proposed 3 comparison with existing method currently existing method for detecting lags between two time series is based on cross correlograms often in practice cross correlograms are applied to the original two time series and lag is determined as the point corresponding to the peak maximum positive or maximum negative correlation however this approach could lead to ambiguities and is not the best recommended approach from a statistical point of view a better approach involves a pre processing step which assures that the two time series used in the cross correlogram analysis are both stationary and white noise the justification for this pre processing step can be understood under the transfer function framework consider two time series yt and xt with yt affected by lagged values of xt the relationship between the two series may be modeled under the transfer function framework as y t ν b x t where ν b j 0 ν j b j here b denotes the back shift operator b j x t x t j ν b is referred to as the transfer function for finite samples from real life examples a rational form is assumed for ν b 1 ν b ω b b b δ b here ω and δ are polynomial functions with finite number of non zero coefficients and b represents the actual lag between the two series our main goal is to estimate b the time delay rewriting eq 1 as δ b ν b ω b b b and expanding we will see that ν j 0 for j 0 b 1 and νj 0 for j b thus in this framework b is determined as the index of the first non zero coefficient νj the above procedure is easy if there is a way to estimate the coefficients νj s as described below cross correlations provide a way to estimate the coefficients νj s cross correlation between the two series at lag k is defined as ρ x y k γ x y k σ x σ y where γxy k is the cross covariance between xt and yt σx and σy are standard deviations of xt and yt respectively if the following two conditions c1 xt and yt are jointly bivariate stationary c2 xt is white noise series are simultaneously met then there exists a scaled relationship between νk and ρxy k 2 ν k σ y σ x ρ x y k thus if c1 and c2 are met then based on eq 2 we may determine which coefficients νj are zero and which are non zero this in turn will help us to determine the lag b putting it all together we may summarize that if conditions c1 and c2 are met then the lag b is the index of the first statistically non zero cross correlation term in other words we may use a cross correlogram to determine the lag the key point here is that the conditions c1 and c2 have to be met in order to apply the cross correlogram method c1 and c2 are simultaneously met if both yt and xt are white noise series one way to assure this in practice is to fit appropriate models ar ma arma or arima separately to xt and yt and use the corresponding residuals to plot the cross correlogram because if the fitted model is accurate then the residuals are white noise this necessitates an extra step in the process considering several models for xt and yt and fitting the most appropriate model at least appropriate enough to generate white noise as residuals this key step known as pre whitening xt and yt is often ignored in practice when cross correlograms are used which in turn could lead to erroneous conclusions one advantage of using the vga based method proposed in this paper is that the pre whitening step is not necessary for the new method we illustrate all the above concepts using the following simulated example we simulated xt from a arima 1 1 0 model that is one ar coefficient one degree of differencing and no ma coefficients ar coefficient was set to 0 7 the sample size was 75 yt was generated as y t 16 0 85 x t 3 1 4 x t 4 e t e t n 0 0 02 note that there are two a priori set lags i e lags at 3 and 4 for this example pre whitening was done for both series by fitting an arima 1 1 0 model and using the residuals autocorrelogram of the residuals showed correlation of one at lag 0 and zero everywhere else indicating that the residuals were indeed white noise the left and middle panels in fig 4 below show cross correlograms without and with pre whitening and the right panel shows the frobenius norm from the vga based method at various lags in the first correlogram the maximum correlation is at lag 3 ρxy 3 0 967 with next two highest correlations at lag 2 ρxy 2 0 959 and lag 4 ρxy 4 0 957 respectively first of all there is ambiguity about the number of lags to be picked based on the first correlogram if we decide to pick lags with the two largest correlations then we will correctly pick lag 3 but incorrectly pick lag 2 note that in the model for yt the effect for lag 4 1 4 was higher than the effect for lag 3 however the first correlogram ranked lag 4 at the third place cross correlogram based on pre whitened series middle panel gives a more clear cut answer only correlations at lags 3 and 4 are statistically different from zero with the one at lag 4 prominently greater than the one at lag 3 in this case the correlogram identified the lags correctly vga based method s frobenius norm is lowest at lag 4 and second lowest at lag 3 the norm value at lag 3 is relatively much closer to that at lag 4 compared to the next nearest values thus in this case also we correctly picked the a priori set lags note that vga based method identified the correct lag without pre whitening while the pre whitening step was necessary for the cross correlogram based method 4 simulations 4 1 sample size we conducted monte carlo simulations to assess the performance of the vga based method as we varied some of the parameters of the two time series ts a and ts b considered in the second section the parameters that we considered were a the ratio of the amplitudes between the two simulated series ts a and ts b b the variance for the noise term rnorm n 0 in the series ts a indicated by and c the variance for the noise term rnorm n 0 in the series ts b note that we did all the simulations in the r statistical software and above we borrowed from r language the term rnorm n 0 sd which stands for n data points from the normal density with mean 0 and standard deviation sd for each simulation scenario considered in this section that is for each set of the above parameters 1000 pairs of ts a and ts b were generated and for each pair time lag was assessed based on the proposed method and compared with the lag that was set a priori the performance of the method was assessed based on the percentage of times that the a priori lag was correctly identified the a priori lags that we considered for each scenario were 2 5 10 and 15 we assumed that in typical examples from hydrogeology 2 will be a small lag and 15 will be a very large lag the reason for considering the ratio of amplitudes was that even if two hydrogeological time series are roughly of the same shape with only a lag between them their amplitudes i e roughly their sizes are often vastly different for ts a and ts b used in the introductory illustrative example in the second section the ratio of their amplitudes was 1 3 one of the questions that was addressed in our simulations was whether our method was still good if we changed this ratio drastically e g to 1 9 another question that we thought should be addressed is that whether the proposed method works only for smooth periodic time series such as the sine series increasing the variance for the noise term in ts a makes it less like a sine series finally increasing the variance of the noise term in ts b makes the shape of ts b quite different from that of ts a and by doing so in our simulations we also addressed the performance of the method in such scenarios our hypothesis was that if we changed the above mentioned parameters to make the relationship between ts a and ts b less ideal than in the illustrative example in the second section the performance of the method will be worse in that sense essentially the purpose of our simulation was to see whether increasing the sample size will improve the performance in such bad scenarios and if so what would be a recommended minimum sample size that would hedge against such scenarios in order to do that we need a reference sample size that is a sample size for which the method s performance was excellent when the relationship between ts a and ts b was reasonably good by reasonably good we mean roughly the same shape and size with only a lag in between them in table 1 above we present the performance of the method for sample sizes 25 and 50 when the ratio of the amplitudes and the noise terms were kept exactly the same as in the illustrative example since table 1 shows that the performance was excellent for n 50 we consider 50 as a good sample size choice if we have reasons to believe may be by visual inspection that there is a nice relationship between ts a and ts b for the next set of simulations we fixed the sample size to be 50 and varied the above mentioned parameters one at a time we varied the parameters one a time rather than simultaneously in order to achieve a meaningful representation of two hydrogeological time series one affecting the other the results of this set of simulations are presented in table 2 the first row presents the performance when the noise terms are kept the same as in the introductory illustrative example but the ratio of the amplitudes was reduced to 1 9 in this case the performance of the method became drastically worse as seen from the table in the next row we present the results when the standard deviation for the noise term for ts a was changed to 50 for the illustrative example it was 25 but the other two parameters were kept the same this was to check whether the performance became worse if the shape of both time series was not roughly like a sine series results from table 2 show that the performance is not affected in this case these results give reasons to believe that our initial choice of a sine series shape did not matter in other words we would think that the method will perform well no matter what the shapes of the two series are as long as both the series are roughly of the same shape and size the third row in table 2 shows the results when only the noise term for ts b was changed this would correspond to making the shape of ts b quite different from that of ts a in this case the performance is affected but not very much as the percentages in the third row are all still above 90 thus based on table 2 the factors that affected the performance was the ratio of the amplitudes and the noise term for ts b and among these two the effect of the former was much more severe than the latter next we checked whether the performance of the method corresponding to the scenario in the first row in table 2 that is ratio of amplitudes equals 1 9 and noise terms for ts a and ts b kept exactly the same as in the illustrative example improved with sample and if so what could be a recommended minimum sample size the results from this set of simulations are presented in table 3 as noted in the table the performance increases very much when the sample size is increased to 90 and is near perfect when the sample size is 180 the percentages for all a priori lags are 100 when the sample size is 365 finally we used simulations to also check the performance of the method when the amplitude of the time series varied seasonally it is well known that the average accumulative precipitation varies seasonally typically for several consecutive months the average accumulative precipitation is high and for several other consecutive months the average accumulative precipitation is low and similarly for water levels in order to mimic this scenario somewhat we generated ts a series using the same parameters as in table 3 except for the amplitude we divided the set of time points into three equal sets of consecutive time points so that when e g n 90 we have the initial 30 time points t 1 t 30 the middle 30 time points t 31 t 60 and finally the last 30 time points t 61 t 90 for the first and last one thirds of the time points an amplitude of 100 and for the middle one third an amplitude of 30 was used in the simulations for ts a ts b was generated with 1 9 as the ratio of amplitudes as in table 3 the results for this new set of simulations are presented in table 4 the results look very similar to that seen in table 3 indicating that the additional seasonal variation of the amplitudes did not have any effect on the performance of the method the take home message from all the simulations results presented above is that if we are considering hydrogeological time series for which measurements were made daily then an year s worth of data will be more than sufficient for the proposed method although the method will work quite well even with 6 months worth of data visually if the two time series looks clearly to be one affecting the other and of roughly the same shape and size then even 2 months worth of data will suffice note that in this section we used the term sample size to refer to the number of time points in the time series and throughout the paper we implicitly assume that the data points in all the time series are measured at equal time intervals 4 2 missing values missing values are common in all time series measurements for hydrogeological phenomena in this section we assess via simulations the performance of the proposed method in the presence of missing values for two different types of imputation methods least observation carried forward locf and mean imputation in locf imputation if a value is missing at any time point we carry forward the previous non missing value in mean value imputation we impute the average of the prior and the subsequent non missing values the missing value mechanism that we considered for our simulations was missing completely at random mcar which means that the missing values are missing exactly as the name implies completely at random there is another commonly considered for example in clinical studies missing value mechanism missing at random mar under which the missingness may depend on the previously observed outcomes under this mechanism both locf and mean imputation are known to be biased but we consider that the missingness in hydrogeological time series do not depend on the previously observed outcomes and hence the mar assumption is unrealistic and thus we did not consider mar for our simulations as a matter of fact the missingness that we have seen for real hydrogeological time series is as follows for example for a time series in which measurements are made daily non missing measurements are seen for a large chunk of consecutive time points 6 months to 2 years followed by a large chunk of data missing at a stretch several weeks or months which again is followed by a large chunk of non missing data and so on when a significant amount of data is missing for a large number of consecutive time points none of the existing imputation methods will work very well in such cases the best strategy is to analyze separately the large chunks of data with no missing values at all nevertheless we conducted the following simulations for hypothetical scenarios in all the simulations reported in this section we fixed the sample size to be 180 and we used the same noise terms for ts a and ts b as in the illustrative example in the second section the ratio of amplitudes was set to be 1 9 as in the simulations for table 3 in order to adhere to the mcar mechanism we randomly set either 9 or 18 or 27 or 36 values to be missing 9 18 27 and 36 correspond to 5 10 15 and 20 of 180 furthermore we considered scenarios where the values were set to be missing for only one time series ts a or for both if it were set to be missing for both then it was at the same time points for both which we think is the more realistic scenario the performance of the proposed method with both locf and mean imputation was near perfect when only 5 of the values that is 9 out of 180 were missing table 5 this was true regardless of whether the values were missing for only one time series or for both and also true across all a priori set lags 2 5 10 and 15 when 10 of the values that is 18 out of 180 were missing for only one time series the method did very well under both locf and mean imputation for all lags when 10 of the values were missing for both time series the performance was still very good when the lags were large 10 or 15 when the lags were small 2 or 5 the performance with both imputation methods was still good but not as good as when the lags were large for example when 10 values were missing and when the lag was 2 the performance with locf was 97 and 92 respectively depending on whether the values were missing for only one time series or both the corresponding values for lag 10 on the other hand were even better 98 and 97 with 15 missing values 27 out of 180 the performance was still good that is in the range 90 97 with locf and mean imputation for lags 2 5 and 10 irrespective of whether it was missing for only one or for both time series although of course if it was missing only for one time series it was better however when the a priori set lag was 15 the performance with locf was weak 84 when 15 values were missing for both time series it was still good 96 with locf when only one time series had 15 missing values and with mean imputation also 92 and 98 with 20 missing values the method worked well under both types of imputations and for all lags only when one time series had missing values when both time series had 20 missing values the performance of locf was not good with small lags 84 for lag 2 and 86 for lag 5 and got worse for larger lags 77 for lag 10 and 79 for lag 15 the performance with mean imputation was slightly better 82 90 81 and 88 for lags 2 5 10 and 15 respectively but still not quite up to the mark in summary based on the above simulation results we consider it acceptable to use the proposed method in conjunction with either of the imputation methods if it is only 5 values missing for only one time series or for both with 6 15 values missing the imputation methods give good results only if it is missing for one time series with about 20 of the values missing for both time series it is definitely not recommended to use the proposed method with either of the imputations although it may be somewhat acceptable if it is missing for only one time series also in general we observed that the performance with mean imputation was slightly better except for one or two scenarios if the statistical practitioner has a preference of one method over the other it may still be recommended to use both for the proposed method at least as a sensitivity analysis finally we emphasize again the point made in the beginning of the section that if large chunks of data are missing at a stretch then the imputation methods are not likely to work in such cases it is better to focus the analysis on other chunks of data with no or very sparse missing values 4 3 multivariate simulations in all the simulations done so far the first time series was simulated using a univariate model and the second time series was generated by obtaining a lagged copy of the first series and then adding random noise to it a better strategy technically is to generate both the time series from a multivariate model and shift the second series to set a lag between the two series a priori in this subsection we present results from such simulation strategies multivariate time series that we considered were all generated from vector arma processes which include vector ar processes and vector ma processes as special cases the general form for a n vector arma p q process zt is given by z t a 1 z t 1 a p z t p ε t b 1 ε t 1 b q ε t q where ai s and bj s are n n matrices i 1 p and j 1 q and zt s and ε t s are vectors with n elements elements of ε t are white noise processes i e serially uncorrelated across time however at each time point there might be correlation among the elements in this paper since we consider only bivariate time series n was set equal to 2 for all the simulations in this subsection all the matrices that we considered were diagonal however we did include a correlation among the elements within each ε t following are the models that we considered m1 x t 0 65 x t 1 0 38 ε t 1 1 ε t 1 y t 0 95 y t 1 0 62 ε t 1 2 ε t 2 y t δ x t or in matrix notation z t a z t 1 ε t b ε t 1 where z t x t y t ε t ε t 1 ε t 2 a diag 0 65 0 95 b diag 0 38 0 62 m2 x t 0 40 x t 1 1 20 x t 2 0 05 x t 3 0 35 x t 4 ε t 1 y t 0 30 y t 1 0 85 y t 2 0 04 y t 3 0 45 y t 4 ε t 2 y t δ x t in matrix notation this will be z t a 1 z t 1 a 2 z t 2 a 3 z t 3 a 4 z t 4 ε t where a 1 diag 0 40 0 30 a 2 diag 1 20 0 85 a 3 diag 0 05 0 04 and a 4 diag 0 35 0 45 m3 x t 0 40 ε t 1 1 ε t 1 y t 0 40 y t 1 ε t 2 y t δ x t in matrix notation z t a z t 1 ε t b ε t 1 where a diag 0 0 40 b diag 0 40 0 m4 x t 0 95 x t 1 ε t 1 y t 0 95 y t 1 0 95 ε t 1 2 ε t 2 y t δ x t in matrix notation z t a z t 1 ε t b ε t 1 where a diag 0 95 0 95 b diag 0 0 95 the pair of time series within m1 were same as the best fitted models for the respective time series i e rainfall and water level fluctuation from the lake okeechobee data and analysis presented below in section 5 thus model m1 was included primarily to assess the validity of the methods for the lake okeechobee data analysis and its conclusions the correlation ρ among the elements of ε t used for generating all the above bivariate series was 0 6 for each model we considered separate simulations with lags 3 and 7 set a priori for most hydrogeological time series pairs a lag of 3 would be considered as small and a lag of 7 considered large bivariate time series based on models 1 3 and 4 were generated using the varma command within the multiwave package in r achard and gannaz 2019 for bivariate time series based on model 2 mar sim command within the mar package barbosa 2015 in r was used the results from the simulations studies corresponding to various scenarios are presented in table 6 below the results are based on 1000 iterations for each simulation scenario ccf1 and ccf2 in table 6 correspond to cross correlation method without and with pre processing step respectively in all scenarios there was no substantial difference between the accuracies obtained in the lag 3 setting compared to that with lag 7 all the methods worked very well for time series pairs generated from m1 even with a sample size of 100 this justifies our use of the methods particularly the vga based method for the analysis of lake okeechobee data for simulations under m2 all three methods were comparable for both sample sizes the accuracy of all three methods were not perfect but only approximately 95 when the sample size was 100 but it reached 100 or near 100 with a sample size of 500 for the next two models only ccf2 performed very well for a sample size of 100 under m3 the accuracy of vga based method was approximately 92 and that of ccf1 was approximately 98 5 although the performances of vga based method and ccf1 were not optimal with the smaller sample size they improved substantially when a sample size of 500 was used under m4 the accuracy of vga based method was approximately 81 and that of ccf1 was approximately 43 when the a priori lag was 3 however with a sample size of 500 the performance of vga based improved substantially to 100 the performance of ccf1 method also improved but not as much as for the vga method the results for the simulations based on m3 and m4 bring out a limitation of the vga based method that its performance is dependent on sample size m3 and m4 are different from m1 and m2 in the following sense the pair of time series in m1 and m2 were both from the same type of time series models arma 1 1 for m1 and ar 4 for m2 although the coefficients differed the two time series within m3 were from different types ma 1 for the first time series and ar 1 for the second time series the two time series within m4 were also from different types ar 1 for the first time series and arma 1 1 for the second time series the above set of simulations seems to suggest that when the underlying pair of time series under consideration are of two different types then vga adapted method performs very well only with large sample sizes thus if there are reasons to believe that the underlying time series are from two different types of models then the vga based method presented in this paper should be used with caution especially for smaller sample sizes next we present analysis of lag between rainfall and water level fluctuation in lake okeechobee to illustrate the methods further 5 lake okeechobee data for this analysis we selected daily water level and rainfall data from two monitoring stations located on lake okeechobee the lake and hydrologic features connected to it are one of the most studied and monitored watershed systems in united states because of its significance in regional flood management and water supply and considering it is a vital fresh water resource for florida the lake and its surrounding wetlands have a suite of monitoring stations that collect water level water quality meteorological and flow data throughout the year the lake system is part of the greater everglades watershed that stretches from the kissimmee river to florida bay with significant flows occurring through the everglades the sfwmd and acoe takes extreme measures to constantly monitor the lake levels and maintain it at optimal levels taking into consideration public s safety water demands and the health of flora and fauna in the estuaries downgradient from the lake daily rainfall and water level data used for this analysis was collected between january 1 2000 and december 31 2018 19 years rainfall data was highly skewed so a fourth root transformation was done before all analyses there were no missing values for water levels and less than 0 5 values missing for rainfall data since the percent of missing values was very small we used imputation based on least observation carried forward locf before conducting the analysis the primary goal of the analysis was to detect the time lag s between daily rainfall and daily changes in water level note that we used daily changes in water level rather than daily water level itself because lake water levels are more complex and tied to many hydrogeological factors in addition to rainfall such as evaporation surface water and groundwater inflow and outflows the number of days of rainfall for each month from january to december averaged across all 19 years were 5 1 4 8 6 2 5 4 8 1 14 2 13 5 16 1 13 2 7 1 5 4 and 5 7 respectively the average number of days with rainfall was substantially higher for the months of june july august and september these four months represent the wet season for the region further we estimated the average annual rainfall for each of the 19 years and noticed that 2014 was the wettest year the first analysis below is mainly for illustration purposes only we applied our proposed method to the wet season june through september of the wettest year 2014 the lowest value of the frobenius norm was at lag 4 norm value 706 the second lowest value at lag 6 norm value 724 the third and fourth lowest values were at lags 15 and 0 norm values were close to each other 742 and 746 it is interesting to note that our analysis indicates a primary lag of about 4 or 6 days and a secondary lag of about 15 days the primary lag may be a reflection of immediate water level response to rainfall events occurring in and around the lake while the secondary lag may be a response to rainfall occurring in the lake vicinity and an increase in inflows via the lake inlet streams caused due to rainfall events occurring upstream from the lake although not prominent there is also some indication of an immediate effect shown by the lag at 0 the above analysis was conducted for the wettest year we repeated the analysis after averaging the data across all 19 years averaging the data will be more statistically appropriate when the results are used for prediction again this analysis was done for illustrative purposes only the averaged data for rainfall and water level change are shown in the upper panel and middle panels of fig 5 respectively the frobenius norm from vga based analysis for different lags are plotted in the bottom panel of fig 5 the lowest value of the norm was at lag 4 followed by lags at 0 and 6 norm values 862 864 and 870 respectively the fourth ranked lag was at lag 15 norm value 876 it is interesting to note that the top four lags were same as that for the wettest year although in the analysis for the averaged data the immediate effect i e lag at 0 became more prominent we also conducted cross correlogram analysis for detecting lag an arma model fit with one ar coefficient and one ma coefficient i e arima 1 0 1 yielded white noise residuals for time series see appendix fig a1 the coefficients obtained with this arma 1 1 fit were the same as the ones used in the two series in the model m1 for multivariate simulations above ar alone or ma alone did not produce white noise residuals the ccf based on the residuals from the arma models are plotted in fig 6 below the significant lags detected in this case are at 0 and 1 thus ccf based analysis detects only immediate effects due to rainfall one of the primary reasons for detecting lags is to use them for prediction based on for example a time series regression in order to compare the predictive performance of the lags obtained via vga versus those obtained via ccf we fitted time series regressions with daily water level change as the dependent variable no intercept term and lagged rainfall data as independent variables in order to accommodate lag terms for rainfall we used only water level data for months of july august and september in the regressions to be precise if june water level data was included and lag 15 for example for rainfall was modelled in the regression then that would have required rainfall data from the month of may which was not used in the original vga or ccf analyses hence the prediction analysis was restricted to water level data from july august and september total 92 days having no intercept is justified as follows a regression with no intercept term in the current context interprets as zero rainfall implies zero change in water level or more technically correctly when there is no rainfall in the recent past the only change in water level is due to random variation for each regression fitted values and 95 confidence intervals for fitted values were calculated accuracy for each regression was determined as the number of actual water level change values that were within the 95 confidence interval band of the fitted values a time series regression with no intercept and vga based lags that is lags at 4 0 and 6 predicted water level change for 20 out of 92 days accurately within statistical error i e within the 95 confidence intervals of fitted values a similar time series regression with ccf based lags that is lags at 0 and 1 showed slight improvement 23 out of 92 days but was comparable to the predictive performance of vga based lags when the fourth ranked lag from vga analysis that is lag at 15 was also included in a regression with the first three lags the regression predicted 28 out of 92 days correctly a slight improvement from ccf lags based prediction including lagged rainfall only in a model will not predict optimally water level change as can be seen from the predictive accuracy values of the above regressions the purpose of the above analysis to compare the performance between the vga based and ccf based methods and we do see that they are comparable in order to predict daily changes in water level better we have to consider variables other than lagged variables including an indicator variable for month and another indicator variable for the week along with lagged rainfall data improved the predictive performance dramatically month and week indicators with the top three vga lags predicted 71 out of 92 days 77 within statistical error while as the indicator variables along with ccf lags predicted 68 days 74 accuracy values reported for both regressions above are considered reasonably good in order to add more flexibility to modeling that is in order to capture rapid fluctuations we also ran regressions with a four day indicator variable instead of the week indicator variable month indicator variable was retained as in the previous regressions a regression with month and 4 day indicator variables and the three vga based lags predicted 80 days 87 correctly while as the same indicator variables with ccf lags predicted 78 days 85 correctly the predictive accuracy for the above models are very good the fitted values and 95 confidence intervals from the above models are plotted in fig 7 below in summary the predictive performance of vga based lags are comparable to ccf based lags in this example lagged rainfall data by itself does not have very good predictive performance for daily changes in water level data other variables such as month and week or four day indicator variables have much more stronger predictive capability the best prediction accuracy 87 was obtained when vga based lags were included with month and four day indicator variables 6 discussion quantifying time lags between two hydrogeological time series is of significance in many modeling contexts there are several examples in hydrogeological literature where one time series is affected by another after a time lag for example it is often hypothesized that time lag between net precipitation and water level changes in a seepage lake is significantly different from a drainage lake seepage lakes are hydraulically isolated from surface water features and primarily fed by groundwater and direct precipitation drainage lakes are typically connected to a network of streams and rivers wisconsin department of natural resources 2009 another example is in the study of karst systems where the pair of hydrological time series could be discharge discharge or rainfall discharge or water level discharge for instance bailly comte et al bailey comte et al 2008 studied the karst river interactions during the flooding of coulazou river in southern france and detected time lags that explained the influence of the river on the water level elevation in a karst aquifer yet another example is the relationship between precipitation and water levels of a shallow well in an unconfined aquifer versus water levels in a relatively deeper well in a semi confined aquifer this relationship is particularly important to water resource managers and groundwater modelers who need to accurately quantify groundwater recharge into aquifers for developing water supply plans for sustainable use of aquifers groundwater recharge defined as entry of water into the saturated zone is influenced by a wide variety of factors including vegetation topography geology climate and soils dripps 2003 dripps et al 2006 groundwater recharge which is a small percentage of the precipitation that eventually reaches the water table is one of the most difficult parameters to quantify this is because processes such as evaporation transpiration and infiltration through unsaturated subsurface must first be estimated to determine the amount of water lost after a rainfall event often times groundwater models are developed by estimating the groundwater recharge using empirical relationships or as a percentage of precipitation it is a common practice to use groundwater recharge as a calibration parameter meaning the recharge value that provides the best calibration to the model is selected as representative for the watershed simulated for temporal simulations the lag time between a rainfall event and groundwater recharge into deeper aquifers are often ignored currently used methods in hydrogeological literature to detect time lags between a pair of time series is based on simple visual inspection or on cross correlograms the latter approach although substantially better than the former if used without pre whitening could lead to ambiguous results as exhibited in a simulated example in the paper a better way to conduct cross correlogram analysis is under the transfer function framework in this paper we briefly reviewed the transfer function framework and showed for the above mentioned simulated example how cross correlogram under this framework i e after pre whitening gives the correct results without ambiguity however pre whitening the series requires careful model fitting in order to obtain the residuals as white noise there could be examples where even the best fit models may not completely yield white noise as residuals in this paper we present an alternate method to detect time lags based on the visibility graph algorithm vga which is a method developed by physicists to convert a time series into a mathematical graph vga has become highly popular in various scientific disciplines and have found wide applications the method for time lag detection proposed in this paper is based on a simple extension of vga in the simulated example mentioned above we showed how the vga based method detects the lag correctly and unambiguously without having to do a pre whitening process as in the transfer function framework however simulations based on multivariate models revealed that when the pair of time series are from two different types of underlying models the new approach performs well only for large sample sizes 7 conclusions the primary objective of the paper was to demonstrate that the vga method is a useful tool that can be applied on hydrogeological data to calculate lags between time series understanding the hydrogeological cycle and water balance within any watershed has practical value for water resource managers from an operational and planning perspective we selected two time series data that are fundamental to hydrogeological science to demonstrate the application of the vga method one is rainfall which is the primary source of recharge for a watershed and the other is water levels which dictate how much water can be used for human consumption or needs to be released for flood management we applied the methods discussed in this paper to detect the time lags between rainfall and water level fluctuation in lake okkechobee in florida vga method detected lags at days 4 0 6 and 15 with the most prominent lag at day 4 ccf based method detected lags at days 0 and 1 one of the purposes of detecting lags is to use them in a regression model for prediction a regression analysis which included month and four day indicator variable in addition to the lags for rainfall data predicted water level fluctuations with 87 accuracy when vga lags were used and with 85 accuracy when ccf lags were used thus the prediction accuracies based on both approaches were comparable in this case both vga and ccf detected an immediate effect due to rainfall lag at 0 however from a scientific perspective lags at 4 or 6 days and at 15 days are important considering they are reflective of inflows from lake inlet streams only vga based method detected these longer lags multivariate simulations study based on the fitted models for the two time series related to lake okeechobee validated the use of the proposed methods for this particular analysis since both time series were from the same type of models predictive accuracy based on regression models also justify the use of the proposed method for lake okeechobee analysis however for datasets where the two time series are from two completely different type of underlying models larger sample sizes are necessary for the vga based approach presented in this paper declaration of competing interest none appendix supplementary material supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2019 103429 appendix b supplementary materials supplementary data s1 supplementary raw research data this is open data under the cc by license http creativecommons org licenses by 4 0 supplementary data s1 
556,identifying time lag between two hydrogeological time series for planning and management of water resources has a long history and is of continuing research interest many hydrogeological studies in the past have used visual inspection and cross correlogram techniques for quantifying the time lag cross correlogram techniques if not done under the transfer function framework could lead to ambiguous results in order to conduct cross correlogram analysis under the transfer function framework careful pre processing steps have to be undertaken which are often ignored in practice in this paper we propose a new approach to compare two sets of hydrogeological time series data using the visibility graph algorithm and show the advantages of using the new approach over the traditional approach application of the new approach is demonstrated by assessing the lags between rainfall and water level fluctuation in lake okeechobee florida we also present simulation studies to better understand the performance of the method for different sample sizes different underlying models and in the presence of missing values keywords time series visibility graph algorithm cross correlogram water levels precipitation 1 introduction long term planning of water resources often requires an understanding of time lag between a precipitation event and corresponding water level or flow response in a lake stream or an aquifer considering precipitation events are the primary source of recharge for groundwater and surface water resources it is critical from an operational standpoint to quantify the time lag and responses due to precipitation in these water bodies especially for lakes whose levels are artificially controlled for flood management and other ecological reasons although existence of time lag between precipitation events and water level responses is supported by empirical data quantitative assessments of time lags are typically done by visual inspection on a graphical plot e g westoff et al 2010 or using cross correlation techniques e g levanon et al 2016 cross correlation method although useful in many cases could lead to ambiguous results if not done under the transfer function framework if cross correlation is done under the transfer function framework certain assumptions such as joint bivariate stationarity of the two time series have to be met wei 2006 box et al 2008 the method also requires diagnostic checking for model adequacy which is rarely done in practice in this paper we present a non parametric method to quantify the time lag using a simple adaptation of the visibility graph algorithm vga this algorithm converts a time series into a graph although originally developed by physicists lacasa et al 2008 lacasa and luque 2010 nuñez et al 2012 it has found wide applications outside the physics literature in our adaptation we consider one of the time series e g water levels as a reference time series and create time shifted copies of the other time series of interest e g precipitation the time series original copies and the reference are then converted to graphs and their corresponding adjacency matrices calculated using vga and then compared the vga method is described in detail in section 2 1 1 data selection to illustrate the vga based approach we compiled long term hydrological time series data which include water level data of a surface water reservoir and corresponding precipitation data from nearby stations the criteria used for data selection was that the time series should have quality continuous data without gaps and real world environmental or anthropogenic significance the surface water reservoir lake selected for analysis is lake okeechobee which is the second largest natural freshwater lake within the contiguous united states covering approximately 730 square miles the primary inflows into the lake are kissimmee river and taylor creek located north of the lake fisheating creek located west of the lake and the primary outflows are the everglades caloosahatchee river and the st lucie river fig 1 water flowing in and out of the lake is controlled by human decisions which includes determining the time and frequency of opening and closing of numerous gates and locks the south florida water management district sfwmd and the us army corps of engineers usace jointly operate the lake s water control structures to achieve water levels in lake okeechobee that balance water supply flood protection and environmental health audobon florida naturalist magazine fall 2005a audobon florida naturalist magazine writer 2005b the comprehensive everglades restoration plan cerp identifies ideal lake okeechobee water level range to be between 12 feet ngvd at the end of dry season and 15 feet ngvd at the end of wet season any rise in lake level exceeding 18 5 feet ngvd may compromise the structural integrity of the herbert hoover dike surrounding the lake hence as lake levels approach 17 feet ngvd large freshwater discharges are made to the st lucie estuary to the east and caloosahatchee estuary to the west disrupting the natural salinity patterns and water chemistry of these estuaries and impacting its flora and fauna lake levels going below 12 feet ngvd can cause water shortages especially during drought years 2 method let us denote the two hydrogeological time series that we are interested in namely precipitation and water levels by p t and wl t or simply p and wl respectively in order to find the time lag between the two time series as a first step we fix one of the series say wl and obtain time shifted copies of the other series p τ 1 p τ κ the key step in our methodology is the conversion of all the above time series into graphs based on the visibility graph algorithm graphs are mathematical constructs that are used to study relationships among various objects in graph models the objects of interest are modeled as nodes or vertices and the relationships among the objects are modeled using edges or lines connecting the vertices visibility graph algorithm vga lacasa et al 2008 lacasa and luque 2010 nuñez et al 2012 is a method that extends usefulness of the techniques and focus of mathematical graph theory to characterize time series it has been shown that the visibility graph inherits several properties of the time series and its study reveals nontrivial information about the time series itself vga has become very popular e g xu et al 2008 marwan et al 2009 donner et al 2010 luque et al 2009 ahmadlou et al 2010 gao et al 2015 ahmadlou et al 2012 elsner et al 2009 zhu et al 2014 yang et al 2009 donges et al 2012 donner and donges 2012 zhang 2017 and has found wide applications outside the physics literature as evidenced by the 700 citations of the original paper the applications have ranged from health applications related to alzheimer s disease ahmadlou et al 2010 autism disorders ahmadlou et al 2012 and sleep studies zhu et al 2014 to geophysical studies donner and donges 2012 such as hurricanes elsner et al 2009 to financial applications yang et al 2009 however to the best of our knowledge our paper is the first paper in which vga has been used for time lag detection fig 2 top panel illustrates how the visibility algorithm works the time series plotted in the upper panel is an approximate sine series specifically a sine series with gaussian white noise added the values at 24 time points are plotted as vertical bars one may imagine these vertical bars as for example buildings along a straight line in a city landscape i e a city block each node in the associated visibility graph shown in the bottom panel corresponds to each time point in the series so the graph in fig 2 has 24 nodes we draw a link or an edge between a pair of nodes say ti and tj if the visual line of sight from the top of the building vertical bar situated at ti towards the top of the building bar at tj is not blocked by any intermediate buildings that is if we were to draw a line from the top of the vertical bar at ti to the top of the vertical bar at tj it should not intersect any intermediate vertical bars visibility lines corresponding to the edges in the graph are plotted as dotted lines in the figure in the upper panel for example there is no edge between t 2 and t 4 since the line of sight not shown between the top points of the vertical bars at these two time points is blocked by the vertical bar at t 3 on the other hand there is an edge between t 1 and t 3 since the corresponding visibility line shown as a dotted line does not intersect the vertical bar at t 2 more formally the following visibility criteria can be established two arbitrary data values tq yq and ts ys will have visibility and consequently will become two connected nodes of the associated graph if any other data tr yr placed between them fulfills y r y s y q y s t s t r t s t q this simple intuitive idea has been proven useful practically because of certain features exhibited by the graphs generated by this algorithm first of all they are connected since each node is connected to at least its neighbors secondly there is no directionality between the edges so that the graph obtained is undirected in addition the visibility graph is invariant under rescaling of the horizontal and vertical axes and under horizontal and vertical translations in other words the graph is invariant under affine transformations of the original time series data in mathematical notation any graph with n nodes could be represented by its n n adjacency matrix a which consists of 0 s and 1 s the i j th element of a is 1 if there is an edge connecting the ith and the jth node 0 otherwise two graphs g 1 and g 2 can be be compared by the metric distance a g 1 a g 2 2 between their corresponding adjacency matrices a g 1 and a g 1 here 2 called the frobenius norm of a matrix is the square root of the sum of the squares of the elements of the matrix that is the square root of the trace of the product of the matrix with itself in mathematical notation if d denotes the matrix a g 1 a g 2 and dij the ijth element of d with i j 1 t then a g 1 a g 2 2 d 2 trace d d t i 1 t j 1 t d i j 2 our proposed method to assess the time lag between the two hydrogeological time series p and wl using the visibility graph approach is as follows convert the wl time series into a visibility graph and obtain its corresponding adjacency matrix awl consider time shifted copies of the p time series p τ 1 p τ κ each shifted in time by a lag from the set τ 1 τ κ convert these time shifted copies of p into their visibility graphs and obtain the corresponding adjacency matrices a p τ 1 a p τ κ we determine the copy a p τ s for which the frobenius norm a w l a p τ s 2 is minimized the time lag between the two original hydrogeological series is then taken as τs we further illustrate our method using the plots in fig 3 the time series in the top panel ts a is a series of 50 values approximately based on a sine function that is a sine series with some white noise added t s a t 100 sin 2 π f t w t where f 80 1000 w t n 0 25 2 the time series ts b plotted in the middle panel of fig 2 is derived from ts a as follows t s b t 1 3 t s a t 2 e t where e t n 0 5 2 that is ts b is derived by shifting ts a to the right by two units by reducing the amplitude to one third that of ts a and adding some white noise in other words ts a and ts b have roughly the same shape although their amplitudes are different and one is shifted by two time units relative to the other as seen in the figure one may think of ts a and ts b as two time series one affecting the other since ts b is shifted to the left physically we would think of ts b affecting ts a e g ts b as precipitation and ts a as water levels physically water levels and precipitation never take negative values so if one really wants to think of ts a and ts b as water levels and precipitation one could think of them as mean subtracted and scaled appropriately we considered time shifted copies of ts b with time shifts from the following set 0 1 2 20 vga was applied and adjacency matrices for the corresponding graphs were obtained distance measure based on the frobenius norm for the time shifted copies of ts b compared to the reference ts a are plotted in the bottom panel of fig 2 the distance measure is minimized at 2 which was the lag that we set a priori thus in this illustrative example the lag was correctly identified by the method that we proposed 3 comparison with existing method currently existing method for detecting lags between two time series is based on cross correlograms often in practice cross correlograms are applied to the original two time series and lag is determined as the point corresponding to the peak maximum positive or maximum negative correlation however this approach could lead to ambiguities and is not the best recommended approach from a statistical point of view a better approach involves a pre processing step which assures that the two time series used in the cross correlogram analysis are both stationary and white noise the justification for this pre processing step can be understood under the transfer function framework consider two time series yt and xt with yt affected by lagged values of xt the relationship between the two series may be modeled under the transfer function framework as y t ν b x t where ν b j 0 ν j b j here b denotes the back shift operator b j x t x t j ν b is referred to as the transfer function for finite samples from real life examples a rational form is assumed for ν b 1 ν b ω b b b δ b here ω and δ are polynomial functions with finite number of non zero coefficients and b represents the actual lag between the two series our main goal is to estimate b the time delay rewriting eq 1 as δ b ν b ω b b b and expanding we will see that ν j 0 for j 0 b 1 and νj 0 for j b thus in this framework b is determined as the index of the first non zero coefficient νj the above procedure is easy if there is a way to estimate the coefficients νj s as described below cross correlations provide a way to estimate the coefficients νj s cross correlation between the two series at lag k is defined as ρ x y k γ x y k σ x σ y where γxy k is the cross covariance between xt and yt σx and σy are standard deviations of xt and yt respectively if the following two conditions c1 xt and yt are jointly bivariate stationary c2 xt is white noise series are simultaneously met then there exists a scaled relationship between νk and ρxy k 2 ν k σ y σ x ρ x y k thus if c1 and c2 are met then based on eq 2 we may determine which coefficients νj are zero and which are non zero this in turn will help us to determine the lag b putting it all together we may summarize that if conditions c1 and c2 are met then the lag b is the index of the first statistically non zero cross correlation term in other words we may use a cross correlogram to determine the lag the key point here is that the conditions c1 and c2 have to be met in order to apply the cross correlogram method c1 and c2 are simultaneously met if both yt and xt are white noise series one way to assure this in practice is to fit appropriate models ar ma arma or arima separately to xt and yt and use the corresponding residuals to plot the cross correlogram because if the fitted model is accurate then the residuals are white noise this necessitates an extra step in the process considering several models for xt and yt and fitting the most appropriate model at least appropriate enough to generate white noise as residuals this key step known as pre whitening xt and yt is often ignored in practice when cross correlograms are used which in turn could lead to erroneous conclusions one advantage of using the vga based method proposed in this paper is that the pre whitening step is not necessary for the new method we illustrate all the above concepts using the following simulated example we simulated xt from a arima 1 1 0 model that is one ar coefficient one degree of differencing and no ma coefficients ar coefficient was set to 0 7 the sample size was 75 yt was generated as y t 16 0 85 x t 3 1 4 x t 4 e t e t n 0 0 02 note that there are two a priori set lags i e lags at 3 and 4 for this example pre whitening was done for both series by fitting an arima 1 1 0 model and using the residuals autocorrelogram of the residuals showed correlation of one at lag 0 and zero everywhere else indicating that the residuals were indeed white noise the left and middle panels in fig 4 below show cross correlograms without and with pre whitening and the right panel shows the frobenius norm from the vga based method at various lags in the first correlogram the maximum correlation is at lag 3 ρxy 3 0 967 with next two highest correlations at lag 2 ρxy 2 0 959 and lag 4 ρxy 4 0 957 respectively first of all there is ambiguity about the number of lags to be picked based on the first correlogram if we decide to pick lags with the two largest correlations then we will correctly pick lag 3 but incorrectly pick lag 2 note that in the model for yt the effect for lag 4 1 4 was higher than the effect for lag 3 however the first correlogram ranked lag 4 at the third place cross correlogram based on pre whitened series middle panel gives a more clear cut answer only correlations at lags 3 and 4 are statistically different from zero with the one at lag 4 prominently greater than the one at lag 3 in this case the correlogram identified the lags correctly vga based method s frobenius norm is lowest at lag 4 and second lowest at lag 3 the norm value at lag 3 is relatively much closer to that at lag 4 compared to the next nearest values thus in this case also we correctly picked the a priori set lags note that vga based method identified the correct lag without pre whitening while the pre whitening step was necessary for the cross correlogram based method 4 simulations 4 1 sample size we conducted monte carlo simulations to assess the performance of the vga based method as we varied some of the parameters of the two time series ts a and ts b considered in the second section the parameters that we considered were a the ratio of the amplitudes between the two simulated series ts a and ts b b the variance for the noise term rnorm n 0 in the series ts a indicated by and c the variance for the noise term rnorm n 0 in the series ts b note that we did all the simulations in the r statistical software and above we borrowed from r language the term rnorm n 0 sd which stands for n data points from the normal density with mean 0 and standard deviation sd for each simulation scenario considered in this section that is for each set of the above parameters 1000 pairs of ts a and ts b were generated and for each pair time lag was assessed based on the proposed method and compared with the lag that was set a priori the performance of the method was assessed based on the percentage of times that the a priori lag was correctly identified the a priori lags that we considered for each scenario were 2 5 10 and 15 we assumed that in typical examples from hydrogeology 2 will be a small lag and 15 will be a very large lag the reason for considering the ratio of amplitudes was that even if two hydrogeological time series are roughly of the same shape with only a lag between them their amplitudes i e roughly their sizes are often vastly different for ts a and ts b used in the introductory illustrative example in the second section the ratio of their amplitudes was 1 3 one of the questions that was addressed in our simulations was whether our method was still good if we changed this ratio drastically e g to 1 9 another question that we thought should be addressed is that whether the proposed method works only for smooth periodic time series such as the sine series increasing the variance for the noise term in ts a makes it less like a sine series finally increasing the variance of the noise term in ts b makes the shape of ts b quite different from that of ts a and by doing so in our simulations we also addressed the performance of the method in such scenarios our hypothesis was that if we changed the above mentioned parameters to make the relationship between ts a and ts b less ideal than in the illustrative example in the second section the performance of the method will be worse in that sense essentially the purpose of our simulation was to see whether increasing the sample size will improve the performance in such bad scenarios and if so what would be a recommended minimum sample size that would hedge against such scenarios in order to do that we need a reference sample size that is a sample size for which the method s performance was excellent when the relationship between ts a and ts b was reasonably good by reasonably good we mean roughly the same shape and size with only a lag in between them in table 1 above we present the performance of the method for sample sizes 25 and 50 when the ratio of the amplitudes and the noise terms were kept exactly the same as in the illustrative example since table 1 shows that the performance was excellent for n 50 we consider 50 as a good sample size choice if we have reasons to believe may be by visual inspection that there is a nice relationship between ts a and ts b for the next set of simulations we fixed the sample size to be 50 and varied the above mentioned parameters one at a time we varied the parameters one a time rather than simultaneously in order to achieve a meaningful representation of two hydrogeological time series one affecting the other the results of this set of simulations are presented in table 2 the first row presents the performance when the noise terms are kept the same as in the introductory illustrative example but the ratio of the amplitudes was reduced to 1 9 in this case the performance of the method became drastically worse as seen from the table in the next row we present the results when the standard deviation for the noise term for ts a was changed to 50 for the illustrative example it was 25 but the other two parameters were kept the same this was to check whether the performance became worse if the shape of both time series was not roughly like a sine series results from table 2 show that the performance is not affected in this case these results give reasons to believe that our initial choice of a sine series shape did not matter in other words we would think that the method will perform well no matter what the shapes of the two series are as long as both the series are roughly of the same shape and size the third row in table 2 shows the results when only the noise term for ts b was changed this would correspond to making the shape of ts b quite different from that of ts a in this case the performance is affected but not very much as the percentages in the third row are all still above 90 thus based on table 2 the factors that affected the performance was the ratio of the amplitudes and the noise term for ts b and among these two the effect of the former was much more severe than the latter next we checked whether the performance of the method corresponding to the scenario in the first row in table 2 that is ratio of amplitudes equals 1 9 and noise terms for ts a and ts b kept exactly the same as in the illustrative example improved with sample and if so what could be a recommended minimum sample size the results from this set of simulations are presented in table 3 as noted in the table the performance increases very much when the sample size is increased to 90 and is near perfect when the sample size is 180 the percentages for all a priori lags are 100 when the sample size is 365 finally we used simulations to also check the performance of the method when the amplitude of the time series varied seasonally it is well known that the average accumulative precipitation varies seasonally typically for several consecutive months the average accumulative precipitation is high and for several other consecutive months the average accumulative precipitation is low and similarly for water levels in order to mimic this scenario somewhat we generated ts a series using the same parameters as in table 3 except for the amplitude we divided the set of time points into three equal sets of consecutive time points so that when e g n 90 we have the initial 30 time points t 1 t 30 the middle 30 time points t 31 t 60 and finally the last 30 time points t 61 t 90 for the first and last one thirds of the time points an amplitude of 100 and for the middle one third an amplitude of 30 was used in the simulations for ts a ts b was generated with 1 9 as the ratio of amplitudes as in table 3 the results for this new set of simulations are presented in table 4 the results look very similar to that seen in table 3 indicating that the additional seasonal variation of the amplitudes did not have any effect on the performance of the method the take home message from all the simulations results presented above is that if we are considering hydrogeological time series for which measurements were made daily then an year s worth of data will be more than sufficient for the proposed method although the method will work quite well even with 6 months worth of data visually if the two time series looks clearly to be one affecting the other and of roughly the same shape and size then even 2 months worth of data will suffice note that in this section we used the term sample size to refer to the number of time points in the time series and throughout the paper we implicitly assume that the data points in all the time series are measured at equal time intervals 4 2 missing values missing values are common in all time series measurements for hydrogeological phenomena in this section we assess via simulations the performance of the proposed method in the presence of missing values for two different types of imputation methods least observation carried forward locf and mean imputation in locf imputation if a value is missing at any time point we carry forward the previous non missing value in mean value imputation we impute the average of the prior and the subsequent non missing values the missing value mechanism that we considered for our simulations was missing completely at random mcar which means that the missing values are missing exactly as the name implies completely at random there is another commonly considered for example in clinical studies missing value mechanism missing at random mar under which the missingness may depend on the previously observed outcomes under this mechanism both locf and mean imputation are known to be biased but we consider that the missingness in hydrogeological time series do not depend on the previously observed outcomes and hence the mar assumption is unrealistic and thus we did not consider mar for our simulations as a matter of fact the missingness that we have seen for real hydrogeological time series is as follows for example for a time series in which measurements are made daily non missing measurements are seen for a large chunk of consecutive time points 6 months to 2 years followed by a large chunk of data missing at a stretch several weeks or months which again is followed by a large chunk of non missing data and so on when a significant amount of data is missing for a large number of consecutive time points none of the existing imputation methods will work very well in such cases the best strategy is to analyze separately the large chunks of data with no missing values at all nevertheless we conducted the following simulations for hypothetical scenarios in all the simulations reported in this section we fixed the sample size to be 180 and we used the same noise terms for ts a and ts b as in the illustrative example in the second section the ratio of amplitudes was set to be 1 9 as in the simulations for table 3 in order to adhere to the mcar mechanism we randomly set either 9 or 18 or 27 or 36 values to be missing 9 18 27 and 36 correspond to 5 10 15 and 20 of 180 furthermore we considered scenarios where the values were set to be missing for only one time series ts a or for both if it were set to be missing for both then it was at the same time points for both which we think is the more realistic scenario the performance of the proposed method with both locf and mean imputation was near perfect when only 5 of the values that is 9 out of 180 were missing table 5 this was true regardless of whether the values were missing for only one time series or for both and also true across all a priori set lags 2 5 10 and 15 when 10 of the values that is 18 out of 180 were missing for only one time series the method did very well under both locf and mean imputation for all lags when 10 of the values were missing for both time series the performance was still very good when the lags were large 10 or 15 when the lags were small 2 or 5 the performance with both imputation methods was still good but not as good as when the lags were large for example when 10 values were missing and when the lag was 2 the performance with locf was 97 and 92 respectively depending on whether the values were missing for only one time series or both the corresponding values for lag 10 on the other hand were even better 98 and 97 with 15 missing values 27 out of 180 the performance was still good that is in the range 90 97 with locf and mean imputation for lags 2 5 and 10 irrespective of whether it was missing for only one or for both time series although of course if it was missing only for one time series it was better however when the a priori set lag was 15 the performance with locf was weak 84 when 15 values were missing for both time series it was still good 96 with locf when only one time series had 15 missing values and with mean imputation also 92 and 98 with 20 missing values the method worked well under both types of imputations and for all lags only when one time series had missing values when both time series had 20 missing values the performance of locf was not good with small lags 84 for lag 2 and 86 for lag 5 and got worse for larger lags 77 for lag 10 and 79 for lag 15 the performance with mean imputation was slightly better 82 90 81 and 88 for lags 2 5 10 and 15 respectively but still not quite up to the mark in summary based on the above simulation results we consider it acceptable to use the proposed method in conjunction with either of the imputation methods if it is only 5 values missing for only one time series or for both with 6 15 values missing the imputation methods give good results only if it is missing for one time series with about 20 of the values missing for both time series it is definitely not recommended to use the proposed method with either of the imputations although it may be somewhat acceptable if it is missing for only one time series also in general we observed that the performance with mean imputation was slightly better except for one or two scenarios if the statistical practitioner has a preference of one method over the other it may still be recommended to use both for the proposed method at least as a sensitivity analysis finally we emphasize again the point made in the beginning of the section that if large chunks of data are missing at a stretch then the imputation methods are not likely to work in such cases it is better to focus the analysis on other chunks of data with no or very sparse missing values 4 3 multivariate simulations in all the simulations done so far the first time series was simulated using a univariate model and the second time series was generated by obtaining a lagged copy of the first series and then adding random noise to it a better strategy technically is to generate both the time series from a multivariate model and shift the second series to set a lag between the two series a priori in this subsection we present results from such simulation strategies multivariate time series that we considered were all generated from vector arma processes which include vector ar processes and vector ma processes as special cases the general form for a n vector arma p q process zt is given by z t a 1 z t 1 a p z t p ε t b 1 ε t 1 b q ε t q where ai s and bj s are n n matrices i 1 p and j 1 q and zt s and ε t s are vectors with n elements elements of ε t are white noise processes i e serially uncorrelated across time however at each time point there might be correlation among the elements in this paper since we consider only bivariate time series n was set equal to 2 for all the simulations in this subsection all the matrices that we considered were diagonal however we did include a correlation among the elements within each ε t following are the models that we considered m1 x t 0 65 x t 1 0 38 ε t 1 1 ε t 1 y t 0 95 y t 1 0 62 ε t 1 2 ε t 2 y t δ x t or in matrix notation z t a z t 1 ε t b ε t 1 where z t x t y t ε t ε t 1 ε t 2 a diag 0 65 0 95 b diag 0 38 0 62 m2 x t 0 40 x t 1 1 20 x t 2 0 05 x t 3 0 35 x t 4 ε t 1 y t 0 30 y t 1 0 85 y t 2 0 04 y t 3 0 45 y t 4 ε t 2 y t δ x t in matrix notation this will be z t a 1 z t 1 a 2 z t 2 a 3 z t 3 a 4 z t 4 ε t where a 1 diag 0 40 0 30 a 2 diag 1 20 0 85 a 3 diag 0 05 0 04 and a 4 diag 0 35 0 45 m3 x t 0 40 ε t 1 1 ε t 1 y t 0 40 y t 1 ε t 2 y t δ x t in matrix notation z t a z t 1 ε t b ε t 1 where a diag 0 0 40 b diag 0 40 0 m4 x t 0 95 x t 1 ε t 1 y t 0 95 y t 1 0 95 ε t 1 2 ε t 2 y t δ x t in matrix notation z t a z t 1 ε t b ε t 1 where a diag 0 95 0 95 b diag 0 0 95 the pair of time series within m1 were same as the best fitted models for the respective time series i e rainfall and water level fluctuation from the lake okeechobee data and analysis presented below in section 5 thus model m1 was included primarily to assess the validity of the methods for the lake okeechobee data analysis and its conclusions the correlation ρ among the elements of ε t used for generating all the above bivariate series was 0 6 for each model we considered separate simulations with lags 3 and 7 set a priori for most hydrogeological time series pairs a lag of 3 would be considered as small and a lag of 7 considered large bivariate time series based on models 1 3 and 4 were generated using the varma command within the multiwave package in r achard and gannaz 2019 for bivariate time series based on model 2 mar sim command within the mar package barbosa 2015 in r was used the results from the simulations studies corresponding to various scenarios are presented in table 6 below the results are based on 1000 iterations for each simulation scenario ccf1 and ccf2 in table 6 correspond to cross correlation method without and with pre processing step respectively in all scenarios there was no substantial difference between the accuracies obtained in the lag 3 setting compared to that with lag 7 all the methods worked very well for time series pairs generated from m1 even with a sample size of 100 this justifies our use of the methods particularly the vga based method for the analysis of lake okeechobee data for simulations under m2 all three methods were comparable for both sample sizes the accuracy of all three methods were not perfect but only approximately 95 when the sample size was 100 but it reached 100 or near 100 with a sample size of 500 for the next two models only ccf2 performed very well for a sample size of 100 under m3 the accuracy of vga based method was approximately 92 and that of ccf1 was approximately 98 5 although the performances of vga based method and ccf1 were not optimal with the smaller sample size they improved substantially when a sample size of 500 was used under m4 the accuracy of vga based method was approximately 81 and that of ccf1 was approximately 43 when the a priori lag was 3 however with a sample size of 500 the performance of vga based improved substantially to 100 the performance of ccf1 method also improved but not as much as for the vga method the results for the simulations based on m3 and m4 bring out a limitation of the vga based method that its performance is dependent on sample size m3 and m4 are different from m1 and m2 in the following sense the pair of time series in m1 and m2 were both from the same type of time series models arma 1 1 for m1 and ar 4 for m2 although the coefficients differed the two time series within m3 were from different types ma 1 for the first time series and ar 1 for the second time series the two time series within m4 were also from different types ar 1 for the first time series and arma 1 1 for the second time series the above set of simulations seems to suggest that when the underlying pair of time series under consideration are of two different types then vga adapted method performs very well only with large sample sizes thus if there are reasons to believe that the underlying time series are from two different types of models then the vga based method presented in this paper should be used with caution especially for smaller sample sizes next we present analysis of lag between rainfall and water level fluctuation in lake okeechobee to illustrate the methods further 5 lake okeechobee data for this analysis we selected daily water level and rainfall data from two monitoring stations located on lake okeechobee the lake and hydrologic features connected to it are one of the most studied and monitored watershed systems in united states because of its significance in regional flood management and water supply and considering it is a vital fresh water resource for florida the lake and its surrounding wetlands have a suite of monitoring stations that collect water level water quality meteorological and flow data throughout the year the lake system is part of the greater everglades watershed that stretches from the kissimmee river to florida bay with significant flows occurring through the everglades the sfwmd and acoe takes extreme measures to constantly monitor the lake levels and maintain it at optimal levels taking into consideration public s safety water demands and the health of flora and fauna in the estuaries downgradient from the lake daily rainfall and water level data used for this analysis was collected between january 1 2000 and december 31 2018 19 years rainfall data was highly skewed so a fourth root transformation was done before all analyses there were no missing values for water levels and less than 0 5 values missing for rainfall data since the percent of missing values was very small we used imputation based on least observation carried forward locf before conducting the analysis the primary goal of the analysis was to detect the time lag s between daily rainfall and daily changes in water level note that we used daily changes in water level rather than daily water level itself because lake water levels are more complex and tied to many hydrogeological factors in addition to rainfall such as evaporation surface water and groundwater inflow and outflows the number of days of rainfall for each month from january to december averaged across all 19 years were 5 1 4 8 6 2 5 4 8 1 14 2 13 5 16 1 13 2 7 1 5 4 and 5 7 respectively the average number of days with rainfall was substantially higher for the months of june july august and september these four months represent the wet season for the region further we estimated the average annual rainfall for each of the 19 years and noticed that 2014 was the wettest year the first analysis below is mainly for illustration purposes only we applied our proposed method to the wet season june through september of the wettest year 2014 the lowest value of the frobenius norm was at lag 4 norm value 706 the second lowest value at lag 6 norm value 724 the third and fourth lowest values were at lags 15 and 0 norm values were close to each other 742 and 746 it is interesting to note that our analysis indicates a primary lag of about 4 or 6 days and a secondary lag of about 15 days the primary lag may be a reflection of immediate water level response to rainfall events occurring in and around the lake while the secondary lag may be a response to rainfall occurring in the lake vicinity and an increase in inflows via the lake inlet streams caused due to rainfall events occurring upstream from the lake although not prominent there is also some indication of an immediate effect shown by the lag at 0 the above analysis was conducted for the wettest year we repeated the analysis after averaging the data across all 19 years averaging the data will be more statistically appropriate when the results are used for prediction again this analysis was done for illustrative purposes only the averaged data for rainfall and water level change are shown in the upper panel and middle panels of fig 5 respectively the frobenius norm from vga based analysis for different lags are plotted in the bottom panel of fig 5 the lowest value of the norm was at lag 4 followed by lags at 0 and 6 norm values 862 864 and 870 respectively the fourth ranked lag was at lag 15 norm value 876 it is interesting to note that the top four lags were same as that for the wettest year although in the analysis for the averaged data the immediate effect i e lag at 0 became more prominent we also conducted cross correlogram analysis for detecting lag an arma model fit with one ar coefficient and one ma coefficient i e arima 1 0 1 yielded white noise residuals for time series see appendix fig a1 the coefficients obtained with this arma 1 1 fit were the same as the ones used in the two series in the model m1 for multivariate simulations above ar alone or ma alone did not produce white noise residuals the ccf based on the residuals from the arma models are plotted in fig 6 below the significant lags detected in this case are at 0 and 1 thus ccf based analysis detects only immediate effects due to rainfall one of the primary reasons for detecting lags is to use them for prediction based on for example a time series regression in order to compare the predictive performance of the lags obtained via vga versus those obtained via ccf we fitted time series regressions with daily water level change as the dependent variable no intercept term and lagged rainfall data as independent variables in order to accommodate lag terms for rainfall we used only water level data for months of july august and september in the regressions to be precise if june water level data was included and lag 15 for example for rainfall was modelled in the regression then that would have required rainfall data from the month of may which was not used in the original vga or ccf analyses hence the prediction analysis was restricted to water level data from july august and september total 92 days having no intercept is justified as follows a regression with no intercept term in the current context interprets as zero rainfall implies zero change in water level or more technically correctly when there is no rainfall in the recent past the only change in water level is due to random variation for each regression fitted values and 95 confidence intervals for fitted values were calculated accuracy for each regression was determined as the number of actual water level change values that were within the 95 confidence interval band of the fitted values a time series regression with no intercept and vga based lags that is lags at 4 0 and 6 predicted water level change for 20 out of 92 days accurately within statistical error i e within the 95 confidence intervals of fitted values a similar time series regression with ccf based lags that is lags at 0 and 1 showed slight improvement 23 out of 92 days but was comparable to the predictive performance of vga based lags when the fourth ranked lag from vga analysis that is lag at 15 was also included in a regression with the first three lags the regression predicted 28 out of 92 days correctly a slight improvement from ccf lags based prediction including lagged rainfall only in a model will not predict optimally water level change as can be seen from the predictive accuracy values of the above regressions the purpose of the above analysis to compare the performance between the vga based and ccf based methods and we do see that they are comparable in order to predict daily changes in water level better we have to consider variables other than lagged variables including an indicator variable for month and another indicator variable for the week along with lagged rainfall data improved the predictive performance dramatically month and week indicators with the top three vga lags predicted 71 out of 92 days 77 within statistical error while as the indicator variables along with ccf lags predicted 68 days 74 accuracy values reported for both regressions above are considered reasonably good in order to add more flexibility to modeling that is in order to capture rapid fluctuations we also ran regressions with a four day indicator variable instead of the week indicator variable month indicator variable was retained as in the previous regressions a regression with month and 4 day indicator variables and the three vga based lags predicted 80 days 87 correctly while as the same indicator variables with ccf lags predicted 78 days 85 correctly the predictive accuracy for the above models are very good the fitted values and 95 confidence intervals from the above models are plotted in fig 7 below in summary the predictive performance of vga based lags are comparable to ccf based lags in this example lagged rainfall data by itself does not have very good predictive performance for daily changes in water level data other variables such as month and week or four day indicator variables have much more stronger predictive capability the best prediction accuracy 87 was obtained when vga based lags were included with month and four day indicator variables 6 discussion quantifying time lags between two hydrogeological time series is of significance in many modeling contexts there are several examples in hydrogeological literature where one time series is affected by another after a time lag for example it is often hypothesized that time lag between net precipitation and water level changes in a seepage lake is significantly different from a drainage lake seepage lakes are hydraulically isolated from surface water features and primarily fed by groundwater and direct precipitation drainage lakes are typically connected to a network of streams and rivers wisconsin department of natural resources 2009 another example is in the study of karst systems where the pair of hydrological time series could be discharge discharge or rainfall discharge or water level discharge for instance bailly comte et al bailey comte et al 2008 studied the karst river interactions during the flooding of coulazou river in southern france and detected time lags that explained the influence of the river on the water level elevation in a karst aquifer yet another example is the relationship between precipitation and water levels of a shallow well in an unconfined aquifer versus water levels in a relatively deeper well in a semi confined aquifer this relationship is particularly important to water resource managers and groundwater modelers who need to accurately quantify groundwater recharge into aquifers for developing water supply plans for sustainable use of aquifers groundwater recharge defined as entry of water into the saturated zone is influenced by a wide variety of factors including vegetation topography geology climate and soils dripps 2003 dripps et al 2006 groundwater recharge which is a small percentage of the precipitation that eventually reaches the water table is one of the most difficult parameters to quantify this is because processes such as evaporation transpiration and infiltration through unsaturated subsurface must first be estimated to determine the amount of water lost after a rainfall event often times groundwater models are developed by estimating the groundwater recharge using empirical relationships or as a percentage of precipitation it is a common practice to use groundwater recharge as a calibration parameter meaning the recharge value that provides the best calibration to the model is selected as representative for the watershed simulated for temporal simulations the lag time between a rainfall event and groundwater recharge into deeper aquifers are often ignored currently used methods in hydrogeological literature to detect time lags between a pair of time series is based on simple visual inspection or on cross correlograms the latter approach although substantially better than the former if used without pre whitening could lead to ambiguous results as exhibited in a simulated example in the paper a better way to conduct cross correlogram analysis is under the transfer function framework in this paper we briefly reviewed the transfer function framework and showed for the above mentioned simulated example how cross correlogram under this framework i e after pre whitening gives the correct results without ambiguity however pre whitening the series requires careful model fitting in order to obtain the residuals as white noise there could be examples where even the best fit models may not completely yield white noise as residuals in this paper we present an alternate method to detect time lags based on the visibility graph algorithm vga which is a method developed by physicists to convert a time series into a mathematical graph vga has become highly popular in various scientific disciplines and have found wide applications the method for time lag detection proposed in this paper is based on a simple extension of vga in the simulated example mentioned above we showed how the vga based method detects the lag correctly and unambiguously without having to do a pre whitening process as in the transfer function framework however simulations based on multivariate models revealed that when the pair of time series are from two different types of underlying models the new approach performs well only for large sample sizes 7 conclusions the primary objective of the paper was to demonstrate that the vga method is a useful tool that can be applied on hydrogeological data to calculate lags between time series understanding the hydrogeological cycle and water balance within any watershed has practical value for water resource managers from an operational and planning perspective we selected two time series data that are fundamental to hydrogeological science to demonstrate the application of the vga method one is rainfall which is the primary source of recharge for a watershed and the other is water levels which dictate how much water can be used for human consumption or needs to be released for flood management we applied the methods discussed in this paper to detect the time lags between rainfall and water level fluctuation in lake okkechobee in florida vga method detected lags at days 4 0 6 and 15 with the most prominent lag at day 4 ccf based method detected lags at days 0 and 1 one of the purposes of detecting lags is to use them in a regression model for prediction a regression analysis which included month and four day indicator variable in addition to the lags for rainfall data predicted water level fluctuations with 87 accuracy when vga lags were used and with 85 accuracy when ccf lags were used thus the prediction accuracies based on both approaches were comparable in this case both vga and ccf detected an immediate effect due to rainfall lag at 0 however from a scientific perspective lags at 4 or 6 days and at 15 days are important considering they are reflective of inflows from lake inlet streams only vga based method detected these longer lags multivariate simulations study based on the fitted models for the two time series related to lake okeechobee validated the use of the proposed methods for this particular analysis since both time series were from the same type of models predictive accuracy based on regression models also justify the use of the proposed method for lake okeechobee analysis however for datasets where the two time series are from two completely different type of underlying models larger sample sizes are necessary for the vga based approach presented in this paper declaration of competing interest none appendix supplementary material supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2019 103429 appendix b supplementary materials supplementary data s1 supplementary raw research data this is open data under the cc by license http creativecommons org licenses by 4 0 supplementary data s1 
557,when the solution in a porous medium is in chemical equilibrium with mineral grains and an external surface is put in contact with water the infiltration of unsaturated solution may be accompanied by mineral reactions that change the morphology of that medium here we study the evolution of this infiltration process considering that the initial porous media are fractal and that the reactions form additional porous material plus soluble products a sierpinski carpet and a menger sponge are the fractal models the mineral inclusions are represented by the non fractal blocks squares and cubes formed in their iterative constructions and reaction rates are described by a thermally activated model in conditions of slow reactions in comparison with the diffusion the interplay between the infiltration by diffusion and the structural changes by the reactions is explained by a combination of kinetic monte carlo simulations of lattice models and a scaling approach at short times the infiltration is subdiffusive but the dissolved mass increases anomalously fast due to delocalization of the reaction which is distributed through a time increasing infiltrated region at long times normal fickean infiltration and reaction are observed the crossover between the two regimes occurs when the thickness of the altered layer formed around the mineral inclusions is of the same order of magnitude of the width a of the smallest gaps between those inclusions the order of magnitude of the crossover time tc is estimated as a vk where k is the reaction rate constant and v is the molar volume of the mineral this time does not depend on the diffusion coefficient of reaction products or on the fractal geometry of the initial medium which suggests the application to a variety of systems estimates of tc are obtained in two recently studied rocks with reported fractal properties and justify the fickean diffusion assumption of previous models for their weathering in geological time scales the morphological evolution of the mineral blocks that partially reacted qualitatively agrees with experiments 1 introduction if a porous medium is filled with a static fluid and an external surface is in contact with a reservoir of a solute then this solute infiltrates into or out of that medium depending on the concentration difference the infiltration front penetrates into a length f that scales in time t as 1 f t n where n 1 2 represents normal fickean infiltration and n 1 2 represents anomalous transport reis 2016 reis and voller 2019 an equivalent infiltration process occurs when the external surface of a dry porous medium is in contact with a reservoir of a fluid at a fixed pressure and the domain geometry constrains the flow to a one dimensional horizontal direction i e without the effect of gravity voller 2015 the cases of subdiffusive transport n 1 2 and of superdiffusive transport n 1 2 were already shown in these processes theoretically voller 2015 reis 2016 reis et al 2018 reis and voller 2019 and experimentally küntz and lavallée 2001 el abd and milczarek 2004 gerasimov et al 2010 li and zhao 2012 el abd 2015 filipovitch et al 2016 delgado et al 2016 the transport of fluids and of solutes in porous media of geological origin is frequently combined with chemical reactions that change the physical structure of those media berkowitz et al 2016 hommel et al 2018 in some cases diffusion is the dominant transport mechanism lockington and parlange 2003 sak et al 2004 bazilevskaya et al 2013 chagneau et al 2015b 2015a for instance celestite precipitation in the pores of a clay was recently shown to block the diffusion of chloride ions chagneau et al 2015b in other cases the reactions may increase the porosity of the medium one example is the formation of weathering rinds chemically altered layers enclosing unweathered cores in basalt clasts sak et al 2004 navarre sitchler et al 2009 the fractal geometry of the pore networks of those rinds and cores were revealed by neutron scattering methods navarre sitchler et al 2013 moreover bazilevskaya et al 2013 suggested that diffusion is the main mechanism for solute transport in weathered diabase at the virginia piedmont united states and fractal properties of unweathered and weathered diabase were also shown in a neutron scattering study bazilevskaya et al 2015 this raises the question on how the reactions of the minerals of a sample with fractal pore geometry affect the diffusive infiltration since there are many other geological materials with fractal pore structure farin and avnir 1987 sahimi 1993 this question has a broad interest in this work we study the interplay between the infiltration driven by concentration gradients in fractal porous media and the reaction between water and inclusions of a mineral we use two deterministic fractals a sierpinski carpet and a menger sponge to model the initial porous matrix while the compact blocks generated in their constructions represent the mineral grains the solution in the pores is in chemical equilibrium with the mineral when an external surface is put in contact with unsaturated solution and the reaction proceeds to transform the mineral into porous material plus soluble diffusing products the microscopic dynamics of diffusion and reaction do not change in time so that any change in the evolution of the infiltration and of the reaction fronts is directly related to changes in the medium geometry consequently at short times we always observe an anomalous behavior we use a combination of kinetic monte carlo simulations of a reactive transport model and a scaling approach to study this problem for rapid reactions the simulations show an initial regime with anomalously slow infiltration and anomalously fast reaction and a long time regime with normal diffusive infiltration and reaction however for slow reactions they only show small deviations from the initial anomalous behavior at the longest simulated times the scaling approach then provides a theoretical explanation for the crossover from anomalous to normal behavior which is predicted for any parameter set a relation between the crossover time and the model parameters is obtained and is confirmed by an extended analysis of the simulation data the crossover is shown to occur when the dissolved thickness of the mineral inclusions is of the same order of magnitude of the width of the gaps between those inclusions and the crossover time is shown to depend on the reaction chemistry and on those gap sizes but not on the transport properties in the above mentioned processes of formation of weathering rinds in basalts and of chemical weathering of diabase rock we estimate the orders of magnitude of the crossover times and show that they are much smaller than the weathering times which supports the assumption of fickean diffusion in previous models for those processes this paper is organized as follows section 2 presents the fractal lattices the model of infiltration and reaction and information on the methods of solution section 3 presents simulation results and the scaling approach to explain short time long time and crossover properties the consequences of the results are discussed in section 4 section 5 summarizes results and conclusions 2 models and methods 2 1 fractal porous media the porous media are defined in two dimensional or three dimensional hypercubic lattices in which a site has lateral size a lattice constant the first fractal is the sierpinski carpet sc whose iterative construction is shown in fig 1 a the order of iteration is denoted as o the part shown in tan color represents a porous material which is permeable to the fluid but is not reactive the corresponding lattice sites are termed p sites this is the fractal part of the sample which has dimension d f ln 16 ln 5 1 7227 in the limit of infinite iterations the remaining part red square blocks in fig 1 a represents inclusions of a reactive mineral which are impermeable to the fluid the lattice sites of these inclusions are termed m sites this is the non fractal part of the sample with dimension 2 note that the length of the smallest inclusions and the width of the narrowest gaps between them are of order a the red blocks in fig 1 a are usually termed removed blocks in models where only the fractal part is physically meaningful this is not the case here because the sites of those blocks may be transformed into porous material when they react see section 2 2 the sc is helpful for basic investigations of transport phenomena in fractals see e g the illustration of streamlines in luo et al 2014 and usually provides results more accurate than those obtained in fractals embedded in three dimensions see e g estimates of random walk dimensions in carpets in balankin et al 2016 moreover recent experiments on glycerin infiltration in a horizontal hele shaw cell with obstacles organized as in the carpets showed anomalous behavior with exponents n in excellent agreement with simulations voller 2015 filipovitch et al 2016 the second fractal is the menger sponge ms whose generator is shown in fig 1 b analogously to the sc we assume that the sites of the fractal represent a permeable porous material p sites and that the red cubes fig 1 b represent reactive inclusions of a mineral m sites which are impermeable to the fluid the fractal dimension of the porous medium is d f ln 98 ln 5 2 8488 the size of the smallest inclusions and the width of the narrowest gaps between them are also of order a this and other sponges were already used as models of geological and construction materials atzeni et al 2008 villalobos et al 2009 cihan et al 2009 the relevance of this class of fractals is also shown in a recent study of darcy like flow in transparent plastic samples with the pore structure of an inverse ms balankin et al 2016 the sc and the ms are fractals with infinite ramification in which two points with increasing distance are connected by an increasingly large number of paths through the medium havlin and ben avraham 2002 gefen et al 1984 this property implies that the transport to long distances is not suppressed by the occlusion of a finite number of gaps it contrasts with the finite ramification of sierpinski gaskets and comb like structures in which infinitely large parts can be isolated by cutting a finite usually small number of connections the order of ramification has drastic consequences in the choice of the methods of solution of diffusion and other statistical models as will be explained in section 2 4 the organized distribution of inclusions in the sc and in the ms also implies that the mass distribution is correlated at distances that reach the total size of the samples this contrasts with critical percolation clusters which are fractal sets generated by random distributions of pore sites stauffer and aharony 1992 2 2 model of infiltration and dissolution consider a reaction of a mineral mi which is schematically represented as 2 mi h 2 o c sp nsp where sp is a soluble product with stoichiometric coefficient c and nsp represents the non soluble products we assume that this reaction is isovolumetric that the nsp form a immobile porous material and that the properties of this material are not different from the properties of the initial porous matrix which has a fractal geometry section 2 1 the product sp moves in the pores according to its concentration gradient this phenomenology is represented in our lattice model but details of the physical interpretation are postponed to section 2 3 the two types of sites m and p were defined in section 2 1 and are also shown in fig 2 a m represents the mineral mi and p represents the initial porous matrix or the nsp in a real porous material the solution may have continuously varying concentration of the product sp however our model simplifies this feature by considering only two states of the solution a saturated state in which the p site contains one s particle and an unsaturated state in which the p site does not have an s particle these states are shown in fig 2 a in the saturated state the pore solution is in chemical equilibrium with the mineral in its nearest neighbor nn sites the unsaturated state represents zero concentration of reaction products so the rate of the reaction of the mineral with the solution has a far from equilibrium value an equivalent model was used by reis 2015 to describe the kinetics of formation of altered layers on the surfaces of mineral grains the initial distribution of m and p sites and of s particles in the sc is illustrated in fig 3 a all p sites are saturated i e they contain an s particle so that the mineral and the solution are in chemical equilibrium at all points at time t 0 the external surface of the medium with z 0 is put in contact with slowly running water the initial configuration of the ms follows the same rules the diffusion of soluble reaction products sp in eq 2 is represented by random walks of the s particles all s particles attempt to hop to a randomly chosen neareast neighbor nn site with rate h the hop is executed only if the target is an unsaturated porous site with no s particle as shown in fig 2 b or if the target site is at the external surface as shown in fig 2 c in the latter case the s particle is removed from the system this is the mechanism that leads to infiltration of unsaturated solution into the medium and the propagation of a reaction front in the absence of reactions this model is equivalent to the infiltration model of reis 2016 and reis and voller 2019 fig 3 a illustrates the hopping attempts of two s particles in the initial configuration of the sc the resulting configuration is shown in fig 3 b which highlights a solid site in contact with unsaturated solution and shows s particles being carried away by the flowing water now we describe the reaction model when an unsaturated p site is a nn of an m site the reaction may occur as illustrated in fig 2 d the two neighbors viz m site and unsaturated p site are both transformed into saturated p sites this rule represents the transformation of the mineral into a porous material eq 2 and the local equilibration of the solution by the reaction products the reaction rate of a given site depends on the number n of nn which are also m sites as proposed in models of thermally activated crystal dissolution lasaga and blum 1986 mccoy and lafemina 1997 fig 4 shows the possible neighborhoods of m sites in a simple cubic lattice with the corresponding values of n in our model in three dimensions which is applicable to the ms the sites with n 3 have reaction rate r these sites are usually termed isolated n 0 single bonded n 1 two bonded n 2 or kinks n 3 the sites with n 4 steps have rate ϵr and those with n 5 terrace have rate ϵ2 r with ϵ 1 with these rules the reaction of kink sites or sites with smaller coordination is much faster than the reaction of the strongly bonded step and terrace sites in the sc the reaction rates are properly modified sites with n 0 1 and 2 react with rate r kinks have n 2 see fig 1 a and sites with n 3 react with rate ϵr where ϵ 1 fig 3 c shows a configuration of the sc after some time of infiltration and reaction the m sites that can react are highlighted a reactive m site and a hopping s particle are indicated by arrows the configuration after the reaction of that m site and the hop of that s particle is shown in fig 3 d first the transformation of the m site into a p site and formation of two new s particles second the hop of the s particle to the right leaving unsaturated solution in contact with the m site at its left highlighted in fig 3 d 2 3 physical interpretation of the model parameters the average time for a hop of an s particle is 3 τ 1 h we only consider slow reactions compared to diffusion so that r h this means that the probability of a reaction to occur at a given point is much smaller than the probability of solution equilibration by diffusion in a neighborhood of size a in a small region with only porous sites the diffusion coefficient d of the soluble products is given by 4 d a 2 e τ where e is the euclidean dimension of the hypercubic lattice note that this diffusion coefficient is characteristic only of homogeneous regions of the porous material in lengthscales in which the medium is fractal subdiffusion is expected bouchaud and georges 1990 havlin and ben avraham 2002 metzler et al 2014 in this case the root mean square displacement d of a tracer particle evolves as 5 d t ν with ν 1 2 dimensional factors were omitted in eq 5 this anomaly is related to the self similar distributions of irregularities such as impenetrable barriers or dead ends which lead to delays in the tracer motion without characteristic time scales havlin and ben avraham 2002 the standard model of thermally activated crystal dissolution in highly unsaturated solution lasaga and blum 1986 mccoy and lafemina 1997 considers that the detachment of a site at the surface occurs with rate h 0 μn where h 0 is a characteristic frequency of order 1012hz μ is an arrhenius factor and n is the number of nn of that site the same as in fig 4 in that model a site represents a molecule or an ion of the mineral recent simulations of dissolution of rough crystalline surfaces with this model showed that de assis and reis 2018 i sites with small coordination n 2 rapidly detach and have no significant effect on the global dissolution rate ii the detachment of kink sites n 3 is the rate limiting step when the surface has a non negligible roughness iii the surface morphology evolves with the increase in the fraction of step sites n 4 and terrace sites n 5 whose detachment is much slower however in our model the lattice site has a different interpretation it represents a mesoscopic volume of mineral or of porous material which have large numbers of molecules thus an m site with a small number of nn n 2 corresponds to a small mineral grain or to a protuberance of a grain the reaction of this m site is expected to be controlled by the detachment of kinks at molecular scale as shown in the simulations of the standard dissolution model de assis and reis 2018 this explains the use of the same reaction rate r for all m sites with n 3 in three dimensions and the same rate for all sites with n 2 in two dimensions we only distinguish their rate r from the rates of steps and terrace sites which are smaller by factors ϵ and ϵ2 respectively in the time interval τ the probability of transformation of a kink solid site in contact with a nn unsaturated pore is 6 p r τ this probability is used as a parameter in the simulations section 2 4 the condition of slow reaction compared to diffusion implies p 1 when a kink site reacts fig 2 e the change in the number of moles of the mineral is δ n m a 3 v where v is its molar volume the area in contact with the unsaturated solution is δ a a 2 considering that the global reaction rate is controlled by the reaction of kink sites we can estimate the reaction rate constant as 7 k p δ n m δ a τ r a v if we consider a homogeneous medium in which the diffusion coefficient can be defined eq 4 then we can use eqs 6 and 7 to obtain 8 p a v 6 k d this shows that p has the form of a second damköhler number which is a dimensionless ratio between rates of reaction and diffusion the factor av 6 adjusts the different units of the two rates finally note that our model only describes the forward reaction of eq 2 the backward case is not modeled because we neglect the possible occurrence of supersaturation in the pore solution 2 4 methods of solution in the absence of reactions the continuous limit of the infiltration model is equivalent to the diffusion equation with a boundary with constant concentration reis and voller 2019 and the infiltration exponent n eq 1 is related to the random walk exponent ν eq 5 in infinitely ramified fractals such as the sc and the ms those exponents cannot be calculated exactly by renormalization schemes gefen et al 1984 for this reason most works on diffusion in carpets and sponges use numerical methods of solution kim et al 1993 reis 2016 a recent work conjectured an expression for the exponent ν in the carpets which is in good agreement with numerical results but there is no proof that it is an exact result balankin et al 2016 in the present model the fractal medium changes due to the reactions which is an additional obstacle to the use of analytical methods of calculation this scenario justifies the use of kinetic monte carlo simulations the model was simulated in the 5th order of iteration of the sc in which the lateral size is l 3125 the values of the probability p ranged from 10 2 to 10 6 the factor to reduce the step site reaction is ϵ 10 4 in all cases the maximal simulation time was 106 τ for each p a number of realizations between 100 and 250 was used to calculate average areas of infiltrated material unsaturated p sites and average areas of the lost material m sites that reacted simulations in de assis and reis 2018 suggest that ϵ 10 4 in calcite dissolution at room temperature and this parameter is expected to have smaller values for minerals with smaller dissolution rates since the parameter that controls the propagation of the reaction front is p or r the choice of a fixed value for ϵ does not have significant effect in the results in the ms the lateral size was l 625 which corresponds to the 4th stage of generation of the fractal the values of p ranged from 10 2 to 10 5 and ϵ 10 4 was considered in all cases the maximal simulation time was 4 104 τ the number of realizations used to calculate average quantities was between 20 and 40 the surface in contact with the flowing solution is z 0 the opposite surface is not reached by the infiltration front during the simulation time i e the unsaturated p sites with the largest z are very far from that surface periodic boundary conditions are adopted in x and y directions in infiltration models without dissolution a small number of realizations in a large lattice was shown to provide accurate values of the infiltrated area or volume at long times reis and voller 2019 here we confirmed that this also occurs with 10 realizations when dissolution is involved despite the use of an improved simulation algorithm the time for computation was long in the three dimensional lattices due to the large number of hopping particles and the large number of possible reaction events particularly for the largest p for instance one configuration of infiltration and reaction in the ms ran for 5 6 days in a xeon processor here we also use a scaling approach to predict relations between the orders of magnitude of characteristic times and lengths of the model for any parameter set under the assumption that the relevant scaling exponent n in eq 1 is known this type of theoretical method is supported by renormalization group and universality concepts and has a focus on power law relations between the relevant physical and chemical quantities similar methods were recently used to distinguish regimes dominated by reaction and diffusion in models of growth of altered layers on mineral surfaces reis 2015 and to connect infiltration and diffusion exponents in fractals reis 2016 reis et al 2018 the consistency of the predictions of the scaling approach is checked with numerical simulation results 3 results 3 1 general features of the infiltration reaction process fig 5 a and b show shapshots of the sc during the infiltration reaction process with p 10 3 and p 10 5 respectively the infiltration length f in two three dimensions is defined as the ratio between the area volume of unsaturated p sites and the length l area l 2 of the infiltration border this is the standard definition in models of fluid infiltration voller 2015 in which the front is sharp and can be extended to problems of solute infiltration in which the front is diffuse reis 2016 reis and voller 2019 the points of the medium with distance smaller than f from the infiltration boundary form the infiltrated region in fig 5 a b the length f is indicated by a solid bar at the left sides of the snapshots in order to represent the typical length in which water penetrates in the sc however note that the concentration of unsaturated sites varies slowly with the depth z the large width of the infiltration front is an expected feature in random walk models for p 10 3 fig 5 a at t τ 10 2 we observe a small number of sites with reactions at t τ 10 3 there is significant dissolution of the smallest m blocks in this case we have r t p t τ 1 which means that all m sites in contact with unsaturated p sites have probability of reaction of order 1 at t τ 10 4 all sites of the smallest blocks reacted and the reaction also advances at the corners and edges of the largest blocks of the infiltrated region in this case r t 10 in that region the porous medium has lost its self similar properties and contains only an approximately periodic distribution of obstacles m blocks with random shape for p 10 5 similar features are observed but at longer time scales at t τ 10 3 we have r t 0 01 which suggests very low dissolution of the mineral in contact with undersaturated solution this is actually observed in the corresponding snapshot of fig 5 b at t τ 10 5 we have r t 1 and most of the smallest blocks of m sites in the infiltrated region have already reacted at t τ 10 6 r t 10 the infiltrated region has uniformly distributed large m blocks partially altered at the corners surrounded by small m blocks with random shapes this region also seems to have lost its fractal self similar geometry fig 6 a and b show configurations of the ms during the infiltration reaction process with p 10 3 at two times infiltration lengths are also indicated similarly to the sc most sites of the smallest blocks of the ms reacted when t τ 10 3 r t 1 and the infiltration length is approximately equal to their lateral sizes when t τ 10 4 r t 10 the smallest blocks of the infiltrated region disappeared and the reaction advances in the corners and edges of the largest blocks again the infiltrated region is an apparently inhomogeneous medium with nearly periodic not fractal distribution of inclusions with rough surfaces 3 2 evolution of the infiltration length the time evolutions of the infiltration lengths in the sc and in the ms are shown in fig 7 a and b respectively for several values of p the case p 0 in which there is no reaction is also shown in those plots the model without reaction is equivalent to the random walk infiltration with particles injected from the borders of the fractals reis 2016 the infiltration exponent n eq 1 is related to the random walk exponent ν eq 5 by 9 n ν d f d b where db is the dimension of the infiltration border reis 2016 the slopes of the plots for the non reactive cases in fig 7 a b are consistent with this relation and the tabulated values of ν balankin 2017 n 0 329 0 002 for the sc and n 0 407 0 012 for the ms in the reactive case p 0 f deviates from the value of the non reactive case and the deviations become larger as p increases in both fractals the long time data for p 10 2 and 10 3 are consistent with normal fickean infiltration there are some fluctuations around the slope 0 5 which are the log periodic oscillations expected in kinetic models in deterministic fractals bab et al 2008 akkermans et al 2012 haber et al 2014 for p 10 3 fig 7 a shows that the infiltration in the sc is entering the normal regime at t τ 10 4 10 5 this is qualitatively consistent with the snapshot in fig 5 a at t τ 10 4 which suggests that the reaction products inside the infiltrated region move in a medium with approximately periodic not self similar obstacles similar results are obtained in the ms for p 10 3 the onset of the normal regime at t τ 104 as suggested in fig 7 b is consistent with the snapshot in fig 6 b in which the infiltrated region has lost the self similarity and became a medium with approximately periodic obstacles however for the slowest reaction rates p 10 6 and 10 5 in the sc p 10 5 and 10 4 in the ms fig 7 a b show only small deviations from the non reactive cases in the ms the deviations are very small for p 10 5 in other words the simulations do not show a clear crossover to normal diffusion for slow reactions the evidence of that crossover will be provided below with the proposal of a scaling approach and the estimates of crossover times sections 3 4 and 3 5 this means that the absence of a normal diffusion regime in the slow reaction plots of fig 7 a b is a consequence of the limitations in the simulation times 3 3 evolution of the reaction length let ar t be the area of m that reacted at time t in the sc and let vr t be the volume that reacted at time t in the ms analogously to the infiltration length here we define a reaction length r as the ratio between ar vr and the length l area l 2 of the infiltration border of the sample r a r l in the sc r v r l 2 in the ms fig 8 a and b show the time evolution of r in the sc and the ms respectively at short times the data for all values of p have slopes larger than 1 which are anomalously large values for comparison in reaction controlled regimes with approximately constant areas attacked by the solution the mass loss increases linearly in time and in diffusion controlled regimes it increases as t 1 2 see e g reeves and rothman 2014 reis 2015 however the fast increase observed here indicates a global reaction rate proportional to dr dt increasing in time the dashed lines in fig 8 a b have slopes 1 n where n is the infiltration exponent obtained in the sc and in the ms using eq 9 those slopes are very close to those of the r t plots this suggests a connection between the reaction rate anomaly and the infiltration anomaly this connection is theoretically addressed in section 3 4 at long times the data for the largest p in fig 8 a have slopes consistent with the diffusion control of the reaction rate in fig 8 b the diffusive regime is not clearly observed which suggests that the convergence of r to the normal regime is slower than the convergence of f possibly this is a consequence of the large difference in the slopes of the short and long time regimes the differences between initial and final slopes of the plots of f are much smaller fig 7 a b 3 4 scaling approach consider the infiltration without reaction in the sc with an external border of a large size l the area of the infiltrated region is 10 a i l f l a t τ n where the appropriate physical dimensions were added to the scaling of eq 1 eq 10 is confirmed by the short time data in fig 7 a note that ai is the total area of unsaturated p sites so the number of unsaturated p sites is ai a 2 now consider the infiltration with reaction in the sc due to the rule of construction of that fractal fig 1 a the perimeter of the blocks has the same fractal dimension df of the whole fractal this can be justified by the calculation of the total number of sites and of the number of sites in that perimeter at any order reis and riera 1993 the ratio between those numbers converges to a finite value as the order increases let nr be the number of reactive m sites in the infiltrated region i e m sites in contact with unsaturated p sites the equality of dimensions of the fractal and of its borders implies that nr is of the same order of the total number of p sites in the infiltrated region nr ai a 2 consequently the length of the blocks in contact with unsaturated p sites is 11 l r a n r a i a at short times rt is the probability of detachment of a kink site at time t the m sites preferentially react at the corners of the blocks and with the progress of the reaction the number of kinks of those blocks increases the reaction advances in each block to transform an average thickness 12 l r t a the area of the blocks that reacted is ar l lr using eqs 10 12 we obtain 13 a r r t a i l a p t τ 1 n in a fractal embedded in two dimensions the mass that reacts is proportional to ar the reaction length scales as ar l so that 14 r a p t τ 1 n this explains the large initial slopes of the plots in fig 8 a which were actually close to 1 n the crossover to fickean infiltration is expected when ar matches the infiltrated area ai in this condition most blocks with lateral size smaller than f have already reacted and the infiltrated region is loosing its self similarity using eq 13 with ai ar at t t c we obtain 15 t c r 1 p 1 τ at t tc the orders of magnitude of the infiltration and reaction lengths are 16 f c r c a p n and the reaction has advanced in a thickness 17 l c a in each block at t tc the infiltrated area increases diffusively the area ar which increased fast at short times is now controlled by the diffusive infiltration because the reaction can only occur at m sites in contact with the unsaturated solution thus the reaction length also increases as t 1 2 analogous arguments apply to the ms in which ai is replaced by the infiltrated volume vi and ar is replaced by the volume vr that reacted at a given time t at short times vr also shows an anomaly as 18 v r r t v i l 2 a p t τ 1 n where l is the lateral size of the infiltration border the crossover to fickean infiltration takes place when vr vi and also leads to eq 15 for the crossover time at the crossover time the infiltration length the reaction length and the average thickness in which the reaction advances in each m block are also given by eqs 16 and 17 at t tc infiltration and reaction lengths scale as t 1 2 the reason for the short time anomaly of the reaction length is the delocalization of the reaction it is spread through mineral grains the m blocks in the whole infiltrated region instead after the crossover it is possible to state that a reaction front follows the infiltration front propagation in the z direction the theoretical approach of this section shows that the crossover from anomalous to normal infiltration and reaction is expected for all sets of model parameters however note that the time necessary for observing a clearly normal regime is much larger than tc eq 15 for instance in the sc with p 10 3 eq 15 predicts tc 103 τ while fig 7 a shows a normal regime for t τ 104 i e more than one time decade longer than tc for smaller values of p 10 4 or less we only expect to observe the normal regime for t τ 105 which is close to the maximal simulation time this explains why the simulation of slow reactions small p in sections 3 2 and 3 3 did not show clear evidence of the normal regime similar arguments explain the simulation results in the ms for p 10 4 3 5 numerical estimates of crossover times the most frequent procedure to calculate the crossover time is to fit short time and long time regimes by power laws and to take tc at the intersection of those fits see e g reis et al 2018 however for the smallest p the normal infiltration regime is neither attained in the sc nor in the ms moreover for the largest p there are oscillations in that regime which makes it difficult to fit the long time data for these reasons the usual procedure to calculate tc does not work with our simulation data here we overcome these difficulties with an alternative method that parallels the calculation of relaxation times in kinetic roughening models proposed in reis 2002 we define characteristic times t δ in which the infiltration length has a given relative deviation from the anomalous value these times may be much smaller than tc but are expected to be proportional to tc the infiltration length depends on p and on the time t so it can be written as f p t in the case without reactions it is f 0 t at a given time t we define the fractional difference f p t with and without reaction as 19 f f p t f 0 t f 0 t we then define the deviation time t δ p as the first time in which f attains a preset value δ i e f p t δ δ and f p t δ for all t t δ for the sc we consider δ 3 and δ 5 for the ms we consider δ 2 and δ 3 the estimates of t δ as a function of p 1 are shown in fig 9 a and b for the sc and ms respectively in both cases the fits for large p 1 have slopes close to 1 the slopes of the fits of the ms data are slightly smaller because the values of δ are smaller and the range of p is narrower these results show that t δ has the scaling predicted in eq 15 for tc for any deviation δ this suggests that a crossover time tc in which the deviation is so large that the infiltration actually crosses over to a normal behavior will also scale as in eq 15 4 discussion 4 1 morphological evolution of the m blocks although the present model is designed for a reaction that produces soluble products and a porous material the morphology of the m blocks after partial reaction figs 5 b and 6 b is similar to that of experiments and simulations with solubilization of all the reaction products for instance recent simulations of dissolution of cubic blocks with nanoscopic sizes showed that the dissolution began in the corners and propagated along the edges producing rounded objects briese et al 2017 moreover recent studies of the dissolution of single calcite crystals showed significant rounding at corners and edges which indicates enhanced dissolution in those regions compared to the flat parts of the grains noiriel et al 2019 yuan et al 2019 in these cases the rounding is observed in scales of micrometers thus the present model provides a realistic morphological evolution of reactive mineral grains in several lengthscales this is an essential feature for modeling inclusions in self similar fractal porous materials 4 2 infiltration and reaction anomalies at short times a nontrivial result of our model is the combination of anomalously slow infiltration with n 1 2 in eq 1 and anomalously fast reaction with reaction length increasing faster than linearly in time or time increasing reaction rate while the former is expected in fractal media the latter was not previously reported the reaction anomaly is an effect of the combination of the linear time increase of the dissolved thickness of each m block which is usual for a constant surface area in contact with unsaturated solution eq 12 and the time increase in the number of m blocks in contact with that solution due to infiltration eq 1 the experimental observation of these phenomena may be possible if the time scale of the process is much smaller than the crossover time in porous media characterized by a surface fractal dimension in short lengthscales the reaction anomaly is also expected in the present model mass fractality or volume fractality is related to the self similarity of the sc or ms since the density of porous sites decreases as the scale of measurement increases mass fractality is observed in length scales larger than a however surface fractality is usually related to the self affine properties of the roughness of internal surfaces barabási and stanley 1995 krug 1997 in our model the partially dissolved blocks inside the infiltration region of fig 6 b are examples of grains with rough surfaces in the case of kinetic roughening of those surfaces it is possible that the area of each reactive grain increases in time particularly in cases of roughening anomaly krug 1997 thus the reaction length may increase faster than t 1 n which means that the anomaly may be enhanced 4 3 crossover time eq 17 shows that the crossover time is of the same order of magnitude of the time for a layer of thickness a at the surface of the blocks to react this thickness corresponds to the size of the smallest gaps between the mineral inclusions and is also close to the size of the smallest inclusions physically the lattice constant a can be interpreted as the largest length scale in which the medium is homogeneous the hopping time τ and the reaction rates r ϵ were defined in the scale of a lattice site where they can be interpreted as average quantities for a large number of atoms or molecules moving in solution and undergoing reactions respectively using eqs 7 and 15 the crossover time may be related to physically measurable quantities as 20 t c a v k thus the crossover depends only on the reaction chemistry and on a characteristic length of the fractal medium but not on the diffusion coefficient of the reaction products in other words the onset of fickean diffusion depends on the rate in which the obstacles of the fractal medium are destroyed by the reaction but not on the mobility of the diffusing products also note that tc does not depend on fractal properties of the medium such as the fractal dimension but only on the largest length scale of homogeneity a eqs 15 and 20 may be extended to other fractals embedded in different spatial dimensions since eq 20 is a very simple relation for tc which depends only on three physical and chemical parameters we expect that it can provide estimates for the order of magnitude of tc in real fractal media where those parameters are known 4 4 crossovers in some experiments and models here we are interested in fractal media in which previous reactive transport models assumed fickean diffusion since our estimates of tc may be used to criticize that assumption as a first application we consider the basaltic andesite clasts from costa rica presented by sak et al 2004 which have weathered rinds of some centimeters surrounding the unweathered cores plagioclase dissolution was the main factor for the disaggregation of the rock and the formation of those rinds sak et al 2004 navarre sitchler et al 2009 neutron scattering techniques were used to analyze the fractal properties of the pore size distributions of the clasts and provided evidence of mass fractality in lengths 1µm and larger while the fractality in smaller length scales was related to roughness of pore solid interfaces navarre sitchler et al 2013 for using eq 20 we consider the homogeneity lengthscale a 1 μ m the main reacting minerals are anorthite and albite sak et al 2004 with an equivalent plagioclase being labradorite which has a dissolution rate constant k 2 10 12 mol m2s see also reeves and rothman 2014 the order of magnitude of the specific volume is v 10 4 m3 mol these values give a crossover time tc 102 yr it is a very long time in laboratory scale where subdiffusion is expected however the weathering ages of those clasts ranged from 50 kyr to 300 kyr which are 2 or 3 orders of magnitude larger than our estimate of tc thus our estimate supports the assumption of fickean diffusion in previous models for weathering rind formation navarre sitchler et al 2011 reeves and rothman 2014 lebedeva et al 2015 however note that our model does not represent details of the reactions and of the diffusion mechanism it only suggests that the changes in the porous medium due to the reactions were sufficient to allow the normal infiltration in the weathering timescale at this point it is also important to stress that our arguments are not invalidated by the surface fractality suggested by the neutron scattering experiments of navarre sitchler et al 2013 for instance figs 5 and 6 clearly show the roughening of the surfaces of the m blocks after the crossover to normal infiltration the roughening is a short scale feature that does not affect the normal diffusion in the region between the rough blocks as a second possible application we consider the samples of diabase whose pore structures were studied by neutron scattering techniques in bazilevskaya et al 2015 the pore size distributions indicated a transition from surface to volume fractal at a size near 300 nm thus for application of our model we consider a 300 nm and the above data for plagioclase which was also the main reacting mineral the crossover time given by eq 15 is t c 10 1 10 2 yr the formation of thin regolith on diabase rock was formerly explained by a model where water transport obeyed a fickean diffusion equation bazilevskaya et al 2013 the thickness of the reaction front was 20 cm and the velocity of that front was 10 13 m s which give a total wheathering time of order 10 kyr this is much longer than our estimate of crossover time which suggests that the model with fickean diffusion is reasonable as a final note in laboratory timescales the infiltration in the unweathered materials mentioned above is expected to be anomalous navarre sitchler et al 2009 measured concentration profiles of an infiltrating br gas in the basaltic clasts for periods from 7 to 34 days which is much smaller than our estimate of tc however those profiles were fitted under the assumption of fickean diffusion which led to estimates of diffusion coefficients in such cases we believe that it would be important to check for possible anomalies in the concentration profiles 5 conclusion deterministic fractals embedded in two and three dimensions namely a sierpinski carpet and a menger sponge were used as models of fractal porous materials and the non fractal squares and cubes formed in their constructions were used as models of inclusions of a reactive mineral the solution inside the pores was initially in chemical equilibrium with the mineral but it was displaced from equilibrium when one external surface was put in contact with running water that can remove the soluble reaction products the transport of those products was diffusive and led to an anomalous infiltration of water into the porous media the reaction of the mineral with the pore solution was modeled by an extension of a thermally activated model of dissolution of mineral surfaces and provided a realistic description of the morphological evolution of mineral blocks in all cases we considered slow reactions in comparison with the diffusion in the lengthscale of a lattice site this corresponds to small damköhler numbers in a homogeneous porous region at short times the infiltration is anomalous subdiffusive as observed in previous works without chemical reactions however the volume changed by the reaction increases anomalously fast faster than linearly because the reaction advances on the surface of the mineral inclusions with approximately constant area while the number of attacked inclusions increases as the infiltration front propagates this corresponds to a delocalization of the reaction front since the reactions are distributed in the whole infiltrated region normal fickean infiltration is observed at long times which is accompanied by diffusive increase of the changed volume in this regime the reaction front follows the infiltration front the crossover between these regimes occurs at a time in which the thickness of the altered layer around the mineral surface is of the same order of magnitude of the width of the smallest gaps between those inclusions after the crossover simulations show an infiltration region with large density of unsaturated sites and with approximately periodic non fractal distribution of mineral blocks with rough surfaces the crossover time is shown to depend on the reaction rate constant and on the width of the smallest gaps between the inclusions which is the smallest homogeneous site of the medium i e fractality occurs in larger scales however that time does not depend on the diffusion coefficient of reaction products in smaller scales because the reaction which destroys the inclusions is the mechanism that suppresses the self similarity moreover that time does not depend on fractal characteristics of the medium we applied the model to rocks where plagioclase was the dominant reactive mineral and whose fractal properties were reported in neutron scattering experiments using experimental estimates of their minimal lengths with fractal properties we obtained crossover times from tens to hundreds of years these values are much smaller than the geological times in which chemical weathering occurred this gives support to the assumption of fickean diffusion in reactive transport models for those processes as proposed in some recent works despite the usual expectation of anomalous diffusion in fractal media we believe that the present work also motivates the incorporation of microscopic models of reaction e g the thermally activated model proposed here in fractal media in which the transport of soluble products is advective it may be useful in applications such as regolith evolution in which structural changes due to chemical weathering are very important or flow of co2 in porous reservoirs which shows dissolution and growth phenomena luquot and gouze 2009 garing et al 2015 nermoen et al 2015 beckingham et al 2017 gray et al 2018 singh et al 2018 cui et al 2018 yang et al 2019 declaration of competing interest the author declares that there is no conflict of interest acknowledgment the author thanks vaughan voller for helpful discussions on infiltration processes this work was supported by the brazilian agencies cnpq 304766 2014 3 faperj e 26 202941 2015 and capes 88887 310427 2018 00 
557,when the solution in a porous medium is in chemical equilibrium with mineral grains and an external surface is put in contact with water the infiltration of unsaturated solution may be accompanied by mineral reactions that change the morphology of that medium here we study the evolution of this infiltration process considering that the initial porous media are fractal and that the reactions form additional porous material plus soluble products a sierpinski carpet and a menger sponge are the fractal models the mineral inclusions are represented by the non fractal blocks squares and cubes formed in their iterative constructions and reaction rates are described by a thermally activated model in conditions of slow reactions in comparison with the diffusion the interplay between the infiltration by diffusion and the structural changes by the reactions is explained by a combination of kinetic monte carlo simulations of lattice models and a scaling approach at short times the infiltration is subdiffusive but the dissolved mass increases anomalously fast due to delocalization of the reaction which is distributed through a time increasing infiltrated region at long times normal fickean infiltration and reaction are observed the crossover between the two regimes occurs when the thickness of the altered layer formed around the mineral inclusions is of the same order of magnitude of the width a of the smallest gaps between those inclusions the order of magnitude of the crossover time tc is estimated as a vk where k is the reaction rate constant and v is the molar volume of the mineral this time does not depend on the diffusion coefficient of reaction products or on the fractal geometry of the initial medium which suggests the application to a variety of systems estimates of tc are obtained in two recently studied rocks with reported fractal properties and justify the fickean diffusion assumption of previous models for their weathering in geological time scales the morphological evolution of the mineral blocks that partially reacted qualitatively agrees with experiments 1 introduction if a porous medium is filled with a static fluid and an external surface is in contact with a reservoir of a solute then this solute infiltrates into or out of that medium depending on the concentration difference the infiltration front penetrates into a length f that scales in time t as 1 f t n where n 1 2 represents normal fickean infiltration and n 1 2 represents anomalous transport reis 2016 reis and voller 2019 an equivalent infiltration process occurs when the external surface of a dry porous medium is in contact with a reservoir of a fluid at a fixed pressure and the domain geometry constrains the flow to a one dimensional horizontal direction i e without the effect of gravity voller 2015 the cases of subdiffusive transport n 1 2 and of superdiffusive transport n 1 2 were already shown in these processes theoretically voller 2015 reis 2016 reis et al 2018 reis and voller 2019 and experimentally küntz and lavallée 2001 el abd and milczarek 2004 gerasimov et al 2010 li and zhao 2012 el abd 2015 filipovitch et al 2016 delgado et al 2016 the transport of fluids and of solutes in porous media of geological origin is frequently combined with chemical reactions that change the physical structure of those media berkowitz et al 2016 hommel et al 2018 in some cases diffusion is the dominant transport mechanism lockington and parlange 2003 sak et al 2004 bazilevskaya et al 2013 chagneau et al 2015b 2015a for instance celestite precipitation in the pores of a clay was recently shown to block the diffusion of chloride ions chagneau et al 2015b in other cases the reactions may increase the porosity of the medium one example is the formation of weathering rinds chemically altered layers enclosing unweathered cores in basalt clasts sak et al 2004 navarre sitchler et al 2009 the fractal geometry of the pore networks of those rinds and cores were revealed by neutron scattering methods navarre sitchler et al 2013 moreover bazilevskaya et al 2013 suggested that diffusion is the main mechanism for solute transport in weathered diabase at the virginia piedmont united states and fractal properties of unweathered and weathered diabase were also shown in a neutron scattering study bazilevskaya et al 2015 this raises the question on how the reactions of the minerals of a sample with fractal pore geometry affect the diffusive infiltration since there are many other geological materials with fractal pore structure farin and avnir 1987 sahimi 1993 this question has a broad interest in this work we study the interplay between the infiltration driven by concentration gradients in fractal porous media and the reaction between water and inclusions of a mineral we use two deterministic fractals a sierpinski carpet and a menger sponge to model the initial porous matrix while the compact blocks generated in their constructions represent the mineral grains the solution in the pores is in chemical equilibrium with the mineral when an external surface is put in contact with unsaturated solution and the reaction proceeds to transform the mineral into porous material plus soluble diffusing products the microscopic dynamics of diffusion and reaction do not change in time so that any change in the evolution of the infiltration and of the reaction fronts is directly related to changes in the medium geometry consequently at short times we always observe an anomalous behavior we use a combination of kinetic monte carlo simulations of a reactive transport model and a scaling approach to study this problem for rapid reactions the simulations show an initial regime with anomalously slow infiltration and anomalously fast reaction and a long time regime with normal diffusive infiltration and reaction however for slow reactions they only show small deviations from the initial anomalous behavior at the longest simulated times the scaling approach then provides a theoretical explanation for the crossover from anomalous to normal behavior which is predicted for any parameter set a relation between the crossover time and the model parameters is obtained and is confirmed by an extended analysis of the simulation data the crossover is shown to occur when the dissolved thickness of the mineral inclusions is of the same order of magnitude of the width of the gaps between those inclusions and the crossover time is shown to depend on the reaction chemistry and on those gap sizes but not on the transport properties in the above mentioned processes of formation of weathering rinds in basalts and of chemical weathering of diabase rock we estimate the orders of magnitude of the crossover times and show that they are much smaller than the weathering times which supports the assumption of fickean diffusion in previous models for those processes this paper is organized as follows section 2 presents the fractal lattices the model of infiltration and reaction and information on the methods of solution section 3 presents simulation results and the scaling approach to explain short time long time and crossover properties the consequences of the results are discussed in section 4 section 5 summarizes results and conclusions 2 models and methods 2 1 fractal porous media the porous media are defined in two dimensional or three dimensional hypercubic lattices in which a site has lateral size a lattice constant the first fractal is the sierpinski carpet sc whose iterative construction is shown in fig 1 a the order of iteration is denoted as o the part shown in tan color represents a porous material which is permeable to the fluid but is not reactive the corresponding lattice sites are termed p sites this is the fractal part of the sample which has dimension d f ln 16 ln 5 1 7227 in the limit of infinite iterations the remaining part red square blocks in fig 1 a represents inclusions of a reactive mineral which are impermeable to the fluid the lattice sites of these inclusions are termed m sites this is the non fractal part of the sample with dimension 2 note that the length of the smallest inclusions and the width of the narrowest gaps between them are of order a the red blocks in fig 1 a are usually termed removed blocks in models where only the fractal part is physically meaningful this is not the case here because the sites of those blocks may be transformed into porous material when they react see section 2 2 the sc is helpful for basic investigations of transport phenomena in fractals see e g the illustration of streamlines in luo et al 2014 and usually provides results more accurate than those obtained in fractals embedded in three dimensions see e g estimates of random walk dimensions in carpets in balankin et al 2016 moreover recent experiments on glycerin infiltration in a horizontal hele shaw cell with obstacles organized as in the carpets showed anomalous behavior with exponents n in excellent agreement with simulations voller 2015 filipovitch et al 2016 the second fractal is the menger sponge ms whose generator is shown in fig 1 b analogously to the sc we assume that the sites of the fractal represent a permeable porous material p sites and that the red cubes fig 1 b represent reactive inclusions of a mineral m sites which are impermeable to the fluid the fractal dimension of the porous medium is d f ln 98 ln 5 2 8488 the size of the smallest inclusions and the width of the narrowest gaps between them are also of order a this and other sponges were already used as models of geological and construction materials atzeni et al 2008 villalobos et al 2009 cihan et al 2009 the relevance of this class of fractals is also shown in a recent study of darcy like flow in transparent plastic samples with the pore structure of an inverse ms balankin et al 2016 the sc and the ms are fractals with infinite ramification in which two points with increasing distance are connected by an increasingly large number of paths through the medium havlin and ben avraham 2002 gefen et al 1984 this property implies that the transport to long distances is not suppressed by the occlusion of a finite number of gaps it contrasts with the finite ramification of sierpinski gaskets and comb like structures in which infinitely large parts can be isolated by cutting a finite usually small number of connections the order of ramification has drastic consequences in the choice of the methods of solution of diffusion and other statistical models as will be explained in section 2 4 the organized distribution of inclusions in the sc and in the ms also implies that the mass distribution is correlated at distances that reach the total size of the samples this contrasts with critical percolation clusters which are fractal sets generated by random distributions of pore sites stauffer and aharony 1992 2 2 model of infiltration and dissolution consider a reaction of a mineral mi which is schematically represented as 2 mi h 2 o c sp nsp where sp is a soluble product with stoichiometric coefficient c and nsp represents the non soluble products we assume that this reaction is isovolumetric that the nsp form a immobile porous material and that the properties of this material are not different from the properties of the initial porous matrix which has a fractal geometry section 2 1 the product sp moves in the pores according to its concentration gradient this phenomenology is represented in our lattice model but details of the physical interpretation are postponed to section 2 3 the two types of sites m and p were defined in section 2 1 and are also shown in fig 2 a m represents the mineral mi and p represents the initial porous matrix or the nsp in a real porous material the solution may have continuously varying concentration of the product sp however our model simplifies this feature by considering only two states of the solution a saturated state in which the p site contains one s particle and an unsaturated state in which the p site does not have an s particle these states are shown in fig 2 a in the saturated state the pore solution is in chemical equilibrium with the mineral in its nearest neighbor nn sites the unsaturated state represents zero concentration of reaction products so the rate of the reaction of the mineral with the solution has a far from equilibrium value an equivalent model was used by reis 2015 to describe the kinetics of formation of altered layers on the surfaces of mineral grains the initial distribution of m and p sites and of s particles in the sc is illustrated in fig 3 a all p sites are saturated i e they contain an s particle so that the mineral and the solution are in chemical equilibrium at all points at time t 0 the external surface of the medium with z 0 is put in contact with slowly running water the initial configuration of the ms follows the same rules the diffusion of soluble reaction products sp in eq 2 is represented by random walks of the s particles all s particles attempt to hop to a randomly chosen neareast neighbor nn site with rate h the hop is executed only if the target is an unsaturated porous site with no s particle as shown in fig 2 b or if the target site is at the external surface as shown in fig 2 c in the latter case the s particle is removed from the system this is the mechanism that leads to infiltration of unsaturated solution into the medium and the propagation of a reaction front in the absence of reactions this model is equivalent to the infiltration model of reis 2016 and reis and voller 2019 fig 3 a illustrates the hopping attempts of two s particles in the initial configuration of the sc the resulting configuration is shown in fig 3 b which highlights a solid site in contact with unsaturated solution and shows s particles being carried away by the flowing water now we describe the reaction model when an unsaturated p site is a nn of an m site the reaction may occur as illustrated in fig 2 d the two neighbors viz m site and unsaturated p site are both transformed into saturated p sites this rule represents the transformation of the mineral into a porous material eq 2 and the local equilibration of the solution by the reaction products the reaction rate of a given site depends on the number n of nn which are also m sites as proposed in models of thermally activated crystal dissolution lasaga and blum 1986 mccoy and lafemina 1997 fig 4 shows the possible neighborhoods of m sites in a simple cubic lattice with the corresponding values of n in our model in three dimensions which is applicable to the ms the sites with n 3 have reaction rate r these sites are usually termed isolated n 0 single bonded n 1 two bonded n 2 or kinks n 3 the sites with n 4 steps have rate ϵr and those with n 5 terrace have rate ϵ2 r with ϵ 1 with these rules the reaction of kink sites or sites with smaller coordination is much faster than the reaction of the strongly bonded step and terrace sites in the sc the reaction rates are properly modified sites with n 0 1 and 2 react with rate r kinks have n 2 see fig 1 a and sites with n 3 react with rate ϵr where ϵ 1 fig 3 c shows a configuration of the sc after some time of infiltration and reaction the m sites that can react are highlighted a reactive m site and a hopping s particle are indicated by arrows the configuration after the reaction of that m site and the hop of that s particle is shown in fig 3 d first the transformation of the m site into a p site and formation of two new s particles second the hop of the s particle to the right leaving unsaturated solution in contact with the m site at its left highlighted in fig 3 d 2 3 physical interpretation of the model parameters the average time for a hop of an s particle is 3 τ 1 h we only consider slow reactions compared to diffusion so that r h this means that the probability of a reaction to occur at a given point is much smaller than the probability of solution equilibration by diffusion in a neighborhood of size a in a small region with only porous sites the diffusion coefficient d of the soluble products is given by 4 d a 2 e τ where e is the euclidean dimension of the hypercubic lattice note that this diffusion coefficient is characteristic only of homogeneous regions of the porous material in lengthscales in which the medium is fractal subdiffusion is expected bouchaud and georges 1990 havlin and ben avraham 2002 metzler et al 2014 in this case the root mean square displacement d of a tracer particle evolves as 5 d t ν with ν 1 2 dimensional factors were omitted in eq 5 this anomaly is related to the self similar distributions of irregularities such as impenetrable barriers or dead ends which lead to delays in the tracer motion without characteristic time scales havlin and ben avraham 2002 the standard model of thermally activated crystal dissolution in highly unsaturated solution lasaga and blum 1986 mccoy and lafemina 1997 considers that the detachment of a site at the surface occurs with rate h 0 μn where h 0 is a characteristic frequency of order 1012hz μ is an arrhenius factor and n is the number of nn of that site the same as in fig 4 in that model a site represents a molecule or an ion of the mineral recent simulations of dissolution of rough crystalline surfaces with this model showed that de assis and reis 2018 i sites with small coordination n 2 rapidly detach and have no significant effect on the global dissolution rate ii the detachment of kink sites n 3 is the rate limiting step when the surface has a non negligible roughness iii the surface morphology evolves with the increase in the fraction of step sites n 4 and terrace sites n 5 whose detachment is much slower however in our model the lattice site has a different interpretation it represents a mesoscopic volume of mineral or of porous material which have large numbers of molecules thus an m site with a small number of nn n 2 corresponds to a small mineral grain or to a protuberance of a grain the reaction of this m site is expected to be controlled by the detachment of kinks at molecular scale as shown in the simulations of the standard dissolution model de assis and reis 2018 this explains the use of the same reaction rate r for all m sites with n 3 in three dimensions and the same rate for all sites with n 2 in two dimensions we only distinguish their rate r from the rates of steps and terrace sites which are smaller by factors ϵ and ϵ2 respectively in the time interval τ the probability of transformation of a kink solid site in contact with a nn unsaturated pore is 6 p r τ this probability is used as a parameter in the simulations section 2 4 the condition of slow reaction compared to diffusion implies p 1 when a kink site reacts fig 2 e the change in the number of moles of the mineral is δ n m a 3 v where v is its molar volume the area in contact with the unsaturated solution is δ a a 2 considering that the global reaction rate is controlled by the reaction of kink sites we can estimate the reaction rate constant as 7 k p δ n m δ a τ r a v if we consider a homogeneous medium in which the diffusion coefficient can be defined eq 4 then we can use eqs 6 and 7 to obtain 8 p a v 6 k d this shows that p has the form of a second damköhler number which is a dimensionless ratio between rates of reaction and diffusion the factor av 6 adjusts the different units of the two rates finally note that our model only describes the forward reaction of eq 2 the backward case is not modeled because we neglect the possible occurrence of supersaturation in the pore solution 2 4 methods of solution in the absence of reactions the continuous limit of the infiltration model is equivalent to the diffusion equation with a boundary with constant concentration reis and voller 2019 and the infiltration exponent n eq 1 is related to the random walk exponent ν eq 5 in infinitely ramified fractals such as the sc and the ms those exponents cannot be calculated exactly by renormalization schemes gefen et al 1984 for this reason most works on diffusion in carpets and sponges use numerical methods of solution kim et al 1993 reis 2016 a recent work conjectured an expression for the exponent ν in the carpets which is in good agreement with numerical results but there is no proof that it is an exact result balankin et al 2016 in the present model the fractal medium changes due to the reactions which is an additional obstacle to the use of analytical methods of calculation this scenario justifies the use of kinetic monte carlo simulations the model was simulated in the 5th order of iteration of the sc in which the lateral size is l 3125 the values of the probability p ranged from 10 2 to 10 6 the factor to reduce the step site reaction is ϵ 10 4 in all cases the maximal simulation time was 106 τ for each p a number of realizations between 100 and 250 was used to calculate average areas of infiltrated material unsaturated p sites and average areas of the lost material m sites that reacted simulations in de assis and reis 2018 suggest that ϵ 10 4 in calcite dissolution at room temperature and this parameter is expected to have smaller values for minerals with smaller dissolution rates since the parameter that controls the propagation of the reaction front is p or r the choice of a fixed value for ϵ does not have significant effect in the results in the ms the lateral size was l 625 which corresponds to the 4th stage of generation of the fractal the values of p ranged from 10 2 to 10 5 and ϵ 10 4 was considered in all cases the maximal simulation time was 4 104 τ the number of realizations used to calculate average quantities was between 20 and 40 the surface in contact with the flowing solution is z 0 the opposite surface is not reached by the infiltration front during the simulation time i e the unsaturated p sites with the largest z are very far from that surface periodic boundary conditions are adopted in x and y directions in infiltration models without dissolution a small number of realizations in a large lattice was shown to provide accurate values of the infiltrated area or volume at long times reis and voller 2019 here we confirmed that this also occurs with 10 realizations when dissolution is involved despite the use of an improved simulation algorithm the time for computation was long in the three dimensional lattices due to the large number of hopping particles and the large number of possible reaction events particularly for the largest p for instance one configuration of infiltration and reaction in the ms ran for 5 6 days in a xeon processor here we also use a scaling approach to predict relations between the orders of magnitude of characteristic times and lengths of the model for any parameter set under the assumption that the relevant scaling exponent n in eq 1 is known this type of theoretical method is supported by renormalization group and universality concepts and has a focus on power law relations between the relevant physical and chemical quantities similar methods were recently used to distinguish regimes dominated by reaction and diffusion in models of growth of altered layers on mineral surfaces reis 2015 and to connect infiltration and diffusion exponents in fractals reis 2016 reis et al 2018 the consistency of the predictions of the scaling approach is checked with numerical simulation results 3 results 3 1 general features of the infiltration reaction process fig 5 a and b show shapshots of the sc during the infiltration reaction process with p 10 3 and p 10 5 respectively the infiltration length f in two three dimensions is defined as the ratio between the area volume of unsaturated p sites and the length l area l 2 of the infiltration border this is the standard definition in models of fluid infiltration voller 2015 in which the front is sharp and can be extended to problems of solute infiltration in which the front is diffuse reis 2016 reis and voller 2019 the points of the medium with distance smaller than f from the infiltration boundary form the infiltrated region in fig 5 a b the length f is indicated by a solid bar at the left sides of the snapshots in order to represent the typical length in which water penetrates in the sc however note that the concentration of unsaturated sites varies slowly with the depth z the large width of the infiltration front is an expected feature in random walk models for p 10 3 fig 5 a at t τ 10 2 we observe a small number of sites with reactions at t τ 10 3 there is significant dissolution of the smallest m blocks in this case we have r t p t τ 1 which means that all m sites in contact with unsaturated p sites have probability of reaction of order 1 at t τ 10 4 all sites of the smallest blocks reacted and the reaction also advances at the corners and edges of the largest blocks of the infiltrated region in this case r t 10 in that region the porous medium has lost its self similar properties and contains only an approximately periodic distribution of obstacles m blocks with random shape for p 10 5 similar features are observed but at longer time scales at t τ 10 3 we have r t 0 01 which suggests very low dissolution of the mineral in contact with undersaturated solution this is actually observed in the corresponding snapshot of fig 5 b at t τ 10 5 we have r t 1 and most of the smallest blocks of m sites in the infiltrated region have already reacted at t τ 10 6 r t 10 the infiltrated region has uniformly distributed large m blocks partially altered at the corners surrounded by small m blocks with random shapes this region also seems to have lost its fractal self similar geometry fig 6 a and b show configurations of the ms during the infiltration reaction process with p 10 3 at two times infiltration lengths are also indicated similarly to the sc most sites of the smallest blocks of the ms reacted when t τ 10 3 r t 1 and the infiltration length is approximately equal to their lateral sizes when t τ 10 4 r t 10 the smallest blocks of the infiltrated region disappeared and the reaction advances in the corners and edges of the largest blocks again the infiltrated region is an apparently inhomogeneous medium with nearly periodic not fractal distribution of inclusions with rough surfaces 3 2 evolution of the infiltration length the time evolutions of the infiltration lengths in the sc and in the ms are shown in fig 7 a and b respectively for several values of p the case p 0 in which there is no reaction is also shown in those plots the model without reaction is equivalent to the random walk infiltration with particles injected from the borders of the fractals reis 2016 the infiltration exponent n eq 1 is related to the random walk exponent ν eq 5 by 9 n ν d f d b where db is the dimension of the infiltration border reis 2016 the slopes of the plots for the non reactive cases in fig 7 a b are consistent with this relation and the tabulated values of ν balankin 2017 n 0 329 0 002 for the sc and n 0 407 0 012 for the ms in the reactive case p 0 f deviates from the value of the non reactive case and the deviations become larger as p increases in both fractals the long time data for p 10 2 and 10 3 are consistent with normal fickean infiltration there are some fluctuations around the slope 0 5 which are the log periodic oscillations expected in kinetic models in deterministic fractals bab et al 2008 akkermans et al 2012 haber et al 2014 for p 10 3 fig 7 a shows that the infiltration in the sc is entering the normal regime at t τ 10 4 10 5 this is qualitatively consistent with the snapshot in fig 5 a at t τ 10 4 which suggests that the reaction products inside the infiltrated region move in a medium with approximately periodic not self similar obstacles similar results are obtained in the ms for p 10 3 the onset of the normal regime at t τ 104 as suggested in fig 7 b is consistent with the snapshot in fig 6 b in which the infiltrated region has lost the self similarity and became a medium with approximately periodic obstacles however for the slowest reaction rates p 10 6 and 10 5 in the sc p 10 5 and 10 4 in the ms fig 7 a b show only small deviations from the non reactive cases in the ms the deviations are very small for p 10 5 in other words the simulations do not show a clear crossover to normal diffusion for slow reactions the evidence of that crossover will be provided below with the proposal of a scaling approach and the estimates of crossover times sections 3 4 and 3 5 this means that the absence of a normal diffusion regime in the slow reaction plots of fig 7 a b is a consequence of the limitations in the simulation times 3 3 evolution of the reaction length let ar t be the area of m that reacted at time t in the sc and let vr t be the volume that reacted at time t in the ms analogously to the infiltration length here we define a reaction length r as the ratio between ar vr and the length l area l 2 of the infiltration border of the sample r a r l in the sc r v r l 2 in the ms fig 8 a and b show the time evolution of r in the sc and the ms respectively at short times the data for all values of p have slopes larger than 1 which are anomalously large values for comparison in reaction controlled regimes with approximately constant areas attacked by the solution the mass loss increases linearly in time and in diffusion controlled regimes it increases as t 1 2 see e g reeves and rothman 2014 reis 2015 however the fast increase observed here indicates a global reaction rate proportional to dr dt increasing in time the dashed lines in fig 8 a b have slopes 1 n where n is the infiltration exponent obtained in the sc and in the ms using eq 9 those slopes are very close to those of the r t plots this suggests a connection between the reaction rate anomaly and the infiltration anomaly this connection is theoretically addressed in section 3 4 at long times the data for the largest p in fig 8 a have slopes consistent with the diffusion control of the reaction rate in fig 8 b the diffusive regime is not clearly observed which suggests that the convergence of r to the normal regime is slower than the convergence of f possibly this is a consequence of the large difference in the slopes of the short and long time regimes the differences between initial and final slopes of the plots of f are much smaller fig 7 a b 3 4 scaling approach consider the infiltration without reaction in the sc with an external border of a large size l the area of the infiltrated region is 10 a i l f l a t τ n where the appropriate physical dimensions were added to the scaling of eq 1 eq 10 is confirmed by the short time data in fig 7 a note that ai is the total area of unsaturated p sites so the number of unsaturated p sites is ai a 2 now consider the infiltration with reaction in the sc due to the rule of construction of that fractal fig 1 a the perimeter of the blocks has the same fractal dimension df of the whole fractal this can be justified by the calculation of the total number of sites and of the number of sites in that perimeter at any order reis and riera 1993 the ratio between those numbers converges to a finite value as the order increases let nr be the number of reactive m sites in the infiltrated region i e m sites in contact with unsaturated p sites the equality of dimensions of the fractal and of its borders implies that nr is of the same order of the total number of p sites in the infiltrated region nr ai a 2 consequently the length of the blocks in contact with unsaturated p sites is 11 l r a n r a i a at short times rt is the probability of detachment of a kink site at time t the m sites preferentially react at the corners of the blocks and with the progress of the reaction the number of kinks of those blocks increases the reaction advances in each block to transform an average thickness 12 l r t a the area of the blocks that reacted is ar l lr using eqs 10 12 we obtain 13 a r r t a i l a p t τ 1 n in a fractal embedded in two dimensions the mass that reacts is proportional to ar the reaction length scales as ar l so that 14 r a p t τ 1 n this explains the large initial slopes of the plots in fig 8 a which were actually close to 1 n the crossover to fickean infiltration is expected when ar matches the infiltrated area ai in this condition most blocks with lateral size smaller than f have already reacted and the infiltrated region is loosing its self similarity using eq 13 with ai ar at t t c we obtain 15 t c r 1 p 1 τ at t tc the orders of magnitude of the infiltration and reaction lengths are 16 f c r c a p n and the reaction has advanced in a thickness 17 l c a in each block at t tc the infiltrated area increases diffusively the area ar which increased fast at short times is now controlled by the diffusive infiltration because the reaction can only occur at m sites in contact with the unsaturated solution thus the reaction length also increases as t 1 2 analogous arguments apply to the ms in which ai is replaced by the infiltrated volume vi and ar is replaced by the volume vr that reacted at a given time t at short times vr also shows an anomaly as 18 v r r t v i l 2 a p t τ 1 n where l is the lateral size of the infiltration border the crossover to fickean infiltration takes place when vr vi and also leads to eq 15 for the crossover time at the crossover time the infiltration length the reaction length and the average thickness in which the reaction advances in each m block are also given by eqs 16 and 17 at t tc infiltration and reaction lengths scale as t 1 2 the reason for the short time anomaly of the reaction length is the delocalization of the reaction it is spread through mineral grains the m blocks in the whole infiltrated region instead after the crossover it is possible to state that a reaction front follows the infiltration front propagation in the z direction the theoretical approach of this section shows that the crossover from anomalous to normal infiltration and reaction is expected for all sets of model parameters however note that the time necessary for observing a clearly normal regime is much larger than tc eq 15 for instance in the sc with p 10 3 eq 15 predicts tc 103 τ while fig 7 a shows a normal regime for t τ 104 i e more than one time decade longer than tc for smaller values of p 10 4 or less we only expect to observe the normal regime for t τ 105 which is close to the maximal simulation time this explains why the simulation of slow reactions small p in sections 3 2 and 3 3 did not show clear evidence of the normal regime similar arguments explain the simulation results in the ms for p 10 4 3 5 numerical estimates of crossover times the most frequent procedure to calculate the crossover time is to fit short time and long time regimes by power laws and to take tc at the intersection of those fits see e g reis et al 2018 however for the smallest p the normal infiltration regime is neither attained in the sc nor in the ms moreover for the largest p there are oscillations in that regime which makes it difficult to fit the long time data for these reasons the usual procedure to calculate tc does not work with our simulation data here we overcome these difficulties with an alternative method that parallels the calculation of relaxation times in kinetic roughening models proposed in reis 2002 we define characteristic times t δ in which the infiltration length has a given relative deviation from the anomalous value these times may be much smaller than tc but are expected to be proportional to tc the infiltration length depends on p and on the time t so it can be written as f p t in the case without reactions it is f 0 t at a given time t we define the fractional difference f p t with and without reaction as 19 f f p t f 0 t f 0 t we then define the deviation time t δ p as the first time in which f attains a preset value δ i e f p t δ δ and f p t δ for all t t δ for the sc we consider δ 3 and δ 5 for the ms we consider δ 2 and δ 3 the estimates of t δ as a function of p 1 are shown in fig 9 a and b for the sc and ms respectively in both cases the fits for large p 1 have slopes close to 1 the slopes of the fits of the ms data are slightly smaller because the values of δ are smaller and the range of p is narrower these results show that t δ has the scaling predicted in eq 15 for tc for any deviation δ this suggests that a crossover time tc in which the deviation is so large that the infiltration actually crosses over to a normal behavior will also scale as in eq 15 4 discussion 4 1 morphological evolution of the m blocks although the present model is designed for a reaction that produces soluble products and a porous material the morphology of the m blocks after partial reaction figs 5 b and 6 b is similar to that of experiments and simulations with solubilization of all the reaction products for instance recent simulations of dissolution of cubic blocks with nanoscopic sizes showed that the dissolution began in the corners and propagated along the edges producing rounded objects briese et al 2017 moreover recent studies of the dissolution of single calcite crystals showed significant rounding at corners and edges which indicates enhanced dissolution in those regions compared to the flat parts of the grains noiriel et al 2019 yuan et al 2019 in these cases the rounding is observed in scales of micrometers thus the present model provides a realistic morphological evolution of reactive mineral grains in several lengthscales this is an essential feature for modeling inclusions in self similar fractal porous materials 4 2 infiltration and reaction anomalies at short times a nontrivial result of our model is the combination of anomalously slow infiltration with n 1 2 in eq 1 and anomalously fast reaction with reaction length increasing faster than linearly in time or time increasing reaction rate while the former is expected in fractal media the latter was not previously reported the reaction anomaly is an effect of the combination of the linear time increase of the dissolved thickness of each m block which is usual for a constant surface area in contact with unsaturated solution eq 12 and the time increase in the number of m blocks in contact with that solution due to infiltration eq 1 the experimental observation of these phenomena may be possible if the time scale of the process is much smaller than the crossover time in porous media characterized by a surface fractal dimension in short lengthscales the reaction anomaly is also expected in the present model mass fractality or volume fractality is related to the self similarity of the sc or ms since the density of porous sites decreases as the scale of measurement increases mass fractality is observed in length scales larger than a however surface fractality is usually related to the self affine properties of the roughness of internal surfaces barabási and stanley 1995 krug 1997 in our model the partially dissolved blocks inside the infiltration region of fig 6 b are examples of grains with rough surfaces in the case of kinetic roughening of those surfaces it is possible that the area of each reactive grain increases in time particularly in cases of roughening anomaly krug 1997 thus the reaction length may increase faster than t 1 n which means that the anomaly may be enhanced 4 3 crossover time eq 17 shows that the crossover time is of the same order of magnitude of the time for a layer of thickness a at the surface of the blocks to react this thickness corresponds to the size of the smallest gaps between the mineral inclusions and is also close to the size of the smallest inclusions physically the lattice constant a can be interpreted as the largest length scale in which the medium is homogeneous the hopping time τ and the reaction rates r ϵ were defined in the scale of a lattice site where they can be interpreted as average quantities for a large number of atoms or molecules moving in solution and undergoing reactions respectively using eqs 7 and 15 the crossover time may be related to physically measurable quantities as 20 t c a v k thus the crossover depends only on the reaction chemistry and on a characteristic length of the fractal medium but not on the diffusion coefficient of the reaction products in other words the onset of fickean diffusion depends on the rate in which the obstacles of the fractal medium are destroyed by the reaction but not on the mobility of the diffusing products also note that tc does not depend on fractal properties of the medium such as the fractal dimension but only on the largest length scale of homogeneity a eqs 15 and 20 may be extended to other fractals embedded in different spatial dimensions since eq 20 is a very simple relation for tc which depends only on three physical and chemical parameters we expect that it can provide estimates for the order of magnitude of tc in real fractal media where those parameters are known 4 4 crossovers in some experiments and models here we are interested in fractal media in which previous reactive transport models assumed fickean diffusion since our estimates of tc may be used to criticize that assumption as a first application we consider the basaltic andesite clasts from costa rica presented by sak et al 2004 which have weathered rinds of some centimeters surrounding the unweathered cores plagioclase dissolution was the main factor for the disaggregation of the rock and the formation of those rinds sak et al 2004 navarre sitchler et al 2009 neutron scattering techniques were used to analyze the fractal properties of the pore size distributions of the clasts and provided evidence of mass fractality in lengths 1µm and larger while the fractality in smaller length scales was related to roughness of pore solid interfaces navarre sitchler et al 2013 for using eq 20 we consider the homogeneity lengthscale a 1 μ m the main reacting minerals are anorthite and albite sak et al 2004 with an equivalent plagioclase being labradorite which has a dissolution rate constant k 2 10 12 mol m2s see also reeves and rothman 2014 the order of magnitude of the specific volume is v 10 4 m3 mol these values give a crossover time tc 102 yr it is a very long time in laboratory scale where subdiffusion is expected however the weathering ages of those clasts ranged from 50 kyr to 300 kyr which are 2 or 3 orders of magnitude larger than our estimate of tc thus our estimate supports the assumption of fickean diffusion in previous models for weathering rind formation navarre sitchler et al 2011 reeves and rothman 2014 lebedeva et al 2015 however note that our model does not represent details of the reactions and of the diffusion mechanism it only suggests that the changes in the porous medium due to the reactions were sufficient to allow the normal infiltration in the weathering timescale at this point it is also important to stress that our arguments are not invalidated by the surface fractality suggested by the neutron scattering experiments of navarre sitchler et al 2013 for instance figs 5 and 6 clearly show the roughening of the surfaces of the m blocks after the crossover to normal infiltration the roughening is a short scale feature that does not affect the normal diffusion in the region between the rough blocks as a second possible application we consider the samples of diabase whose pore structures were studied by neutron scattering techniques in bazilevskaya et al 2015 the pore size distributions indicated a transition from surface to volume fractal at a size near 300 nm thus for application of our model we consider a 300 nm and the above data for plagioclase which was also the main reacting mineral the crossover time given by eq 15 is t c 10 1 10 2 yr the formation of thin regolith on diabase rock was formerly explained by a model where water transport obeyed a fickean diffusion equation bazilevskaya et al 2013 the thickness of the reaction front was 20 cm and the velocity of that front was 10 13 m s which give a total wheathering time of order 10 kyr this is much longer than our estimate of crossover time which suggests that the model with fickean diffusion is reasonable as a final note in laboratory timescales the infiltration in the unweathered materials mentioned above is expected to be anomalous navarre sitchler et al 2009 measured concentration profiles of an infiltrating br gas in the basaltic clasts for periods from 7 to 34 days which is much smaller than our estimate of tc however those profiles were fitted under the assumption of fickean diffusion which led to estimates of diffusion coefficients in such cases we believe that it would be important to check for possible anomalies in the concentration profiles 5 conclusion deterministic fractals embedded in two and three dimensions namely a sierpinski carpet and a menger sponge were used as models of fractal porous materials and the non fractal squares and cubes formed in their constructions were used as models of inclusions of a reactive mineral the solution inside the pores was initially in chemical equilibrium with the mineral but it was displaced from equilibrium when one external surface was put in contact with running water that can remove the soluble reaction products the transport of those products was diffusive and led to an anomalous infiltration of water into the porous media the reaction of the mineral with the pore solution was modeled by an extension of a thermally activated model of dissolution of mineral surfaces and provided a realistic description of the morphological evolution of mineral blocks in all cases we considered slow reactions in comparison with the diffusion in the lengthscale of a lattice site this corresponds to small damköhler numbers in a homogeneous porous region at short times the infiltration is anomalous subdiffusive as observed in previous works without chemical reactions however the volume changed by the reaction increases anomalously fast faster than linearly because the reaction advances on the surface of the mineral inclusions with approximately constant area while the number of attacked inclusions increases as the infiltration front propagates this corresponds to a delocalization of the reaction front since the reactions are distributed in the whole infiltrated region normal fickean infiltration is observed at long times which is accompanied by diffusive increase of the changed volume in this regime the reaction front follows the infiltration front the crossover between these regimes occurs at a time in which the thickness of the altered layer around the mineral surface is of the same order of magnitude of the width of the smallest gaps between those inclusions after the crossover simulations show an infiltration region with large density of unsaturated sites and with approximately periodic non fractal distribution of mineral blocks with rough surfaces the crossover time is shown to depend on the reaction rate constant and on the width of the smallest gaps between the inclusions which is the smallest homogeneous site of the medium i e fractality occurs in larger scales however that time does not depend on the diffusion coefficient of reaction products in smaller scales because the reaction which destroys the inclusions is the mechanism that suppresses the self similarity moreover that time does not depend on fractal characteristics of the medium we applied the model to rocks where plagioclase was the dominant reactive mineral and whose fractal properties were reported in neutron scattering experiments using experimental estimates of their minimal lengths with fractal properties we obtained crossover times from tens to hundreds of years these values are much smaller than the geological times in which chemical weathering occurred this gives support to the assumption of fickean diffusion in reactive transport models for those processes as proposed in some recent works despite the usual expectation of anomalous diffusion in fractal media we believe that the present work also motivates the incorporation of microscopic models of reaction e g the thermally activated model proposed here in fractal media in which the transport of soluble products is advective it may be useful in applications such as regolith evolution in which structural changes due to chemical weathering are very important or flow of co2 in porous reservoirs which shows dissolution and growth phenomena luquot and gouze 2009 garing et al 2015 nermoen et al 2015 beckingham et al 2017 gray et al 2018 singh et al 2018 cui et al 2018 yang et al 2019 declaration of competing interest the author declares that there is no conflict of interest acknowledgment the author thanks vaughan voller for helpful discussions on infiltration processes this work was supported by the brazilian agencies cnpq 304766 2014 3 faperj e 26 202941 2015 and capes 88887 310427 2018 00 
558,soil moisture has a strong impact on climate hydrology and agronomy at different space scales from the continent global scale to the local watershed passive microwave sensors like smos satellite soil moisture and ocean salinity allow a global study of soil moisture on the entire globe to have access to kilometric variability disaggregation algorithms have been developed such as the disaggregation based on physical and theoretical scale change dispatch this method improves the space resolution of smos soil moisture from 40 km to 1 km to do this it combines coarse scale 40 km smos products with fine scale 1 km optical thermal data validation studies on specific scales showed the potential of dispatch to enhance the spatio temporal correlation of disaggregated sm with in situ measurements under low vegetated semi arid regions although the efficiency of the method was revealed in these regions no studies fully explored its statistical behavior over a continuum of space scales in this paper we studied and compared the spatial multi scale statistics of the different input and output datasets involved in dispatch downscaling to do this we applied spectral and multifractal analysis on the respective products for the region of southeastern australia from june to december 2010 fractal and multifractal properties in the framework of the universal multifractal model were observed on inputs of dispatch smos soil moisture modis vegetation indices and surface temperature which confirmed and completed some results reported in existing literature for the output disaggregated soil moisture two scaling regimes were observed with a transition scale observed at about ten kilometers considering spectral analysis at large scales 10 km disaggregated soil moisture was found to have the same scaling as the original smos soil moisture on finer scales 10 km a different behavior was noticed with a higher value of the slope of the power spectrum the same scale break was detected on statistical moments showing that both spectral and multifractal properties of dispatch soil moisture are characterized by this twofold scaling signature keywords soil moisture multi scale analysis multifractals disaggregation smos dispatch 1 introduction soil moisture sm is a key component of the climate system and is strongly heterogeneous at many time and space scales interactions between land surface and atmosphere such as water energy and carbon fluxes are strongly related to sm ochsner et al 2013 it has a significant role in the water cycle as it impacts runoff infiltration and evaporation processes thus sm is an important variable in several scientific fields such as hydrology western et al 2004 meteorology dai et al 2004 climatology anderson et al 2007 and water resource management engman 1991 sm is heterogeneously distributed at different space scales from few centimeters to several kilometers this variability is due to environmental factors impacting directly sm at specific scale ranges brocca et al 2007 crow et al 2012 jana 2010 vereecken et al 2014 for instance we could mention here soil properties texture and structure acting at the field scale topography features at the watershed scale land cover vegetation and meteorological forcing at the regional and continental scales many ground measurement techniques have been developed to acquire highly resolved sm data sets down to centimeters in space and minutes in time for more details see dobriyal et al 2012 robinson et al 2008 robock et al 2000 although these methods are recognized as reliable and easy to implement they are not adapted to represent spatial heterogeneity of sm at regional and continental scales collow et al 2012 crow et al 2012 regional and global scale variability of sm may be acquired and studied with the help of remote sensing different active and passive microwave satellites allow daily measurement of surface soil moisture in the first 5 cm of the soil column petropoulos et al 2015 wigneron et al 2003 these satellites acquire sm information thanks to the relationship between the soil dielectric constant and water content active microwave sensors measure the energy reflected by the soil after sending a microwave pulse to the surface backscatter we find c band synthetic aperture radars sar like sentinel 1 satellite s1 from european space agency wagner et al 2009 and c band scatterometers like the advanced scatterometer ascat bartalis et al 2007 these active sensors can provide space resolution from few meters s1 to tens of kilometers ascat their main drawback is their sensitivity to vegetation and surface roughness which can alter useful information sm in the signal measured passive sensors however are less sensitive to scattering conditions they measure the self emission from the land surface radiances good results were obtained by c and x band radiometers like the advanced microwave scanning radiometer earth observing system amsr e njoku et al 2003 owe et al 2001 or by l band radiometers such as soil moisture and ocean salinity smos kerr et al 2010 and the recent soil moisture active passive smap mission entekhabi et al 2010a l band microwaves 1 4 ghz have the benefit of little sensitivity to vegetation providing optimal estimation of sm on a wider range of land cover conditions l band based satellite missions deliver sm products with a revisit time of 2 to 3 days however because of technological constraints the spatial resolution is coarse 30 55 km much coarser than the kilometer scale this is a problem since hydro agricultural applications need better resolved information below kilometric space scales walker and houser 2004 to address this issue downscaling methods have been created to improve the low spatial resolution of satellite data downscaling algorithms are characterized by their input data satellite products auxiliary ground measurements etc and by the type of method physical or statistical peng et al 2017 reviewed the several methods developed so far and proposed a three group classification satellite based methods methods using geo information data and model based methods the first group i gathers downscaling techniques which combines satellite passive microwave products with satellite highly resolved auxiliary data such as radar or optical thermal observations this takes advantage of the assets of complementary remote sensing measurement techniques considering the fusion with high resolution radar a change detection method was proposed by njoku et al 2002 to merge coarse scale passive microwave soil moisture products and fine scale active backscatter data this technique consists in the linear relationship between soil moisture and backscatter data assuming the time invariance of vegetation and surface roughness effects the methodology was further tested in other experiments narayan et al 2006 piles et al 2009 and proved its efficiency for improving the spatial details of soil moisture statistical tools were also used to combine active and passive products such as bayesian merging method zhan et al 2006 or wavelet based image enhancement method montzka et al 2016 this kind of approach showed the great potential of radar for improving soil moisture resolution in particular for higher vegetation water content and different land cover types akbar and moghaddam 2015 a possible limitation of this approach is the time lag between active and passive data due to the low revisit rate of high resolution radar recently smap satellite was launched to bypass this problem embedding on board one radiometer and one radar das et al 2011 unfortunately the radar failed and no active passive combination could be performed however the previous studies made to prepare the mission showed good capacity to improve spatial resolution of satellite products by merging active and passive microwave data another type of satellite based method is the combination of passive microwave data with optical and thermal remote sensing data the interest is to have the additional information of high spatial resolution and short revisit time of the optical thermal products the concept is to use highly resolved vegetation cover and surface temperature products to downscale coarse scale soil moisture product based on the surface temperature vegetation index triangular feature space proposed by carlson et al 1994 2007 zhan et al 2002 and later chauhan et al 2003 developed and applied this method based on a polynomial function linking high resolution sm with surface temperature vegetation cover and surface albedo at coarse resolution scaling factors regression coefficients are estimated from this polynomial function and then used at high resolution in the same function to calculate the high resolution sm using ndvi normalized difference vegetation index and lst land surface temperature obtained from the lst ndvi feature space an improved version was proposed by piles et al 2011 using brightness temperatures instead of albedo showing better results when comparing downscaled sm with in situ measurements for instance this downscaling technique was used to improve the resolution of amsr e soil moisture merging it with optical thermal data measured from modis moderate resolution imaging spectroradiometer choi and hur 2012 or msg seviri meteosat second generation enhanced visible and infrared imager zhao and li 2013 the main problem in this methodology is the non conservativity of sm between fine scale and coarse scale sm based on the same theory other downscaling algorithms were proposed to relate the downscaled sm with coarse observations of sm an operationally implemented method is the downscaling algorithm dispatch disaggregation based on physical and theoretical scale change merlin et al 2008a molero et al 2016 this algorithm is more physical because it uses soil evaporation processes to connect optical thermal and sm data different applications of dispatch were realized to increase the 40 km resolution of smos sm to 1 km and even 100 m respectively with modis merlin et al 2012 and landsat 7 merlin et al 2013 products the originality of the method is the estimation of a sm proxy called soil evaporative efficiency see sections 4 2 and 6 2 the latter has the advantage compared to land surface temperature or evapotranspiration to be more linked to sm and to be quite constant during the day some improvements still need to be made about the modeling of see especially on elevation and illumination effects malbéteau et al 2017 or soil properties and atmospheric conditions merlin et al 2016 comparable evaporation based methods were developed using different proxies of sm such as the soil wetness index kim and hogue 2012 or the vegetation temperature condition index peng et al 2016 both applied in the simple downscaling method ucla we can also mention algorithms directly improving the resolution of brightness temperature products instead of retrieved sm based on the relation between daily temperature change and daily average sm song et al 2014 generally these downscaling methods present a significant asset considering the time coherence between the merged products but some limitations exist indeed the cloud sensitivity of optical thermal sensors makes the application of these methods possible only under clear sky conditions djamai et al 2016 since sm is directly linked to geoinformation data such as topography soil properties and vegetation attributes werbylo and niemann 2014 a second group ii of downscaling methods were also proposed these methods take advantage of highly resolved geoinformation data giving information on the local attributes of the zone studied and could give access to very high spatial resolution of sm topography for example was often used in downscaling approaches as an auxiliary data busch et al 2012 pellenq et al 2003 however certain types of geoinformation data like soil properties are usually provided by ground observations which are really specific to the studied area thus the application is limited to local areas and it may not be suitable for global scale study of sm the third class iii of methods concerns model based downscaling techniques there are two types of models used here on the one hand there are hydrological land surface models these ones are more site specific because they try to link coarse scale remotely sensed sm and fine scale parameters obtained from local land surface models the downscaling can be done through optimization techniques ines et al 2013 linear regressions loew and mauser 2008 or bivariate relationships verhoest et al 2015 on the other hand there are models that analyze and describe statistics across scales they are more generic and try to preserve statistical properties across scales for example kaheil et al 2008 proposed a wavelet based downscaling method in order to model spatial statistical properties of fine scale sm thanks to coarse scale airborne sm products other approaches are based on the scaling or fractal properties of sm across spatial scales bindlish and barros 2002 proposed a fractal interpolation method applied on airborne sm products measured from electronically scanned thinned array radiometer estar they used power spectra to represent the fractal behavior of sm and could improve spatial resolution from 200 m to 40 m a few years later mascaro et al 2010 applied log poisson multifractal cascades on remote sensing sm to generate simulations of fine scale sm the challenge here is to preserve non stationarity from coarse to fine scales nevertheless particular efforts are made to overpass this problem for example kim and barros 2002a adapted the fractal interpolation method applying a sliding window on specific parts of the original field they could simulate fractal variability while taking into account the local statistics of the field the downscaling methods of the three groups presented above have their own advantages and disadvantages with more or less efficiency according to specific surface or climate conditions in this study we focus on the evaluation of multi scale variability of sm products generated by the method dispatch merlin et al 2008a molero et al 2016 despite its limitations related to cloud cover this semi physical downscaling algorithm combines low sensitivity to vegetation of l band microwaves high spatial resolution of optical thermal data and it is dispensed from estimation errors commonly generated by land surface models several studies have been realized so far to evaluate and validate this method malbéteau et al 2016 merlin et al 2013 2015 molero et al 2016 in general the assessment of downscaling algorithms is made comparing fine scale output products with ground measurements different performance metrics are used such as correlation root mean square error or bias albergel et al 2013 al bitar et al 2012 entekhabi et al 2010b more recently merlin et al 2015 proposed a new metric that estimates the gain given by the downscaling method in terms of representativeness of downscaled data compared to non downscaled data to take into account scale mismatch between downscaled and ground measurements upscaling techniques have been developed in order to bring downscaled and ground data together at common space scales crow et al 2012 for example merlin et al 2013 applied the dispatch algorithm on smos data while using both modis moderate resolution imaging spectroradiometer and landsat 7 auxiliary data coarse scale satellite data downscaled data and aggregated ground measurements were compared at three different scales 40 km 3 km and 100 m good results confirmed the potential of dispatch to improve the spatio temporal correlation of remotely sensed sm with in situ measurements however the drawback of these validation techniques is that they are restricted to specific scales thus the validation of disaggregated sm products over a continuum of space scales has not been fully explored yet investigation of the multi scale statistics and of possible scaling properties of these products could provide relevant information on this aspect during the last thirty years several studies were carried out to describe the statistical properties of sm across spatial scales famiglietti et al 2008 rodriguez iturbe et al 1995 different analytical methods were proposed the most commonly used are spectral wavelet analysis si 2008 and multifractal analysis kim and barros 2002b mascaro et al 2010 oldak et al 2002 in 1995 rodriguez iturbe et al highlighted for the first time the fractal behavior of sm from remote sensing the spatial variance of sm followed a power law decay as a function of aggregation scales ranging from 30 m to 1 km washita experiment 1992 usa later studies showed that such a scaling behavior of sm variance could be extended to wider range of scales up to regional scale hu et al 1997 and even to continental scales rötzer et al 2015 similar research works demonstrated that increasing area extent increasing size of the total area induced the increase of sm variance according to a power law function famiglietti et al 2008 rötzer et al 2015 brocca et al 2012 moreover in oldak et al 2002 the fractal scaling of sm was revealed to be multifractal the power law was also applicable to the first six statistical moments of airborne sm products for scales ranging from hundreds of meters to tens of kilometers washita 92 experiment and southern great plains experiment 1997 usa multifractal scaling was then detected in sm fields das and mohanty 2006 kim and barros 2002b lovejoy et al 2008 mascaro et al 2010 since sm variability is directly related to the amount of soil wetness brocca et al 2007 famiglietti et al 2008 it may be expected that scaling properties of sm may vary according to the state of sm indeed when plotting sm variance power law in log log coordinates rodriguez iturbe et al 1995 and manfreda et al 2007 found that the corresponding slope of the curve was increased during drier periods revealing seasonal variations of sm scaling rötzer et al 2015 moreover it was observed that sm variability was not governed by a single scaling behavior but by different scaling regimes depending on the range of scales at the field scale sm variability is mainly related to land surface characteristics such as soil properties or topography whereas at larger scales it is impacted by meteorological quantities like rainfall or evapotranspiration cayan and georgakakos 1995 entin et al 2000 studies based on semi variograms ryu and famiglietti 2006 korres et al 2015 and spectral moments analysis kim and barros 2002b revealed the presence of scale breaks closed to this transition scale between land surface and meteorological regimes though the aforementioned characteristics of sm highlight its complexity and its high degree of nonlinearity due to hydrometeorological processes acting at different space scales attesting the necessity to better understand the scaling behavior of sm for applications such as data assimilation or downscaling rötzer et al 2015 in this paper we propose an alternative and complementary method for verifying the multiscaling behavior of dispatch products to do this we studied and compared the statistical spatial properties across scales of the downscaled sm the original smos sm and the modis auxiliary data by applying spectral and multifractal analysis in the framework of the universal multifractal um model schertzer and lovejoy 1987 the definition of multifractal formalism is given in section 2 with a particular attention paid to um parametrization the methodology followed for multifractal and spectral analysis is detailed in section 3 section 4 describes the case study and the data set then the different results obtained from spectral and multifractal analysis are presented in section 5 finally section 6 proposes explanations to the multiscaling behaviors of dispatch sm and a general conclusion of this study is given in section 7 2 theory of multifractals during the last century several studies showed that many geophysical processes could present scale invariance properties this was first anticipated by richardson 1922 in the case of turbulence he described turbulent flows as cascade processes that transfer kinetic energy from large to small scales based on this approach statistical models of turbulence were proposed such as the famous kolmogorov law 1941 to describe velocity increments later research works generalized the study to take into account the heterogeneity of the energy flux kolmogorov 1962 obukhov 1962 yaglom 1966 multi scale models such as multiplicative cascades were therefore proposed to reproduce scale invariance properties through the use of fractal geometry later scale invariance was noticed in other geophysical fields in his study of the coast of britain mandelbrot 1967 revealed the presence of fractal properties in topography 2 1 from fractal sets to multifractal fields the concept of fractal dimension has been used in many works related to multi scale analysis and geophysical modeling indeed the term fractal refers to any entity time series or 2d 3d random field in which each part presents similar properties geometrically or statistically to the ensemble in this manner the structure of a fractal entity is characterized by scale invariance initially the notion of fractal was introduced in the late 19th century in geometry with the creation of sets i e mathematical objects having unusual properties especially a non integer hausdorff dimension called later by mandelbrot as fractal dimension mandelbrot 1967 scale invariance in the statistical sense was theoretically proposed by kolmogorov in 1940 with the introduction of the fractional brownian motion this model could generate random time series whose trajectories present fractal properties in terms of statistical distribution it illustrates the physical interest of fractal random processes since brownian motions are somewhat ubiquitous in physics mandelbrot and van ness 1968 made it famous by introducing it to more physical models in particular the first fractal stochastic models of topography were developed based on this theory mandelbrot 1975 these stochastic models aim to represent the simple scaling monofractal behavior of geophysical processes in this context the fractal dimension or scaling parameter is assumed to be unique restricting multi scale modeling to a specific class of variability however most geophysical processes are characterized by more complex statistics in case of operational hydrology rare and extreme events present in precipitation or soil moisture for example correspond to high order statistics and need to be detected hubert et al 1993 therefore multifractal models characterized by an infinite spectrum of fractal dimensions have been proposed to account for a more exhaustive set of statistics schertzer and lovejoy 1987 based on the findings of parisi and frisch 1985 initially established the multifractal formalism through the fundamental equation 1 p r ϕ λ λ γ λ c γ where φλ is a positive normalized random scalar process time series or random field defined on r 2 or r 3 the mean of the process is assumed to be statistically conserved across scales λ is the observation resolution here defined as the inverse of the scale that can be seen as the sampling time or pixel size for time and space domain processes respectively and indicates an equality within the limits of slowly varying functions eq 1 expresses the fact that for a multifractal process the probability of exceeding a threshold varies as a power law of the resolution with exponent c γ this exponent is called as fractal codimension of the process depending on the amplitude of thresholds the thresholds are defined by the following power law 2 t λ λ γ with γ the notion of singularity characterizing the amplitude of the process independently of the scale each singularity is associated to a fractal codimension c γ corresponding to a family of thresholds of various amplitudes from a more physical point of view high singularities detected by high thresholds are related to rare and extreme events with high fractal codimensions and inversely low box counting fractal dimensions df mandelbrot 1967 indeed the latter are related to the dimension of space d through the relation c γ d df therefore c γ can be described as a codimension function increasing with γ which completely characterizes the multi scale statistical properties of the field φλ in general if the field is multifractal c γ is found to be convex and positive with a fixed point c 1 imposed by the condition of canonical conservation whereas monofractality is associated to the trivial case c γ const since probability distributions and statistical moments are related by a mellin transform schertzer and lovejoy 1987 proposed an equivalent equation to 1 3 ϕ λ q λ k q where is the statistical averaging operator q is the order of the moment q 0 and k q is the moment scaling function eq 3 expresses that for any fixed moment order statistical moments and resolution are linked through a power law singularities and moment orders are directly linked since the moment scaling function k q is the legendre transform of the codimension function c γ similarly to c γ k q is a convex function with the special case k 1 0 related to the conservation of the mean across scales which entirely characterizes the multifractal field 2 2 multiplicative cascades multiplicative cascades are stochastic models that can be used to build multifractal fields cascades are multiplicative processes because they are defined by an iterative multiplicative construction considering a two dimensional random signal field each pixel at resolution λ n 1 with n the construction level of the cascade is the product of the embedding pixel at coarser resolution λ n multiplied by a random variable με this is described by the following equation 4 ϕ λ n 1 μ ε ϕ λ n in this manner the statistical properties of the field ϕ λ n 1 are directly related to the statistical properties of the coarser field ϕ λ n if all the multiplicative random variables used for each step of the iterative construction are independent and identically distributed and distributed independently of the scale the final field presents scale invariant properties several models of cascades have been developed so far first models were built within the framework of turbulence such as the α model schertzer and lovejoy 1984 which corresponds to discrete construction of cascades the multiplicative random variables are limited to two possible fixed values respectively leading to increasing or decreasing pixel value when the resolution is refined later more elaborated models were constructed generalizing the discrete case to continuous cascades dubrulle 1994 schertzer and lovejoy 1987 1991 1997 she and levêque 1994 the latter are based on an infinite number of steps between any pair of resolutions leading to continuity in scale the benefit of continuous cascades is twofold first they can represent possibly more realistic structures by avoiding any arbitrary discretization of scales moreover they often converge toward random processes which are characterized by a small number of degrees of freedom special cases of log infinitely divisible distributions this is interesting considering that multifractal fields built by multiplicative cascade processes would otherwise need an infinite number of scaling parameters one for each fractal dimension for example she and levêque 1994 proposed a continuous cascade model based on log poisson statistics and schertzer and lovejoy 1987 used log stable random variables to build the universal multifractal model in both models only two fundamental parameters are needed to fully define multifractality 2 3 universal multifractals physically multifractal fields built by log poisson or log stable cascades have a high degree of generality in geophysics log poisson model has been successfully applied to different geophysical variables such as rain deidda 2000 or even soil moisture mascaro et al 2010 however this model can have disadvantages of representing a restricted range of variabilities which may make it unsuitable for modeling processes with unbounded singularities on the other hand by assuming the stability of the random variables and suitable renormalization um model is likely adapted for characterizing a wide range of processes topography lavallée et al 1993 rain and clouds tessier et al 1993 and more recently soil moisture and vegetation optical indexes lovejoy et al 2008 moreover a possibly more immediate physical interpretation of the parameters is found in this model for mathematical and physical arguments supporting the universality of um model see schertzer and lovejoy 1997 see also gupta and waymire 1997 for discussion about its generality um model defines the moment scaling function using two universal parameters through the following equation schertzer and lovejoy 1987 5 k q c 1 α 1 q α q where α is the degree of multifractality of the field it varies between 0 monofractality and 2 log normality and expresses how fast the codimension evolves as a function of the singularity the second parameter c 1 is the codimension giving the dominant contribution to the mean value of the field related to moment of order 1 c 1 k 1 physically it indicates inhomogeneity dispersion of the field it varies from 0 homogeneous field to the dimension d of the embedding space very intermittent field because of legendre transform c c 1 k 1 is also defined as the fixed point of the codimension function 2 4 fif model generally most of the geophysical fields are non conservative i e integrated processes defined by a certain degree of fractional integration this appellation comes from multifractal cascade models see gagnon et al 2006 for detailed explanations on this formalism thus to account for a wider range of processes an extension of the um model to non conservative fields has been proposed schertzer and lovejoy 1991 the fractionally integrated flux fif model it expresses the degree of fractional integration of the um field using a third parameter h the latter is called the order of integration and defines the non conservativity of the field in plain words the larger is h the smoother is the field the integrated flux is noted r λ and is characterized by a power law variation of its stationary increments 6 δ r λ ϕ λ δ x h where δr λ are the increments fluctuations of the flux estimated over a varying window δx which is equivalent to the space scale l note that when h 0 the equation corresponds to the conservative case φλ additionally in the case of two dimensional fluxes eq 6 also applies for other directions i e δy increments with the same exponent h if the process is isotropic hereafter in this article the appellation proposed in lovejoy and schertzer 2010 will be followed non integrated cascades will be called conservative fluxes due to the conservation of the mean and fractionally integrated non conservative processes will be called random fields or simply fields 3 multifractal analysis methodology the different techniques used to analyze the multi scale properties of dispatch related products are detailed in this section the methodology is based on the multifractal theory presented in the precedent section because our study treats only satellite images we will focus on the two dimensional versions of these techniques 3 1 power spectrum preliminary evidence of scaling spectral analysis is a methodology often used in geophysics to characterize in an easy and rapid way some scaling properties of fields over different space scales lovejoy et al 2008 thanks to its high sensitivity to scale breaks scaling regimes can be easily identified in a first step the two dimensional power spectral density p kx ky of the data under analysis x is estimated 7 p k x k y f f t x 2 with p the power spectral density defined on both vertical and horizontal image axis corresponding respectively to kx and ky wavenumbers spatial frequencies here the estimation of the psd is done through a two dimensional fft or fast fourier transform then the one dimensional isotropic angle integrated power spectrum e k is obtained lovejoy et al 2008 8 8 e k k k p k d k where k is the modulus of the wavenumber and is the euclidean norm since it expresses space frequencies k is directly related to the space resolution λ if the process presents scaling properties the spectrum should follow a power law where β is the negative slope of e k on a log log graph 9 e k k β β is called the spectral exponent and is directly related to the fif parameters through the equation 10 β 1 2 h k 2 in this manner β also gives first indications about the possible conservative nature of the field since integrated flux h 0 should correspond to spectral exponent greater than 1 note that power spectrum is a second order statistic hence the term k 2 3 2 statistical moments multifractal properties to test the presence of multifractal properties in the data eq 3 statistical moments and moment scaling function need to be estimated to do this different steps must be followed first the underlying conservative field ϕ λ m a x has to be reconstructed from the data at the maximum observation resolution λ max because the possible existence of a fractional integration of order h eq 6 a fractional derivative of the same order should be done in this study the modulus of the gradient was applied to the data indeed this operator provides a simple and good numerical approximation of the fractional derivation without prior knowledge of h order lavallée et al 1993 11 ϕ λ m a x r λ m a x x 2 r λ m a x y 2 once the conservative field is retrieved ϕ λ m a x is normalized by its mean the second step involves the degradation of the field at lower resolutions λ λ max it aims to approximate the inversion of the stochastic multiplicative cascade by iteratively averaging the field at coarser scales each coarse pixel level n of the cascade is obtained by a simple average of neighboring finer pixels level n 1 note that each roughened pixel size is a power of two multiplied greater than the observation scale lmin λ max 1 finally empirical moments i e computing q th order moments in eq 3 while replacing statistical averages by empirical averages are then computed for various orders and resolutions on a log log graph the different moments are plotted as a function of the resolution if linearity is observed for each moment curve at least over a significant range of resolutions eq 3 is therefore verified which is the signature of multifractality the empirical moment scaling function can be estimated with k q corresponding to each linear fit of qth order moment afterwards the universal parameters α and c 1 may be obtained by optimization according to the um model form of k q eq 5 3 3 structure functions some evidence of non conservativity a convenient way to reveal the non conservative fractionally integrated nature of the integrated flux r λ eq 6 is to compute its first order structure function 12 δ r λ δ x r λ x δ x r λ x if the flux is indeed non conservative the order of integration h should be the slope of the increments δr λ plotted in a log log graph as a function of space scale δx this technique will be used in this study to estimate the h parameter 4 case study and data 4 1 c4dis processor and satellites products the different products analyzed in this study are input and output data of c4dis catds level 4 disaggregation processor molero et al 2016 this processor includes the first operational version of the dispatch algorithm taking into account the best configurations according to the latest studies merlin et al 2010a 2010b 2013 because the algorithm is still evolving c4dis products are called as scientific molero et al 2016 users can have access on demand to the products over specific areas of the world as presented earlier the downscaling method combines smos microwave data and modis optical thermal data the sm data is given by the smos level 3 daily global sm product reference mir clf31a d this product is provided by the center aval de traitement des données smos catds which is the french ground segment for smos level 3 and level 4 products the sm data is acquired every day at a radiometric resolution that varies between 35 and 55 km 40 km in average from l band brightness temperature measurements kerr et al 2012 wigneron et al 2007 smos level 3 products are delivered on the ease equal area scalable earth grid with a grid spacing of 25 km 25 km the optical thermal products come from the modis sensor embedded on both aqua and terra satellites two types of auxiliary data are used in dispatch first there is land surface temperature lst it is extracted from the modis level 3 daily products myd11a1 aqua and mod11a1 terra these temperature products are estimated from thermal infrared radiances emitted from the surface 3 15 μm then the second auxiliary data is normalized difference vegetation index ndvi given by the level 3 16 day terra product mod13a2 the vegetation index is computed from surface reflectances in red 0 7 μm and near infrared 0 8 μm wavelengths both lst and ndvi products are provided at 1 km resolution by the nasa land processes distributed active archive center lp daac they are presented on a sinusoidal grid with a grid spacing slightly smaller than kilometer 0 93 km 0 93 km solano et al 2010 wan 2006 we may notice that lst products have daily time resolution whereas ndvi products are representative of a period of 16 days output dispatch products are generated every day by the c4dis processor their resolution is that of modis products 1 km and they are presented on an equal spaced lat lon wgs84 grid with a grid spacing of 0 01 1 12 km for simplicity in the following we ll make the approximation 0 01 1 km one single downscaled image is the result of the combination of four downsampled smos sm images one modis ndvi image and up to six modis lst images corresponding to 3 consecutive days of aqua and terra acquisitions for more details on the combination methodology see malbéteau et al 2016 merlin et al 2012 molero et al 2016 in other words in the final product each high resolution output pixel comes from the average of 24 possible disaggregated pixels up to 24 sm lst possible pairs the advantage of this composition is that uncertainty in downscaled sm can be potentially reduced and estimated and time coverage is improved malbéteau et al 2016 4 2 dispatch algorithm dispatch is based on a semi empirical model that estimates the soil evaporative efficiency see from high resolution hr 1 km lst and ndvi products the method is based on the separation of modis lst into its soil and vegetation components respectively referred in this study as t s hr and t v hr to do this the approach relies on a variant of the trapezoid method from moran et al 1994 which interprets the feature space defined by modis lst and ndvi derived fractional vegetation cover f v hr the purpose here is to extract the soil temperature t s hr according to the following equations 13 t s h r l s t f v h r t v h r 1 f v h r with 14 f v h r n d v i n d v i s o i l n d v i v e g e t n d v i s o i l in eq 13 the vegetation temperature t v hr is calculated according to moran et al 1994 ndvisoil and ndviveget in 14 are respectively the ndvi obtained from bare soil set to 0 15 and from full cover vegetation set to 0 90 then modis derived soil temperature t s hr allows to estimate seehr at 1 km resolution following the methodology proposed by merlin et al 2012 15 s e e h r t s m a x t s h r t s m a x t s m i n where t s max and t s min are endmembers estimated from the approximations of merlin et al 2013 considering the relations between the minimum maximum of lst and the associated f v hr more details can be found on these estimates in molero et al 2016 p 4 see is used to describe the spatial variability of sm within the low resolution lr 40 km pixel given by smos product high resolution sm smhr is linked to high resolution see seehr through the linear model proposed by budyko 1961 and manabe 1969 16 s e e h r s m h r s m p where smp is a lr parameter depending on atmospheric conditions and soil properties in the c4dis processor this parameter is computed at low resolution at each execution from daily smos sm smlr and see averaged inside the lr pixel seelr 17 s m p s m l r s e e l r the disaggregation is finally realized by applying a first order taylor expansion to the see and sm dataset the downscaling relationship is written as 18 s m h r s m l r s m s e e l r s e e h r s e e l r with sm seelr the partial derivative of sm relative to see computed at low resolution here this derivative simply equals the smp parameter estimated according to 17 4 3 study area australia is a wide country with an area of almost 8 million km² and characterized by various surface and climate conditions thus it is a suitable area to study spatial variations of soil moisture over a wide range of scales many studies on sm have been carried out in australia in order to monitor sm variability using ground airborne and satellites data smith et al 2012 among others we can mention the national airborne field experiment 2006 nafe 06 merlin et al 2008b and the australian airborne calibration validation experiments for smos aaces peischl et al 2012 these experiments were realized over the murrumbidgee catchment 82 000 km² fig 1 located at the southeastern part of australia because of its variable climatic conditions humid in the east semi arid in the west this region was used for validating satellites missions such as smap panciera et al 2014 or smos sm products delivered by smos were assessed during the aaces experiments which took place in 2010 over two periods january february aaces 1 and september aaces 2 wide spatially distributed networks of in situ measurements oznet hydrological monitoring network smith et al 2012 and transect flights polarimetric l band multibeam radiometer peischl et al 2012 were used to validate smos data in this context benefiting from a dense sm dataset at different space scales some of the first applications of dispatch algorithm were realized during the aaces experiments merlin et al 2012 these works showed the efficiency of dispatch under low vegetated semi arid areas and its potential to evaluate coarse scale smos products later studies malbéteau et al 2016 molero et al 2016 continued the evaluation and improvement of dispatch algorithm over the murrumbidgee catchment in this paper dispatch analysis is made during the 7 month period from june to december 2010 taking advantage of previous dispatch studies over this period we choose to extend the study area from the murrumbidgee catchment to the murray darling basin mdb 1 million km² fig 1 the first reason of this choice is related to the main objective of the study which is the analysis of dispatch related products over different space scales though our study covers a large range of scales from the pixel size kilometer scale to the full basin extent 1300 1400 km² giving a new point of view considering dispatch validation moreover spectral and multifractal tools presented in section 3 cannot be properly applied if the data size is not sufficient enough because of its low resolution it would be inappropriate to do multiscale analysis of smos sm over the murrumbidgee catchment images would be smaller than 5 5 pixels mdb is located in southeastern australia and contains more than 20 catchments such as murrumbidgee in its south part fig 1 the climate is sub tropical in the north east average annual precipitation up to 1500 mm semi arid in the west average annual precipitation less than 300 mm and mostly temperate in the south snowfall during winter on the peaks of the great dividing range regarding to land use west is made of wide plains essentially composed of saltbush shrublands and mulga lands from south to north east there are the mountains of the great dividing range reaching 2 300 m in altitude irrigation dry land cropping and pastures are spread over the basin but most of the irrigated areas are located in the south like murrumbidgee region 4 4 data preprocessing before applying the multi scale analysis preprocessing must be done on the different satellite products the first preprocessing step is to handle the missing values because of technology or acquisition conditions all satellite sensors provide products that present more or less missing values these can be caused by failures in the data acquisition or delivering or even voluntarily generated by the production center when discarding incorrect values in our case smos products can be affected by unauthorised emissions that cause radio frequency interference rfi smos sm used in this study are pre filtered by catds in order to remove pixels with more than 10 rfi probability kerr et al 2013 olivia et al 2012 considering modis products cloud pixels are also removed to avoid the impact of atmosphere on downscaled data though missing values in output dispatch products are mainly caused by the accumulation of missing values coming from inputs thanks to the 24 averaged hr outputs combination implemented in c4dis processor section 4 1 the probability to get missing values in the final averaged downscaled product is reduced in our study we applied bilinear interpolation in each satellite image to fill in missing data noted nan to do this properly some conditions were established to minimize the impact of data interpolation on spectral and multifractal analysis each image with more than 40 of nan were discarded moreover in order to treat separately land surface nan values from sea areas located outside the continent the latter were filled with zeros previous studies showed that biased multifractal parameters could be obtained from data containing significant proportion of zeros de montera et al 2009 verrier et al 2010 2011 thus we made sure to select images whose ground area contains a minimum of sea pixels less than 10 in a second stage sub images of 2 n 2 n pixels need to be selected over the mdb area to estimate statistical moments over different spatial resolutions images must indeed be square with a number of pixels equal to a power of two along each dimension section 3 because of different satellites projection grids and spatial resolutions selected sub images from different satellites do not cover exactly the same area and they do not completely match to the original mdb area fig 2 presents examples of sub images obtained for dispatch sm smos sm and modis ndvi whose size is respectively 1024 1024 64 64 and 1024 1024 pixels for readability during preprocessing all modis products were projected from sinusoidal to orthogonal lat lon coordinates sohrabinia 2012 considering the different grid spacing of the products and sub images size condition the sub image selected for smos sm covers the entire mdb 1600 1600 km² whereas the sub images selected for dispatch and modis products are smaller around 1000 1000 km² it is important to notice that while they have similar spatial resolution and a same number of pixels dispatch sm and modis images do not exactly correspond to the same ground area this is caused by slightly different grid spacing for the two products 1 km for dispatch and 0 93 km for modis solano et al 2010 wan 2006 for simplicity we ll consider in the following that both dispatch and modis products present a grid spacing of around 1 km table 1 summarizes the main characteristics of our preprocessed satellite dataset two important observations should be highlighted first considering their daily revisit time few dispatch sm and modis lst images are retained over the full june december period only 12 maps for dispatch and around 70 maps for modis lst this is directly related to the significant number of missing values that is in average 30 in these two types of products therefore missing values in downscaled sm seem to be mostly generated by those in lst products probably due to the presence of clouds in the data then another point concerns the different surface areas of the preprocessed products because they do not fully overlap smos and dispatch sub images may capture different sm dynamics extreme events occurring in northern mdb are observed in smos data whereas it may not be taken into account in dispatch data however we ensured that all products did have the widest area in common focusing on irrigated regions in the middle south part of the basin like murrumbidgee 5 results 5 1 spatial power spectra fig 3 shows the mean power spectra estimated over the full period june december 2010 of the different input and output products involved in dispatch it represents an average spectrum based on individual spectra obtained within the period each spectrum is plotted in log log coordinates with horizontal axis converted into space scale l k 1 expressed in kilometers considering smos sm and modis products the mean spectra are found to be scaling over the entire range of scales this is observed by a linear evolution of log e k eq 9 with coefficients of determination r² greater than 0 9 for each spectrum table 2 note that r² is used as a measure of the goodness of scaling estimated from the linear regression between log e k and log l however a different behavior is noticed for the disaggregated sm spectrum two scale ranges seem to appear with an increasing slope on scales lower than about ten kilometers a segmentation algorithm was applied on this spectrum d errico 2017 which confirmed a scale break at l 10 km according to the different values of spectral slopes obtained table 2 a three group classification was proposed β 1 smos sm modis vegetation index and disaggregated sm l 10 km for these three products the negative slope is found to be close to one though according to eq 10 this may reveal the conservative nature of the fields h 0 moreover these values are quite similar to the estimates proposed in literature lovejoy et al 2008 found β 1 2 for both vegetation and soil moisture indexes from modis products guadalajara central spain july 2006 previous studies on topography especially on volcanic surfaces laferrière and gaonac h 1999 found comparable results with quite low degree of fractional integration since topography can affect the spatial distribution of sm and vegetation kim and barros 2002b it is not surprising to observe similar scaling behavior between these fields 1 β 2 modis surface temperature from both aqua and terra satellites lst spectra have β values greater than 1 here surface temperature seems to correspond to a non conservative field h 0 these spectral slopes may be comparable to those obtained in literature on precipitation fields lovejoy and schertzer 2008 showing possible connections between the spatial distribution of surface temperature and that of rainfall and therefore with the underlying turbulent atmospheric dynamic schmitt et al 1993 β 2 disaggregated sm l 10 km on small scales dispatch sm spectrum presents a relatively large slope reflecting a high degree of fractional integration h 0 5 to our knowledge such high value of spectral exponent has never been observed in previous studies on sm fields however comparable scaling was obtained on sm time series revealing spectral slopes greater than 2 katul et al 2007 from these spectral observations a similar scaling seems to appear between the original smos sm and the disaggregated sm on scales greater than 10 km but this behavior is found to change for scales lower than about ten kilometers a comment may also be made on lst power spectra and their linear regressions although r² coefficients present good values on the entire range of scales 0 9 a scale break may be observed at about the same spatial scale found for dispatch spectrum l 10 km the scale break seems less pronounced but it could be related to that of dispatch this point will be discussed in section 6 2 this twofold scaling regime of dispatch sm can be also observed on each specific date of the study period with r² coefficients greater than 0 9 on almost all images and on both scale ranges fig 4 a shows the time series of the individual spectral exponents estimated for all products i e spectra computed for each image from june to december a significant difference of β values is observed between the two scale ranges of disaggregated sm for example on july 9 fig 4b power spectra are found to be similar as mean ones presented above fig 3 in particular the same scale break is still observed for disaggregated sm at about ten kilometers another remark concerns the amplitude of the scale break according to seasons fig 4a shows that for disaggregated sm the difference between the spectral exponents of small scales and large scales respectively blue triangle and blue star symbols is more important during the last three months of the period at small scales the spectral slope suffers a drastic change from around 1 9 jun jul aug sept to 2 3 oct nov dec i e spring and early summer in australia the amplitude of the scale break observed in dispatch sm could be related to the seasonal conditions of the study area this will be discussed in section 6 1 5 2 multifractal analysis the moments of the normalized absolute gradients were estimated at all accessible resolutions since divergence for q greater than qd 3 was reported in most of the literature hubert et al 2007 and because of sample size limitations in this study moments were computed for orders set from 0 to 3 in steps of 0 1 fig 5 shows the mean moments over the 7 month period plotted in log log coordinates as a function of the space scale l λ 1 for each product multifractal regimes are identified on specific scale ranges the power law described by eq 3 is well verified over these spatial scales corresponding to a linear variation of l o g m l q for all orders of moments ϕ λ q m l q this behavior means that a multifractal model is well adapted on the corresponding scale ranges considering vegetation and temperature modis products a scaling regime is found on scales greater than 8 km fig 5a c on these scales moments curves were fitted by linear regression red fit lines on fig 5 and the corresponding scaling functions k q were computed red yellow and green curves fig 5e um parameters were then estimated applying derivative free minimization method between empirical scaling function k q and the model form of k q described in eq 5 for the vegetation parameters values are found to be α 1 74 and c 1 0 03 table 2 they are quite close to those estimated by lovejoy et al 2008 on similar ndvi modis products α 2 and c 1 0 06 for aqua surface temperature we found the same parameter values as the vegetation ones α 1 7 and c 1 0 03 which is related to the very similar k q functions for all orders q slightly different parameters are found for terra products α 1 91 and c 1 0 04 this difference could be due to the different acquisition time of the two satellites 10 30 for terra and 13 30 for aqua this may have some effect on the multiscaling behavior of surface temperature another reason to this difference could be the larger scaling regime considered for terra a multifractal behavior is observed on scales ranging from 8 km to 1024 km against 8 km to 300 km for aqua and ndvi products anyway these results confirm ndvi and reveal lst not yet studied at this time the multifractal properties of the considered modis products in both cases they are characterized by a high degree of multifractality α is close to 2 value corresponding to the log normal case and by a low dispersion of the field c 1 0 1 smos sm products show good multifractal behavior too moments are found to be well fitted r² 0 99 cf table 2 on most of the aggregation scales apart from the 2 greatest scales 1600 km and 800 km scaling function was computed over spatial scales going from the 25 km observation scale to 400 km purple curve in k q graph fig 5 compared to modis products a growing divergence is noticed between smos and ndvi lst scaling functions especially for orders q greater than 1 this scaling behavior is confirmed by different um parameters α 1 46 and c 1 0 16 to our knowledge no application of the um model has already been made on remotely sensed sm from passive microwaves therefore it is difficult to compare these results with literature however although they didn t use the um model kim and barros 2002b studied spatial scaling properties of passive microwave sm estimated from airborne l band radiometer southern great plains experiment 1997 usa they observed a multifractal scaling on a similar scale range 1 6 km to 250 km which is coherent with our results lovejoy et al 2008 indeed applied the um model but on an optical sm index estimated from modis reflectances lampkin and yool 2004 they found α 2 and c 1 0 05 over lower spatial scales 0 5 km to 25 km these parameter values are quite different from ours the different scale range and the different study area guadalajara central spain in lovejoy et al 2008 between their work and ours could be a possible explanation to this result another reason might be linked to the nature of the signal studied optical estimated indexes like modis sm index are more sensitive to land cover such as vegetation fabre et al 2015 haubrock et al 2008 then polluting the scaling properties of sm focusing now on disaggregated sm products a change of slope is noticed for each of the statistical moments the same segmentation algorithm was applied on all moment curves revealing a scale break at about ten kilometers two multifractal scaling regimes may be observed here confirming the twofold scaling behavior found in the power spectra considering larger scales l 10 km red fit lines on fig 5 estimated um parameters are α 1 64 and c 1 0 03 they are close to the parameters found for our modis products ndvi and lst with a high degree of multifractality and a low dispersion of the field for smaller scales l 10 km green fit lines on fig 5 the degree of multifractality is almost unchanged α 1 59 compared to the large scales regime however the dispersion parameter is increased c 1 0 09 which is three times the value obtained on greater scales though the difference between the two multifractal scaling regimes seems to be mainly linked to the dispersion of sm through scales if we refer to the multifractal analysis of modis sm index made by lovejoy et al 2008 our estimates are coherent considering α for both ranges of scales and c 1 on large scales lovejoy et al 2008 didn t notice any scale break therefore it is difficult to comment our estimate of c 1 at small scales nevertheless kim and barros 2002b observed a similar scale break at about the same 10 km scale on passive microwave sm indeed they noticed two scaling regimes from variance spectra and moments graphs the twofold scaling behavior of dispatch sm products looks consistent with the scale break identified first by kim and barros 2002b 6 discussion 6 1 a physically explained twofold scaling behavior of soil moisture since sm variability is impacted by several environmental factors brocca et al 2007 crown et al 2012 the scale break observed on disaggregated sm could be the result of processes acting at different space scales at finer scales l 10 km spatial structure of sm is governed by infiltration or runoff which are mainly related to the soil properties texture structure hawley et al 1983 famiglietti et al 1998 on the other hand at larger scales l 10 km sm variability is more affected by evapotranspiration processes mohanty and skaggs 2001 or precipitation jackson et al 1999 a similar scale break at 10 km was also noted by kim and barros 2002b based on power spectra and statistical moments of sm estimated from airborne l band radiometer southern great plains experiment 1997 usa sm retrievals were obtained at 1 km nominal resolution from the ƭ ω model jackson and schmugge 1991 which depends on vegetation water content vwc estimates based on ndvi they observed that the relationship between the spatial structure of sm and landscape characteristics was strongly modulated by the wetness of the soil indeed they applied an eof analysis empirical orthogonal function between sm and auxiliary data which are topography vwc and soil content this revealed that sm was much correlated to topography during rain events whereas stronger correlation with vegetation water content was noticed during drier periods mainly governed by evapotranspiration processes these results are interesting since other research studies also observed similar scale break in the case of precipitation products obtained from radar at 1 km resolution southeastern france gires et al 2011 indeed a transition in spectra and moments was noticed at about twenty kilometers not far from our 10 km scale break however some limitations relative to radar data acquisition must be taken into account considering these results indeed constraints due to algorithmic processing change from polar to cartesian coordinates impact of missing data temporal integration and to physics attenuation by rainfall etc may impact the scaling properties of precipitation radar images moreover the z r relationship between radar reflectivity and rain rate marshall and palmer 1948 remains somehow controversial with a non robust parameterization from a multi scale point of view verrier et al 2013 thus in this context the scale break detected by gires et al 2011 may not be as relevant as it could be however they also analyzed the multifractal behavior of simulated precipitations generated on the same area at 2 km resolution from the meso nh atmospheric model lafore et al 1997 the analysis revealed the presence of a comparable scale break at about 30 km which tends to show that this transition scale in precipitation data is not an artifact since rainfall is an important forcing of sm it may be thought that a break in the rainfall spectra would affect the sm in a more significant way when the rain event is important moreover a theoretical model of sm in the time domain was proposed by katul et al 2007 to relate the scaling of precipitation to that of sm the spectral exponents of these two variables were found to be connected over time scales finer than 7 days through the simple equation β sm β p 2 with β sm and β p the negative spectral slopes of respectively soil moisture and precipitation time series despite these results were observed on time series it may corroborate the possible dependence between the sm variability and that of heavy rainfall even in the space domain considering seasonal variations the power spectra of disaggregated sm seem to reveal a pronounced twofold scaling behavior especially during spring and early summer october to december period since dispatch images are mainly located over the middle south part of the murray darling basin climate is then mostly temperate therefore the last months of the study period correspond to a drier landscape thus the two scaling regimes seem to be even more distinct when the soil is drier to demonstrate this effect the spatial mean of dispatch sm μ sm and the absolute difference β large β small were computed for each disaggregated image in fig 6 the normalized anomalies of these two variables are in line with this hypothesis blue and red circle symbols a more pronounced twofold scaling behavior seems to be found on the driest days oct nov dec kim and barros 2002b noticed a similar behavior with lower scaling differences during rain events observed on both spectra and moments of sm in certain dates they even noticed that there was no scale break at all corresponding to very high wetness conditions of the soil moreover we estimated the position of the scale breaks on each power spectrum during the period corresponding normalized anomalies plotted in gray star symbols fig 6 although it is positioned on average around 10 km not shown here but the mean value over the full period was estimated at 13 km the transition scale between the two scaling regimes seems to follow a decreasing trend as the soil is drying with estimated scale breaks ranging from 15 km in wet period to 12 km in dry period a comparable behavior was observed by kim and barros 2002b showing that the position and the amplitude of the scale break in the scaling behavior of sm is dependent on the state of sm and thus on the hydrometeorological conditions like rain evapotranspiration and infiltration processes to go further on the dependences between seasons and sm scaling kim and barros 2002b observed that multifractality was almost always involved on scales smaller than 10 km whatever the dryness of the soil however on scales greater that 10 km multifractality was found to become monofractality especially during drier conditions at large scales between 25 km and 400 km a comparable effect was noticed on our smos sm products purple circles fig 7 the multifractality index α is decreased from around 1 6 june to 1 3 december which may reflect a moderate decrease of multifractality during the study period therefore multifractal properties of sm at large scales seem to be related to the soil dryness this may give complementary explanations to the twofold scaling behavior of sm on the other hand considering dispatch sm a rather constant evolution of α is noticed on both small scales blue triangle symbols and large scales blue star symbols the first case confirms the idea that multifractality is not dryness dependent on smaller scales whereas the second is in contradiction with this assumption thus the latter should be considered cautiously to explain the scaling properties of sm 6 2 a model induced twofold scaling behavior of soil moisture in relatively recent works mascaro et al 2010 mascaro and vivoni 2012 scale invariance and multifractality were noticed from sm products measured from airborne l band radiometers southern great plains 1997 and 1999 experiments usa in these studies a log poisson multifractal model was applied she and levêque 1994 and a single scaling regime was observed on statistical moments from 0 8 km to 25 6 km scales although this result confirms the multifractal properties of sm on space scales similar to ours it refutes the existence of two scaling regimes no scale break was observed at about ten kilometers since the log poisson model is based on a similar universal theory as the um model continuous cascades it is somewhat unexpected not to detect the same transition on comparable sm products same technology and same scale range to investigate if this difference could be related to the case study different areas or periods we compared our dispatch products to fine scale airborne data acquired during the aaces 2 mission peischl et al 2012 this mission was performed in september 2010 during which transect flights were carried out over the murrumbidgee catchment brightness temperatures bt were acquired from l band radiometer on both h and v polarizations at a nominal 1 km spatial resolution the study area was divided in 5 patches of 50 100 km² each corresponding to a single flight day 13 16 19 21 22 september we gathered these patches into one single bt image and we selected a sub image of 128 128 km² to verify the presence of two scaling regimes in the data we applied spectral analysis on both h polarized and v polarized bt sub images in fig 8 the power spectrum of h polarized bt was compared to the power spectra of dispatch related products on equivalent period the spectra of each satellite product available between the 13 and 22 of september were averaged together since no dispatch sm products were pre selected on this period because of too many nan we chose the nearest available product which corresponds to 4 october in fig 8 one single linear fit is observed on bt power spectrum over the entire scale range from 1 km to 128 km and with a spectral slope equivalent to that of smos sm spectrum β 1 note that v polarized spectrum was not plotted here but it was found to be very similar to the h polarized one different scaling behaviors were noticed for aaces bt and dispatch sm on similar area and similar period this may sustain the idea that the scale break observed at 10 km could be caused by the dispatch model and specifically by the way in which the multi scale properties of each product are mixed in the algorithm to verify this hypothesis a simplified version of the c4dis processor was implemented in order to study the multi scale behavior of the different variables combined and generated through the algorithm to do this the method proposed by molero et al 2016 was followed which includes the two main steps described in section 4 2 1 the estimation of seehr variable soil evaporative efficiency from modis products eqs 13 15 and 2 the proper disaggregation process of sm from smos products seehr and smp parameter eqs 17 18 according to this method our algorithm was applied on smos and modis products acquired on november 19 2010 a sub area was selected 700 x 700 km² in order to have a smaller number of missing data and thus to get the minimum impact of gap filling on the studied products fig 9 shows the power spectra obtained from the input products of dispatch lst ndvi intermediate products ts see and output product sm mean the latter product is the average of the 6 disaggregated sm images obtained from the 6 sm lst combinations see section 4 1 here just one smos image of 25 km grid spacing was combined with the modis products indeed both cases with one smos image and four downsampled ones were implemented section 4 1 and molero et al 2016 and no significant differences were observed between the final products and between their power spectra therefore for simplicity of implementation only the case of one smos image was considered here for comparison the power spectrum of c4dis sm product acquired on the same date and on the same sub area was also plotted here the segmentation algorithm used in section 5 was applied on each power spectrum a geometric mean was estimated from the different scale breaks obtained revealing two averaged scale breaks which are nearly common to all spectra the first at almost ten kilometers l 9 km and the second at about thirty kilometers l 33 km to evaluate the link between the multi scale behavior of each product spectral exponents were estimated on the two following scale ranges from 33 km to 9 km large scales and from 9 km to 1 km small scales comparing our sm mean product with sm c4dis product table 3 a very similar scaling is observed on large scales β large 1 3 on small scales high spectral exponents are found with β small 2 for sm mean and β small 2 86 for sm c4dis these different spectral slopes on finer scales could be related to the non implementation of some filtering steps in our algorithm which are indeed coded in c4dis processor corrections of topography effects filtering lst data with low quality etc molero et al 2016 despite these small differences between sm mean and sm c4dis spectra the scale break remains noticeable as it seems to be on the other products of the algorithm to demonstrate this the absolute difference δβ β large β small was computed as an indicator of the amplitude of the scale break values greater than 0 6 were found for lst aqua and terra ts aqua and see aqua products these results seem to reveal that modis lst products would be the cause of the scale break located at about ten kilometers in the disaggregated sm product this scale break would propagate in the algorithm through the estimation of ts and see a possible explanation to this scale break in lst products may be related to the physical nature of the signal used indeed optical thermal sensors can be characterized by modified spectral slopes near the satellite resolution moreover this effect seems more important on aqua lst δβ 0 83 than on terra lst δβ 0 62 similar differences can be observed between the mean power spectra of aqua and terra lst over the full period fig 3 in section 5 1 thus it may be thought that the amplitude of the scale break could be related to the diurnal cycle of surface temperature since surface temperatures measured from aqua are acquired at the hottest hours of the day 13 30 there might be a correlation between the amplitude of scale break and the level of surface temperature considering the scale break observed at about 30 km this one may not be related to the multi scale properties of modis products but possibly to the combination of different products defined on different grid spacings indeed dispatch algorithm combines and creates products which have either the grid spacing of modis data 1 km or the grid spacing of smos data 25 km for example the estimation of see eq 15 combines end members t s max and t s min defined on the smos grid with another product ts defined on the modis grid as seen on fig 10 a footprint of smos pixels is then observable on the resulting image of see this property is due to the resampling strategy of smos data and to the end members that are defined on the smos grid this systematic footprint is visible in the real domain but can also have an impact in the fourier domain indeed sharp transitions at the smos pixels limits may create spurious convolutions by cardinal sine like functions which may affect the spectrum on the disaggregated sm this effect can generate an imperfect transition between the part of the spectrum related to smos sm l 25 km and the part related to modis products l 25 km regarding the behavior of see power spectra on scales greater than thirty kilometers fig 9 a lower spectral slope is observed β 0 5 comparing to that obtained on finer scales β 1 for 33 9 km this could be related to the oversampling of smos data generating harmonics on fine scales and therefore not including variability on large scales in this manner not only large scales but even fine scales could be affected by this effect the latter may also contribute in a way to the accentuation of the spectral drop observed at finer scales on the final disaggregated sm product 7 conclusion during the last century several studies were carried out to investigate the scaling properties of sm very diversified technologies were used to access and study the spatial structure of sm airborne microwaves products satellite optical indices etc moreover different approaches have been considered such as power spectra statistical moments fractal dimensions and even different types of cascade models log poisson universal multifractal and even no explicit parameterization in some cases in this study we analyzed the multifractal behavior of remotely sensed sm products over space scales ranging from the kilometric field scale to the continental scale universal multifractal model was applied for the first time on smos sm data giving access to large scale variabilities of sm over the australian landscape fractal and multifractal properties were observed which confirmed and completed some results reported in existing literature the relevant aspect of the present work may be the multi scale analysis of the outputs of the disaggregation algorithm dispatch merlin et al 2008a molero et al 2016 this deterministic algorithm improves the space resolution of smos sm products from 40 km to 1 km to do this it combines coarse scale smos sm with fine scale 1 km modis optical thermal data although several validation studies have been realized on this downscaling method malbéteau et al 2016 merlin et al 2012 2013 2015 molero et al 2016 none fully explored its statistical behavior over a continuum of space scales in this context we applied fractal and multifractal analysis on the different products involved in dispatch algorithm including disaggregated and original sm products and modis auxiliary data which are vegetation indices ndvi and surface temperatures lst input products of dispatch revealed relatively good scaling properties over the considered scale ranges indeed ndvi lst and original sm were characterized by a power law evolution of their power spectra and statistical moments meaning respectively fractality and multifractality however a specific scaling behavior was noticed for the output disaggregated sm two scaling regimes were obtained with a transition scale observed at about ten kilometers on both spectra and moments considering spectral analysis on large scales l 10 km disaggregated sm was found to have the same scaling as the original sm measured from satellite on finer scales l 10 km a different behavior was noticed with an increasing slope of the power spectrum similar scale break was detected on statistical moments showing that both spectral and multifractal properties of dispatch sm are characterized by this twofold scaling signature two possible arguments were given to explain the specific scaling of the disaggregated sm first a more physical interpretation may indicate that this twofold scaling behavior would be related to the real properties of sm as it was previously observed by kim and barros 2002b such scale break would be reflective of nonlinear hydrometeorological processes rainfall infiltration evapotranspiration acting at different space scales and modulated by terrain soils and vegetation distributions the spatial structure of sm may be more impacted by infiltration or runoff at the field scale whereas it would be mainly controlled by evapotranspiration or precipitation at the regional continental scale a more significant scaling transition was observed on the driest days early summer which may support the link between sm and external forcing agents such as precipitation a second explanation would be more algorithmic and directly related to the processing of the different products used and created within the algorithm the model used in dispatch would generate sm whose statistics are not properly distributed across scales this may occur at two levels in the algorithm first some modis products properties such as breaks in the scaling may be retrieved in the final dispatch products indeed a spectral drop at about the same ten kilometers scale was detected on lst power spectra although it is less pronounced than on disaggregated sm this scale break may be introduced by modis lst and amplified by the disaggregation model since one single scaling regime was noticed on brightness temperature bt products acquired in the l band over the same area and the same period these observations suggest that the unexpected scaling in modis products would be caused by the technology specific to optical thermal sensors then another impact of the algorithm on the multi scale properties of sm may be related to signal processing artifacts occurring with the combination of several products defined with different grid spacings this combination is required to permit conservativity between input and output sm products however from a signal processing point of view this could create systematic footprints on the final image i e visible smos pixels in the downscaled products and therefore affect the power spectrum convolutions by cardinal sine like functions at this point it is difficult to determine which of the physical or algorithmic factors would be at the origin of this twofold scaling behavior though a plausible hypothesis may be that both factors could affect the scaling of disaggregated sm indeed a scale break at about the smos sm resolution could be initially produced by combination artifacts which would be more or less amplified in the algorithm according to seasonal conditions resulting in moving the scale break during the period to finer scales further work need to be addressed to fully explain these results in particular to determine to what extent each of the two factors impacts the scaling of dispatch sm complementary auxiliary data should be compared to our products indeed an eof or comparable analysis made on dispatch sm and topography vegetation water content or soil content would provide relevant information about the connection between the spatial variability of these products and help with interpretation moreover it would be interesting to verify if precipitation products can be characterized by a similar scale break on equivalent space scales and over the same area murray darling basin however it must be considered that such a comparison might be complex to interpret since to our knowledge no theoretical model has been proposed yet to relate the spatial scaling properties of sm and that of rainfall as it was already done in the time domain by katul et al 2007 in the same way the comparison between dispatch sm and airborne bt is not that trivial because relatively complex operations are involved to get inverted sm from bt to illustrate this mascaro and vivoni 2012 noticed monofractality from bt data whereas multifractality was observed from the corresponding inverted sm data the scaling properties of bt could be affected during the inversion process explaining why the single scaling we observed on bt does not imply single scaling of dispatch sm therefore multifractal analysis of proper fine scale sm products may clarify this idea and help validating dispatch sm variability in the hypothesis of a model induced scale break current work is engaged to quantify the effect of modis products and the trace of pixel smos on different dates and on proper operational conditions analysis of products used and generated within the c4dis processor an application of dispatch using landat 7 auxiliary data instead of modis products was realized by merlin et al 2013 allowing a disaggregation process at sub kilometric scales 100 m since landsat 7 provides optical thermal data with higher resolution than modis it could be interesting to verify if both landsat 7 and the resulting disaggregated sm product would be characterized by a similar scale break but shifted on finer scales than the 10 km scale observed on modis therefore the results obtained could help to quantify the real impact of optical thermal auxiliary data on the multi scale properties of dispatch sm on a more operational point a view if this impact is confirmed the results obtained may help to define a specific scale below which the variability generated by the disaggregation model may not be as reliable as it should be concerning the impact of smos pixels footprint effects on the disaggregated product a solution could be to filter out the sharp transitions at smos pixels limits however this should be done with caution since such filters may excessively attenuate the variance at smaller scales another way to investigate our observations of dispatch sm is to focus on its dynamical behavior over different aggregation scales indeed one of the main problem in downscaling a dynamical behavior arises from the fact that the dynamical behavior of an aggregated signal can be approximated by the same deterministic equation structure only when the aggregated area is phase synchronized mangiarotti et al 2016 considering this issue the applicability of deterministic downscaling methods like dispatch may not be that obvious over certain spatial scales leading to several difficulties and perhaps contributing to explain the scaling irregularities observed in this study finally a possibility could be to compare the sm variability produced by dispatch with that created by fractal stochastic downscaling methods based on scaling properties these methods preserve the probability distribution from large to fine scales in precipitation several studies applied these algorithms on rainfall data rebora et al 2006 sharma et al 2007 research works proposed methods developed on multiplicative cascade such as log poisson deidda 2000 or um model gires et al 2012 revealing some potential to quantify uncertainty and representativeness errors between coarse scale and in situ measurements concerning sm downscaling some studies used such fractal based methods bindlish and barros 2002 kim and barros 2002a mascaro et al 2010 in our study it may be interesting to apply this kind of method on smos products this would consists in injecting in the um model the values of α and c 1 parameters obtained from smos products on large scales and then continuing the cascade at higher resolutions following this procedure the fine scale field will have the same scaling properties as the coarse scale one however since the disaggregation is based on random generator an ensemble of possible fields can be proposed from just one pair of α c 1 parameter therefore this kind of methodology may not be fully suitable in the case of operational hydro agricultural applications in particular when determining the position of the extremes to overcome this inconvenient a combination of the two approaches may be an interesting compromise between statistical scaling and evaporation based determinism for example in dispatch algorithm an idea might be to find a modified estimator of see that would be used in the disaggregation eq 18 this modified see would be computed by applying a 2d filter on the original see which would be actually equivalent to perform a fractional integration of order δh hrequested hnon filtered with hrequested and hnon filtered measured respectively from smos soil moisture at large scales and from non filtered see for scales under 10 km doing this the spectral slope of see may be adjusted like that of the final disaggregated soil moisture thus coarse scale and fine scale fields could be related through a common degree of fractional integration which may contribute to limit the twofold scaling behavior observed on the disaggregated product in practice this modification would not be easy to implement since the filtering should be properly dimensioned in order to affect only the small scales between 1 and 10 km moreover this texture based image correction may impact the physical properties of see so there would be a compromise to be made on this aspect acknowledgements we are grateful to c rüdiger and j walker from monash university melbourne australia for their advice and for giving us access to aaces data http www moisturemap monash edu au aaces we also want to thank the smos team particularly a mialon and a al bitar for fruitful discussions this study was supported by the région occitanie france and iut paul sabatier toulouse france smos products were acquired from the center aval de traitement des données smos catds which is the french ground segment developed by the center national d etudes spatiales cnes france in collaboration with ifremer brest france modis products were obtained from the land processes distributed active archive center lp daac operating as a partnership between the nasa and the u s geological survey usgs 
558,soil moisture has a strong impact on climate hydrology and agronomy at different space scales from the continent global scale to the local watershed passive microwave sensors like smos satellite soil moisture and ocean salinity allow a global study of soil moisture on the entire globe to have access to kilometric variability disaggregation algorithms have been developed such as the disaggregation based on physical and theoretical scale change dispatch this method improves the space resolution of smos soil moisture from 40 km to 1 km to do this it combines coarse scale 40 km smos products with fine scale 1 km optical thermal data validation studies on specific scales showed the potential of dispatch to enhance the spatio temporal correlation of disaggregated sm with in situ measurements under low vegetated semi arid regions although the efficiency of the method was revealed in these regions no studies fully explored its statistical behavior over a continuum of space scales in this paper we studied and compared the spatial multi scale statistics of the different input and output datasets involved in dispatch downscaling to do this we applied spectral and multifractal analysis on the respective products for the region of southeastern australia from june to december 2010 fractal and multifractal properties in the framework of the universal multifractal model were observed on inputs of dispatch smos soil moisture modis vegetation indices and surface temperature which confirmed and completed some results reported in existing literature for the output disaggregated soil moisture two scaling regimes were observed with a transition scale observed at about ten kilometers considering spectral analysis at large scales 10 km disaggregated soil moisture was found to have the same scaling as the original smos soil moisture on finer scales 10 km a different behavior was noticed with a higher value of the slope of the power spectrum the same scale break was detected on statistical moments showing that both spectral and multifractal properties of dispatch soil moisture are characterized by this twofold scaling signature keywords soil moisture multi scale analysis multifractals disaggregation smos dispatch 1 introduction soil moisture sm is a key component of the climate system and is strongly heterogeneous at many time and space scales interactions between land surface and atmosphere such as water energy and carbon fluxes are strongly related to sm ochsner et al 2013 it has a significant role in the water cycle as it impacts runoff infiltration and evaporation processes thus sm is an important variable in several scientific fields such as hydrology western et al 2004 meteorology dai et al 2004 climatology anderson et al 2007 and water resource management engman 1991 sm is heterogeneously distributed at different space scales from few centimeters to several kilometers this variability is due to environmental factors impacting directly sm at specific scale ranges brocca et al 2007 crow et al 2012 jana 2010 vereecken et al 2014 for instance we could mention here soil properties texture and structure acting at the field scale topography features at the watershed scale land cover vegetation and meteorological forcing at the regional and continental scales many ground measurement techniques have been developed to acquire highly resolved sm data sets down to centimeters in space and minutes in time for more details see dobriyal et al 2012 robinson et al 2008 robock et al 2000 although these methods are recognized as reliable and easy to implement they are not adapted to represent spatial heterogeneity of sm at regional and continental scales collow et al 2012 crow et al 2012 regional and global scale variability of sm may be acquired and studied with the help of remote sensing different active and passive microwave satellites allow daily measurement of surface soil moisture in the first 5 cm of the soil column petropoulos et al 2015 wigneron et al 2003 these satellites acquire sm information thanks to the relationship between the soil dielectric constant and water content active microwave sensors measure the energy reflected by the soil after sending a microwave pulse to the surface backscatter we find c band synthetic aperture radars sar like sentinel 1 satellite s1 from european space agency wagner et al 2009 and c band scatterometers like the advanced scatterometer ascat bartalis et al 2007 these active sensors can provide space resolution from few meters s1 to tens of kilometers ascat their main drawback is their sensitivity to vegetation and surface roughness which can alter useful information sm in the signal measured passive sensors however are less sensitive to scattering conditions they measure the self emission from the land surface radiances good results were obtained by c and x band radiometers like the advanced microwave scanning radiometer earth observing system amsr e njoku et al 2003 owe et al 2001 or by l band radiometers such as soil moisture and ocean salinity smos kerr et al 2010 and the recent soil moisture active passive smap mission entekhabi et al 2010a l band microwaves 1 4 ghz have the benefit of little sensitivity to vegetation providing optimal estimation of sm on a wider range of land cover conditions l band based satellite missions deliver sm products with a revisit time of 2 to 3 days however because of technological constraints the spatial resolution is coarse 30 55 km much coarser than the kilometer scale this is a problem since hydro agricultural applications need better resolved information below kilometric space scales walker and houser 2004 to address this issue downscaling methods have been created to improve the low spatial resolution of satellite data downscaling algorithms are characterized by their input data satellite products auxiliary ground measurements etc and by the type of method physical or statistical peng et al 2017 reviewed the several methods developed so far and proposed a three group classification satellite based methods methods using geo information data and model based methods the first group i gathers downscaling techniques which combines satellite passive microwave products with satellite highly resolved auxiliary data such as radar or optical thermal observations this takes advantage of the assets of complementary remote sensing measurement techniques considering the fusion with high resolution radar a change detection method was proposed by njoku et al 2002 to merge coarse scale passive microwave soil moisture products and fine scale active backscatter data this technique consists in the linear relationship between soil moisture and backscatter data assuming the time invariance of vegetation and surface roughness effects the methodology was further tested in other experiments narayan et al 2006 piles et al 2009 and proved its efficiency for improving the spatial details of soil moisture statistical tools were also used to combine active and passive products such as bayesian merging method zhan et al 2006 or wavelet based image enhancement method montzka et al 2016 this kind of approach showed the great potential of radar for improving soil moisture resolution in particular for higher vegetation water content and different land cover types akbar and moghaddam 2015 a possible limitation of this approach is the time lag between active and passive data due to the low revisit rate of high resolution radar recently smap satellite was launched to bypass this problem embedding on board one radiometer and one radar das et al 2011 unfortunately the radar failed and no active passive combination could be performed however the previous studies made to prepare the mission showed good capacity to improve spatial resolution of satellite products by merging active and passive microwave data another type of satellite based method is the combination of passive microwave data with optical and thermal remote sensing data the interest is to have the additional information of high spatial resolution and short revisit time of the optical thermal products the concept is to use highly resolved vegetation cover and surface temperature products to downscale coarse scale soil moisture product based on the surface temperature vegetation index triangular feature space proposed by carlson et al 1994 2007 zhan et al 2002 and later chauhan et al 2003 developed and applied this method based on a polynomial function linking high resolution sm with surface temperature vegetation cover and surface albedo at coarse resolution scaling factors regression coefficients are estimated from this polynomial function and then used at high resolution in the same function to calculate the high resolution sm using ndvi normalized difference vegetation index and lst land surface temperature obtained from the lst ndvi feature space an improved version was proposed by piles et al 2011 using brightness temperatures instead of albedo showing better results when comparing downscaled sm with in situ measurements for instance this downscaling technique was used to improve the resolution of amsr e soil moisture merging it with optical thermal data measured from modis moderate resolution imaging spectroradiometer choi and hur 2012 or msg seviri meteosat second generation enhanced visible and infrared imager zhao and li 2013 the main problem in this methodology is the non conservativity of sm between fine scale and coarse scale sm based on the same theory other downscaling algorithms were proposed to relate the downscaled sm with coarse observations of sm an operationally implemented method is the downscaling algorithm dispatch disaggregation based on physical and theoretical scale change merlin et al 2008a molero et al 2016 this algorithm is more physical because it uses soil evaporation processes to connect optical thermal and sm data different applications of dispatch were realized to increase the 40 km resolution of smos sm to 1 km and even 100 m respectively with modis merlin et al 2012 and landsat 7 merlin et al 2013 products the originality of the method is the estimation of a sm proxy called soil evaporative efficiency see sections 4 2 and 6 2 the latter has the advantage compared to land surface temperature or evapotranspiration to be more linked to sm and to be quite constant during the day some improvements still need to be made about the modeling of see especially on elevation and illumination effects malbéteau et al 2017 or soil properties and atmospheric conditions merlin et al 2016 comparable evaporation based methods were developed using different proxies of sm such as the soil wetness index kim and hogue 2012 or the vegetation temperature condition index peng et al 2016 both applied in the simple downscaling method ucla we can also mention algorithms directly improving the resolution of brightness temperature products instead of retrieved sm based on the relation between daily temperature change and daily average sm song et al 2014 generally these downscaling methods present a significant asset considering the time coherence between the merged products but some limitations exist indeed the cloud sensitivity of optical thermal sensors makes the application of these methods possible only under clear sky conditions djamai et al 2016 since sm is directly linked to geoinformation data such as topography soil properties and vegetation attributes werbylo and niemann 2014 a second group ii of downscaling methods were also proposed these methods take advantage of highly resolved geoinformation data giving information on the local attributes of the zone studied and could give access to very high spatial resolution of sm topography for example was often used in downscaling approaches as an auxiliary data busch et al 2012 pellenq et al 2003 however certain types of geoinformation data like soil properties are usually provided by ground observations which are really specific to the studied area thus the application is limited to local areas and it may not be suitable for global scale study of sm the third class iii of methods concerns model based downscaling techniques there are two types of models used here on the one hand there are hydrological land surface models these ones are more site specific because they try to link coarse scale remotely sensed sm and fine scale parameters obtained from local land surface models the downscaling can be done through optimization techniques ines et al 2013 linear regressions loew and mauser 2008 or bivariate relationships verhoest et al 2015 on the other hand there are models that analyze and describe statistics across scales they are more generic and try to preserve statistical properties across scales for example kaheil et al 2008 proposed a wavelet based downscaling method in order to model spatial statistical properties of fine scale sm thanks to coarse scale airborne sm products other approaches are based on the scaling or fractal properties of sm across spatial scales bindlish and barros 2002 proposed a fractal interpolation method applied on airborne sm products measured from electronically scanned thinned array radiometer estar they used power spectra to represent the fractal behavior of sm and could improve spatial resolution from 200 m to 40 m a few years later mascaro et al 2010 applied log poisson multifractal cascades on remote sensing sm to generate simulations of fine scale sm the challenge here is to preserve non stationarity from coarse to fine scales nevertheless particular efforts are made to overpass this problem for example kim and barros 2002a adapted the fractal interpolation method applying a sliding window on specific parts of the original field they could simulate fractal variability while taking into account the local statistics of the field the downscaling methods of the three groups presented above have their own advantages and disadvantages with more or less efficiency according to specific surface or climate conditions in this study we focus on the evaluation of multi scale variability of sm products generated by the method dispatch merlin et al 2008a molero et al 2016 despite its limitations related to cloud cover this semi physical downscaling algorithm combines low sensitivity to vegetation of l band microwaves high spatial resolution of optical thermal data and it is dispensed from estimation errors commonly generated by land surface models several studies have been realized so far to evaluate and validate this method malbéteau et al 2016 merlin et al 2013 2015 molero et al 2016 in general the assessment of downscaling algorithms is made comparing fine scale output products with ground measurements different performance metrics are used such as correlation root mean square error or bias albergel et al 2013 al bitar et al 2012 entekhabi et al 2010b more recently merlin et al 2015 proposed a new metric that estimates the gain given by the downscaling method in terms of representativeness of downscaled data compared to non downscaled data to take into account scale mismatch between downscaled and ground measurements upscaling techniques have been developed in order to bring downscaled and ground data together at common space scales crow et al 2012 for example merlin et al 2013 applied the dispatch algorithm on smos data while using both modis moderate resolution imaging spectroradiometer and landsat 7 auxiliary data coarse scale satellite data downscaled data and aggregated ground measurements were compared at three different scales 40 km 3 km and 100 m good results confirmed the potential of dispatch to improve the spatio temporal correlation of remotely sensed sm with in situ measurements however the drawback of these validation techniques is that they are restricted to specific scales thus the validation of disaggregated sm products over a continuum of space scales has not been fully explored yet investigation of the multi scale statistics and of possible scaling properties of these products could provide relevant information on this aspect during the last thirty years several studies were carried out to describe the statistical properties of sm across spatial scales famiglietti et al 2008 rodriguez iturbe et al 1995 different analytical methods were proposed the most commonly used are spectral wavelet analysis si 2008 and multifractal analysis kim and barros 2002b mascaro et al 2010 oldak et al 2002 in 1995 rodriguez iturbe et al highlighted for the first time the fractal behavior of sm from remote sensing the spatial variance of sm followed a power law decay as a function of aggregation scales ranging from 30 m to 1 km washita experiment 1992 usa later studies showed that such a scaling behavior of sm variance could be extended to wider range of scales up to regional scale hu et al 1997 and even to continental scales rötzer et al 2015 similar research works demonstrated that increasing area extent increasing size of the total area induced the increase of sm variance according to a power law function famiglietti et al 2008 rötzer et al 2015 brocca et al 2012 moreover in oldak et al 2002 the fractal scaling of sm was revealed to be multifractal the power law was also applicable to the first six statistical moments of airborne sm products for scales ranging from hundreds of meters to tens of kilometers washita 92 experiment and southern great plains experiment 1997 usa multifractal scaling was then detected in sm fields das and mohanty 2006 kim and barros 2002b lovejoy et al 2008 mascaro et al 2010 since sm variability is directly related to the amount of soil wetness brocca et al 2007 famiglietti et al 2008 it may be expected that scaling properties of sm may vary according to the state of sm indeed when plotting sm variance power law in log log coordinates rodriguez iturbe et al 1995 and manfreda et al 2007 found that the corresponding slope of the curve was increased during drier periods revealing seasonal variations of sm scaling rötzer et al 2015 moreover it was observed that sm variability was not governed by a single scaling behavior but by different scaling regimes depending on the range of scales at the field scale sm variability is mainly related to land surface characteristics such as soil properties or topography whereas at larger scales it is impacted by meteorological quantities like rainfall or evapotranspiration cayan and georgakakos 1995 entin et al 2000 studies based on semi variograms ryu and famiglietti 2006 korres et al 2015 and spectral moments analysis kim and barros 2002b revealed the presence of scale breaks closed to this transition scale between land surface and meteorological regimes though the aforementioned characteristics of sm highlight its complexity and its high degree of nonlinearity due to hydrometeorological processes acting at different space scales attesting the necessity to better understand the scaling behavior of sm for applications such as data assimilation or downscaling rötzer et al 2015 in this paper we propose an alternative and complementary method for verifying the multiscaling behavior of dispatch products to do this we studied and compared the statistical spatial properties across scales of the downscaled sm the original smos sm and the modis auxiliary data by applying spectral and multifractal analysis in the framework of the universal multifractal um model schertzer and lovejoy 1987 the definition of multifractal formalism is given in section 2 with a particular attention paid to um parametrization the methodology followed for multifractal and spectral analysis is detailed in section 3 section 4 describes the case study and the data set then the different results obtained from spectral and multifractal analysis are presented in section 5 finally section 6 proposes explanations to the multiscaling behaviors of dispatch sm and a general conclusion of this study is given in section 7 2 theory of multifractals during the last century several studies showed that many geophysical processes could present scale invariance properties this was first anticipated by richardson 1922 in the case of turbulence he described turbulent flows as cascade processes that transfer kinetic energy from large to small scales based on this approach statistical models of turbulence were proposed such as the famous kolmogorov law 1941 to describe velocity increments later research works generalized the study to take into account the heterogeneity of the energy flux kolmogorov 1962 obukhov 1962 yaglom 1966 multi scale models such as multiplicative cascades were therefore proposed to reproduce scale invariance properties through the use of fractal geometry later scale invariance was noticed in other geophysical fields in his study of the coast of britain mandelbrot 1967 revealed the presence of fractal properties in topography 2 1 from fractal sets to multifractal fields the concept of fractal dimension has been used in many works related to multi scale analysis and geophysical modeling indeed the term fractal refers to any entity time series or 2d 3d random field in which each part presents similar properties geometrically or statistically to the ensemble in this manner the structure of a fractal entity is characterized by scale invariance initially the notion of fractal was introduced in the late 19th century in geometry with the creation of sets i e mathematical objects having unusual properties especially a non integer hausdorff dimension called later by mandelbrot as fractal dimension mandelbrot 1967 scale invariance in the statistical sense was theoretically proposed by kolmogorov in 1940 with the introduction of the fractional brownian motion this model could generate random time series whose trajectories present fractal properties in terms of statistical distribution it illustrates the physical interest of fractal random processes since brownian motions are somewhat ubiquitous in physics mandelbrot and van ness 1968 made it famous by introducing it to more physical models in particular the first fractal stochastic models of topography were developed based on this theory mandelbrot 1975 these stochastic models aim to represent the simple scaling monofractal behavior of geophysical processes in this context the fractal dimension or scaling parameter is assumed to be unique restricting multi scale modeling to a specific class of variability however most geophysical processes are characterized by more complex statistics in case of operational hydrology rare and extreme events present in precipitation or soil moisture for example correspond to high order statistics and need to be detected hubert et al 1993 therefore multifractal models characterized by an infinite spectrum of fractal dimensions have been proposed to account for a more exhaustive set of statistics schertzer and lovejoy 1987 based on the findings of parisi and frisch 1985 initially established the multifractal formalism through the fundamental equation 1 p r ϕ λ λ γ λ c γ where φλ is a positive normalized random scalar process time series or random field defined on r 2 or r 3 the mean of the process is assumed to be statistically conserved across scales λ is the observation resolution here defined as the inverse of the scale that can be seen as the sampling time or pixel size for time and space domain processes respectively and indicates an equality within the limits of slowly varying functions eq 1 expresses the fact that for a multifractal process the probability of exceeding a threshold varies as a power law of the resolution with exponent c γ this exponent is called as fractal codimension of the process depending on the amplitude of thresholds the thresholds are defined by the following power law 2 t λ λ γ with γ the notion of singularity characterizing the amplitude of the process independently of the scale each singularity is associated to a fractal codimension c γ corresponding to a family of thresholds of various amplitudes from a more physical point of view high singularities detected by high thresholds are related to rare and extreme events with high fractal codimensions and inversely low box counting fractal dimensions df mandelbrot 1967 indeed the latter are related to the dimension of space d through the relation c γ d df therefore c γ can be described as a codimension function increasing with γ which completely characterizes the multi scale statistical properties of the field φλ in general if the field is multifractal c γ is found to be convex and positive with a fixed point c 1 imposed by the condition of canonical conservation whereas monofractality is associated to the trivial case c γ const since probability distributions and statistical moments are related by a mellin transform schertzer and lovejoy 1987 proposed an equivalent equation to 1 3 ϕ λ q λ k q where is the statistical averaging operator q is the order of the moment q 0 and k q is the moment scaling function eq 3 expresses that for any fixed moment order statistical moments and resolution are linked through a power law singularities and moment orders are directly linked since the moment scaling function k q is the legendre transform of the codimension function c γ similarly to c γ k q is a convex function with the special case k 1 0 related to the conservation of the mean across scales which entirely characterizes the multifractal field 2 2 multiplicative cascades multiplicative cascades are stochastic models that can be used to build multifractal fields cascades are multiplicative processes because they are defined by an iterative multiplicative construction considering a two dimensional random signal field each pixel at resolution λ n 1 with n the construction level of the cascade is the product of the embedding pixel at coarser resolution λ n multiplied by a random variable με this is described by the following equation 4 ϕ λ n 1 μ ε ϕ λ n in this manner the statistical properties of the field ϕ λ n 1 are directly related to the statistical properties of the coarser field ϕ λ n if all the multiplicative random variables used for each step of the iterative construction are independent and identically distributed and distributed independently of the scale the final field presents scale invariant properties several models of cascades have been developed so far first models were built within the framework of turbulence such as the α model schertzer and lovejoy 1984 which corresponds to discrete construction of cascades the multiplicative random variables are limited to two possible fixed values respectively leading to increasing or decreasing pixel value when the resolution is refined later more elaborated models were constructed generalizing the discrete case to continuous cascades dubrulle 1994 schertzer and lovejoy 1987 1991 1997 she and levêque 1994 the latter are based on an infinite number of steps between any pair of resolutions leading to continuity in scale the benefit of continuous cascades is twofold first they can represent possibly more realistic structures by avoiding any arbitrary discretization of scales moreover they often converge toward random processes which are characterized by a small number of degrees of freedom special cases of log infinitely divisible distributions this is interesting considering that multifractal fields built by multiplicative cascade processes would otherwise need an infinite number of scaling parameters one for each fractal dimension for example she and levêque 1994 proposed a continuous cascade model based on log poisson statistics and schertzer and lovejoy 1987 used log stable random variables to build the universal multifractal model in both models only two fundamental parameters are needed to fully define multifractality 2 3 universal multifractals physically multifractal fields built by log poisson or log stable cascades have a high degree of generality in geophysics log poisson model has been successfully applied to different geophysical variables such as rain deidda 2000 or even soil moisture mascaro et al 2010 however this model can have disadvantages of representing a restricted range of variabilities which may make it unsuitable for modeling processes with unbounded singularities on the other hand by assuming the stability of the random variables and suitable renormalization um model is likely adapted for characterizing a wide range of processes topography lavallée et al 1993 rain and clouds tessier et al 1993 and more recently soil moisture and vegetation optical indexes lovejoy et al 2008 moreover a possibly more immediate physical interpretation of the parameters is found in this model for mathematical and physical arguments supporting the universality of um model see schertzer and lovejoy 1997 see also gupta and waymire 1997 for discussion about its generality um model defines the moment scaling function using two universal parameters through the following equation schertzer and lovejoy 1987 5 k q c 1 α 1 q α q where α is the degree of multifractality of the field it varies between 0 monofractality and 2 log normality and expresses how fast the codimension evolves as a function of the singularity the second parameter c 1 is the codimension giving the dominant contribution to the mean value of the field related to moment of order 1 c 1 k 1 physically it indicates inhomogeneity dispersion of the field it varies from 0 homogeneous field to the dimension d of the embedding space very intermittent field because of legendre transform c c 1 k 1 is also defined as the fixed point of the codimension function 2 4 fif model generally most of the geophysical fields are non conservative i e integrated processes defined by a certain degree of fractional integration this appellation comes from multifractal cascade models see gagnon et al 2006 for detailed explanations on this formalism thus to account for a wider range of processes an extension of the um model to non conservative fields has been proposed schertzer and lovejoy 1991 the fractionally integrated flux fif model it expresses the degree of fractional integration of the um field using a third parameter h the latter is called the order of integration and defines the non conservativity of the field in plain words the larger is h the smoother is the field the integrated flux is noted r λ and is characterized by a power law variation of its stationary increments 6 δ r λ ϕ λ δ x h where δr λ are the increments fluctuations of the flux estimated over a varying window δx which is equivalent to the space scale l note that when h 0 the equation corresponds to the conservative case φλ additionally in the case of two dimensional fluxes eq 6 also applies for other directions i e δy increments with the same exponent h if the process is isotropic hereafter in this article the appellation proposed in lovejoy and schertzer 2010 will be followed non integrated cascades will be called conservative fluxes due to the conservation of the mean and fractionally integrated non conservative processes will be called random fields or simply fields 3 multifractal analysis methodology the different techniques used to analyze the multi scale properties of dispatch related products are detailed in this section the methodology is based on the multifractal theory presented in the precedent section because our study treats only satellite images we will focus on the two dimensional versions of these techniques 3 1 power spectrum preliminary evidence of scaling spectral analysis is a methodology often used in geophysics to characterize in an easy and rapid way some scaling properties of fields over different space scales lovejoy et al 2008 thanks to its high sensitivity to scale breaks scaling regimes can be easily identified in a first step the two dimensional power spectral density p kx ky of the data under analysis x is estimated 7 p k x k y f f t x 2 with p the power spectral density defined on both vertical and horizontal image axis corresponding respectively to kx and ky wavenumbers spatial frequencies here the estimation of the psd is done through a two dimensional fft or fast fourier transform then the one dimensional isotropic angle integrated power spectrum e k is obtained lovejoy et al 2008 8 8 e k k k p k d k where k is the modulus of the wavenumber and is the euclidean norm since it expresses space frequencies k is directly related to the space resolution λ if the process presents scaling properties the spectrum should follow a power law where β is the negative slope of e k on a log log graph 9 e k k β β is called the spectral exponent and is directly related to the fif parameters through the equation 10 β 1 2 h k 2 in this manner β also gives first indications about the possible conservative nature of the field since integrated flux h 0 should correspond to spectral exponent greater than 1 note that power spectrum is a second order statistic hence the term k 2 3 2 statistical moments multifractal properties to test the presence of multifractal properties in the data eq 3 statistical moments and moment scaling function need to be estimated to do this different steps must be followed first the underlying conservative field ϕ λ m a x has to be reconstructed from the data at the maximum observation resolution λ max because the possible existence of a fractional integration of order h eq 6 a fractional derivative of the same order should be done in this study the modulus of the gradient was applied to the data indeed this operator provides a simple and good numerical approximation of the fractional derivation without prior knowledge of h order lavallée et al 1993 11 ϕ λ m a x r λ m a x x 2 r λ m a x y 2 once the conservative field is retrieved ϕ λ m a x is normalized by its mean the second step involves the degradation of the field at lower resolutions λ λ max it aims to approximate the inversion of the stochastic multiplicative cascade by iteratively averaging the field at coarser scales each coarse pixel level n of the cascade is obtained by a simple average of neighboring finer pixels level n 1 note that each roughened pixel size is a power of two multiplied greater than the observation scale lmin λ max 1 finally empirical moments i e computing q th order moments in eq 3 while replacing statistical averages by empirical averages are then computed for various orders and resolutions on a log log graph the different moments are plotted as a function of the resolution if linearity is observed for each moment curve at least over a significant range of resolutions eq 3 is therefore verified which is the signature of multifractality the empirical moment scaling function can be estimated with k q corresponding to each linear fit of qth order moment afterwards the universal parameters α and c 1 may be obtained by optimization according to the um model form of k q eq 5 3 3 structure functions some evidence of non conservativity a convenient way to reveal the non conservative fractionally integrated nature of the integrated flux r λ eq 6 is to compute its first order structure function 12 δ r λ δ x r λ x δ x r λ x if the flux is indeed non conservative the order of integration h should be the slope of the increments δr λ plotted in a log log graph as a function of space scale δx this technique will be used in this study to estimate the h parameter 4 case study and data 4 1 c4dis processor and satellites products the different products analyzed in this study are input and output data of c4dis catds level 4 disaggregation processor molero et al 2016 this processor includes the first operational version of the dispatch algorithm taking into account the best configurations according to the latest studies merlin et al 2010a 2010b 2013 because the algorithm is still evolving c4dis products are called as scientific molero et al 2016 users can have access on demand to the products over specific areas of the world as presented earlier the downscaling method combines smos microwave data and modis optical thermal data the sm data is given by the smos level 3 daily global sm product reference mir clf31a d this product is provided by the center aval de traitement des données smos catds which is the french ground segment for smos level 3 and level 4 products the sm data is acquired every day at a radiometric resolution that varies between 35 and 55 km 40 km in average from l band brightness temperature measurements kerr et al 2012 wigneron et al 2007 smos level 3 products are delivered on the ease equal area scalable earth grid with a grid spacing of 25 km 25 km the optical thermal products come from the modis sensor embedded on both aqua and terra satellites two types of auxiliary data are used in dispatch first there is land surface temperature lst it is extracted from the modis level 3 daily products myd11a1 aqua and mod11a1 terra these temperature products are estimated from thermal infrared radiances emitted from the surface 3 15 μm then the second auxiliary data is normalized difference vegetation index ndvi given by the level 3 16 day terra product mod13a2 the vegetation index is computed from surface reflectances in red 0 7 μm and near infrared 0 8 μm wavelengths both lst and ndvi products are provided at 1 km resolution by the nasa land processes distributed active archive center lp daac they are presented on a sinusoidal grid with a grid spacing slightly smaller than kilometer 0 93 km 0 93 km solano et al 2010 wan 2006 we may notice that lst products have daily time resolution whereas ndvi products are representative of a period of 16 days output dispatch products are generated every day by the c4dis processor their resolution is that of modis products 1 km and they are presented on an equal spaced lat lon wgs84 grid with a grid spacing of 0 01 1 12 km for simplicity in the following we ll make the approximation 0 01 1 km one single downscaled image is the result of the combination of four downsampled smos sm images one modis ndvi image and up to six modis lst images corresponding to 3 consecutive days of aqua and terra acquisitions for more details on the combination methodology see malbéteau et al 2016 merlin et al 2012 molero et al 2016 in other words in the final product each high resolution output pixel comes from the average of 24 possible disaggregated pixels up to 24 sm lst possible pairs the advantage of this composition is that uncertainty in downscaled sm can be potentially reduced and estimated and time coverage is improved malbéteau et al 2016 4 2 dispatch algorithm dispatch is based on a semi empirical model that estimates the soil evaporative efficiency see from high resolution hr 1 km lst and ndvi products the method is based on the separation of modis lst into its soil and vegetation components respectively referred in this study as t s hr and t v hr to do this the approach relies on a variant of the trapezoid method from moran et al 1994 which interprets the feature space defined by modis lst and ndvi derived fractional vegetation cover f v hr the purpose here is to extract the soil temperature t s hr according to the following equations 13 t s h r l s t f v h r t v h r 1 f v h r with 14 f v h r n d v i n d v i s o i l n d v i v e g e t n d v i s o i l in eq 13 the vegetation temperature t v hr is calculated according to moran et al 1994 ndvisoil and ndviveget in 14 are respectively the ndvi obtained from bare soil set to 0 15 and from full cover vegetation set to 0 90 then modis derived soil temperature t s hr allows to estimate seehr at 1 km resolution following the methodology proposed by merlin et al 2012 15 s e e h r t s m a x t s h r t s m a x t s m i n where t s max and t s min are endmembers estimated from the approximations of merlin et al 2013 considering the relations between the minimum maximum of lst and the associated f v hr more details can be found on these estimates in molero et al 2016 p 4 see is used to describe the spatial variability of sm within the low resolution lr 40 km pixel given by smos product high resolution sm smhr is linked to high resolution see seehr through the linear model proposed by budyko 1961 and manabe 1969 16 s e e h r s m h r s m p where smp is a lr parameter depending on atmospheric conditions and soil properties in the c4dis processor this parameter is computed at low resolution at each execution from daily smos sm smlr and see averaged inside the lr pixel seelr 17 s m p s m l r s e e l r the disaggregation is finally realized by applying a first order taylor expansion to the see and sm dataset the downscaling relationship is written as 18 s m h r s m l r s m s e e l r s e e h r s e e l r with sm seelr the partial derivative of sm relative to see computed at low resolution here this derivative simply equals the smp parameter estimated according to 17 4 3 study area australia is a wide country with an area of almost 8 million km² and characterized by various surface and climate conditions thus it is a suitable area to study spatial variations of soil moisture over a wide range of scales many studies on sm have been carried out in australia in order to monitor sm variability using ground airborne and satellites data smith et al 2012 among others we can mention the national airborne field experiment 2006 nafe 06 merlin et al 2008b and the australian airborne calibration validation experiments for smos aaces peischl et al 2012 these experiments were realized over the murrumbidgee catchment 82 000 km² fig 1 located at the southeastern part of australia because of its variable climatic conditions humid in the east semi arid in the west this region was used for validating satellites missions such as smap panciera et al 2014 or smos sm products delivered by smos were assessed during the aaces experiments which took place in 2010 over two periods january february aaces 1 and september aaces 2 wide spatially distributed networks of in situ measurements oznet hydrological monitoring network smith et al 2012 and transect flights polarimetric l band multibeam radiometer peischl et al 2012 were used to validate smos data in this context benefiting from a dense sm dataset at different space scales some of the first applications of dispatch algorithm were realized during the aaces experiments merlin et al 2012 these works showed the efficiency of dispatch under low vegetated semi arid areas and its potential to evaluate coarse scale smos products later studies malbéteau et al 2016 molero et al 2016 continued the evaluation and improvement of dispatch algorithm over the murrumbidgee catchment in this paper dispatch analysis is made during the 7 month period from june to december 2010 taking advantage of previous dispatch studies over this period we choose to extend the study area from the murrumbidgee catchment to the murray darling basin mdb 1 million km² fig 1 the first reason of this choice is related to the main objective of the study which is the analysis of dispatch related products over different space scales though our study covers a large range of scales from the pixel size kilometer scale to the full basin extent 1300 1400 km² giving a new point of view considering dispatch validation moreover spectral and multifractal tools presented in section 3 cannot be properly applied if the data size is not sufficient enough because of its low resolution it would be inappropriate to do multiscale analysis of smos sm over the murrumbidgee catchment images would be smaller than 5 5 pixels mdb is located in southeastern australia and contains more than 20 catchments such as murrumbidgee in its south part fig 1 the climate is sub tropical in the north east average annual precipitation up to 1500 mm semi arid in the west average annual precipitation less than 300 mm and mostly temperate in the south snowfall during winter on the peaks of the great dividing range regarding to land use west is made of wide plains essentially composed of saltbush shrublands and mulga lands from south to north east there are the mountains of the great dividing range reaching 2 300 m in altitude irrigation dry land cropping and pastures are spread over the basin but most of the irrigated areas are located in the south like murrumbidgee region 4 4 data preprocessing before applying the multi scale analysis preprocessing must be done on the different satellite products the first preprocessing step is to handle the missing values because of technology or acquisition conditions all satellite sensors provide products that present more or less missing values these can be caused by failures in the data acquisition or delivering or even voluntarily generated by the production center when discarding incorrect values in our case smos products can be affected by unauthorised emissions that cause radio frequency interference rfi smos sm used in this study are pre filtered by catds in order to remove pixels with more than 10 rfi probability kerr et al 2013 olivia et al 2012 considering modis products cloud pixels are also removed to avoid the impact of atmosphere on downscaled data though missing values in output dispatch products are mainly caused by the accumulation of missing values coming from inputs thanks to the 24 averaged hr outputs combination implemented in c4dis processor section 4 1 the probability to get missing values in the final averaged downscaled product is reduced in our study we applied bilinear interpolation in each satellite image to fill in missing data noted nan to do this properly some conditions were established to minimize the impact of data interpolation on spectral and multifractal analysis each image with more than 40 of nan were discarded moreover in order to treat separately land surface nan values from sea areas located outside the continent the latter were filled with zeros previous studies showed that biased multifractal parameters could be obtained from data containing significant proportion of zeros de montera et al 2009 verrier et al 2010 2011 thus we made sure to select images whose ground area contains a minimum of sea pixels less than 10 in a second stage sub images of 2 n 2 n pixels need to be selected over the mdb area to estimate statistical moments over different spatial resolutions images must indeed be square with a number of pixels equal to a power of two along each dimension section 3 because of different satellites projection grids and spatial resolutions selected sub images from different satellites do not cover exactly the same area and they do not completely match to the original mdb area fig 2 presents examples of sub images obtained for dispatch sm smos sm and modis ndvi whose size is respectively 1024 1024 64 64 and 1024 1024 pixels for readability during preprocessing all modis products were projected from sinusoidal to orthogonal lat lon coordinates sohrabinia 2012 considering the different grid spacing of the products and sub images size condition the sub image selected for smos sm covers the entire mdb 1600 1600 km² whereas the sub images selected for dispatch and modis products are smaller around 1000 1000 km² it is important to notice that while they have similar spatial resolution and a same number of pixels dispatch sm and modis images do not exactly correspond to the same ground area this is caused by slightly different grid spacing for the two products 1 km for dispatch and 0 93 km for modis solano et al 2010 wan 2006 for simplicity we ll consider in the following that both dispatch and modis products present a grid spacing of around 1 km table 1 summarizes the main characteristics of our preprocessed satellite dataset two important observations should be highlighted first considering their daily revisit time few dispatch sm and modis lst images are retained over the full june december period only 12 maps for dispatch and around 70 maps for modis lst this is directly related to the significant number of missing values that is in average 30 in these two types of products therefore missing values in downscaled sm seem to be mostly generated by those in lst products probably due to the presence of clouds in the data then another point concerns the different surface areas of the preprocessed products because they do not fully overlap smos and dispatch sub images may capture different sm dynamics extreme events occurring in northern mdb are observed in smos data whereas it may not be taken into account in dispatch data however we ensured that all products did have the widest area in common focusing on irrigated regions in the middle south part of the basin like murrumbidgee 5 results 5 1 spatial power spectra fig 3 shows the mean power spectra estimated over the full period june december 2010 of the different input and output products involved in dispatch it represents an average spectrum based on individual spectra obtained within the period each spectrum is plotted in log log coordinates with horizontal axis converted into space scale l k 1 expressed in kilometers considering smos sm and modis products the mean spectra are found to be scaling over the entire range of scales this is observed by a linear evolution of log e k eq 9 with coefficients of determination r² greater than 0 9 for each spectrum table 2 note that r² is used as a measure of the goodness of scaling estimated from the linear regression between log e k and log l however a different behavior is noticed for the disaggregated sm spectrum two scale ranges seem to appear with an increasing slope on scales lower than about ten kilometers a segmentation algorithm was applied on this spectrum d errico 2017 which confirmed a scale break at l 10 km according to the different values of spectral slopes obtained table 2 a three group classification was proposed β 1 smos sm modis vegetation index and disaggregated sm l 10 km for these three products the negative slope is found to be close to one though according to eq 10 this may reveal the conservative nature of the fields h 0 moreover these values are quite similar to the estimates proposed in literature lovejoy et al 2008 found β 1 2 for both vegetation and soil moisture indexes from modis products guadalajara central spain july 2006 previous studies on topography especially on volcanic surfaces laferrière and gaonac h 1999 found comparable results with quite low degree of fractional integration since topography can affect the spatial distribution of sm and vegetation kim and barros 2002b it is not surprising to observe similar scaling behavior between these fields 1 β 2 modis surface temperature from both aqua and terra satellites lst spectra have β values greater than 1 here surface temperature seems to correspond to a non conservative field h 0 these spectral slopes may be comparable to those obtained in literature on precipitation fields lovejoy and schertzer 2008 showing possible connections between the spatial distribution of surface temperature and that of rainfall and therefore with the underlying turbulent atmospheric dynamic schmitt et al 1993 β 2 disaggregated sm l 10 km on small scales dispatch sm spectrum presents a relatively large slope reflecting a high degree of fractional integration h 0 5 to our knowledge such high value of spectral exponent has never been observed in previous studies on sm fields however comparable scaling was obtained on sm time series revealing spectral slopes greater than 2 katul et al 2007 from these spectral observations a similar scaling seems to appear between the original smos sm and the disaggregated sm on scales greater than 10 km but this behavior is found to change for scales lower than about ten kilometers a comment may also be made on lst power spectra and their linear regressions although r² coefficients present good values on the entire range of scales 0 9 a scale break may be observed at about the same spatial scale found for dispatch spectrum l 10 km the scale break seems less pronounced but it could be related to that of dispatch this point will be discussed in section 6 2 this twofold scaling regime of dispatch sm can be also observed on each specific date of the study period with r² coefficients greater than 0 9 on almost all images and on both scale ranges fig 4 a shows the time series of the individual spectral exponents estimated for all products i e spectra computed for each image from june to december a significant difference of β values is observed between the two scale ranges of disaggregated sm for example on july 9 fig 4b power spectra are found to be similar as mean ones presented above fig 3 in particular the same scale break is still observed for disaggregated sm at about ten kilometers another remark concerns the amplitude of the scale break according to seasons fig 4a shows that for disaggregated sm the difference between the spectral exponents of small scales and large scales respectively blue triangle and blue star symbols is more important during the last three months of the period at small scales the spectral slope suffers a drastic change from around 1 9 jun jul aug sept to 2 3 oct nov dec i e spring and early summer in australia the amplitude of the scale break observed in dispatch sm could be related to the seasonal conditions of the study area this will be discussed in section 6 1 5 2 multifractal analysis the moments of the normalized absolute gradients were estimated at all accessible resolutions since divergence for q greater than qd 3 was reported in most of the literature hubert et al 2007 and because of sample size limitations in this study moments were computed for orders set from 0 to 3 in steps of 0 1 fig 5 shows the mean moments over the 7 month period plotted in log log coordinates as a function of the space scale l λ 1 for each product multifractal regimes are identified on specific scale ranges the power law described by eq 3 is well verified over these spatial scales corresponding to a linear variation of l o g m l q for all orders of moments ϕ λ q m l q this behavior means that a multifractal model is well adapted on the corresponding scale ranges considering vegetation and temperature modis products a scaling regime is found on scales greater than 8 km fig 5a c on these scales moments curves were fitted by linear regression red fit lines on fig 5 and the corresponding scaling functions k q were computed red yellow and green curves fig 5e um parameters were then estimated applying derivative free minimization method between empirical scaling function k q and the model form of k q described in eq 5 for the vegetation parameters values are found to be α 1 74 and c 1 0 03 table 2 they are quite close to those estimated by lovejoy et al 2008 on similar ndvi modis products α 2 and c 1 0 06 for aqua surface temperature we found the same parameter values as the vegetation ones α 1 7 and c 1 0 03 which is related to the very similar k q functions for all orders q slightly different parameters are found for terra products α 1 91 and c 1 0 04 this difference could be due to the different acquisition time of the two satellites 10 30 for terra and 13 30 for aqua this may have some effect on the multiscaling behavior of surface temperature another reason to this difference could be the larger scaling regime considered for terra a multifractal behavior is observed on scales ranging from 8 km to 1024 km against 8 km to 300 km for aqua and ndvi products anyway these results confirm ndvi and reveal lst not yet studied at this time the multifractal properties of the considered modis products in both cases they are characterized by a high degree of multifractality α is close to 2 value corresponding to the log normal case and by a low dispersion of the field c 1 0 1 smos sm products show good multifractal behavior too moments are found to be well fitted r² 0 99 cf table 2 on most of the aggregation scales apart from the 2 greatest scales 1600 km and 800 km scaling function was computed over spatial scales going from the 25 km observation scale to 400 km purple curve in k q graph fig 5 compared to modis products a growing divergence is noticed between smos and ndvi lst scaling functions especially for orders q greater than 1 this scaling behavior is confirmed by different um parameters α 1 46 and c 1 0 16 to our knowledge no application of the um model has already been made on remotely sensed sm from passive microwaves therefore it is difficult to compare these results with literature however although they didn t use the um model kim and barros 2002b studied spatial scaling properties of passive microwave sm estimated from airborne l band radiometer southern great plains experiment 1997 usa they observed a multifractal scaling on a similar scale range 1 6 km to 250 km which is coherent with our results lovejoy et al 2008 indeed applied the um model but on an optical sm index estimated from modis reflectances lampkin and yool 2004 they found α 2 and c 1 0 05 over lower spatial scales 0 5 km to 25 km these parameter values are quite different from ours the different scale range and the different study area guadalajara central spain in lovejoy et al 2008 between their work and ours could be a possible explanation to this result another reason might be linked to the nature of the signal studied optical estimated indexes like modis sm index are more sensitive to land cover such as vegetation fabre et al 2015 haubrock et al 2008 then polluting the scaling properties of sm focusing now on disaggregated sm products a change of slope is noticed for each of the statistical moments the same segmentation algorithm was applied on all moment curves revealing a scale break at about ten kilometers two multifractal scaling regimes may be observed here confirming the twofold scaling behavior found in the power spectra considering larger scales l 10 km red fit lines on fig 5 estimated um parameters are α 1 64 and c 1 0 03 they are close to the parameters found for our modis products ndvi and lst with a high degree of multifractality and a low dispersion of the field for smaller scales l 10 km green fit lines on fig 5 the degree of multifractality is almost unchanged α 1 59 compared to the large scales regime however the dispersion parameter is increased c 1 0 09 which is three times the value obtained on greater scales though the difference between the two multifractal scaling regimes seems to be mainly linked to the dispersion of sm through scales if we refer to the multifractal analysis of modis sm index made by lovejoy et al 2008 our estimates are coherent considering α for both ranges of scales and c 1 on large scales lovejoy et al 2008 didn t notice any scale break therefore it is difficult to comment our estimate of c 1 at small scales nevertheless kim and barros 2002b observed a similar scale break at about the same 10 km scale on passive microwave sm indeed they noticed two scaling regimes from variance spectra and moments graphs the twofold scaling behavior of dispatch sm products looks consistent with the scale break identified first by kim and barros 2002b 6 discussion 6 1 a physically explained twofold scaling behavior of soil moisture since sm variability is impacted by several environmental factors brocca et al 2007 crown et al 2012 the scale break observed on disaggregated sm could be the result of processes acting at different space scales at finer scales l 10 km spatial structure of sm is governed by infiltration or runoff which are mainly related to the soil properties texture structure hawley et al 1983 famiglietti et al 1998 on the other hand at larger scales l 10 km sm variability is more affected by evapotranspiration processes mohanty and skaggs 2001 or precipitation jackson et al 1999 a similar scale break at 10 km was also noted by kim and barros 2002b based on power spectra and statistical moments of sm estimated from airborne l band radiometer southern great plains experiment 1997 usa sm retrievals were obtained at 1 km nominal resolution from the ƭ ω model jackson and schmugge 1991 which depends on vegetation water content vwc estimates based on ndvi they observed that the relationship between the spatial structure of sm and landscape characteristics was strongly modulated by the wetness of the soil indeed they applied an eof analysis empirical orthogonal function between sm and auxiliary data which are topography vwc and soil content this revealed that sm was much correlated to topography during rain events whereas stronger correlation with vegetation water content was noticed during drier periods mainly governed by evapotranspiration processes these results are interesting since other research studies also observed similar scale break in the case of precipitation products obtained from radar at 1 km resolution southeastern france gires et al 2011 indeed a transition in spectra and moments was noticed at about twenty kilometers not far from our 10 km scale break however some limitations relative to radar data acquisition must be taken into account considering these results indeed constraints due to algorithmic processing change from polar to cartesian coordinates impact of missing data temporal integration and to physics attenuation by rainfall etc may impact the scaling properties of precipitation radar images moreover the z r relationship between radar reflectivity and rain rate marshall and palmer 1948 remains somehow controversial with a non robust parameterization from a multi scale point of view verrier et al 2013 thus in this context the scale break detected by gires et al 2011 may not be as relevant as it could be however they also analyzed the multifractal behavior of simulated precipitations generated on the same area at 2 km resolution from the meso nh atmospheric model lafore et al 1997 the analysis revealed the presence of a comparable scale break at about 30 km which tends to show that this transition scale in precipitation data is not an artifact since rainfall is an important forcing of sm it may be thought that a break in the rainfall spectra would affect the sm in a more significant way when the rain event is important moreover a theoretical model of sm in the time domain was proposed by katul et al 2007 to relate the scaling of precipitation to that of sm the spectral exponents of these two variables were found to be connected over time scales finer than 7 days through the simple equation β sm β p 2 with β sm and β p the negative spectral slopes of respectively soil moisture and precipitation time series despite these results were observed on time series it may corroborate the possible dependence between the sm variability and that of heavy rainfall even in the space domain considering seasonal variations the power spectra of disaggregated sm seem to reveal a pronounced twofold scaling behavior especially during spring and early summer october to december period since dispatch images are mainly located over the middle south part of the murray darling basin climate is then mostly temperate therefore the last months of the study period correspond to a drier landscape thus the two scaling regimes seem to be even more distinct when the soil is drier to demonstrate this effect the spatial mean of dispatch sm μ sm and the absolute difference β large β small were computed for each disaggregated image in fig 6 the normalized anomalies of these two variables are in line with this hypothesis blue and red circle symbols a more pronounced twofold scaling behavior seems to be found on the driest days oct nov dec kim and barros 2002b noticed a similar behavior with lower scaling differences during rain events observed on both spectra and moments of sm in certain dates they even noticed that there was no scale break at all corresponding to very high wetness conditions of the soil moreover we estimated the position of the scale breaks on each power spectrum during the period corresponding normalized anomalies plotted in gray star symbols fig 6 although it is positioned on average around 10 km not shown here but the mean value over the full period was estimated at 13 km the transition scale between the two scaling regimes seems to follow a decreasing trend as the soil is drying with estimated scale breaks ranging from 15 km in wet period to 12 km in dry period a comparable behavior was observed by kim and barros 2002b showing that the position and the amplitude of the scale break in the scaling behavior of sm is dependent on the state of sm and thus on the hydrometeorological conditions like rain evapotranspiration and infiltration processes to go further on the dependences between seasons and sm scaling kim and barros 2002b observed that multifractality was almost always involved on scales smaller than 10 km whatever the dryness of the soil however on scales greater that 10 km multifractality was found to become monofractality especially during drier conditions at large scales between 25 km and 400 km a comparable effect was noticed on our smos sm products purple circles fig 7 the multifractality index α is decreased from around 1 6 june to 1 3 december which may reflect a moderate decrease of multifractality during the study period therefore multifractal properties of sm at large scales seem to be related to the soil dryness this may give complementary explanations to the twofold scaling behavior of sm on the other hand considering dispatch sm a rather constant evolution of α is noticed on both small scales blue triangle symbols and large scales blue star symbols the first case confirms the idea that multifractality is not dryness dependent on smaller scales whereas the second is in contradiction with this assumption thus the latter should be considered cautiously to explain the scaling properties of sm 6 2 a model induced twofold scaling behavior of soil moisture in relatively recent works mascaro et al 2010 mascaro and vivoni 2012 scale invariance and multifractality were noticed from sm products measured from airborne l band radiometers southern great plains 1997 and 1999 experiments usa in these studies a log poisson multifractal model was applied she and levêque 1994 and a single scaling regime was observed on statistical moments from 0 8 km to 25 6 km scales although this result confirms the multifractal properties of sm on space scales similar to ours it refutes the existence of two scaling regimes no scale break was observed at about ten kilometers since the log poisson model is based on a similar universal theory as the um model continuous cascades it is somewhat unexpected not to detect the same transition on comparable sm products same technology and same scale range to investigate if this difference could be related to the case study different areas or periods we compared our dispatch products to fine scale airborne data acquired during the aaces 2 mission peischl et al 2012 this mission was performed in september 2010 during which transect flights were carried out over the murrumbidgee catchment brightness temperatures bt were acquired from l band radiometer on both h and v polarizations at a nominal 1 km spatial resolution the study area was divided in 5 patches of 50 100 km² each corresponding to a single flight day 13 16 19 21 22 september we gathered these patches into one single bt image and we selected a sub image of 128 128 km² to verify the presence of two scaling regimes in the data we applied spectral analysis on both h polarized and v polarized bt sub images in fig 8 the power spectrum of h polarized bt was compared to the power spectra of dispatch related products on equivalent period the spectra of each satellite product available between the 13 and 22 of september were averaged together since no dispatch sm products were pre selected on this period because of too many nan we chose the nearest available product which corresponds to 4 october in fig 8 one single linear fit is observed on bt power spectrum over the entire scale range from 1 km to 128 km and with a spectral slope equivalent to that of smos sm spectrum β 1 note that v polarized spectrum was not plotted here but it was found to be very similar to the h polarized one different scaling behaviors were noticed for aaces bt and dispatch sm on similar area and similar period this may sustain the idea that the scale break observed at 10 km could be caused by the dispatch model and specifically by the way in which the multi scale properties of each product are mixed in the algorithm to verify this hypothesis a simplified version of the c4dis processor was implemented in order to study the multi scale behavior of the different variables combined and generated through the algorithm to do this the method proposed by molero et al 2016 was followed which includes the two main steps described in section 4 2 1 the estimation of seehr variable soil evaporative efficiency from modis products eqs 13 15 and 2 the proper disaggregation process of sm from smos products seehr and smp parameter eqs 17 18 according to this method our algorithm was applied on smos and modis products acquired on november 19 2010 a sub area was selected 700 x 700 km² in order to have a smaller number of missing data and thus to get the minimum impact of gap filling on the studied products fig 9 shows the power spectra obtained from the input products of dispatch lst ndvi intermediate products ts see and output product sm mean the latter product is the average of the 6 disaggregated sm images obtained from the 6 sm lst combinations see section 4 1 here just one smos image of 25 km grid spacing was combined with the modis products indeed both cases with one smos image and four downsampled ones were implemented section 4 1 and molero et al 2016 and no significant differences were observed between the final products and between their power spectra therefore for simplicity of implementation only the case of one smos image was considered here for comparison the power spectrum of c4dis sm product acquired on the same date and on the same sub area was also plotted here the segmentation algorithm used in section 5 was applied on each power spectrum a geometric mean was estimated from the different scale breaks obtained revealing two averaged scale breaks which are nearly common to all spectra the first at almost ten kilometers l 9 km and the second at about thirty kilometers l 33 km to evaluate the link between the multi scale behavior of each product spectral exponents were estimated on the two following scale ranges from 33 km to 9 km large scales and from 9 km to 1 km small scales comparing our sm mean product with sm c4dis product table 3 a very similar scaling is observed on large scales β large 1 3 on small scales high spectral exponents are found with β small 2 for sm mean and β small 2 86 for sm c4dis these different spectral slopes on finer scales could be related to the non implementation of some filtering steps in our algorithm which are indeed coded in c4dis processor corrections of topography effects filtering lst data with low quality etc molero et al 2016 despite these small differences between sm mean and sm c4dis spectra the scale break remains noticeable as it seems to be on the other products of the algorithm to demonstrate this the absolute difference δβ β large β small was computed as an indicator of the amplitude of the scale break values greater than 0 6 were found for lst aqua and terra ts aqua and see aqua products these results seem to reveal that modis lst products would be the cause of the scale break located at about ten kilometers in the disaggregated sm product this scale break would propagate in the algorithm through the estimation of ts and see a possible explanation to this scale break in lst products may be related to the physical nature of the signal used indeed optical thermal sensors can be characterized by modified spectral slopes near the satellite resolution moreover this effect seems more important on aqua lst δβ 0 83 than on terra lst δβ 0 62 similar differences can be observed between the mean power spectra of aqua and terra lst over the full period fig 3 in section 5 1 thus it may be thought that the amplitude of the scale break could be related to the diurnal cycle of surface temperature since surface temperatures measured from aqua are acquired at the hottest hours of the day 13 30 there might be a correlation between the amplitude of scale break and the level of surface temperature considering the scale break observed at about 30 km this one may not be related to the multi scale properties of modis products but possibly to the combination of different products defined on different grid spacings indeed dispatch algorithm combines and creates products which have either the grid spacing of modis data 1 km or the grid spacing of smos data 25 km for example the estimation of see eq 15 combines end members t s max and t s min defined on the smos grid with another product ts defined on the modis grid as seen on fig 10 a footprint of smos pixels is then observable on the resulting image of see this property is due to the resampling strategy of smos data and to the end members that are defined on the smos grid this systematic footprint is visible in the real domain but can also have an impact in the fourier domain indeed sharp transitions at the smos pixels limits may create spurious convolutions by cardinal sine like functions which may affect the spectrum on the disaggregated sm this effect can generate an imperfect transition between the part of the spectrum related to smos sm l 25 km and the part related to modis products l 25 km regarding the behavior of see power spectra on scales greater than thirty kilometers fig 9 a lower spectral slope is observed β 0 5 comparing to that obtained on finer scales β 1 for 33 9 km this could be related to the oversampling of smos data generating harmonics on fine scales and therefore not including variability on large scales in this manner not only large scales but even fine scales could be affected by this effect the latter may also contribute in a way to the accentuation of the spectral drop observed at finer scales on the final disaggregated sm product 7 conclusion during the last century several studies were carried out to investigate the scaling properties of sm very diversified technologies were used to access and study the spatial structure of sm airborne microwaves products satellite optical indices etc moreover different approaches have been considered such as power spectra statistical moments fractal dimensions and even different types of cascade models log poisson universal multifractal and even no explicit parameterization in some cases in this study we analyzed the multifractal behavior of remotely sensed sm products over space scales ranging from the kilometric field scale to the continental scale universal multifractal model was applied for the first time on smos sm data giving access to large scale variabilities of sm over the australian landscape fractal and multifractal properties were observed which confirmed and completed some results reported in existing literature the relevant aspect of the present work may be the multi scale analysis of the outputs of the disaggregation algorithm dispatch merlin et al 2008a molero et al 2016 this deterministic algorithm improves the space resolution of smos sm products from 40 km to 1 km to do this it combines coarse scale smos sm with fine scale 1 km modis optical thermal data although several validation studies have been realized on this downscaling method malbéteau et al 2016 merlin et al 2012 2013 2015 molero et al 2016 none fully explored its statistical behavior over a continuum of space scales in this context we applied fractal and multifractal analysis on the different products involved in dispatch algorithm including disaggregated and original sm products and modis auxiliary data which are vegetation indices ndvi and surface temperatures lst input products of dispatch revealed relatively good scaling properties over the considered scale ranges indeed ndvi lst and original sm were characterized by a power law evolution of their power spectra and statistical moments meaning respectively fractality and multifractality however a specific scaling behavior was noticed for the output disaggregated sm two scaling regimes were obtained with a transition scale observed at about ten kilometers on both spectra and moments considering spectral analysis on large scales l 10 km disaggregated sm was found to have the same scaling as the original sm measured from satellite on finer scales l 10 km a different behavior was noticed with an increasing slope of the power spectrum similar scale break was detected on statistical moments showing that both spectral and multifractal properties of dispatch sm are characterized by this twofold scaling signature two possible arguments were given to explain the specific scaling of the disaggregated sm first a more physical interpretation may indicate that this twofold scaling behavior would be related to the real properties of sm as it was previously observed by kim and barros 2002b such scale break would be reflective of nonlinear hydrometeorological processes rainfall infiltration evapotranspiration acting at different space scales and modulated by terrain soils and vegetation distributions the spatial structure of sm may be more impacted by infiltration or runoff at the field scale whereas it would be mainly controlled by evapotranspiration or precipitation at the regional continental scale a more significant scaling transition was observed on the driest days early summer which may support the link between sm and external forcing agents such as precipitation a second explanation would be more algorithmic and directly related to the processing of the different products used and created within the algorithm the model used in dispatch would generate sm whose statistics are not properly distributed across scales this may occur at two levels in the algorithm first some modis products properties such as breaks in the scaling may be retrieved in the final dispatch products indeed a spectral drop at about the same ten kilometers scale was detected on lst power spectra although it is less pronounced than on disaggregated sm this scale break may be introduced by modis lst and amplified by the disaggregation model since one single scaling regime was noticed on brightness temperature bt products acquired in the l band over the same area and the same period these observations suggest that the unexpected scaling in modis products would be caused by the technology specific to optical thermal sensors then another impact of the algorithm on the multi scale properties of sm may be related to signal processing artifacts occurring with the combination of several products defined with different grid spacings this combination is required to permit conservativity between input and output sm products however from a signal processing point of view this could create systematic footprints on the final image i e visible smos pixels in the downscaled products and therefore affect the power spectrum convolutions by cardinal sine like functions at this point it is difficult to determine which of the physical or algorithmic factors would be at the origin of this twofold scaling behavior though a plausible hypothesis may be that both factors could affect the scaling of disaggregated sm indeed a scale break at about the smos sm resolution could be initially produced by combination artifacts which would be more or less amplified in the algorithm according to seasonal conditions resulting in moving the scale break during the period to finer scales further work need to be addressed to fully explain these results in particular to determine to what extent each of the two factors impacts the scaling of dispatch sm complementary auxiliary data should be compared to our products indeed an eof or comparable analysis made on dispatch sm and topography vegetation water content or soil content would provide relevant information about the connection between the spatial variability of these products and help with interpretation moreover it would be interesting to verify if precipitation products can be characterized by a similar scale break on equivalent space scales and over the same area murray darling basin however it must be considered that such a comparison might be complex to interpret since to our knowledge no theoretical model has been proposed yet to relate the spatial scaling properties of sm and that of rainfall as it was already done in the time domain by katul et al 2007 in the same way the comparison between dispatch sm and airborne bt is not that trivial because relatively complex operations are involved to get inverted sm from bt to illustrate this mascaro and vivoni 2012 noticed monofractality from bt data whereas multifractality was observed from the corresponding inverted sm data the scaling properties of bt could be affected during the inversion process explaining why the single scaling we observed on bt does not imply single scaling of dispatch sm therefore multifractal analysis of proper fine scale sm products may clarify this idea and help validating dispatch sm variability in the hypothesis of a model induced scale break current work is engaged to quantify the effect of modis products and the trace of pixel smos on different dates and on proper operational conditions analysis of products used and generated within the c4dis processor an application of dispatch using landat 7 auxiliary data instead of modis products was realized by merlin et al 2013 allowing a disaggregation process at sub kilometric scales 100 m since landsat 7 provides optical thermal data with higher resolution than modis it could be interesting to verify if both landsat 7 and the resulting disaggregated sm product would be characterized by a similar scale break but shifted on finer scales than the 10 km scale observed on modis therefore the results obtained could help to quantify the real impact of optical thermal auxiliary data on the multi scale properties of dispatch sm on a more operational point a view if this impact is confirmed the results obtained may help to define a specific scale below which the variability generated by the disaggregation model may not be as reliable as it should be concerning the impact of smos pixels footprint effects on the disaggregated product a solution could be to filter out the sharp transitions at smos pixels limits however this should be done with caution since such filters may excessively attenuate the variance at smaller scales another way to investigate our observations of dispatch sm is to focus on its dynamical behavior over different aggregation scales indeed one of the main problem in downscaling a dynamical behavior arises from the fact that the dynamical behavior of an aggregated signal can be approximated by the same deterministic equation structure only when the aggregated area is phase synchronized mangiarotti et al 2016 considering this issue the applicability of deterministic downscaling methods like dispatch may not be that obvious over certain spatial scales leading to several difficulties and perhaps contributing to explain the scaling irregularities observed in this study finally a possibility could be to compare the sm variability produced by dispatch with that created by fractal stochastic downscaling methods based on scaling properties these methods preserve the probability distribution from large to fine scales in precipitation several studies applied these algorithms on rainfall data rebora et al 2006 sharma et al 2007 research works proposed methods developed on multiplicative cascade such as log poisson deidda 2000 or um model gires et al 2012 revealing some potential to quantify uncertainty and representativeness errors between coarse scale and in situ measurements concerning sm downscaling some studies used such fractal based methods bindlish and barros 2002 kim and barros 2002a mascaro et al 2010 in our study it may be interesting to apply this kind of method on smos products this would consists in injecting in the um model the values of α and c 1 parameters obtained from smos products on large scales and then continuing the cascade at higher resolutions following this procedure the fine scale field will have the same scaling properties as the coarse scale one however since the disaggregation is based on random generator an ensemble of possible fields can be proposed from just one pair of α c 1 parameter therefore this kind of methodology may not be fully suitable in the case of operational hydro agricultural applications in particular when determining the position of the extremes to overcome this inconvenient a combination of the two approaches may be an interesting compromise between statistical scaling and evaporation based determinism for example in dispatch algorithm an idea might be to find a modified estimator of see that would be used in the disaggregation eq 18 this modified see would be computed by applying a 2d filter on the original see which would be actually equivalent to perform a fractional integration of order δh hrequested hnon filtered with hrequested and hnon filtered measured respectively from smos soil moisture at large scales and from non filtered see for scales under 10 km doing this the spectral slope of see may be adjusted like that of the final disaggregated soil moisture thus coarse scale and fine scale fields could be related through a common degree of fractional integration which may contribute to limit the twofold scaling behavior observed on the disaggregated product in practice this modification would not be easy to implement since the filtering should be properly dimensioned in order to affect only the small scales between 1 and 10 km moreover this texture based image correction may impact the physical properties of see so there would be a compromise to be made on this aspect acknowledgements we are grateful to c rüdiger and j walker from monash university melbourne australia for their advice and for giving us access to aaces data http www moisturemap monash edu au aaces we also want to thank the smos team particularly a mialon and a al bitar for fruitful discussions this study was supported by the région occitanie france and iut paul sabatier toulouse france smos products were acquired from the center aval de traitement des données smos catds which is the french ground segment developed by the center national d etudes spatiales cnes france in collaboration with ifremer brest france modis products were obtained from the land processes distributed active archive center lp daac operating as a partnership between the nasa and the u s geological survey usgs 
559,digital rock physics is a promising approach for achieving more cheaper and faster rock property characterization of digital images of rock samples to successfully deliver on this potential we must correctly interpret the digitally derived properties in the context of the limitations imposed by imaging constraints to this end we show that a combination of limited image resolution a biased segmentation of images with coarse resolution and a finite field of view of images generated by the present micro computed tomography micro ct technology leads to systematic underestimation of porosity down to a factor of 0 5 and overestimation of permeability up to a factor of 10 calculated using the digital rock physics drp we demonstrate these imaging limitations can be overcome by identifying good measures of image resolution and representative elementary volume and applying appropriate transforms these transforms expand the operating envelop of drp transforms for finite image resolution and limited field of view can be estimated directly from the micro ct images however implementation of transforms related to errors in image segmentation require either a higher resolution image e g nano computed tomography scanning electron microscopy or laboratory measured constraints mercury injection capillary pressure nmr porosity additionally we suggest how insights from these transforms can be used to define operating envelopes and optimize imaging resolution and field of view to achieve more reliable results from digital rock characterization and simulations keywords digital rock porosity permeability numerical simulations glossary of symbols symbol description ϕ i image porosity ϕ porosity inferred from an infinitely large image of infinite resolution ϕ t total porosity α r resolution correction for porosity α s segmentation correction for porosity α v field of view correction for porosity k i image permeability k permeability inferred from an infinitely large image of infinite resolution β r resolution correction for permeability β s segmentation correction for permeability β v field of view correction for permeability p mercury pressure in micp p d entry pressure in micp ϕ p fractional bulk volume occupied by mercury at pressure p p max maximum pressure that can be achieved in a micp simulation on a 3d image δ x image voxel size σ surface tension θ contact angle d d pore throat size dominant pore throat size corresponding to entry pressure that is estimated from laboratory measured micp curve p d m entry pressure inferred from a laboratory measured micp curve ϕ m expected image porosity estimated using laboratory measured micp n m ratio of dominant pore throat of size dd and image voxel size g m micp curvature parameter inferred from a laboratory measured micp curve α m resolution factor for porosity inferred from an image p d i entry pressure as inferred from an image p d inc uncorrected entry pressure for closure as inferred from an image ϕ i expected image porosity estimated using an image n i ratio of dominant pore throat of size and image voxel size inferred from an image g i micp curvature parameter as inferred from an image ω ratio of maximum and minimum diameters of a sinusoidal channel ω sin ratio of effective diameter and entry pore throat diameter d eff effective grain size or diameter n rev ratio of field of view and the effective grain size 1 introduction digital rock physics drp or simply digital rock dr is a rapidly advancing technology that relies on digital images of rocks to simulate multiphysics at the pore scale and predict properties of complex rocks e g porosity permeability compressibility moreover digital images of rocks allow for the quantification of any potential links between effective properties of rocks and rock texture shaped by geologic forces for the energy industry drp aims to achieve more cheaper and faster results as compared to conventional laboratory measurements andrä et al 2013a b arns et al 2005 2002 blunt et al 2013 dvorkin et al 2011 2008 kanckstedt et al 2001 2008 2009 øren et al 2006 saxena and mavko 2016 saxena et al 2017c a typical workflow of drp includes pore scale imaging image processing and segmentation numerical simulations and rock property analysis fig 1 in this paper we discuss a critical but often overlooked step which is the post processing and transformation of the computed results generated by the drp workflow to compensate for limitations in pore scale imaging porosity and absolute permeability are among the most fundamental rock properties and thus accurate estimation of porosity and permeability using images is crucial for maturing the digital rock technology we report a systematic offset between laboratory measurements of porosity and permeability and those computed using micro ct images for reservoir rocks this bias in image derived porosity and permeability trend cannot solely be explained by differences in numerical simulation engines or the choice of boundary conditions because these can account for up to 10 30 variation in computed permeability saxena et al 2017a this bias also cannot be accounted by differences in microstructures generated by various segmentation algorithms such differences are within 1 2 porosity units as long as all phases are segmented consistently saxena et al 2017b the objectives of this paper are multifold first we illustrate the discrepancy between image derived and laboratory measured properties using a considerable dataset second we investigate the reasons for the observed discrepancy between image derived and laboratory measured properties finally we propose models to transform or correct image derived porosity and permeability we apply the transforms on the dataset and observe significant improvements in reproducing the measured properties using digital computations the organization of this paper is as follows we begin the paper with a brief description of various microstructures and rocks considered in this study next we present expressions for various transforms that are required for micro ct computed properties and discuss the implications of these transforms we conclude with insights and recommendations for estimating complex reservoir properties from direct pore scale simulations 2 digital rock computations here we compare image derived and laboratory measured properties of various microstructures including 4 monodispersed grain packs 5 outcrop rocks that were previously used for several benchmark studies andrä et al 2013 saxena et al 2017a b and a global database of reservoir rocks further details are in appendix a all rocks contain negligible amounts of clay 2 laboratory measured porosity is the so called total porosity i e the ratio of the entire pore space in a rock to its bulk volume permeability was measured in the laboratory by pushing brine through the rock under laminar flow conditions for digital rock computations mini plugs were extracted from the original plugs used for laboratory measurements and were subsequently imaged with a state of the art micro ct detector at roughly the same image resolution voxel size 2 µm table 1 all three dimensional micro ct images were filtered with a proprietary non local means filter algorithm to boost signal to noise ratio upon filtering these images were segmented using spatial fuzzy c means clustering sfcm chuang et al 2006 algorithm into hydraulically conductive pores and non conductive minerals fig 2 the sfcm algorithm combines spatial statistics for each voxel with the fuzzy c means degree of membership to segment the image into its constituent phases volume fraction of pores estimated from the segmented image is referred here as the image porosity permeability was computed using a single phase lattice boltzmann method lbm solver further details on the numerical engine convergence criteria and boundary conditions can be found in appendix b for a more general introduction to the lbm we refer to benzi et al 1992 succi 2001 keehm et al 2001 saxena et al 2017a and alpak et al 2018 the computed permeability using this workflow is referred here as the image computed permeability a comparison between image computed versus laboratory measured porosity and permeability are shown in figs 3 5 we note a systematic offset when comparing laboratory measurements of porosity and permeability with those computed using micro ct images we find that digital rock analysis leads to systematic underestimation of porosity by down to a factor of 0 5 we note that image computations severely overestimate permeability of rocks by up to a factor of 10 for samples with laboratory measured permeability less than 100 mdarcy this observation is consistent with the recent study by saxena et al 2018 who predicted that due to imaging considerations the lower bound for simulating accurate permeability using micro ct images of 2 µm voxel size is around 100 mdarcy this lower bound drops to 25 mdarcy for images of 1 µm voxel size there are many key considerations when comparing laboratory data with digital rock results firstly the quality of digital rock simulations is determined by a variety of factors including the degree of resolved rock components the quality of image reconstruction and the signal to noise ratio in images secondly digital rock computations are impacted by every step of the workflow fig 1 including any biases in image segmentation that can lead to under or overestimation of porosity finally the two sets of results were obtained at different scales or field of view since laboratory measurements were performed on 40 mm cylindrical core plugs whereas digital rock computations were carried out on 4 mm size cubes extracted from the larger core plugs used for laboratory measurements this means that digital rock simulations were computed on 1 1000th of the rock volume used for laboratory measurements this substantial difference in the physical size of the analyzed sample creates a bias in the computed results even for perfectly homogenous samples with no geological layering within the plug these biases must be corrected before comparing image simulation results with laboratory measurements to this end we propose the following relations 1 ϕ ϕ i α r α s α v 2 k k i β r β s β v in eqs 1 and 2 ϕ and k denote porosity and permeability respectively subscript i denotes properties computed using micro ct images subscript denote the corresponding properties of a hypothetical image of infinite resolution and infinitely large field of view that has been segmented perfectly to estimate total porosity ϕ t excess pore volume within porous clays identified in images must be added to the corrected porosity ϕ parameters α and β are the correction factors and subscripts r s and v denote transforms for image resolution bias in estimating pore volume due to uncertainty in image segmentation and field of view or upscaling respectively there may be additional correction factors required for rocks whose properties are sensitivity to various other factors including differences in stress state swelling of clays electrochemical effects and severe heterogeneity to name but a few we ignore these additional transforms as these do not apply to the rocks analyzed in this study in the next section we will derive expressions for parameters α and β that depend on microstructural or geometric attributes that can be extracted directly using the original micro ct image 3 transforms for image porosity we begin with estimating the α parameters due to limited image resolution a portion of rock porosity is always unresolved this portion is significant when porosity is estimated using x ray computed tomography images of rocks recently saxena et al 2019 derived the following correction factors for image resolution 3 α r e g m l o g 10 n m the parameter α r describes how porosity contained within self similar pore structures in rocks can be predicted using an image even when these structures are below image resolution and therefore can be used to correct for the missing sub resolution porosity for further details on derivation of the result in eq 3 refer appendix c the parameter n denotes the ratio of pore throat size entry pore throat dd for mercury in a micp experiment and image voxel size δx and thus a better measure of the degree to which the entry pore throat is resolved compared to voxel size parameter g is the so called curvature of a micp curve thomeer 1960 parameter g is precisely 0 for pipes approximately 0 2 for sandstones and approximately 0 3 for carbonates subscript m denotes the laboratory measured parameter whereas subscript i denotes the value inferred from an image at infinite image resolution the two sets are the same we have found general good agreement between laboratory measured and image computed micp curves upon the required closure transforms thomeer 1960 an example is shown in fig 6 a for which gm gi 0 3 and nm ni 7 note that it is important that the comparison between laboratory measured and image computed micp curves is performed for fractional bulk volume of mercury i e mercury volume rock volume and not for saturation curves i e mercury volume pore volume fig 6b because raw image porosity is not equal to total porosity due to the resolution limit upon analysis of laboratory measured micp data and multiple micro ct images acquired at various resolutions of the grain packs and the outcrop rocks we find that gm gi and nm ni fig 7 thus entry pore throat size estimated using micro ct images is generally larger than that inferred from laboratory measurements this bias occurs due to over segmentation of pores in coarse resolution images especially when n 5 and thus can be addressed using a more robust segmentation approach from this analysis we propose the following empirical relation to calculate nm from ni fig 7 4 n m n i 2 e 0 4 n i 1 we now focus on image segmentation correction for porosity i e factor α s this correction depends on the quality of image segmentation which itself is sensitive to image resolution noise and segmentation method berg et al 2018 saxena et al 2017b to achieve perfect segmentation i e α s 1 it is necessary to correctly identify all types of rock pores as well as optimally partition pores and grains that are visible in an image this can only be estimated if additional constraints are available or if every section of a three dimensional image is manually checked for perfect segmentation the result in eq 3 can yield constraints on image porosity and segmentation if ϕ can be inferred using laboratory measured porosity e g micp nmr this is because we can predict the expected image porosity ϕ m using the following relation 5 ϕ m ϕ e g l o g 10 n m if correct segmentation is performed then ϕ i ϕ m still regardless of the availability of constraints for image segmentation the preferred protocol should be to improve the quality of image segmentation when possible rather than apply a large correction finally the field of view correction for porosity is expected be minimal i e α v 1 if large enough field of view is selected for computation of porosity various studies have shown that fluctuations in porosity values stabilize rapidly with increasing in sample volume andrä et al 2013 keehm 2003 mu et al 2016 saxena et al 2018 the necessary field of view to minimize the impact of this transform i e α v 1 is achieved if 5 grains are captured in each dimension i e 125 grains in a 3d volume however if α v 1 this would imply a very small field of view and therefore any further computations should be avoided 4 transforms for image permeability in this section we derive expressions for the β parameters that are needed to transform permeability estimated using a micro ct image the image resolution correction factor β r follows directly from the observation that if the image resolution is poor i e n 5 then micp simulations overestimate the entry pore throat size and that permeability depends on the square of pore throat size swanson 1981 thomeer 1983 5 β r n i n m 2 n i n i 2 e 0 4 n i 1 2 for cases where segmentation is inherently uncertain we propose a transform for image segmentation that is based on the observation that when a micro ct image is segmented with various segmentation algorithms fig 8 then the computed permeability values k exhibit the following dependency on segmented porosity ϕ saxena et al 2017b 6 k k 1 ϕ ϕ r e f 3 1 ϕ r e f 1 ϕ 2 in eq 6 subscript ref denotes properties corresponding to a reference segmentation thus δϕ bias in porosity due to incorrect image segmentation leads to the following bias in permeability 7 δ k 3 k r e f ϕ r e f δ ϕ for example consider a rock whose porosity is 20 porosity units p u i e percentage of total rock volume and permeability is 100 mdarcy if the bias in estimating porosity due to image segmentation is 3 p u then the bias in permeability will be 45 mdarcy therefore when pore volume is incorrectly estimated due to segmentation its impact on permeability can be compensated using the expression 8 β s ϕ i ϕ m 3 1 ϕ m 1 ϕ i 2 where ϕ m is the rock porosity containing only pores larger than the image voxel size it follows directly from eq 8 that the transforms related to image segmentation can only be applied if ϕ m or ϕ are known as noted in the case of the porosity transform if α s 1 or β s 1 the preference should be to re segment the image appropriately so that α s β s 1 correction for field of view or rock volume is within the purview of the topic of representative elementary volume rev bear 1975 hill 1963 torquato 1991 here we restrict the discussion to homogenous rocks that are completely devoid of fine scale laminations dual pore throat systems or other microstructural details that cannot be captured in a digital image of 4 mm various authors mu et al 2016 saxena et al 2018 have shown that permeability computed for concentric cubes of segmented images do not completely stabilize even when digital images are larger than 4 mm in one direction for even homogenous rocks e g fontainebleau therefore we must consider how the vast scale differences between image derived and laboratory measured permeability impact the comparison especially given that digital rock images have disproportionately fewer pore throats i e the narrowest constrictions in the largest pores compared to those present in 40 mm size plugs used for laboratory measurements the micp experiment is a good analog for a comparison fig 9 the pressure at which mercury begins to enter a digital rock in a numerical simulation is considerably lower compared to the entry pressure inferred from laboratory measurement of micp this is because mercury first enters a digital rock sample via outer pore bodies instead of pore throats because they can be accessed at much lower pressure compared to the pore throats compensating for the biases introduced by the outer pore bodies to flow properties is referred here as closure correction although this phenomenon also occurs in a physical micp measurement the volumetric contributions of the outer pore body is negligible compared to the total volume of mercury injected in the physical sample simply due to considerably larger sample size this relative contribution is larger for rocks with lower permeability fig 9 we define the ratio of outer pore body size to image voxel size as ninc for a numerical simulation on a digital image the apparent pore throat size neff must be between entry pore throat ni and pore body size ninc and thus we can put strict bounds on the correction factor 9 1 β v ω 2 where ω is the ratio of ninc and ni it is apparent that parameter β v must decline with larger field of view and this decline would be steeper for permeable rocks therefore simply assuming β v ω2 would lead to an overcorrection of image permeability for all rocks we now consider an alternative approach that approximates pores as sinusoidal channels with diameters ninc and ni and use empiricism to analyze how effective channel width changes with field of view and rock permeability using ω we now express parameter β v as 10 β v ω 2 p if the digital rock image has sufficiently large field of view to reproduce the fluid flow observed in a laboratory experiment then p 0 and β v 1 hence no field of view transforms are required however if flow is dominated by the outer pore body then the calculated permeability will not be representative and thus p 1 this situation might occur if there are too few grains in the digital rock such that fluid flow entering the rock pores is overestimated this effect could be significant if the rock has low permeability next we empirically establish the variation of coefficient p with parameters that can be extracted directly using the original micro ct image of a rock we back calculate the parameter p using the laboratory measured permeability after compensating for both resolution and segmentation transforms eqs 5 and 8 this analysis was repeated for grain packs and outcrop rocks of various sizes fig 10 we find that the following empirical relation describes the variation of parameter p 11 p 1 e a d d ϕ n r e v 0 5 2 where a 3 in eq 11 nrev denotes the ratio of field of view and the effective grain size deff as defined by saxena et al 2018 such that it approximates the grains across the flow direction the expression in eq 11 is not unique and is merely a good fit the magnitude of β v increases with the ratio of ninc and ni decreasing pore throat size decreasing porosity decreasing number of grains across the flow direction the application of permeability transform due to biases in image resolution image segmentation and field of view should be kept at a minimum and the preferred protocol is to compensate for these biases by improving the image when possible rather than apply a correction to the computed permeability for instance the need for any resolution and segmentation transform can be minimized by re imaging the rock at a higher resolution which can also improve image quality for image segmentation similarly it is possible to compensate for closure correction effects using appropriate boundary conditions for flow around the outer pore bodies of the digital image so that the field of view correction can be minimized further detailed investigation is still required to minimize the need for transforms 5 application for reservoir rocks we now apply the transforms to estimate porosity and permeability for the database of reservoir rocks these rocks were not used for any calibration to derive the expressions or fit parameters for the transforms we also include results from the recent digital rock study by chhatre et al 2018 values for image resolution transforms α r β r were estimated directly using the segmented micro ct images the comparison between laboratory measured porosity and permeability and those computed using digital rock improves upon application of the image derived resolution transform figs 11 and 12 this comparison further improves when field of view transforms and image segmentation transforms are applied figs 13 and 14 the trend between porosity and permeability after transforms for resolution and field of view compares well with those measured in the laboratory fig 15 this comparison improves further when laboratory measured porosity is used to compensate for the segmentation bias in porosity fig 16 for rocks with low permeability values 5 mdarcy even upon corrections for image resolution and field of view we still note relatively large disagreement between image derived and laboratory measured permeability there can be several reasons for this discrepancy including larger uncertainty in segmenting poorly resolved images with relatively small pore throats inability of the resolution and field of view transforms to capture the required microstructural details and of course relatively large uncertainty in measuring low permeability in laboratory 6 imaging protocols to minimize the need for transforms the transforms discussed in this paper should be applied prior to comparing image computed properties with those measured in the laboratory still prior to any calculations the rock samples should be imaged at sufficiently high image resolution and large field of view to minimize the magnitude of any corrections unfortunately the magnitude of corrections due to limited image resolution can only be minimized at the expense of corrections required for limited field of view this is because current micro ct detectors can only capture a limited number of voxels and can image small features at higher resolution or large features at coarser resolution this tradeoff between image resolution and field of view is insignificant for porosity as rev requirements can be generally met even for a very small field of view saxena et al 2018 therefore to estimate porosity from images it is advisable to image rocks at the highest available image resolution as per eq 3 so that the magnitude of correction can be minimized however for permeability the tradeoff between image resolution and field of view can be significant saxena et al 2018 still an optimal combination of the imaging parameters exists for which the combined magnitude of transforms is minimized we now illustrate that using rough estimates of expected rock properties it is possible to establish an optimal combination of imaging parameters i e field of view and voxel size we consider a case where the micro ct detector can capture up to 20003 voxels i e m 2000 we consider four rocks a high permeability sample k 1000 mdarcy ϕ 0 3 gm 0 1 an intermediate permeability sample k 100 mdarcy ϕ 0 2 gm 0 2 a low permeability sample k 10 mdarcy ϕ 0 15 gm 0 3 and a very low permeability sample k 1 mdarcy ϕ 0 15 gm 0 3 to calculate the magnitude of the transforms for permeability for the four rocks we need to calculate factors β r and β v for the combinations of voxel size δx assumed to vary between 0 1 10 µm and field of view m δx factors β r and β v depend on parameters nm gm nrev and the ratio ninc ni these parameters can be estimated since for the four rocks we know porosity and permeability here we assume ninc ni ω 5 the parameter nm can be estimated using pore throat size dd for an image of voxel size δx using nm dd δx pore throat size can be empirically related to porosity and permeability using thomeer s permeability model thomeer 1983 12 k 38068 g m 4 3 ϕ d d 213 2 in eq 12 k is laboratory measured permeability in mdarcy dd is entry pore throat size in µm and ϕ is laboratory measured porosity in fractions using eq 12 we can estimate dd for a given combination of porosity ϕ and permeability k the parameter nrev can be calculated using nrev m δx deff we can infer deff using the following empirical relation between grain size and pore throat size suggested by saxena et al 2018 13 d e f f 0 5 ϕ 2 d d hence using eqs 12 and 13 we can estimate β r and β v for the four rocks as a function of voxel size δx and field of view m δx as expected we find that the magnitude of the required corrections for high permeability rock are not significant 10 and the optimal voxel size for imaging is 2 7 µm which results in a maximum field of view of 5 4 mm in each dimension fig 17 the intermediate permeability rock can be imaged at roughly the same voxel size as the higher permeability rock but the magnitude of corrections is greater 40 the magnitude of corrections is significantly larger for the low permeability rock factor of 2 moreover the low permeability rock should be imaged at higher resolution i e voxel size 2 µm interestingly the very low permeability rock should be imaged at a very high resolution voxel size 1 µm to minimize already very large corrections fig 17 one way to effectively reduce the magnitude of corrections is to improve micro ct technology so that a larger number of voxels can be captured so that images of higher resolution can be obtained for large enough field of view an example is shown in fig 18 which presents the same information as in fig 17 but for a micro ct detector that can capture up to 50003 voxels i e m 5000 this analysis can be performed to infer optimal imaging parameters for a given reservoir and maximize quantitative information for further modeling 7 conclusions modern imaging can identify features that are larger than the image voxel size e g µm for x ray computer tomography nm for scanning electron microscopy a portion of rock porosity is always unresolved due to finite image resolution regardless of the imaging technique this portion however is significant when porosity is estimated using x ray computed tomography micro ct images of rocks therefore micro ct imaging is not a suitable technique for estimating total porosity as a minimum image derived porosity must be corrected for limited image resolution before it can be compared with laboratory measured total porosity we use concepts of capillary physics in rocks to quantify the impact of image resolution on image derived porosity and develop novel transforms to derive the corrected porosity that compensate for the limited image resolution without the need for higher resolution imaging that is only possible at the expense of image field of view or physical laboratory measurement furthermore the sub resolution pore volume predicted by our method can also be used to correct the fluid saturation inferred from multiphase flow simulations on segmented micro ct images for the missing pore volume image resolution also impacts permeability coarser image resolution leads to an artificial increase in pore throat size in segmented images and thus permeability is overestimated we find that finite sample size or field of view of digital rocks also leads to a systematic and in some cases drastic overestimation in permeability when compared to laboratory permeability which are performed on samples with significantly larger field of view compared to numerical simulations we quantify this effect and suggest relevant transforms applicable to relatively homogenous rocks we also suggest a transform that can account for the impact of incorrect image segmentation on permeability if a laboratory measured constraint on porosity is available we conclude that these transforms are critically important for estimating meaningful properties of conventional sandstone and carbonate reservoir rocks and can allow us to further mature digital rock as a technology for existing and future fields still whenever possible the application of transforms should be kept at a minimum and the preferred protocol is to compensate for these biases by improving the image when possible rather than apply a correction to the computed property additionally understanding the sensitivities of the transforms allows us to balance the tradeoff between imaging resolution and field of view to achieve more accurate properties using digital rock characterization and simulations acknowledgements we thank l taras bryndzia saad saleh dmitry shaporov chaitanya pradhan majeed shaikh kunj tandon umang agarwal steffen berg stefan hertel marisa rydzy and ove bjorn wilson for discussions supplementary material supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2019 103419 appendix d supplementary materials image application 1 appendix a description of grain packs and rocks each grain pack is composed of nearly spherical grains of glass we refer to these packs as gp1 gp2 gp3 and gp4 listed in increasing order of sphere diameter for reference solution of grain pack permeability we use thomeer s permeability relation thomeer 1983 other reference solutions for sphere packs lead to similar permeability values e g hinch 1977 howells 1974 rumpf and gupte 1971 torquato 2002 the five outcrop rocks selected for this study span a range of grain sizes from 0 05 to 0 5 mm and range in porosity from 0 05 to 0 25 table 1 these samples were selected because they cover a range of compositions and textures that may be encountered rocks b1 and b5 are from the berea formation churcher et al 1991 which is a sub angular to sub rounded mississippian age sandstone rock f is from the fontainebleau formation which is a sub rounded to rounded oligocene age sandstone this sample was well cemented with low porosity rocks g1 and g2 are from the castlegate formation of utah which is a sub angular to sub rounded mesozoic sandstone xrd analyses indicates that the castlegate sandstone sample consists mainly of quartz grains with trace amounts of carbonate cement the reservoir rocks samples a s j c and d come from various rock formations around the world and are of different geologic origin and ages table 1 the texture and microstructure of these reservoir rocks is considerably more complicated when compared to grain packs or the outcrop rock samples we also include three samples from chhatre et al 2018 include an austin chalk sample ac indiana limestone sample il and scioto sandstone sample t we did not have any images for these samples but used the reported image porosity laboratory measured porosity and micp simulation results reported in chhatre et al 2018 appendix b the lbm solver and boundary conditions the flow simulations were performed using an implementation of the lattice boltzmann method lbm the lbm implementation is based on the d3q19 approach and the multiple relaxation time mrt technique that capture complex flow physics the mrt method is described in detail all flow computations were performed using the periodic boundary conditions for the main flow direction and the directions normal to the flow the computations were run on a large linux based cpu gpu hpc cluster appendix c impact of image voxel size on porosity we derive the relation between image voxel size and image derived porosity using the concept of capillary pressure saxena et al 2019 mercury injection capillary pressure micp curves that are routinely measured in laboratory describe the relation between the pressure applied on the mercury to enter pore throats to overcome capillary pressure and the fractional bulk volume i e volume of invaded mercury divided by rock volume that is occupied by mercury at that pressure various empirical models brooks and corey 1964 leverett 1941 thomeer 1960 can be used to fit the measured micp data these models can be expressed in the following general form c 1 ϕ p ϕ e g m l o g 10 p p d in eq c 1 ϕ p is the fractional bulk volume occupied by mercury at pressure p and ϕ is the fractional bulk volume occupied at infinite pressure parameter pd is the pressure required to enter the dominant pore throats and is also sometimes referred as the entry pressure parameter gm describes the shape of the transition zone in a micp curve thus providing a measure of rock complexity and distribution of pore throat size parameters pd and gm can be directly inferred from a laboratory measured micp curve swanson 1981 thomeer 1983 1960 alternatively these parameters can also be inferred using image based simulations hilpert and miller 2001 micp parameter ϕ is usually slightly smaller than the total rock porosity that is measured using helium gas this is because helium can invade secondary pores which remain inaccessible by mercury highest pressure achieved in a typical laboratory is around 100 000 psi pore throat size d penetrated at a given pressure p is given by the young laplace equation p 4σcosθ d where σ is mercury air surface tension 480 dyne cm and θ is the contact angle 140 if p is expressed in psi and d is expressed µm then 4σcosθ 213 2 using eq c 1 we now derive the relation between image voxel size and image derived porosity the highest pressure that can be achieved when simulating the micp curve using an image of voxel size δx is given by young laplace equation c 2 p max 4 σ cos θ δ x substituting eq c 2 in eq c 1 and rewriting eq c 1 in terms of the entry or dominant pore throat size dd 4σcosθ pd we get a relation between ϕ and the expected image porosity ϕ m c 3 ϕ m ϕ e g m l o g 10 d d δ x here we refer to a quantity x with subscript m we are referring to the ground truth of the quantity x with no associated errors due to image acquisition or image processing we refer to image derived properties with subscript i ideally xm and xi should be equal if correct image processing segmentation is followed this is seldom the case and hence the distinction we now define a dimensionless parameter that describes pore throat resolution c 4 n m d d δ x 4 σ cos θ δ x p d nm corresponds to the number of voxels available to resolve the dominant pore throat of size dd for the remainder of the paper we will refer to nm as the pore throat resolution parameter as it depends on the image resolution as well as the size and the shape of the dominant pore throat using eqs c 3 and c 4 we get c 5 ϕ m ϕ e g m l o g 10 n m transforms in eq c 5 apply if one can infer the curvature parameter gm using the simulated micp curve this should be possible if the pressure in a micp curve at the inflection point pa is lower than p max which is guaranteed if c 6 n m 10 g m l o g 10 e eq c 6 describes the working envelope of image derived transport properties alpak et al 2018 f o alpak f gray n saxena j dietderich r hofmann s berg a distributed parallel multiple relaxation time lattice boltzmann method on general purpose graphics processing units for the rapid and scalable computation of absolute permeability from high resolution 3d micro ct images comput geosci 2018 10 1007 s10596 018 9727 7 alpak f o gray f saxena n dietderich j hofmann r berg s 2018 a distributed parallel multiple relaxation time lattice boltzmann method on general purpose graphics processing units for the rapid and scalable computation of absolute permeability from high resolution 3d micro ct images comput geosci 10 1007 s10596 018 9727 7 andrä et al 2013 h andrä n combaret j dvorkin e glatt j han m kabel y keehm f krzikalla m lee c madonna m marsh t mukerji e h saenger r sain n saxena s ricker a wiegmann x zhan digital rock physics benchmarks part i imaging and segmentation comput geosci 50 2013 10 1016 j cageo 2012 09 005 andrä h combaret n dvorkin j glatt e han j kabel m keehm y krzikalla f lee m madonna c marsh m mukerji t saenger e h sain r saxena n ricker s wiegmann a zhan x 2013 digital rock physics benchmarks part i iimaging and segmentation comput geosci 50 10 1016 j cageo 2012 09 005 andrä et al 2013a heiko andrä n combaret j dvorkin e glatt j han m kabel y keehm f krzikalla m lee c madonna m marsh t mukerji e h saenger r sain n saxena s ricker a wiegmann x zhan digital rock physics benchmarks part ii computing effective properties comput geosci 50 2013 33 43 10 1016 j cageo 2012 09 008 andrä heiko combaret n dvorkin j glatt e han j kabel m keehm y krzikalla f lee m madonna c marsh m mukerji t saenger e h sain r saxena n ricker s wiegmann a zhan x 2013a digital rock physics benchmarks part ii ccomputing effective properties comput geosci 50 33 43 10 1016 j cageo 2012 09 008 andrä et al 2013b heiko andrä n combaret j dvorkin e glatt j han m kabel y keehm f krzikalla m lee c madonna m marsh t mukerji e h saenger r sain n saxena s ricker a wiegmann x zhan digital rock physics benchmarks part i imaging and segmentation comput geosci 50 2013 25 32 10 1016 j cageo 2012 09 005 andrä heiko combaret n dvorkin j glatt e han j kabel m keehm y krzikalla f lee m madonna c marsh m mukerji t saenger e h sain r saxena n ricker s wiegmann a zhan x 2013b digital rock physics benchmarks part i iimaging and segmentation comput geosci 50 25 32 10 1016 j cageo 2012 09 005 arns et al 2005 c h arns f bauget a limaye a sakellariou t senden a sheppard r m sok v pinczewski s bakke l i berge p oren m knackstedt pore scale characterization of carbonates using x ray microtomography spe j 10 2005 26 29 10 2118 90368 pa arns c h bauget f limaye a sakellariou a senden t sheppard a sok r m pinczewski v bakke s berge l i oren p knackstedt m 2005 pore scale characterization of carbonates using x ray microtomography spe j 10 26 29 10 2118 90368 pa arns et al 2002 c h arns m a knackstedt w v pinczewski e j garboczi computation of linear elastic properties from microtomographic images methodology and agreement between theory and experiment geophysics 67 2002 1396 10 1190 1 1512785 arns c h knackstedt m a pinczewski w v garboczi e j 2002 computation of linear elastic properties from microtomographic images mmethodology and agreement between theory and experiment geophysics67 1396 10 1190 1 1512785 bear 1975 j bear dynamics of fluids in porous media soil sci 1975 10 1097 00010694 197508000 00022 bear j 1975 dynamics of fluids in porous media soil sci 10 1097 00010694 197508000 00022 benzi et al 1992 r benzi s succi m vergassola the lattice boltzmann equation theory and applications phys rep 1992 10 1016 0370 1573 92 90090 m benzi r succi s vergassola m 1992 the lattice boltzmann equation theory and applications phys rep 10 1016 0370 1573 92 90090 m berg et al 2018 s berg n saxena m shaik c pradhan generation of ground truth images to validate micro ct image processing pipelines lead edge 37 2018 412 420 10 1190 tle37060412 1 berg s saxena n shaik m pradhan c 2018 generation of ground truth images to validate micro ct image processing pipelines lead edge37 412 420 10 1190 tle37060412 1 blunt et al 2013 m j blunt b bijeljic h dong o gharbi s iglauer p mostaghimi a paluszny c pentland pore scale imaging and modelling adv water resour 51 2013 197 216 10 1016 j advwatres 2012 03 003 blunt m j bijeljic b dong h gharbi o iglauer s mostaghimi p paluszny a pentland c 2013 pore scale imaging and modelling adv water resour 51 197 216 10 1016 j advwatres 2012 03 003 brooks and corey 1964 r brooks a corey hydraulic properties of porous media hydrol pap color state univ 3 1964 37 pp 10 13031 2013 40684 brooks r corey a 1964 hydraulic properties of porous media hydrol pap color state univ 3 37 pp 10 13031 2013 40684 chhatre et al 2018 chhatre s s sahoo h leonardi s vidal k rainey j braun e m patel p 2018 a blind study of four digital rock physics vendor laboratories on porosity absolute permeability and primary drainage capillary pressure data on tight outcrops chuang et al 2006 k s chuang h l tzeng s chen j wu t j chen fuzzy c means clustering with spatial information for image segmentation comput med imaging graph 30 2006 9 15 10 1016 j compmedimag 2005 10 001 chuang k s tzeng h l chen s wu j chen t j 2006 fuzzy c means clustering with spatial information for image segmentation comput med imaging graph 30 9 15 10 1016 j compmedimag 2005 10 001 churcher et al 1991 churcher p l french p r shaw j c schramm l l 1991 rock properties of berea sandstone baker dolomite and indiana limestone in spe international symposium on oilfield chemistry pp 431 466 https doi org 10 2118 21044 ms dvorkin et al 2008 j dvorkin m armbruster c baldwin q fang n derzhi c gomez b nur a nur y mu the future of rock physics computational methods vs lab testing first break 26 2008 63 68 dvorkin j armbruster m baldwin c fang q derzhi n gomez c nur b nur a mu y 2008 the future of rock physics ccomputational methods vs lab testing first break26 63 68 dvorkin et al 2011 j dvorkin n derzhi e diaz q fang relevance of computational rock physics geophysics 76 2011 e141 e153 10 1190 geo2010 0352 1 dvorkin j derzhi n diaz e fang q 2011 relevance of computational rock physics geophysics76 e141 e153 10 1190 geo2010 0352 1 hill 1963 r hill elastic properties of reinforced solids some theoretical principles j mech phys solids 11 1963 357 372 10 1016 0022 5096 63 90036 x hill r 1963 elastic properties of reinforced solids ssome theoretical principles j mech phys solids11 357 372 10 1016 0022 5096 63 90036 x hilpert and miller 2001 m hilpert c t miller pore morphology based simulation of drainage in totally wetting porous media adv water resour 24 2001 243 255 10 1016 s0309 1708 00 00056 7 hilpert m miller c t 2001 pore morphology based simulation of drainage in totally wetting porous media adv water resour 24 243 255 10 1016 s0309 1708 00 00056 7 hinch 1977 e j hinch an averaged equation approach to particle interactions in a fluid suspension j fluid mech 83 1977 695 720 10 1017 s0022112077001414 hinch e j 1977 an averaged equation approach to particle interactions in a fluid suspension j fluid mech 83 695 720 10 1017 s0022112077001414 howells 1974 i d howells drag due to the motion of a newtonian fluid through a sparse random array of small fixed rigid objects j fluid mech 64 1974 449 476 10 1017 s0022112074002503 howells i d 1974 drag due to the motion of a newtonian fluid through a sparse random array of small fixed rigid objects j fluid mech 64 449 476 10 1017 s0022112074002503 kanckstedt et al 2001 m a kanckstedt a p sheppard m sahimi pore network modelling of two phase flow in porous rock the effect of correlated heterogeneity adv water resour 24 2001 257 277 10 1016 s0309 1708 00 00057 9 kanckstedt m a sheppard a p sahimi m 2001 pore network modelling of two phase flow in porous rock tthe effect of correlated heterogeneity adv water resour 24 257 277 10 1016 s0309 1708 00 00057 9 keehm 2003 y keehm computational rock physics transport properties in porous media and applications 2003 stanford univ keehm y 2003 computational rock physics transport properties in porous media and applications stanford univ keehm et al 2001 y keehm t mukerji a nur computational rock physics at the pore scale transport properties and diagenesis in realistic pore geometries lead edge 2001 10 1190 1 1438904 keehm y mukerji t nur a 2001 computational rock physics at the pore scale ttransport properties and diagenesis in realistic pore geometries lead edge 10 1190 1 1438904 knackstedt et al 2008 m a knackstedt c arns m madadi a p sheppard s latham r sok g bächle g eberli elastic and flow properties of carbonate core derived from 3d x ray ct images seg tech progr expand abstr 27 2008 1804 1809 10 1190 1 3059394 2008 knackstedt m a arns c madadi m sheppard a p latham s sok r bächle g eberli g 2008 elastic and flow properties of carbonate core derived from 3d x ray ct images seg tech progr expand abstr 200827 1804 1809 10 1190 1 3059394 knackstedt et al 2009 mark knackstedt s latham m madadi a sheppard t varslot c arns digital rock physics 3d imaging of core material and correlations to acoustic and flow properties lead edge 28 2009 28 33 10 1190 1 3064143 knackstedt mark latham s madadi m sheppard a varslot t arns c 2009 digital rock physics 3d imaging of core material and correlations to acoustic and flow properties lead edge28 28 33 10 1190 1 3064143 leverett 1941 m c leverett capillary behavior in porous solids trans aime 142 1941 152 169 10 2118 941152 g leverett m c 1941 capillary behavior in porous solids trans aime142 152 169 10 2118 941152 g mu et al 2016 y mu r sungkorn j toelke identifying the representative flow unit for capillary dominated two phase flow in porous media using morphology based pore scale modeling adv water resour 95 2016 16 28 10 1016 j advwatres 2016 02 004 mu y sungkorn r toelke j 2016 identifying the representative flow unit for capillary dominated two phase flow in porous media using morphology based pore scale modeling adv water resour 95 16 28 10 1016 j advwatres 2016 02 004 øren et al 2006 p øren s bakke h rueslåtten digital core laboratory rock and flow properties derived from computer generated rocks int symp soc core anal 2006 1 12 øren p bakke s rueslåtten h 2006 digital core laboratory rrock and flow properties derived from computer generated rocks int symp soc core anal 1 12 rumpf and gupte 1971 h c h rumpf a r gupte einflüsse der porosität und korngrößenverteilung im widerstandsgesetz der porenströmung chemie ing tech cit 43 1971 367 375 10 1002 cite 330430610 rumpf h c h gupte a r 1971 einflüsse der porosität und korngrößenverteilung im widerstandsgesetz der porenströmung chemie ing tech cit43 367 375 10 1002 cite 330430610 saxena et al 2017a n saxena r hofmann f o alpak s berg j dietderich u agarwal k tandon s hunter j freeman o wilson and benchmarks for pore scale flow simulated using micro ct images of porous media and digital rocks adv water resour 109 2017 211 235 10 1016 j advwatres 2017 09 007 saxena n hofmann r alpak f o berg s dietderich j agarwal u tandon k hunter s freeman j wilson o 2017a 
559,digital rock physics is a promising approach for achieving more cheaper and faster rock property characterization of digital images of rock samples to successfully deliver on this potential we must correctly interpret the digitally derived properties in the context of the limitations imposed by imaging constraints to this end we show that a combination of limited image resolution a biased segmentation of images with coarse resolution and a finite field of view of images generated by the present micro computed tomography micro ct technology leads to systematic underestimation of porosity down to a factor of 0 5 and overestimation of permeability up to a factor of 10 calculated using the digital rock physics drp we demonstrate these imaging limitations can be overcome by identifying good measures of image resolution and representative elementary volume and applying appropriate transforms these transforms expand the operating envelop of drp transforms for finite image resolution and limited field of view can be estimated directly from the micro ct images however implementation of transforms related to errors in image segmentation require either a higher resolution image e g nano computed tomography scanning electron microscopy or laboratory measured constraints mercury injection capillary pressure nmr porosity additionally we suggest how insights from these transforms can be used to define operating envelopes and optimize imaging resolution and field of view to achieve more reliable results from digital rock characterization and simulations keywords digital rock porosity permeability numerical simulations glossary of symbols symbol description ϕ i image porosity ϕ porosity inferred from an infinitely large image of infinite resolution ϕ t total porosity α r resolution correction for porosity α s segmentation correction for porosity α v field of view correction for porosity k i image permeability k permeability inferred from an infinitely large image of infinite resolution β r resolution correction for permeability β s segmentation correction for permeability β v field of view correction for permeability p mercury pressure in micp p d entry pressure in micp ϕ p fractional bulk volume occupied by mercury at pressure p p max maximum pressure that can be achieved in a micp simulation on a 3d image δ x image voxel size σ surface tension θ contact angle d d pore throat size dominant pore throat size corresponding to entry pressure that is estimated from laboratory measured micp curve p d m entry pressure inferred from a laboratory measured micp curve ϕ m expected image porosity estimated using laboratory measured micp n m ratio of dominant pore throat of size dd and image voxel size g m micp curvature parameter inferred from a laboratory measured micp curve α m resolution factor for porosity inferred from an image p d i entry pressure as inferred from an image p d inc uncorrected entry pressure for closure as inferred from an image ϕ i expected image porosity estimated using an image n i ratio of dominant pore throat of size and image voxel size inferred from an image g i micp curvature parameter as inferred from an image ω ratio of maximum and minimum diameters of a sinusoidal channel ω sin ratio of effective diameter and entry pore throat diameter d eff effective grain size or diameter n rev ratio of field of view and the effective grain size 1 introduction digital rock physics drp or simply digital rock dr is a rapidly advancing technology that relies on digital images of rocks to simulate multiphysics at the pore scale and predict properties of complex rocks e g porosity permeability compressibility moreover digital images of rocks allow for the quantification of any potential links between effective properties of rocks and rock texture shaped by geologic forces for the energy industry drp aims to achieve more cheaper and faster results as compared to conventional laboratory measurements andrä et al 2013a b arns et al 2005 2002 blunt et al 2013 dvorkin et al 2011 2008 kanckstedt et al 2001 2008 2009 øren et al 2006 saxena and mavko 2016 saxena et al 2017c a typical workflow of drp includes pore scale imaging image processing and segmentation numerical simulations and rock property analysis fig 1 in this paper we discuss a critical but often overlooked step which is the post processing and transformation of the computed results generated by the drp workflow to compensate for limitations in pore scale imaging porosity and absolute permeability are among the most fundamental rock properties and thus accurate estimation of porosity and permeability using images is crucial for maturing the digital rock technology we report a systematic offset between laboratory measurements of porosity and permeability and those computed using micro ct images for reservoir rocks this bias in image derived porosity and permeability trend cannot solely be explained by differences in numerical simulation engines or the choice of boundary conditions because these can account for up to 10 30 variation in computed permeability saxena et al 2017a this bias also cannot be accounted by differences in microstructures generated by various segmentation algorithms such differences are within 1 2 porosity units as long as all phases are segmented consistently saxena et al 2017b the objectives of this paper are multifold first we illustrate the discrepancy between image derived and laboratory measured properties using a considerable dataset second we investigate the reasons for the observed discrepancy between image derived and laboratory measured properties finally we propose models to transform or correct image derived porosity and permeability we apply the transforms on the dataset and observe significant improvements in reproducing the measured properties using digital computations the organization of this paper is as follows we begin the paper with a brief description of various microstructures and rocks considered in this study next we present expressions for various transforms that are required for micro ct computed properties and discuss the implications of these transforms we conclude with insights and recommendations for estimating complex reservoir properties from direct pore scale simulations 2 digital rock computations here we compare image derived and laboratory measured properties of various microstructures including 4 monodispersed grain packs 5 outcrop rocks that were previously used for several benchmark studies andrä et al 2013 saxena et al 2017a b and a global database of reservoir rocks further details are in appendix a all rocks contain negligible amounts of clay 2 laboratory measured porosity is the so called total porosity i e the ratio of the entire pore space in a rock to its bulk volume permeability was measured in the laboratory by pushing brine through the rock under laminar flow conditions for digital rock computations mini plugs were extracted from the original plugs used for laboratory measurements and were subsequently imaged with a state of the art micro ct detector at roughly the same image resolution voxel size 2 µm table 1 all three dimensional micro ct images were filtered with a proprietary non local means filter algorithm to boost signal to noise ratio upon filtering these images were segmented using spatial fuzzy c means clustering sfcm chuang et al 2006 algorithm into hydraulically conductive pores and non conductive minerals fig 2 the sfcm algorithm combines spatial statistics for each voxel with the fuzzy c means degree of membership to segment the image into its constituent phases volume fraction of pores estimated from the segmented image is referred here as the image porosity permeability was computed using a single phase lattice boltzmann method lbm solver further details on the numerical engine convergence criteria and boundary conditions can be found in appendix b for a more general introduction to the lbm we refer to benzi et al 1992 succi 2001 keehm et al 2001 saxena et al 2017a and alpak et al 2018 the computed permeability using this workflow is referred here as the image computed permeability a comparison between image computed versus laboratory measured porosity and permeability are shown in figs 3 5 we note a systematic offset when comparing laboratory measurements of porosity and permeability with those computed using micro ct images we find that digital rock analysis leads to systematic underestimation of porosity by down to a factor of 0 5 we note that image computations severely overestimate permeability of rocks by up to a factor of 10 for samples with laboratory measured permeability less than 100 mdarcy this observation is consistent with the recent study by saxena et al 2018 who predicted that due to imaging considerations the lower bound for simulating accurate permeability using micro ct images of 2 µm voxel size is around 100 mdarcy this lower bound drops to 25 mdarcy for images of 1 µm voxel size there are many key considerations when comparing laboratory data with digital rock results firstly the quality of digital rock simulations is determined by a variety of factors including the degree of resolved rock components the quality of image reconstruction and the signal to noise ratio in images secondly digital rock computations are impacted by every step of the workflow fig 1 including any biases in image segmentation that can lead to under or overestimation of porosity finally the two sets of results were obtained at different scales or field of view since laboratory measurements were performed on 40 mm cylindrical core plugs whereas digital rock computations were carried out on 4 mm size cubes extracted from the larger core plugs used for laboratory measurements this means that digital rock simulations were computed on 1 1000th of the rock volume used for laboratory measurements this substantial difference in the physical size of the analyzed sample creates a bias in the computed results even for perfectly homogenous samples with no geological layering within the plug these biases must be corrected before comparing image simulation results with laboratory measurements to this end we propose the following relations 1 ϕ ϕ i α r α s α v 2 k k i β r β s β v in eqs 1 and 2 ϕ and k denote porosity and permeability respectively subscript i denotes properties computed using micro ct images subscript denote the corresponding properties of a hypothetical image of infinite resolution and infinitely large field of view that has been segmented perfectly to estimate total porosity ϕ t excess pore volume within porous clays identified in images must be added to the corrected porosity ϕ parameters α and β are the correction factors and subscripts r s and v denote transforms for image resolution bias in estimating pore volume due to uncertainty in image segmentation and field of view or upscaling respectively there may be additional correction factors required for rocks whose properties are sensitivity to various other factors including differences in stress state swelling of clays electrochemical effects and severe heterogeneity to name but a few we ignore these additional transforms as these do not apply to the rocks analyzed in this study in the next section we will derive expressions for parameters α and β that depend on microstructural or geometric attributes that can be extracted directly using the original micro ct image 3 transforms for image porosity we begin with estimating the α parameters due to limited image resolution a portion of rock porosity is always unresolved this portion is significant when porosity is estimated using x ray computed tomography images of rocks recently saxena et al 2019 derived the following correction factors for image resolution 3 α r e g m l o g 10 n m the parameter α r describes how porosity contained within self similar pore structures in rocks can be predicted using an image even when these structures are below image resolution and therefore can be used to correct for the missing sub resolution porosity for further details on derivation of the result in eq 3 refer appendix c the parameter n denotes the ratio of pore throat size entry pore throat dd for mercury in a micp experiment and image voxel size δx and thus a better measure of the degree to which the entry pore throat is resolved compared to voxel size parameter g is the so called curvature of a micp curve thomeer 1960 parameter g is precisely 0 for pipes approximately 0 2 for sandstones and approximately 0 3 for carbonates subscript m denotes the laboratory measured parameter whereas subscript i denotes the value inferred from an image at infinite image resolution the two sets are the same we have found general good agreement between laboratory measured and image computed micp curves upon the required closure transforms thomeer 1960 an example is shown in fig 6 a for which gm gi 0 3 and nm ni 7 note that it is important that the comparison between laboratory measured and image computed micp curves is performed for fractional bulk volume of mercury i e mercury volume rock volume and not for saturation curves i e mercury volume pore volume fig 6b because raw image porosity is not equal to total porosity due to the resolution limit upon analysis of laboratory measured micp data and multiple micro ct images acquired at various resolutions of the grain packs and the outcrop rocks we find that gm gi and nm ni fig 7 thus entry pore throat size estimated using micro ct images is generally larger than that inferred from laboratory measurements this bias occurs due to over segmentation of pores in coarse resolution images especially when n 5 and thus can be addressed using a more robust segmentation approach from this analysis we propose the following empirical relation to calculate nm from ni fig 7 4 n m n i 2 e 0 4 n i 1 we now focus on image segmentation correction for porosity i e factor α s this correction depends on the quality of image segmentation which itself is sensitive to image resolution noise and segmentation method berg et al 2018 saxena et al 2017b to achieve perfect segmentation i e α s 1 it is necessary to correctly identify all types of rock pores as well as optimally partition pores and grains that are visible in an image this can only be estimated if additional constraints are available or if every section of a three dimensional image is manually checked for perfect segmentation the result in eq 3 can yield constraints on image porosity and segmentation if ϕ can be inferred using laboratory measured porosity e g micp nmr this is because we can predict the expected image porosity ϕ m using the following relation 5 ϕ m ϕ e g l o g 10 n m if correct segmentation is performed then ϕ i ϕ m still regardless of the availability of constraints for image segmentation the preferred protocol should be to improve the quality of image segmentation when possible rather than apply a large correction finally the field of view correction for porosity is expected be minimal i e α v 1 if large enough field of view is selected for computation of porosity various studies have shown that fluctuations in porosity values stabilize rapidly with increasing in sample volume andrä et al 2013 keehm 2003 mu et al 2016 saxena et al 2018 the necessary field of view to minimize the impact of this transform i e α v 1 is achieved if 5 grains are captured in each dimension i e 125 grains in a 3d volume however if α v 1 this would imply a very small field of view and therefore any further computations should be avoided 4 transforms for image permeability in this section we derive expressions for the β parameters that are needed to transform permeability estimated using a micro ct image the image resolution correction factor β r follows directly from the observation that if the image resolution is poor i e n 5 then micp simulations overestimate the entry pore throat size and that permeability depends on the square of pore throat size swanson 1981 thomeer 1983 5 β r n i n m 2 n i n i 2 e 0 4 n i 1 2 for cases where segmentation is inherently uncertain we propose a transform for image segmentation that is based on the observation that when a micro ct image is segmented with various segmentation algorithms fig 8 then the computed permeability values k exhibit the following dependency on segmented porosity ϕ saxena et al 2017b 6 k k 1 ϕ ϕ r e f 3 1 ϕ r e f 1 ϕ 2 in eq 6 subscript ref denotes properties corresponding to a reference segmentation thus δϕ bias in porosity due to incorrect image segmentation leads to the following bias in permeability 7 δ k 3 k r e f ϕ r e f δ ϕ for example consider a rock whose porosity is 20 porosity units p u i e percentage of total rock volume and permeability is 100 mdarcy if the bias in estimating porosity due to image segmentation is 3 p u then the bias in permeability will be 45 mdarcy therefore when pore volume is incorrectly estimated due to segmentation its impact on permeability can be compensated using the expression 8 β s ϕ i ϕ m 3 1 ϕ m 1 ϕ i 2 where ϕ m is the rock porosity containing only pores larger than the image voxel size it follows directly from eq 8 that the transforms related to image segmentation can only be applied if ϕ m or ϕ are known as noted in the case of the porosity transform if α s 1 or β s 1 the preference should be to re segment the image appropriately so that α s β s 1 correction for field of view or rock volume is within the purview of the topic of representative elementary volume rev bear 1975 hill 1963 torquato 1991 here we restrict the discussion to homogenous rocks that are completely devoid of fine scale laminations dual pore throat systems or other microstructural details that cannot be captured in a digital image of 4 mm various authors mu et al 2016 saxena et al 2018 have shown that permeability computed for concentric cubes of segmented images do not completely stabilize even when digital images are larger than 4 mm in one direction for even homogenous rocks e g fontainebleau therefore we must consider how the vast scale differences between image derived and laboratory measured permeability impact the comparison especially given that digital rock images have disproportionately fewer pore throats i e the narrowest constrictions in the largest pores compared to those present in 40 mm size plugs used for laboratory measurements the micp experiment is a good analog for a comparison fig 9 the pressure at which mercury begins to enter a digital rock in a numerical simulation is considerably lower compared to the entry pressure inferred from laboratory measurement of micp this is because mercury first enters a digital rock sample via outer pore bodies instead of pore throats because they can be accessed at much lower pressure compared to the pore throats compensating for the biases introduced by the outer pore bodies to flow properties is referred here as closure correction although this phenomenon also occurs in a physical micp measurement the volumetric contributions of the outer pore body is negligible compared to the total volume of mercury injected in the physical sample simply due to considerably larger sample size this relative contribution is larger for rocks with lower permeability fig 9 we define the ratio of outer pore body size to image voxel size as ninc for a numerical simulation on a digital image the apparent pore throat size neff must be between entry pore throat ni and pore body size ninc and thus we can put strict bounds on the correction factor 9 1 β v ω 2 where ω is the ratio of ninc and ni it is apparent that parameter β v must decline with larger field of view and this decline would be steeper for permeable rocks therefore simply assuming β v ω2 would lead to an overcorrection of image permeability for all rocks we now consider an alternative approach that approximates pores as sinusoidal channels with diameters ninc and ni and use empiricism to analyze how effective channel width changes with field of view and rock permeability using ω we now express parameter β v as 10 β v ω 2 p if the digital rock image has sufficiently large field of view to reproduce the fluid flow observed in a laboratory experiment then p 0 and β v 1 hence no field of view transforms are required however if flow is dominated by the outer pore body then the calculated permeability will not be representative and thus p 1 this situation might occur if there are too few grains in the digital rock such that fluid flow entering the rock pores is overestimated this effect could be significant if the rock has low permeability next we empirically establish the variation of coefficient p with parameters that can be extracted directly using the original micro ct image of a rock we back calculate the parameter p using the laboratory measured permeability after compensating for both resolution and segmentation transforms eqs 5 and 8 this analysis was repeated for grain packs and outcrop rocks of various sizes fig 10 we find that the following empirical relation describes the variation of parameter p 11 p 1 e a d d ϕ n r e v 0 5 2 where a 3 in eq 11 nrev denotes the ratio of field of view and the effective grain size deff as defined by saxena et al 2018 such that it approximates the grains across the flow direction the expression in eq 11 is not unique and is merely a good fit the magnitude of β v increases with the ratio of ninc and ni decreasing pore throat size decreasing porosity decreasing number of grains across the flow direction the application of permeability transform due to biases in image resolution image segmentation and field of view should be kept at a minimum and the preferred protocol is to compensate for these biases by improving the image when possible rather than apply a correction to the computed permeability for instance the need for any resolution and segmentation transform can be minimized by re imaging the rock at a higher resolution which can also improve image quality for image segmentation similarly it is possible to compensate for closure correction effects using appropriate boundary conditions for flow around the outer pore bodies of the digital image so that the field of view correction can be minimized further detailed investigation is still required to minimize the need for transforms 5 application for reservoir rocks we now apply the transforms to estimate porosity and permeability for the database of reservoir rocks these rocks were not used for any calibration to derive the expressions or fit parameters for the transforms we also include results from the recent digital rock study by chhatre et al 2018 values for image resolution transforms α r β r were estimated directly using the segmented micro ct images the comparison between laboratory measured porosity and permeability and those computed using digital rock improves upon application of the image derived resolution transform figs 11 and 12 this comparison further improves when field of view transforms and image segmentation transforms are applied figs 13 and 14 the trend between porosity and permeability after transforms for resolution and field of view compares well with those measured in the laboratory fig 15 this comparison improves further when laboratory measured porosity is used to compensate for the segmentation bias in porosity fig 16 for rocks with low permeability values 5 mdarcy even upon corrections for image resolution and field of view we still note relatively large disagreement between image derived and laboratory measured permeability there can be several reasons for this discrepancy including larger uncertainty in segmenting poorly resolved images with relatively small pore throats inability of the resolution and field of view transforms to capture the required microstructural details and of course relatively large uncertainty in measuring low permeability in laboratory 6 imaging protocols to minimize the need for transforms the transforms discussed in this paper should be applied prior to comparing image computed properties with those measured in the laboratory still prior to any calculations the rock samples should be imaged at sufficiently high image resolution and large field of view to minimize the magnitude of any corrections unfortunately the magnitude of corrections due to limited image resolution can only be minimized at the expense of corrections required for limited field of view this is because current micro ct detectors can only capture a limited number of voxels and can image small features at higher resolution or large features at coarser resolution this tradeoff between image resolution and field of view is insignificant for porosity as rev requirements can be generally met even for a very small field of view saxena et al 2018 therefore to estimate porosity from images it is advisable to image rocks at the highest available image resolution as per eq 3 so that the magnitude of correction can be minimized however for permeability the tradeoff between image resolution and field of view can be significant saxena et al 2018 still an optimal combination of the imaging parameters exists for which the combined magnitude of transforms is minimized we now illustrate that using rough estimates of expected rock properties it is possible to establish an optimal combination of imaging parameters i e field of view and voxel size we consider a case where the micro ct detector can capture up to 20003 voxels i e m 2000 we consider four rocks a high permeability sample k 1000 mdarcy ϕ 0 3 gm 0 1 an intermediate permeability sample k 100 mdarcy ϕ 0 2 gm 0 2 a low permeability sample k 10 mdarcy ϕ 0 15 gm 0 3 and a very low permeability sample k 1 mdarcy ϕ 0 15 gm 0 3 to calculate the magnitude of the transforms for permeability for the four rocks we need to calculate factors β r and β v for the combinations of voxel size δx assumed to vary between 0 1 10 µm and field of view m δx factors β r and β v depend on parameters nm gm nrev and the ratio ninc ni these parameters can be estimated since for the four rocks we know porosity and permeability here we assume ninc ni ω 5 the parameter nm can be estimated using pore throat size dd for an image of voxel size δx using nm dd δx pore throat size can be empirically related to porosity and permeability using thomeer s permeability model thomeer 1983 12 k 38068 g m 4 3 ϕ d d 213 2 in eq 12 k is laboratory measured permeability in mdarcy dd is entry pore throat size in µm and ϕ is laboratory measured porosity in fractions using eq 12 we can estimate dd for a given combination of porosity ϕ and permeability k the parameter nrev can be calculated using nrev m δx deff we can infer deff using the following empirical relation between grain size and pore throat size suggested by saxena et al 2018 13 d e f f 0 5 ϕ 2 d d hence using eqs 12 and 13 we can estimate β r and β v for the four rocks as a function of voxel size δx and field of view m δx as expected we find that the magnitude of the required corrections for high permeability rock are not significant 10 and the optimal voxel size for imaging is 2 7 µm which results in a maximum field of view of 5 4 mm in each dimension fig 17 the intermediate permeability rock can be imaged at roughly the same voxel size as the higher permeability rock but the magnitude of corrections is greater 40 the magnitude of corrections is significantly larger for the low permeability rock factor of 2 moreover the low permeability rock should be imaged at higher resolution i e voxel size 2 µm interestingly the very low permeability rock should be imaged at a very high resolution voxel size 1 µm to minimize already very large corrections fig 17 one way to effectively reduce the magnitude of corrections is to improve micro ct technology so that a larger number of voxels can be captured so that images of higher resolution can be obtained for large enough field of view an example is shown in fig 18 which presents the same information as in fig 17 but for a micro ct detector that can capture up to 50003 voxels i e m 5000 this analysis can be performed to infer optimal imaging parameters for a given reservoir and maximize quantitative information for further modeling 7 conclusions modern imaging can identify features that are larger than the image voxel size e g µm for x ray computer tomography nm for scanning electron microscopy a portion of rock porosity is always unresolved due to finite image resolution regardless of the imaging technique this portion however is significant when porosity is estimated using x ray computed tomography micro ct images of rocks therefore micro ct imaging is not a suitable technique for estimating total porosity as a minimum image derived porosity must be corrected for limited image resolution before it can be compared with laboratory measured total porosity we use concepts of capillary physics in rocks to quantify the impact of image resolution on image derived porosity and develop novel transforms to derive the corrected porosity that compensate for the limited image resolution without the need for higher resolution imaging that is only possible at the expense of image field of view or physical laboratory measurement furthermore the sub resolution pore volume predicted by our method can also be used to correct the fluid saturation inferred from multiphase flow simulations on segmented micro ct images for the missing pore volume image resolution also impacts permeability coarser image resolution leads to an artificial increase in pore throat size in segmented images and thus permeability is overestimated we find that finite sample size or field of view of digital rocks also leads to a systematic and in some cases drastic overestimation in permeability when compared to laboratory permeability which are performed on samples with significantly larger field of view compared to numerical simulations we quantify this effect and suggest relevant transforms applicable to relatively homogenous rocks we also suggest a transform that can account for the impact of incorrect image segmentation on permeability if a laboratory measured constraint on porosity is available we conclude that these transforms are critically important for estimating meaningful properties of conventional sandstone and carbonate reservoir rocks and can allow us to further mature digital rock as a technology for existing and future fields still whenever possible the application of transforms should be kept at a minimum and the preferred protocol is to compensate for these biases by improving the image when possible rather than apply a correction to the computed property additionally understanding the sensitivities of the transforms allows us to balance the tradeoff between imaging resolution and field of view to achieve more accurate properties using digital rock characterization and simulations acknowledgements we thank l taras bryndzia saad saleh dmitry shaporov chaitanya pradhan majeed shaikh kunj tandon umang agarwal steffen berg stefan hertel marisa rydzy and ove bjorn wilson for discussions supplementary material supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2019 103419 appendix d supplementary materials image application 1 appendix a description of grain packs and rocks each grain pack is composed of nearly spherical grains of glass we refer to these packs as gp1 gp2 gp3 and gp4 listed in increasing order of sphere diameter for reference solution of grain pack permeability we use thomeer s permeability relation thomeer 1983 other reference solutions for sphere packs lead to similar permeability values e g hinch 1977 howells 1974 rumpf and gupte 1971 torquato 2002 the five outcrop rocks selected for this study span a range of grain sizes from 0 05 to 0 5 mm and range in porosity from 0 05 to 0 25 table 1 these samples were selected because they cover a range of compositions and textures that may be encountered rocks b1 and b5 are from the berea formation churcher et al 1991 which is a sub angular to sub rounded mississippian age sandstone rock f is from the fontainebleau formation which is a sub rounded to rounded oligocene age sandstone this sample was well cemented with low porosity rocks g1 and g2 are from the castlegate formation of utah which is a sub angular to sub rounded mesozoic sandstone xrd analyses indicates that the castlegate sandstone sample consists mainly of quartz grains with trace amounts of carbonate cement the reservoir rocks samples a s j c and d come from various rock formations around the world and are of different geologic origin and ages table 1 the texture and microstructure of these reservoir rocks is considerably more complicated when compared to grain packs or the outcrop rock samples we also include three samples from chhatre et al 2018 include an austin chalk sample ac indiana limestone sample il and scioto sandstone sample t we did not have any images for these samples but used the reported image porosity laboratory measured porosity and micp simulation results reported in chhatre et al 2018 appendix b the lbm solver and boundary conditions the flow simulations were performed using an implementation of the lattice boltzmann method lbm the lbm implementation is based on the d3q19 approach and the multiple relaxation time mrt technique that capture complex flow physics the mrt method is described in detail all flow computations were performed using the periodic boundary conditions for the main flow direction and the directions normal to the flow the computations were run on a large linux based cpu gpu hpc cluster appendix c impact of image voxel size on porosity we derive the relation between image voxel size and image derived porosity using the concept of capillary pressure saxena et al 2019 mercury injection capillary pressure micp curves that are routinely measured in laboratory describe the relation between the pressure applied on the mercury to enter pore throats to overcome capillary pressure and the fractional bulk volume i e volume of invaded mercury divided by rock volume that is occupied by mercury at that pressure various empirical models brooks and corey 1964 leverett 1941 thomeer 1960 can be used to fit the measured micp data these models can be expressed in the following general form c 1 ϕ p ϕ e g m l o g 10 p p d in eq c 1 ϕ p is the fractional bulk volume occupied by mercury at pressure p and ϕ is the fractional bulk volume occupied at infinite pressure parameter pd is the pressure required to enter the dominant pore throats and is also sometimes referred as the entry pressure parameter gm describes the shape of the transition zone in a micp curve thus providing a measure of rock complexity and distribution of pore throat size parameters pd and gm can be directly inferred from a laboratory measured micp curve swanson 1981 thomeer 1983 1960 alternatively these parameters can also be inferred using image based simulations hilpert and miller 2001 micp parameter ϕ is usually slightly smaller than the total rock porosity that is measured using helium gas this is because helium can invade secondary pores which remain inaccessible by mercury highest pressure achieved in a typical laboratory is around 100 000 psi pore throat size d penetrated at a given pressure p is given by the young laplace equation p 4σcosθ d where σ is mercury air surface tension 480 dyne cm and θ is the contact angle 140 if p is expressed in psi and d is expressed µm then 4σcosθ 213 2 using eq c 1 we now derive the relation between image voxel size and image derived porosity the highest pressure that can be achieved when simulating the micp curve using an image of voxel size δx is given by young laplace equation c 2 p max 4 σ cos θ δ x substituting eq c 2 in eq c 1 and rewriting eq c 1 in terms of the entry or dominant pore throat size dd 4σcosθ pd we get a relation between ϕ and the expected image porosity ϕ m c 3 ϕ m ϕ e g m l o g 10 d d δ x here we refer to a quantity x with subscript m we are referring to the ground truth of the quantity x with no associated errors due to image acquisition or image processing we refer to image derived properties with subscript i ideally xm and xi should be equal if correct image processing segmentation is followed this is seldom the case and hence the distinction we now define a dimensionless parameter that describes pore throat resolution c 4 n m d d δ x 4 σ cos θ δ x p d nm corresponds to the number of voxels available to resolve the dominant pore throat of size dd for the remainder of the paper we will refer to nm as the pore throat resolution parameter as it depends on the image resolution as well as the size and the shape of the dominant pore throat using eqs c 3 and c 4 we get c 5 ϕ m ϕ e g m l o g 10 n m transforms in eq c 5 apply if one can infer the curvature parameter gm using the simulated micp curve this should be possible if the pressure in a micp curve at the inflection point pa is lower than p max which is guaranteed if c 6 n m 10 g m l o g 10 e eq c 6 describes the working envelope of image derived transport properties alpak et al 2018 f o alpak f gray n saxena j dietderich r hofmann s berg a distributed parallel multiple relaxation time lattice boltzmann method on general purpose graphics processing units for the rapid and scalable computation of absolute permeability from high resolution 3d micro ct images comput geosci 2018 10 1007 s10596 018 9727 7 alpak f o gray f saxena n dietderich j hofmann r berg s 2018 a distributed parallel multiple relaxation time lattice boltzmann method on general purpose graphics processing units for the rapid and scalable computation of absolute permeability from high resolution 3d micro ct images comput geosci 10 1007 s10596 018 9727 7 andrä et al 2013 h andrä n combaret j dvorkin e glatt j han m kabel y keehm f krzikalla m lee c madonna m marsh t mukerji e h saenger r sain n saxena s ricker a wiegmann x zhan digital rock physics benchmarks part i imaging and segmentation comput geosci 50 2013 10 1016 j cageo 2012 09 005 andrä h combaret n dvorkin j glatt e han j kabel m keehm y krzikalla f lee m madonna c marsh m mukerji t saenger e h sain r saxena n ricker s wiegmann a zhan x 2013 digital rock physics benchmarks part i iimaging and segmentation comput geosci 50 10 1016 j cageo 2012 09 005 andrä et al 2013a heiko andrä n combaret j dvorkin e glatt j han m kabel y keehm f krzikalla m lee c madonna m marsh t mukerji e h saenger r sain n saxena s ricker a wiegmann x zhan digital rock physics benchmarks part ii computing effective properties comput geosci 50 2013 33 43 10 1016 j cageo 2012 09 008 andrä heiko combaret n dvorkin j glatt e han j kabel m keehm y krzikalla f lee m madonna c marsh m mukerji t saenger e h sain r saxena n ricker s wiegmann a zhan x 2013a digital rock physics benchmarks part ii ccomputing effective properties comput geosci 50 33 43 10 1016 j cageo 2012 09 008 andrä et al 2013b heiko andrä n combaret j dvorkin e glatt j han m kabel y keehm f krzikalla m lee c madonna m marsh t mukerji e h saenger r sain n saxena s ricker a wiegmann x zhan digital rock physics benchmarks part i imaging and segmentation comput geosci 50 2013 25 32 10 1016 j cageo 2012 09 005 andrä heiko combaret n dvorkin j glatt e han j kabel m keehm y krzikalla f lee m madonna c marsh m mukerji t saenger e h sain r saxena n ricker s wiegmann a zhan x 2013b digital rock physics benchmarks part i iimaging and segmentation comput geosci 50 25 32 10 1016 j cageo 2012 09 005 arns et al 2005 c h arns f bauget a limaye a sakellariou t senden a sheppard r m sok v pinczewski s bakke l i berge p oren m knackstedt pore scale characterization of carbonates using x ray microtomography spe j 10 2005 26 29 10 2118 90368 pa arns c h bauget f limaye a sakellariou a senden t sheppard a sok r m pinczewski v bakke s berge l i oren p knackstedt m 2005 pore scale characterization of carbonates using x ray microtomography spe j 10 26 29 10 2118 90368 pa arns et al 2002 c h arns m a knackstedt w v pinczewski e j garboczi computation of linear elastic properties from microtomographic images methodology and agreement between theory and experiment geophysics 67 2002 1396 10 1190 1 1512785 arns c h knackstedt m a pinczewski w v garboczi e j 2002 computation of linear elastic properties from microtomographic images mmethodology and agreement between theory and experiment geophysics67 1396 10 1190 1 1512785 bear 1975 j bear dynamics of fluids in porous media soil sci 1975 10 1097 00010694 197508000 00022 bear j 1975 dynamics of fluids in porous media soil sci 10 1097 00010694 197508000 00022 benzi et al 1992 r benzi s succi m vergassola the lattice boltzmann equation theory and applications phys rep 1992 10 1016 0370 1573 92 90090 m benzi r succi s vergassola m 1992 the lattice boltzmann equation theory and applications phys rep 10 1016 0370 1573 92 90090 m berg et al 2018 s berg n saxena m shaik c pradhan generation of ground truth images to validate micro ct image processing pipelines lead edge 37 2018 412 420 10 1190 tle37060412 1 berg s saxena n shaik m pradhan c 2018 generation of ground truth images to validate micro ct image processing pipelines lead edge37 412 420 10 1190 tle37060412 1 blunt et al 2013 m j blunt b bijeljic h dong o gharbi s iglauer p mostaghimi a paluszny c pentland pore scale imaging and modelling adv water resour 51 2013 197 216 10 1016 j advwatres 2012 03 003 blunt m j bijeljic b dong h gharbi o iglauer s mostaghimi p paluszny a pentland c 2013 pore scale imaging and modelling adv water resour 51 197 216 10 1016 j advwatres 2012 03 003 brooks and corey 1964 r brooks a corey hydraulic properties of porous media hydrol pap color state univ 3 1964 37 pp 10 13031 2013 40684 brooks r corey a 1964 hydraulic properties of porous media hydrol pap color state univ 3 37 pp 10 13031 2013 40684 chhatre et al 2018 chhatre s s sahoo h leonardi s vidal k rainey j braun e m patel p 2018 a blind study of four digital rock physics vendor laboratories on porosity absolute permeability and primary drainage capillary pressure data on tight outcrops chuang et al 2006 k s chuang h l tzeng s chen j wu t j chen fuzzy c means clustering with spatial information for image segmentation comput med imaging graph 30 2006 9 15 10 1016 j compmedimag 2005 10 001 chuang k s tzeng h l chen s wu j chen t j 2006 fuzzy c means clustering with spatial information for image segmentation comput med imaging graph 30 9 15 10 1016 j compmedimag 2005 10 001 churcher et al 1991 churcher p l french p r shaw j c schramm l l 1991 rock properties of berea sandstone baker dolomite and indiana limestone in spe international symposium on oilfield chemistry pp 431 466 https doi org 10 2118 21044 ms dvorkin et al 2008 j dvorkin m armbruster c baldwin q fang n derzhi c gomez b nur a nur y mu the future of rock physics computational methods vs lab testing first break 26 2008 63 68 dvorkin j armbruster m baldwin c fang q derzhi n gomez c nur b nur a mu y 2008 the future of rock physics ccomputational methods vs lab testing first break26 63 68 dvorkin et al 2011 j dvorkin n derzhi e diaz q fang relevance of computational rock physics geophysics 76 2011 e141 e153 10 1190 geo2010 0352 1 dvorkin j derzhi n diaz e fang q 2011 relevance of computational rock physics geophysics76 e141 e153 10 1190 geo2010 0352 1 hill 1963 r hill elastic properties of reinforced solids some theoretical principles j mech phys solids 11 1963 357 372 10 1016 0022 5096 63 90036 x hill r 1963 elastic properties of reinforced solids ssome theoretical principles j mech phys solids11 357 372 10 1016 0022 5096 63 90036 x hilpert and miller 2001 m hilpert c t miller pore morphology based simulation of drainage in totally wetting porous media adv water resour 24 2001 243 255 10 1016 s0309 1708 00 00056 7 hilpert m miller c t 2001 pore morphology based simulation of drainage in totally wetting porous media adv water resour 24 243 255 10 1016 s0309 1708 00 00056 7 hinch 1977 e j hinch an averaged equation approach to particle interactions in a fluid suspension j fluid mech 83 1977 695 720 10 1017 s0022112077001414 hinch e j 1977 an averaged equation approach to particle interactions in a fluid suspension j fluid mech 83 695 720 10 1017 s0022112077001414 howells 1974 i d howells drag due to the motion of a newtonian fluid through a sparse random array of small fixed rigid objects j fluid mech 64 1974 449 476 10 1017 s0022112074002503 howells i d 1974 drag due to the motion of a newtonian fluid through a sparse random array of small fixed rigid objects j fluid mech 64 449 476 10 1017 s0022112074002503 kanckstedt et al 2001 m a kanckstedt a p sheppard m sahimi pore network modelling of two phase flow in porous rock the effect of correlated heterogeneity adv water resour 24 2001 257 277 10 1016 s0309 1708 00 00057 9 kanckstedt m a sheppard a p sahimi m 2001 pore network modelling of two phase flow in porous rock tthe effect of correlated heterogeneity adv water resour 24 257 277 10 1016 s0309 1708 00 00057 9 keehm 2003 y keehm computational rock physics transport properties in porous media and applications 2003 stanford univ keehm y 2003 computational rock physics transport properties in porous media and applications stanford univ keehm et al 2001 y keehm t mukerji a nur computational rock physics at the pore scale transport properties and diagenesis in realistic pore geometries lead edge 2001 10 1190 1 1438904 keehm y mukerji t nur a 2001 computational rock physics at the pore scale ttransport properties and diagenesis in realistic pore geometries lead edge 10 1190 1 1438904 knackstedt et al 2008 m a knackstedt c arns m madadi a p sheppard s latham r sok g bächle g eberli elastic and flow properties of carbonate core derived from 3d x ray ct images seg tech progr expand abstr 27 2008 1804 1809 10 1190 1 3059394 2008 knackstedt m a arns c madadi m sheppard a p latham s sok r bächle g eberli g 2008 elastic and flow properties of carbonate core derived from 3d x ray ct images seg tech progr expand abstr 200827 1804 1809 10 1190 1 3059394 knackstedt et al 2009 mark knackstedt s latham m madadi a sheppard t varslot c arns digital rock physics 3d imaging of core material and correlations to acoustic and flow properties lead edge 28 2009 28 33 10 1190 1 3064143 knackstedt mark latham s madadi m sheppard a varslot t arns c 2009 digital rock physics 3d imaging of core material and correlations to acoustic and flow properties lead edge28 28 33 10 1190 1 3064143 leverett 1941 m c leverett capillary behavior in porous solids trans aime 142 1941 152 169 10 2118 941152 g leverett m c 1941 capillary behavior in porous solids trans aime142 152 169 10 2118 941152 g mu et al 2016 y mu r sungkorn j toelke identifying the representative flow unit for capillary dominated two phase flow in porous media using morphology based pore scale modeling adv water resour 95 2016 16 28 10 1016 j advwatres 2016 02 004 mu y sungkorn r toelke j 2016 identifying the representative flow unit for capillary dominated two phase flow in porous media using morphology based pore scale modeling adv water resour 95 16 28 10 1016 j advwatres 2016 02 004 øren et al 2006 p øren s bakke h rueslåtten digital core laboratory rock and flow properties derived from computer generated rocks int symp soc core anal 2006 1 12 øren p bakke s rueslåtten h 2006 digital core laboratory rrock and flow properties derived from computer generated rocks int symp soc core anal 1 12 rumpf and gupte 1971 h c h rumpf a r gupte einflüsse der porosität und korngrößenverteilung im widerstandsgesetz der porenströmung chemie ing tech cit 43 1971 367 375 10 1002 cite 330430610 rumpf h c h gupte a r 1971 einflüsse der porosität und korngrößenverteilung im widerstandsgesetz der porenströmung chemie ing tech cit43 367 375 10 1002 cite 330430610 saxena et al 2017a n saxena r hofmann f o alpak s berg j dietderich u agarwal k tandon s hunter j freeman o wilson and benchmarks for pore scale flow simulated using micro ct images of porous media and digital rocks adv water resour 109 2017 211 235 10 1016 j advwatres 2017 09 007 saxena n hofmann r alpak f o berg s dietderich j agarwal u tandon k hunter s freeman j wilson o 2017a 
