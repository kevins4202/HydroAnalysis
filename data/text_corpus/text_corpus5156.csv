index,text
25780,accurate estimates of water volumes are crucial for water management this study applies an automated methodology to detect small scale on farm dams and develops a novel application of bayesian inference to jointly simulate volumes and uncertainties a linear relationship was assumed between flooded pixel area images derived from lidar datasets and water index rasters derived from sentinel 2 images using a markov chain monte carlo mcmc method led to accurate estimates of water elevations and reservoir volumes with a systematic error of about 2 5 and 10 of the maximum capacity during the study period in keepit dam and pamamaroo lake respectively additionally the method quantifies uncertainties of volumes the presence of woody vegetation growing at the reservoir walls leads to a deterioration of estimates this methodology may be used as an auditing tool in water governance schemes or to gain knowledge on water losses at the field scale keywords mcmc on farm dams uncertainty volume estimation 1 introduction water is a strategic asset for agriculture which is increasingly being monitored by farmers since a gain in water use efficiency can lead to a reduction in production costs fan et al 2018 moreover since meteorological trends around the world indicate an intensification in the hydrological cycle ziegler et al 2003 yao et al 2019 rottler et al 2020 with a positive overall global trend of temperatures collins et al 2013 causing a potential positive feedback on evapotranspiration miralles et al 2014 zhang et al 2016a b c agricultural water demand is expected to increase in coming years this implies more pressure on water resources bindi and olesen 2011 misra 2014 and makes water monitoring at different scales increasingly necessary on farm dams in australia represent one strategy to mitigate uncertainty in water availability and to increase opportunities for irrigation pittock 2016 these have caused significant changes to the landscape and their effect on water availability and natural ecosystems is still being investigated kingsford 2000 porter 2002 they also imply a change in the hydrological cycle and are potentially linked to high evaporation losses craig et al 2005 fuentes et al 2020 however increased water use can also lead to water over exploitation either by a lack of knowledge about the availability of water resources or by deficiencies in the water governance schema chaffin et al 2014 porras et al 2018 damaging water resources and ecosystems gleeson et al 2012 pellicer martínez and martínez paz 2016 this has led to increasing public pressure to control over exploitation improve water resource management and to prioritization of different water uses to address water scarcity pahl wostl et al 2007 water governance systems play an important role to address the balance between available water and water needs through different levels of stakeholder participation rogers and hall 2003 several factors result in a higher stakeholder involvement to participate in water management plans such as the availability and accessibility of information and the different degrees of water stress pahl wostl et al 2007 carr et al 2012 however regardless of strong social involvement or institutions water auditing is needed as an important tool to manage over exploitation barrington et al 2013 alternatively water accountability at irrigation companies can help deal with current challenges including higher public scrutiny of the use of water resources this can foster increased transparency of water resource consumption addressing public concerns berg and phillips 2017 a key aspect in both strategies water auditing and water accountability is the availability of data to monitor water resources berg and phillips 2017 escriva bou et al 2020 conventional storage monitoring in reservoirs has been achieved through bathymetric or terrain elevation surveys and complemented by installing level meters or pressure transducers which can be used to derive storage capacities from level volume curves herschy 2012 however improvements in spatio temporal resolution of satellite products has increased the range of potential applications of remote sensing data beck et al 2000 rose et al 2015 karthikeyan et al 2020 chawla et al 2020 allowing monitoring of different processes at multiple scales entwistle et al 2018 distinct components of the water budget can be estimated using remote sensing data pekel et al 2016 zhang et al 2016a b c nevertheless monitoring of storages at the field scale using remote sensing data has increased only slowly due to limitations in the spatio temporal resolution rodrigues et al 2012 ogilvie et al 2018 for instance even though several satellite altimetry datasets exist these are not intended for inland water monitoring zhang et al 2020 most of them have satellite tracks largely spaced cazenave 2019 and moderate spatial resolutions along track 300 m in sentinel 3 and jason 3 dong et al 2018 kittel et al 2021 which limit their use in the monitoring of small reservoirs on the other hand most surface reflectance products have a spatial resolution of at least 10 m which makes it hard to accurately delineate the water limit in on farm dams with steep side walls in addition limited elevation data for the bottom of the dams also limits quantifying the water volumes fuentes et al 2019 even if several studies have addressed water detection with different degrees of accuracy mueller et al 2016 pekel et al 2016 huang et al 2018 a smaller number of studies have tried to go from surface water extent to surface water volumes karran et al 2017 fuentes et al 2019 pereira et al 2019 and even fewer have tried to quantify the uncertainties related to the estimates deng et al 2020 the integration of geographic information systems and remote sensing data layers in google earth engine run on the cloud through colab notebooks provide important infrastructure reducing local computational requirements and pre processing time and widening the integration of tools and functionality for analysis gorelick et al 2017 this study aims to evaluate a methodology developed for the automated detection monitoring and quantification of on farm dam water volumes and associated uncertainties based on available surface reflectance data from copernicus satellite sentinel 2 and lidar data which might allow governmental institutions and landholders to contribute to transparent water accountability 2 materials and methods 2 1 study area and data the focus area is the namoi catchment located in new south wales australia the catchment covers around 42 000 km2 fig 1 cotton production which strongly relies on the available water during the summer season hulugalle and scott 2008 forms one of the major land uses in the catchment because of this water dependence and the unpredictable nature of flows in the australian climate several landholders have built on farm dams to secure water during the growing season two main datasets were used in this study tiles of lidar data at 1 m horizontal resolution were obtained from the elevation and depth foundation spatial data https elevation fsdf org au these were mosaicked to cover larger areas of the catchment including the boggabri narrabri wee waa and walgett towns from east to west and their surroundings where agricultural land uses dominate the landscape fig 2 surface reflectance images from the sentinel 2 satellite sentinel 2 msi level 2a covering the catchment extent for the calendar year 2019 drusch et al 2012 were used to trace water reservoirs these included 1 514 ortorectified and atmospherically corrected scenes the sentinel 2 cloud probability dataset obtained using the s2cloudless library sanchez et al 2020 was also used to mask all pixels in the surface reflectance images with a cloud probability higher than 30 the scene classification map band contained in the images was further used to mask cloud shadows in the collection even after this some remnant cloud shadows and clouds remained in the collection once these masks were applied therefore an additional filter was applied to exclude images with more than 10 of pixels containing clouds through the cloudy pixel percentage property stored in the images which reduced the scenes to 935 images other ancillary datasets included a height above the nearest drainage hand topographic model obtained from srtm nobre et al 2011 the elevations from srtm the mcd12 land cover sulla menashe and friedl 2018 and the major hydrologic network and major reservoir vectors from the geofabric dataset bureau of meteorology 2014 finally reservoir monitoring data from the waternsw webpage https www waternsw com au including the daily water levels and volumes at pamamaroo lake and keepit dam were also used 2 2 automated on farm reservoir detection due to water scarcity reservoirs may be empty during the year because farmers adjust their cropping decisions based on the expectation of water in dry years crop production is reduced to cope with the water restrictions therefore we developed an automated methodology in google earth engine to trace active reservoirs in the monitoring year fig 3 the water index developed by fisher et al 2016 for australian areas highlighted in equation 1 performs well in eastern australia 1 w i 1 7204 171 ρ b 3 3 ρ b 4 70 ρ b 8 45 ρ b 1 1 71 ρ b 1 2 where w i is the estimated water index and ρ b3 ρ b4 ρ b8 ρ b11 ρ b12 correspond to the reflectance bands in the green central wavelength of 560 nm and 559 nm for the s2a and s2b sensors red 664 5 nm and 665 nm for the s2a and s2b sensors near infrared 835 1 nm and 833 nm for the s2a and s2b sensors short wave infrared 1 1613 7 nm and 1610 4 nm for the s2a and s2b sensors and short wave infrared 2 2202 4 nm and 2185 7 nm for the s2a and s2b sensors wavelength spectrum respectively the original index was developed using landsat images but given the similarity in the wavelength range of the landsat and sentinel 2 multispectral sensors we generalized it using the sentinel 2 collection fig 3 1 in the reservoir detection step once the water index was applied to the surface reflectance images fig 3 2 water masks were created fig 3 3 assuming pure water pixels to present values greater than 0 based on fisher et al 2016 in which a histogram of pixel values for purely water fully flooded and non water bodies was presented compared with other commonly used indexes for water detection such as the normalised difference water index ndwi or the modified normalised difference water index mndwi that combine only two reflectance bands the water index applied w i uses 5 reflectance bands in fisher et al 2016 w i achieved the best accuracy in the water detection compared against other indexes including ndwi and mndwi then the water mask collection generated from sentinel 2 images was used to generate a water occurrence probability map fig 3 4 defined by equation 2 2 p o 1 n i 1 n o i where po is the water occurrence probability between 0 and 1 n are the times that a clear unmasked pixel takes place in the image collection i corresponds to each observation in the collection being o i the occurrence of water in the observation i from the water occurrence probability map we took active on farm reservoirs to have a water occurrence probability greater than 0 2 i e a pixel containing water in more than 20 of unmasked observations a conservative value considering that on farm dams are mainly used for irrigation and most crops grow for at least 3 months fig 3 5 once pixels were delimited under these characteristics the remaining water bodies in the water occurrence probability map were vectorised fig 3 6 and filtered based on the following conditions 1 elevations 410 m 2 hand 50 3 water bodies not intersecting major river reservoirs and barren areas 4 0 01 km2 reservoir area 0 5 km2 5 ratio area perimeter 25 fig 3 7 some of these constraints are empirical such as the use of elevation in the catchment which limits the selection to agricultural reservoirs and disregards some reservoirs used for mining operations other criteria are generic and are associated with characteristics of on farm reservoirs such as their dimensions shapes and location with respect to water sources which avoids the selection of channels streams and other natural open water bodies from the active on farm reservoirs detected through this procedure we selected for this study only those reservoirs that were empty when the lidar data was acquired which meant the bottom elevations of the reservoirs could be captured 2 3 data pre processing and look up table generation to perform inference on the water level and the volume of water within each reservoir we propose a statistical forward model that mirrors an approximate version of the process generating the sentinel 2 surface reflectance images fig 4 our initial assumption given the water index differences between dry and flooded pixels was that the water index unlike the binary definition used in the reservoir detection step is linearly related to the flooded area within each sentinel 2 pixel this implies the water index represents a continuous numerical variable ranging between approximately 50 and 50 this assumption reflects the physics of the sensing process the water index is a linear combination of measurements made by the multi spectral instrument with each pixel combining the reflected radiance at different wavelengths of the spectrum within a single resolution element on the ground the distribution of residuals around this relation however may be set by differences in terrain or water properties at the extremes and will be discussed further below a raster map of the flooded area within each surface reflectance pixel can be calculated given a hypothesized water level using the lidar data the water volume can also be calculated by numerically integrating the lidar elevation profile beneath a given water level this forward model for flooded areas and water volumes retains sensitivity to the position of the water boundary at the fine resolution of the lidar data rather than the coarse resolution of the surface reflection data the method s sensitivity is particularly important for water boundaries on steep slopes such as for local reservoir walls to speed up our calculations we pre compute and cache a table of water volumes and flooded area raster images as functions of the water level for each dam the calculations assumed a set of water levels spaced on a grid at 10 cm intervals from the bottom elevation of the dam up to 0 8 m below the crest of the dam walls based on local design criteria barrett 2007 volumes were estimated in google earth engine by creating a binary mask image containing all pixels inside the dam in the lidar data pixels that were smaller or equal than the water level in the iteration were transformed into ones while pixels with elevations greater than the water level were filled with 0 the masked image was multiplied by the water level and the lidar elevations were subtracted from the values to get a water depth raster lastly the water depth raster was multiplied by the pixel areas and lumped to the dam border to obtain the water volume associated with the water level this resulted in water level volume curves for the reservoirs rasters of flooded pixel areas were generated in google earth engine for the look up table as follows all pixels at higher elevation than the assumed water level with values of 0 in the binary mask were masked out and the resulting flooded pixels with values of 1 were vectorised within the reservoir at 1 m horizontal resolution using a reduction to vectors since the high resolution of the lidar images resulted in noisy features a set of rules were used to clean the feature collections vectorised water level contours with less than 100 vertices in geometry collections and multipolygon features were filtered out while vectors in polygon features with less than 50 vertices were removed a comparison of the effect of these filters is presented in the supplementary materials subsequently the resulting flooded polygons were re rasterised and mosaicked with a dry raster layer a raster with 0 values in all pixels with the dimensions of the reservoir from the resulting raster the flooded pixel area image was generated by summing and converting the mosaicked raster of flooded pixels at the lidar resolution 1 m into the sentinel 2 resolution 10 m this process was repeated for each 0 1 m water elevation iteration the water index w i and set of partially flooded area rasters a i were sampled and transformed into arrays migrating from google earth engine into google colab notebooks for further processing these were combined with the corresponding level volume series in the look up table so that the flooded pixel areas depend on the water levels 3 a i f h where h corresponds to the water level and a i are the relative flooded areas of pixels for arbitrary water levels the water volumes and flooded pixel areas were interpolated linearly on the grid of the cached values fig 5 shows the empirical linear relationship between the areas of partially flooded pixels and the sentinel 2 water index lending support to our initial assumption this shows how in general fully flooded pixels tend to have positive water index values but with a large dispersion which may include some negative pixels caused by resolution differences between surface reflectance images and the terrain elevation or algae vegetation growth among others on the other hand dry pixels and partially flooded pixels show a direct linear trend mostly in the range of negative values however fully empty and fully flooded pixels tend to divert from the linear behaviour since the water index relies on a combination of bands at different resolutions 10 and 20 m it causes spatial autocorrelation in the water index images obtained at the higher resolution therefore the resolution of the water index images was calculated at 20 m we can improve upon this initial model by accounting for two additional effects first the sentinel 2 collection contains misregistration of the data leading to a slight spatial shift in some images we accounted for this by resampling the flooded pixel area images with a three tap lanczos kernel burger and burge 2010 based on two parameters a coordinate and an ordinate offset x off and y off both in pixels this resampling method retains detail in the flooded pixel rasters without requiring us to pre calculate them for every possible registration offset since lanczos resampling can cause ringing artifacts when applied to sharp edges the flooded pixel areas were first calculated on a 10 m grid and then blurred using a fast fourier transform convolution with a gaussian kernel of size s m pixels this convolution also accounts for any advection caused by the air column underneath the satellite especially in the near infrared wavelength spectrum which can affect the linear relationship between the flooded areas and the water index the resampled rasters were finally rebinned to match the 20 m resolution of the water index images 2 4 reservoir volume modeling and uncertainty analysis the inference of the water level and its uncertainty was subsequently performed in a bayesian framework bolstad and curran 2016 using markov chain monte carlo mcmc sampling through the emcee library for python foreman mackey et al 2019 given observed data d bayesian probability assigns a relative posterior probability p θ d to the model parameters θ according to bayes s rule 4 p θ d p d θ p θ where the likelihood p d θ is the probability distribution of the observed data d given assumed model parameters θ and the prior p θ is a probability distribution representing beliefs about θ before taking the data into account arising for example from expert knowledge or previous observations the constant of proportionality while not easily calculated is not needed if mcmc is used since most variables of interest can be calculated by computing averages over the posterior samples of some function of the sampled parameter vector we will first describe the elements of the model related to the likelihood the assumed linear relation between the water index and the flooded pixel area a i is expressed in equation 5 5 w i a b a i where w i is the predicted water index a and b are the intercept and slope of the linear regression since a represents the intercept of the relationship it corresponds to the dry pixels partial flooded area 0 which should be negative while b is the slope of a direct linear relationship and would have positive values these definitions will be considered when defining the priors since the noise is assumed to arise from variable characteristics of different categories of pixels in the images the variances of the fully flooded and fully empty pixel populations were assumed to be different from partially flooded pixels parametric expressions taking into account these differences between pixel categories are shown in equations 6 8 6 w empty exp a i a fsc 7 w full exp a i 1 a fsc 8 w partial 1 w empty w full where w empty w full and w partial are weights associated with the fully empty fully flooded and partially flooded pixels respectively while a fsc is a fractional scale of pixel area where a transition from linear to exponential behaviour occurs at the extreme pixels set to 0 05 or 5 of the area of a sentinel 2 pixel this transition scale is included to prevent discontinuities in the model predictions at parameter values where individual pixels transition between classes the final variance σ t o t 2 of the data corresponds to the sum of the weighted variance for each class 9 σ tot 2 w empty σ empty 2 w full σ f u l l 2 w partial σ 2 where σ empty σ full and σ are the standard deviation of fully empty fully flooded and partially flooded pixels based on these relationships we assume independent gaussian noise with variance σ tot 2 so that the log likelihood of our model is given by 10 log p w i a i a b σ σ empty σ full s m x off y off 1 2 i 1 n w i w i 2 σ tot 2 1 2 n log 2 π σ tot 2 where n is the length of the data and w i is the observed value of the water index in pixel i the selected priors for the evaluation were investigated as we progressed in the study by first assuming weakly informative uniform priors and examining the distribution of maximum a posteriori map parameter values across reservoirs for each parameter distributions for slope and intercept in the linear relationship were tested for several reservoirs similarly the deviation for different pixel classes were evaluated by sampling these classes in different dams additionally the offset priors in the lanczos resampling and the size of the gaussian kernel in the convolution were decided based on the learning stage derived from using the sentinel 2 collection the priors for the slope b and intercept a of the linear response of the water index per unit flooded area were assigned a log normal and a normal distribution respectively 11 a n 4 4 2 a n d b log n 0 135 0 2 2 the priors for standard deviations of fully flooded fully empty and partially flooded pixels were assigned uniform distributions 12 σ empty u 0 8 σ full u 0 4 a n d σ u 0 10 the priors for x off and y off used in the lanczos resampling and the width s m of the gaussian blurring kernel were also assigned uniform distributions 13 x off u 3 3 y off u 3 3 a n d s m u 0 02 3 lastly the prior for water elevation was based on a uniform distribution taken from the bottom elevation of the dam to the top water elevation at the dam from these priors and the mcmc sampling the water elevation in reservoirs was assumed to be the map elevation and the uncertainty was estimated from the distribution of water elevations at the 16th and 84th percentiles the difference between percentiles 16 and 84 or the 68 confidence interval corresponds to the mean 1 standard deviation for a gaussian distribution although we find the parameter distributions are not in general perfectly gaussian we pick these quantiles to correspond to a reader s expectation of the standard error on a parameter finally a linear interpolation based on the water level volume curves converted the simulated water elevations into volumes 2 5 validation of estimates since no measured water levels from on farm reservoirs are available in the study area the information of pamamaroo lake and keepit dam lake were included as validation data to directly assess the algorithm s performance pamamaroo lake is one of the so called menindee lakes which are artificial reservoirs located in western new south wales near the town of menindee latitude 32 41 longitude 142 42 connected to the darling river pamamaroo lake has been artificially modified to increase its storage capacity it is a regulated storage system that has a relatively flat bottom steep walls at the border fuentes et al 2019 an inlet regulator at its entrance and an outlet that is also regulated to allow the filling of other menindee lakes these characteristics make it comparable in structure but at a larger scale to the on farm reservoirs studied on the other hand keepit dam is an artificial reservoir located in the upper namoi catchment within the study region and connected to the namoi river fig 6 which is used to store water for agriculture and to control the streamflow in the namoi river it has a steep wall on its outlet a characteristic that is shared by on farm reservoirs and due to the higher elevations in the area the rest of the perimeter also has fairly sharp elevation changes https roads waterways transport nsw gov au documents maritime usingwaterways maps boating maps 15 lake keepit pdf time series of levels and volumes stored in these two reservoirs were downloaded and used as a reference to verify the volume predictions validation performance was based on the root mean squared error rmse the mean absolute error mae the bias of the predictions i e the mean error and the determination coefficient r2 3 results a map of water occurrences as percentage using sentinel 2 images is presented in fig 7 3 1 reservoir detection clearly the water index detects open water even though cloud shadows and some artifacts still exist in the source data the preprocessing techniques clouds and shadows masking and water occurrence thresholding mitigate these sources of error additionally using a threshold for the detection of seasonal water bodies further reduces the miss classification allowing us to focus on particular water features rather than on episodic sources of error the automatic detection of on farm reservoirs performed well this can be qualitatively demonstrated looking at specific agricultural areas that are densely irrigated fig 8 the filters applied to the water polygons vectorised from the water mask lead to a visually correct delineation of on farm dams additionally by setting a threshold to the ratio between the area and perimeter of the dams resulted in filtering out most agriculture channels billabongs and streams the methodology does not necessarily lead to a perfect delineation of reservoirs at full capacity because of the water occurrence threshold nevertheless it appears to provide a good detection of locations were active reservoirs occur 3 2 volume estimation and uncertainties the water elevation estimated as the map from the mcmc analysis varied between reservoirs and in time figs 9 and 10 present corner plots of the mcmc for specific reservoirs and dates in this case the distribution of water elevations from the reservoir images in the left pane of the figures has only a small uncertainty in the range of few centimeters for samples at one standard deviation away from the median the marginal distributions of most parameters show approximately gaussian behaviour some nontrivial covariances between parameters can be observed in the posterior for example the water level shows a negative correlation with the intercept and a positive correlation with the slope of the forward model for the detector sensitivity to flooded area flooded polygons associated with map water elevations selected from the mcmc are given for two reservoirs at different dates in figs 11 and 12 these also give the water elevation associated with the maximum log likelihood and how the log likelihood changes as a function of elevation in the second column of the panel additionally the relationship between flooded pixel areas and the water index is shown in the middle column panel which varies depending on the date and image while the histograms in the penultimate column of the panel show residuals normalised by the standard deviation of the linear relationship the processing of the flooded pixel area rasters clearly leads to a stronger linear relationship of flooded pixel areas and the water index compared to the relationship presented in fig 5 using unaltered surface reflectance images the selection of the water levels at reservoirs and the associated water polygons demonstrate a good representation of the relationship used in the mcmc however some slight differences can be observed between the reflectance images and the water polygons which may be the result from errors in the digital elevation model or changes in the reservoir surface caused by sedimentation processes or earthworks in the reservoir 3 3 time series since surface reflectance images from the sentinel 2 satellite have a capture repeat time of 5 days time series of water volumes in reservoirs can be obtained to evaluate water use and water orders in this case a single reservoir was selected to evaluate the time series fig 13 highlights in the upper panel surface reflectance data for a particular reservoir at different dates while the lower panel highlights the time series of volumes interpolated from the map water elevations and the 68th percentile confidence interval including the uncertainty derived from lidar images in the vertical dimension 0 2 m hodgson and bresnahan 2004 small changes in the reservoir volumes are detected using the proposed methodology in some cases water elevations can be detected even when remnant clouds are present however clouds shadows and empty pixels can affect the detected water elevation filling and irrigation events can be easily detected from these time series but they can be obscured by the repeat time of the satellite passes and the cloud presence 3 4 validation validation plots for the keepit dam and the pamamaroo lake are highlighted in figs 14 and 15 respectively however these validation reservoirs have different characteristics than on farm reservoirs one of the major differences and problems in relation to the mcmc is the size of the validation reservoirs which increases the computing time for processing tasks such as the convolution and the lanczos resampling even if these take a few seconds per image the mcmc requires thousands of iterations which makes this methodology quite time consuming when applied to large reservoirs although we have retained all available detail here the long processing time could be ameliorated in future analyses by rebinning block averaging the models and observation images to a coarser grid the accuracy of the method at the resolution we consider here will be more limited by systematic limitations in the forward model as a fraction of total reservoir volume than by the size of the data the water elevations and volumes estimated in the keepit dam are quite accurate with rmse of 0 41 m and 2 91 hm3 this last value implying a mean residual of 2 5 of the maximum capacity of the dam in the study period however the range of observations in the data series are all less than half the maximum capacity of the dam and tend to be sparsely distributed due to several continuous years of rainfall deficits in the region wittwer et al 2020 additionally water elevations were slightly overestimated in the upper range and underestimated in the lower range of the measurements pamamaroo lake estimates on the other hand present a similar behaviour but in this case errors are higher and up to 0 51 m in water elevations and 32 45 hm3 in volumes which is around 10 of the reservoir maximum capacity the high dispersion in the upper range of predictions for pamamaroo lake is related to the presence of vegetation in the perimeter of the lake which tends to affect the water detection fig 16 leading to a sort of upper limit in the water level detection and underestimation of water elevations 4 discussion in this study as in ma et al 2019 and fuentes et al 2019 we have demonstrated that combining publicly available digital terrain models at high resolution and surface reflectance data allows quantification of on farm reservoir volumes since reservoirs can be accurately monitored using remote sensing data water governance institutions should move forward to survey agricultural areas using lidar to obtain high resolution elevation images over large extents this is important if the aim is to develop a transparent water governance system where consumers users and governmental organizations can actively participate together making water management decisions previous studies quantifying reservoir volumes have mostly concentrated on large reservoirs fuentes et al 2019 since the number of sources of reflectance data at high resolution have increased progressively in the last decades kavzoglu and yildiz 2014 this will result in the detection of smaller reservoirs and the quantification of volumes the methodologies presented in this study can be automatised for the detection of on farm reservoirs while the mcmc sampling through a simple linear relationship observed between flooded areas and a water index and the selection of an objective function can be used for the water elevation selection and quantification of uncertainties our results showed that the approach used was not really sensitive to the priors chosen on the contrary common issues derived from remote sensing data such as the misregistration of the images or the oversampling caused by combining different band resolutions were more important for the model performance thus the recognition of some major causes for error patterns driven by the data sources used are key components for improving the model applied through the processing of images and need to be considered when working with remote sensing datasets the validation of the results shows that vegetation affects the water detection and the selection of the water elevation which could be clearly observed in pamamaroo lake most on farm reservoirs should be clear of vegetation as this is suggested for efficient operation barrett 2007 while some herbaceous vegetation might be covering the bottom of on farm dams as they dry the vegetation at the perimeter of pamamaroo lake is mainly composed of woody species cunningham et al 2013 and therefore the same conditions that lead to a decrease in the accuracy of the detection in pamamaroo lake may not apply for small reservoirs despite these sources of errors the results for pamamaroo lake rmse 32 45 had better accuracy than those found using landsat images supervised classifications and sampling the elevations at the water perimeter of the reservoir rmse 48 72 fuentes et al 2019 additionally these results are in the low range of errors compared with avisse et al 2017 which reports mean relative errors ranging from 3 to 35 for storage and up to 49 for water elevation with regard to uncertainties in the reservoir volume modeling deng et al 2020 using landsat images 30 m resolution and shuttle radar topography mission dem data discussed and calculated different sources of uncertainty i seasonal and annual storage uncertainties from inundation areas and the standard deviation of water elevations ii filling gap uncertainties from average seasonal volume estimates and seasonal volumes retrieved from area volume curves and iii an overall annual and seasonal uncertainty from the ratio between the mean seasonal volume deviation and the maximum seasonal estimated volume however the reported error in that study is limited to highlighting the determination coefficient to indicate the accuracy in their estimates r2 0 9 the approach used in the present study is slightly different given the original assumption of a linear relationship between partially flooded areas and the water index which was empirically evaluated later and the dependence of partially flooded areas to the water levels we decided to emulate the water index from the water levels since this task allows us to optimise the water level selected by reducing the errors maximising the log likelihood we chose inference bayesian statistics mcmc to obtain date specific prediction distributions this methodology together with the high resolution of the terrain models used and the higher resolution of sentinel 2 images compared to landsat led to quite small uncertainties the reservoir volume monitoring can help closing the water budget at the field scale mdemu et al 2009 van beek et al 2011 this may generate better knowledge about the different water losses at the field scale including open water evaporation and seepage losses fowe et al 2015 and to estimate the water use efficiency for crops additionally it can be used to remotely detect irrigation events in paddocks and pumping events to fill the reservoirs however surface reflectance images of sentinel have a recurrence interval of at least 5 days copernicus 2017 additionally clouds may mask observations leading to larger gaps in the monitoring these gaps in the recording may obscure some of the actual water management decisions at the field scale such as irrigation events or lead to a lag in the observations the most significant stumbling block to apply the proposed methodology operationally is that it depends on two data sources which globally do not yet exist everywhere lidar images are snapshots of the earth surface covering small areas surveyed during specific campaigns while several locations in australia have been surveyed and made publicly available this is not common in other countries additionally only a fraction of the reservoirs in the catchment were empty when the lidar survey took place therefore most reservoirs do not have a publicly available terrain model of the bottom elevations which means the water volumes cannot be estimated there are different solutions to address this problem the most obvious is to increase the number of lidar survey campaigns at different times which might also improve our understanding of land processes such as vegetation growth and removal and elevation changes in the landscape however other alternatives have also been implemented for instance some studies have used interferometric synthetic aperture radar data to obtain bottom elevations in dry reservoirs yet this comes with higher error margins than the lidar data vanthof and kelly 2019 zhang et al 2016a b c others have made assumptions to generalize in the storage capacity area relationship in reservoirs liebe et al 2005 rodrigues et al 2012 these are based on the fact that the building of reservoirs follows engineering criteria specific for certain regions therefore these generalized relationships must be regarded as region specific but even within regions uncertainties in the predictions can be expected due to the generalization for example a generalization of the area volume relationship for reservoirs with an associated digital terrain model associated might be used for queensland and new south wales in australia to monitor on farm reservoir volumes in time this would expand the water volume estimates to those reservoirs that lack elevation information and the uncertainty in the predictions may also be approximated through bootstrap or monte carlo sampling techniques this together with the collection of real data from on farm reservoirs for validation are part of our ongoing research in this area 5 conclusions this paper demonstrates that surface reflectance data and its combination with lidar data at high spatial resolution can be used to automate the on farm reservoir detection and to derive a simple linear relationship between flooded pixel areas and water indices based on bands in the sentinel 2 images this can be further developed to estimate water elevation and volumes in on farm reservoirs and associated uncertainty using bayesian statistics moreover processing of the images and a prior knowledge of the datasets and parameters are important aspects to improve the mcmc performance and should not be neglected when working with remote sensing datasets the methodology can be used in different size reservoirs although applying the method to large reservoirs at full model resolution will require proportionally more computation time some degree of downsampling or block averaging of the available data will be advantageous when analyzing the largest reservoirs the developed methodology to quantify volumes in on farm reservoirs has a range of applications it allows irrigators to accurately estimate their farm scale water losses and to evaluate water use efficiency of crops it may also be used in a water governance system as a water auditing tool and it provide transparency in the water information chain between producers governmental institutions and consumers however additional validation of the proposed methodology is still required in small scale reservoirs which will be addressed in a future research code availability code associated with the present project can be found in the following repository https github com ifuentessr reservoir ems declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this project is part of the watersense project and has received funding from the european union s horizon 2020 research and innovation programme h2020 space 2018 2020 under grant agreement no 870344 appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105095 
25780,accurate estimates of water volumes are crucial for water management this study applies an automated methodology to detect small scale on farm dams and develops a novel application of bayesian inference to jointly simulate volumes and uncertainties a linear relationship was assumed between flooded pixel area images derived from lidar datasets and water index rasters derived from sentinel 2 images using a markov chain monte carlo mcmc method led to accurate estimates of water elevations and reservoir volumes with a systematic error of about 2 5 and 10 of the maximum capacity during the study period in keepit dam and pamamaroo lake respectively additionally the method quantifies uncertainties of volumes the presence of woody vegetation growing at the reservoir walls leads to a deterioration of estimates this methodology may be used as an auditing tool in water governance schemes or to gain knowledge on water losses at the field scale keywords mcmc on farm dams uncertainty volume estimation 1 introduction water is a strategic asset for agriculture which is increasingly being monitored by farmers since a gain in water use efficiency can lead to a reduction in production costs fan et al 2018 moreover since meteorological trends around the world indicate an intensification in the hydrological cycle ziegler et al 2003 yao et al 2019 rottler et al 2020 with a positive overall global trend of temperatures collins et al 2013 causing a potential positive feedback on evapotranspiration miralles et al 2014 zhang et al 2016a b c agricultural water demand is expected to increase in coming years this implies more pressure on water resources bindi and olesen 2011 misra 2014 and makes water monitoring at different scales increasingly necessary on farm dams in australia represent one strategy to mitigate uncertainty in water availability and to increase opportunities for irrigation pittock 2016 these have caused significant changes to the landscape and their effect on water availability and natural ecosystems is still being investigated kingsford 2000 porter 2002 they also imply a change in the hydrological cycle and are potentially linked to high evaporation losses craig et al 2005 fuentes et al 2020 however increased water use can also lead to water over exploitation either by a lack of knowledge about the availability of water resources or by deficiencies in the water governance schema chaffin et al 2014 porras et al 2018 damaging water resources and ecosystems gleeson et al 2012 pellicer martínez and martínez paz 2016 this has led to increasing public pressure to control over exploitation improve water resource management and to prioritization of different water uses to address water scarcity pahl wostl et al 2007 water governance systems play an important role to address the balance between available water and water needs through different levels of stakeholder participation rogers and hall 2003 several factors result in a higher stakeholder involvement to participate in water management plans such as the availability and accessibility of information and the different degrees of water stress pahl wostl et al 2007 carr et al 2012 however regardless of strong social involvement or institutions water auditing is needed as an important tool to manage over exploitation barrington et al 2013 alternatively water accountability at irrigation companies can help deal with current challenges including higher public scrutiny of the use of water resources this can foster increased transparency of water resource consumption addressing public concerns berg and phillips 2017 a key aspect in both strategies water auditing and water accountability is the availability of data to monitor water resources berg and phillips 2017 escriva bou et al 2020 conventional storage monitoring in reservoirs has been achieved through bathymetric or terrain elevation surveys and complemented by installing level meters or pressure transducers which can be used to derive storage capacities from level volume curves herschy 2012 however improvements in spatio temporal resolution of satellite products has increased the range of potential applications of remote sensing data beck et al 2000 rose et al 2015 karthikeyan et al 2020 chawla et al 2020 allowing monitoring of different processes at multiple scales entwistle et al 2018 distinct components of the water budget can be estimated using remote sensing data pekel et al 2016 zhang et al 2016a b c nevertheless monitoring of storages at the field scale using remote sensing data has increased only slowly due to limitations in the spatio temporal resolution rodrigues et al 2012 ogilvie et al 2018 for instance even though several satellite altimetry datasets exist these are not intended for inland water monitoring zhang et al 2020 most of them have satellite tracks largely spaced cazenave 2019 and moderate spatial resolutions along track 300 m in sentinel 3 and jason 3 dong et al 2018 kittel et al 2021 which limit their use in the monitoring of small reservoirs on the other hand most surface reflectance products have a spatial resolution of at least 10 m which makes it hard to accurately delineate the water limit in on farm dams with steep side walls in addition limited elevation data for the bottom of the dams also limits quantifying the water volumes fuentes et al 2019 even if several studies have addressed water detection with different degrees of accuracy mueller et al 2016 pekel et al 2016 huang et al 2018 a smaller number of studies have tried to go from surface water extent to surface water volumes karran et al 2017 fuentes et al 2019 pereira et al 2019 and even fewer have tried to quantify the uncertainties related to the estimates deng et al 2020 the integration of geographic information systems and remote sensing data layers in google earth engine run on the cloud through colab notebooks provide important infrastructure reducing local computational requirements and pre processing time and widening the integration of tools and functionality for analysis gorelick et al 2017 this study aims to evaluate a methodology developed for the automated detection monitoring and quantification of on farm dam water volumes and associated uncertainties based on available surface reflectance data from copernicus satellite sentinel 2 and lidar data which might allow governmental institutions and landholders to contribute to transparent water accountability 2 materials and methods 2 1 study area and data the focus area is the namoi catchment located in new south wales australia the catchment covers around 42 000 km2 fig 1 cotton production which strongly relies on the available water during the summer season hulugalle and scott 2008 forms one of the major land uses in the catchment because of this water dependence and the unpredictable nature of flows in the australian climate several landholders have built on farm dams to secure water during the growing season two main datasets were used in this study tiles of lidar data at 1 m horizontal resolution were obtained from the elevation and depth foundation spatial data https elevation fsdf org au these were mosaicked to cover larger areas of the catchment including the boggabri narrabri wee waa and walgett towns from east to west and their surroundings where agricultural land uses dominate the landscape fig 2 surface reflectance images from the sentinel 2 satellite sentinel 2 msi level 2a covering the catchment extent for the calendar year 2019 drusch et al 2012 were used to trace water reservoirs these included 1 514 ortorectified and atmospherically corrected scenes the sentinel 2 cloud probability dataset obtained using the s2cloudless library sanchez et al 2020 was also used to mask all pixels in the surface reflectance images with a cloud probability higher than 30 the scene classification map band contained in the images was further used to mask cloud shadows in the collection even after this some remnant cloud shadows and clouds remained in the collection once these masks were applied therefore an additional filter was applied to exclude images with more than 10 of pixels containing clouds through the cloudy pixel percentage property stored in the images which reduced the scenes to 935 images other ancillary datasets included a height above the nearest drainage hand topographic model obtained from srtm nobre et al 2011 the elevations from srtm the mcd12 land cover sulla menashe and friedl 2018 and the major hydrologic network and major reservoir vectors from the geofabric dataset bureau of meteorology 2014 finally reservoir monitoring data from the waternsw webpage https www waternsw com au including the daily water levels and volumes at pamamaroo lake and keepit dam were also used 2 2 automated on farm reservoir detection due to water scarcity reservoirs may be empty during the year because farmers adjust their cropping decisions based on the expectation of water in dry years crop production is reduced to cope with the water restrictions therefore we developed an automated methodology in google earth engine to trace active reservoirs in the monitoring year fig 3 the water index developed by fisher et al 2016 for australian areas highlighted in equation 1 performs well in eastern australia 1 w i 1 7204 171 ρ b 3 3 ρ b 4 70 ρ b 8 45 ρ b 1 1 71 ρ b 1 2 where w i is the estimated water index and ρ b3 ρ b4 ρ b8 ρ b11 ρ b12 correspond to the reflectance bands in the green central wavelength of 560 nm and 559 nm for the s2a and s2b sensors red 664 5 nm and 665 nm for the s2a and s2b sensors near infrared 835 1 nm and 833 nm for the s2a and s2b sensors short wave infrared 1 1613 7 nm and 1610 4 nm for the s2a and s2b sensors and short wave infrared 2 2202 4 nm and 2185 7 nm for the s2a and s2b sensors wavelength spectrum respectively the original index was developed using landsat images but given the similarity in the wavelength range of the landsat and sentinel 2 multispectral sensors we generalized it using the sentinel 2 collection fig 3 1 in the reservoir detection step once the water index was applied to the surface reflectance images fig 3 2 water masks were created fig 3 3 assuming pure water pixels to present values greater than 0 based on fisher et al 2016 in which a histogram of pixel values for purely water fully flooded and non water bodies was presented compared with other commonly used indexes for water detection such as the normalised difference water index ndwi or the modified normalised difference water index mndwi that combine only two reflectance bands the water index applied w i uses 5 reflectance bands in fisher et al 2016 w i achieved the best accuracy in the water detection compared against other indexes including ndwi and mndwi then the water mask collection generated from sentinel 2 images was used to generate a water occurrence probability map fig 3 4 defined by equation 2 2 p o 1 n i 1 n o i where po is the water occurrence probability between 0 and 1 n are the times that a clear unmasked pixel takes place in the image collection i corresponds to each observation in the collection being o i the occurrence of water in the observation i from the water occurrence probability map we took active on farm reservoirs to have a water occurrence probability greater than 0 2 i e a pixel containing water in more than 20 of unmasked observations a conservative value considering that on farm dams are mainly used for irrigation and most crops grow for at least 3 months fig 3 5 once pixels were delimited under these characteristics the remaining water bodies in the water occurrence probability map were vectorised fig 3 6 and filtered based on the following conditions 1 elevations 410 m 2 hand 50 3 water bodies not intersecting major river reservoirs and barren areas 4 0 01 km2 reservoir area 0 5 km2 5 ratio area perimeter 25 fig 3 7 some of these constraints are empirical such as the use of elevation in the catchment which limits the selection to agricultural reservoirs and disregards some reservoirs used for mining operations other criteria are generic and are associated with characteristics of on farm reservoirs such as their dimensions shapes and location with respect to water sources which avoids the selection of channels streams and other natural open water bodies from the active on farm reservoirs detected through this procedure we selected for this study only those reservoirs that were empty when the lidar data was acquired which meant the bottom elevations of the reservoirs could be captured 2 3 data pre processing and look up table generation to perform inference on the water level and the volume of water within each reservoir we propose a statistical forward model that mirrors an approximate version of the process generating the sentinel 2 surface reflectance images fig 4 our initial assumption given the water index differences between dry and flooded pixels was that the water index unlike the binary definition used in the reservoir detection step is linearly related to the flooded area within each sentinel 2 pixel this implies the water index represents a continuous numerical variable ranging between approximately 50 and 50 this assumption reflects the physics of the sensing process the water index is a linear combination of measurements made by the multi spectral instrument with each pixel combining the reflected radiance at different wavelengths of the spectrum within a single resolution element on the ground the distribution of residuals around this relation however may be set by differences in terrain or water properties at the extremes and will be discussed further below a raster map of the flooded area within each surface reflectance pixel can be calculated given a hypothesized water level using the lidar data the water volume can also be calculated by numerically integrating the lidar elevation profile beneath a given water level this forward model for flooded areas and water volumes retains sensitivity to the position of the water boundary at the fine resolution of the lidar data rather than the coarse resolution of the surface reflection data the method s sensitivity is particularly important for water boundaries on steep slopes such as for local reservoir walls to speed up our calculations we pre compute and cache a table of water volumes and flooded area raster images as functions of the water level for each dam the calculations assumed a set of water levels spaced on a grid at 10 cm intervals from the bottom elevation of the dam up to 0 8 m below the crest of the dam walls based on local design criteria barrett 2007 volumes were estimated in google earth engine by creating a binary mask image containing all pixels inside the dam in the lidar data pixels that were smaller or equal than the water level in the iteration were transformed into ones while pixels with elevations greater than the water level were filled with 0 the masked image was multiplied by the water level and the lidar elevations were subtracted from the values to get a water depth raster lastly the water depth raster was multiplied by the pixel areas and lumped to the dam border to obtain the water volume associated with the water level this resulted in water level volume curves for the reservoirs rasters of flooded pixel areas were generated in google earth engine for the look up table as follows all pixels at higher elevation than the assumed water level with values of 0 in the binary mask were masked out and the resulting flooded pixels with values of 1 were vectorised within the reservoir at 1 m horizontal resolution using a reduction to vectors since the high resolution of the lidar images resulted in noisy features a set of rules were used to clean the feature collections vectorised water level contours with less than 100 vertices in geometry collections and multipolygon features were filtered out while vectors in polygon features with less than 50 vertices were removed a comparison of the effect of these filters is presented in the supplementary materials subsequently the resulting flooded polygons were re rasterised and mosaicked with a dry raster layer a raster with 0 values in all pixels with the dimensions of the reservoir from the resulting raster the flooded pixel area image was generated by summing and converting the mosaicked raster of flooded pixels at the lidar resolution 1 m into the sentinel 2 resolution 10 m this process was repeated for each 0 1 m water elevation iteration the water index w i and set of partially flooded area rasters a i were sampled and transformed into arrays migrating from google earth engine into google colab notebooks for further processing these were combined with the corresponding level volume series in the look up table so that the flooded pixel areas depend on the water levels 3 a i f h where h corresponds to the water level and a i are the relative flooded areas of pixels for arbitrary water levels the water volumes and flooded pixel areas were interpolated linearly on the grid of the cached values fig 5 shows the empirical linear relationship between the areas of partially flooded pixels and the sentinel 2 water index lending support to our initial assumption this shows how in general fully flooded pixels tend to have positive water index values but with a large dispersion which may include some negative pixels caused by resolution differences between surface reflectance images and the terrain elevation or algae vegetation growth among others on the other hand dry pixels and partially flooded pixels show a direct linear trend mostly in the range of negative values however fully empty and fully flooded pixels tend to divert from the linear behaviour since the water index relies on a combination of bands at different resolutions 10 and 20 m it causes spatial autocorrelation in the water index images obtained at the higher resolution therefore the resolution of the water index images was calculated at 20 m we can improve upon this initial model by accounting for two additional effects first the sentinel 2 collection contains misregistration of the data leading to a slight spatial shift in some images we accounted for this by resampling the flooded pixel area images with a three tap lanczos kernel burger and burge 2010 based on two parameters a coordinate and an ordinate offset x off and y off both in pixels this resampling method retains detail in the flooded pixel rasters without requiring us to pre calculate them for every possible registration offset since lanczos resampling can cause ringing artifacts when applied to sharp edges the flooded pixel areas were first calculated on a 10 m grid and then blurred using a fast fourier transform convolution with a gaussian kernel of size s m pixels this convolution also accounts for any advection caused by the air column underneath the satellite especially in the near infrared wavelength spectrum which can affect the linear relationship between the flooded areas and the water index the resampled rasters were finally rebinned to match the 20 m resolution of the water index images 2 4 reservoir volume modeling and uncertainty analysis the inference of the water level and its uncertainty was subsequently performed in a bayesian framework bolstad and curran 2016 using markov chain monte carlo mcmc sampling through the emcee library for python foreman mackey et al 2019 given observed data d bayesian probability assigns a relative posterior probability p θ d to the model parameters θ according to bayes s rule 4 p θ d p d θ p θ where the likelihood p d θ is the probability distribution of the observed data d given assumed model parameters θ and the prior p θ is a probability distribution representing beliefs about θ before taking the data into account arising for example from expert knowledge or previous observations the constant of proportionality while not easily calculated is not needed if mcmc is used since most variables of interest can be calculated by computing averages over the posterior samples of some function of the sampled parameter vector we will first describe the elements of the model related to the likelihood the assumed linear relation between the water index and the flooded pixel area a i is expressed in equation 5 5 w i a b a i where w i is the predicted water index a and b are the intercept and slope of the linear regression since a represents the intercept of the relationship it corresponds to the dry pixels partial flooded area 0 which should be negative while b is the slope of a direct linear relationship and would have positive values these definitions will be considered when defining the priors since the noise is assumed to arise from variable characteristics of different categories of pixels in the images the variances of the fully flooded and fully empty pixel populations were assumed to be different from partially flooded pixels parametric expressions taking into account these differences between pixel categories are shown in equations 6 8 6 w empty exp a i a fsc 7 w full exp a i 1 a fsc 8 w partial 1 w empty w full where w empty w full and w partial are weights associated with the fully empty fully flooded and partially flooded pixels respectively while a fsc is a fractional scale of pixel area where a transition from linear to exponential behaviour occurs at the extreme pixels set to 0 05 or 5 of the area of a sentinel 2 pixel this transition scale is included to prevent discontinuities in the model predictions at parameter values where individual pixels transition between classes the final variance σ t o t 2 of the data corresponds to the sum of the weighted variance for each class 9 σ tot 2 w empty σ empty 2 w full σ f u l l 2 w partial σ 2 where σ empty σ full and σ are the standard deviation of fully empty fully flooded and partially flooded pixels based on these relationships we assume independent gaussian noise with variance σ tot 2 so that the log likelihood of our model is given by 10 log p w i a i a b σ σ empty σ full s m x off y off 1 2 i 1 n w i w i 2 σ tot 2 1 2 n log 2 π σ tot 2 where n is the length of the data and w i is the observed value of the water index in pixel i the selected priors for the evaluation were investigated as we progressed in the study by first assuming weakly informative uniform priors and examining the distribution of maximum a posteriori map parameter values across reservoirs for each parameter distributions for slope and intercept in the linear relationship were tested for several reservoirs similarly the deviation for different pixel classes were evaluated by sampling these classes in different dams additionally the offset priors in the lanczos resampling and the size of the gaussian kernel in the convolution were decided based on the learning stage derived from using the sentinel 2 collection the priors for the slope b and intercept a of the linear response of the water index per unit flooded area were assigned a log normal and a normal distribution respectively 11 a n 4 4 2 a n d b log n 0 135 0 2 2 the priors for standard deviations of fully flooded fully empty and partially flooded pixels were assigned uniform distributions 12 σ empty u 0 8 σ full u 0 4 a n d σ u 0 10 the priors for x off and y off used in the lanczos resampling and the width s m of the gaussian blurring kernel were also assigned uniform distributions 13 x off u 3 3 y off u 3 3 a n d s m u 0 02 3 lastly the prior for water elevation was based on a uniform distribution taken from the bottom elevation of the dam to the top water elevation at the dam from these priors and the mcmc sampling the water elevation in reservoirs was assumed to be the map elevation and the uncertainty was estimated from the distribution of water elevations at the 16th and 84th percentiles the difference between percentiles 16 and 84 or the 68 confidence interval corresponds to the mean 1 standard deviation for a gaussian distribution although we find the parameter distributions are not in general perfectly gaussian we pick these quantiles to correspond to a reader s expectation of the standard error on a parameter finally a linear interpolation based on the water level volume curves converted the simulated water elevations into volumes 2 5 validation of estimates since no measured water levels from on farm reservoirs are available in the study area the information of pamamaroo lake and keepit dam lake were included as validation data to directly assess the algorithm s performance pamamaroo lake is one of the so called menindee lakes which are artificial reservoirs located in western new south wales near the town of menindee latitude 32 41 longitude 142 42 connected to the darling river pamamaroo lake has been artificially modified to increase its storage capacity it is a regulated storage system that has a relatively flat bottom steep walls at the border fuentes et al 2019 an inlet regulator at its entrance and an outlet that is also regulated to allow the filling of other menindee lakes these characteristics make it comparable in structure but at a larger scale to the on farm reservoirs studied on the other hand keepit dam is an artificial reservoir located in the upper namoi catchment within the study region and connected to the namoi river fig 6 which is used to store water for agriculture and to control the streamflow in the namoi river it has a steep wall on its outlet a characteristic that is shared by on farm reservoirs and due to the higher elevations in the area the rest of the perimeter also has fairly sharp elevation changes https roads waterways transport nsw gov au documents maritime usingwaterways maps boating maps 15 lake keepit pdf time series of levels and volumes stored in these two reservoirs were downloaded and used as a reference to verify the volume predictions validation performance was based on the root mean squared error rmse the mean absolute error mae the bias of the predictions i e the mean error and the determination coefficient r2 3 results a map of water occurrences as percentage using sentinel 2 images is presented in fig 7 3 1 reservoir detection clearly the water index detects open water even though cloud shadows and some artifacts still exist in the source data the preprocessing techniques clouds and shadows masking and water occurrence thresholding mitigate these sources of error additionally using a threshold for the detection of seasonal water bodies further reduces the miss classification allowing us to focus on particular water features rather than on episodic sources of error the automatic detection of on farm reservoirs performed well this can be qualitatively demonstrated looking at specific agricultural areas that are densely irrigated fig 8 the filters applied to the water polygons vectorised from the water mask lead to a visually correct delineation of on farm dams additionally by setting a threshold to the ratio between the area and perimeter of the dams resulted in filtering out most agriculture channels billabongs and streams the methodology does not necessarily lead to a perfect delineation of reservoirs at full capacity because of the water occurrence threshold nevertheless it appears to provide a good detection of locations were active reservoirs occur 3 2 volume estimation and uncertainties the water elevation estimated as the map from the mcmc analysis varied between reservoirs and in time figs 9 and 10 present corner plots of the mcmc for specific reservoirs and dates in this case the distribution of water elevations from the reservoir images in the left pane of the figures has only a small uncertainty in the range of few centimeters for samples at one standard deviation away from the median the marginal distributions of most parameters show approximately gaussian behaviour some nontrivial covariances between parameters can be observed in the posterior for example the water level shows a negative correlation with the intercept and a positive correlation with the slope of the forward model for the detector sensitivity to flooded area flooded polygons associated with map water elevations selected from the mcmc are given for two reservoirs at different dates in figs 11 and 12 these also give the water elevation associated with the maximum log likelihood and how the log likelihood changes as a function of elevation in the second column of the panel additionally the relationship between flooded pixel areas and the water index is shown in the middle column panel which varies depending on the date and image while the histograms in the penultimate column of the panel show residuals normalised by the standard deviation of the linear relationship the processing of the flooded pixel area rasters clearly leads to a stronger linear relationship of flooded pixel areas and the water index compared to the relationship presented in fig 5 using unaltered surface reflectance images the selection of the water levels at reservoirs and the associated water polygons demonstrate a good representation of the relationship used in the mcmc however some slight differences can be observed between the reflectance images and the water polygons which may be the result from errors in the digital elevation model or changes in the reservoir surface caused by sedimentation processes or earthworks in the reservoir 3 3 time series since surface reflectance images from the sentinel 2 satellite have a capture repeat time of 5 days time series of water volumes in reservoirs can be obtained to evaluate water use and water orders in this case a single reservoir was selected to evaluate the time series fig 13 highlights in the upper panel surface reflectance data for a particular reservoir at different dates while the lower panel highlights the time series of volumes interpolated from the map water elevations and the 68th percentile confidence interval including the uncertainty derived from lidar images in the vertical dimension 0 2 m hodgson and bresnahan 2004 small changes in the reservoir volumes are detected using the proposed methodology in some cases water elevations can be detected even when remnant clouds are present however clouds shadows and empty pixels can affect the detected water elevation filling and irrigation events can be easily detected from these time series but they can be obscured by the repeat time of the satellite passes and the cloud presence 3 4 validation validation plots for the keepit dam and the pamamaroo lake are highlighted in figs 14 and 15 respectively however these validation reservoirs have different characteristics than on farm reservoirs one of the major differences and problems in relation to the mcmc is the size of the validation reservoirs which increases the computing time for processing tasks such as the convolution and the lanczos resampling even if these take a few seconds per image the mcmc requires thousands of iterations which makes this methodology quite time consuming when applied to large reservoirs although we have retained all available detail here the long processing time could be ameliorated in future analyses by rebinning block averaging the models and observation images to a coarser grid the accuracy of the method at the resolution we consider here will be more limited by systematic limitations in the forward model as a fraction of total reservoir volume than by the size of the data the water elevations and volumes estimated in the keepit dam are quite accurate with rmse of 0 41 m and 2 91 hm3 this last value implying a mean residual of 2 5 of the maximum capacity of the dam in the study period however the range of observations in the data series are all less than half the maximum capacity of the dam and tend to be sparsely distributed due to several continuous years of rainfall deficits in the region wittwer et al 2020 additionally water elevations were slightly overestimated in the upper range and underestimated in the lower range of the measurements pamamaroo lake estimates on the other hand present a similar behaviour but in this case errors are higher and up to 0 51 m in water elevations and 32 45 hm3 in volumes which is around 10 of the reservoir maximum capacity the high dispersion in the upper range of predictions for pamamaroo lake is related to the presence of vegetation in the perimeter of the lake which tends to affect the water detection fig 16 leading to a sort of upper limit in the water level detection and underestimation of water elevations 4 discussion in this study as in ma et al 2019 and fuentes et al 2019 we have demonstrated that combining publicly available digital terrain models at high resolution and surface reflectance data allows quantification of on farm reservoir volumes since reservoirs can be accurately monitored using remote sensing data water governance institutions should move forward to survey agricultural areas using lidar to obtain high resolution elevation images over large extents this is important if the aim is to develop a transparent water governance system where consumers users and governmental organizations can actively participate together making water management decisions previous studies quantifying reservoir volumes have mostly concentrated on large reservoirs fuentes et al 2019 since the number of sources of reflectance data at high resolution have increased progressively in the last decades kavzoglu and yildiz 2014 this will result in the detection of smaller reservoirs and the quantification of volumes the methodologies presented in this study can be automatised for the detection of on farm reservoirs while the mcmc sampling through a simple linear relationship observed between flooded areas and a water index and the selection of an objective function can be used for the water elevation selection and quantification of uncertainties our results showed that the approach used was not really sensitive to the priors chosen on the contrary common issues derived from remote sensing data such as the misregistration of the images or the oversampling caused by combining different band resolutions were more important for the model performance thus the recognition of some major causes for error patterns driven by the data sources used are key components for improving the model applied through the processing of images and need to be considered when working with remote sensing datasets the validation of the results shows that vegetation affects the water detection and the selection of the water elevation which could be clearly observed in pamamaroo lake most on farm reservoirs should be clear of vegetation as this is suggested for efficient operation barrett 2007 while some herbaceous vegetation might be covering the bottom of on farm dams as they dry the vegetation at the perimeter of pamamaroo lake is mainly composed of woody species cunningham et al 2013 and therefore the same conditions that lead to a decrease in the accuracy of the detection in pamamaroo lake may not apply for small reservoirs despite these sources of errors the results for pamamaroo lake rmse 32 45 had better accuracy than those found using landsat images supervised classifications and sampling the elevations at the water perimeter of the reservoir rmse 48 72 fuentes et al 2019 additionally these results are in the low range of errors compared with avisse et al 2017 which reports mean relative errors ranging from 3 to 35 for storage and up to 49 for water elevation with regard to uncertainties in the reservoir volume modeling deng et al 2020 using landsat images 30 m resolution and shuttle radar topography mission dem data discussed and calculated different sources of uncertainty i seasonal and annual storage uncertainties from inundation areas and the standard deviation of water elevations ii filling gap uncertainties from average seasonal volume estimates and seasonal volumes retrieved from area volume curves and iii an overall annual and seasonal uncertainty from the ratio between the mean seasonal volume deviation and the maximum seasonal estimated volume however the reported error in that study is limited to highlighting the determination coefficient to indicate the accuracy in their estimates r2 0 9 the approach used in the present study is slightly different given the original assumption of a linear relationship between partially flooded areas and the water index which was empirically evaluated later and the dependence of partially flooded areas to the water levels we decided to emulate the water index from the water levels since this task allows us to optimise the water level selected by reducing the errors maximising the log likelihood we chose inference bayesian statistics mcmc to obtain date specific prediction distributions this methodology together with the high resolution of the terrain models used and the higher resolution of sentinel 2 images compared to landsat led to quite small uncertainties the reservoir volume monitoring can help closing the water budget at the field scale mdemu et al 2009 van beek et al 2011 this may generate better knowledge about the different water losses at the field scale including open water evaporation and seepage losses fowe et al 2015 and to estimate the water use efficiency for crops additionally it can be used to remotely detect irrigation events in paddocks and pumping events to fill the reservoirs however surface reflectance images of sentinel have a recurrence interval of at least 5 days copernicus 2017 additionally clouds may mask observations leading to larger gaps in the monitoring these gaps in the recording may obscure some of the actual water management decisions at the field scale such as irrigation events or lead to a lag in the observations the most significant stumbling block to apply the proposed methodology operationally is that it depends on two data sources which globally do not yet exist everywhere lidar images are snapshots of the earth surface covering small areas surveyed during specific campaigns while several locations in australia have been surveyed and made publicly available this is not common in other countries additionally only a fraction of the reservoirs in the catchment were empty when the lidar survey took place therefore most reservoirs do not have a publicly available terrain model of the bottom elevations which means the water volumes cannot be estimated there are different solutions to address this problem the most obvious is to increase the number of lidar survey campaigns at different times which might also improve our understanding of land processes such as vegetation growth and removal and elevation changes in the landscape however other alternatives have also been implemented for instance some studies have used interferometric synthetic aperture radar data to obtain bottom elevations in dry reservoirs yet this comes with higher error margins than the lidar data vanthof and kelly 2019 zhang et al 2016a b c others have made assumptions to generalize in the storage capacity area relationship in reservoirs liebe et al 2005 rodrigues et al 2012 these are based on the fact that the building of reservoirs follows engineering criteria specific for certain regions therefore these generalized relationships must be regarded as region specific but even within regions uncertainties in the predictions can be expected due to the generalization for example a generalization of the area volume relationship for reservoirs with an associated digital terrain model associated might be used for queensland and new south wales in australia to monitor on farm reservoir volumes in time this would expand the water volume estimates to those reservoirs that lack elevation information and the uncertainty in the predictions may also be approximated through bootstrap or monte carlo sampling techniques this together with the collection of real data from on farm reservoirs for validation are part of our ongoing research in this area 5 conclusions this paper demonstrates that surface reflectance data and its combination with lidar data at high spatial resolution can be used to automate the on farm reservoir detection and to derive a simple linear relationship between flooded pixel areas and water indices based on bands in the sentinel 2 images this can be further developed to estimate water elevation and volumes in on farm reservoirs and associated uncertainty using bayesian statistics moreover processing of the images and a prior knowledge of the datasets and parameters are important aspects to improve the mcmc performance and should not be neglected when working with remote sensing datasets the methodology can be used in different size reservoirs although applying the method to large reservoirs at full model resolution will require proportionally more computation time some degree of downsampling or block averaging of the available data will be advantageous when analyzing the largest reservoirs the developed methodology to quantify volumes in on farm reservoirs has a range of applications it allows irrigators to accurately estimate their farm scale water losses and to evaluate water use efficiency of crops it may also be used in a water governance system as a water auditing tool and it provide transparency in the water information chain between producers governmental institutions and consumers however additional validation of the proposed methodology is still required in small scale reservoirs which will be addressed in a future research code availability code associated with the present project can be found in the following repository https github com ifuentessr reservoir ems declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this project is part of the watersense project and has received funding from the european union s horizon 2020 research and innovation programme h2020 space 2018 2020 under grant agreement no 870344 appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105095 
25781,the wildland urban interface wui is defined as a geographic area where human developments and flammable vegetation merge in a wildfire prone environment losses due to wildfire have been rising in the past decade attributed to changes in vegetation growth fuel availability and increased land developments in wui this paper studies the process of wildfire spread inside wui communities the fire spread rate within wui communities is determined for nine wildfires that were ranked among the most destructive wildfires in north america an improved quasi empirical model that considers radiation and fire spotting as modes of fire spread inside a community is proposed the new model is validated using the documented spread rates during the 2007 witch and guejito fires and the 2017 tubbs fire the proposed model is computationally efficient and can be used to quantify fire spread rate and the number of affected structures inside a community during a wildfire event keywords wildfires wildland urban interface community fire spread modeling 1 introduction wildfires have always been part of the natural landscape for a healthy ecosystem yet these fires are projected to become more frequent and intense due to changing weather patterns and human suppression activities in the past century the economic and social impacts of wildfires have been rising in recent years and now represent a global concern as of november 2020 6 out of the 10 most destructive wildfires in the us in terms of insured losses happened in 2017 and 2018 also 13 out of the 20 most destructive fires in terms of structure losses happened since 2017 insurance information institute 2020 california department of forestry and fire protection 2020 direct and indirect consequences of wildfires including utility disruptions and environmental impacts have been significant masoudvaziri et al 2020 the current approach in managing the fire hazard within communities is neither sufficient nor sustainable over the past several decades fire safety research has spent a great deal of effort to understand fire dynamics inside buildings in parallel significant research has been conducted to understand wildfire behavior in the wildland area cruz et al 2018 bakhshaii and johnson 2019 but research on large outdoor fires within communities e g due to wildfires and development of codes and standards for such fires lag behind a workshop by the american society for testing and materials astm concludes that current codes and standards for communities susceptible to wildfire risk are not adequate manzello and quarles 2015 the wildland urban interface wui is defined as a geographic area where human developments and flammable vegetation merge in a wildfire prone environment international code council 2018 not all the wui communities are threatened by wildfires as several conditions such as the risk of extreme fire events and the lack of protection and defense for buildings inside wui communities should occur concurrently cohen 2008 stein et al 2013 caton et al 2017 in this paper any mention of communities or wui communities refers to the wui communities that are affected attacked by wildfires although extreme fire hazards cannot be eliminated the amount of incurred damage depends on decisions made by policymakers and engineers and the attitude towards risk the us departments of agriculture usda and interior doi developed the national fire plan usda and doi 2007 after the devastating destruction of communities due to wildfires in 2000 in 2001 a 10 year comprehensive strategy plan was developed to reduce wildfire risks to communities and the environment usda and doi 2007 this effort led to the healthy forests restoration act of 2003 which developed a framework for community wildfire protection plans cwpps that calls for communities to work collaboratively to develop cwpps the plan must identify and prioritize fuel treatment and set forth strategies to reduce the ignitability of houses and other infrastructure ushf 2006 in may 2016 the white house 2016 issued an executive order to mitigate wildfire risks to federal buildings located in the wui reduce risks to people and help minimize property loss due to wildfire as a result of these efforts a number of programs such as firewise nfpa 2021 have been developed to provide guidance to communities for wildfire mitigation actions yet the federal programs have focused more on wildland fire management e g prescribed fires and not sufficiently addressed the equally important community aspects this is affirmed by the increasing wui losses in recent years while the impacts of fire on communities lives and properties can be managed by better understanding the problem and establishing proper policies mathematical models are needed to investigate the process of fire spread in a wui community in a wui community a fire line advances based on three primary mechanisms thermal radiation direct flame contact and firebrands a comprehensive model would integrate these three mechanisms of fire spread but such a validated model for application to wui communities does not exist yet empirical fire spread models have been developed for the problem of fire following earthquake but they have not been validated for wui communities under wildfires many of the existing fire spread models are semi empirical lee et al 2008 mahmoud and chulahwat 2020 with a few coarse attempts to include physics based processes such as spotting and thermal radiation mahmoud and chulahwat 2018 li and davidson 2013a 2013b himoto and tanaka 2008 iwami et al 2004 lee and davidson 2010a 2010b nussle et al 2004 otake et al 2003 the majority of urban fire spread models are developed in japan for post earthquake fire spread and are suitable for an urban environment involving equally spaced and equally sized square buildings in dense urban areas hamada 1975 himoto et al 2008 lee and davidson 2010b have developed a more comprehensive spread model for post earthquake fires the spread model considers fire evolution within a room from room to room inside a building and from building to building by flame spread but the level of required input information implies a number of assumptions hindering practicality for application to a community this paper starts with assessing the suitability of an existing urban fire spread model to predict fire progression inside a wui community fire spread rate within communities is determined for nine wildfires ranked among the most destructive wildfires in north america the same nine fires are analyzed using a simplified approach to assess the validity of an existing model in addition the paper proposes a quasi empirical model as categorized per sullivan s review sullivan 2009 which considers radiation and firebrand mechanisms as modes of fire spread inside a community the proposed model is validated using the documented spread rates during the 2007 witch and guejito fires and the 2017 tubbs fire the two selected communities as part of the validation study have different characteristics in terms of layout and building density wind velocity during the wildfire event etc demonstrating the applicability of the model to capture different scenarios 2 fire spread rate during recent fires in wui communities fire spread rate fsr inside a wui community refers to how fast the fire travels inside that community the fsr in this study is calculated as the distance between coordinates of two locations in the community where fire spread occurred divided by the corresponding time duration for which the fire traveled between the two coordinates the value of fsr can vary during a fire event due to heterogeneity in land cover topography and wind patterns therefore the mean value should be taken to characterize the event within a community a fast approaching fire from the wildland reduces the available time for evacuation and for first respondents to control the fire propagation when ignitions occur faster than the number of structural fires that are being put out by the fire crews a conflagration can occur within a neighborhood and the fire becomes out of control fire spread data within the wildland is available for most of the historic wildfire events and is often represented as increments in the burned area over a specific period of time usually in the range of several hours fire contours published by agencies including but not limited to the california department of forestry and fire protection cal fire the united states geological survey usgs and the u s forest service show the borders of regions that have been burned by a fire at a certain point in time during the fire event the precision of this information and satellite images are usually adequate to describe and study the fire in the wildland but that is not the case for most of the fires inside a community due to the much higher spatial and temporal resolution required despite its importance the fsr inside communities affected by wildfires is not always reported and in many cases precise information to determine the fsr is not available the required information on fire propagation and characteristics of communities to calculate fsr can be obtained from different sources such as reconnaissance reports satellite images aerial images from unmanned aircraft systems and social media satellite images are reliable sources of information however current satellite images do not offer an ideal temporal resolution for tracking the advancement of fire inside a community in most cases two different satellite images of the same fire event are separated by a gap of several hours which allows for monitoring fire spread on the wildland but do not capture the evolution of an event within a wui community it should also be noted that the current spatial resolution of freely available satellite images is approximately one pixel for every 10 60 m gisgeography 2021 considering the size of typical wui structures with an average length between 16 and 25 m some information could be lost if satellite images with lower resolution were used based on the above satellite images were used to quantify the fsr in wui communities for nine of the most damaging recent fire events in the us as listed in table 1 details of data sources for the nine events are provided in szasdi bardales 2019 and the associated wind speeds are provided in table 2 meanwhile scawthorn 1987 provides a list of historic 20th century north american urban fire events for which data were available to calculate the fsr this list includes events as old as the 1900 ottawa and hull fires in canada to a fire event in 1985 in philadelphia pa 3 existing simplified fire spread models simplified models are usually developed using empirical approaches and by performing regression analysis of data from past events such an approach was used as the first attempt in modeling inter building fire spread for urban scenarios dating back to the middle of the 20th century one of the characteristics of simplified models is that they are straightforward to implement without the need for complex computational platforms the empirical equations in these models relate the fsr with the parameters that are known to influence fire propagation the precision in these models is directly related to the reliability of the input data and the number of time steps used for the analysis in general the existing simplified fire spread models for application in community fires were generated specifically for the case of post earthquake fires the two most commonly used simplified models are the hamada model hamada 1975 and the tosho model tokyo fire department 1997 this paper will focus on the hamada model while more information on the tosho model can be found in tokyo fire department 1997 it should be noted that the hazus program developed by federal emergency management agency also implemented the same version of the hamada model to assess the risk of fire spread after an earthquake in a community scawthorn 2009 fema hazus 2014 the hamada model characterizes fire spread as a function of wind velocity and the average distances between buildings in a community hamada 1975 the fsr coefficients are determined based on empirical relationships from past japanese urban fire events including the fire after the 1923 kanto earthquake and wartime fires scawthorn 2005 the model assumes an elliptical shape for the fire expansion and the spread is faster in the downwind direction aligns with the major axis of the ellipse and slower in both the sidewind and upwind directions the model simplifies the locations of the structures in a regular grid all the structures are considered to have the same construction material and dimensions the model does not consider the streets as fire barriers and therefore street layouts are not included as an input as an input the model requires the user to provide information on the average length of one side of the buildings the average distance between buildings the wind velocity and the average building density ratio within the community represented by the built upness factor for brevity the series of equations are not provided here and can be found in szasdi bardales 2019 the hamada model was applied to the nine recent fire cases discussed in section 2 detailed data such as building layout in the community and wind speed during the fire event was collected to simulate the fire events the predicted fsr based on the hamada model is listed in table 2 a similar exercise was performed by scawthorn in 1987 scawthorn 1987 the hamada model s predicted fsr for 32 historical 20th century cases was also collected and included in this study fig 1 shows the comparison of the observed versus predicted fsr values for the nine investigated fire events in color as well as older 20th century fires in gray scawthorn 1987 concluded that the hamada model predictions agreed well with most of the observed fsrs except for a group of cases in which fire spread by firebrands between wooden buildings was significant meanwhile it is clear that in the majority of recent fires six out of nine wildfire events the model underestimates the fire spread rate within the communities and is not able to capture the true behavior see table 1 and fig 1 most of these recent fires in the us and the canadian communities correspond to wui wildfires at times of high winds and dry weather conditions that significantly differ from those on which hamada s model is based on the horse river fire 2016 tubbs fire 2017 and camp fire 2018 especially accentuate the level of such underestimation with an average error of 320 these results show a necessity for developing a new model that is capable of replicating the characteristics of modern wui fires 4 the proposed streamlined wildland urban interface fire tracing swuift model this section proposes a new quasi empirical model to simulate fire propagation in wui communities the proposed model is validated against observed data the primary mechanisms of fire spread between buildings are thermal radiation fire spotting by firebrands and direct flame contact the current version of the proposed model incorporates thermal radiation and firebrands while future work will expand on the model to include direct flame contact an efficient fire spread model should balance between fidelity the availability of input data and fast execution the proposed model has been developed to apply physics based principles while maintaining some of the advantages of simplified models to have an efficient implementation and to reduce the computational cost the proposed model simplifies the community layout into a regular grid of cells the content of each cell can take one of the three pre defined fuel models structure vegetation and non combustible the model captures the dynamic nature of a fire event by predicting the status of the fire within the community at individual time steps of 5 min the model records the thermal radiative flux and the firebrands that have landed at any cell for every time step and determines the status of individual structures and the duration of a fire in a specific structure it also accounts for variable wind speed at every time step the following sections provide details of the model mechanics implemented fire spread modes and the corresponding assumptions in the model 4 1 layout and wind the proposed swuift model characterizes fire events by two features 1 community layout and 2 wind speed and direction a given community layout is rasterized into 10 10 m grids the selected spatial resolution of 10 m captures a reasonable level of granularity while keeping the computational cost manageable here three fuel models are introduced structure vegetation non combustible the fuel model is a parametrization of different properties of the fuel bed and facilitates fuel bed classifications to be used as an input to the spread model scott 2005 each grid cell is assigned a fuel type based on the dominant larger than 50 land cover in that cell for this purpose the landfire s 13 anderson fire behavior fuel model anderson 1981 reeves et al 2009 rollins 2009 and microsoft s building footprints microsoft 2019 are retrieved and processed vegetation is defined as any vegetation with a height greater than 1 ft the non combustible classification represents land covers such as parking lots roads water bodies etc fig 2 demonstrates the application of the defined fuel model to the trails community in california to be discussed in detail in section 5 wind data is the other input for the model which should be provided for every time step forecast data can be used for predicting scenarios and planning applications while historic data from the closest weather station to a community can be used to replicate an event the model requires average wind speed and 3 s wind gust as inputs where the wind speed is randomly selected for each time step assuming 80 likelihood for average wind speed and 20 likelihood for the wind gust 4 2 mechanics of the model the swuift model simulates the fire spread and similar to other spread models requires the initial ignition s grid cell location and time step as an input ignitions can be defined both outside the community i e an approaching fire line from the wildland and inside the community i e where firebrands cause ignition ahead of the wildfire perimeter reaching the community the model is capable of accounting for any number of ignitions over the duration of the simulation once an ignition is defined the model tracks the fire development a structure cell can be ignited by either radiative heat or firebrands and when the fire is at the fully developed phase the cell emits energy and generates firebrands a vegetation cell can only be ignited by firebrands and when burning it generates firebrands noncombustible cells do not ignite and have no positive contribution to the fire spread all structures in the wui community are assumed to be residential housings each structure cell is coded to behave as a burning compartment of 10 10 m with the configuration illustrated in fig 3 the compartment boundaries are assumed to be 10 cm thick normal wood the typical temperature time evolution for the assumed configuration is generated using ozone cadorin and franssen 2003 cadorin et al 2003 ozone is a zone model taking into account the fuel amount and type openings and thermal properties of boundaries and calculates the fire evolution the adopted temperature time curve for the model is shown in fig 4 based on the compartment fire analysis it takes 22 min 5 timesteps for the fire to reach the maximum temperature the fire is then in a fully developed phase for 31 timesteps followed by the cooling phase the structure cell is assumed to contribute to fire spread by radiating energy and generating firebrands only during the fully developed phase due to its floor area a typical house in a wui community is usually represented by a few connected structure cells the raster matrix keeps track of the structure cells that belong to the same house when the fire in any of these cells reaches a fully developed stage the rest of the cells belonging to the same house will ignite unless they have been ignited earlier due to radiation or firebrands from other sources this mechanism is implemented to mimic the fire spread inside a house vegetative fuels e g piles of needles and leaves burn faster than structures due to larger aspect ratios better ventilation and larger porosity compared to structure fuel albini and reinhardt 1995 manzello et al 2007 mell et al 2009 dietenberg 2010 morandini et al 2013 therefore it is assumed that a burning vegetation cell in the model contributes to the spread only by firebrand generation and excludes radiation contribution given the limited available data a vegetation cell burns in 1 timestep after ignition i e 5 min the swuift model follows a step by step procedure in each timestep of the simulation 1 ignitions are input to the model and can be added at any timestep these known ignitions usually include initial ignitions at time zero where the wildfire line approaches the community or are caused by firebrands received inside the community ahead of the fire 2 if a structure cell which is part of a particular house reaches the fully developed phase of the fire the rest of the house cells that are not ignited by radiation or firebrands will ignite 3 contributions of burning cells are calculated at each timestep burning vegetation cells generate firebrands and structure cells with fully developed fire generate both radiative heat and firebrands 4 the accumulated energy in unignited structure cells received from the burning structure cells is calculated and if the specified threshold is reached the cells are ignited 5 generated firebrands from burning vegetation cells and structure cells land further down the fire line into the community following the wind direction and if enough firebrands are accumulated unignited cells vegetation and or structure ignite fig 5 summarizes the procedure listed above details of the model are further described in the following subsections 4 3 radiation thermal radiation is the physical phenomenon by which thermal energy is transferred from one body to another in the form of electromagnetic waves at any given time the heat flux at a receiving surface is given by 1 q ϕ ε e σ t e 4 where q is the radiant heat flux in w m2 ϕ is configuration factor ε e is the emissivity of the emitting surface σ is the stefan boltzmann constant 5 67 10 8 w m2k4 and t e is the absolute temperature of the emitting surface in k the emissivity refers to the efficiency of a surface as a radiator and its value is in the range of 0 1 during fire conditions most of the hot surfaces such as smoke or flames have an emissivity between 0 7 and 1 0 buchanan 2017 the configuration factor which accounts for cases in which the emitting surface is not directly facing the receiving surface can be calculated based on buchanan 2017 as mentioned earlier the swuift model assumes that a burning structure cell only emits radiation during the fully developed phase of the fire fig 4 shows the assigned temperature time evolution of fire to compartments in the study where the fully developed fire phase starts 5 timesteps after the ignition and lasts for 155 min 31 timesteps the amount of thermal radiative flux received at a given cell is then determined at every time step using eq 1 although thermal radiation is independent of the wind speed and direction the resulting flames are indeed influenced by the wind in general flames will be oriented in the direction at which the wind is blowing to account for the effect of wind on the geometry of the flames it is assumed that the emitting surface is rectangular in shape and is perpendicular to the direction of the wind therefore only the structures located in front of the emitting surface and including 90 with respect to the wind direction i e half plane along the wind direction will receive radiation waterman 1969 observed that a spontaneous ignition of various woods due to a radiant exposure generally occurs when the exposed surface reaches a heat flux of 0 8 cal cm2 sec which is equivalent to approximately 12 5 kw m2 although waterman observed that ignition occurred after 1 min of exposure to the aforementioned levels of radiation further research offered more details on the relation of the radiant heat flux and the required time of exposure for spontaneous ignition of wood quintiere 2006 it was shown that the critical ignition fluxes across and along the wood grains are 12 0 kw m2 and 9 0 kw m2 respectively the model calculates the aggregated flux at a given surface as the addition of all the individual fluxes generated by all the structure cells that are burning within the fully developed phase of the fire over time if the flux at any specific point in time exceeds a threshold ignition will occur the considered value for the critical ignition flux is 14 kw m2 slightly larger than the reported values in tests which are for bare materials considering tiling or finishing 4 4 fire spotting fire spotting is proved to be an important mechanism of fire spread in wui cohen 2008 koo et al 2010 maranghides et al 2015 caton et al 2017 a firebrand or an ember is a small incandescent particle that has separated from a burning material and transported by wind if a firebrand lands over a combustible material it could cause an ignition spreading the fire to an area distant from the original location fire spotting is a mechanism of fire spread that is difficult to model therefore to this date there is no available definitive physics based procedure for modeling the three phases of spotting generation transport and ignition in a holistic approach the three phases should be properly accounted for in a fire spread model to successfully replicate the occurrence of ignitions during a fire event for the purpose of this study a combination of experimental data and probabilistic models is adapted to quantify ignition due to firebrands the process of fire spotting starts with particles of burning material detaching from the fuel body they are then transported by the atmospheric turbulence intensified by the fire eventually the particles will exit the turbulent flow and land on surfaces the maximum height at which the particles elevate and the characteristics of their trajectory are influenced by the characteristics of the fire weather as well as their shape and density which in turn are closely related to the specific characteristics of the vegetation or material that is burning an ignition can occur if one or several incandescent firebrands land over a combustible material some firebrands would have been completely consumed through combustion and their temperature would have considerably reduced when they land lowering or even eliminating the probability of ignition caton et al 2017 hakes et al 2017 given the number of involved contributing factors and the substantial variation at different stages of spotting many studies have been conducted focusing on one of the three phases these studies generally include data collection from real incidents or experimental works studies have investigated various features of firebrands such as mass size type of material which will be discussed in the following sections in the swuift model both vegetation and structure cells generate firebrands and in return can potentially get ignited by firebrands the implemented firebrand model in this study is based on the review and analysis of an extensive collection of studies considering the involved uncertainty and unknowns and to avoid the expensive computational cost the mass of firebrands is used as the key parameter in the model while assuming that the firebrands are identical in shape the following sections provide details of the simulated procedure for fire spotting 4 4 1 generation of firebrands different experimental studies in laboratory or field have been conducted to investigate firebrand generation from different fuels and their characteristics such as mass size projected area number of particles etc various features affect the characteristics of a firebrand such as the surrounding airflow fire intensity fuel moisture content and fuel type some studies have focused on vegetative fuels manzello et al 2009 el houssami et al 2016 filkov et al 2017 bahrani 2020 hudson and blunck 2020 and some on structural fuels yoshioka et al 2004 suzuki et al 2013 2014 suzuki and manzello 2016 2021 despite different conditions and high variations in experiment setups a common finding among these studies is that the majority of generated firebrands are small 1 g with heavy right tail distributions in both mass and cross sectional area this finding is aligned with reports compiled after fire incidents foote et al 2011 rissel and ridenour 2013 hasemi et al 2014 suzuki 2017 moreover the traveling distance of firebrands have been investigated and no definitive relationship or significant correlation between distance and particle properties e g mass and size have been found suzuki et al 2012 hedayati 2018 suzuki and manzello 2018 zhou et al 2019 putting the results of these studies together and taking the computational efficiency of the proposed model into account it is assumed that identical firebrands with a mass of less than 1 g are generated from both structure and vegetation cells the fuel models are differentiated by the number of firebrands generated in each timestep the generation of firebrands by a structure cell is modeled based on the relation proposed by lee 2009 describing the number of firebrands produced from a single building as a function of the wind speed the relationship was derived based on the experimental findings of waterman 1969 waterman 1969 reported results of an experimental study on firebrand generation by various roof constructions including full scale segments of wood shingles asphalt shingles roll roofing cement asbestos shingles built up roofing and no covering firebrand production was studied under the internal pressure of the fire chamber and with additional pressures to simulate more intense fires and or wind effects the produced firebrands were trapped by a screen enclosure and dropped into a quenching pool all firebrands were sorted dried and weighed lee and davidson 2010b proposed equation 2 for the total number of generated firebrands from a structure based on the gathered data from the experiments described above 2 n b t o t a l 306 77 e 0 1879 v a r o o f where v is the wind speed in m s 1 and a r o o f is the roof area in m2 the roof area is calculated for the defined stereotype building however the wind speed varies over each timestep a burning structure cell generates a firebrand only during the fully developed phase of the fire i e firebrand generation is time dependent based on the discussion provided in section 4 2 thus equation 3 is used to calculate the number of firebrands from a structure cell in a given timestep where 31 is the number of timesteps that a structure cell burns as a fully developed fire 3 n b s t r i 306 77 e 0 1879 v i a r o o f 31 here v i is the wind speed at time step i in m s 1 a recent numerical study wickramasinghe 2020 conducted a reverse analysis to calculate the number of firebrands generated from a douglas fir tree by tuning inputs of their model to match the data by the national institute of standards and technology nist manzello et al 2007 mell et al 2009 the study uses the fire dynamic simulator fds model to simulate and analyze the fire mcgrattan et al 2013 and the simulation follows a specific distribution of mass to produce firebrands it is determined that a tree with circular base of 1 5 m in diameter and a height of 2 6 m generates about 87 g of firebrands assuming that a 10 m 10 m vegetation cell is fully covered with vegetation equation 4 provides n b v e g the number of firebrands generated from a vegetation cell 4 n b v e g 87 m b 10 2 0 75 2 π 4923 m b where m b is the mass of a firebrand in the model 4 4 2 transfer mechanism of firebrands himoto and tanaka 2005 proposed a probabilistic approach for determining the landing distribution of firebrands simulations of firebrands scattered in a turbulent boundary layer showed that the travel distances of firebrands in the downwind and the sidewind directions could be described by a lognormal and a normal distribution respectively himoto and tanaka 2005 in this approach the downwind distribution is a function of particle s properties such as width and density as well as surrounding s such as wind speed and heat source dimension and the sidewind distribution is independent of wind speed and particle s features himoto and tanaka 2005 in a cellular automata model inspired by the aforementioned model zhao 2011 proposed a simplified ellipse to characterize firebrand travel distance which was parametrized as a step function of wind speed as a proxy to account for the effect of firebrand spotting this model is also implemented in another cellular automata model on wui fire spread jiang et al 2020 in the present work the following lognormal and normal distributions are implemented for the dispersion of firebrands generated from a burning cell in downwind x and sidewind y directions respectively 5 p x 1 x σ x 2 π exp l n x μ x 2 2 σ x 2 0 x 6 q y 1 2 π σ y exp y μ y 2 2 σ y 2 0 y in the above equations μ x ln 30 v where v is the wind speed in m s 1 σ x 0 3 μ y 0 and σ y 4 85 the output values measure the distance from a cell s center point in meters for example a generated value of 60 m from equation 5 for a northerly wind implies that the firebrand lands on the 6th cell south of its origin each cell is 10 m long 4 4 3 ignition mechanism due to firebrands multiple experiments have been carried out to study the ignitability of fuel beds of different types and configurations and the potential of firebrands to cause ignition given their features such as mass size burning status flaming glowing smoldering ellis 2015 hernandez et al 2018 urban et al 2019 some studies compared the potential of ignition by a single firebrand versus a pile of firebrands manzello et al 2006 it has been shown that piles of firebrands have a higher potential to ignite a fuel bed and the pile mass is a good metric to characterize the pile hakes et al 2019 tao et al 2020 the high variation in dimension and mass of individual firebrands make it difficult to characterize a bulk of particles from its constituents hence the pile mass seems to be more practical for analysis and model application experiments and field studies specifically for structural components show that the accumulation of firebrands at corners or interstices increases the chance of ignition dowling 1994 manzello and suzuki 2017 meerpoel pietri et al 2020 given that the stereotype house introduced in section 4 2 is designed with a gable roof it is assumed that accumulated firebrands on two 10 cm stripes on the edges of a cell i e the roof will be effective to cause ignition firebrands received from different cells are scattered on the stripes following probabilistic distributions the firebrands landing distributions follow a uniform distribution along the width of the stripes and a lognormal distribution with the mean of 0 01 and standard deviation of 0 5 along the length of the stripes each group of firebrands arriving from a similar source is randomly positioned along the length of the cell fig 6 depicts the stripes and distributions santamaria et al 2015 investigated the ignition of wooden structures by firebrand accumulation it is shown that a pile of particles with an initial mass of 60 g deposited on a circular area of 78 53 cm2 could cause ignition the mass reduction due to the burning of particles is estimated at 60 adapted from this experimental study here the ignition condition of a structure cell due to firebrands is determined as the accumulation of 24 g of firebrands landed on a circle with a diameter of 10 cm area of about 78 53 cm2 in a given timestep in other words a structure cell ignites if there are a sufficient number of firebrands landed close to each other within a 10 cm circle on the aforementioned stripes to model the ignition due to spotting typically some values for probability of ignition are assumed for the firebrands as a function of features such as firebrand size or mass and ignition of a building is evaluated randomly based on the landed firebrands and the associated probabilities himoto and tanaka 2008 himoto et al 2008 lee and davidson 2010b the proposed methodology in swuift accounts for the uncertainties associated with ignitions due to firebrands by relating the ignition criterion to the spatial distribution of the particles and their final location on the roof of a building this way three points are addressed a the high level of randomness in fire spotting and b the effects of distance and source of the brands since those generated from the same building or vegetation cell are transported and distributed together see fig 6 and c the effect of firebrand piles and mass accumulation on ignition suzuki and manzello 2020 conducted experiments on the effect of accumulation of firebrands on a 1 22 m 1 22 m shredded hardwood mulch under different wind speeds and moisture contents the majority of results suggest an ignition time under 300 s which is equal to the timestep in the swuift model also an upper limit of about 3 5 g of firebrands is required for ignition a cell in the model can be carpeted by about 64 of such equivalent fuel beds hence a vegetation cell ignites with more than 224 g of firebrands 5 application to two case studies the swuift model is applied to two case studies 1 the trails community affected by the 2007 witch and guejito fire and 2 the fountain grove community affected by the 2017 tubbs fire the two cases are chosen based on the following reasons 1 the two communities have different layout configurations such as the size layout of houses and vegetation density inside the community 2 the fire behavior in the selected two events was different in the case of the trails community scattered clusters of burned structures were observed in the second case the fire line moved quickly over the fountain grove community eventually destroying most of the structures furthermore the effective fire spread duration within the two communities are different 3 the availability of information to initialize the ignitions and validate the model performance is important detailed data especially for the trails community is available the two case studies provide the opportunity to evaluate the model for different scenarios to assess the model s performance the location of burned structures and the average rate of spread are compared with the observations and measurements from the available references although there could be different levels of damage to a structure i e partially damaged fully burned the criterion here is whether or not structures are ignited in the simulation in comparison with the real case and ideally if a similar spread rate is observed 5 1 the trails community on october 22 2007 the trails community in ranch bernardo california was hit by two wildfires namely the guejito fire from the north and and a few hours later the witch fire from the southeast although the affected structures were scattered across the community most of them were located on the northern side of the community which was hit by the guejito fire detailed information about this incident can be found in published reports by nist maranghides and mell 2009 2011 maranghides et al 2013 while it was estimated that the fire line reached the community at around 3 50 a m all reported times are in the local time zone the first structure ignition inside the community was reported at 2 30 a m when the fire was more than 4 km away some other houses that were also located close to the northern interface of the community with the wildland were ignited before the arrival of the fire front based on the descriptions in the nist reports szasdi bardales 2019 concluded that a total of 48 structures were ignited inside the community until 5 30 a m these ignitions might have been caused by the direct effect of the wildfire or as a result of building to building fire propagation szasdi bardales estimated the locations of the structures that were ignited directly due to the wildfire from the wildland the same cluster of ignitions is implemented as the initial ignition points in the model fig 7 illustrates the layout of the community and the inputs for the model the fire scenario is simulated for 3 h starting at 2 30 a m over the course of the simulation the wind is reported along east and northeast directions driving the general direction of the fire spread this information is obtained from the historical data at the ramona airport weather station fig 8 demonstrates the simulated fire spread inside the community the status of structures at 5 30 a m is compared with the information extracted from the nist reports it is observed that the clusters of burned structures except for one at the far most southwest are well captured the average fsr based on observed data across the community is estimated as 340 m h 1 szasdi bardales 2019 while hamada s estimate for fsr was 977 m h 1 using the simulated results and selecting two structures that are 750 m apart and were ignited at 3 15 a m and 5 30 a m the calculated fsr from the model is 330 m h 1 the two selected structures are one of the first and last structures ignited over the course of the simulation the structures are specified by red circles in fig 8a 5 2 the fountain grove community tubbs fire the most destructive wildfire in california at the time was ignited at 9 43 p m on october 8 2017 and in about 4 h reached the fountain grove community approaching from the north to the best of the authors knowledge there is no reconnaissance report of the tubb s fire at the time this paper is drafted however shortly after the incident the new york times nyt published an article illustrating the spread of fire at hourly intervals based on data from various sources watkins et al 2017 according to the nyt article the tubbs fire reached the fountain grove community around 2 a m on october 9 2017 and completely burned the community strong north winds were blowing during the time fig 9 depicts the extent of the tubbs fire in relation to the fountain grove community based on the available information the model is applied to the case study starting at 2 00 a m and for the duration of 1 h when the fire line moves out of the community the layout of the community and the corresponding inputs for the model are presented in fig 10 fig 11 illustrates the simulation results although not all the structures are ignited during the first hour of simulation the vegetation is burned to the full extent and the fire front is captured properly the available information from published articles only estimates the movement of the fire front the fire front leaves the community around 3 00 a m and not the details about the status of the structures the estimated fsr based on the observed data is 1780 m h 1 szasdi bardales 2019 while hamada s estimate was 388 m h 1 using the simulated results and selecting two structures that are about 600 m apart and were ignited at 2 00 a m and 2 30 a m the calculated fsr from the model is 1200 m h 1 the two selected structures are specified by red circles in fig 11 based on an article in los angeles times krishnakumar et al 2017 almost all the structures in the community were destroyed i e all of the structures were ignited if the model runs for a longer duration until about 4 30 a m as shown in fig 11e the rest of buildings will ignite due to fire spread from burning structures in conclusion the fire front passes through the community in 1 h igniting a series of structures and vegetation which in turn leads to further fire spread to other structures in the hours to follow 5 3 comparison of the two case studies further analysis of the model outputs for the two case studies provides insight on the performance of the swuift model fig 12 presents the total number of ignited structures for both case studies and differentiated based on the mode of ignition spotting radiation or known input ignitions when analyzing the findings it should be noted that the fire duration and behavior in the two case studies had notable differences the effect of fire spotting is apparent in both cases the model particularly is doing well for the case of the trails community where in reality the structural ignitions are scattered over both time and space in the fountain grove community details cannot be examined due to the lack of data however the significant effect of fire spotting is observed as there are ignited structures on the side of the community opposite to and far from the simulated fire front in early stages of the simulation in addition the spotting takes the fire further ahead of the fire front and radiation causes further expansion of the fire in the proximity of the affected areas there are some idle intervals no increase in the number of ignited structures in the simulations followed by rapid jumps every simulation begins with an idle interval because it takes five timesteps for the first ignited structure to develop and actively contribute to fire spread igniting other structures this justifies the initial idle interval followed by the jumps representing new ignited structures the same process takes place once new structures are ignited also a quadratic increase in the number of ignited structures especially those by radiation is observed over time this aligns well with the general idea of fire spread which progresses in 2d one considerable difference between the two case studies is the defensible actions taken during the event defensive actions were taken in the trails community based on the nist reports maranghides and mell 2009 2011 maranghides et al 2013 and they were judged to be effective given the scatter of the burning structures and the limited damage following a good number of ignitions on the other hand given the extent of the fire line and the fast spread rate in the tubbs fire and the fact that the priority of the first responders is to save lives before saving structures much could not be done to save structures in the fountain grove community resulting in the whole community burning down the current version of the model may overestimate the number of burnt structures but future versions will include active firefighting actions to provide guidance for response during wildfire events and study fire spread rates that can be controlled 6 discussion the streamlined model proposed in this study considers the critical mechanisms of wui fire spread based on the physics of the process and using the available experimental data and empirical models the effect of direct flame contact is not incorporated in this version of the swuift model the main reason is that in wui communities the separation between structures is considerably larger when compared with urban cities or other non wui communities yet it is possible that two or more structures are close enough such that one would be at risk of ignition due to direct contact with flames from other structures or nearby vegetations future versions of this model will include this mode of fire spread in general there are still knowledge gaps on the wildfire phenomenon and corresponding details despite the huge efforts laboratory experiments have not been able to fully replicate the wildfire environment and more data need to be obtained this is mostly owed to the complexity of these fires especially on such a large scale and with a high number of variables on a typical laptop and using one core of computational power simulating the cases of trails and fountain grove community fires takes 7 min fig 8e and 3 min fig 11d respectively computational efficiency is one of the primary objectives of the proposed model to meet this objective and also due to the lack of available data some important assumptions are made to simplify the calculations and to generalize the available knowledge first the implemented fuel model is independent of the construction type and unique details of structures the same statement holds true for vegetation therefore the current version of the model differentiates various communities mainly based on their layout second firebrands are assumed to be identical although it is understood that firebrands could have different shapes using the firebrand pile mass as the ignition criterion makes the model implementation practical third the effect of spotting is decoupled from the thermal radiative heat flux fourth topography which is known to be a major factor for spread affecting both radiation and transport of firebrands is not included in this version of the model as discussed in section 5 3 defensible actions before and during an incident play an important role suppression actions can be added to the model but there are not many detailed data available for model validation moreover uncertainties in the model parameters can be added in the future 7 conclusion communities in the wildland urban interface wui have been exposed to faster spreading wildfires in recent decades and the frequency of destructive wui fires has been increasing the available urban fire spread models which are not inherently designed for wui fires cannot capture the fire spread rate fsr inside the communities and underestimate the frs in most cases this paper proposes a streamlined wui fire tracing swuift model to simulate the spread of wui fires inside communities the model relies on the physics of the processes and accounts for fire spotting and building to building spread via radiation wind velocity and direction and compartment fire development inside buildings the established swuift model differentiates communities by considering their layout and land cover at a high resolution i e 10 m the swuift model is validated against two historic wui fires the witch and guejito fires in 2007 and the tubbs fire in 2017 the two case studies involve two communities with different sizes and building layouts moreover the wildfire scenarios initial ignitions and defensible actions during the incidents are distinctly different in the two cases hence the model performance is evaluated for two different wui fire scenarios it is shown that the model captures the behavior of fire spread within the communities with a reasonable prediction of fsr and without significant computational cost the model is also able to distinguish and record the cause of building ignition due to either radiation or firebrands the future work will expand the model to include effects of topography and will add a module on first response and suppression actions by firefighters inside a community the current version of the model leverages empirical relationships for fire spotting and fuel models as more information and data on such topics become available the model will be updated accordingly and more detailed fuel models will be included declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements fernando szasdi bardales gratefully acknowledge partial support for this research from the fulbright foreign student program and the u s embassy in guatemala any opinions findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the fulbright foreign student program this study was partially funded by the state university of new york suny research seed grant program any opinions findings and conclusions expressed in this paper are those of the authors and do not necessarily represent those of the sponsor 
25781,the wildland urban interface wui is defined as a geographic area where human developments and flammable vegetation merge in a wildfire prone environment losses due to wildfire have been rising in the past decade attributed to changes in vegetation growth fuel availability and increased land developments in wui this paper studies the process of wildfire spread inside wui communities the fire spread rate within wui communities is determined for nine wildfires that were ranked among the most destructive wildfires in north america an improved quasi empirical model that considers radiation and fire spotting as modes of fire spread inside a community is proposed the new model is validated using the documented spread rates during the 2007 witch and guejito fires and the 2017 tubbs fire the proposed model is computationally efficient and can be used to quantify fire spread rate and the number of affected structures inside a community during a wildfire event keywords wildfires wildland urban interface community fire spread modeling 1 introduction wildfires have always been part of the natural landscape for a healthy ecosystem yet these fires are projected to become more frequent and intense due to changing weather patterns and human suppression activities in the past century the economic and social impacts of wildfires have been rising in recent years and now represent a global concern as of november 2020 6 out of the 10 most destructive wildfires in the us in terms of insured losses happened in 2017 and 2018 also 13 out of the 20 most destructive fires in terms of structure losses happened since 2017 insurance information institute 2020 california department of forestry and fire protection 2020 direct and indirect consequences of wildfires including utility disruptions and environmental impacts have been significant masoudvaziri et al 2020 the current approach in managing the fire hazard within communities is neither sufficient nor sustainable over the past several decades fire safety research has spent a great deal of effort to understand fire dynamics inside buildings in parallel significant research has been conducted to understand wildfire behavior in the wildland area cruz et al 2018 bakhshaii and johnson 2019 but research on large outdoor fires within communities e g due to wildfires and development of codes and standards for such fires lag behind a workshop by the american society for testing and materials astm concludes that current codes and standards for communities susceptible to wildfire risk are not adequate manzello and quarles 2015 the wildland urban interface wui is defined as a geographic area where human developments and flammable vegetation merge in a wildfire prone environment international code council 2018 not all the wui communities are threatened by wildfires as several conditions such as the risk of extreme fire events and the lack of protection and defense for buildings inside wui communities should occur concurrently cohen 2008 stein et al 2013 caton et al 2017 in this paper any mention of communities or wui communities refers to the wui communities that are affected attacked by wildfires although extreme fire hazards cannot be eliminated the amount of incurred damage depends on decisions made by policymakers and engineers and the attitude towards risk the us departments of agriculture usda and interior doi developed the national fire plan usda and doi 2007 after the devastating destruction of communities due to wildfires in 2000 in 2001 a 10 year comprehensive strategy plan was developed to reduce wildfire risks to communities and the environment usda and doi 2007 this effort led to the healthy forests restoration act of 2003 which developed a framework for community wildfire protection plans cwpps that calls for communities to work collaboratively to develop cwpps the plan must identify and prioritize fuel treatment and set forth strategies to reduce the ignitability of houses and other infrastructure ushf 2006 in may 2016 the white house 2016 issued an executive order to mitigate wildfire risks to federal buildings located in the wui reduce risks to people and help minimize property loss due to wildfire as a result of these efforts a number of programs such as firewise nfpa 2021 have been developed to provide guidance to communities for wildfire mitigation actions yet the federal programs have focused more on wildland fire management e g prescribed fires and not sufficiently addressed the equally important community aspects this is affirmed by the increasing wui losses in recent years while the impacts of fire on communities lives and properties can be managed by better understanding the problem and establishing proper policies mathematical models are needed to investigate the process of fire spread in a wui community in a wui community a fire line advances based on three primary mechanisms thermal radiation direct flame contact and firebrands a comprehensive model would integrate these three mechanisms of fire spread but such a validated model for application to wui communities does not exist yet empirical fire spread models have been developed for the problem of fire following earthquake but they have not been validated for wui communities under wildfires many of the existing fire spread models are semi empirical lee et al 2008 mahmoud and chulahwat 2020 with a few coarse attempts to include physics based processes such as spotting and thermal radiation mahmoud and chulahwat 2018 li and davidson 2013a 2013b himoto and tanaka 2008 iwami et al 2004 lee and davidson 2010a 2010b nussle et al 2004 otake et al 2003 the majority of urban fire spread models are developed in japan for post earthquake fire spread and are suitable for an urban environment involving equally spaced and equally sized square buildings in dense urban areas hamada 1975 himoto et al 2008 lee and davidson 2010b have developed a more comprehensive spread model for post earthquake fires the spread model considers fire evolution within a room from room to room inside a building and from building to building by flame spread but the level of required input information implies a number of assumptions hindering practicality for application to a community this paper starts with assessing the suitability of an existing urban fire spread model to predict fire progression inside a wui community fire spread rate within communities is determined for nine wildfires ranked among the most destructive wildfires in north america the same nine fires are analyzed using a simplified approach to assess the validity of an existing model in addition the paper proposes a quasi empirical model as categorized per sullivan s review sullivan 2009 which considers radiation and firebrand mechanisms as modes of fire spread inside a community the proposed model is validated using the documented spread rates during the 2007 witch and guejito fires and the 2017 tubbs fire the two selected communities as part of the validation study have different characteristics in terms of layout and building density wind velocity during the wildfire event etc demonstrating the applicability of the model to capture different scenarios 2 fire spread rate during recent fires in wui communities fire spread rate fsr inside a wui community refers to how fast the fire travels inside that community the fsr in this study is calculated as the distance between coordinates of two locations in the community where fire spread occurred divided by the corresponding time duration for which the fire traveled between the two coordinates the value of fsr can vary during a fire event due to heterogeneity in land cover topography and wind patterns therefore the mean value should be taken to characterize the event within a community a fast approaching fire from the wildland reduces the available time for evacuation and for first respondents to control the fire propagation when ignitions occur faster than the number of structural fires that are being put out by the fire crews a conflagration can occur within a neighborhood and the fire becomes out of control fire spread data within the wildland is available for most of the historic wildfire events and is often represented as increments in the burned area over a specific period of time usually in the range of several hours fire contours published by agencies including but not limited to the california department of forestry and fire protection cal fire the united states geological survey usgs and the u s forest service show the borders of regions that have been burned by a fire at a certain point in time during the fire event the precision of this information and satellite images are usually adequate to describe and study the fire in the wildland but that is not the case for most of the fires inside a community due to the much higher spatial and temporal resolution required despite its importance the fsr inside communities affected by wildfires is not always reported and in many cases precise information to determine the fsr is not available the required information on fire propagation and characteristics of communities to calculate fsr can be obtained from different sources such as reconnaissance reports satellite images aerial images from unmanned aircraft systems and social media satellite images are reliable sources of information however current satellite images do not offer an ideal temporal resolution for tracking the advancement of fire inside a community in most cases two different satellite images of the same fire event are separated by a gap of several hours which allows for monitoring fire spread on the wildland but do not capture the evolution of an event within a wui community it should also be noted that the current spatial resolution of freely available satellite images is approximately one pixel for every 10 60 m gisgeography 2021 considering the size of typical wui structures with an average length between 16 and 25 m some information could be lost if satellite images with lower resolution were used based on the above satellite images were used to quantify the fsr in wui communities for nine of the most damaging recent fire events in the us as listed in table 1 details of data sources for the nine events are provided in szasdi bardales 2019 and the associated wind speeds are provided in table 2 meanwhile scawthorn 1987 provides a list of historic 20th century north american urban fire events for which data were available to calculate the fsr this list includes events as old as the 1900 ottawa and hull fires in canada to a fire event in 1985 in philadelphia pa 3 existing simplified fire spread models simplified models are usually developed using empirical approaches and by performing regression analysis of data from past events such an approach was used as the first attempt in modeling inter building fire spread for urban scenarios dating back to the middle of the 20th century one of the characteristics of simplified models is that they are straightforward to implement without the need for complex computational platforms the empirical equations in these models relate the fsr with the parameters that are known to influence fire propagation the precision in these models is directly related to the reliability of the input data and the number of time steps used for the analysis in general the existing simplified fire spread models for application in community fires were generated specifically for the case of post earthquake fires the two most commonly used simplified models are the hamada model hamada 1975 and the tosho model tokyo fire department 1997 this paper will focus on the hamada model while more information on the tosho model can be found in tokyo fire department 1997 it should be noted that the hazus program developed by federal emergency management agency also implemented the same version of the hamada model to assess the risk of fire spread after an earthquake in a community scawthorn 2009 fema hazus 2014 the hamada model characterizes fire spread as a function of wind velocity and the average distances between buildings in a community hamada 1975 the fsr coefficients are determined based on empirical relationships from past japanese urban fire events including the fire after the 1923 kanto earthquake and wartime fires scawthorn 2005 the model assumes an elliptical shape for the fire expansion and the spread is faster in the downwind direction aligns with the major axis of the ellipse and slower in both the sidewind and upwind directions the model simplifies the locations of the structures in a regular grid all the structures are considered to have the same construction material and dimensions the model does not consider the streets as fire barriers and therefore street layouts are not included as an input as an input the model requires the user to provide information on the average length of one side of the buildings the average distance between buildings the wind velocity and the average building density ratio within the community represented by the built upness factor for brevity the series of equations are not provided here and can be found in szasdi bardales 2019 the hamada model was applied to the nine recent fire cases discussed in section 2 detailed data such as building layout in the community and wind speed during the fire event was collected to simulate the fire events the predicted fsr based on the hamada model is listed in table 2 a similar exercise was performed by scawthorn in 1987 scawthorn 1987 the hamada model s predicted fsr for 32 historical 20th century cases was also collected and included in this study fig 1 shows the comparison of the observed versus predicted fsr values for the nine investigated fire events in color as well as older 20th century fires in gray scawthorn 1987 concluded that the hamada model predictions agreed well with most of the observed fsrs except for a group of cases in which fire spread by firebrands between wooden buildings was significant meanwhile it is clear that in the majority of recent fires six out of nine wildfire events the model underestimates the fire spread rate within the communities and is not able to capture the true behavior see table 1 and fig 1 most of these recent fires in the us and the canadian communities correspond to wui wildfires at times of high winds and dry weather conditions that significantly differ from those on which hamada s model is based on the horse river fire 2016 tubbs fire 2017 and camp fire 2018 especially accentuate the level of such underestimation with an average error of 320 these results show a necessity for developing a new model that is capable of replicating the characteristics of modern wui fires 4 the proposed streamlined wildland urban interface fire tracing swuift model this section proposes a new quasi empirical model to simulate fire propagation in wui communities the proposed model is validated against observed data the primary mechanisms of fire spread between buildings are thermal radiation fire spotting by firebrands and direct flame contact the current version of the proposed model incorporates thermal radiation and firebrands while future work will expand on the model to include direct flame contact an efficient fire spread model should balance between fidelity the availability of input data and fast execution the proposed model has been developed to apply physics based principles while maintaining some of the advantages of simplified models to have an efficient implementation and to reduce the computational cost the proposed model simplifies the community layout into a regular grid of cells the content of each cell can take one of the three pre defined fuel models structure vegetation and non combustible the model captures the dynamic nature of a fire event by predicting the status of the fire within the community at individual time steps of 5 min the model records the thermal radiative flux and the firebrands that have landed at any cell for every time step and determines the status of individual structures and the duration of a fire in a specific structure it also accounts for variable wind speed at every time step the following sections provide details of the model mechanics implemented fire spread modes and the corresponding assumptions in the model 4 1 layout and wind the proposed swuift model characterizes fire events by two features 1 community layout and 2 wind speed and direction a given community layout is rasterized into 10 10 m grids the selected spatial resolution of 10 m captures a reasonable level of granularity while keeping the computational cost manageable here three fuel models are introduced structure vegetation non combustible the fuel model is a parametrization of different properties of the fuel bed and facilitates fuel bed classifications to be used as an input to the spread model scott 2005 each grid cell is assigned a fuel type based on the dominant larger than 50 land cover in that cell for this purpose the landfire s 13 anderson fire behavior fuel model anderson 1981 reeves et al 2009 rollins 2009 and microsoft s building footprints microsoft 2019 are retrieved and processed vegetation is defined as any vegetation with a height greater than 1 ft the non combustible classification represents land covers such as parking lots roads water bodies etc fig 2 demonstrates the application of the defined fuel model to the trails community in california to be discussed in detail in section 5 wind data is the other input for the model which should be provided for every time step forecast data can be used for predicting scenarios and planning applications while historic data from the closest weather station to a community can be used to replicate an event the model requires average wind speed and 3 s wind gust as inputs where the wind speed is randomly selected for each time step assuming 80 likelihood for average wind speed and 20 likelihood for the wind gust 4 2 mechanics of the model the swuift model simulates the fire spread and similar to other spread models requires the initial ignition s grid cell location and time step as an input ignitions can be defined both outside the community i e an approaching fire line from the wildland and inside the community i e where firebrands cause ignition ahead of the wildfire perimeter reaching the community the model is capable of accounting for any number of ignitions over the duration of the simulation once an ignition is defined the model tracks the fire development a structure cell can be ignited by either radiative heat or firebrands and when the fire is at the fully developed phase the cell emits energy and generates firebrands a vegetation cell can only be ignited by firebrands and when burning it generates firebrands noncombustible cells do not ignite and have no positive contribution to the fire spread all structures in the wui community are assumed to be residential housings each structure cell is coded to behave as a burning compartment of 10 10 m with the configuration illustrated in fig 3 the compartment boundaries are assumed to be 10 cm thick normal wood the typical temperature time evolution for the assumed configuration is generated using ozone cadorin and franssen 2003 cadorin et al 2003 ozone is a zone model taking into account the fuel amount and type openings and thermal properties of boundaries and calculates the fire evolution the adopted temperature time curve for the model is shown in fig 4 based on the compartment fire analysis it takes 22 min 5 timesteps for the fire to reach the maximum temperature the fire is then in a fully developed phase for 31 timesteps followed by the cooling phase the structure cell is assumed to contribute to fire spread by radiating energy and generating firebrands only during the fully developed phase due to its floor area a typical house in a wui community is usually represented by a few connected structure cells the raster matrix keeps track of the structure cells that belong to the same house when the fire in any of these cells reaches a fully developed stage the rest of the cells belonging to the same house will ignite unless they have been ignited earlier due to radiation or firebrands from other sources this mechanism is implemented to mimic the fire spread inside a house vegetative fuels e g piles of needles and leaves burn faster than structures due to larger aspect ratios better ventilation and larger porosity compared to structure fuel albini and reinhardt 1995 manzello et al 2007 mell et al 2009 dietenberg 2010 morandini et al 2013 therefore it is assumed that a burning vegetation cell in the model contributes to the spread only by firebrand generation and excludes radiation contribution given the limited available data a vegetation cell burns in 1 timestep after ignition i e 5 min the swuift model follows a step by step procedure in each timestep of the simulation 1 ignitions are input to the model and can be added at any timestep these known ignitions usually include initial ignitions at time zero where the wildfire line approaches the community or are caused by firebrands received inside the community ahead of the fire 2 if a structure cell which is part of a particular house reaches the fully developed phase of the fire the rest of the house cells that are not ignited by radiation or firebrands will ignite 3 contributions of burning cells are calculated at each timestep burning vegetation cells generate firebrands and structure cells with fully developed fire generate both radiative heat and firebrands 4 the accumulated energy in unignited structure cells received from the burning structure cells is calculated and if the specified threshold is reached the cells are ignited 5 generated firebrands from burning vegetation cells and structure cells land further down the fire line into the community following the wind direction and if enough firebrands are accumulated unignited cells vegetation and or structure ignite fig 5 summarizes the procedure listed above details of the model are further described in the following subsections 4 3 radiation thermal radiation is the physical phenomenon by which thermal energy is transferred from one body to another in the form of electromagnetic waves at any given time the heat flux at a receiving surface is given by 1 q ϕ ε e σ t e 4 where q is the radiant heat flux in w m2 ϕ is configuration factor ε e is the emissivity of the emitting surface σ is the stefan boltzmann constant 5 67 10 8 w m2k4 and t e is the absolute temperature of the emitting surface in k the emissivity refers to the efficiency of a surface as a radiator and its value is in the range of 0 1 during fire conditions most of the hot surfaces such as smoke or flames have an emissivity between 0 7 and 1 0 buchanan 2017 the configuration factor which accounts for cases in which the emitting surface is not directly facing the receiving surface can be calculated based on buchanan 2017 as mentioned earlier the swuift model assumes that a burning structure cell only emits radiation during the fully developed phase of the fire fig 4 shows the assigned temperature time evolution of fire to compartments in the study where the fully developed fire phase starts 5 timesteps after the ignition and lasts for 155 min 31 timesteps the amount of thermal radiative flux received at a given cell is then determined at every time step using eq 1 although thermal radiation is independent of the wind speed and direction the resulting flames are indeed influenced by the wind in general flames will be oriented in the direction at which the wind is blowing to account for the effect of wind on the geometry of the flames it is assumed that the emitting surface is rectangular in shape and is perpendicular to the direction of the wind therefore only the structures located in front of the emitting surface and including 90 with respect to the wind direction i e half plane along the wind direction will receive radiation waterman 1969 observed that a spontaneous ignition of various woods due to a radiant exposure generally occurs when the exposed surface reaches a heat flux of 0 8 cal cm2 sec which is equivalent to approximately 12 5 kw m2 although waterman observed that ignition occurred after 1 min of exposure to the aforementioned levels of radiation further research offered more details on the relation of the radiant heat flux and the required time of exposure for spontaneous ignition of wood quintiere 2006 it was shown that the critical ignition fluxes across and along the wood grains are 12 0 kw m2 and 9 0 kw m2 respectively the model calculates the aggregated flux at a given surface as the addition of all the individual fluxes generated by all the structure cells that are burning within the fully developed phase of the fire over time if the flux at any specific point in time exceeds a threshold ignition will occur the considered value for the critical ignition flux is 14 kw m2 slightly larger than the reported values in tests which are for bare materials considering tiling or finishing 4 4 fire spotting fire spotting is proved to be an important mechanism of fire spread in wui cohen 2008 koo et al 2010 maranghides et al 2015 caton et al 2017 a firebrand or an ember is a small incandescent particle that has separated from a burning material and transported by wind if a firebrand lands over a combustible material it could cause an ignition spreading the fire to an area distant from the original location fire spotting is a mechanism of fire spread that is difficult to model therefore to this date there is no available definitive physics based procedure for modeling the three phases of spotting generation transport and ignition in a holistic approach the three phases should be properly accounted for in a fire spread model to successfully replicate the occurrence of ignitions during a fire event for the purpose of this study a combination of experimental data and probabilistic models is adapted to quantify ignition due to firebrands the process of fire spotting starts with particles of burning material detaching from the fuel body they are then transported by the atmospheric turbulence intensified by the fire eventually the particles will exit the turbulent flow and land on surfaces the maximum height at which the particles elevate and the characteristics of their trajectory are influenced by the characteristics of the fire weather as well as their shape and density which in turn are closely related to the specific characteristics of the vegetation or material that is burning an ignition can occur if one or several incandescent firebrands land over a combustible material some firebrands would have been completely consumed through combustion and their temperature would have considerably reduced when they land lowering or even eliminating the probability of ignition caton et al 2017 hakes et al 2017 given the number of involved contributing factors and the substantial variation at different stages of spotting many studies have been conducted focusing on one of the three phases these studies generally include data collection from real incidents or experimental works studies have investigated various features of firebrands such as mass size type of material which will be discussed in the following sections in the swuift model both vegetation and structure cells generate firebrands and in return can potentially get ignited by firebrands the implemented firebrand model in this study is based on the review and analysis of an extensive collection of studies considering the involved uncertainty and unknowns and to avoid the expensive computational cost the mass of firebrands is used as the key parameter in the model while assuming that the firebrands are identical in shape the following sections provide details of the simulated procedure for fire spotting 4 4 1 generation of firebrands different experimental studies in laboratory or field have been conducted to investigate firebrand generation from different fuels and their characteristics such as mass size projected area number of particles etc various features affect the characteristics of a firebrand such as the surrounding airflow fire intensity fuel moisture content and fuel type some studies have focused on vegetative fuels manzello et al 2009 el houssami et al 2016 filkov et al 2017 bahrani 2020 hudson and blunck 2020 and some on structural fuels yoshioka et al 2004 suzuki et al 2013 2014 suzuki and manzello 2016 2021 despite different conditions and high variations in experiment setups a common finding among these studies is that the majority of generated firebrands are small 1 g with heavy right tail distributions in both mass and cross sectional area this finding is aligned with reports compiled after fire incidents foote et al 2011 rissel and ridenour 2013 hasemi et al 2014 suzuki 2017 moreover the traveling distance of firebrands have been investigated and no definitive relationship or significant correlation between distance and particle properties e g mass and size have been found suzuki et al 2012 hedayati 2018 suzuki and manzello 2018 zhou et al 2019 putting the results of these studies together and taking the computational efficiency of the proposed model into account it is assumed that identical firebrands with a mass of less than 1 g are generated from both structure and vegetation cells the fuel models are differentiated by the number of firebrands generated in each timestep the generation of firebrands by a structure cell is modeled based on the relation proposed by lee 2009 describing the number of firebrands produced from a single building as a function of the wind speed the relationship was derived based on the experimental findings of waterman 1969 waterman 1969 reported results of an experimental study on firebrand generation by various roof constructions including full scale segments of wood shingles asphalt shingles roll roofing cement asbestos shingles built up roofing and no covering firebrand production was studied under the internal pressure of the fire chamber and with additional pressures to simulate more intense fires and or wind effects the produced firebrands were trapped by a screen enclosure and dropped into a quenching pool all firebrands were sorted dried and weighed lee and davidson 2010b proposed equation 2 for the total number of generated firebrands from a structure based on the gathered data from the experiments described above 2 n b t o t a l 306 77 e 0 1879 v a r o o f where v is the wind speed in m s 1 and a r o o f is the roof area in m2 the roof area is calculated for the defined stereotype building however the wind speed varies over each timestep a burning structure cell generates a firebrand only during the fully developed phase of the fire i e firebrand generation is time dependent based on the discussion provided in section 4 2 thus equation 3 is used to calculate the number of firebrands from a structure cell in a given timestep where 31 is the number of timesteps that a structure cell burns as a fully developed fire 3 n b s t r i 306 77 e 0 1879 v i a r o o f 31 here v i is the wind speed at time step i in m s 1 a recent numerical study wickramasinghe 2020 conducted a reverse analysis to calculate the number of firebrands generated from a douglas fir tree by tuning inputs of their model to match the data by the national institute of standards and technology nist manzello et al 2007 mell et al 2009 the study uses the fire dynamic simulator fds model to simulate and analyze the fire mcgrattan et al 2013 and the simulation follows a specific distribution of mass to produce firebrands it is determined that a tree with circular base of 1 5 m in diameter and a height of 2 6 m generates about 87 g of firebrands assuming that a 10 m 10 m vegetation cell is fully covered with vegetation equation 4 provides n b v e g the number of firebrands generated from a vegetation cell 4 n b v e g 87 m b 10 2 0 75 2 π 4923 m b where m b is the mass of a firebrand in the model 4 4 2 transfer mechanism of firebrands himoto and tanaka 2005 proposed a probabilistic approach for determining the landing distribution of firebrands simulations of firebrands scattered in a turbulent boundary layer showed that the travel distances of firebrands in the downwind and the sidewind directions could be described by a lognormal and a normal distribution respectively himoto and tanaka 2005 in this approach the downwind distribution is a function of particle s properties such as width and density as well as surrounding s such as wind speed and heat source dimension and the sidewind distribution is independent of wind speed and particle s features himoto and tanaka 2005 in a cellular automata model inspired by the aforementioned model zhao 2011 proposed a simplified ellipse to characterize firebrand travel distance which was parametrized as a step function of wind speed as a proxy to account for the effect of firebrand spotting this model is also implemented in another cellular automata model on wui fire spread jiang et al 2020 in the present work the following lognormal and normal distributions are implemented for the dispersion of firebrands generated from a burning cell in downwind x and sidewind y directions respectively 5 p x 1 x σ x 2 π exp l n x μ x 2 2 σ x 2 0 x 6 q y 1 2 π σ y exp y μ y 2 2 σ y 2 0 y in the above equations μ x ln 30 v where v is the wind speed in m s 1 σ x 0 3 μ y 0 and σ y 4 85 the output values measure the distance from a cell s center point in meters for example a generated value of 60 m from equation 5 for a northerly wind implies that the firebrand lands on the 6th cell south of its origin each cell is 10 m long 4 4 3 ignition mechanism due to firebrands multiple experiments have been carried out to study the ignitability of fuel beds of different types and configurations and the potential of firebrands to cause ignition given their features such as mass size burning status flaming glowing smoldering ellis 2015 hernandez et al 2018 urban et al 2019 some studies compared the potential of ignition by a single firebrand versus a pile of firebrands manzello et al 2006 it has been shown that piles of firebrands have a higher potential to ignite a fuel bed and the pile mass is a good metric to characterize the pile hakes et al 2019 tao et al 2020 the high variation in dimension and mass of individual firebrands make it difficult to characterize a bulk of particles from its constituents hence the pile mass seems to be more practical for analysis and model application experiments and field studies specifically for structural components show that the accumulation of firebrands at corners or interstices increases the chance of ignition dowling 1994 manzello and suzuki 2017 meerpoel pietri et al 2020 given that the stereotype house introduced in section 4 2 is designed with a gable roof it is assumed that accumulated firebrands on two 10 cm stripes on the edges of a cell i e the roof will be effective to cause ignition firebrands received from different cells are scattered on the stripes following probabilistic distributions the firebrands landing distributions follow a uniform distribution along the width of the stripes and a lognormal distribution with the mean of 0 01 and standard deviation of 0 5 along the length of the stripes each group of firebrands arriving from a similar source is randomly positioned along the length of the cell fig 6 depicts the stripes and distributions santamaria et al 2015 investigated the ignition of wooden structures by firebrand accumulation it is shown that a pile of particles with an initial mass of 60 g deposited on a circular area of 78 53 cm2 could cause ignition the mass reduction due to the burning of particles is estimated at 60 adapted from this experimental study here the ignition condition of a structure cell due to firebrands is determined as the accumulation of 24 g of firebrands landed on a circle with a diameter of 10 cm area of about 78 53 cm2 in a given timestep in other words a structure cell ignites if there are a sufficient number of firebrands landed close to each other within a 10 cm circle on the aforementioned stripes to model the ignition due to spotting typically some values for probability of ignition are assumed for the firebrands as a function of features such as firebrand size or mass and ignition of a building is evaluated randomly based on the landed firebrands and the associated probabilities himoto and tanaka 2008 himoto et al 2008 lee and davidson 2010b the proposed methodology in swuift accounts for the uncertainties associated with ignitions due to firebrands by relating the ignition criterion to the spatial distribution of the particles and their final location on the roof of a building this way three points are addressed a the high level of randomness in fire spotting and b the effects of distance and source of the brands since those generated from the same building or vegetation cell are transported and distributed together see fig 6 and c the effect of firebrand piles and mass accumulation on ignition suzuki and manzello 2020 conducted experiments on the effect of accumulation of firebrands on a 1 22 m 1 22 m shredded hardwood mulch under different wind speeds and moisture contents the majority of results suggest an ignition time under 300 s which is equal to the timestep in the swuift model also an upper limit of about 3 5 g of firebrands is required for ignition a cell in the model can be carpeted by about 64 of such equivalent fuel beds hence a vegetation cell ignites with more than 224 g of firebrands 5 application to two case studies the swuift model is applied to two case studies 1 the trails community affected by the 2007 witch and guejito fire and 2 the fountain grove community affected by the 2017 tubbs fire the two cases are chosen based on the following reasons 1 the two communities have different layout configurations such as the size layout of houses and vegetation density inside the community 2 the fire behavior in the selected two events was different in the case of the trails community scattered clusters of burned structures were observed in the second case the fire line moved quickly over the fountain grove community eventually destroying most of the structures furthermore the effective fire spread duration within the two communities are different 3 the availability of information to initialize the ignitions and validate the model performance is important detailed data especially for the trails community is available the two case studies provide the opportunity to evaluate the model for different scenarios to assess the model s performance the location of burned structures and the average rate of spread are compared with the observations and measurements from the available references although there could be different levels of damage to a structure i e partially damaged fully burned the criterion here is whether or not structures are ignited in the simulation in comparison with the real case and ideally if a similar spread rate is observed 5 1 the trails community on october 22 2007 the trails community in ranch bernardo california was hit by two wildfires namely the guejito fire from the north and and a few hours later the witch fire from the southeast although the affected structures were scattered across the community most of them were located on the northern side of the community which was hit by the guejito fire detailed information about this incident can be found in published reports by nist maranghides and mell 2009 2011 maranghides et al 2013 while it was estimated that the fire line reached the community at around 3 50 a m all reported times are in the local time zone the first structure ignition inside the community was reported at 2 30 a m when the fire was more than 4 km away some other houses that were also located close to the northern interface of the community with the wildland were ignited before the arrival of the fire front based on the descriptions in the nist reports szasdi bardales 2019 concluded that a total of 48 structures were ignited inside the community until 5 30 a m these ignitions might have been caused by the direct effect of the wildfire or as a result of building to building fire propagation szasdi bardales estimated the locations of the structures that were ignited directly due to the wildfire from the wildland the same cluster of ignitions is implemented as the initial ignition points in the model fig 7 illustrates the layout of the community and the inputs for the model the fire scenario is simulated for 3 h starting at 2 30 a m over the course of the simulation the wind is reported along east and northeast directions driving the general direction of the fire spread this information is obtained from the historical data at the ramona airport weather station fig 8 demonstrates the simulated fire spread inside the community the status of structures at 5 30 a m is compared with the information extracted from the nist reports it is observed that the clusters of burned structures except for one at the far most southwest are well captured the average fsr based on observed data across the community is estimated as 340 m h 1 szasdi bardales 2019 while hamada s estimate for fsr was 977 m h 1 using the simulated results and selecting two structures that are 750 m apart and were ignited at 3 15 a m and 5 30 a m the calculated fsr from the model is 330 m h 1 the two selected structures are one of the first and last structures ignited over the course of the simulation the structures are specified by red circles in fig 8a 5 2 the fountain grove community tubbs fire the most destructive wildfire in california at the time was ignited at 9 43 p m on october 8 2017 and in about 4 h reached the fountain grove community approaching from the north to the best of the authors knowledge there is no reconnaissance report of the tubb s fire at the time this paper is drafted however shortly after the incident the new york times nyt published an article illustrating the spread of fire at hourly intervals based on data from various sources watkins et al 2017 according to the nyt article the tubbs fire reached the fountain grove community around 2 a m on october 9 2017 and completely burned the community strong north winds were blowing during the time fig 9 depicts the extent of the tubbs fire in relation to the fountain grove community based on the available information the model is applied to the case study starting at 2 00 a m and for the duration of 1 h when the fire line moves out of the community the layout of the community and the corresponding inputs for the model are presented in fig 10 fig 11 illustrates the simulation results although not all the structures are ignited during the first hour of simulation the vegetation is burned to the full extent and the fire front is captured properly the available information from published articles only estimates the movement of the fire front the fire front leaves the community around 3 00 a m and not the details about the status of the structures the estimated fsr based on the observed data is 1780 m h 1 szasdi bardales 2019 while hamada s estimate was 388 m h 1 using the simulated results and selecting two structures that are about 600 m apart and were ignited at 2 00 a m and 2 30 a m the calculated fsr from the model is 1200 m h 1 the two selected structures are specified by red circles in fig 11 based on an article in los angeles times krishnakumar et al 2017 almost all the structures in the community were destroyed i e all of the structures were ignited if the model runs for a longer duration until about 4 30 a m as shown in fig 11e the rest of buildings will ignite due to fire spread from burning structures in conclusion the fire front passes through the community in 1 h igniting a series of structures and vegetation which in turn leads to further fire spread to other structures in the hours to follow 5 3 comparison of the two case studies further analysis of the model outputs for the two case studies provides insight on the performance of the swuift model fig 12 presents the total number of ignited structures for both case studies and differentiated based on the mode of ignition spotting radiation or known input ignitions when analyzing the findings it should be noted that the fire duration and behavior in the two case studies had notable differences the effect of fire spotting is apparent in both cases the model particularly is doing well for the case of the trails community where in reality the structural ignitions are scattered over both time and space in the fountain grove community details cannot be examined due to the lack of data however the significant effect of fire spotting is observed as there are ignited structures on the side of the community opposite to and far from the simulated fire front in early stages of the simulation in addition the spotting takes the fire further ahead of the fire front and radiation causes further expansion of the fire in the proximity of the affected areas there are some idle intervals no increase in the number of ignited structures in the simulations followed by rapid jumps every simulation begins with an idle interval because it takes five timesteps for the first ignited structure to develop and actively contribute to fire spread igniting other structures this justifies the initial idle interval followed by the jumps representing new ignited structures the same process takes place once new structures are ignited also a quadratic increase in the number of ignited structures especially those by radiation is observed over time this aligns well with the general idea of fire spread which progresses in 2d one considerable difference between the two case studies is the defensible actions taken during the event defensive actions were taken in the trails community based on the nist reports maranghides and mell 2009 2011 maranghides et al 2013 and they were judged to be effective given the scatter of the burning structures and the limited damage following a good number of ignitions on the other hand given the extent of the fire line and the fast spread rate in the tubbs fire and the fact that the priority of the first responders is to save lives before saving structures much could not be done to save structures in the fountain grove community resulting in the whole community burning down the current version of the model may overestimate the number of burnt structures but future versions will include active firefighting actions to provide guidance for response during wildfire events and study fire spread rates that can be controlled 6 discussion the streamlined model proposed in this study considers the critical mechanisms of wui fire spread based on the physics of the process and using the available experimental data and empirical models the effect of direct flame contact is not incorporated in this version of the swuift model the main reason is that in wui communities the separation between structures is considerably larger when compared with urban cities or other non wui communities yet it is possible that two or more structures are close enough such that one would be at risk of ignition due to direct contact with flames from other structures or nearby vegetations future versions of this model will include this mode of fire spread in general there are still knowledge gaps on the wildfire phenomenon and corresponding details despite the huge efforts laboratory experiments have not been able to fully replicate the wildfire environment and more data need to be obtained this is mostly owed to the complexity of these fires especially on such a large scale and with a high number of variables on a typical laptop and using one core of computational power simulating the cases of trails and fountain grove community fires takes 7 min fig 8e and 3 min fig 11d respectively computational efficiency is one of the primary objectives of the proposed model to meet this objective and also due to the lack of available data some important assumptions are made to simplify the calculations and to generalize the available knowledge first the implemented fuel model is independent of the construction type and unique details of structures the same statement holds true for vegetation therefore the current version of the model differentiates various communities mainly based on their layout second firebrands are assumed to be identical although it is understood that firebrands could have different shapes using the firebrand pile mass as the ignition criterion makes the model implementation practical third the effect of spotting is decoupled from the thermal radiative heat flux fourth topography which is known to be a major factor for spread affecting both radiation and transport of firebrands is not included in this version of the model as discussed in section 5 3 defensible actions before and during an incident play an important role suppression actions can be added to the model but there are not many detailed data available for model validation moreover uncertainties in the model parameters can be added in the future 7 conclusion communities in the wildland urban interface wui have been exposed to faster spreading wildfires in recent decades and the frequency of destructive wui fires has been increasing the available urban fire spread models which are not inherently designed for wui fires cannot capture the fire spread rate fsr inside the communities and underestimate the frs in most cases this paper proposes a streamlined wui fire tracing swuift model to simulate the spread of wui fires inside communities the model relies on the physics of the processes and accounts for fire spotting and building to building spread via radiation wind velocity and direction and compartment fire development inside buildings the established swuift model differentiates communities by considering their layout and land cover at a high resolution i e 10 m the swuift model is validated against two historic wui fires the witch and guejito fires in 2007 and the tubbs fire in 2017 the two case studies involve two communities with different sizes and building layouts moreover the wildfire scenarios initial ignitions and defensible actions during the incidents are distinctly different in the two cases hence the model performance is evaluated for two different wui fire scenarios it is shown that the model captures the behavior of fire spread within the communities with a reasonable prediction of fsr and without significant computational cost the model is also able to distinguish and record the cause of building ignition due to either radiation or firebrands the future work will expand the model to include effects of topography and will add a module on first response and suppression actions by firefighters inside a community the current version of the model leverages empirical relationships for fire spotting and fuel models as more information and data on such topics become available the model will be updated accordingly and more detailed fuel models will be included declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements fernando szasdi bardales gratefully acknowledge partial support for this research from the fulbright foreign student program and the u s embassy in guatemala any opinions findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the fulbright foreign student program this study was partially funded by the state university of new york suny research seed grant program any opinions findings and conclusions expressed in this paper are those of the authors and do not necessarily represent those of the sponsor 
25782,model ensembles have several benefits compared to single model applications but are not frequently used within the lake modelling community setting up and running multiple lake models can be challenging and time consuming despite the many similarities between the existing models forcing data hypsograph etc here we present an r package lakeensemblr that facilitates running ensembles of five different vertical one dimensional hydrodynamic lake models flake glm gotm simstrat mylake the package requires input in a standardised format and a single configuration file lakeensemblr formats these files to the input required by each model and provides functions to run and calibrate the models the outputs of the different models are compiled into a single file and several post processing operations are supported lakeensemblr s workflow standardisation can simplify model benchmarking and uncertainty quantification and improve collaborations between scientists we showcase the successful application of lakeensemblr for two different lakes graphical abstract image 1 keywords ensemble modeling vertical one dimensional lake model r package calibration thermal structure hydrodynamics 1 introduction numerical process based lake models are powerful tools to simulate processes occurring in aquatic ecosystems these models enable the users to investigate scientific and engineering hypotheses or scenarios which would otherwise not be feasible or even possible to field test for physical logistical political or financial reasons over recent decades the understanding of fluid dynamics and physical transport processes in lakes has improved thanks to enhanced field monitoring and intensive laboratory studies csanady 1975 imberger 1985 imberger and hamblin 1982 imboden 1973 kitaigorodskii and miropolsky 1970 spigel et al 1986 spigel and imberger 1980 with better empirical relationships and physical understanding of processes the pioneer lake models that emerged from these studies were essential to addressing emerging water quality issues like eutrophication french and imberger 1984 today one dimensional 1d lake models are frequently used to characterise lake hydrodynamics these models assume complete and instantaneous horizontal mixing in many systems this is a reasonable assumption because vertical thermal gradients are typically much larger than horizontal thermal gradients the assumption holds for lakes with a small to moderate surface area that are not affected by coriolis acceleration or other significant horizontal transport processes patterson et al 1984 to model water column thermal dynamics resulting from atmospheric exchange processes inflow entrainment and turbulence different theoretical approaches have been developed and applied in lake models e g bulk models energy balance approach models and models that use a pure turbulence approach to account for mixing goudsmit et al 2002 alternative approaches apply simpler schemes to solve advection diffusion equations or use constants for transport processes since the 1980s there has been a rapid expansion in the publication of process based aquatic ecosystem models however the aquatic ecosystem community has not fully exploited the diversity of available models by comparing the performance of models against one another which affords both the opportunity to identify technical improvements but also improve overall model predictions janssen et al 2015 critical voices still highlight the problem that modelling teams tend to reinvent the wheel mooij et al 2010 instead of building on existing software the lake model intercomparison project lakemip had several key findings regarding the current state of lake modelling 1 the majority of lake models replicate surface temperature dynamics coherently well stepanenko et al 2013 2 individual lake models clearly outperform others for specific lake sites thiery et al 2014 and 3 models that explicitly incorporate sediment heating and resolve turbulence over lake depth are better suited to represent lakes in numerical meteorological studies and to research hydrodynamic processes for deep lakes stepanenko et al 2013 thiery et al 2014 most authors agree that open community approaches as well as publishing the model as open source code are the best steps for sustainable development and to ensure future technical improvements frassl et al 2019 janssen et al 2015 read et al 2016 still a lack of common community framework for model calibration validation and processing has resulted in few studies that quantify model performance benchmarking and minimal progress in improving code and applications arhonditsis et al 2014 hipsey et al 2020 in the 1990s atmospheric researchers popularised the use of ensemble modeling in operational forecasting and uncertainty predictions parker 2013 ensemble modeling involves either running the same model multiple times with different settings or running multiple models on the same study site one of the main advantages of model ensembles is that the uncertainty in the model predictions can be estimated trolle et al 2014 wu et al 2020 this allows the modeller to assess the likelihood of occurrence of certain model predictions connected to this ensemble runs of an individual model are a means of taking into account nonuniqueness i e equifinality see beven 2006 in parameter sets gal et al 2014 nielsen et al 2014 the average of individual model runs from different models can be a more robust predictor than any of the individual model runs kobler and schmid 2019 trolle et al 2014 and sources therein if only the best model is retained valuable information in other model fits is disregarded baker and ellison 2008 an ensemble of multiple models supports the identification of methodological and technical differences and shortcomings between the different models and covers a wide set of different parameterisations of processes this can improve the understanding of model performance and guide future model development frassl et al 2019 janssen et al 2015 model ensembles are now widely used in meteorological forecasting gneiting and raftery 2005 leutbecher and palmer 2008 flood forecasting wu et al 2020 and climate studies mu et al 2017 parker 2010 ensemble models have gained momentum in large scale water quality studies van vliet et al 2019 but their adoption in limnology has been slow we believe the limnology community recognises the benefits of using ensembles and multi model simulations nielsen et al 2014 stepanenko et al 2010 but lacks scientific software to facilitate lake ensemble modelling past efforts to apply multiple lake models to the same study systems nielsen et al 2014 trolle et al 2014 yao et al 2014 isimip frieler et al 2017 gal et al 2020 kobler and schmid 2019 lakemip stepanenko et al 2010 have often been the result of large international collaborations while these initiatives have revealed pertinent new information the labour required to build these networks is a barrier to broader implementation to remove these barriers and facilitate running ensembles of lake models we developed lakeensemblr here we describe the package version 1 0 0 and apply it to predict temperature and ice cover in two lakes lakeensemblr is a numerical framework to run five 1d hydrodynamic lake models simultaneously see supplement table c1 using the same configuration and driver data in the form of a package in the r software environment r core team 2020 the model source codes are open source and the model executables can be run on windows macos and linux platforms the two main objectives of lakeensemblr are a to improve the accessibility of different hydrodynamic models for new users and b to allow experienced users to utilise the powerful approach of running an ensemble of lake models in a consistent and coherent framework these two aims are achieved through six key aspects of its functionality 1 facilitating easy setup and configuration of model files 2 running all models with standardised input files 3 standardising model output 4 providing tools for convenient post processing 5 standardising calibration routines and 6 aggregating and enabling for ensemble averaging to account for different sources of uncertainty between the models the structure of the package allows future development and addition of more models and the code is freely accessible under a gnu general public license v2 0 2 methods 2 1 model descriptions 2 1 1 flake flake freshwater lake model see supplement table c1 is a bulk model that was developed primarily for fast lake to atmosphere coupling within numerical weather prediction models mironov 2008 2005 flake simulates lake systems using a two layer parametric representation focusing on the heat budget the upper well mixed layer is considered thermally homogeneous whereas the temperature in the lower stably stratified layer is approximated by a self similar dimensionless shape profile flake also uses self similarity to model ice and sediment temperatures due to its computational efficiency flake has been widely used in numerical weather prediction models mironov et al 2010 šeparović et al 2013 and lake studies on both global and local scale thiery et al 2014 vörös et al 2010 woolway et al 2019 lakeensemblr version 1 0 0 uses a version of flake that has been adapted to include heat input through inflows pers comm georgiy kirillin the default flake model option implemented in lakeensemblr simulates the vertical temperature dynamics up to the mean depth of the lake as flake assumes a rectangular shape of the basin and does not incorporate the lake s specific hypsography the assumptions of flake match best when using the mean depth of the lake therefore the flake simulations extend to a shallower depth than the other hydrodynamic models 2 1 2 glm the general lake model glm see supplement table c1 is a vertical 1d hydrodynamic lake model developed by the university of western australia hipsey et al 2019 glm applies a flexible lagrangian structure to replicate mixing dynamics here neighboring layers either split or merge depending on the density of the layers surface mixing dynamics are calculated via an energy balance approach where the available kinetic energy is compared to the potential energy of the water column the model has been widely applied for example to simulate seasonal dynamics of temperature and ice cover bueche et al 2017 fenocchi et al 2018 project impacts of water management measures on lake ecosystems feldbauer et al 2020 ladwig et al 2018 weber et al 2017 and to assess scenarios regarding extreme events mi et al 2018 soares et al 2019 it has also been rigorously tested in a large number of lakes bruce et al 2018 in the version 1 0 0 of lakeensemblr version 3 1 0 of glm is used 2 1 3 gotm the general ocean turbulence model gotm see supplement table c1 was developed by burchard et al 1999 it is a vertical 1d hydrodynamic water column model that includes important hydrodynamic and thermodynamic processes related to vertical mixing in natural waters umlauf et al 2005 it was initially developed for modelling turbulence in the ocean burchard et al 2006 but it has been adapted for use in hydrodynamic modelling in lakes sachse et al 2014 gotm has been used to model the dissolution of co2 in lakes enstad et al 2008 extreme events in a eutrophic marine system ciglenečki et al 2015 impact of macrophytes on water quality sachse et al 2014 and hindcasting and future climate change projections of the thermal structure of a lake ayala et al 2020 moras et al 2019 lakeensemblr version 1 0 0 uses version 5 4 0 of the lake branch of gotm 2 1 4 simstrat simstrat is a vertical 1d hydrodynamic lake model see supplement table c1 combining a buoyancy extended k epsilon model with seiche parameterisation and was originally developed by goudsmit et al 2002 simulated variables include surface energy fluxes and vertical profiles of turbulent diffusivity and water temperature multiple options for external forcing are available as well as variable wind drag coefficients inflow settings and ice and snow formation gaudard et al 2019 simstrat has been successfully applied in lakes and reservoirs of varying morphometry in different climate zones and in scenarios regarding climate warming kobler and schmid 2019 schwefel et al 2016 stepanenko et al 2013 thiery et al 2014 the model is currently maintained by the surface waters research and management department of eawag switzerland and version 2 4 1 is currently used in lakeensemblr 2 1 5 mylake mylake multi year lake simulation model see supplement table c1 is a vertical 1d lake model developed and hosted by the norwegian institute for water research niva the university of helsinki finland and université laval canada saloranta and andersen 2007 mylake simulates daily vertical profiles of lake water temperature density stratification seasonal ice and snow cover sediment water dynamics and phosphorus phytoplankton interactions saloranta and andersen 2007 the model has been used to simulate water temperature ice and phytoplankton dynamics in mostly northern and alpine regions couture et al 2018 kobler and schmid 2019 saloranta et al 2009 the version used in lakeensemblr version 1 0 0 is written in r and corresponds to the mylake matlab version 1 2 2 2 r package description r is an open source and freely available statistical program that is widely used in the limnological community and has previously been used for community developed tools such as rlakeanalyzer read et al 2011 winslow et al 2019 and lakemetabolizer winslow et al 2016 all core functions in lakeensemblr version 1 0 0 have associated documentation with replicable examples all of which can be accessed through help functions within r tested with versions 3 6 2 and 4 0 2 r core team 2020 2 2 1 main workflow the package works with one centralised configuration file in which the user defines the settings of the model run and provides the locations of the standardised input files see box 1 the package exports the settings in the configuration file and the standardised input files to the requirements of each individual model export config function after which the models can be run run ensemble function the resulting water temperatures densities and ice cover thickness of the individual models are then compiled into a netcdf file and can be extracted or plotted in r fig 1 if observations are provided these are added to the netcdf file as well optionally this process can be repeated with different forcing files or different parameter sets to add multiple ensemble members to the netcdf run ensemble function add true argument this supports multi model ensembles as well as simulations of multiple parameterisations of the same model s the combined model output can either be stored in text or netcdf format in case observations are provided parameter values of the different models can be calibrated cali ensemble function see section calibration algorithms fig 1 2 2 2 data requirements the minimum data requirements to run lakeensemblr are a hypsographic file a light extinction coefficient an initial temperature profile and a time series of meteorological forcing variables in the lakeensemblr configuration file the user needs to provide the location of the files the files should have specific headings so the program can identify what information is provided see supplement a in the hypsographic file the surface area m2 per depth m of the lake is given the light extinction coefficient m 1 can be either given as a single value or varying over time an initial temperature profile is needed if temperature observations are not provided for the simulation starting date the meteorological forcing must have a constant time step and not contain missing values required meteorological forcing data include air temperature c and downwelling shortwave radiation w m2 wind speed m s needs to be given as well either as a scalar or a vector including wind direction either relative humidity or dewpoint temperature c needs to be provided and if relative humidity is not provided it is calculated from dewpoint temperature and air temperature according to the weathermetrics package anderson et al 2013 downwelling longwave radiation w m2 can either be provided directly to the models or if it is not will be calculated internally from cloud cover air temperature c and humidity relative humidity or dewpoint temperature according to konzelmann et al 1994 air pressure at lake surface level is also needed to run the models but air pressure at sea level can be provided instead in which case air pressure at lake surface level is estimated using the barometric formula assuming a sea level temperature of 15 c berberan santos et al 1997 lastly providing precipitation mm h or mm d is optional but omitting it will cause the models that require precipitation gotm and glm to be run with a precipitation of 0 which may result in issues with the water balance the influence of direct precipitation on the heat budget tends to be minimal imboden and wüest 1995 optional data that can be provided are discharge m3 s temperature c and salinity psu of inflows as well as water temperature and ice thickness observations in the present version of lakeensemblr outflow discharges can only be set to be identical to inflows due to the many differences between the models in water balance calculations varying water levels are therefore not yet supported although users can change model specific settings related to the water balance observations are used for initialising temperature profiles calibration and plotting if provided observations are added to the output netcdf file 2 2 3 getting started the lakeensemblr code is available on github https github com aemon j lakeensemblr and needs to be installed into the r environment following instructions on the github page lakeensemblr itself cannot run the models but instead this is done through supporting r packages flaker glm3r gotmr simstratr mylaker which contain ways of running each model on the platforms windows macos or linux through executables contained in the packages or having the model code in r after lakeensemblr is installed a folder containing the setup for the ensemble run should be created this can be done by editing the template folder provided within the package or by copying a setup from https github com aemon j ler examples the lakeensemblr configuration file in yaml format contains all modifiable settings and input file paths the input files themselves e g for meteorology or inflows need to be in comma delimited format and need to have the correct column headers templates for any file can be generated through the get template function once the configuration file and the input files have been set up the export config function can be run this function exports the settings in the lakeensemblr configuration file and the lakeensemblr input files as required by each individual model this means that for some models units are converted model parameters are changed or input files are saved in a different format the setup for each individual model is placed in its own directory after running export config the ensemble can be run through the run ensemble function in each model folder the model specific output is generated which is then written to a netcdf file or text files user choice in a shared output folder run ensemble runs the models without calibration the cali ensemble function runs the calibration following the specifications in the calibration section of the lakeensemblr configuration file and stores the results of the calibration in the folder specified by the out f argument if netcdf output is chosen several functions are available in the package to visualise the output plot heatmap plot ensemble plot resid load the data into r load var determine start and end of stratification and ice cover analyse ncdf or calculate goodness of fit calc fit each function has documentation that can be loaded in r by typing name function while the running and calibration of the models is controlled by the r code both the input and output files are in formats that are accessible by a wide array of software therefore it is possible for users to do the pre and post processing with different software a vignette is available on the lakeensemblr github repository which describes step by step how to run an ensemble with multiple code examples a wiki is available with additional information and frequently asked questions 2 2 4 calibration algorithms the lakeensemblr package provides functionality for automated parameter estimation using one of three methods a simple calibration method based on latin hypercube sampling a markov chain monte carlo approach mcmc and a method for constrained fitting of the models to data using one of several available standard optimisation algorithms the last two methods are implementations of the r package fme soetaert and petzoldt 2010 using the functions modmcmc and modfit respectively details about the mcmc and constrained fitting can be obtained from soetaert and petzoldt 2010 and the sources given therein the latin hypercube sampling method uses upper and lower bounds for all parameters that are to be calibrated and then samples evenly within the parameter space given by these bounds e g mckay et al 2000 then the models are run and evaluated for all sampled parameters sets by default six measures of model performance are calculated root mean square error rmse nash sutcliffe efficiency nse pearson correlation coefficient r mean error bias mean absolute error mae and normalised mean absolute error nmae see table c2 in the supplement the user can also supply their own quality function which calculates measures of fit from modeled and observed data each of the three calibration methods can be run in parallel computation where the models are distributed over the available cores the parameters which are to be estimated and their upper and lower bounds if applicable are specified in the master configuration file scaling factors of meteorological forcing are parameters that are often calibrated in models e g ayala et al 2020 gaudard et al 2019 some models within lakeensemblr have internal parameters that scale the meteorological forcing but not all in order to be able to use the same scaling factors for all five models the calibration section of the master configuration file distinguishes between model specific parameters and meteorological scaling parameters all three calibration methods can be used to obtain parameters that optimise the chosen model performance measure for the individual models if common optimum scaling factors for all models in the ensemble are wanted the user needs to apply their own method to aggregate the scaling factors of the models 2 2 5 combining multiple ensemble runs uncertainty of lake model output comes from different sources that are related to forcing data initial conditions model parameter values or structural reasons like process description and numerical methods thomas et al 2020 lakeensemblr foremost tackles the uncertainties related to structural differences between different models but lakeensemblr can also be used to address other sources of uncertainties the run ensemble function allows to add different model runs to a single netcdf file using this functionality model runs with different parameterisations forcing data or initial conditions can be run and compared many diagnostic functions like calc fit or plot ensemble have two additional arguments dim and dim index to select which dimension should be used 3 example application of lakeensemblr we applied the lakeensemblr package to two lake case studies lough feeagh ie and langtjern no lough feeagh is a temperate monomictic lake with a maximum depth of 46 m and a surface area of 3 9 km2 langtjern is a shallow dimictic lake with a maximum depth of 12 m and a surface area of 0 23 km2 langtjern is separated into three distinct basins and our modelling efforts concentrated in the north basin with a maximum depth of 9 m and surface area of 0 06 km2 a detailed description of lough feeagh can be found in allott et al 2005 or de eyto et al 2016 and a detailed description of langtjern can be found in couture et al 2015 henriksen and wright 1977 wright 1983 the latin hypercube sampling method with 500 parameter sets was applied to both study cases for each model the parameter set with the lowest rmse was selected one full year was used to calibrate the models 2013 for lough feeagh may 2014 to may 2015 for langtjern and the following year was reserved for validation of the simulated temperatures scaling factors for wind speed and shortwave radiation were calibrated for all five models and in addition model specific parameters k min gotm coef mix hyp glm c relax c flake a seiche simstrat and c shelter mylake were calibrated as well these parameters were selected from parameters used for calibration in previous studies see supplement table c3 the inflows and outflows were omitted in all simulations for the langtjern simulation hourly meteorological forcing was used to explore water temperature and ice dynamics whereas for lough feeagh the models were calibrated and validated using both hourly and daily averaged values to compare performance of water temperature except for mylake which only operates at the daily time scale in this section we provide an example of how lakeensemblr can be used to partition and quantify different sources of uncertainty boundary conditions initial conditions parameter and structure uncertainty in order to do this the lough feeagh ensemble was run a total of 300 times over a period of 16 days during the stratified period june 12th to june 27th 2013 while different factors were varied to estimate their impact on the simulation output to isolate the effect of initial conditions the models were run using 100 different initial temperature profiles that were drawn from a normal distribution around the observed value with a standard deviation of 0 1 c for boundary conditions the models were forced with 100 different versions of the meteorological data where random noise was added to air temperature and wind speed from normal distributions with a mean of 0 c and a standard deviation of 0 5 c and a mean of 0 m s and a standard deviation of 0 5 m s respectively for parameter uncertainty 100 parameter values were drawn for each calibrated parameter using either a normal or lognormal distribution table c4 to quantify and compare the variation of the different model runs between the different sources of uncertainty the standard deviation of the water temperature for each time step at two depths 0 9 m and 16 m of the output was calculated across the 100 ensembles for each model separately for lough feeagh we additionally ran an ensemble with different parameterisation of the five models to compare the uncertainty related to the chosen model with the uncertainty related to the calibrated parameters and scaling factors for each individual model starting from the latin hypercube calibration using daily forcing data we first selected the best 10 parameter sets in terms of their rmse for each model from these sets we extracted the range of the calibrated parameter and scaling factors and then sampled 20 parameter sets for each model within this range using latin hypercube sampling then we ran the ensemble using these parameter sets and combined all ensemble runs in one netcdf file 3 1 lough feeagh water temperature dynamics both simulations in lough feeagh using daily and hourly meteorological forcing generally produced satisfactory results of simulated temperature in the calibration period compared to other simulations e g arhonditsis et al 2006 or arhonditsis and brett 2004 with rmse 1 3 c for daily forcing table 1 fig 2 and rmse 0 9 c for hourly forcing table 2 except for flake even the uncalibrated model runs had satisfactory model performance and calibration improved the model fits further compared to the calibration period most models performed worse during the validation period table 1 for daily data and table 2 for hourly data except for simstrat during the calibration phase all models tended to underestimate water temperatures over all depths and throughout the year fig 3 on average ranging from about 0 1 c glm hourly forcing tables 2 1 c gotm daily forcing table 1 in general the calibrated model performance was better using hourly forcing data compared to daily forcing data of the five models flake performed poorest when using daily forcing data and glm performed poorest when using hourly forcing data the best performing model differed between hourly and daily forcing data with gotm performing best when using hourly data calibration phase and simstrat performing best when using daily data calibration and validation phase in all models the largest residuals were seen at observed temperatures of 10 15 c during the time of the onset and end of summer stratification and around the depth of the thermocline fig 3 using daily average forcing data the ensemble average was amongst the best performing fits and when using hourly forcing data the ensemble mean outperformed the individual models in most of the calculated performance measures due to errors of individual models cancelling each other out in the ensemble mean tables 1 and 2 3 2 langtjern lake ice dynamics the models flake gotm mylake and simstrat accurately captured the onset of ice cover on langtjern 5 to 9 days while glm had larger errors 10 to 17 days fig 4 the ensemble mean which was calculated by taking the average of the day of year when ice onset and ice off occurred was also relatively accurate 3 to 6 days for capturing the disappearance of ice cover there was larger variability between the models compared to ice onset in both years gotm and simstrat predicted ice off too early 44 to 16 days glm overestimated ice off in 2015 and 2016 by 27 to 19 days respectively whereas flake and mylake predicted ice off relatively accurately both years 1 to 8 days the temperature profiles had a larger rmse for the calibration and validation period in general for langtjern compared to lough feeagh particularly mylake 3 62 4 24 c and gotm 3 36 4 70 c table 3 these models failed to accurately simulate the stratification structure with increased mixing during the summer months leading to larger errors flake had the lowest uncalibrated rmse 2 02 c which was further reduced following calibration 1 08 c for summary plots of langtjern of the model ensemble and residuals see figure b1 and b2 3 3 uncertainty partitioning parameter uncertainty had the largest effect on the standard deviation of water temperatures at the depth of 0 9 m compared to initial conditions and boundary conditions for all the models except flake in lough feeagh fig 5 each of the parameters chosen were to account for mixing within the water column but their implementation in each model is different due to the different formulation of mixing equations in each model also the distributions of these parameters were not comparable between models with some being normally distributed while others were log normal distributed table c4 as such parameter uncertainty cannot accurately be compared between models but it can be accounted for when using a one model ensemble across the different models boundary conditions were more sensitive for glm than for the other models at both 0 9 m and 16 m depth with regards to uncertainty in the initial conditions flake and glm had higher standard deviation at 0 9 m compared with gotm simstrat and mylake glm had a much higher standard deviation at 16 m for initial conditions boundary conditions and parameter uncertainty this is partly due to the strong stratification which is seen in glm figure b3 for parameter uncertainty gotm simstrat and glm had a high standard deviation at 0 9 m and 16 m while it was lower for mylake and flake had the lowest uncertainty s 3 4 multi parameter ensemble the model specific parameters and scaling factors that resulted in good model performance had a broad distribution see figure b4 in the supplement as an example for the model specific parameters of flake glm and simstrat as well as for the shortwave radiation scaling factor for flake and simstrat this distribution spanned more than 75 of the range given in the calibration process this suggests that the chosen parameters are interrelated and there might not be a single best parameter set that the parameters were non sensitive or that the parameter range in the calibration was too narrow the application of a multi parameter ensemble is showing the uncertainty related to not being able to clearly identify a single best parameter set fig 6 the uncertainty of the simulated water temperature was larger during summer months and at greater depths for all models for the water temperature close to the surface 0 9 m depth the uncertainty due to the chosen model was slightly larger than the one related to the calibrated parameters throughout the year for all models at 16 m depth the uncertainty due to the calibrated parameter was about the same as the one related to the used model 3 5 discussion as the simulations with hourly time step in lough feeagh show the ensemble mean can outperform individual lake models which is in line with the findings of trolle et al 2014 and kobler and schmid 2019 for the lough feeagh simulations with a daily time step the simstrat model performed best followed by the ensemble mean and mylake using hourly time steps gotm performed best of the four models individually albeit not as good as the ensemble mean in langtjern flake simulated water temperature profiles best while simstrat and mylake performed the worst although these two models simulated ice on and ice off well in both lough feeagh and langtjern most models performed worse in the validation period than in the calibration period which is to be expected due to the short 1 year calibration period as shown in this study and also observed while testing lakeensemblr in multiple other lakes unpublished results the best performing model could vary per study case and no single model consistently outperformed others this shows an advantage of using ensembles compared to single model simulations which are not likely to provide an optimal fit in every circumstance while ensembles can incorporate individual strengths of multiple models similarly ensemble modelling can highlight weaknesses of individual models compared to others which can further aid in model selection or refinement ensemble predictions also give an indication of the uncertainty due to a different process description or parameterisation this uncertainty may vary over depth or time e g figs 2 and 5 an increased uncertainty in ensemble predictions represents diverging behaviour of different ensemble members it might be important to interpret model predictions during periods with increased uncertainty with additional caution and ensembles are a way to identify these periods for a single set of parameters the investigation of model specific residuals in particular e g fig 3 supports the quantification of uncertainty and the identification of better suited models for specific case studies in the lough feeagh case study the models gotm mylake and simstrat had a bias for simulated water temperatures near the lake bottom and during fall mixing fig 3 a and 3 b by looking at the depth discrete residual dynamics fig 3 c as well as the density distribution of residuals fig 3 d the model with the lowest overall bias for lough feeagh was glm scattering over the whole vertical axis and simstrat negative bias at surface and positive bias at bottom running a calibrated model ensemble allows the user to quantify these model specific biases and uncertainties making scenario projections or forecastings more robust additionally running ensembles with different parameterisations initial conditions or different boundary conditions can help to quantify the uncertainties related to the respective source similarly to kobler and schmid 2019 and yao et al 2014 there was large variation between the different models in predicting ice cover phenology fig 4 however most models captured the overall timing of ice on and ice off which play a key role in the subsequent timing of stratification and several ecological processes in a lake the ensemble represents the large uncertainty that is inherent in modelling lake ice cover sharma et al 2019 which is important to account for when modelling lakes with periodic ice cover it has recently been shown that the ensemble mean of ice timing and thickness can perform better than the individual models kobler and schmid 2019 which was supported here a key part of modeling is being able to identify and quantify the different sources of uncertainty this is especially important if the model is to be used in a forecasting framework thomas et al 2020 used a single one dimensional hydrodynamic model and partitioned out the sources of uncertainty over a 16 day forecast of water temperature profiles in a reservoir using the lakeensemblr framework this can be explored and quantified further using multiple models the brief examples that are shown in figs 5 and 6 are a way in which such an analysis can be conducted and the information gained from this exploration can inform decisions on model and parameter selection 4 summary 4 1 framework lakeensemblr facilitates the pre processing of data that is needed to run multiple 1d models and combines the results into a single standardised output file each model in the package requires a different format and structure of its configuration and input files this has been standardised in lakeensemblr by requiring only one set of input and configuration files and by using the same format for all input files by having to specify a specific header for each column of an input file mistakes involving column order and units are avoided and in the configuration file only a reference to the file location needs to be given instead of having to specify which column contains what information lakeensemblr relies on r packages for each model hosted on github and archived in zenodo see software availability these packages contain pre compiled model executables for the platforms windows macos and linux or the model code in r this greatly facilitates user access to the models as the ability to run the models is gained fully within the r environment some models provide pre compiled executables on their respective websites but often for only one platform which regularly requires users to compile the model themselves lakeensemblr removes this initial hurdle for modellers who want to apply one or multiple models the calibration methods provided in lakeensemblr can all be applied to the models without requiring the user to write custom calibration scripts the ability to use the same calibration method for multiple models increases the comparability of the simulations results in the present study confirm that lakeensemblr s calibration methods can markedly improve model fit like the input each model generates its own specific output often in different file types and consisting of different variables and units lakeensemblr combines these outputs into one standardised format either in text or netcdf this allows quick application of the post processing functions provided in lakeensemblr e g analyse ncdf and plot heatmap but also makes it easier for users to extract output and process the results in their preferred way the standardised output is only generated for variables that are shared between the models however the full model specific output is still available in the model output folders and can be accessed by the users by facilitating pre processing running calibration and post processing lakeensemblr supports accessible model ensemble applications by aquatic modellers new to the field however because all files required to run the models are present in the model folders it in no way restricts more experienced users from using the full functionality of each of the different models the model parameters section of the lakeensemblr configuration file allows the user to change any parameter in the model specific configuration files and files generated by lakeensemblr s export config function can be manually altered before starting the ensemble run 4 2 recommendations for use lakeensemblr eases the configuration running and processing of a hydrodynamic lake model ensemble and allows the user to explore the results in various ways however by making it easier to apply multiple models there is the risk that less attention will be paid to individual model setup and that models may be applied to situations beyond what they were designed and tested for for example by considering five models at once the overall number of parameters increases markedly and the user might be tempted to only use default parameter settings without critical consideration of the consequences in order to properly calibrate a model and avoid problems such as nonuniqueness of calibrated parameter sets i e equifinality see beven 2006 it is important to make deliberate decisions and employ rigorous model validation in addition to looking at single performance metrics for the simulated state variables it is advisable to assess the model s capability to reproduce fluxes and emerging properties patterns and relationships hipsey et al 2020 in order to find and select the right parameters to calibrate the best practice approach would be to apply a sensitivity analysis e g andersen et al 2021 many methods for sensitivity analysis are available but the latin hypercube sampling method included in lakeensemblr can be used as an initial approach to quantify sensitivity where a complete sensitivity analysis is not feasible expert or a priori knowledge on the models should be used to select the calibration parameters in the present study we aimed at demonstrating the possibility of calibration with lakeensemblr rather than exploring the parameter sensitivity of each model and we chose model parameters based on the parameter selection done in previous studies see table c3 in the supplement for parameters that were calibrated in previous studies however the possibility to combine runs with multiple models and parameterisations also is an opportunity to tackle issues regarding sources of uncertainty lakeensemblr can be used to quantify different sources of uncertainty boundary conditions initial conditions parameter model structure increase understanding about what model works best under different circumstances and also within model comparisons can be made although not applied in the present study post processing techniques applied in other research fields such as blending vannitsem et al 2020 can be applied to the ensemble result so that ensemble members are weighted and more information is retrieved from the ensemble however we advocate the use of lakeensemblr within established modelling practices e g arhonditsis and brett 2004 hipsey et al 2020 rather than as a replacement 4 3 outlook the simulations in lough feeagh and langtjern showcase the main functionalities of the package however lakeensemblr can be applied to a wider range of locations and scenarios in long term climate simulations lake model ensembles have been applied as part of the inter sectoral impact model intercomparison project isimip frieler et al 2017 vanderkelen et al 2020 and lakeensemblr can facilitate similar efforts ensembles offer several possibilities for weekly or seasonal forecasting efforts e g krishnamurti et al 2000 thomas et al 2020 and lakeensemblr can be run not only with multiple models but also forced with several different weather forecasts studies of processes in lake physics that are difficult to model such as consequences of extreme weather events mesman et al 2020 or lake ice phenology yao et al 2014 can especially benefit from an ensemble approach while lakeensemblr currently only covers hydrodynamic models its predictions can also serve as input for water quality models such a water quality ensemble can ultimately serve to assess and qualify the performance of multiple aquatic ecosystem models hipsey et al 2020 while also giving uncertainty to the ecological impacts of management scenarios on ecosystems more applications are possible and the modular structure of the lakeensemblr code allows for the addition of new models and continued development although the advantages of ensemble modelling have been acknowledged by the lake modelling community until now no software to run multiple lake models for a single study site was available lakeensemblr provides the necessary tools to widely apply ensembles of 1d lake models additionally to facilitating pre processing of data running of an ensemble of models and standardising output lakeensemblr allows the aquatic science community to start rigorous intra model comparison studies of alternative process based vertical 1d hydrodynamic lake models prior to the development of lakeensemblr having an ensemble of models bound together with a consistent application programming interface rigorous tests and comparison of alternative model codes were rare we sincerely hope that lakeensemblr can provide a consistent framework for lake ensemble studies uncertainty partitioning investigations and intra comparison modelling studies author contributions t n m j p m r l and j f conceptualised the study t n m j p m r l and j f wrote most of the package code with contributions of r p t s and j r t n m j p m r l j f f o r p j j v and j r tested the package during development t n m j p m r l and j f wrote the manuscript with input from the other authors all authors participated in discussions during package development and the publication process funding t n m was funded by the watexr project which is part of era4cs an era net initiated by jpi climate and funded by mineco es formas se bmbf de epa ie rcn no and ifd dk with co funding by the european union grant number 690 462 and also by nsf grants deb 1926050 and dbi 1933016 j p m was funded by the european union shorizon 2020research and innovation programme under the marie skłodowska curie grant agreement no 722518 mantel itn r l was funded through a national science foundation abi development grant dbi 1759865 j f was funded by the european social fund and co financed by tax funds based on the budget approved by the members of the saxon state parliament r m p was funded by sentinel north research internship scholarship program for foreign students at université laval and student travel award support from gleon t s was supported by german science foundation grants dfg ki 853 13 1 and cdz 1259 j j v was supported by a natural sciences and engineering research council of canada discovery grant rgpin 2018 06389 k c r was funded by national science foundation grants 1754265 1638704 and 1761805 software and data availability the lakeensemblr code is available at https github com aemon j lakeensemblr lakeensemblr and the packages it relies upon flaker glm3r gotmr simstratr mylaker glmtools gotmtools can be installed in r following the instructions on the github page using the install github function of the devtools package wickham et al 2020 the packages to run the models do not contain the source code of each model only the executables for windows macos and linux links to the websites of the respective models are provided on github example set ups of lakeensemblr are provided at https github com aemon j ler examples for further instructions on how to run lakeensemblr we refer the reader to the aemon j github page https github com aemon j lakeensemblr where a vignette and a wiki are available with detailed instructions and code examples lakeensemblr version 1 0 0 and the model packages have been archived in zenodo under the following dois lakeensemblr 10 5281 zenodo 4146899 flaker 10 5281 zenodo 4139807 glm3r 10 5281 zenodo 4146848 gotmr 10 5281 zenodo 4139780 simstratr 10 5281 zenodo 4139731 mylaker 10 5281 zenodo 4067998 when using lakeensemblr for a publication please also cite the sources of the respective models that you are including in your ensemble see citation lakeensemblr declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements we thank the global lakes ecological observatory network gleon and the aquatic ecosystem modellers network junior aemon j for bringing this group of researchers together aemon j is a global early career network for aquatic ecosystem modellers that strives to advance the scientific field of numerical modelling in aquatic ecosystems as well as to build a welcoming community for novice and advanced modellers projects like this require a wide range of expertise and this large group made the creation of this r package and the writing of this manuscript possible we also express our gratitude to the research institutes and the individuals that made changes to the models that allowed us to incorporate them into lakeensemblr or provided assistance with bug fixing and compiling most notably igb berlin germany and helmholtz centre for environmental research ufz germany flake the university of western australia australia and virginia tech usa glm eawag switzerland simstrat aarhus university denmark and bolding bruggeman gotm and université laval canada mylake we thank the marine institute ireland and niva norway for collecting and sharing the data used in this study lastly we thank kaelin cawley and other gleon members for their participation in discussions in the early stage of the project reference to trade names does not imply endorsement by the u s government appendix a supplementary data the following is are the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105101 
25782,model ensembles have several benefits compared to single model applications but are not frequently used within the lake modelling community setting up and running multiple lake models can be challenging and time consuming despite the many similarities between the existing models forcing data hypsograph etc here we present an r package lakeensemblr that facilitates running ensembles of five different vertical one dimensional hydrodynamic lake models flake glm gotm simstrat mylake the package requires input in a standardised format and a single configuration file lakeensemblr formats these files to the input required by each model and provides functions to run and calibrate the models the outputs of the different models are compiled into a single file and several post processing operations are supported lakeensemblr s workflow standardisation can simplify model benchmarking and uncertainty quantification and improve collaborations between scientists we showcase the successful application of lakeensemblr for two different lakes graphical abstract image 1 keywords ensemble modeling vertical one dimensional lake model r package calibration thermal structure hydrodynamics 1 introduction numerical process based lake models are powerful tools to simulate processes occurring in aquatic ecosystems these models enable the users to investigate scientific and engineering hypotheses or scenarios which would otherwise not be feasible or even possible to field test for physical logistical political or financial reasons over recent decades the understanding of fluid dynamics and physical transport processes in lakes has improved thanks to enhanced field monitoring and intensive laboratory studies csanady 1975 imberger 1985 imberger and hamblin 1982 imboden 1973 kitaigorodskii and miropolsky 1970 spigel et al 1986 spigel and imberger 1980 with better empirical relationships and physical understanding of processes the pioneer lake models that emerged from these studies were essential to addressing emerging water quality issues like eutrophication french and imberger 1984 today one dimensional 1d lake models are frequently used to characterise lake hydrodynamics these models assume complete and instantaneous horizontal mixing in many systems this is a reasonable assumption because vertical thermal gradients are typically much larger than horizontal thermal gradients the assumption holds for lakes with a small to moderate surface area that are not affected by coriolis acceleration or other significant horizontal transport processes patterson et al 1984 to model water column thermal dynamics resulting from atmospheric exchange processes inflow entrainment and turbulence different theoretical approaches have been developed and applied in lake models e g bulk models energy balance approach models and models that use a pure turbulence approach to account for mixing goudsmit et al 2002 alternative approaches apply simpler schemes to solve advection diffusion equations or use constants for transport processes since the 1980s there has been a rapid expansion in the publication of process based aquatic ecosystem models however the aquatic ecosystem community has not fully exploited the diversity of available models by comparing the performance of models against one another which affords both the opportunity to identify technical improvements but also improve overall model predictions janssen et al 2015 critical voices still highlight the problem that modelling teams tend to reinvent the wheel mooij et al 2010 instead of building on existing software the lake model intercomparison project lakemip had several key findings regarding the current state of lake modelling 1 the majority of lake models replicate surface temperature dynamics coherently well stepanenko et al 2013 2 individual lake models clearly outperform others for specific lake sites thiery et al 2014 and 3 models that explicitly incorporate sediment heating and resolve turbulence over lake depth are better suited to represent lakes in numerical meteorological studies and to research hydrodynamic processes for deep lakes stepanenko et al 2013 thiery et al 2014 most authors agree that open community approaches as well as publishing the model as open source code are the best steps for sustainable development and to ensure future technical improvements frassl et al 2019 janssen et al 2015 read et al 2016 still a lack of common community framework for model calibration validation and processing has resulted in few studies that quantify model performance benchmarking and minimal progress in improving code and applications arhonditsis et al 2014 hipsey et al 2020 in the 1990s atmospheric researchers popularised the use of ensemble modeling in operational forecasting and uncertainty predictions parker 2013 ensemble modeling involves either running the same model multiple times with different settings or running multiple models on the same study site one of the main advantages of model ensembles is that the uncertainty in the model predictions can be estimated trolle et al 2014 wu et al 2020 this allows the modeller to assess the likelihood of occurrence of certain model predictions connected to this ensemble runs of an individual model are a means of taking into account nonuniqueness i e equifinality see beven 2006 in parameter sets gal et al 2014 nielsen et al 2014 the average of individual model runs from different models can be a more robust predictor than any of the individual model runs kobler and schmid 2019 trolle et al 2014 and sources therein if only the best model is retained valuable information in other model fits is disregarded baker and ellison 2008 an ensemble of multiple models supports the identification of methodological and technical differences and shortcomings between the different models and covers a wide set of different parameterisations of processes this can improve the understanding of model performance and guide future model development frassl et al 2019 janssen et al 2015 model ensembles are now widely used in meteorological forecasting gneiting and raftery 2005 leutbecher and palmer 2008 flood forecasting wu et al 2020 and climate studies mu et al 2017 parker 2010 ensemble models have gained momentum in large scale water quality studies van vliet et al 2019 but their adoption in limnology has been slow we believe the limnology community recognises the benefits of using ensembles and multi model simulations nielsen et al 2014 stepanenko et al 2010 but lacks scientific software to facilitate lake ensemble modelling past efforts to apply multiple lake models to the same study systems nielsen et al 2014 trolle et al 2014 yao et al 2014 isimip frieler et al 2017 gal et al 2020 kobler and schmid 2019 lakemip stepanenko et al 2010 have often been the result of large international collaborations while these initiatives have revealed pertinent new information the labour required to build these networks is a barrier to broader implementation to remove these barriers and facilitate running ensembles of lake models we developed lakeensemblr here we describe the package version 1 0 0 and apply it to predict temperature and ice cover in two lakes lakeensemblr is a numerical framework to run five 1d hydrodynamic lake models simultaneously see supplement table c1 using the same configuration and driver data in the form of a package in the r software environment r core team 2020 the model source codes are open source and the model executables can be run on windows macos and linux platforms the two main objectives of lakeensemblr are a to improve the accessibility of different hydrodynamic models for new users and b to allow experienced users to utilise the powerful approach of running an ensemble of lake models in a consistent and coherent framework these two aims are achieved through six key aspects of its functionality 1 facilitating easy setup and configuration of model files 2 running all models with standardised input files 3 standardising model output 4 providing tools for convenient post processing 5 standardising calibration routines and 6 aggregating and enabling for ensemble averaging to account for different sources of uncertainty between the models the structure of the package allows future development and addition of more models and the code is freely accessible under a gnu general public license v2 0 2 methods 2 1 model descriptions 2 1 1 flake flake freshwater lake model see supplement table c1 is a bulk model that was developed primarily for fast lake to atmosphere coupling within numerical weather prediction models mironov 2008 2005 flake simulates lake systems using a two layer parametric representation focusing on the heat budget the upper well mixed layer is considered thermally homogeneous whereas the temperature in the lower stably stratified layer is approximated by a self similar dimensionless shape profile flake also uses self similarity to model ice and sediment temperatures due to its computational efficiency flake has been widely used in numerical weather prediction models mironov et al 2010 šeparović et al 2013 and lake studies on both global and local scale thiery et al 2014 vörös et al 2010 woolway et al 2019 lakeensemblr version 1 0 0 uses a version of flake that has been adapted to include heat input through inflows pers comm georgiy kirillin the default flake model option implemented in lakeensemblr simulates the vertical temperature dynamics up to the mean depth of the lake as flake assumes a rectangular shape of the basin and does not incorporate the lake s specific hypsography the assumptions of flake match best when using the mean depth of the lake therefore the flake simulations extend to a shallower depth than the other hydrodynamic models 2 1 2 glm the general lake model glm see supplement table c1 is a vertical 1d hydrodynamic lake model developed by the university of western australia hipsey et al 2019 glm applies a flexible lagrangian structure to replicate mixing dynamics here neighboring layers either split or merge depending on the density of the layers surface mixing dynamics are calculated via an energy balance approach where the available kinetic energy is compared to the potential energy of the water column the model has been widely applied for example to simulate seasonal dynamics of temperature and ice cover bueche et al 2017 fenocchi et al 2018 project impacts of water management measures on lake ecosystems feldbauer et al 2020 ladwig et al 2018 weber et al 2017 and to assess scenarios regarding extreme events mi et al 2018 soares et al 2019 it has also been rigorously tested in a large number of lakes bruce et al 2018 in the version 1 0 0 of lakeensemblr version 3 1 0 of glm is used 2 1 3 gotm the general ocean turbulence model gotm see supplement table c1 was developed by burchard et al 1999 it is a vertical 1d hydrodynamic water column model that includes important hydrodynamic and thermodynamic processes related to vertical mixing in natural waters umlauf et al 2005 it was initially developed for modelling turbulence in the ocean burchard et al 2006 but it has been adapted for use in hydrodynamic modelling in lakes sachse et al 2014 gotm has been used to model the dissolution of co2 in lakes enstad et al 2008 extreme events in a eutrophic marine system ciglenečki et al 2015 impact of macrophytes on water quality sachse et al 2014 and hindcasting and future climate change projections of the thermal structure of a lake ayala et al 2020 moras et al 2019 lakeensemblr version 1 0 0 uses version 5 4 0 of the lake branch of gotm 2 1 4 simstrat simstrat is a vertical 1d hydrodynamic lake model see supplement table c1 combining a buoyancy extended k epsilon model with seiche parameterisation and was originally developed by goudsmit et al 2002 simulated variables include surface energy fluxes and vertical profiles of turbulent diffusivity and water temperature multiple options for external forcing are available as well as variable wind drag coefficients inflow settings and ice and snow formation gaudard et al 2019 simstrat has been successfully applied in lakes and reservoirs of varying morphometry in different climate zones and in scenarios regarding climate warming kobler and schmid 2019 schwefel et al 2016 stepanenko et al 2013 thiery et al 2014 the model is currently maintained by the surface waters research and management department of eawag switzerland and version 2 4 1 is currently used in lakeensemblr 2 1 5 mylake mylake multi year lake simulation model see supplement table c1 is a vertical 1d lake model developed and hosted by the norwegian institute for water research niva the university of helsinki finland and université laval canada saloranta and andersen 2007 mylake simulates daily vertical profiles of lake water temperature density stratification seasonal ice and snow cover sediment water dynamics and phosphorus phytoplankton interactions saloranta and andersen 2007 the model has been used to simulate water temperature ice and phytoplankton dynamics in mostly northern and alpine regions couture et al 2018 kobler and schmid 2019 saloranta et al 2009 the version used in lakeensemblr version 1 0 0 is written in r and corresponds to the mylake matlab version 1 2 2 2 r package description r is an open source and freely available statistical program that is widely used in the limnological community and has previously been used for community developed tools such as rlakeanalyzer read et al 2011 winslow et al 2019 and lakemetabolizer winslow et al 2016 all core functions in lakeensemblr version 1 0 0 have associated documentation with replicable examples all of which can be accessed through help functions within r tested with versions 3 6 2 and 4 0 2 r core team 2020 2 2 1 main workflow the package works with one centralised configuration file in which the user defines the settings of the model run and provides the locations of the standardised input files see box 1 the package exports the settings in the configuration file and the standardised input files to the requirements of each individual model export config function after which the models can be run run ensemble function the resulting water temperatures densities and ice cover thickness of the individual models are then compiled into a netcdf file and can be extracted or plotted in r fig 1 if observations are provided these are added to the netcdf file as well optionally this process can be repeated with different forcing files or different parameter sets to add multiple ensemble members to the netcdf run ensemble function add true argument this supports multi model ensembles as well as simulations of multiple parameterisations of the same model s the combined model output can either be stored in text or netcdf format in case observations are provided parameter values of the different models can be calibrated cali ensemble function see section calibration algorithms fig 1 2 2 2 data requirements the minimum data requirements to run lakeensemblr are a hypsographic file a light extinction coefficient an initial temperature profile and a time series of meteorological forcing variables in the lakeensemblr configuration file the user needs to provide the location of the files the files should have specific headings so the program can identify what information is provided see supplement a in the hypsographic file the surface area m2 per depth m of the lake is given the light extinction coefficient m 1 can be either given as a single value or varying over time an initial temperature profile is needed if temperature observations are not provided for the simulation starting date the meteorological forcing must have a constant time step and not contain missing values required meteorological forcing data include air temperature c and downwelling shortwave radiation w m2 wind speed m s needs to be given as well either as a scalar or a vector including wind direction either relative humidity or dewpoint temperature c needs to be provided and if relative humidity is not provided it is calculated from dewpoint temperature and air temperature according to the weathermetrics package anderson et al 2013 downwelling longwave radiation w m2 can either be provided directly to the models or if it is not will be calculated internally from cloud cover air temperature c and humidity relative humidity or dewpoint temperature according to konzelmann et al 1994 air pressure at lake surface level is also needed to run the models but air pressure at sea level can be provided instead in which case air pressure at lake surface level is estimated using the barometric formula assuming a sea level temperature of 15 c berberan santos et al 1997 lastly providing precipitation mm h or mm d is optional but omitting it will cause the models that require precipitation gotm and glm to be run with a precipitation of 0 which may result in issues with the water balance the influence of direct precipitation on the heat budget tends to be minimal imboden and wüest 1995 optional data that can be provided are discharge m3 s temperature c and salinity psu of inflows as well as water temperature and ice thickness observations in the present version of lakeensemblr outflow discharges can only be set to be identical to inflows due to the many differences between the models in water balance calculations varying water levels are therefore not yet supported although users can change model specific settings related to the water balance observations are used for initialising temperature profiles calibration and plotting if provided observations are added to the output netcdf file 2 2 3 getting started the lakeensemblr code is available on github https github com aemon j lakeensemblr and needs to be installed into the r environment following instructions on the github page lakeensemblr itself cannot run the models but instead this is done through supporting r packages flaker glm3r gotmr simstratr mylaker which contain ways of running each model on the platforms windows macos or linux through executables contained in the packages or having the model code in r after lakeensemblr is installed a folder containing the setup for the ensemble run should be created this can be done by editing the template folder provided within the package or by copying a setup from https github com aemon j ler examples the lakeensemblr configuration file in yaml format contains all modifiable settings and input file paths the input files themselves e g for meteorology or inflows need to be in comma delimited format and need to have the correct column headers templates for any file can be generated through the get template function once the configuration file and the input files have been set up the export config function can be run this function exports the settings in the lakeensemblr configuration file and the lakeensemblr input files as required by each individual model this means that for some models units are converted model parameters are changed or input files are saved in a different format the setup for each individual model is placed in its own directory after running export config the ensemble can be run through the run ensemble function in each model folder the model specific output is generated which is then written to a netcdf file or text files user choice in a shared output folder run ensemble runs the models without calibration the cali ensemble function runs the calibration following the specifications in the calibration section of the lakeensemblr configuration file and stores the results of the calibration in the folder specified by the out f argument if netcdf output is chosen several functions are available in the package to visualise the output plot heatmap plot ensemble plot resid load the data into r load var determine start and end of stratification and ice cover analyse ncdf or calculate goodness of fit calc fit each function has documentation that can be loaded in r by typing name function while the running and calibration of the models is controlled by the r code both the input and output files are in formats that are accessible by a wide array of software therefore it is possible for users to do the pre and post processing with different software a vignette is available on the lakeensemblr github repository which describes step by step how to run an ensemble with multiple code examples a wiki is available with additional information and frequently asked questions 2 2 4 calibration algorithms the lakeensemblr package provides functionality for automated parameter estimation using one of three methods a simple calibration method based on latin hypercube sampling a markov chain monte carlo approach mcmc and a method for constrained fitting of the models to data using one of several available standard optimisation algorithms the last two methods are implementations of the r package fme soetaert and petzoldt 2010 using the functions modmcmc and modfit respectively details about the mcmc and constrained fitting can be obtained from soetaert and petzoldt 2010 and the sources given therein the latin hypercube sampling method uses upper and lower bounds for all parameters that are to be calibrated and then samples evenly within the parameter space given by these bounds e g mckay et al 2000 then the models are run and evaluated for all sampled parameters sets by default six measures of model performance are calculated root mean square error rmse nash sutcliffe efficiency nse pearson correlation coefficient r mean error bias mean absolute error mae and normalised mean absolute error nmae see table c2 in the supplement the user can also supply their own quality function which calculates measures of fit from modeled and observed data each of the three calibration methods can be run in parallel computation where the models are distributed over the available cores the parameters which are to be estimated and their upper and lower bounds if applicable are specified in the master configuration file scaling factors of meteorological forcing are parameters that are often calibrated in models e g ayala et al 2020 gaudard et al 2019 some models within lakeensemblr have internal parameters that scale the meteorological forcing but not all in order to be able to use the same scaling factors for all five models the calibration section of the master configuration file distinguishes between model specific parameters and meteorological scaling parameters all three calibration methods can be used to obtain parameters that optimise the chosen model performance measure for the individual models if common optimum scaling factors for all models in the ensemble are wanted the user needs to apply their own method to aggregate the scaling factors of the models 2 2 5 combining multiple ensemble runs uncertainty of lake model output comes from different sources that are related to forcing data initial conditions model parameter values or structural reasons like process description and numerical methods thomas et al 2020 lakeensemblr foremost tackles the uncertainties related to structural differences between different models but lakeensemblr can also be used to address other sources of uncertainties the run ensemble function allows to add different model runs to a single netcdf file using this functionality model runs with different parameterisations forcing data or initial conditions can be run and compared many diagnostic functions like calc fit or plot ensemble have two additional arguments dim and dim index to select which dimension should be used 3 example application of lakeensemblr we applied the lakeensemblr package to two lake case studies lough feeagh ie and langtjern no lough feeagh is a temperate monomictic lake with a maximum depth of 46 m and a surface area of 3 9 km2 langtjern is a shallow dimictic lake with a maximum depth of 12 m and a surface area of 0 23 km2 langtjern is separated into three distinct basins and our modelling efforts concentrated in the north basin with a maximum depth of 9 m and surface area of 0 06 km2 a detailed description of lough feeagh can be found in allott et al 2005 or de eyto et al 2016 and a detailed description of langtjern can be found in couture et al 2015 henriksen and wright 1977 wright 1983 the latin hypercube sampling method with 500 parameter sets was applied to both study cases for each model the parameter set with the lowest rmse was selected one full year was used to calibrate the models 2013 for lough feeagh may 2014 to may 2015 for langtjern and the following year was reserved for validation of the simulated temperatures scaling factors for wind speed and shortwave radiation were calibrated for all five models and in addition model specific parameters k min gotm coef mix hyp glm c relax c flake a seiche simstrat and c shelter mylake were calibrated as well these parameters were selected from parameters used for calibration in previous studies see supplement table c3 the inflows and outflows were omitted in all simulations for the langtjern simulation hourly meteorological forcing was used to explore water temperature and ice dynamics whereas for lough feeagh the models were calibrated and validated using both hourly and daily averaged values to compare performance of water temperature except for mylake which only operates at the daily time scale in this section we provide an example of how lakeensemblr can be used to partition and quantify different sources of uncertainty boundary conditions initial conditions parameter and structure uncertainty in order to do this the lough feeagh ensemble was run a total of 300 times over a period of 16 days during the stratified period june 12th to june 27th 2013 while different factors were varied to estimate their impact on the simulation output to isolate the effect of initial conditions the models were run using 100 different initial temperature profiles that were drawn from a normal distribution around the observed value with a standard deviation of 0 1 c for boundary conditions the models were forced with 100 different versions of the meteorological data where random noise was added to air temperature and wind speed from normal distributions with a mean of 0 c and a standard deviation of 0 5 c and a mean of 0 m s and a standard deviation of 0 5 m s respectively for parameter uncertainty 100 parameter values were drawn for each calibrated parameter using either a normal or lognormal distribution table c4 to quantify and compare the variation of the different model runs between the different sources of uncertainty the standard deviation of the water temperature for each time step at two depths 0 9 m and 16 m of the output was calculated across the 100 ensembles for each model separately for lough feeagh we additionally ran an ensemble with different parameterisation of the five models to compare the uncertainty related to the chosen model with the uncertainty related to the calibrated parameters and scaling factors for each individual model starting from the latin hypercube calibration using daily forcing data we first selected the best 10 parameter sets in terms of their rmse for each model from these sets we extracted the range of the calibrated parameter and scaling factors and then sampled 20 parameter sets for each model within this range using latin hypercube sampling then we ran the ensemble using these parameter sets and combined all ensemble runs in one netcdf file 3 1 lough feeagh water temperature dynamics both simulations in lough feeagh using daily and hourly meteorological forcing generally produced satisfactory results of simulated temperature in the calibration period compared to other simulations e g arhonditsis et al 2006 or arhonditsis and brett 2004 with rmse 1 3 c for daily forcing table 1 fig 2 and rmse 0 9 c for hourly forcing table 2 except for flake even the uncalibrated model runs had satisfactory model performance and calibration improved the model fits further compared to the calibration period most models performed worse during the validation period table 1 for daily data and table 2 for hourly data except for simstrat during the calibration phase all models tended to underestimate water temperatures over all depths and throughout the year fig 3 on average ranging from about 0 1 c glm hourly forcing tables 2 1 c gotm daily forcing table 1 in general the calibrated model performance was better using hourly forcing data compared to daily forcing data of the five models flake performed poorest when using daily forcing data and glm performed poorest when using hourly forcing data the best performing model differed between hourly and daily forcing data with gotm performing best when using hourly data calibration phase and simstrat performing best when using daily data calibration and validation phase in all models the largest residuals were seen at observed temperatures of 10 15 c during the time of the onset and end of summer stratification and around the depth of the thermocline fig 3 using daily average forcing data the ensemble average was amongst the best performing fits and when using hourly forcing data the ensemble mean outperformed the individual models in most of the calculated performance measures due to errors of individual models cancelling each other out in the ensemble mean tables 1 and 2 3 2 langtjern lake ice dynamics the models flake gotm mylake and simstrat accurately captured the onset of ice cover on langtjern 5 to 9 days while glm had larger errors 10 to 17 days fig 4 the ensemble mean which was calculated by taking the average of the day of year when ice onset and ice off occurred was also relatively accurate 3 to 6 days for capturing the disappearance of ice cover there was larger variability between the models compared to ice onset in both years gotm and simstrat predicted ice off too early 44 to 16 days glm overestimated ice off in 2015 and 2016 by 27 to 19 days respectively whereas flake and mylake predicted ice off relatively accurately both years 1 to 8 days the temperature profiles had a larger rmse for the calibration and validation period in general for langtjern compared to lough feeagh particularly mylake 3 62 4 24 c and gotm 3 36 4 70 c table 3 these models failed to accurately simulate the stratification structure with increased mixing during the summer months leading to larger errors flake had the lowest uncalibrated rmse 2 02 c which was further reduced following calibration 1 08 c for summary plots of langtjern of the model ensemble and residuals see figure b1 and b2 3 3 uncertainty partitioning parameter uncertainty had the largest effect on the standard deviation of water temperatures at the depth of 0 9 m compared to initial conditions and boundary conditions for all the models except flake in lough feeagh fig 5 each of the parameters chosen were to account for mixing within the water column but their implementation in each model is different due to the different formulation of mixing equations in each model also the distributions of these parameters were not comparable between models with some being normally distributed while others were log normal distributed table c4 as such parameter uncertainty cannot accurately be compared between models but it can be accounted for when using a one model ensemble across the different models boundary conditions were more sensitive for glm than for the other models at both 0 9 m and 16 m depth with regards to uncertainty in the initial conditions flake and glm had higher standard deviation at 0 9 m compared with gotm simstrat and mylake glm had a much higher standard deviation at 16 m for initial conditions boundary conditions and parameter uncertainty this is partly due to the strong stratification which is seen in glm figure b3 for parameter uncertainty gotm simstrat and glm had a high standard deviation at 0 9 m and 16 m while it was lower for mylake and flake had the lowest uncertainty s 3 4 multi parameter ensemble the model specific parameters and scaling factors that resulted in good model performance had a broad distribution see figure b4 in the supplement as an example for the model specific parameters of flake glm and simstrat as well as for the shortwave radiation scaling factor for flake and simstrat this distribution spanned more than 75 of the range given in the calibration process this suggests that the chosen parameters are interrelated and there might not be a single best parameter set that the parameters were non sensitive or that the parameter range in the calibration was too narrow the application of a multi parameter ensemble is showing the uncertainty related to not being able to clearly identify a single best parameter set fig 6 the uncertainty of the simulated water temperature was larger during summer months and at greater depths for all models for the water temperature close to the surface 0 9 m depth the uncertainty due to the chosen model was slightly larger than the one related to the calibrated parameters throughout the year for all models at 16 m depth the uncertainty due to the calibrated parameter was about the same as the one related to the used model 3 5 discussion as the simulations with hourly time step in lough feeagh show the ensemble mean can outperform individual lake models which is in line with the findings of trolle et al 2014 and kobler and schmid 2019 for the lough feeagh simulations with a daily time step the simstrat model performed best followed by the ensemble mean and mylake using hourly time steps gotm performed best of the four models individually albeit not as good as the ensemble mean in langtjern flake simulated water temperature profiles best while simstrat and mylake performed the worst although these two models simulated ice on and ice off well in both lough feeagh and langtjern most models performed worse in the validation period than in the calibration period which is to be expected due to the short 1 year calibration period as shown in this study and also observed while testing lakeensemblr in multiple other lakes unpublished results the best performing model could vary per study case and no single model consistently outperformed others this shows an advantage of using ensembles compared to single model simulations which are not likely to provide an optimal fit in every circumstance while ensembles can incorporate individual strengths of multiple models similarly ensemble modelling can highlight weaknesses of individual models compared to others which can further aid in model selection or refinement ensemble predictions also give an indication of the uncertainty due to a different process description or parameterisation this uncertainty may vary over depth or time e g figs 2 and 5 an increased uncertainty in ensemble predictions represents diverging behaviour of different ensemble members it might be important to interpret model predictions during periods with increased uncertainty with additional caution and ensembles are a way to identify these periods for a single set of parameters the investigation of model specific residuals in particular e g fig 3 supports the quantification of uncertainty and the identification of better suited models for specific case studies in the lough feeagh case study the models gotm mylake and simstrat had a bias for simulated water temperatures near the lake bottom and during fall mixing fig 3 a and 3 b by looking at the depth discrete residual dynamics fig 3 c as well as the density distribution of residuals fig 3 d the model with the lowest overall bias for lough feeagh was glm scattering over the whole vertical axis and simstrat negative bias at surface and positive bias at bottom running a calibrated model ensemble allows the user to quantify these model specific biases and uncertainties making scenario projections or forecastings more robust additionally running ensembles with different parameterisations initial conditions or different boundary conditions can help to quantify the uncertainties related to the respective source similarly to kobler and schmid 2019 and yao et al 2014 there was large variation between the different models in predicting ice cover phenology fig 4 however most models captured the overall timing of ice on and ice off which play a key role in the subsequent timing of stratification and several ecological processes in a lake the ensemble represents the large uncertainty that is inherent in modelling lake ice cover sharma et al 2019 which is important to account for when modelling lakes with periodic ice cover it has recently been shown that the ensemble mean of ice timing and thickness can perform better than the individual models kobler and schmid 2019 which was supported here a key part of modeling is being able to identify and quantify the different sources of uncertainty this is especially important if the model is to be used in a forecasting framework thomas et al 2020 used a single one dimensional hydrodynamic model and partitioned out the sources of uncertainty over a 16 day forecast of water temperature profiles in a reservoir using the lakeensemblr framework this can be explored and quantified further using multiple models the brief examples that are shown in figs 5 and 6 are a way in which such an analysis can be conducted and the information gained from this exploration can inform decisions on model and parameter selection 4 summary 4 1 framework lakeensemblr facilitates the pre processing of data that is needed to run multiple 1d models and combines the results into a single standardised output file each model in the package requires a different format and structure of its configuration and input files this has been standardised in lakeensemblr by requiring only one set of input and configuration files and by using the same format for all input files by having to specify a specific header for each column of an input file mistakes involving column order and units are avoided and in the configuration file only a reference to the file location needs to be given instead of having to specify which column contains what information lakeensemblr relies on r packages for each model hosted on github and archived in zenodo see software availability these packages contain pre compiled model executables for the platforms windows macos and linux or the model code in r this greatly facilitates user access to the models as the ability to run the models is gained fully within the r environment some models provide pre compiled executables on their respective websites but often for only one platform which regularly requires users to compile the model themselves lakeensemblr removes this initial hurdle for modellers who want to apply one or multiple models the calibration methods provided in lakeensemblr can all be applied to the models without requiring the user to write custom calibration scripts the ability to use the same calibration method for multiple models increases the comparability of the simulations results in the present study confirm that lakeensemblr s calibration methods can markedly improve model fit like the input each model generates its own specific output often in different file types and consisting of different variables and units lakeensemblr combines these outputs into one standardised format either in text or netcdf this allows quick application of the post processing functions provided in lakeensemblr e g analyse ncdf and plot heatmap but also makes it easier for users to extract output and process the results in their preferred way the standardised output is only generated for variables that are shared between the models however the full model specific output is still available in the model output folders and can be accessed by the users by facilitating pre processing running calibration and post processing lakeensemblr supports accessible model ensemble applications by aquatic modellers new to the field however because all files required to run the models are present in the model folders it in no way restricts more experienced users from using the full functionality of each of the different models the model parameters section of the lakeensemblr configuration file allows the user to change any parameter in the model specific configuration files and files generated by lakeensemblr s export config function can be manually altered before starting the ensemble run 4 2 recommendations for use lakeensemblr eases the configuration running and processing of a hydrodynamic lake model ensemble and allows the user to explore the results in various ways however by making it easier to apply multiple models there is the risk that less attention will be paid to individual model setup and that models may be applied to situations beyond what they were designed and tested for for example by considering five models at once the overall number of parameters increases markedly and the user might be tempted to only use default parameter settings without critical consideration of the consequences in order to properly calibrate a model and avoid problems such as nonuniqueness of calibrated parameter sets i e equifinality see beven 2006 it is important to make deliberate decisions and employ rigorous model validation in addition to looking at single performance metrics for the simulated state variables it is advisable to assess the model s capability to reproduce fluxes and emerging properties patterns and relationships hipsey et al 2020 in order to find and select the right parameters to calibrate the best practice approach would be to apply a sensitivity analysis e g andersen et al 2021 many methods for sensitivity analysis are available but the latin hypercube sampling method included in lakeensemblr can be used as an initial approach to quantify sensitivity where a complete sensitivity analysis is not feasible expert or a priori knowledge on the models should be used to select the calibration parameters in the present study we aimed at demonstrating the possibility of calibration with lakeensemblr rather than exploring the parameter sensitivity of each model and we chose model parameters based on the parameter selection done in previous studies see table c3 in the supplement for parameters that were calibrated in previous studies however the possibility to combine runs with multiple models and parameterisations also is an opportunity to tackle issues regarding sources of uncertainty lakeensemblr can be used to quantify different sources of uncertainty boundary conditions initial conditions parameter model structure increase understanding about what model works best under different circumstances and also within model comparisons can be made although not applied in the present study post processing techniques applied in other research fields such as blending vannitsem et al 2020 can be applied to the ensemble result so that ensemble members are weighted and more information is retrieved from the ensemble however we advocate the use of lakeensemblr within established modelling practices e g arhonditsis and brett 2004 hipsey et al 2020 rather than as a replacement 4 3 outlook the simulations in lough feeagh and langtjern showcase the main functionalities of the package however lakeensemblr can be applied to a wider range of locations and scenarios in long term climate simulations lake model ensembles have been applied as part of the inter sectoral impact model intercomparison project isimip frieler et al 2017 vanderkelen et al 2020 and lakeensemblr can facilitate similar efforts ensembles offer several possibilities for weekly or seasonal forecasting efforts e g krishnamurti et al 2000 thomas et al 2020 and lakeensemblr can be run not only with multiple models but also forced with several different weather forecasts studies of processes in lake physics that are difficult to model such as consequences of extreme weather events mesman et al 2020 or lake ice phenology yao et al 2014 can especially benefit from an ensemble approach while lakeensemblr currently only covers hydrodynamic models its predictions can also serve as input for water quality models such a water quality ensemble can ultimately serve to assess and qualify the performance of multiple aquatic ecosystem models hipsey et al 2020 while also giving uncertainty to the ecological impacts of management scenarios on ecosystems more applications are possible and the modular structure of the lakeensemblr code allows for the addition of new models and continued development although the advantages of ensemble modelling have been acknowledged by the lake modelling community until now no software to run multiple lake models for a single study site was available lakeensemblr provides the necessary tools to widely apply ensembles of 1d lake models additionally to facilitating pre processing of data running of an ensemble of models and standardising output lakeensemblr allows the aquatic science community to start rigorous intra model comparison studies of alternative process based vertical 1d hydrodynamic lake models prior to the development of lakeensemblr having an ensemble of models bound together with a consistent application programming interface rigorous tests and comparison of alternative model codes were rare we sincerely hope that lakeensemblr can provide a consistent framework for lake ensemble studies uncertainty partitioning investigations and intra comparison modelling studies author contributions t n m j p m r l and j f conceptualised the study t n m j p m r l and j f wrote most of the package code with contributions of r p t s and j r t n m j p m r l j f f o r p j j v and j r tested the package during development t n m j p m r l and j f wrote the manuscript with input from the other authors all authors participated in discussions during package development and the publication process funding t n m was funded by the watexr project which is part of era4cs an era net initiated by jpi climate and funded by mineco es formas se bmbf de epa ie rcn no and ifd dk with co funding by the european union grant number 690 462 and also by nsf grants deb 1926050 and dbi 1933016 j p m was funded by the european union shorizon 2020research and innovation programme under the marie skłodowska curie grant agreement no 722518 mantel itn r l was funded through a national science foundation abi development grant dbi 1759865 j f was funded by the european social fund and co financed by tax funds based on the budget approved by the members of the saxon state parliament r m p was funded by sentinel north research internship scholarship program for foreign students at université laval and student travel award support from gleon t s was supported by german science foundation grants dfg ki 853 13 1 and cdz 1259 j j v was supported by a natural sciences and engineering research council of canada discovery grant rgpin 2018 06389 k c r was funded by national science foundation grants 1754265 1638704 and 1761805 software and data availability the lakeensemblr code is available at https github com aemon j lakeensemblr lakeensemblr and the packages it relies upon flaker glm3r gotmr simstratr mylaker glmtools gotmtools can be installed in r following the instructions on the github page using the install github function of the devtools package wickham et al 2020 the packages to run the models do not contain the source code of each model only the executables for windows macos and linux links to the websites of the respective models are provided on github example set ups of lakeensemblr are provided at https github com aemon j ler examples for further instructions on how to run lakeensemblr we refer the reader to the aemon j github page https github com aemon j lakeensemblr where a vignette and a wiki are available with detailed instructions and code examples lakeensemblr version 1 0 0 and the model packages have been archived in zenodo under the following dois lakeensemblr 10 5281 zenodo 4146899 flaker 10 5281 zenodo 4139807 glm3r 10 5281 zenodo 4146848 gotmr 10 5281 zenodo 4139780 simstratr 10 5281 zenodo 4139731 mylaker 10 5281 zenodo 4067998 when using lakeensemblr for a publication please also cite the sources of the respective models that you are including in your ensemble see citation lakeensemblr declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements we thank the global lakes ecological observatory network gleon and the aquatic ecosystem modellers network junior aemon j for bringing this group of researchers together aemon j is a global early career network for aquatic ecosystem modellers that strives to advance the scientific field of numerical modelling in aquatic ecosystems as well as to build a welcoming community for novice and advanced modellers projects like this require a wide range of expertise and this large group made the creation of this r package and the writing of this manuscript possible we also express our gratitude to the research institutes and the individuals that made changes to the models that allowed us to incorporate them into lakeensemblr or provided assistance with bug fixing and compiling most notably igb berlin germany and helmholtz centre for environmental research ufz germany flake the university of western australia australia and virginia tech usa glm eawag switzerland simstrat aarhus university denmark and bolding bruggeman gotm and université laval canada mylake we thank the marine institute ireland and niva norway for collecting and sharing the data used in this study lastly we thank kaelin cawley and other gleon members for their participation in discussions in the early stage of the project reference to trade names does not imply endorsement by the u s government appendix a supplementary data the following is are the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105101 
25783,the increasing pressure on earth s ecosystems due to climate change is becoming more and more evident and the impacts of climate change are especially visible on coral reefs understanding how climate change interacts with the physical environment of reefs to impact coral growth and reef development is critically important to predicting the persistence of reefs into the future in this study a biophysical model was developed including four environmental factors in a feedback loop with the coral s biology 1 light 2 hydrodynamics 3 temperature and 4 ph the submodels are online coupled i e regularly exchanging information and feedbacks while the model runs this ensures computational efficiency despite the widely ranged timescales the composed biophysical model provides a significant step forward in understanding the processes that modulate the evolution of coral reefs as it is the first construction of a model in which the hydrodynamics are included in the feedback loop keywords coral reefs biophysical model timescales hydrodynamics light temperature ph 1 introduction coral reefs provide one of the most compelling examples of the impacts of increasing human pressures on the earth s ecosystems e g hoegh guldberg et al 2007 hughes et al 2019 although local pressures such as overfishing and urban development can cause considerable impacts to adjacent coral reef systems global pressures from climate change and ocean acidification are an overarching and ever increasing concern chan and connolly 2013 bruno et al 2019 for instance mass bleaching events which were extremely rare before the 1980s hoegh guldberg 1999 have increased in frequency in recent decades and have become iconic examples of human induced changes affecting life on the planet hughes et al 2018 coral reef systems are highly valued for their role as biodiversity hotspots wilkinson 2008 but are also critically important to the stability of other associated coastal ecosystems such as seagrass meadows and mangroves reefs are known to protect these systems from hydrodynamic mostly wave energy ferrario et al 2014 while also exchanging food nutrients and organisms with them gillis et al 2014 similarly coral reefs also protect coastal communities against wave driven flooding beck et al 2018 storlazzi et al 2019 and coastal erosion sheppard et al 2005 these ecosystem services are tightly coupled to the long term development of reefs and their ecological state thus understanding the processes controlling changes in reef condition is of critical importance denny and gaylord 2010 helmuth et al 2005 kearney and porter 2009 the ability to predict the developmental capacity of corals depends on the proper modelling of the biophysical and ecological interactions between the reef and its environment house et al 2018 in this paper we present a proof of concept of a novel biophysical model that enables investigation of climate change related external forcing such as sea level rise temperature increases and ocean acidification on coral reef development the model concept is based on the interactions between key processes categorized in three groups 1 the direct physical environment of the coral 2 the physiological state of the corals and 3 the long term morphological development see fig 1 modelling these feedback mechanisms provides a method of predicting the long term development of coral reef systems the effects of coral reef geomorphology on hydrodynamics in particular on wave damping has been extensively studied in recent years e g lowe et al 2008 weitzman et al 2015 zeller et al 2015 the attenuation of the flow due to the coral canopy is an interplay between the characteristics of the canopy and the hydrodynamics when the canopy is open water can flow easily through the reef structure and so the damping is limited this is a relative openness as it is considered with respect to the wave conditions the shorter the wave the larger the open spaces inside the canopy are relatively long waves e g tides and currents are therefore attenuated more with the same canopy design compared to short waves e g wind waves e g lowe et al 2005a compared to the timescale of hydrodynamics coral reef growth is an extremely slow process consequently for the purpose of modelling hydrodynamics around reefs reef geomorphology is considered constant in most applications hearn et al 2001 lowe et al 2005b or not taken into account at all buddemeier et al 2008 evenhuis et al 2015 silverman et al 2007 however for future projections at longer timescales such as sea level rise temperature increase or ocean acidification i e order of decades consideration of coral growth and adjustments of coral morphology are important coral morphology is known to adjust directly to the environmental conditions todd 2008 and depends on how physiological processes photosynthesis calcification vary in relation to the environment light flow temperature ph as a result relationships that have been extensively described in the literature e g anthony and hoegh guldberg 2003 donner 2011 evenhuis et al 2015 jimenez et al 2011 the aim of this study was to gain more insight into the key processes determining the development of a coral reef this led to development of a biophysical model that is aimed to be robust flexible and process based the model describes the response of the coral reef in terms of growth and survival to various environmental fluctuations flow and wave related information on the environment is derived from direct coupling to a hydrodynamic model this coupling is two way as the feedback loop between the coral and its environment is closed by including the morphological development of the coral reef and feeding that information back into the hydrodynamic computations so called online coupling such coupled modelling poses challenges in model efficiency due to the wide range of timescales involved as the model time step is controlled by the smallest time scale therefore methods to bridge the gaps between the timescales are explored in this paper holistic models that incorporate multiple timescales are important in understanding the future of coral reef development as the processes playing at the various timescales are interconnected e g wind waves order of seconds affect the coral reef morphological development order of millennia which affects the wind waves 2 model description the core of the developed biophysical model is the feedback loop between the coral and its environment where the coral s response is split in its physiology and its morphology see fig 1 part of the environment is considered as forcing and is not modulated by the coral reef i e excluded from the feedback loop the box environment in fig 1 includes only the environmental factors that are included in the feedback loop to model this feedback loop requirements are 1 adequate characterisation of the environment 2 description of the physiological responses of the corals to the environment 3 description of the morphological response of the coral to the environmental and physiological variations and 4 feedback of the morphological adaptation to the hydrodynamic framework to translate it into renewed environmental parameters for coral growth and development the following environmental factors that are considered to influence the development of corals are included 1 light 2 water flow 3 temperature and 4 ph pratchett et al 2015 light is included due to the process of light enhanced calcification which describes the enhancement of the calcification rate due to photosynthesis of symbiotic zooxanthellae eyal et al 2019 goreau 1959 the effects of flow are based on the supply and removal of nutrients and waste e g atkinson and bilger 1992 hearn et al 2001 mass et al 2010 the temperature is the primary cause of coral bleaching e g baird et al 2009 jokiel and coles 1977 as well as a general modifier of physiological and biochemical rates lastly the ph is included by means of the aragonite saturation state which can impact coral calcification rate and is the generally accepted approach to take into account ocean acidification in coral dynamics e g gattuso et al 1998 langdon and atkinson 2005 ries et al 2010 the main processes in coral development are threefold 1 growth 2 degradation and 3 recovery in our model degradation of coral reefs can result from two processes bleaching related mortality and storm damage hoegh guldberg 1999 madin and connolly 2006 wilkinson and souter 2008 other damaging processes e g predation lenihan et al 2015 and disease díaz and madin 2011 were excluded as they cannot directly be related to the environmental factors included in this study when the coral is damaged and possibly dies due to coral bleaching its skeleton remains and influences the hydrodynamics in case of storm damage the coral is partially dislodged which also affects the geomorphology of the coral reef these pathways are displayed in fig 2 bleaching events follow the right hand side and storm events the left hand side 2 1 coral environment the morphology of corals affects both the macro and micro environments that surround them e g jimenez et al 2011 lowe et al 2005a monismith 2007 due to this feedback the environmental conditions environing the coral may differ from the ambient water therefore the micro environments related to light flow and temperature are discussed below the micro environment related to ph was left out because it was considered homogeneous over a coral reef due to 1 the small depth over a coral reef and 2 the well mixed water column due to turbulence e g reidenbach et al 2006b 2 1 1 light micro environment light attenuates with depth freitas et al 2019 kratzer et al 2003 where the light that can be used for photosynthesis depends on the coral s morphology due to shading effects hoogenboom et al 2008 muko et al 2000 stambler and dubinsky 2005 therefore a representative light intensity was determined based on the light micro environment this representative light intensity was defined as the biomass averaged light intensity here the biomass was defined as the surface of the coral receiving light green shading in fig 3 allemand et al 2004 jokiel 2011 hoegh guldberg 1988 the definition of the biomass incorporates the spreading of light to determine the extent of the shading see fig 3 where this spreading reduces with increasing depth jokiel 2011 2 1 2 flow micro environment the presence of corals collectively a coral canopy highly influences the hydrodynamics both waves e g lowe et al 2009 monismith et al 2015 and currents e g baptist 2005 nepf and vivoni 2000 nikora et al 2013 only an attenuated flow remains within the coral canopy e g lowe et al 2005a weitzman et al 2015 zeller et al 2015 here termed the flow micro environment this flow micro environment is important for the supply of nutrients and the removal of waste lowe et al 2005b reidenbach et al 2006a for the characterisation of the flow micro environment we used the wave attenuation formulations of lowe et al 2005a and zeller et al 2015 the flow attenuation in the canopy was modelled following zeller et al 2015 and van rooijen et al 2018 wave and current induced flows were assumed to interact linearly bijker 1967 lowe et al 2005a the drag coefficient was determined dynamically etminan et al 2017 van rooijen et al 2018 2 1 3 thermal micro environment the thermal micro environment may result in an increased temperature up to 1 c above the ambient water brodersen et al 2014 fabricius 2006 jimenez et al 2011 this discrepancy arises due to the presence of a thermal boundary layer which is related to the flow micro environment through the turbulent boundary layer jimenez et al 2011 within this boundary layer diffusive transport dominates over advective processes in addition the illuminated surface of the coral acts as a source of heat leading to a local temperature elevation the temperature within the thermal boundary layer is the actual temperature experienced by the coral and is potentially important in determining critical boundaries for the occurrence of thermally induced bleaching see secs 2 2 3 and 2 4 1 for the definition of the thermal micro environment we followed the formulations by jimenez et al 2011 2 2 coral physiology coral physiology describes the rate of growth of individual corals which is described using the calcification rate the calcification rate was defined as the product of environmental dependencies and two calibration parameters based on evenhuis et al 2015 whereas this forms a linear basis non linearities can be included due to the formulations of the environmental dependencies of growth an important factor modulating calcification rate is the photosynthetic dependencies due to the principle of light enhanced calcification eyal et al 2019 goreau 1959 these photosynthetic dependencies include the effects of light temperature and flow where the ph only affects the calcification rate this section describes all four environmental dependencies included 2 2 1 photosynthetic light dependency the photosynthetic light dependency is given by the photosynthesis irradiance curve e g chalker et al 1983 jassby and platt 1976 to reduce the number of input parameters the dark respiration was defined such that the net photosynthesis equals zero at the base of the euphotic depth zone defined as the depth with 1 light penetration the remaining parameters maximum photosynthetic rate and saturation light intensity were dynamically modelled depending on photo acclimatisation formulations on the photo acclimatisation follow anthony and hoegh guldberg 2003 and chalker et al 1983 2 2 2 photosynthetic flow dependency photosynthesis depends on flow through the diffusive boundary layer where transport is diffusion limited atkinson and bilger 1992 hearn et al 2001 mass et al 2010 the presence of a diffusive boundary layer can limit the supply of nutrients as diffusion is much slower than advection e g jimenez et al 2011 within the diffusive boundary layer the transport rate is described by fick s first law where the concentration gradient was assumed linear and consequently the transport rate is inversely related to the thickness of the diffusive boundary layer jimenez et al 2011 mass et al 2010 the thickness of the diffusive boundary layer is inversely related to the flow velocity e g absi 2009 resulting in a linear relationship between the photosynthetic flow dependency and the flow velocity in line with comeau et al 2014 lenihan et al 2015 however photosynthesis is not flow limited for velocities above 0 10 ms 1 hurd 2000 the linear relationship was capped at this point using a tangent hyperbolic function to avoid discontinuities 2 2 3 photosynthetic thermal dependency photosynthetic thermal dependency describes the thermal limitations on the symbiosis between the coral animal and its zooxanthellae together known as the coral holobiont baird et al 2009 two components were distinguished 1 the adapted temperature response and 2 the thermal envelope evenhuis et al 2015 the adapted temperature response describes the thermal range in which the symbiosis functions normally between a lower and a higher temperature a cubic equation with an optimum in between the limits describes the dependence of the symbiosis on temperature thermally specialised coral species have narrow ranges and high optimal rates eurytopic coral species have broader ranges but lower optimum values the thermal envelope expresses the principle of increased biochemical reactions at higher temperature according to the arrhenius equation evenhuis et al 2015 in between the thermal limits all rates are higher as the limits shift to higher temperature ranges the limits of the thermal range are not fixed but slowly adapt to the temperatures the coral holobiont has experienced over time donner 2011 logan et al 2014 palumbi et al 2014 the formulation for this thermal acclimatisation was based on but modified from the principle of degree heating weeks following donner 2011 the upper and lower limits of the thermal range were based on a running mean of the respectively monthly maximum and minimum means over a period of 60 years when possible this period is based on logan et al 2014 2 2 4 ocean ph dependency the oceanic carbon system consists of a complex interplay between the different forms of dissolved inorganic carbon and ph mucci 1983 roy et al 1993 weiss 1974 in the model this complexity has been reduced to a dependency of calcification on the aragonite saturation state as is common practice in coral studies gattuso et al 1998 langdon and atkinson 2005 ries et al 2010 the dependency was modelled as a modification of the monod equation 2 3 coral morphology a schematic of the coral s morphology figs 3 and 4 is included in the model to estimate the rates of several processes light reaching the surface of the coral is an example of such a process other examples are flow and wave attenuation and dislodgement of corals by storms to keep the formulations manageable but still retain the possibility to represent the major types of corals the coral morphology was simplified to a tabular shape i e a two layer cylinder in literature the coral is often represented by simpler cylindrical shapes e g lowe et al 2005a storlazzi et al 2005 zeller et al 2015 here a two layer cylinder is used to better represent storm damage see sec 2 4 2 furthermore a two layer cylinder does not exclude the representation of cylindrical shapes such as massive coral colonies but it increases the possibility of representing coral morphologies see fig 4 for a range of morphologies coral morphology is a trait with high phenotypic plasticity i e the morphology of a particular species can vary based on local environmental conditions todd 2008 the optimal morphology of a coral in a location is based on a multitude of environmental factors chappell 1980 of which only the light and flow conditions were included in this study to determine the optimal morphology qualitative descriptions from literature were used as a basis for quantitative relationships in the model high light intensity results generally in vertically directed growth whereas low light intensity promotes horizontal growth e g chappell 1980 hoogenboom et al 2008 muko et al 2000 high flow velocity enhances the compactness of the coral s morphology whereas low flow velocity results in more fragile structures e g kaandorp 1995 kaandorp and sloot 2001 kruszyński et al 2007 a schematic overview of the optimal morphology due to environmental gradients is presented in fig 4 the morphological development was described by a set of partial differential equations directing the morphological parameters in the direction of optimal morphology for the governing conditions morphological adaptations were constrained by the mass balance as the calcification rate sets the boundaries for the possible rate of change of the morphology 2 4 coral survival the coral survival was related to three key processes 1 coral bleaching 2 coral dislodgement and 3 coral recruitment although these processes occur at different timescales they are grouped in this section due to their overlapping topic 2 4 1 coral bleaching when a coral bleaches it passes multiple states before it dies baird and marshall 2002 lough et al 1999 in this study we included four such states after evenhuis et al 2015 1 healthy 2 pale 3 bleached and 4 recovering see fig 2 this is in line with other studies except the addition of the recovering state jokiel and coles 1977 marshall and baird 2000 in this state the coral seems healthy i e it has recovered its pigmentation but it has not yet resumed growth evenhuis et al 2015 the total population cover at space x and time t was described by a vector composed of the cover of each of the four population states the dynamics of this population were described by a set of partial differential equations expressing the transitions between the different states following evenhuis et al 2015 these dynamics were related to the photosynthetic efficiency as this expresses the most direct response to the environmental conditions as discussed earlier calcification rate was constrained by photosynthetic rate as its main source of energy but in addition also depends on the population states as only healthy corals are capable of growth 2 4 2 coral dislodgement the leading mechanism for dislodgement of coral colonies is a storm event because the substratum is in general substantially weaker than the coral skeleton itself macintyre and marshall 1988 madin 2005 madin et al 2013 this damage can be direct due to the wave load directly impacting on the coral structure madin 2005 madin and connolly 2006 storlazzi et al 2005 it can also be indirect due to tumbling coral fragments knowlton et al 1981 smith and hughes 1999 the direct damage can further be categorized in breakage of the coral skeleton chamberlain 1978 schuhmacher and plewka 1981 and dislodgement of the whole coral colony hongo et al 2012 madin 2005 madin and connolly 2006 we used the dislodgement model of madin and connolly 2006 due to its simplicity but still proven robustness hongo et al 2012 this model defines a dislodgement criterion consisting of two dimensionless parameters 1 dislodgement mechanical threshold dmt and 2 colony shape factor csf the first relates the strength to the load and the latter includes the effect of the morphology this model has a binary nature dislodgement happens or not in combination with the simplifying assumption that only a single morphology occurs per computational cell it results in the undesirable feature that either all corals from a cell disappear or all stay in place to avoid this binary response the model of madin and connolly 2006 was modified to represent a continuous relationship between the load i e dislodgement mechanical threshold and the resistance i e colony shape factor 2 4 3 coral recruitment bleaching and storm events may cause the complete disappearance of living coral cover on a reef section however this does not necessarily mean that this reef section will never be repopulated to include this recovery mechanism the recruitment of corals has to be taken into account coral recruitment was simplified to an annual contribution to the coral cover and volume representing a mass spawning event of corals guest et al 2005 mangubhai and harrison 2008 vize 2006 3 timescales the processes that control coral reef development span a wide range of timescales as presented in fig 5 waves vary over the order of seconds e g holthuijsen 2007 while the morphological development of a reef occurs over the order of decades to millennia e g lough et al 2016 toth et al 2018 updating the morphological development every second would not yield significant differences in the morphology conversely modelling wave conditions over scales of decades and beyond would be computationally prohibitive therefore the processes were split into modules and coupled at different intervals a method commonly used in hydrodynamics and morphodynamics this online coupling principle is illustrated in fig 6 the smallest basic time interval is used by the hydrodynamic model which updates at a timescale of seconds the upper limit of this time step is determined by the stability criteria that govern hydrodynamic models physiological rates of the corals were updated with a time step of days implying that day averaged conditions for light thermal hydrodynamic and acidic conditions were used to drive the physiology morphological adjustments used an annual time step in this way unnecessary long calculations were avoided as not all adjustments have to be performed at the stability limited time step of the hydrodynamic model to further accelerate the model two acceleration methods from the fields of hydrodynamics and morphodynamics were considered 1 input reduction and 2 model reduction li et al 2018 the method of input reduction focuses on reducing the hydrodynamic input parameters by determining a representative set of wave conditions based on the full wave climate benedet et al 2016 walstra et al 2013 in this study this method was implemented to its extreme by defining one set of wave conditions representing normal conditions for one year in addition storm conditions were defined which hit the reef based on their return periods when based on randomness a storm occurred the hydrodynamic model was used to simulate the storm hydrodynamics this may lead to partial damage to the reef which was updated accordingly see sec 2 4 2 the method of model reduction aims at reducing the number of input parameters and model processes li et al 2018 in the balance between a computationally efficient but less complete model versus a complete but expensive model this approach attempts to exclude the formulations that bring the least information compared to their cost here we evaluated costs and benefits for the three computationally most expensive processes 1 calculating the thermal micro environment 2 calculating the flow micro environment and 3 including the flow in the physiology all together but retaining the coupling with the hydrodynamic model for simulating storms 4 model coupling to create the feedback loop between the biological and physical processes the biological model was coupled online to the delft3d flexible mesh hydrodynamic model deltares 2019 in which short waves were simulated using the swan wave model booij et al 1999 delft3d flexible mesh is a process based numerical model that solves the navier stokes equations for the purposes in this study the hydrodynamics were determined based on depth averaged 2d simulations this study has not modified the hydrodynamic model in any way only an online coupling with the biological component has been established the newly developed coral reef model as described in this paper was written in python the model code controlled both the biological and the hydrodynamic modules using the bmi wrapper 1 1 the bmi wrapper is a package in python that enables the control of delft3d flexible mesh from python this package can be downloaded from github https github com openearth bmi python for this online coupling the python code initialised the hydrodynamic model called the hydrodynamic calculations for the duration of one biological step updated the coral physiology and growth stored the contribution to morphological development then started a new coral growth time step after simulation of the coral growth for one year the morphology was updated and the changed morphology was fed into the hydrodynamic model after which a new series of coral growth steps was performed this so called online coupling of the different model components is one of the novel aspects of the developed biophysical model 5 model validation settings 5 1 process validation runs the validation of the biophysical model was performed per process as no data set was available that covered all aspects furthermore bellocchi et al 2009 suggests the use of modular validation runs for complex models as the one developed in this study these runs were based on single or grouped field studies as indicated below in the validation runs the environmental conditions as described in the studies were replicated and the model results were compared with the published data not all validation runs are presented in this paper for readability 5 1 1 calcification rate calcification rate was considered as one of the key processes to validate the biophysical model to as it determines the growth rate of the corals and the coral reef as a whole for this validation field data from multiple studies were used that covered multiple sites per study howells et al 2018 lough and barnes 2000 lough et al 2016 scoffin et al 1992 these long term calcification rates were compared to the model results in all studies the reefs were considered spatially homogeneous no spatial variations of calcification rate within the reef could be studied therefore the validation of the calcification rate is based on reef scale averages which putatively includes many species the field data and the model results were compared by means of the root mean squared error 5 1 2 bleaching the modelling of bleaching processes was validated using bleaching reports and studies describing the response of multiple coral species to thermal stresses in a diversity of studies baird and marshall 2002 bayraktarov et al 2013 berkelmans and van oppen 2006 dias et al 2018 howells et al 2013 jokiel and coles 1977 from this list of validations we highlight one of these studies in which multiple aspects are incorporated namely the study by howells et al 2013 in this study corals were transplanted between the central and the southern regions of the great barrier reef to investigate the importance of thermal acclimatisation with this approach howells et al 2013 not only report on the onset and aftermath of bleaching but also on the determination of the thermal limits even though magnetic island which is part of the study by howells et al 2013 is a marginal reef the study provided excellent validation material because it includes multiple facets of the developed model and so functions as a good illustration for this proof of concept study 5 2 integrated model runs to test and examine the coupled model an archetypical fringing reef was defined on which representative environmental conditions were applied this fringing reef starts at the sea surface and reaches a depth beyond the euphotic depth light input temperature wave impact and currents were loosely based on conditions on the great barrier reef 2 2 light conditions were based on the orbit of the earth and the latitude the hydrodynamic conditions were based on literature madin 2004 2005 massel and done 1993 thermal conditions were based on sea surface temperature from noaa oi sst v2 high resolution dataset provided by the noaa oar esrl psd boulder colorado usa from their web site at https www esrl noaa gov psd and extrapolated to simulate a hundred years with a realistic frequency of storms of different severity the spatial domain of the model used for these integrated model runs was 200 m in alongshore direction and 600 m in cross shore direction and a spatial resolution of 2 5 2 5 m was used this spatial domain and resolution was used for the whole biophysical model i e also for the biological component the spatial domain of the coupled waves module i e swan is much wider to limit the influence of the boundaries in the region of interest due to wave spreading the waves flatten at the boundaries of the model domain the spatial domain of this enlarged spatial domain was 1000 m in alongshore direction and 700 m in cross shore direction and a spatial resolution of 10 10 m was used the validation of storm damage on the coral reef was only qualitative due to lack of quantitative data on the subject most emphasis was placed on the evaluation of the depth gradient in storm damage for the evaluation of the input reduction strategy the hydrodynamics were updated once per year due to the slow growth of corals that impose little changes on the timescale of years this strategy assumes that daily fluctuations in wave conditions have little effect on the coral development the influence of this assumption was evaluated by considering the effects of a difference in wave height up to approximately 20 on both the calcification rate and the bleaching response several runs were performed to test strategies for model reduction in the evaluation of the model reduction the sensitivity of the model output to the exclusion of selected processes related to thermal and flow boundary layers and to any flow interaction with physiology see sec 3 was assessed in combination with the resulting reduction in simulation time 6 results 6 1 process validation runs due to the biophysical model s complexity first a modular validation was performed as suggested by bellocchi et al 2009 which showed good agreement here only the validation of the biophysical modelling of coral cover and calcification rate are presented in figs 7 and 8 respectively these validations are discussed in more detail below in the study of howells et al 2013 introduced in section 5 1 2 corals were transplanted from two different locations on the great barrier reef hundreds of kilometres apart magnetic island is located in central great barrier reef and miall island is located in southern great barrier reef due to their different temperature histories their thermal limits differ and so their responses to the thermal conditions during the study period a bleaching event occurred at magnetic island due to heat stress see fig 7 left column and a bleaching event occurred at miall island due to cold stress see fig 7 right column as the corals from miall island were acclimated to colder temperatures the damage due to the bleaching event at magnetic island was substantially more severe the cold water bleaching event at miall island resulted in almost no bleaching for the corals from miall island which were more adapted to colder temperatures while severe bleaching occurred among the corals originating from magnetic island unfortunately the central great barrier reef was affected by flooding during the study period see fig 7 resulting in significant additional coral mortality howells et al 2013 the data from howells et al 2013 after this flooding event were not taken into account in the assessment of the model validation overall the model simulated the effects of a bleaching event and its aftermath reasonably well however it did not completely follow the field data almost certainly a better match could have been achieved with parameter tuning but that was not the aim of this study in the validation of the calcification rate one parameter remained open to be fitted to the data the calcification constant the four studies considered howells et al 2018 lough and barnes 2000 lough et al 2016 scoffin et al 1992 spanned a wide spectrum of different environmental conditions locations and coral species from these studies a best estimate for the calcification constant was g c 0 5 kg m 2d 1 a substantial fraction of the within study and cross study variation in calcification rates was covered by the model see fig 8 and little systematic bias was present in the model predictions part of the variation in the observed data however remains unexplained although there is a known difference in calcification rates between indo pacific reefs and western atlantic reefs dullo 2005 which may require re tuning of this parameter depending on the location of interest the model performance on storm damage was only evaluated qualitatively due to a lack of data on storm impacts the representative reef used for the test of the integrated model was also used to establish the reaction to storm disturbance the results clearly illustrate the effects of depth on the dislodgement fig 9 beneath a threshold depth of approximately five metres there was no storm damage damage increased with decreasing water depth 6 2 model acceleration the validity of the imposed input reduction was assessed by means of a sensitivity analysis during normal wave conditions the results indicate that a change in approximately 20 of significant wave height did not impose substantial differences in coral development as defined by coral cover fig 10 a and coral volume fig 10b the effects of the model reductions were also assessed by means of a sensitivity analysis in which the coral development of an ideal fringing reef were simulated for hundred years the duration of the simulations is presented in table 1 3 3 simulations were executed on a server with 4 cores 3 5 ghz 16 gb ram where the relative difference is of importance the biophysical model is 40 times faster and so computationally less expensive when the biological part is only coupled with the hydrodynamic model to simulate storm events the outcomes of the configurations were within the accuracy of the input parameters for long term simulations see fig 11 in this case derived from climate projections 7 discussion this proof of concept study shows that the developed biophysical model enables the simulation of long term coral reef development against low computational costs the results of this conceptual study show that the main determinants of long term coral reef development are the occurrence and severity of bleaching and storm events and the capability of the reef to recover from such events furthermore the developed biophysical model is as was aimed for robust flexible and process based 7 1 model design the biophysical model developed in this study was based on previous models representing physiological processes buddemeier et al 2008 evenhuis et al 2015 silverman et al 2007 as well as storm damage madin 2005 madin and connolly 2006 what is different about our new setup is the connection between these different processes and between the biological and physical aspects of coral development which previously were separated because the size and morphology of corals are key in their dislodgement susceptibility madin and connolly 2006 storlazzi et al 2005 the growth rate and shape are important characteristics to incorporate when making long term predictions on the other hand the coral s size and shape are highly influenced by the hydrodynamics and the other environmental factors evenhuis et al 2015 developed a physiological model which is at the basis of the physiological processes in this study that is able to predict the physiology well but they did not take storm damage into account on the other hand madin and connolly 2006 and storlazzi et al 2005 developed models to simulate storm damage when the coral morphology is considered known which makes them unsuitable for long term predictions as the reef morphology develops over time the core of the developed biophysical model is linear where the effects of the environmental factors on the growth rate of the coral are expressed by means of dependencies e g the photosynthetic light dependency is multiplied by the photosynthetic thermal dependency this allows for the addition of other dependencies not yet incorporated such as nutrients this linear structure of the core does not exclude the incorporation of non linear cross dependencies such as the combined effects of light flow and thermal conditions that are encapsulated in the thermal micro environment other non linear cross dependencies can be added in a similar fashion furthermore the biophysical model is an efficient tool to make predictions of coral reef development as different levels of detail can be achieved see table 1 the biophysical model contains two parameters that can be used to accommodate for the physiological differences between coral species 1 the species constant and 2 the calcification constant the species constant reflects the different survival tactics of a coral species where slow growers are commonly less susceptible to bleaching than fast growers evenhuis et al 2015 marshall and baird 2000 this species constant is part of the formulations of the calcification rate and the bleaching response the calcification constant allows for further tuning of the coral calcification rate to available data the results of this study show that the order of magnitude of this calcification constant is g c 0 5 kgm 2d 1 the current model design does not allow for multiple values of the two parameters per simulation thus these parameters are to be tuned to reflect the coral reef as it is populated by one type of coral species the option to simulate different coral species per simulation is the goal of further developments of the biophysical model this addition is most likely to add to the accuracy of the model results and would enable the option to simulate the dynamics of a coral reef ecosystem in addition the morphological development of the coral can be tuned by means of a set of parameters describing the sensitivity of the coral morphological development to the environmental factors here the volumetric expansion of the corals is linearly related to the calcification rate as there is a strong correlation between calcification rate and linear extension e g lough 2008 pratchett et al 2015 suggesting a similar correlation between calcification rate and volumetric expansion 7 2 model efficiency the input reduction has been used to its extreme because this study is a proof of concept of the biophysical model nevertheless the results suggest that this level of reduction is more generally applicable due to the limited differences in the physiological response on the wave conditions see fig 10 the limited effect of excluding the thermal micro environment as part of the model reduction on the outcomes is in line with the fact that coral health and reef development is often related to the sea surface temperature instead of the thermal micro environment buddemeier et al 2008 donner 2011 evenhuis et al 2015 silverman et al 2007 the differences that arise from excluding the flow micro environment are limited because the flow is only used for the photosynthetic flow dependency which is evenly well fitted to the in canopy flow as to the bulk flow because the photosynthetic flow dependency is commonly a secondary research topic atkinson and bilger 1992 mass et al 2010 schutter et al 2011 and there is still large uncertainty with it excluding this process results in flow independence of the physiology which is often modelled as such buddemeier et al 2008 silverman et al 2007 evenhuis et al 2015 the results from the input and model reduction comply with each other as they both indicate the irrelevance of the flow conditions on the physiology of corals under everyday conditions however completely decoupling the biological processes from the hydrodynamics excludes the effects of storms this would hamper the modelling as storm events have a significant influence on coral reefs scheffers and scheffers 2006 wilkinson and souter 2008 furthermore other processes that are not yet included in the biophysical model rely on more detailed hydrodynamic information such as sedimentation luijendijk et al 2019 ranasinghe et al 2011 and larval dispersal bradbury and snelgrove 2001 cowen et al 2006 cowen and sponaugle 2009 these aspects can alter the survival of coral reefs in the face of climate change e g following the deep and turbid reef refugia hypotheses bongaerts et al 2010 cacciapaglia and van woesik 2016 glynn 1996 van woesik et al 2012 the generic coupled nature of the developed model offers many opportunities to improve modelling of coral reef development by deriving specific modules for these factors it is likely that such extensions are more fruitful than the computationally expensive but ecologically rather irrelevant diffusive boundary effects which are at the basis of the thermal and flow micro environments 7 3 essential data this conceptual study was used to determine the key aspects in coral reef dynamics both biological and physical one such key aspect is the accurate prediction of significant events like bleaching and storms both result in substantial changes on the short term but also on the long term as recovery of coral reefs is very slow the developed model is able to give good insights into the damage due to bleaching events with relatively easily accessible data however data on storm damage is limited as well as data on the coral morphology and its development both the morphological aspects are key in predicting potential loss and decay of corals due to storm conditions unfortunately accurate data on the coral morphology and its development is hard to come by largely due to the slow development but also due to its dependency on short term processes such as wave conditions one approach to this issue is focusing on trait based grouping of corals madin et al 2016 here the coral morphology functions as one of the key traits e g zawada et al 2019 this trait based approach might also reduce the uncertainty of other essential parameters used in the model e g the calcification constant madin et al 2016 8 conclusions the biophysical model developed in this study is a first attempt to couple the biology and physics of coral reefs to gain better insight into the complex world of coral reefs the model design consists of a linear basis so new modules can easily be added furthermore it is not site specific and can be used worldwide with the correct tuning of parameters to region specifies e g the calcification constant the knowledge on the photosynthetic flow dependency is too scarce to justify the computational costs of implementation this however does not implicate the decoupling of the biology from the physics due to the relevance of storm events on the development of coral reefs improvements on the biophysical model might include the processes initiated by sedimentation and coral recruitment in a dynamical manner these processes require detailed information on the hydrodynamics and so a more intensive coupling between the biology and the physics would be necessary author contributions g h p h and j d designed the research g h performed the research g h analysed the data g h coded the model and g h p h j d c s and l t wrote the manuscript funding this study was financed by deltares a not for profit research and consultancy institute the hydralab project supported the participation of j d on this effort the u s geological survey s coastal and marine hazards and resources program supported the participation of c s and l t on this effort software availability the software developed in this study is available on github https github com ghendrickx coralmodel the biophysical model is open for collaboration the model is written in python 3 the model works independent from delft3d flexible mesh which is used for the hydrodynamic modelling in this study however the online coupling with delft3d requires certain settings in python any use of trade firm or product names is for descriptive purposes only and does not imply endorsement by the u s government declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the hydralab project is funded by the eu horizon 2020 research and innovation programme and grant agreement number 654110 in addition we would like to thank julie d pietrzak and arjen p luijendijk for there valuable contribution to the research design furthermore we would like to thank selena johnson usgs for her reading and commenting the manuscript appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105103 
25783,the increasing pressure on earth s ecosystems due to climate change is becoming more and more evident and the impacts of climate change are especially visible on coral reefs understanding how climate change interacts with the physical environment of reefs to impact coral growth and reef development is critically important to predicting the persistence of reefs into the future in this study a biophysical model was developed including four environmental factors in a feedback loop with the coral s biology 1 light 2 hydrodynamics 3 temperature and 4 ph the submodels are online coupled i e regularly exchanging information and feedbacks while the model runs this ensures computational efficiency despite the widely ranged timescales the composed biophysical model provides a significant step forward in understanding the processes that modulate the evolution of coral reefs as it is the first construction of a model in which the hydrodynamics are included in the feedback loop keywords coral reefs biophysical model timescales hydrodynamics light temperature ph 1 introduction coral reefs provide one of the most compelling examples of the impacts of increasing human pressures on the earth s ecosystems e g hoegh guldberg et al 2007 hughes et al 2019 although local pressures such as overfishing and urban development can cause considerable impacts to adjacent coral reef systems global pressures from climate change and ocean acidification are an overarching and ever increasing concern chan and connolly 2013 bruno et al 2019 for instance mass bleaching events which were extremely rare before the 1980s hoegh guldberg 1999 have increased in frequency in recent decades and have become iconic examples of human induced changes affecting life on the planet hughes et al 2018 coral reef systems are highly valued for their role as biodiversity hotspots wilkinson 2008 but are also critically important to the stability of other associated coastal ecosystems such as seagrass meadows and mangroves reefs are known to protect these systems from hydrodynamic mostly wave energy ferrario et al 2014 while also exchanging food nutrients and organisms with them gillis et al 2014 similarly coral reefs also protect coastal communities against wave driven flooding beck et al 2018 storlazzi et al 2019 and coastal erosion sheppard et al 2005 these ecosystem services are tightly coupled to the long term development of reefs and their ecological state thus understanding the processes controlling changes in reef condition is of critical importance denny and gaylord 2010 helmuth et al 2005 kearney and porter 2009 the ability to predict the developmental capacity of corals depends on the proper modelling of the biophysical and ecological interactions between the reef and its environment house et al 2018 in this paper we present a proof of concept of a novel biophysical model that enables investigation of climate change related external forcing such as sea level rise temperature increases and ocean acidification on coral reef development the model concept is based on the interactions between key processes categorized in three groups 1 the direct physical environment of the coral 2 the physiological state of the corals and 3 the long term morphological development see fig 1 modelling these feedback mechanisms provides a method of predicting the long term development of coral reef systems the effects of coral reef geomorphology on hydrodynamics in particular on wave damping has been extensively studied in recent years e g lowe et al 2008 weitzman et al 2015 zeller et al 2015 the attenuation of the flow due to the coral canopy is an interplay between the characteristics of the canopy and the hydrodynamics when the canopy is open water can flow easily through the reef structure and so the damping is limited this is a relative openness as it is considered with respect to the wave conditions the shorter the wave the larger the open spaces inside the canopy are relatively long waves e g tides and currents are therefore attenuated more with the same canopy design compared to short waves e g wind waves e g lowe et al 2005a compared to the timescale of hydrodynamics coral reef growth is an extremely slow process consequently for the purpose of modelling hydrodynamics around reefs reef geomorphology is considered constant in most applications hearn et al 2001 lowe et al 2005b or not taken into account at all buddemeier et al 2008 evenhuis et al 2015 silverman et al 2007 however for future projections at longer timescales such as sea level rise temperature increase or ocean acidification i e order of decades consideration of coral growth and adjustments of coral morphology are important coral morphology is known to adjust directly to the environmental conditions todd 2008 and depends on how physiological processes photosynthesis calcification vary in relation to the environment light flow temperature ph as a result relationships that have been extensively described in the literature e g anthony and hoegh guldberg 2003 donner 2011 evenhuis et al 2015 jimenez et al 2011 the aim of this study was to gain more insight into the key processes determining the development of a coral reef this led to development of a biophysical model that is aimed to be robust flexible and process based the model describes the response of the coral reef in terms of growth and survival to various environmental fluctuations flow and wave related information on the environment is derived from direct coupling to a hydrodynamic model this coupling is two way as the feedback loop between the coral and its environment is closed by including the morphological development of the coral reef and feeding that information back into the hydrodynamic computations so called online coupling such coupled modelling poses challenges in model efficiency due to the wide range of timescales involved as the model time step is controlled by the smallest time scale therefore methods to bridge the gaps between the timescales are explored in this paper holistic models that incorporate multiple timescales are important in understanding the future of coral reef development as the processes playing at the various timescales are interconnected e g wind waves order of seconds affect the coral reef morphological development order of millennia which affects the wind waves 2 model description the core of the developed biophysical model is the feedback loop between the coral and its environment where the coral s response is split in its physiology and its morphology see fig 1 part of the environment is considered as forcing and is not modulated by the coral reef i e excluded from the feedback loop the box environment in fig 1 includes only the environmental factors that are included in the feedback loop to model this feedback loop requirements are 1 adequate characterisation of the environment 2 description of the physiological responses of the corals to the environment 3 description of the morphological response of the coral to the environmental and physiological variations and 4 feedback of the morphological adaptation to the hydrodynamic framework to translate it into renewed environmental parameters for coral growth and development the following environmental factors that are considered to influence the development of corals are included 1 light 2 water flow 3 temperature and 4 ph pratchett et al 2015 light is included due to the process of light enhanced calcification which describes the enhancement of the calcification rate due to photosynthesis of symbiotic zooxanthellae eyal et al 2019 goreau 1959 the effects of flow are based on the supply and removal of nutrients and waste e g atkinson and bilger 1992 hearn et al 2001 mass et al 2010 the temperature is the primary cause of coral bleaching e g baird et al 2009 jokiel and coles 1977 as well as a general modifier of physiological and biochemical rates lastly the ph is included by means of the aragonite saturation state which can impact coral calcification rate and is the generally accepted approach to take into account ocean acidification in coral dynamics e g gattuso et al 1998 langdon and atkinson 2005 ries et al 2010 the main processes in coral development are threefold 1 growth 2 degradation and 3 recovery in our model degradation of coral reefs can result from two processes bleaching related mortality and storm damage hoegh guldberg 1999 madin and connolly 2006 wilkinson and souter 2008 other damaging processes e g predation lenihan et al 2015 and disease díaz and madin 2011 were excluded as they cannot directly be related to the environmental factors included in this study when the coral is damaged and possibly dies due to coral bleaching its skeleton remains and influences the hydrodynamics in case of storm damage the coral is partially dislodged which also affects the geomorphology of the coral reef these pathways are displayed in fig 2 bleaching events follow the right hand side and storm events the left hand side 2 1 coral environment the morphology of corals affects both the macro and micro environments that surround them e g jimenez et al 2011 lowe et al 2005a monismith 2007 due to this feedback the environmental conditions environing the coral may differ from the ambient water therefore the micro environments related to light flow and temperature are discussed below the micro environment related to ph was left out because it was considered homogeneous over a coral reef due to 1 the small depth over a coral reef and 2 the well mixed water column due to turbulence e g reidenbach et al 2006b 2 1 1 light micro environment light attenuates with depth freitas et al 2019 kratzer et al 2003 where the light that can be used for photosynthesis depends on the coral s morphology due to shading effects hoogenboom et al 2008 muko et al 2000 stambler and dubinsky 2005 therefore a representative light intensity was determined based on the light micro environment this representative light intensity was defined as the biomass averaged light intensity here the biomass was defined as the surface of the coral receiving light green shading in fig 3 allemand et al 2004 jokiel 2011 hoegh guldberg 1988 the definition of the biomass incorporates the spreading of light to determine the extent of the shading see fig 3 where this spreading reduces with increasing depth jokiel 2011 2 1 2 flow micro environment the presence of corals collectively a coral canopy highly influences the hydrodynamics both waves e g lowe et al 2009 monismith et al 2015 and currents e g baptist 2005 nepf and vivoni 2000 nikora et al 2013 only an attenuated flow remains within the coral canopy e g lowe et al 2005a weitzman et al 2015 zeller et al 2015 here termed the flow micro environment this flow micro environment is important for the supply of nutrients and the removal of waste lowe et al 2005b reidenbach et al 2006a for the characterisation of the flow micro environment we used the wave attenuation formulations of lowe et al 2005a and zeller et al 2015 the flow attenuation in the canopy was modelled following zeller et al 2015 and van rooijen et al 2018 wave and current induced flows were assumed to interact linearly bijker 1967 lowe et al 2005a the drag coefficient was determined dynamically etminan et al 2017 van rooijen et al 2018 2 1 3 thermal micro environment the thermal micro environment may result in an increased temperature up to 1 c above the ambient water brodersen et al 2014 fabricius 2006 jimenez et al 2011 this discrepancy arises due to the presence of a thermal boundary layer which is related to the flow micro environment through the turbulent boundary layer jimenez et al 2011 within this boundary layer diffusive transport dominates over advective processes in addition the illuminated surface of the coral acts as a source of heat leading to a local temperature elevation the temperature within the thermal boundary layer is the actual temperature experienced by the coral and is potentially important in determining critical boundaries for the occurrence of thermally induced bleaching see secs 2 2 3 and 2 4 1 for the definition of the thermal micro environment we followed the formulations by jimenez et al 2011 2 2 coral physiology coral physiology describes the rate of growth of individual corals which is described using the calcification rate the calcification rate was defined as the product of environmental dependencies and two calibration parameters based on evenhuis et al 2015 whereas this forms a linear basis non linearities can be included due to the formulations of the environmental dependencies of growth an important factor modulating calcification rate is the photosynthetic dependencies due to the principle of light enhanced calcification eyal et al 2019 goreau 1959 these photosynthetic dependencies include the effects of light temperature and flow where the ph only affects the calcification rate this section describes all four environmental dependencies included 2 2 1 photosynthetic light dependency the photosynthetic light dependency is given by the photosynthesis irradiance curve e g chalker et al 1983 jassby and platt 1976 to reduce the number of input parameters the dark respiration was defined such that the net photosynthesis equals zero at the base of the euphotic depth zone defined as the depth with 1 light penetration the remaining parameters maximum photosynthetic rate and saturation light intensity were dynamically modelled depending on photo acclimatisation formulations on the photo acclimatisation follow anthony and hoegh guldberg 2003 and chalker et al 1983 2 2 2 photosynthetic flow dependency photosynthesis depends on flow through the diffusive boundary layer where transport is diffusion limited atkinson and bilger 1992 hearn et al 2001 mass et al 2010 the presence of a diffusive boundary layer can limit the supply of nutrients as diffusion is much slower than advection e g jimenez et al 2011 within the diffusive boundary layer the transport rate is described by fick s first law where the concentration gradient was assumed linear and consequently the transport rate is inversely related to the thickness of the diffusive boundary layer jimenez et al 2011 mass et al 2010 the thickness of the diffusive boundary layer is inversely related to the flow velocity e g absi 2009 resulting in a linear relationship between the photosynthetic flow dependency and the flow velocity in line with comeau et al 2014 lenihan et al 2015 however photosynthesis is not flow limited for velocities above 0 10 ms 1 hurd 2000 the linear relationship was capped at this point using a tangent hyperbolic function to avoid discontinuities 2 2 3 photosynthetic thermal dependency photosynthetic thermal dependency describes the thermal limitations on the symbiosis between the coral animal and its zooxanthellae together known as the coral holobiont baird et al 2009 two components were distinguished 1 the adapted temperature response and 2 the thermal envelope evenhuis et al 2015 the adapted temperature response describes the thermal range in which the symbiosis functions normally between a lower and a higher temperature a cubic equation with an optimum in between the limits describes the dependence of the symbiosis on temperature thermally specialised coral species have narrow ranges and high optimal rates eurytopic coral species have broader ranges but lower optimum values the thermal envelope expresses the principle of increased biochemical reactions at higher temperature according to the arrhenius equation evenhuis et al 2015 in between the thermal limits all rates are higher as the limits shift to higher temperature ranges the limits of the thermal range are not fixed but slowly adapt to the temperatures the coral holobiont has experienced over time donner 2011 logan et al 2014 palumbi et al 2014 the formulation for this thermal acclimatisation was based on but modified from the principle of degree heating weeks following donner 2011 the upper and lower limits of the thermal range were based on a running mean of the respectively monthly maximum and minimum means over a period of 60 years when possible this period is based on logan et al 2014 2 2 4 ocean ph dependency the oceanic carbon system consists of a complex interplay between the different forms of dissolved inorganic carbon and ph mucci 1983 roy et al 1993 weiss 1974 in the model this complexity has been reduced to a dependency of calcification on the aragonite saturation state as is common practice in coral studies gattuso et al 1998 langdon and atkinson 2005 ries et al 2010 the dependency was modelled as a modification of the monod equation 2 3 coral morphology a schematic of the coral s morphology figs 3 and 4 is included in the model to estimate the rates of several processes light reaching the surface of the coral is an example of such a process other examples are flow and wave attenuation and dislodgement of corals by storms to keep the formulations manageable but still retain the possibility to represent the major types of corals the coral morphology was simplified to a tabular shape i e a two layer cylinder in literature the coral is often represented by simpler cylindrical shapes e g lowe et al 2005a storlazzi et al 2005 zeller et al 2015 here a two layer cylinder is used to better represent storm damage see sec 2 4 2 furthermore a two layer cylinder does not exclude the representation of cylindrical shapes such as massive coral colonies but it increases the possibility of representing coral morphologies see fig 4 for a range of morphologies coral morphology is a trait with high phenotypic plasticity i e the morphology of a particular species can vary based on local environmental conditions todd 2008 the optimal morphology of a coral in a location is based on a multitude of environmental factors chappell 1980 of which only the light and flow conditions were included in this study to determine the optimal morphology qualitative descriptions from literature were used as a basis for quantitative relationships in the model high light intensity results generally in vertically directed growth whereas low light intensity promotes horizontal growth e g chappell 1980 hoogenboom et al 2008 muko et al 2000 high flow velocity enhances the compactness of the coral s morphology whereas low flow velocity results in more fragile structures e g kaandorp 1995 kaandorp and sloot 2001 kruszyński et al 2007 a schematic overview of the optimal morphology due to environmental gradients is presented in fig 4 the morphological development was described by a set of partial differential equations directing the morphological parameters in the direction of optimal morphology for the governing conditions morphological adaptations were constrained by the mass balance as the calcification rate sets the boundaries for the possible rate of change of the morphology 2 4 coral survival the coral survival was related to three key processes 1 coral bleaching 2 coral dislodgement and 3 coral recruitment although these processes occur at different timescales they are grouped in this section due to their overlapping topic 2 4 1 coral bleaching when a coral bleaches it passes multiple states before it dies baird and marshall 2002 lough et al 1999 in this study we included four such states after evenhuis et al 2015 1 healthy 2 pale 3 bleached and 4 recovering see fig 2 this is in line with other studies except the addition of the recovering state jokiel and coles 1977 marshall and baird 2000 in this state the coral seems healthy i e it has recovered its pigmentation but it has not yet resumed growth evenhuis et al 2015 the total population cover at space x and time t was described by a vector composed of the cover of each of the four population states the dynamics of this population were described by a set of partial differential equations expressing the transitions between the different states following evenhuis et al 2015 these dynamics were related to the photosynthetic efficiency as this expresses the most direct response to the environmental conditions as discussed earlier calcification rate was constrained by photosynthetic rate as its main source of energy but in addition also depends on the population states as only healthy corals are capable of growth 2 4 2 coral dislodgement the leading mechanism for dislodgement of coral colonies is a storm event because the substratum is in general substantially weaker than the coral skeleton itself macintyre and marshall 1988 madin 2005 madin et al 2013 this damage can be direct due to the wave load directly impacting on the coral structure madin 2005 madin and connolly 2006 storlazzi et al 2005 it can also be indirect due to tumbling coral fragments knowlton et al 1981 smith and hughes 1999 the direct damage can further be categorized in breakage of the coral skeleton chamberlain 1978 schuhmacher and plewka 1981 and dislodgement of the whole coral colony hongo et al 2012 madin 2005 madin and connolly 2006 we used the dislodgement model of madin and connolly 2006 due to its simplicity but still proven robustness hongo et al 2012 this model defines a dislodgement criterion consisting of two dimensionless parameters 1 dislodgement mechanical threshold dmt and 2 colony shape factor csf the first relates the strength to the load and the latter includes the effect of the morphology this model has a binary nature dislodgement happens or not in combination with the simplifying assumption that only a single morphology occurs per computational cell it results in the undesirable feature that either all corals from a cell disappear or all stay in place to avoid this binary response the model of madin and connolly 2006 was modified to represent a continuous relationship between the load i e dislodgement mechanical threshold and the resistance i e colony shape factor 2 4 3 coral recruitment bleaching and storm events may cause the complete disappearance of living coral cover on a reef section however this does not necessarily mean that this reef section will never be repopulated to include this recovery mechanism the recruitment of corals has to be taken into account coral recruitment was simplified to an annual contribution to the coral cover and volume representing a mass spawning event of corals guest et al 2005 mangubhai and harrison 2008 vize 2006 3 timescales the processes that control coral reef development span a wide range of timescales as presented in fig 5 waves vary over the order of seconds e g holthuijsen 2007 while the morphological development of a reef occurs over the order of decades to millennia e g lough et al 2016 toth et al 2018 updating the morphological development every second would not yield significant differences in the morphology conversely modelling wave conditions over scales of decades and beyond would be computationally prohibitive therefore the processes were split into modules and coupled at different intervals a method commonly used in hydrodynamics and morphodynamics this online coupling principle is illustrated in fig 6 the smallest basic time interval is used by the hydrodynamic model which updates at a timescale of seconds the upper limit of this time step is determined by the stability criteria that govern hydrodynamic models physiological rates of the corals were updated with a time step of days implying that day averaged conditions for light thermal hydrodynamic and acidic conditions were used to drive the physiology morphological adjustments used an annual time step in this way unnecessary long calculations were avoided as not all adjustments have to be performed at the stability limited time step of the hydrodynamic model to further accelerate the model two acceleration methods from the fields of hydrodynamics and morphodynamics were considered 1 input reduction and 2 model reduction li et al 2018 the method of input reduction focuses on reducing the hydrodynamic input parameters by determining a representative set of wave conditions based on the full wave climate benedet et al 2016 walstra et al 2013 in this study this method was implemented to its extreme by defining one set of wave conditions representing normal conditions for one year in addition storm conditions were defined which hit the reef based on their return periods when based on randomness a storm occurred the hydrodynamic model was used to simulate the storm hydrodynamics this may lead to partial damage to the reef which was updated accordingly see sec 2 4 2 the method of model reduction aims at reducing the number of input parameters and model processes li et al 2018 in the balance between a computationally efficient but less complete model versus a complete but expensive model this approach attempts to exclude the formulations that bring the least information compared to their cost here we evaluated costs and benefits for the three computationally most expensive processes 1 calculating the thermal micro environment 2 calculating the flow micro environment and 3 including the flow in the physiology all together but retaining the coupling with the hydrodynamic model for simulating storms 4 model coupling to create the feedback loop between the biological and physical processes the biological model was coupled online to the delft3d flexible mesh hydrodynamic model deltares 2019 in which short waves were simulated using the swan wave model booij et al 1999 delft3d flexible mesh is a process based numerical model that solves the navier stokes equations for the purposes in this study the hydrodynamics were determined based on depth averaged 2d simulations this study has not modified the hydrodynamic model in any way only an online coupling with the biological component has been established the newly developed coral reef model as described in this paper was written in python the model code controlled both the biological and the hydrodynamic modules using the bmi wrapper 1 1 the bmi wrapper is a package in python that enables the control of delft3d flexible mesh from python this package can be downloaded from github https github com openearth bmi python for this online coupling the python code initialised the hydrodynamic model called the hydrodynamic calculations for the duration of one biological step updated the coral physiology and growth stored the contribution to morphological development then started a new coral growth time step after simulation of the coral growth for one year the morphology was updated and the changed morphology was fed into the hydrodynamic model after which a new series of coral growth steps was performed this so called online coupling of the different model components is one of the novel aspects of the developed biophysical model 5 model validation settings 5 1 process validation runs the validation of the biophysical model was performed per process as no data set was available that covered all aspects furthermore bellocchi et al 2009 suggests the use of modular validation runs for complex models as the one developed in this study these runs were based on single or grouped field studies as indicated below in the validation runs the environmental conditions as described in the studies were replicated and the model results were compared with the published data not all validation runs are presented in this paper for readability 5 1 1 calcification rate calcification rate was considered as one of the key processes to validate the biophysical model to as it determines the growth rate of the corals and the coral reef as a whole for this validation field data from multiple studies were used that covered multiple sites per study howells et al 2018 lough and barnes 2000 lough et al 2016 scoffin et al 1992 these long term calcification rates were compared to the model results in all studies the reefs were considered spatially homogeneous no spatial variations of calcification rate within the reef could be studied therefore the validation of the calcification rate is based on reef scale averages which putatively includes many species the field data and the model results were compared by means of the root mean squared error 5 1 2 bleaching the modelling of bleaching processes was validated using bleaching reports and studies describing the response of multiple coral species to thermal stresses in a diversity of studies baird and marshall 2002 bayraktarov et al 2013 berkelmans and van oppen 2006 dias et al 2018 howells et al 2013 jokiel and coles 1977 from this list of validations we highlight one of these studies in which multiple aspects are incorporated namely the study by howells et al 2013 in this study corals were transplanted between the central and the southern regions of the great barrier reef to investigate the importance of thermal acclimatisation with this approach howells et al 2013 not only report on the onset and aftermath of bleaching but also on the determination of the thermal limits even though magnetic island which is part of the study by howells et al 2013 is a marginal reef the study provided excellent validation material because it includes multiple facets of the developed model and so functions as a good illustration for this proof of concept study 5 2 integrated model runs to test and examine the coupled model an archetypical fringing reef was defined on which representative environmental conditions were applied this fringing reef starts at the sea surface and reaches a depth beyond the euphotic depth light input temperature wave impact and currents were loosely based on conditions on the great barrier reef 2 2 light conditions were based on the orbit of the earth and the latitude the hydrodynamic conditions were based on literature madin 2004 2005 massel and done 1993 thermal conditions were based on sea surface temperature from noaa oi sst v2 high resolution dataset provided by the noaa oar esrl psd boulder colorado usa from their web site at https www esrl noaa gov psd and extrapolated to simulate a hundred years with a realistic frequency of storms of different severity the spatial domain of the model used for these integrated model runs was 200 m in alongshore direction and 600 m in cross shore direction and a spatial resolution of 2 5 2 5 m was used this spatial domain and resolution was used for the whole biophysical model i e also for the biological component the spatial domain of the coupled waves module i e swan is much wider to limit the influence of the boundaries in the region of interest due to wave spreading the waves flatten at the boundaries of the model domain the spatial domain of this enlarged spatial domain was 1000 m in alongshore direction and 700 m in cross shore direction and a spatial resolution of 10 10 m was used the validation of storm damage on the coral reef was only qualitative due to lack of quantitative data on the subject most emphasis was placed on the evaluation of the depth gradient in storm damage for the evaluation of the input reduction strategy the hydrodynamics were updated once per year due to the slow growth of corals that impose little changes on the timescale of years this strategy assumes that daily fluctuations in wave conditions have little effect on the coral development the influence of this assumption was evaluated by considering the effects of a difference in wave height up to approximately 20 on both the calcification rate and the bleaching response several runs were performed to test strategies for model reduction in the evaluation of the model reduction the sensitivity of the model output to the exclusion of selected processes related to thermal and flow boundary layers and to any flow interaction with physiology see sec 3 was assessed in combination with the resulting reduction in simulation time 6 results 6 1 process validation runs due to the biophysical model s complexity first a modular validation was performed as suggested by bellocchi et al 2009 which showed good agreement here only the validation of the biophysical modelling of coral cover and calcification rate are presented in figs 7 and 8 respectively these validations are discussed in more detail below in the study of howells et al 2013 introduced in section 5 1 2 corals were transplanted from two different locations on the great barrier reef hundreds of kilometres apart magnetic island is located in central great barrier reef and miall island is located in southern great barrier reef due to their different temperature histories their thermal limits differ and so their responses to the thermal conditions during the study period a bleaching event occurred at magnetic island due to heat stress see fig 7 left column and a bleaching event occurred at miall island due to cold stress see fig 7 right column as the corals from miall island were acclimated to colder temperatures the damage due to the bleaching event at magnetic island was substantially more severe the cold water bleaching event at miall island resulted in almost no bleaching for the corals from miall island which were more adapted to colder temperatures while severe bleaching occurred among the corals originating from magnetic island unfortunately the central great barrier reef was affected by flooding during the study period see fig 7 resulting in significant additional coral mortality howells et al 2013 the data from howells et al 2013 after this flooding event were not taken into account in the assessment of the model validation overall the model simulated the effects of a bleaching event and its aftermath reasonably well however it did not completely follow the field data almost certainly a better match could have been achieved with parameter tuning but that was not the aim of this study in the validation of the calcification rate one parameter remained open to be fitted to the data the calcification constant the four studies considered howells et al 2018 lough and barnes 2000 lough et al 2016 scoffin et al 1992 spanned a wide spectrum of different environmental conditions locations and coral species from these studies a best estimate for the calcification constant was g c 0 5 kg m 2d 1 a substantial fraction of the within study and cross study variation in calcification rates was covered by the model see fig 8 and little systematic bias was present in the model predictions part of the variation in the observed data however remains unexplained although there is a known difference in calcification rates between indo pacific reefs and western atlantic reefs dullo 2005 which may require re tuning of this parameter depending on the location of interest the model performance on storm damage was only evaluated qualitatively due to a lack of data on storm impacts the representative reef used for the test of the integrated model was also used to establish the reaction to storm disturbance the results clearly illustrate the effects of depth on the dislodgement fig 9 beneath a threshold depth of approximately five metres there was no storm damage damage increased with decreasing water depth 6 2 model acceleration the validity of the imposed input reduction was assessed by means of a sensitivity analysis during normal wave conditions the results indicate that a change in approximately 20 of significant wave height did not impose substantial differences in coral development as defined by coral cover fig 10 a and coral volume fig 10b the effects of the model reductions were also assessed by means of a sensitivity analysis in which the coral development of an ideal fringing reef were simulated for hundred years the duration of the simulations is presented in table 1 3 3 simulations were executed on a server with 4 cores 3 5 ghz 16 gb ram where the relative difference is of importance the biophysical model is 40 times faster and so computationally less expensive when the biological part is only coupled with the hydrodynamic model to simulate storm events the outcomes of the configurations were within the accuracy of the input parameters for long term simulations see fig 11 in this case derived from climate projections 7 discussion this proof of concept study shows that the developed biophysical model enables the simulation of long term coral reef development against low computational costs the results of this conceptual study show that the main determinants of long term coral reef development are the occurrence and severity of bleaching and storm events and the capability of the reef to recover from such events furthermore the developed biophysical model is as was aimed for robust flexible and process based 7 1 model design the biophysical model developed in this study was based on previous models representing physiological processes buddemeier et al 2008 evenhuis et al 2015 silverman et al 2007 as well as storm damage madin 2005 madin and connolly 2006 what is different about our new setup is the connection between these different processes and between the biological and physical aspects of coral development which previously were separated because the size and morphology of corals are key in their dislodgement susceptibility madin and connolly 2006 storlazzi et al 2005 the growth rate and shape are important characteristics to incorporate when making long term predictions on the other hand the coral s size and shape are highly influenced by the hydrodynamics and the other environmental factors evenhuis et al 2015 developed a physiological model which is at the basis of the physiological processes in this study that is able to predict the physiology well but they did not take storm damage into account on the other hand madin and connolly 2006 and storlazzi et al 2005 developed models to simulate storm damage when the coral morphology is considered known which makes them unsuitable for long term predictions as the reef morphology develops over time the core of the developed biophysical model is linear where the effects of the environmental factors on the growth rate of the coral are expressed by means of dependencies e g the photosynthetic light dependency is multiplied by the photosynthetic thermal dependency this allows for the addition of other dependencies not yet incorporated such as nutrients this linear structure of the core does not exclude the incorporation of non linear cross dependencies such as the combined effects of light flow and thermal conditions that are encapsulated in the thermal micro environment other non linear cross dependencies can be added in a similar fashion furthermore the biophysical model is an efficient tool to make predictions of coral reef development as different levels of detail can be achieved see table 1 the biophysical model contains two parameters that can be used to accommodate for the physiological differences between coral species 1 the species constant and 2 the calcification constant the species constant reflects the different survival tactics of a coral species where slow growers are commonly less susceptible to bleaching than fast growers evenhuis et al 2015 marshall and baird 2000 this species constant is part of the formulations of the calcification rate and the bleaching response the calcification constant allows for further tuning of the coral calcification rate to available data the results of this study show that the order of magnitude of this calcification constant is g c 0 5 kgm 2d 1 the current model design does not allow for multiple values of the two parameters per simulation thus these parameters are to be tuned to reflect the coral reef as it is populated by one type of coral species the option to simulate different coral species per simulation is the goal of further developments of the biophysical model this addition is most likely to add to the accuracy of the model results and would enable the option to simulate the dynamics of a coral reef ecosystem in addition the morphological development of the coral can be tuned by means of a set of parameters describing the sensitivity of the coral morphological development to the environmental factors here the volumetric expansion of the corals is linearly related to the calcification rate as there is a strong correlation between calcification rate and linear extension e g lough 2008 pratchett et al 2015 suggesting a similar correlation between calcification rate and volumetric expansion 7 2 model efficiency the input reduction has been used to its extreme because this study is a proof of concept of the biophysical model nevertheless the results suggest that this level of reduction is more generally applicable due to the limited differences in the physiological response on the wave conditions see fig 10 the limited effect of excluding the thermal micro environment as part of the model reduction on the outcomes is in line with the fact that coral health and reef development is often related to the sea surface temperature instead of the thermal micro environment buddemeier et al 2008 donner 2011 evenhuis et al 2015 silverman et al 2007 the differences that arise from excluding the flow micro environment are limited because the flow is only used for the photosynthetic flow dependency which is evenly well fitted to the in canopy flow as to the bulk flow because the photosynthetic flow dependency is commonly a secondary research topic atkinson and bilger 1992 mass et al 2010 schutter et al 2011 and there is still large uncertainty with it excluding this process results in flow independence of the physiology which is often modelled as such buddemeier et al 2008 silverman et al 2007 evenhuis et al 2015 the results from the input and model reduction comply with each other as they both indicate the irrelevance of the flow conditions on the physiology of corals under everyday conditions however completely decoupling the biological processes from the hydrodynamics excludes the effects of storms this would hamper the modelling as storm events have a significant influence on coral reefs scheffers and scheffers 2006 wilkinson and souter 2008 furthermore other processes that are not yet included in the biophysical model rely on more detailed hydrodynamic information such as sedimentation luijendijk et al 2019 ranasinghe et al 2011 and larval dispersal bradbury and snelgrove 2001 cowen et al 2006 cowen and sponaugle 2009 these aspects can alter the survival of coral reefs in the face of climate change e g following the deep and turbid reef refugia hypotheses bongaerts et al 2010 cacciapaglia and van woesik 2016 glynn 1996 van woesik et al 2012 the generic coupled nature of the developed model offers many opportunities to improve modelling of coral reef development by deriving specific modules for these factors it is likely that such extensions are more fruitful than the computationally expensive but ecologically rather irrelevant diffusive boundary effects which are at the basis of the thermal and flow micro environments 7 3 essential data this conceptual study was used to determine the key aspects in coral reef dynamics both biological and physical one such key aspect is the accurate prediction of significant events like bleaching and storms both result in substantial changes on the short term but also on the long term as recovery of coral reefs is very slow the developed model is able to give good insights into the damage due to bleaching events with relatively easily accessible data however data on storm damage is limited as well as data on the coral morphology and its development both the morphological aspects are key in predicting potential loss and decay of corals due to storm conditions unfortunately accurate data on the coral morphology and its development is hard to come by largely due to the slow development but also due to its dependency on short term processes such as wave conditions one approach to this issue is focusing on trait based grouping of corals madin et al 2016 here the coral morphology functions as one of the key traits e g zawada et al 2019 this trait based approach might also reduce the uncertainty of other essential parameters used in the model e g the calcification constant madin et al 2016 8 conclusions the biophysical model developed in this study is a first attempt to couple the biology and physics of coral reefs to gain better insight into the complex world of coral reefs the model design consists of a linear basis so new modules can easily be added furthermore it is not site specific and can be used worldwide with the correct tuning of parameters to region specifies e g the calcification constant the knowledge on the photosynthetic flow dependency is too scarce to justify the computational costs of implementation this however does not implicate the decoupling of the biology from the physics due to the relevance of storm events on the development of coral reefs improvements on the biophysical model might include the processes initiated by sedimentation and coral recruitment in a dynamical manner these processes require detailed information on the hydrodynamics and so a more intensive coupling between the biology and the physics would be necessary author contributions g h p h and j d designed the research g h performed the research g h analysed the data g h coded the model and g h p h j d c s and l t wrote the manuscript funding this study was financed by deltares a not for profit research and consultancy institute the hydralab project supported the participation of j d on this effort the u s geological survey s coastal and marine hazards and resources program supported the participation of c s and l t on this effort software availability the software developed in this study is available on github https github com ghendrickx coralmodel the biophysical model is open for collaboration the model is written in python 3 the model works independent from delft3d flexible mesh which is used for the hydrodynamic modelling in this study however the online coupling with delft3d requires certain settings in python any use of trade firm or product names is for descriptive purposes only and does not imply endorsement by the u s government declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the hydralab project is funded by the eu horizon 2020 research and innovation programme and grant agreement number 654110 in addition we would like to thank julie d pietrzak and arjen p luijendijk for there valuable contribution to the research design furthermore we would like to thank selena johnson usgs for her reading and commenting the manuscript appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105103 
25784,land use regression lur is a widely used method to develop prediction models in environmental sciences however the process of creating and applying lur models is repetitive and time consuming the xlur tool was developed to automate this process while at the same time providing a detailed log of the model building process for reproducibility and providing evaluation metrics to assess model quality the aim of this research is to provide a technical demonstration of the use of xlur in two scenarios we demonstrate the use of the xlur tool to build models for predicting pm10 concentrations in greater manchester and intestinal enterococci along the northwest coast of england the examples show how the tool facilitates a model building using standard published protocols and b assessment of prediction quality as is common with lur approaches prediction quality is reliant on data and the characteristics of the phenomena being modelled keywords land use regression gis wizard hybrid spatial data 1 introduction 1 1 what is land use regression land use regression lur is a statistical method which uses geospatial data to develop prediction models in environmental and health sciences it is predominantly used in air pollution and epidemiological research to predict pollutant concentrations empirically for a set of locations within a given a study area beelen et al 2013 eeftens et al 2012a molter et al 2010a 2010b these locations are frequently specific receptor points where no existing monitoring data are available such as homes or schools however predictions can also be made for regularly spaced points across the area of interest and therefore used to create maps surfaces of the spatial variation of pollutant concentrations this can be particularly helpful in urban settings where spatial gradients in some pollutant species can be especially marked borge et al 2016 subsequently lur has been used in studies associated with a range of other environmental phenomena for instance it has been used to map and model other atmospheric characteristics such as noise aguilera et al 2014 foraster et al 2011 goudreau et al 2014 ragettli et al 2016 xie et al 2011 and air temperature alcoforado and andrade 2006 coseo and larsen 2014 heusinkveld et al 2014 hsu et al 2020 tsin et al 2020 applications related to land characteristics include biodiversity cawsey et al 2002 harding et al 1998 ray et al 2002 surface imperviousness chabaeva et al 2004 and radiation messier et al 2015 sonesten 2001 takata et al 2014 finally there are several examples of the use of lur to understand water microbiology eleria and vogel 2005 kelsey et al 2004 mallin et al 2000 hou et al 2006 nevers and whitman 2008 olyphant and whitman 2004 crowther et al 2003 and water chemistry sliva and williams 2001 irish et al 1998 the underlying principle of lur is that a measured quantity e g pollutant concentration water contamination level at a given location can be explained by the presence intensity and influence of sources and sinks which increase and decrease the measured quantity respectively the classical lur approach uses supervised stepwise forward linear regression analysis and fits the same function throughout the area of interest i e it is a global regression model briggs et al 1997 2000 accordingly the empirical associations between monitored values and their predictors must have spatial stationarity szymanowski and kryza 2012 in other words the associations must be held as being broadly equivalent across the entire area of interest lur models are developed by using measured data from a number of monitoring sites which sample the variable of interest the variable of interest acts as the dependent variable and data on the surrounding environment are extracted as potential predictor variables in a multiple linear regression analysis for example in an air pollution study particulate matter concentrations might be measured at fifty monitoring sites then for each monitoring site potential predictor variables are extracted such as the area of industrial land use around the monitoring site the distance to the nearest road the number of motor vehicles on the nearest road etc the particulate matter concentrations and the potential predictors are entered into a supervised machine learning process which will try to construct a parsimonious multiple linear regression model this model can then be used to predict particulate matter concentrations at any point within the study area in many epidemiological studies particularly those with limited resources for exposure modelling empirical methods like lur are often preferred to dispersion model approaches or other models based on mathematical models of physical processes this is due to the facts that they are relatively cost effective computationally efficient and require comparatively little data and expertise on complex physical and chemical processes beelen et al 2013 however where existing dispersion model output data exist but perhaps not with sufficient spatial granularity they can be used as additional predictors molter et al 2010a forms of lur which combine both standard predictor data with existing dispersion or other model outputs are referred to as hybrid lur models in common with all model outputs lur estimates are subject to considerable uncertainty and their reliability depends on data quality in terms of both the samples the measurements made of the phenomenon of interest and predictors as direct or proxy indicators of what factors influence measurement values at particular locations reliability also depends on how well the phenomenon can be modelled using a global function and how well it is represented by the spatial predictor data available the selection of predictors will also inevitably rely on how well an environmental phenomenon is understood as a spatial process 1 2 the escape study as mentioned above lur has been predominantly used to model urban air pollution and there are many examples of lur models for different pollutant species in the academic literature the standardised method for lur also known as the classical lur approach that most air pollution studies use is based on the method developed for the european study of cohorts for air pollution effects escape beelen et al 2013 eeftens et al 2012a escape was a multi centre pan european study on chronic health effects of air pollution which utilised existing cohort studies and a standardised exposure assessment method the manual providing a detailed description of the development of lur models for escape is freely available http www escapeproject eu manuals escape exposure manualv9 pdf the escape lur method can be broadly divided into four steps 1 the measurement of air pollution at a number of monitoring sites in each escape study area 2 the extraction of land use and other predictor data using geographic information system software 3 statistical analyses to obtain a parsimonious prediction model 4 the application of the prediction model to unmeasured sites within the study area steps 2 3 and 4 are very repetitive and time consuming and therefore are prone to human error 1 3 the xlur tool the xlur tool molter 2020 was developed to automate the process of creating and applying lur models for an ongoing air pollution study in indonesia it provides a wizard style interface to calculate and extract potential predictor variables which simplifies step 2 described above importantly the tool facilitates the generation of predictor variables from input data saving time and also reducing the gis skills required to carry out preparatory spatial analysis this means that the method can be used by a range of environmental specialists who are not necessarily gis specialists or statisticians xlur largely automates steps 3 and 4 thereby reducing the potential for human error and saving time the xlur tool uses the escape methodology however since lur can be used to model a range of environmental processes xlur was designed as a general purpose tool not solely for air pollution or air pollution epidemiological research this means it provides more options for the extraction of predictor variables than a standard air pollution lur tool would but it still uses the robust supervised machine learning process set out in the escape manual furthermore the tool also facilitates the inclusion of additional model outputs as predictor variables thus supporting the development of hybrid lur models which were not part of the original escape methodology de hoogh et al 2016 the primary aim of this manuscript is to provide a technical demonstration of the application of the xlur software tool in a variety of environmental settings xlur will be used in two scenarios firstly it will be used to model urban air pollution secondly it will be used to model bathing water quality bathing water quality was selected as a scenario because it is located in a non urban setting and the pollutant is fundamentally different to air pollution this scenario also illustrates the potential wider applicability of lur beyond examples which have been reported in the literature to date section 2 of this manuscript describes the design and architecture of the xlur software tool section 3 presents the first scenario while section 4 presents the second scenario finally section 5 discusses the computational performance of the xlur tool and its strengths and limitations 2 description of xlur xlur is a toolbox written in python for use in arcgis pro which is a widely used gis software package it is available via github https github com anmolter xlur under a gnu general public license v3 0 a detailed manual for xlur is provided in the github repository https anmolter github io xlur xlur documentation pdf therefore this section will be limited to a general overview the xlur toolbox contains two tools buildlur which creates new lur models and applylur which uses the lur models created in buildlur to predict values at unmeasured locations both tools use a wizard style graphic user interface therefore no knowledge of the python language is required from the user 2 1 buildlur to start creating a new lur model all input data need to be stored in a single file geodatabase as the standard relational database format used in esri gis packages data must be stored as projected data since many of the tool functions rely on distance calculations the network location of this file geodatabase is entered on the first page of the buildlur wizard together with a network location for output files the projected coordinate system to be used in the analysis and a layer demarcating the study area the output files created by buildlur are a new file geodatabase a sqlite database and text files files are copied from the input file geodatabase into the new file geodatabase to prevent input files from being overwritten or changed the sqlite database is used to process intermediate data and to store the lur models the text files include a log of the model development process descriptive statistics of the input data and diagnostics of the final lur models on the second page of the buildlur wizard the user selects the point feature class containing the monitoring locations and their associated measurement values as attributes the data in this file are used as the dependent variable in the lur analysis xlur will do some high level checks of the data for example it will display a warning message if there are missing values in the dependent variable missing values erroneously entered as zeros will be treated as true zeros however xlur does not evaluate the dataset in detail or clean it and the user must ensure the validity of the input data on the third page of the wizard the user is presented with several options to create potential predictor variables for the lur model users can create as many or as few predictors as they like after creating a predictor the wizard returns to this selection page which allows the user to create another predictor or to move on to the statistical analysis the aim of xlur is to be widely applicable in environmental research therefore the logic for the creation of predictor variables in xlur is based on general concepts and existing scientific knowledge about the factors which underpin how the phenomenon of interest spatially varies in the nomenclature used in the xlur tool variables are considered to be either sources acting to increase measured values or sinks acting to reduce measured values of environmental agents this supports applications beyond air pollution studies these general concepts can be organised based on the type of data and the type of analysis as illustrated in fig 1 it can be seen that predictor variables can be created from vector point line and area data and from raster cell based data creating predictors from raster data in xlur is relatively simple as only the value of the raster cell that is spatially coincident with the monitoring site can be extracted in contrast creating predictors from vector data provides more options predictors can be buffer based i e a circular buffer is drawn around each monitoring site and features within the buffer are analysed or distance based i e the distance from a monitoring site to the nearest feature is utilised as vector data can be of polygon line or point data type different methods can be used to aggregate the data within the buffer for example for polygon data the total area within a buffer the area weighted value of the polygon layer or the area times the value of the polygon can be extracted while for point data the total count of points the sum of all point values the mean of all point values or the median of all point values can be extracted for distance based predictors several methods are also available for example the spatial analysis will identify the nearest polygon to the monitoring site and the distance from the monitoring site to the nearest polygon in addition to the distance buildlur can also calculate the inverse distance and inverse distance squared as well as extract a value from the nearest polygon and multiply this value with distance etc this means buildlur provides 32 different options for predictor variables to offer a flexible approach suitable for different environmental fields however it is the responsibility of the user to ensure that choices made are reasonable in terms of current scientific understanding of how and why the environmental phenomenon of interest varies in space this includes specifying whether a variable is expected to act as a source or a sink i e to have a positive or negative association with the measured values for instance in the case of air pollution traffic flows are expected to act as a source and would be given a positive sign whereas areas of urban greenspace would be expected to have a negative association and therefore given a negative sign once the predictor variables have been created the user can move on to the final page of the buildlur wizard the final step is the supervised stepwise forward linear regression analysis to obtain a parsimonious model this step is automated but the variable selection process is fully documented in the log and model diagnostics are provided as part of the output files the variable selection process is described in detail in the escape exposure manual http www escapeproject eu manuals escape exposure manualv9 pdf and a flow chart of the decision making process is shown in fig 2 the final models are stored in the sqlite database for later use with the applylur tool after the models have been built xlur will automatically run a leave one out cross validation loocv of each model in line with the original escape protocol http www escapeproject eu manuals escape exposure manualv9 pdf the results of the loocv are stored in the output folder see supplementary material 2 2 applylur creating a lur model is usually only the first step in environmental research studies the second step is to apply this model to predict values at unmeasured locations this step tends to be more computationally intensive and time consuming than creating a lur model because it usually involves applying the model to a large number of receptor points the applylur tool requires minimal input from the user to apply the model and parallel processing capabilities of arcgis pro can enable faster processing depending on the system set up similar to buildlur on the first page of applylur the user is required to enter the network location of the output file geodatabase created by buildlur and the sqlite database the user can then select a lur model stored in the sqlite database on the next page there are three options for the receptor points the user can select an existing file with receptor points the tool can create a grid of receptor points at regular intervals or the tool can create a user defined number of random points the first option is used when there are specific points of interest that require predictions while the second and third options are often used to visualise the spatial variability of the environmental agent across the study area once an option for the receptor points has been selected the applylur tool can be run the predicted values at the receptor points are stored as a new point feature class in the output file geodatabase 3 scenario 1 traffic related air pollution in the first scenario the xlur tool was used to estimate air pollutant concentrations for the greater manchester area greater manchester is a large urban area in the northwest of england for which air pollution lur models have been developed in previous studies beelen et al 2013 eeftens et al 2012a molter et al 2010a 2010b the pollutant modelled in this scenario was the annual mean concentration of particulate matter with a diameter less than 10 μm pm10 which was measured at 15 automatic monitoring stations in 2017 https cleanairgm com data hub monitoring reports data sources for potential predictor variables were the land use land cover data from the corine land cover dataset https land copernicus eu pan european corine land cover land use data from openstreetmap https www openstreetmap org population density data from the office for national statistics road line data from the ordnance survey mastermap itn layer annual average daily traffic flow data from the department for transport and altitude data from the ordnance survey terrain 5 digital terrain model 5 km cell size xlur will automatically add the geographic coordinates x y as potential predictors of larger scale spatial trends such as north south or east west gradients to demonstrate all features of xlur a hybrid model was developed which used data from the department for environment food and rural affairs defra 1 km background pm10 concentration map https uk air defra gov uk data laqm background home as the mandatory variable table 1 summarises the input data and associated direction of effects which is based on the escape exposure manual in total 151 predictor variables were entered into the tool xlur found a satisfactory model after evaluating 42 intermediate models table 2 the model explains an unusually large amount of variability within the data adjusted r2 0 95 however the variance inflation factors vif do not indicate collinearity within the predictors moran s i index suggests no significant p 0 05 spatial autocorrelation within the residuals the model diagnostics suggest that the model is acceptable and the leave one out cross validation suggests a good performance adjusted r2 0 88 all outputs that are automatically generated by xlur which includes the leave one out cross validation and model diagnostics are provided in the supplementary material it should be noted that the p value of the mandatory variable is greater than 0 1 which suggests that this variable may not have been included in the model if the classical lur model option had been used the hybrid option was used to demonstrate this functionality of xlur however in this particular case the addition of the mandatory variable may not improve the model to investigate this further we re ran xlur using the classical model option the model found using the classical option did not include the background pm10 variable however similar to the model shown in table 2 it did include variables representing the number of buses on the nearest major road the number of heavy vehicles on the nearest major road multiplied with the inverse distance to the nearest major road natural land use within a 1000 m buffer and the number of buses multiplied by road length within a 300 m buffer the classical model explained a similar amount of variability adjusted r2 0 93 as the hybrid model and the results of the leave one out cross validation were also similar adjusted r2 0 89 all outputs from the classical model are provided in the supplementary material to visualise the predicted pm10 surface the lur model shown in table 2 was applied to a grid of regularly spaced points at 200 m intervals across the study area fig 3 the highest concentrations are predicted along the major arterial roads in the study area and around the local town centres in contrast the lowest concentrations are predicted along the more rural and upland eastern side of the study area towards the pennine hills the model developed by xlur had a higher adjusted r2 0 95 than previous lur models developed for the same study area 0 70 molter et al 2010a 0 84 eeftens et al 2012a however these values are not directly comparable because the previous studies used input data from different years 2005 2008 and from different sources e g molter et al used predictions from a dispersion model as the dependent variable molter et al 2010a eeftens et al used data measured by 20 harvard impactors during a dedicated monitoring campaign eeftens et al 2012b the monitors were placed in different locations and the annual average pollutant concentration was based on two week composite samples collected in three seasons a further difference is that neither of the previous studies used the hybrid methodology used by xlur however as discussed above the hybrid model may not have been necessary in this scenario nevertheless the model developed by xlur uses similar predictor variables as the previous models for example the model by molter et al also used buses on major roads and the y coordinate while the model by eeftens et al also used high density residential land use in a 100 m buffer and natural land cover in a 300 m buffer this suggests that the lur methodology is relatively consistent in identifying predictor variables of pm10 pollutant concentrations in greater manchester the model developed in this scenario was based on a relatively small number 15 of monitoring sites the size of automatic urban monitoring networks is usually constrained by the high costs of regulatory monitoring equipment in an ideal scenario a dedicated monitoring campaign with a large number of monitoring sites should be used to develop an lur model especially since this would also provide an opportunity for hold out validation van den bossche et al 2018 however even with low cost sensor technology dense monitoring network are not feasible in many areas around world and small automatic monitoring networks are often the only data source available it is important to note that to develop a reliable model the available sampling locations need to be assessed carefully to ensure that the full range of variability in air pollution across the study area is represented morawska et al 2002 nejadkoorki et al 2011 the descriptive statistics output provided by xlur assists with this process it should also be noted that the use of leave one out cross validation can overestimate the predictive capability of models basagaña et al 2012 van den bossche et al 2018 4 scenario 2 bathing water quality faecal coliform pollution in recreational waters is a known human health risk potentially causing gastrointestinal infections upper respiratory tract infections or infections of the eyes ears nose or skin who 2003 the main sources of faecal coliforms are sewage effluent and surface run off of animal waste the european union eu bathing water directive 2006 7 ec european parliament 2006 which was implemented in england in 2015 provides standards for indicator microorganisms in bathing waters as well as provisions for the monitoring and management of bathing waters stidson et al 2012 in england the environment agency is responsible for monitoring water quality in designated bathing waters during the bathing season may september they may take up to 20 water samples from each bathing water site water samples are analysed for escherichia coli and intestinal enterococci and results are published online https environment data gov uk bwq profiles in the second scenario the xlur tool was used to model intestinal enterococci concentrations along a part of the northwest coast of england stretching from west kirby in the south to silecroft in the north data from the 2015 bathing season was obtained for 22 sites at each site 20 samples were collected from may to september samples were collected at irregular time intervals ranging from 2 to 25 days figure s1 in the supplementary material shows the dates on which samples were collected by monitored site due to the irregular sampling intervals and the skewed distribution within each site the geometric mean of the intestinal enterococci measurements across the entire bathing season was calculated for each site data sources for potential predictor variables were based on previous studies kelsey et al 2004 lipp et al 2001 mallin et al 2000 young and thackston 1999 crowther et al 2003 and included corine land cover data impervious surface area from the copernicus land monitoring survey https land copernicus eu manmade and natural topography from the ordnance survey mastermap topography layer population density dog ownership data from the agricultural census and the location of wastewater treatment works geographic coordinates were automatically added as potential predictors by xlur table 3 summarises the input data and its assumed associated direction of effects in total 107 predictor variables were entered into the tool xlur found a satisfactory model after evaluating 166 intermediate models which is shown in table 4 the model explains a moderate amount of variability within the data adjusted r2 0 43 and the variance inflation factors do not indicate collinearity within the predictors moran s i index suggests no significant p 0 05 spatial autocorrelation within the residuals the model diagnostics suggest that the model is acceptable however the leave one out cross validation suggests a relatively poor performance of the model adjusted r2 0 22 pointing to the need to consider additional variables and data sources compared to the model for air pollution which had five predictor variables in addition to the mandatory variable this model has four predictor variables the lur model shown in table 4 was applied to 500 points randomly located on beaches within the study area fig 4 lur has been used in a small number of studies to model faecal coliform concentrations in coastal or lake beach waters hou et al 2006 mallin et al 2000 nevers and whitman 2008 olyphant and whitman 2004 in estuaries kelsey et al 2004 lipp et al 2001 or in inland rivers eleria and vogel 2005 young and thackston 1999 crowther et al 2003 most of these studies were carried out in the us and not all of them used linear regression but also logistic lipp et al 2001 or time series regression olyphant and whitman 2004 not all previous studies provided r2 statistics for their models but based on the available data model performances varied widely with r2 ranging from 0 32 to 0 95 this shows that although the performance of our model is only moderate it is within the range found in previous research the predictor variables included in our model indicate that domestic dogs are a source of faecal coliform pollution on beaches in the study area similar results were found by kelsey et al 2004 in an estuary in south carolina and young thackston young and thackston 1999 in an urban watershed in tennessee furthermore our model suggests that pig farming is a contributing factor a previous study in wales crowther et al 2003 also found that improved pasture which is grassland intensively used for livestock grazing is a predictor of faecal coliform concentrations in the rheidol ystwith river catchment faecal coliform abundance is influenced by the physical and chemical properties of the water body as well as meteorological factors especially rainfall many previous studies included rainfall in their analysis or conducted separate analyses for high flow and low flow conditions since lur models spatial variability and not temporal variability in the dependent variable results will generally be less robust in the case of environmental processes with a strong temporal signal one solution may be to develop time sensitive models for instance using dependent variables representing specific time periods for example in this analysis we could have used monthly or even weekly averages however due to the irregular sampling frequency figure s1 supplementary material we decided to use the geometric mean of the whole bathing season similar to the approach used by crowther et al 2003 nevertheless successful models using classical lur will still require a sufficient spatial signal in the specific independent predictors used in our case although we expect rainfall to be primarily a temporal determinant of faecal coliform concentrations it is still possible that the model might have been improved by considering direct or proxy measures of the relative spatial variability of rainfall across the study area the moderate amount of variability explained by the lur model could be seen as a potential limitation however it is more likely to be a limitation of the publicly accessible data used to develop the model rather than a limitation of the lur method implemented in the xlur tool for example it is possible that additional data on sewage outfall locations water temperature and turbidity or run off described above would improve the model it is also possible that lur is not the best method to model bathing water quality this is because intestinal enterococci may reach the monitoring sites via a combination of different pathways e g surface run off freshwater streams and each of these pathways may have a complex interaction with the intestinal enterococci due to individual physical and chemical parameters in addition the monitoring sites are located in the intertidal zone which is a difficult environment to model as a single geographical unit since it sits at the intersection of land and water as well as the intersection of fresh and saltwater therefore it is possible that to obtain detailed predictions of bathing water quality further spatial or temporal refinement would be needed it may also mean that a process model or a different type of empirical model might be promising in this specific case for example a recently published agent based model successfully simulated spatio temporal processes of faecal coliform loading and die off as well as transfer mechanisms neill et al 2020a 2020b nevertheless lur may still be a useful scoping tool for a large study area prior to using other modelling approaches on specific target areas furthermore it should be kept in mind that by definition a model is only an approximation of the true value therefore it is unlikely that a model will be able to explain all variability observed especially in environmental settings with complex underlying processes 5 discussion 5 1 computational performance one aim of the xlur tool is to simplify and accelerate the process of developing and applying lur models model run times depend on computer specifications the number of receptor points and model complexity xlur records the time it took to complete each page of the interface using an intel xeon w 2123 3 6 ghz cpu with 32 gb of ram and windows 10 os it took 19 min 55 s to build the model in scenario 1 and it took 88 min to build the model in scenario 2 the time consuming step in scenario 2 was the extraction of impervious surface area within the seven buffers 57 min which was due to the fact that this dataset had a fine spatial resolution 20 m grid cells the time it took to apply the models to varying numbers of receptors is shown in fig 5 it can be seen that applying the model developed in scenario 2 to 10 000 receptors took less than 2 min while applying the model developed in scenario 1 to 32 000 receptors took less than 3 min applying the model developed in scenario 1 to 120 000 receptors took around 30 min which is still faster than applying the model manually i e extracting data for each predictor and processing it 5 2 strengths and limitations we have demonstrated several strengths of the xlur tool xlur can greatly accelerate the development and application of lur models it reduces human error by largely automating lur data extraction processing and statistical analysis stages through its wizard style interface xlur makes lur more accessible to users who are not gis experts or statisticians and thus facilitates wider application of lur and hybrid lur in a range of environmental fields the model building and application process is automatically documented in a log file which makes models easily reproducible currently there are a few other tools available to develop lur models rlur pylur openlur lautenschlager et al 2020 ma et al 2020 morley and gulliver 2018 each of these tools has its own merits and suitability will largely depend on a user s software preferences however all of these other tools are aimed at modelling air pollution and none explicitly supports the development of hybrid models to use one of them for a different application would require a user to make changes to the source code in contrast xlur was designed to be more widely applicable and it approaches the extraction of predictor variables from a more general perspective allowing a wide range of analyses to support the generation of variables although xlur classifies variables as sources and sinks these categories can be used for any variables with hypothesised positive or negative associations with the outcome variable of interest despite its advantages xlur has a number of limitations xlur is a tool for use within a proprietary software package esri arcgis pro v 2 2 which means a software license is required although xlur aids domain experts who have limited gis skills some gis skills are still required for generating preparing and processing geospatial data inputs due to the fact that it is based on the escape methodology xlur uses supervised stepwise forward regression but other methods are available to develop models based on spatial predictor variables including non linear methods e g general additive models and data mining methods e g random forests kerckhoffs et al 2019 a few recent studies have compared different methods to develop models in an air pollution context kerckhoffs et al 2019 van den bossche et al 2018 however they found that unless the relationship between the dependent and predictor variable is very complex stepwise regression showed a similar performance as the advanced non linear and data mining methods kerckhoffs et al 2019 it should be noted that xlur currently only uses leave one out cross validation loocv to evaluate the final model frequently the number of monitoring or sample sites is too small to apply hold out validation methods without affecting the model development process however previous studies have found that loocv can overestimate model performance basagaña et al 2012 van den bossche et al 2018 wang et al 2013 therefore ideally model performance should be evaluated through hold out validation i e splitting the data into separate training and test datasets or through an external dataset van den bossche et al 2018 molter et al 2010a functionality for hold out and external validation is currently not included in xlur but under consideration for a future release it is strongly recommended that xlur users should carry out additional model validations if sufficient data are available finally xlur has the same limitations as lur methods more widely model reliability is dependent on data availability and quality the extent of understanding of an environmental phenomenon as a spatial process and the underlying complexity of the process being modelled despite its data requirements tools like xlur can support more accessible and cost effective modelling of environmental phenomena compared to alternative physical or mathematical approaches beelen et al 2013 6 conclusion this manuscript demonstrates that the xlur tool can largely automate the development and application of classical and hybrid lur models xlur s user friendly wizard style interface which was designed as a tool in a widely used gis software package improves accessibility of this cost effect modelling method for air pollution specialists moreover the xlur tool also increases accessibility of the lur method for specialists working in other areas of environmental science and physical geography the predictive power of the model built to demonstrate the bathing water quality example was limited by data availability however given the increasing availability of large spatial datasets it is possible that in the future research could explore the potential of lur for other quantitative geographically varying phenomena this could be done either alone or alongside other complementary methods for example geographically weighted regression or random forests further exploratory work could include social and economic phenomena however as with all models it is still important to take account of the known evidence of the phenomena of interest data characteristics the evidence based selection of variables directions of effects and model robustness software xlur was developed by anna mölter and was first released in 2020 it is a python toolbox for use in arcgis and requires arcgis pro v 2 2 4 or higher esri redlands ca with a spatial analyst license the minimum hardware requirements for arcgis pro v 2 2 are 2 cores hyperthreaded x64 with sse2 extensions 4 gb ram 32 gb free storage 2 gb dedicated graphics memory xlur is written in python and is available on github https github com anmolter xlur under a gnu general public license v3 0 contributions a m designed the research and performed the analysis a m and s l wrote the manuscript funding this work is supported via the nerc newton dipi urban hybrid models for air pollution exposure udara project pis prof g mcfiggans faculty of science and engineering the university of manchester uk and dr d driejana faculty of civil and environmental engineering institut teknologi bandung indonesia ne p014631 1 it builds on work carried out in the european union s seventh framework programme theme env 2007 1 2 2 2 european cohort on air pollution declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper appendix a supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 multimedia component 3 multimedia component 3 multimedia component 4 multimedia component 4 multimedia component 5 multimedia component 5 multimedia component 6 multimedia component 6 multimedia component 7 multimedia component 7 multimedia component 8 multimedia component 8 multimedia component 9 multimedia component 9 multimedia component 10 multimedia component 10 multimedia component 11 multimedia component 11 multimedia component 12 multimedia component 12 multimedia component 13 multimedia component 13 multimedia component 14 multimedia component 14 multimedia component 15 multimedia component 15 multimedia component 16 multimedia component 16 multimedia component 17 multimedia component 17 multimedia component 18 multimedia component 18 multimedia component 19 multimedia component 19 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105108 
25784,land use regression lur is a widely used method to develop prediction models in environmental sciences however the process of creating and applying lur models is repetitive and time consuming the xlur tool was developed to automate this process while at the same time providing a detailed log of the model building process for reproducibility and providing evaluation metrics to assess model quality the aim of this research is to provide a technical demonstration of the use of xlur in two scenarios we demonstrate the use of the xlur tool to build models for predicting pm10 concentrations in greater manchester and intestinal enterococci along the northwest coast of england the examples show how the tool facilitates a model building using standard published protocols and b assessment of prediction quality as is common with lur approaches prediction quality is reliant on data and the characteristics of the phenomena being modelled keywords land use regression gis wizard hybrid spatial data 1 introduction 1 1 what is land use regression land use regression lur is a statistical method which uses geospatial data to develop prediction models in environmental and health sciences it is predominantly used in air pollution and epidemiological research to predict pollutant concentrations empirically for a set of locations within a given a study area beelen et al 2013 eeftens et al 2012a molter et al 2010a 2010b these locations are frequently specific receptor points where no existing monitoring data are available such as homes or schools however predictions can also be made for regularly spaced points across the area of interest and therefore used to create maps surfaces of the spatial variation of pollutant concentrations this can be particularly helpful in urban settings where spatial gradients in some pollutant species can be especially marked borge et al 2016 subsequently lur has been used in studies associated with a range of other environmental phenomena for instance it has been used to map and model other atmospheric characteristics such as noise aguilera et al 2014 foraster et al 2011 goudreau et al 2014 ragettli et al 2016 xie et al 2011 and air temperature alcoforado and andrade 2006 coseo and larsen 2014 heusinkveld et al 2014 hsu et al 2020 tsin et al 2020 applications related to land characteristics include biodiversity cawsey et al 2002 harding et al 1998 ray et al 2002 surface imperviousness chabaeva et al 2004 and radiation messier et al 2015 sonesten 2001 takata et al 2014 finally there are several examples of the use of lur to understand water microbiology eleria and vogel 2005 kelsey et al 2004 mallin et al 2000 hou et al 2006 nevers and whitman 2008 olyphant and whitman 2004 crowther et al 2003 and water chemistry sliva and williams 2001 irish et al 1998 the underlying principle of lur is that a measured quantity e g pollutant concentration water contamination level at a given location can be explained by the presence intensity and influence of sources and sinks which increase and decrease the measured quantity respectively the classical lur approach uses supervised stepwise forward linear regression analysis and fits the same function throughout the area of interest i e it is a global regression model briggs et al 1997 2000 accordingly the empirical associations between monitored values and their predictors must have spatial stationarity szymanowski and kryza 2012 in other words the associations must be held as being broadly equivalent across the entire area of interest lur models are developed by using measured data from a number of monitoring sites which sample the variable of interest the variable of interest acts as the dependent variable and data on the surrounding environment are extracted as potential predictor variables in a multiple linear regression analysis for example in an air pollution study particulate matter concentrations might be measured at fifty monitoring sites then for each monitoring site potential predictor variables are extracted such as the area of industrial land use around the monitoring site the distance to the nearest road the number of motor vehicles on the nearest road etc the particulate matter concentrations and the potential predictors are entered into a supervised machine learning process which will try to construct a parsimonious multiple linear regression model this model can then be used to predict particulate matter concentrations at any point within the study area in many epidemiological studies particularly those with limited resources for exposure modelling empirical methods like lur are often preferred to dispersion model approaches or other models based on mathematical models of physical processes this is due to the facts that they are relatively cost effective computationally efficient and require comparatively little data and expertise on complex physical and chemical processes beelen et al 2013 however where existing dispersion model output data exist but perhaps not with sufficient spatial granularity they can be used as additional predictors molter et al 2010a forms of lur which combine both standard predictor data with existing dispersion or other model outputs are referred to as hybrid lur models in common with all model outputs lur estimates are subject to considerable uncertainty and their reliability depends on data quality in terms of both the samples the measurements made of the phenomenon of interest and predictors as direct or proxy indicators of what factors influence measurement values at particular locations reliability also depends on how well the phenomenon can be modelled using a global function and how well it is represented by the spatial predictor data available the selection of predictors will also inevitably rely on how well an environmental phenomenon is understood as a spatial process 1 2 the escape study as mentioned above lur has been predominantly used to model urban air pollution and there are many examples of lur models for different pollutant species in the academic literature the standardised method for lur also known as the classical lur approach that most air pollution studies use is based on the method developed for the european study of cohorts for air pollution effects escape beelen et al 2013 eeftens et al 2012a escape was a multi centre pan european study on chronic health effects of air pollution which utilised existing cohort studies and a standardised exposure assessment method the manual providing a detailed description of the development of lur models for escape is freely available http www escapeproject eu manuals escape exposure manualv9 pdf the escape lur method can be broadly divided into four steps 1 the measurement of air pollution at a number of monitoring sites in each escape study area 2 the extraction of land use and other predictor data using geographic information system software 3 statistical analyses to obtain a parsimonious prediction model 4 the application of the prediction model to unmeasured sites within the study area steps 2 3 and 4 are very repetitive and time consuming and therefore are prone to human error 1 3 the xlur tool the xlur tool molter 2020 was developed to automate the process of creating and applying lur models for an ongoing air pollution study in indonesia it provides a wizard style interface to calculate and extract potential predictor variables which simplifies step 2 described above importantly the tool facilitates the generation of predictor variables from input data saving time and also reducing the gis skills required to carry out preparatory spatial analysis this means that the method can be used by a range of environmental specialists who are not necessarily gis specialists or statisticians xlur largely automates steps 3 and 4 thereby reducing the potential for human error and saving time the xlur tool uses the escape methodology however since lur can be used to model a range of environmental processes xlur was designed as a general purpose tool not solely for air pollution or air pollution epidemiological research this means it provides more options for the extraction of predictor variables than a standard air pollution lur tool would but it still uses the robust supervised machine learning process set out in the escape manual furthermore the tool also facilitates the inclusion of additional model outputs as predictor variables thus supporting the development of hybrid lur models which were not part of the original escape methodology de hoogh et al 2016 the primary aim of this manuscript is to provide a technical demonstration of the application of the xlur software tool in a variety of environmental settings xlur will be used in two scenarios firstly it will be used to model urban air pollution secondly it will be used to model bathing water quality bathing water quality was selected as a scenario because it is located in a non urban setting and the pollutant is fundamentally different to air pollution this scenario also illustrates the potential wider applicability of lur beyond examples which have been reported in the literature to date section 2 of this manuscript describes the design and architecture of the xlur software tool section 3 presents the first scenario while section 4 presents the second scenario finally section 5 discusses the computational performance of the xlur tool and its strengths and limitations 2 description of xlur xlur is a toolbox written in python for use in arcgis pro which is a widely used gis software package it is available via github https github com anmolter xlur under a gnu general public license v3 0 a detailed manual for xlur is provided in the github repository https anmolter github io xlur xlur documentation pdf therefore this section will be limited to a general overview the xlur toolbox contains two tools buildlur which creates new lur models and applylur which uses the lur models created in buildlur to predict values at unmeasured locations both tools use a wizard style graphic user interface therefore no knowledge of the python language is required from the user 2 1 buildlur to start creating a new lur model all input data need to be stored in a single file geodatabase as the standard relational database format used in esri gis packages data must be stored as projected data since many of the tool functions rely on distance calculations the network location of this file geodatabase is entered on the first page of the buildlur wizard together with a network location for output files the projected coordinate system to be used in the analysis and a layer demarcating the study area the output files created by buildlur are a new file geodatabase a sqlite database and text files files are copied from the input file geodatabase into the new file geodatabase to prevent input files from being overwritten or changed the sqlite database is used to process intermediate data and to store the lur models the text files include a log of the model development process descriptive statistics of the input data and diagnostics of the final lur models on the second page of the buildlur wizard the user selects the point feature class containing the monitoring locations and their associated measurement values as attributes the data in this file are used as the dependent variable in the lur analysis xlur will do some high level checks of the data for example it will display a warning message if there are missing values in the dependent variable missing values erroneously entered as zeros will be treated as true zeros however xlur does not evaluate the dataset in detail or clean it and the user must ensure the validity of the input data on the third page of the wizard the user is presented with several options to create potential predictor variables for the lur model users can create as many or as few predictors as they like after creating a predictor the wizard returns to this selection page which allows the user to create another predictor or to move on to the statistical analysis the aim of xlur is to be widely applicable in environmental research therefore the logic for the creation of predictor variables in xlur is based on general concepts and existing scientific knowledge about the factors which underpin how the phenomenon of interest spatially varies in the nomenclature used in the xlur tool variables are considered to be either sources acting to increase measured values or sinks acting to reduce measured values of environmental agents this supports applications beyond air pollution studies these general concepts can be organised based on the type of data and the type of analysis as illustrated in fig 1 it can be seen that predictor variables can be created from vector point line and area data and from raster cell based data creating predictors from raster data in xlur is relatively simple as only the value of the raster cell that is spatially coincident with the monitoring site can be extracted in contrast creating predictors from vector data provides more options predictors can be buffer based i e a circular buffer is drawn around each monitoring site and features within the buffer are analysed or distance based i e the distance from a monitoring site to the nearest feature is utilised as vector data can be of polygon line or point data type different methods can be used to aggregate the data within the buffer for example for polygon data the total area within a buffer the area weighted value of the polygon layer or the area times the value of the polygon can be extracted while for point data the total count of points the sum of all point values the mean of all point values or the median of all point values can be extracted for distance based predictors several methods are also available for example the spatial analysis will identify the nearest polygon to the monitoring site and the distance from the monitoring site to the nearest polygon in addition to the distance buildlur can also calculate the inverse distance and inverse distance squared as well as extract a value from the nearest polygon and multiply this value with distance etc this means buildlur provides 32 different options for predictor variables to offer a flexible approach suitable for different environmental fields however it is the responsibility of the user to ensure that choices made are reasonable in terms of current scientific understanding of how and why the environmental phenomenon of interest varies in space this includes specifying whether a variable is expected to act as a source or a sink i e to have a positive or negative association with the measured values for instance in the case of air pollution traffic flows are expected to act as a source and would be given a positive sign whereas areas of urban greenspace would be expected to have a negative association and therefore given a negative sign once the predictor variables have been created the user can move on to the final page of the buildlur wizard the final step is the supervised stepwise forward linear regression analysis to obtain a parsimonious model this step is automated but the variable selection process is fully documented in the log and model diagnostics are provided as part of the output files the variable selection process is described in detail in the escape exposure manual http www escapeproject eu manuals escape exposure manualv9 pdf and a flow chart of the decision making process is shown in fig 2 the final models are stored in the sqlite database for later use with the applylur tool after the models have been built xlur will automatically run a leave one out cross validation loocv of each model in line with the original escape protocol http www escapeproject eu manuals escape exposure manualv9 pdf the results of the loocv are stored in the output folder see supplementary material 2 2 applylur creating a lur model is usually only the first step in environmental research studies the second step is to apply this model to predict values at unmeasured locations this step tends to be more computationally intensive and time consuming than creating a lur model because it usually involves applying the model to a large number of receptor points the applylur tool requires minimal input from the user to apply the model and parallel processing capabilities of arcgis pro can enable faster processing depending on the system set up similar to buildlur on the first page of applylur the user is required to enter the network location of the output file geodatabase created by buildlur and the sqlite database the user can then select a lur model stored in the sqlite database on the next page there are three options for the receptor points the user can select an existing file with receptor points the tool can create a grid of receptor points at regular intervals or the tool can create a user defined number of random points the first option is used when there are specific points of interest that require predictions while the second and third options are often used to visualise the spatial variability of the environmental agent across the study area once an option for the receptor points has been selected the applylur tool can be run the predicted values at the receptor points are stored as a new point feature class in the output file geodatabase 3 scenario 1 traffic related air pollution in the first scenario the xlur tool was used to estimate air pollutant concentrations for the greater manchester area greater manchester is a large urban area in the northwest of england for which air pollution lur models have been developed in previous studies beelen et al 2013 eeftens et al 2012a molter et al 2010a 2010b the pollutant modelled in this scenario was the annual mean concentration of particulate matter with a diameter less than 10 μm pm10 which was measured at 15 automatic monitoring stations in 2017 https cleanairgm com data hub monitoring reports data sources for potential predictor variables were the land use land cover data from the corine land cover dataset https land copernicus eu pan european corine land cover land use data from openstreetmap https www openstreetmap org population density data from the office for national statistics road line data from the ordnance survey mastermap itn layer annual average daily traffic flow data from the department for transport and altitude data from the ordnance survey terrain 5 digital terrain model 5 km cell size xlur will automatically add the geographic coordinates x y as potential predictors of larger scale spatial trends such as north south or east west gradients to demonstrate all features of xlur a hybrid model was developed which used data from the department for environment food and rural affairs defra 1 km background pm10 concentration map https uk air defra gov uk data laqm background home as the mandatory variable table 1 summarises the input data and associated direction of effects which is based on the escape exposure manual in total 151 predictor variables were entered into the tool xlur found a satisfactory model after evaluating 42 intermediate models table 2 the model explains an unusually large amount of variability within the data adjusted r2 0 95 however the variance inflation factors vif do not indicate collinearity within the predictors moran s i index suggests no significant p 0 05 spatial autocorrelation within the residuals the model diagnostics suggest that the model is acceptable and the leave one out cross validation suggests a good performance adjusted r2 0 88 all outputs that are automatically generated by xlur which includes the leave one out cross validation and model diagnostics are provided in the supplementary material it should be noted that the p value of the mandatory variable is greater than 0 1 which suggests that this variable may not have been included in the model if the classical lur model option had been used the hybrid option was used to demonstrate this functionality of xlur however in this particular case the addition of the mandatory variable may not improve the model to investigate this further we re ran xlur using the classical model option the model found using the classical option did not include the background pm10 variable however similar to the model shown in table 2 it did include variables representing the number of buses on the nearest major road the number of heavy vehicles on the nearest major road multiplied with the inverse distance to the nearest major road natural land use within a 1000 m buffer and the number of buses multiplied by road length within a 300 m buffer the classical model explained a similar amount of variability adjusted r2 0 93 as the hybrid model and the results of the leave one out cross validation were also similar adjusted r2 0 89 all outputs from the classical model are provided in the supplementary material to visualise the predicted pm10 surface the lur model shown in table 2 was applied to a grid of regularly spaced points at 200 m intervals across the study area fig 3 the highest concentrations are predicted along the major arterial roads in the study area and around the local town centres in contrast the lowest concentrations are predicted along the more rural and upland eastern side of the study area towards the pennine hills the model developed by xlur had a higher adjusted r2 0 95 than previous lur models developed for the same study area 0 70 molter et al 2010a 0 84 eeftens et al 2012a however these values are not directly comparable because the previous studies used input data from different years 2005 2008 and from different sources e g molter et al used predictions from a dispersion model as the dependent variable molter et al 2010a eeftens et al used data measured by 20 harvard impactors during a dedicated monitoring campaign eeftens et al 2012b the monitors were placed in different locations and the annual average pollutant concentration was based on two week composite samples collected in three seasons a further difference is that neither of the previous studies used the hybrid methodology used by xlur however as discussed above the hybrid model may not have been necessary in this scenario nevertheless the model developed by xlur uses similar predictor variables as the previous models for example the model by molter et al also used buses on major roads and the y coordinate while the model by eeftens et al also used high density residential land use in a 100 m buffer and natural land cover in a 300 m buffer this suggests that the lur methodology is relatively consistent in identifying predictor variables of pm10 pollutant concentrations in greater manchester the model developed in this scenario was based on a relatively small number 15 of monitoring sites the size of automatic urban monitoring networks is usually constrained by the high costs of regulatory monitoring equipment in an ideal scenario a dedicated monitoring campaign with a large number of monitoring sites should be used to develop an lur model especially since this would also provide an opportunity for hold out validation van den bossche et al 2018 however even with low cost sensor technology dense monitoring network are not feasible in many areas around world and small automatic monitoring networks are often the only data source available it is important to note that to develop a reliable model the available sampling locations need to be assessed carefully to ensure that the full range of variability in air pollution across the study area is represented morawska et al 2002 nejadkoorki et al 2011 the descriptive statistics output provided by xlur assists with this process it should also be noted that the use of leave one out cross validation can overestimate the predictive capability of models basagaña et al 2012 van den bossche et al 2018 4 scenario 2 bathing water quality faecal coliform pollution in recreational waters is a known human health risk potentially causing gastrointestinal infections upper respiratory tract infections or infections of the eyes ears nose or skin who 2003 the main sources of faecal coliforms are sewage effluent and surface run off of animal waste the european union eu bathing water directive 2006 7 ec european parliament 2006 which was implemented in england in 2015 provides standards for indicator microorganisms in bathing waters as well as provisions for the monitoring and management of bathing waters stidson et al 2012 in england the environment agency is responsible for monitoring water quality in designated bathing waters during the bathing season may september they may take up to 20 water samples from each bathing water site water samples are analysed for escherichia coli and intestinal enterococci and results are published online https environment data gov uk bwq profiles in the second scenario the xlur tool was used to model intestinal enterococci concentrations along a part of the northwest coast of england stretching from west kirby in the south to silecroft in the north data from the 2015 bathing season was obtained for 22 sites at each site 20 samples were collected from may to september samples were collected at irregular time intervals ranging from 2 to 25 days figure s1 in the supplementary material shows the dates on which samples were collected by monitored site due to the irregular sampling intervals and the skewed distribution within each site the geometric mean of the intestinal enterococci measurements across the entire bathing season was calculated for each site data sources for potential predictor variables were based on previous studies kelsey et al 2004 lipp et al 2001 mallin et al 2000 young and thackston 1999 crowther et al 2003 and included corine land cover data impervious surface area from the copernicus land monitoring survey https land copernicus eu manmade and natural topography from the ordnance survey mastermap topography layer population density dog ownership data from the agricultural census and the location of wastewater treatment works geographic coordinates were automatically added as potential predictors by xlur table 3 summarises the input data and its assumed associated direction of effects in total 107 predictor variables were entered into the tool xlur found a satisfactory model after evaluating 166 intermediate models which is shown in table 4 the model explains a moderate amount of variability within the data adjusted r2 0 43 and the variance inflation factors do not indicate collinearity within the predictors moran s i index suggests no significant p 0 05 spatial autocorrelation within the residuals the model diagnostics suggest that the model is acceptable however the leave one out cross validation suggests a relatively poor performance of the model adjusted r2 0 22 pointing to the need to consider additional variables and data sources compared to the model for air pollution which had five predictor variables in addition to the mandatory variable this model has four predictor variables the lur model shown in table 4 was applied to 500 points randomly located on beaches within the study area fig 4 lur has been used in a small number of studies to model faecal coliform concentrations in coastal or lake beach waters hou et al 2006 mallin et al 2000 nevers and whitman 2008 olyphant and whitman 2004 in estuaries kelsey et al 2004 lipp et al 2001 or in inland rivers eleria and vogel 2005 young and thackston 1999 crowther et al 2003 most of these studies were carried out in the us and not all of them used linear regression but also logistic lipp et al 2001 or time series regression olyphant and whitman 2004 not all previous studies provided r2 statistics for their models but based on the available data model performances varied widely with r2 ranging from 0 32 to 0 95 this shows that although the performance of our model is only moderate it is within the range found in previous research the predictor variables included in our model indicate that domestic dogs are a source of faecal coliform pollution on beaches in the study area similar results were found by kelsey et al 2004 in an estuary in south carolina and young thackston young and thackston 1999 in an urban watershed in tennessee furthermore our model suggests that pig farming is a contributing factor a previous study in wales crowther et al 2003 also found that improved pasture which is grassland intensively used for livestock grazing is a predictor of faecal coliform concentrations in the rheidol ystwith river catchment faecal coliform abundance is influenced by the physical and chemical properties of the water body as well as meteorological factors especially rainfall many previous studies included rainfall in their analysis or conducted separate analyses for high flow and low flow conditions since lur models spatial variability and not temporal variability in the dependent variable results will generally be less robust in the case of environmental processes with a strong temporal signal one solution may be to develop time sensitive models for instance using dependent variables representing specific time periods for example in this analysis we could have used monthly or even weekly averages however due to the irregular sampling frequency figure s1 supplementary material we decided to use the geometric mean of the whole bathing season similar to the approach used by crowther et al 2003 nevertheless successful models using classical lur will still require a sufficient spatial signal in the specific independent predictors used in our case although we expect rainfall to be primarily a temporal determinant of faecal coliform concentrations it is still possible that the model might have been improved by considering direct or proxy measures of the relative spatial variability of rainfall across the study area the moderate amount of variability explained by the lur model could be seen as a potential limitation however it is more likely to be a limitation of the publicly accessible data used to develop the model rather than a limitation of the lur method implemented in the xlur tool for example it is possible that additional data on sewage outfall locations water temperature and turbidity or run off described above would improve the model it is also possible that lur is not the best method to model bathing water quality this is because intestinal enterococci may reach the monitoring sites via a combination of different pathways e g surface run off freshwater streams and each of these pathways may have a complex interaction with the intestinal enterococci due to individual physical and chemical parameters in addition the monitoring sites are located in the intertidal zone which is a difficult environment to model as a single geographical unit since it sits at the intersection of land and water as well as the intersection of fresh and saltwater therefore it is possible that to obtain detailed predictions of bathing water quality further spatial or temporal refinement would be needed it may also mean that a process model or a different type of empirical model might be promising in this specific case for example a recently published agent based model successfully simulated spatio temporal processes of faecal coliform loading and die off as well as transfer mechanisms neill et al 2020a 2020b nevertheless lur may still be a useful scoping tool for a large study area prior to using other modelling approaches on specific target areas furthermore it should be kept in mind that by definition a model is only an approximation of the true value therefore it is unlikely that a model will be able to explain all variability observed especially in environmental settings with complex underlying processes 5 discussion 5 1 computational performance one aim of the xlur tool is to simplify and accelerate the process of developing and applying lur models model run times depend on computer specifications the number of receptor points and model complexity xlur records the time it took to complete each page of the interface using an intel xeon w 2123 3 6 ghz cpu with 32 gb of ram and windows 10 os it took 19 min 55 s to build the model in scenario 1 and it took 88 min to build the model in scenario 2 the time consuming step in scenario 2 was the extraction of impervious surface area within the seven buffers 57 min which was due to the fact that this dataset had a fine spatial resolution 20 m grid cells the time it took to apply the models to varying numbers of receptors is shown in fig 5 it can be seen that applying the model developed in scenario 2 to 10 000 receptors took less than 2 min while applying the model developed in scenario 1 to 32 000 receptors took less than 3 min applying the model developed in scenario 1 to 120 000 receptors took around 30 min which is still faster than applying the model manually i e extracting data for each predictor and processing it 5 2 strengths and limitations we have demonstrated several strengths of the xlur tool xlur can greatly accelerate the development and application of lur models it reduces human error by largely automating lur data extraction processing and statistical analysis stages through its wizard style interface xlur makes lur more accessible to users who are not gis experts or statisticians and thus facilitates wider application of lur and hybrid lur in a range of environmental fields the model building and application process is automatically documented in a log file which makes models easily reproducible currently there are a few other tools available to develop lur models rlur pylur openlur lautenschlager et al 2020 ma et al 2020 morley and gulliver 2018 each of these tools has its own merits and suitability will largely depend on a user s software preferences however all of these other tools are aimed at modelling air pollution and none explicitly supports the development of hybrid models to use one of them for a different application would require a user to make changes to the source code in contrast xlur was designed to be more widely applicable and it approaches the extraction of predictor variables from a more general perspective allowing a wide range of analyses to support the generation of variables although xlur classifies variables as sources and sinks these categories can be used for any variables with hypothesised positive or negative associations with the outcome variable of interest despite its advantages xlur has a number of limitations xlur is a tool for use within a proprietary software package esri arcgis pro v 2 2 which means a software license is required although xlur aids domain experts who have limited gis skills some gis skills are still required for generating preparing and processing geospatial data inputs due to the fact that it is based on the escape methodology xlur uses supervised stepwise forward regression but other methods are available to develop models based on spatial predictor variables including non linear methods e g general additive models and data mining methods e g random forests kerckhoffs et al 2019 a few recent studies have compared different methods to develop models in an air pollution context kerckhoffs et al 2019 van den bossche et al 2018 however they found that unless the relationship between the dependent and predictor variable is very complex stepwise regression showed a similar performance as the advanced non linear and data mining methods kerckhoffs et al 2019 it should be noted that xlur currently only uses leave one out cross validation loocv to evaluate the final model frequently the number of monitoring or sample sites is too small to apply hold out validation methods without affecting the model development process however previous studies have found that loocv can overestimate model performance basagaña et al 2012 van den bossche et al 2018 wang et al 2013 therefore ideally model performance should be evaluated through hold out validation i e splitting the data into separate training and test datasets or through an external dataset van den bossche et al 2018 molter et al 2010a functionality for hold out and external validation is currently not included in xlur but under consideration for a future release it is strongly recommended that xlur users should carry out additional model validations if sufficient data are available finally xlur has the same limitations as lur methods more widely model reliability is dependent on data availability and quality the extent of understanding of an environmental phenomenon as a spatial process and the underlying complexity of the process being modelled despite its data requirements tools like xlur can support more accessible and cost effective modelling of environmental phenomena compared to alternative physical or mathematical approaches beelen et al 2013 6 conclusion this manuscript demonstrates that the xlur tool can largely automate the development and application of classical and hybrid lur models xlur s user friendly wizard style interface which was designed as a tool in a widely used gis software package improves accessibility of this cost effect modelling method for air pollution specialists moreover the xlur tool also increases accessibility of the lur method for specialists working in other areas of environmental science and physical geography the predictive power of the model built to demonstrate the bathing water quality example was limited by data availability however given the increasing availability of large spatial datasets it is possible that in the future research could explore the potential of lur for other quantitative geographically varying phenomena this could be done either alone or alongside other complementary methods for example geographically weighted regression or random forests further exploratory work could include social and economic phenomena however as with all models it is still important to take account of the known evidence of the phenomena of interest data characteristics the evidence based selection of variables directions of effects and model robustness software xlur was developed by anna mölter and was first released in 2020 it is a python toolbox for use in arcgis and requires arcgis pro v 2 2 4 or higher esri redlands ca with a spatial analyst license the minimum hardware requirements for arcgis pro v 2 2 are 2 cores hyperthreaded x64 with sse2 extensions 4 gb ram 32 gb free storage 2 gb dedicated graphics memory xlur is written in python and is available on github https github com anmolter xlur under a gnu general public license v3 0 contributions a m designed the research and performed the analysis a m and s l wrote the manuscript funding this work is supported via the nerc newton dipi urban hybrid models for air pollution exposure udara project pis prof g mcfiggans faculty of science and engineering the university of manchester uk and dr d driejana faculty of civil and environmental engineering institut teknologi bandung indonesia ne p014631 1 it builds on work carried out in the european union s seventh framework programme theme env 2007 1 2 2 2 european cohort on air pollution declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper appendix a supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 multimedia component 3 multimedia component 3 multimedia component 4 multimedia component 4 multimedia component 5 multimedia component 5 multimedia component 6 multimedia component 6 multimedia component 7 multimedia component 7 multimedia component 8 multimedia component 8 multimedia component 9 multimedia component 9 multimedia component 10 multimedia component 10 multimedia component 11 multimedia component 11 multimedia component 12 multimedia component 12 multimedia component 13 multimedia component 13 multimedia component 14 multimedia component 14 multimedia component 15 multimedia component 15 multimedia component 16 multimedia component 16 multimedia component 17 multimedia component 17 multimedia component 18 multimedia component 18 multimedia component 19 multimedia component 19 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105108 
