index,text
25605,sensors measuring environmental phenomena at high frequency commonly report anomalies related to fouling sensor drift and calibration and datalogging and transmission issues suitability of data for analyses and decision making often depends on manual review and adjustment of data machine learning techniques have potential to automate identification and correction of anomalies streamlining the quality control process we explored approaches for automating anomaly detection and correction of aquatic sensor data for implementation in a python package pyhydroqc we applied both classical and deep learning time series regression models that estimate values identify anomalies based on dynamic thresholds and offer correction estimates techniques were developed and performance assessed using data reviewed corrected and labeled by technicians in an aquatic monitoring use case auto regressive integrated moving average arima consistently performed best and aggregating results from multiple models improved detection pyhydroqc includes custom functions and a workflow for anomaly detection and correction keywords aquatic sensors quality control anomaly detection python data management software and data availability name of software pyhydroqc description a python package for automated detection and correction of anomalies in aquatic sensor data developer and contact information amber jones amber jones usu edu year first available 2021 program language python 3 7 hardware required personal computer running microsoft windows apple macos or linux software required pyhydroqc uses the following python packages all of which are available via the python package index pypi numpy 1 19 1 pandas 1 1 0 matplotlib 3 3 0 scipy 1 5 2 pmdarima 1 6 1 tensorflow 2 3 keras 2 4 3 statsmodels 0 11 1 scikit learn 0 23 2 os warnings pickle random software availability the pyhydroqc software is open source and is released under the berkeley software distribution version 3 bsd3 software license it can be installed within a python environment from the python package index pypi using the pip utility source code documentation and examples for the software are freely available in github at https github com ambersjones pyhydroqc and zenodo jones et al 2022 dataset availability a resource containing the input data processing scripts results and code to generate plots in this manuscript is described and stored in hydroshare jones et al 2022 additional documentation a resource containing an example jupyter notebook with instructions is described and stored in hydroshare jones 2022 all functions included in the package are documented here https ambersjones github io pyhydroqc 1 introduction observation of environmental phenomena using in situ sensors is increasingly common as sensors and related peripherals become more affordable and as cyberinfrastructure and expertise to support their operation have grown hart and martinez 2006 pellerin et al 2016 rode et al 2016 sensors are subject to environmental factors that affect measurements and their suitability for subsequent analyses data from environmental sensors include anomalous points and biases that are artifacts of instrument noise or drift power failures transmission errors or unusual ambient conditions horsburgh et al 2015 wagner et al 2006 protocols for ensuring quality of environmental sensor data quality assurance and mechanisms for performing data post processing quality control are challenges and key components of sensor network cyberinfrastructure campbell et al 2013 gries et al 2014 jones et al 2015 as the quantity of sensor data increases there is a commensurate need for practices that ensure resultant data are of high quality for subsequent analyses and exploration campbell et al 2013 gibert et al 2016 in current practice quality control post processing of sensor data is expensive and tedious tools exist to assist practitioners and technicians in reviewing data and performing corrections gries et al 2014 horsburgh et al 2015 sheldon 2008 however quality control remains a time consuming and manual process consisting of an interactive sequence of steps performing corrections generally requires expert knowledge about the sensor and the phenomena being observed as well as conditions at the monitoring location fiebrich et al 2010 white et al 2010 furthermore the quality control process involves subjectivity as individual technicians may make different correction decisions jones et al 2018 as a result it is difficult to transfer the institutional knowledge required to post process data and even for trained and experienced technicians quality control remains a daunting task as datasets grow in size and complexity for environmental observatories with ongoing data collection for one network a substantial delay of approximately six months between data collection and availability of reviewed and processed datasets allowed for thorough review and correction jones et al 2017 for cases where observations are used for real time decisions related to public health and water treatment the impacts of anomalous data are costly as sensor datasets continue to grow it is not tenable for scientists and technicians to manually perform quality control tasks gibert et al 2018 neither is it advisable to use or publish data without performing corrections to mitigate for errors as a result there is a recognized need for automating and improving quality control post processing for high frequency in situ sensor data in this vein automated data driven techniques to detect anomalies in streaming sensor data are documented in the realm of research hill and minsker 2010 leigh et al 2018 russo et al 2020 talagala et al 2019 however they are unfamiliar to practitioners generally lack robust and accessible software implementations and are not typically reproducible furthermore while basic checks and more complex algorithms may identify and flag potentially erroneous values e g dereszynski and dietterich 2007 hill et al 2009 taylor and loescher 2013 these procedures are generally not capable of applying corrective actions thus the specific questions we pursued with this research are 1 how can data driven methods be applied to automatically detect and correct anomalies in aquatic sensor data and 2 how can these methods be packaged into an overall workflow and reusable software for general application regression models are one class of data driven techniques that can be used as anomaly detectors for time series data by making a prediction based on previous data either univariate or multivariate and comparing the residual of the modeled and observed values to a threshold because regression models produce an estimate they are well suited for detection and correction of anomalous data although it is a substantial step in quality control post processing automated anomaly correction has not been widely examined a handful of studies replaced raw data with modeled forecasts to exclude anomalies from model input but did not generate a corrected version of the dataset hill and minsker 2010 leigh et al 2018 in this work we implemented and compared several regression models for anomaly detection and explored new approaches for anomaly correction although effectively implemented for specific case studies none of the techniques described in the cited studies have been packaged as accessible software for broad application and dissemination without reusable code the specifics of the algorithms as implemented with environmental data cannot be examined further tested or applied to other datasets rather than a model calibrated to a specific variable site combination practitioners need tools that can be applied to a broad suite of variables and or monitoring locations documented in a reusable and reproducible way thus we sought to package the tools we developed as open source software that could easily be deployed in a commonly available analytical environment in this paper we present a python package pyhydroqc that implements a set of methods for data driven anomaly detection and correction for aquatic sensor data observed with high frequency in time our approach includes machine learning algorithms for detection labeling and correction of anomalous points multiple years of aquatic monitoring data from the logan river observatory lro that have been reviewed and corrected by trained technicians were used as a case study for developing and testing automated detection and correction methods the algorithms are encapsulated in a python package that is publicly available and open source see software and data availability section example scripts are also shared as jupyter notebooks that can be run with case study data to demonstrate the functionality and performance of the tools we developed as there are many potential approaches to anomaly detection additional techniques can be incorporated by adding new functions to the package that can be intrgrated into the workflow thus the specific contributions of this work include 1 advancing the algorithms and methods for automated quality control of aquatic sensor data and 2 developing and demonstrating software tools that can make the process more approachable for data technicians and scientists we anticipate that this work will be of interest to researchers practitioners and technicians that maintain environmental monitoring networks the python package can be used in any python environment and potential users should be familiar with scripting in python or a similar language but do not need specific training or expertise section 2 outlines the methods we implemented for detecting anomalies and performing corrections in the context of the structure and design of the pyhydroqc python package including a description of the case study that drove the implementation in section 3 we report the performance of the techniques on case study data and offer recommendations for next steps followed by conclusions in section 4 appendix a contains related background including an overview of relevant literature and additional motivation for the work reported 2 methods 2 1 pyhydroqc software design and implementation this work implements methods for anomaly detection and correction for environmental time series data within a python based software package a subset of data driven regression models are situated within an overall workflow that includes practical steps to facilitate anomaly detection and correction the following sections describe the approaches for anomaly detection and correction including details of how the software supports the workflow while many classes of algorithms could be used for detecting anomalies in aquatic sensor data we selected time series regression models that were relatively straightforward to implement and that we anticipate will meet the needs and considerations of many applications specifically we investigated auto regressive integrated moving average arima several types of long short term memory lstm and facebook prophet arima has been successfully implemented to detect anomalies in environmental data hill and minsker 2010 leigh et al 2018 papacharalampous et al 2019 lstm is a class of artificial neural networks anns and though applications to environmental data anomalies are limited studies from other fields have detected anomalies with lstm models hundman et al 2018 lindemann et al 2019 malhotra et al 2016 yin et al 2020 prophet was investigated but not included in the python package because prophet is geared toward social media and business applications taylor and letham 2018 we found that its applicability to environmental data is insufficient it failed to capture seasonal shifts in the timing of daily cycles and model features did not represent environmental phenomena this paper focuses on a subset of models but the modular design of the python package allows for the implementation of additional techniques the software design and development were driven by the following steps as a workflow for anomaly detection and correction fig 1 and each is described in more detail in the sections that follow 1 import raw sensor data into a memory resident data structure 2 perform rules based anomaly detection and correction as a first pass at quality control including addressing sensor calibration 3 build one or more models for predicting observed values a determine model hyperparameters b transform and scale data if necessary c build and fit models d execute the model to determine model predictions and residuals 4 post process model results a determine dynamic thresholds based on model residuals and user defined parameters b detect anomalies where the absolute value of the model residual exceeds the defined threshold c widen and index anomalous events 5 compare technician labeled and detected anomalous events rules based and model based detections inclusive to assign confusion matrix categories and report metrics this step is only applicable if labeled data are available 6 combine detections identified by multiple models for an aggregate anomaly detection if rules based detection has been performed those detections are included 7 perform model based correction for points identified as anomalous in addition to performing the workflow steps requirements that drove our design included 1 open source software development to facilitate deployment and use by others 2 cross platform compatibility for use on windows macos and linux platforms 3 modular and extensible architecture that enables each workflow step to be executed independently along with integration of new additional functionality and 4 simple deployment a python package was selected as the platform for software implementation the python language meets the open source and cross platform requirements and existing tools and libraries in python support steps in the workflow including loading and manipulating large datasets and developing data driven models in a python package functions that comprise each step in the workflow can be called by scripts in a modular manner each of the steps can be performed independently facilitating flexibility in use a python package also supports extensibility as new functions can be added without impacting existing functionality finally python packages can be published to the python package index pypi https pypi org making deployment straightforward and ensuring that algorithms can be applied in any python coding environment the anomaly detection and correction workflow steps are encapsulated by functions in the pyhydroqc python package described in the following sections high level workflow wrapper functions arima detect lstm univar detect and lstm multivar detect call more granular functions specific to each data and model type to perform steps 2 7 fig 1 and generate objects of the modelworkflow class for clarity each function is named and described in this paper however most users will use the overarching workflow function calls example python scripts and jupyter notebooks see software availability section illustrate how the workflow functions are implemented for the data use case described in this paper a full list of functions with inputs and outputs is found in appendix b and with the package documentation 2 1 1 data format and import pyhydroqc operates on pandas data frames which are high performance two dimensional tabular data structures for representing data in memory pandas development team 2008 data frames can be created and saved or output as comma separated values csv files for pyhydroqc to perform anomaly detection and correction input data need to be formatted as a data frame for each variable of interest indexed by date time with a column of raw data if technician labels or corrections are available for determining anomaly detection metrics they are included as additional columns in the data frame in general most date time formats reported by sensor systems will be interpreted by pandas as date time objects in the rare case that the date time format is not supported some pre processing may be required pyhydroqc also supports environmental sensor data formatted as one table or file with a single date time column and multiple columns of measurements one for each sensor output for flat files with this structure the pyhydroqc get data function wraps the read csv function from the pandas library to import data into python and parse into separate pandas data frames for each variable as required by the anomaly detection and correction functions 2 1 2 rules based detection and correction rules based detection is an important precursor to detection using models leigh et al 2018 taylor and loescher 2013 and the results of this step contribute to the overall set of detected anomalies whether a result of sensor failure or another cause some anomalies are low hanging fruit that can be detected by rules based preprocessing that performs a first pass of the data preprocessing the data is motivated in part by the need to train models on a dataset absent of extreme outliers or artifacts that models cannot capture by applying rules based anomaly detection and correction a first degree of correction is made for subsequent input into data driven models we created python functions to detect and correct out of range and persistent data furthermore some aquatic sensors commonly exhibit drift which requires sensor calibration and subsequent data correction because calibration shift and the preceding drift are subtle and difficult for any type of model to detect we developed a rules based routine that attempts to identify these events basic correction methods for these anomaly types were also implemented as python functions 2 1 2 1 range and persistence checks the function range check adds a column to the data frame and populates it with an anomalous label if the observation is outside of user defined thresholds or a valid label if it is within the thresholds ranges should be determined specific to each sensor based on physics and the environment in which the sensor is deployed and can be refined based on site specific patterns data persistence refers to the sensor reporting a repeated value which is unlikely in natural systems although sensors may report repeated values due to limitations in resolution for the persistence function the user defines a minimum duration of repeated values for data to be considered anomalous if repeated values exceed that duration the points are classified as anomalous by populating the column from the range check function beyond these basic checks additional rules of increasing complexity could be added to the pyhydroqc package and the anomaly detection workflow examples include ranges that vary seasonally rate of change checks and differencing checks once anomalous points are identified by the python functions that implement these rules labels are carried through to the model based detection steps labeled points are omitted from model training either by logical exclusion or for models requiring an unbroken time series for training by interpolating between valid points linear interpolation is performed using the interpolate function over the entire time series as a preliminary correction step so that model input is more valid if the complete workflow is followed values initially corrected using linear interpolation are replaced by the model based correction described in section 2 1 9 2 1 2 2 calibration and drift correction environmental sensors commonly drift and many aquatic sensors specific conductance ph dissolved oxygen require regular calibration to known standards drift causes a gradual increasing or decreasing trend separate from daily and seasonal patterns and a calibration event manifests as a localized shift that corrects subsequent data up or down these trends and shifts can be subtle and difficult to identify without a detailed record of calibration dates in preliminary work the model based detectors described in subsequent sections were unable to consistently identify these data patterns detected shifts due to calibration events were undiscernible from other localized anomalies thus it is important to address calibration events early in the quality control process because it is preferable that model based detectors be trained on data that are free from drift for calibration and drift correction we implemented functions to mimic a typical manual workflow performing post processing correction for drift and calibration involves review of data comparison of field records to data shifts to identify points corresponding to calibrations and application of a drift correction that uses start and end points and the gap of the calibration shift to retroactively correct data between two calibrations in our experience calibration events are typically reviewed and corrected one at a time while recognizing the difficulty of definitively identifying calibration events in an automated way we designed functions for detection functions calib edge detect calib detect calib overlap and correction functions find gap lin drift cor of data affected by drift and calibration the algorithms take advantage of characteristics of calibration events specifically that events only occur during certain hours of the day they may involve a shift in observed data and that when returned to the water sensors may report the same values for several time steps until the sensor stabilizes two separate approaches identify calibration events 1 where there is a discernible shift in the data or 2 persistence occurs over a limited window of points both are restricted to hours and days when technicians would be in the field given dates of calibration a gap value needs to be specified for correcting past data a function find gap identifies the greatest shift for a given window of time to determine a gap value and the precise point that should be shifted while accounting for outlier spikes commonly associated with calibrations a function for linear drift correction lin drift cor corrects for drift and calibration events given start and end dates and a gap value of the calibration shift while the calibration event detectors may not adequately identify events requiring technician review or input this process is a step toward automation as it evaluates gap values according to a set of rules rather than arbitrary determination by technicians as illustrated in jones et al 2018 and allows for bulk correction of calibration events 2 1 3 model based detection using arima arima is a time series forecasting model where inputs correspond to past time steps of the variable of interest and the output is a predicted value for that variable at the next time step arima uses three parameters to define a linear model equation 1 1 y t i 1 p φ i y t i ε t i 1 q θ i ε t i where y t is the model output or the prediction for time step t p is the number of previous points in the series to be used in the model q is the number of moving average terms to include φ i are the fitted coefficients for auto regression θ i are fitted model coefficients for the moving average and ε t is the moving average error term not shown in the equation is the term d which is the order of differencing applied to the data y before this equation is evaluated the parameters p d q can be determined manually or automatically manual parameter determination involves time series decomposition and the review of auto correlation plots which is tedious for numerous data series automatic determination of the parameters is effective but can be computationally demanding pyhydroqc includes a function pdq for automated determination using the pmdarima package smith 2017 given p d q model training involves determining the values of the coefficients for the terms in the linear equation φ i and θ i based on actual data in pyhydroqc the function build arima model constructs and trains an arima model given input time series data and input parameters p d q it relies on the sarimax function from the statsmodel package seabold and perktold 2010 to fit an arima model based on equation 1 make model predictions for each time step and compare predictions to observations input data should be free from gaps so the anomaly detection workflow uses output of the rules based detection with linear interpolation of any identified anomalies as input for arima modeling 2 1 4 model based detection using lstm lstm is a type of neural network model architecture specifically designed for time dependent and sequenced data lstm models consist of recurrent cells or units each corresponding to one time step a cell uses gates to control the flow of information in and out of the cell and how much of the past data that the cell remembers for computing output to train an lstm model the weights of the connections within and between the gates are iteratively refined based on training data there are many variations of lstm architecture greff et al 2017 for our implementation we compared several lstm model types that are appropriate to time series data modeling for anomaly detection vanilla and bidirectional univariate and multivariate in contrast with other neural network architectures for which many layers are advised for fitting data more shallow lstm have been used because of the internal complexity of lstm cells géron 2017 greff et al 2017 hundman et al 2018 other model types could be constructed model layers and complexity could be added and the input parameters could be tuned to each time series parameters can be defined by users and can be adjusted to investigate sensitivity and we describe our approach for parameter selection in section 3 1 4 the objective of this work was not to achieve the best time series model but rather to detect anomalies so fine tuning models was not required or pursued instead comparisons were made between a few basic lstm variations with the same parameter settings as mentioned pyhydroqc workflow functions call multiple lower level functions for lstm models each type is implemented within the workflow function by an associated model wrapper function lstm univar lstm multivar lstm univar bidir lstm multivar bidir which calls functions specific to that model type for preprocessing model building model training and model evaluation shown in fig 1 and described in the jupyter notebook example script the model wrappers return objects of the class lstmmodelcontainer containing model predictions and residuals for each time step similar to the output of build arima model the model wrapper functions also include an option for saving lstm models for future use which is important because lstm model training and development is stochastic resulting in a new model each time the algorithm is run we developed models for a particular sensor deployed to a certain location so the models are variable and location specific and can be reused for that data series after training 2 1 4 1 vanilla and bidirectional lstm pyhydroqc implements the vanilla type of lstm model greff et al 2017 which consists of a single layer lstm in a sequence to one manner i e the model returns a single output based on a sequence of inputs given a user specified number of past time steps the model output is a single value for the next point in time bidirectional lstm models use observations both before and after the point of interest to provide information for model prediction by encoding a vanilla lstm model with a bidirectional wrapper input data are traversed both forward and backward in sequence and model output is the value to have occurred in the middle of the sequence in pyhydroqc parallel functions structure input data to contain a user specified number of time steps prior to the point of interest for vanilla lstm and prior to and following the point of interest for bidirectional lstm functions further described in section 2 1 5 3 2 1 4 2 univariate and multivariate lstm either univariate or multivariate input data may be used for vanilla and bidirectional lstm through the lstm workflow functions and model wrapper functions the workflow functions lstm detect univar and lstm detect multivar prepare data and report results for univariate or multivariate data and call the associated model wrapper functions lstm univar and lstm univar bidir for univariate lstm multivar bidir and lstm multivar for multivariate for multivariate data the models use data for all observed variables as input and output estimates of the same variables for the point of interest model errors are examined for each variable and independent thresholds are set for anomaly detection 2 1 4 3 lstm preprocessing model building and training the functions for preprocessing model building and model training are compiled as sequenced steps in the lstm model wrapper functions fig 1 preprocessing for lstm models involves scaling reshaping and ensuring that training data are valid which is facilitated by using the output of the rules based detection data must be scaled so that extreme values do not have an outsized impact on the model and pyhydroqc includes a function for scaling create scaler based on the standardscaler function from the scikitlearn package which subtracts the mean and divides by the standard deviation to scale the data pedregosa et al 2011 reshaping data creates a sequence of immediately previous points i e model input for each data value i e model output pyhydroqc functions create sequenced dataset and create bidir sequenced dataset reshape data based on a user defined number of past time steps to build a model structure the pyhydroqc functions create vanilla model and create bidir model use the sequential model from the keras package keras development team n d with model layers lstm dense and bidirectional and the suite of user specified hyperparameters accepted by the sequential model to train the model the functions create training dataset and create bidir training dataset select a subset of data based on a user defined number of random points ensuring that none were identified as anomalous by the rules based detection these points are reshaped and used for training the lstm model the function train model uses the keras early stopping feature so that model training ceases when the error of the test and validation sets randomly selected by the algorithm are approximately equal 2 1 5 post processing dynamic threshold determination and anomaly detection a key component of model based anomaly detection using regression approaches is determination of the threshold that regulates whether a point is marked as anomalous or valid aquatic data vary seasonally daily and with environmental events changes that may not be adequately captured by a model a dynamic threshold has the potential to improve detection accuracy by applying a narrower range i e higher sensitivity when the model predictions are more precise and a wider range when model predictions are more variable in particular by using a dynamic threshold we hoped to identify localized outliers that are within the absolute expected range of values but are relatively distinct for a narrower time window and which were undetectable with a constant threshold pyhydroqc implements a dynamic threshold following the format of confidence intervals and prediction intervals used in other studies hundman et al 2018 leigh et al 2018 for each data point a threshold is determined based on a moving window of points equation 2 2 t μ z α 2 σ i f z α 2 σ m i n μ m i n o t h e r w i s e where t is the threshold μ is the mean of the user defined moving window model residuals σ is the standard deviation of the moving window model residuals α is a user defined value to adjust the width of the threshold z α 2 is the α 2 quantile of a normal distribution and min is a user defined parameter for the minimum threshold value note that min may be set to zero having no effect or to a non zero value to prevent too many false positives i e detections that are not anomalies this can occur when model residuals are low over an extended period and the dynamic threshold is smaller than the resolution or uncertainty inherent in the sensor given a time series of model residuals the set dynamic threshold function in pyhydroqc determines upper and lower thresholds for each point in a series using equation 2 with a user defined moving window the number of points used to calculate μ and σ the detect anomalies function then compares the dynamic threshold values to the residuals for each time step to determine whether a point is anomalous if rules based detection was performed the anomalies detected in that step are propagated through the workflow and are included in the detections output by this step 2 1 6 post processing anomaly events and widening in comparing anomalies identified by the model based detectors to anomalies labeled by technicians we observed mismatches related to resolution and lags in model approximations related to model smoothing when an anomaly is identified either the technician or the algorithm must determine how many points to label to address this in a systematic way pyhydroqc generalizes anomalies into numbered events consisting of groups of anomalous points by widening the detection window to include points before and after anomalies detected by the algorithm as well as points labeled by the technician overlap between the two is more likely in pyhydroqc the anomaly events function groups contiguous anomalous points as events by adding a column to the data frame with incrementing numbers as an index for each anomalous event to perform widening for each anomalous event the function assigns the event s index to points before and after the event the number of points is user defined effectively adding those points to the event 2 1 7 performance metrics for data with technician labels the function compare events determines valid and invalid detections by comparing events detected by the algorithm to those labeled by the technician each point is classified as true positive true negative false positive or false negative when there is any overlap between detected events and labeled events i e any portion of a labeled event is detected all points are classed as true positives to indicate that the labeled event was detected for accuracy the points assigned as anomalous on the edges of events by widening are removed from the event as part of this step a confusion matrix compares model classifications to actual data to evaluate overall performance by reporting total true positives true negatives false positives and false negatives leigh et al 2018 tan et al 2019 additional metrics that are commonly reported include positive predictive value precision negative predictive value accuracy recall and f scores li et al 2017 in pyhydroqc the function metrics determines the performance metric outputs in table 1 as aggregates of precision and recall f scores combine true positives false positives and false negatives into a single assessment score to assess models cook et al 2020 the f1 score gives equal weight to false positives and false negatives while the f2 score gives greater weight to false negatives f scores range from 0 to 1 with 1 being the upper bound because anomalies are sparse relative to the total number of data points the datasets are considered imbalanced chandola et al 2009 counts of true negatives are overwhelming resulting in high accuracy which may make it difficult to compare between models tan et al 2019 as a result anomaly detection focuses on true positives false positives and false negatives anomaly detection requires a balance between increasing true positives while reducing both false negatives and false positives objectives that may be mutually exclusive and depend on model sensitivity our preferred approach is to err on the side of sensitivity in the detector to minimize false negatives along with maximizing true positives even at the expense of increased false positives the f2 score supports this aim by more heavily weighting false negatives while the f1 score equally weights true negatives and false negatives cook et al 2020 2 1 8 aggregate detections in applying multiple models rather than select the single best performing model a robust approach is to aggregate results so that a point identified by any of the models as anomalous is considered a detection to address this pyhydroqc includes a function aggregate results for combining anomalies detected by the different model types into a single column of detected anomalies the outcome of aggregation is that a point is classed as anomalous if it was detected by any of the considered models when a point is identified as anomalous by either rules based detection or by any of the models it is denoted with a boolean in columns of output data frames one that corresponds to each model so the source of the anomaly may be traced through the process because rules based detections are propagated through the workflow and are present in the detections associated with each model the aggregation automatically includes the rules based detections 2 1 9 model based correction a primary goal of this work was to suggest corrections for anomalous points which is enabled by using time series regression methods for anomaly detection while the model predictions used to determine anomalies could be simply substituted as corrections the prevalence of consecutive anomalous points means that anomalous points would be used to determine corrections to prevent this correction models were implemented at a more granular scale a function generate corrections was developed that implements piecewise arima models using the following steps 1 given a data frame of observations with anomalies detected assign consecutive points with either valid or anomalous labels to alternating groups the function group bools adds a column populated with 0 for valid points and assigns each anomalous event a unique integer 2 ensure that sets of valid data points are large enough to generate forecast predictions where valid data points are in between anomalous points and the duration is too small to use as model input the function arima group merges them with previous and subsequent anomalous points into one anomalous group by resetting the group s incrementing index 3 for each anomalous group beginning with the group of shortest duration and progressing in order of increasing duration develop 2 arima models one based on the preceding valid points and one based on subsequent valid points using a specified maximum number of points for model development use the piecewise models to make forecasts and backcasts and blend them using the function xfade to get a single correction estimate for each point in the anomalous group 4 in the data frame populate a new column with the correction estimates for points in anomalous groups and with the observations for the points in valid groups to blend the forecast and backcast the values are weighted according to the proximity to each end point of the anomalous event as shown in equation 3 which is encoded in the function xfade 3 y k a k n k n 1 b k k 1 n 1 where y k is the correction estimate for each time step k in the anomalous group n is the total number of data points in the anomalous group to be corrected k 0 n 1 and a k and b k are the arima forecasted and backcasted values respectively examples in section 3 4 illustrate this concept because the arima correction is based on points immediately proximate instead of using the hyperparameters and model generated for the dataset as a whole each forecast and backcast is an individual arima model with hyperparameters and model fit based on the window of valid data using more granular models allows models to be tuned to that local time window and helps prevent errors that might arise from not having enough valid data points to estimate a point e g if p 9 for the time series as a whole at least 9 valid data points are required to avoid overfitting and to conserve computational resources the generate corrections function includes a user defined limit on the duration of data used to develop and train piecewise models to generate the forecasts and backcasts instead of applying corrections sequentially the correction function first corrects the events of shortest length and then corrects events of increasing duration in this manner corrected estimates are available as model inputs when needed for correcting longer events this helps ensure that the period of valid data before or after an anomalous event is sufficient to capture patterns 2 2 experimental use case logan river observatory data the primary objective of this work was to advance automation of quality control post processing specifically for environmental sensor data as an extensive test case we used data collected within the lro where high frequency monitoring is conducted at several climate and aquatic sites within the logan river watershed in northern utah usa http lro usu edu neilson et al 2021 monitoring sites were established and infrastructure was originally deployed using protocols described by jones et al 2017 the lro is similar to many research sites throughout the world where in situ monitoring of aquatic climatic and terrestrial variables is performed in support of research activities utah state university manages the monitoring network including site maintenance and data dissemination available at http lrodata usu edu the upper logan river watershed consists of mountainous forest and rangeland with limited development while the lower watershed is agricultural and urban with multiple agricultural diversions hydrology is generally driven by snowmelt and the upper watershed is characterized by karst topography aquatic monitoring sites are located in both the upper mountain canyon and lower urban agricultural sections and include sensors for water level water temperature ph dissolved oxygen specific conductance and turbidity fig 2 raw sensor observations are recorded on field dataloggers streamed to a central base station and loaded to an operational database jones et al 2015 technicians perform quality control post processing on collected data using a suite of interactive tools to generate a quality controlled copy of data horsburgh et al 2015 in this process technicians review data and consult with the record of field activities to identify label and correct anomalous points or events in the data lro data exhibit a number of anomaly types including outliers artificial persistence drift and others described by horsburgh et al 2015 currently post processing consumes approximately half of a full time technician s time with additional support from hourly assistants we sought to move toward automated methods to reduce the time and resources required to perform quality control post processing to test pyhydroqc we used data from the six aquatic sites shown in fig 2 for four variables common to aquatic monitoring and measured at all lro sites temperature ph specific conductance and dissolved oxygen most of the sites include over 6 years of data at 15 min intervals with few to no gaps for both raw and labeled corrected data to assess performance we used the metrics implemented in pyhydroqc to compare automated anomaly detection with the manual results produced by technicians lro sensor data were exported from a relational database observations data model horsburgh et al 2008 to flat csv files corresponding to each site indexed by a single date time column with columns for the measurements output by each aquatic sensor the pyhydroqc get data function was used to read the csv files into individual pandas data frames for subsequent analyses testing the software against case study data was performed on a 2017 macbook pro laptop with 16 gb ram and a 3 1 ghz quad core intel core i7 processor 3 results and discussion 3 1 preprocessing and settings the following subsections present the parameters configuration and settings used by each anomaly detection and correction procedure anomalies detected by the combination of rules range and persistence and models with thresholds arima and lstm are reported together in section 3 3 3 1 1 rules based detection and correction range and persistence checks for the lro data range thresholds were determined specific to each sensor based on manufacturer reported ranges and were further refined according to past observations at each site table c1 the maximum allowable persistence durations were also based on review of raw observations and varied with sensor initially persistence durations were set lower 5 10 time steps however those durations resulted in many false positives as sensors regularly reported repeated values for more than 10 time steps we observed that repeated values are often caused by limitations in sensor resolution so persistence durations were increased 30 45 time steps table c1 anomalies detected by these functions retained labels through subsequent steps so the metrics resulting from rules based detection are reported with the overall anomaly detection results in section 3 3 anomalies detected by the range and persistence checks were initially corrected by linear interpolation which is identical to the lro protocol used by technicians to manually correct over short periods however in the pyhydroqc anomaly detection and correction workflow the linear interpolation correction is an intermediate step to facilitate more accurate model development these points retain an anomalous label through subsequent steps of the workflow and are eventually corrected using the model correction algorithm consequently the final correction is performed by the model overwriting the interpolated points in the final corrected dataset 3 1 2 rules based detection and correction calibration and drift correction results from the calibration detection algorithms were compared to calibration events identified and corrected by technicians for all sensors at one site main street the persistence functions calib detect and calib overlap identified about 25 of the calibration events with a high false positive rate 5x the persistence we observed following a calibration may be specific to the sensors used in the lro ysi multiparameter sondes and not broadly applicable the edge detection function calib edge detect identified about 40 of calibrations for ph but was less successful 10 for specific conductance and dissolved oxygen additional effort could be applied to improve calibration event detection and to refine the parameters of the edge detection function threshold and width in theory the model algorithms should identify these local shifts as anomalies however although the observed values may deviate from the modeled the residuals were often within the dynamic thresholds as defined in table c1 and so were not detected as anomalies adjusting threshold settings may identify more calibration events but cause oversensitivity furthermore the corrective action required for calibration events is different from that of other anomaly types so the detection step should be separate although calibration events were not automatically detected with high accuracy the function for finding gap values was effective at determining valid gap values and end times for calibration shifts in a review of the results of the find gap function out of 100 distinct calibrations the total for all variables at main street revision was made for only 6 instances with calibration dates and gap values as inputs the function for linear drift correction was executed for all calibrated sensors specific conductance ph dissolved oxygen for the main street site many of the automatically determined gap values approximated the values used by the technician for correction in which case the linear drift correction was comparable to the technician correction some automatically determined values were judged as preferable to the technician selected gap value e g fig 3 in our experience selecting a viable gap value and performing drift correction can be the most time consuming aspect of manual quality control so although the algorithms we designed were not successful in identifying a majority of calibration events technicians typically record the dates of calibration and automatically determining the gap value and performing drift correction in batch is a significant improvement furthermore using an algorithm for this step increases consistency the range of gap values selected by multiple technicians was the primary source of quality control subjectivity identified by jones et al 2018 based on our testing using the lro data our recommended workflow for addressing drift and calibration events is to 1 identify a list of calibration dates generally from field notes although the pyhydroqc functions may be useful 2 determine gap values and associated times using the find gap function 3 review those shifts and make any adjustments and 4 use the dates and gap values as inputs to the linear drift correction function code for performing these steps including generating plots of gap values for review are demonstrated in example notebooks 3 1 3 model based detection and correction threshold determination the dynamic threshold used to evaluate differences between simulated and observed values directly impacts which observations are detected as anomalous or valid for the lro data we used trial and error to settle on window sizes alpha values and minimum range values for determining thresholds table c1 the same threshold settings were used for all model types we found that moving windows longer than a single day resulted in too much smoothing to the threshold and introduced artifacts due to daily patterns in model residuals in general window sizes of 5 10 h corresponding to 20 40 time steps were selected to balance between over smoothing of longer windows and highly dynamic thresholds of shorter windows an added benefit of smaller window sizes is that fewer computational resources are required to determine thresholds relatively small alpha values were selected 0 001 0 00001 to create a sufficiently high threshold range with larger alpha values the narrow threshold range was overly sensitive resulting in too many false positives minimum values were similar for all sensors across sites with a few exceptions as illustrated in fig 4 the pattern of spread in thresholds tracks with the variability in model residuals and residuals that exceed the threshold are detected anomalies 3 1 4 model based detection and correction model parameters and settings to create arima models p d q were determined for each lro data series over the full duration of data using the pdq function table c1 to build compile and train lstm models consistent parameters and settings were used for all of the lro data series and the several varieties of lstm models table c2 default settings and commonly used parameters géron 2017 keras development team n d were selected with minimal tuning to achieve the goal of satisfactory rather than perfect models models were trained with a randomized subset of 20 000 data points from each data series corresponding to approximately 10 of the dataset this number of data points was determined to be robust given that increasing the number of training points did not change the overall results and training with 20 000 data points was less resource intensive compared with larger training sets anomalous events in both technician labeled data and model detected data were widened by a single point widening factor 1 this setting was used for all data series and all model types 3 2 anomaly detection example examples help demonstrate the performance of the workflow for both successful and unsuccessful anomaly detection fig 5 additional examples in appendix d on 2018 11 11 the arima model detected an event that was not labeled by the technician false positive although this is a false positive the model with a dynamic threshold behaved as designed in detecting a localized outlier the events on 2018 11 12 and 2018 11 13 consist of points both detected by the algorithm and labeled by the technician true positive not all points labeled by the technician were detected as anomalies by the model however performing widening and considering the overlapping sets of points as anomalous events resulted in true positives for all of these points the event on 2018 11 14 was not detected by the algorithm but was labeled by the technician false negative there is nothing in the original data to indicate that something was amiss so it is unclear why the points were labeled as anomalous by the technician the technician has expert knowledge or is following protocol that the algorithm is unable to discern in assessing algorithm performance we defer to technician labels as a benchmark however the quality control process is subjective jones et al 2018 and data are not perfectly labeled making reliance on technician labels as a gold standard problematic russo et al 2020 in the lro data we identified numerous cases where it was unclear why some data points were labeled and others were not see appendix d which may be due to multiple technicians and evolving protocols among other reasons 3 3 combined anomaly detection results the f2 scores for all time series table 2 combine true positives false positives and false negatives to indicate overall performance for each model type rules based detection and an aggregate of all models higher scores indicate better model performance f2 1 would be a perfect score fig 6 is a visual illustration of the confusion matrix where each panel corresponds to a time series and each bar to a model type the bottom portion of each bar light blue represents true positives the middle portion orange represents false negatives and the sum of those is equivalent to all technician labeled points the top portion of each bar purple represents false positives the dashed lines distinguish the proportion of anomalies identified by rules based detection true positives below the lower dashed line black were detected by rules while those above it were only detected by models likewise false positives below the upper dashed line gray were detected by rules and false positives above it were detected by only models anomalies detected by rules those below each line may have also been detected by models so there may be overlap the results illustrate some general trends regarding the performance of both rules based and model based detection 3 3 1 detections due to rules and threshold settings for several time series the rules based algorithm accounts for the majority of anomaly true positive detections e g temperature at several sites dissolved oxygen at franklin basin in these cases the model detection did not provide many additional detections in other cases e g temperature at tony grove all ph time series most specific conductance and dissolved oxygen time series the true positives are split between rules based and model based indicating that the models captured anomalous events that the rules based detection missed this demonstrates the value of using both approaches in tandem in some cases the success of the model s in detecting anomalies true positives is offset by a large number of false positives particularly high counts of false positives indicate oversensitivity due to either persistence durations that are too short or to thresholds that are too tight both of which may result in too many detections in particular dissolved oxygen at franklin basin and mendon and specific conductance at blacksmith fork exhibit high rates of false positives given that most are under the rules based line the false positives are attributable to oversensitivity in rules range check or persistence duration rather than inadequate threshold settings the similar rates of false positives between models for many time series indicates that using the same threshold settings for all model types is acceptable cases with a large portion of false negatives undetected anomalies across models indicate that the models were not sensitive enough e g temperature at main street and blacksmith fork better detection might occur with tighter thresholds or adjusted rules based settings practitioners need to consider the tradeoffs with model sensitivity in determining threshold settings under the assumption that anomalies identified by the algorithm would be further reviewed by a technician the thresholds can be set to capture more potential anomalies erring on the side of false positives however sensitivity must be balanced to avoid excessive false positives from narrow thresholds 3 3 2 model comparison the detections between all models were generally comparable e g temperature at most sites ph at most sites dissolved oxygen at several sites although for a few time series there were distinct variations in results between models e g specific conductance at franklin basin and tony grove dissolved oxygen at tony grove arima models gave the best average f2 score table 2 they generally outperformed lstm models for the cases with differences in model performance and were often slightly better than the lstm models for the time series with comparable results arima was generally more sensitive detecting more true positives than the lstm models at the expense of detecting more false positives results from the lstm models varied without a discernible pattern in one case the univariate bidirectional model excelled temperature at main street while in other cases the multivariate vanilla was preferred specific conductance at franklin basin and tony grove dissolved oxygen at the water lab for some of these series the more successful models detected a few points that were part of long events that were labeled anomalous by technicians which improved the performance of those model types although the multivariate and bidirectional models include more information in their input either with additional variables or additional points in time these models did not broadly outperform their simpler counterparts with regard to the multivariate model while there is some physical relationship between the variables of interest the lstm model has no explicit physical drivers we might expect that the behavior of other variables related to diurnal cycle or ambient events to offer some predictive information however most types of anomalies occur for a single variable independent of other sensors some conditions such as sediment or ice built up around a multi parameter sensing sonde or an issue with power to the sensors may impact more than one sensor simultaneously in these cases there is value in noticing concurrent anomalies in multiple sensors because not all studies measure all variables single sensor algorithms are more versatile and can perform well as shown in these results there may be systems in which variables are so physically related that a more directly dependent relationship model can be more reliable differences in anomaly detection between the model types could be due to several factors arima and lstm models have inherently different structures with distinct processes for hyperparameter tuning and model training arima models use a limited number of hyperparameters three which were tuned by automated optimization while lstm models include several hyperparameters for which minimal tuning was performed it is possible that lstm models could be improved with additional tuning however the process may not be worth the effort given that the objective of modeling was to detect anomalies rather than generate a perfect model as one example we observed lstm models consistently biased toward the overall time series mean which was reduced when developed with input sequences containing fewer previous data points 5 versus 10 another possible explanation for the poorer performance of lstm models is a result of the training process lstm models were trained on a randomized subset of available data due to the stochastic nature of training data selection and initialization of weights a new model is developed each time the algorithm is run although pyhydroqc can save models for future use if a distinct set of training data was used or learning converges to a local minimum it may cause the seemingly arbitrary failure of some lstm models on certain time series to test this lstm models were regenerated the resulting metrics were similar to those reported in table 2 this indicates that the size of the training sets is sufficient so that the strength of the model does not depend on the specific randomized subset of data used for training independently developing and training multiple models on the same time series is a straightforward check for training data robustness although we tested across a range of sites that span elevation land use and hydrologic regime within the lro these locations do not represent the full spectrum of sites across the world investigating the suitability of the algorithm to additional physical settings is an important next step more directly examining the performance of each model type related to physical characteristics of locations may help inform transferability of the techniques 3 3 3 model aggregation the comparability of most of the results suggests that using any one of the models may be acceptable however rather than select a single model aggregating detections by the multiple models may improve results f2 scores of aggregated anomaly detection table 2 indicate overall good performance for most time series f2 0 8 also illustrated by confusion matrix plots fig 7 for some time series the aggregation does not add high value presumably because the same points were detected by multiple models however for a few time series in particular aggregating detections of multiple models had a synergistic effect such that the aggregate f2 score is higher than that of any single model e g temperature at main street dissolved oxygen at tony grove lower f2 scores 0 8 that persist after aggregating model detections are a result of either high rates of false positives dissolved oxygen at franklin basin or false negatives temperature at blacksmith fork both of which could be addressed by tuning rules and threshold settings as described rather than perfecting models the results affirm that time series regression methods with dynamic thresholds and widening are an effective tool for automating anomaly detection and correction and implementing these techniques can streamline the quality control process without the models a technician would need to review 200 000 data points for each of the time series used in this case study by using the pyhydroqc anomaly detection workflow the number of data points for review referring to combined rules and model detections is reduced by at least an order of magnitude e g 20 000 for ph at franklin basin even for cases with high rates of false positives e g 4000 for dissolved oxygen at franklin basin 3 4 model based correction examples the model based anomaly correction implemented in pyhydroqc generally resulted in smooth data profiles without outstanding nonlinearities fig 8 the method offers a viable path for correcting many anomalous events although results varied depending on the duration the variable the season and the reliability of anomaly detection for shorter durations e g approximately 2 h fig 8a the model corrected data are similar to the technician correction i e linear interpolation for longer periods the blended forecasts and backcasts can estimate patterns diurnal cycles fig 8b and c that would not be practical for a technician to approximate in these cases technicians did not attempt corrections but set data to a no data value 9999 in other cases the model did not capture data patterns particularly for extended periods see appendix d for examples some models overgeneralized and missed patterns while others focused on a single dominant feature overall the correction algorithm better captured diurnal patterns in temperature and ph data while regular patterns in specific conductance and dissolved oxygen were less consistently approximated 3 5 combined correction results quantifying the overall performance of the correction algorithm for each time series is impractical because no gold standard exists for comparison algorithm corrected data cannot be quantitatively compared to technician corrected data because the technician corrected data are subjective contain correction and labeling errors and include many periods where the values were set to a designated no data value e g 9999 for the lro for correcting lro data technicians followed one of the following paths 1 linear interpolation for periods less than 4 h or 2 setting values to 9999 for longer periods where interpolation was deemed unreasonable technicians also performed linear drift correction between identified calibration events the model based correction algorithm is not designed to correct for drift which was performed as part of the rules based steps section 3 1 2 without a benchmark correction algorithm performance cannot be definitively measured for each time series leaving evaluation to be done qualitatively on a case by case basis section 3 4 and appendix d we considered simulating artificially introduced anomalies which are then corrected and compared to valid raw data however it is unclear what frequency and duration of artificial anomalies would be appropriate and how to propagate artificial anomalies through multiple concurrently measured variables i e in the case of multivariate models we determined that analysis to be outside the scope of this work in an attempt to assess the value of the correction algorithm in terms of relative accuracy we considered the total number of points in each series that were altered from the raw data by the technician or the algorithm and that were set to values outside of a valid range table 3 ranges specific to each time series were adopted from the range checks in rules based preprocessing table c1 to determine whether altered points were valid technician corrections resulting in invalid values generally correspond to data changed to the no data value of 9999 causes of invalid values produced by the correction algorithm may include periods where anomaly detection was not adequately inclusive so the points corrected by the algorithm were overly influenced by anomalous points that were not labeled as such figure c5 in another scenario anomalous data may be close to the range limits resulting in forecasts backcasts and corrections outside of the valid range e g the estimations of peaks in fig 8b exceed the upper limit for that time series for most cases the algorithm correction resulted in significantly fewer invalid values than the technician correction for 16 out of 24 time series most of the temperature specific conductance and ph series the number of invalid points produced by the algorithm correction was less than 100 out of 200 000 total points while the number of invalid points produced by the technician was significantly higher ranging from 22 to 8541 for five of the time series primarily dissolved oxygen the algorithm correction resulted in a higher number of invalid values for some of these series the anomaly detection was also less performant e g dissolved oxygen at franklin basin tony grove and mendon fig 7 these results highlight the need to review anomaly detections and refine settings to improve anomaly detection although the corrections classed as valid were within an acceptable range for that time series the correction may not have approximated observed data patterns so review of proposed algorithm corrections is necessary the overarching benefit of the correction in pyhydroqc is that the algorithm may capture diurnal patterns to suggest values that a technician could not estimate however anomalous events need review prior to correction as do correction suggestions adjacent data may be inadequate to generate correction estimates for the full duration of an anomalous event a more complete workflow could offer correction options for each anomalous event for review and selection by a technician 4 conclusions we developed a new python package pyhydroqc that enables application of rules based and time series regression techniques coupled with dynamic thresholds as part of a workflow to detect and correct anomalies in aquatic sensor data functions to implement the models and supporting steps in the workflow are contained in the python package and documented within the github repository available functions include rules based anomaly detection calibration detection and drift correction model development and estimation threshold determination anomaly detection and widening performance metrics reporting and model based correction although this workflow advances the automation of sensor data post processing and can be implemented in any python environment a python package and scripts may not be intuitive tools for some technicians a graphical user interface offering more interactive review could be built on top of the underlying functionality contained in pyhydroqc another potential next step is investigating execution of the software on an edge computing device to streamline the process and reduce reliance on centralized systems we tested the methods on 24 time series of aquatic data from the logan river in northern utah the case study sites varied in physical characteristics and spanned 5 6 years of high frequency data based on our case study the anomaly detection workflow enabled by pyhydroqc was successful with high detection rates arima models were most performant likely due to differences in model structure and development rather than using constant thresholds dynamic thresholds allowed for responsiveness to data variability adjusting threshold settings impacts the sensitivity of model detection we suggest erring on the side of oversensitivity and then reviewing detections a correction algorithm used blended forecasts and backcasts of local models to make correction estimates that follow data patterns for events of up to several days for some observed variables these approximations surpass a technician s ability to correct anomalous data but each corrected event needs review a rules based approach was successful in determining calibration gap values and performing linear drift correction with calibration dates as input though not completely automated this work helps to streamline the process of quality control related to sensor drift and calibration beyond the case study data applying the techniques to datasets from other locations and environmental variables outside of the aquatic domain is an important next step manual detection and correction performed by technicians is an extended process that overlaps with other tasks to perform quality control for 3 6 month durations of a single time series takes multiple days of dedicated effort in comparison implementing the complete pyhydroqc workflow for anomaly detection and correction for all variables at a single site for a single year of data takes a few hours to run in the background on a personal computer a technician will still need to review results however we submit that the package and workflow offer significant resource savings throughout this process the technician was treated as an oracle with technician labels dictating algorithm performance the subjectivity inherent in manual quality control and uneven application of labels by technicians highlight the need for improving consistency in quality control which is an important driver of automating post processing given that computers are not subjective in their decisions as the volume of environmental sensor data continues to increase so does the need for performing post processing quality control this work contributes tools and approaches that can be used to streamline and automate the quality control process to reduce the costs of manual quality control facilitate a post processing workflow that is reproducible defensible and consistent and provide reliable data for analysis and decision making declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this research was primarily funded by the united states national science foundation under grant number 1931297 any opinions findings and conclusions or recommendations expressed are those of the authors and do not necessarily reflect the views of the national science foundation additional funding support was provided by the utah water research laboratory at utah state university ongoing data collection and management in the lro is supported by funding from the utah state legislature administered by the utah division of water resources additional funding for the lro has been provided by utah state university logan city and the cache water district we gratefully acknowledge the work of the many technicians and students who have participated in maintaining and operating the lro instrumentation along with producing the quality controlled sensor data that were used in testing the software we developed appendix a background manual post processing by a technician remains the most commonly implemented approach for correcting anomalies in environmental sensor data software tools have been developed to assist technicians in performing quality control wherein anomalies are identified visually or using filters or rules that are implemented based on user input horsburgh et al 2015 sheldon 2008 while initially straightforward to implement manual post processing is resource intensive requires significant expertise and may be implemented unevenly within and between sensor networks additionally manual approaches may not be reproducible making it difficult to track the provenance of data from raw measurements to quality controlled products data driven anomaly detection has the potential to address the deficiencies of manual post processing by streamlining and standardizing the workflow numerous data driven approaches have been documented for anomaly detection chandola et al 2009 cook et al 2020 tan et al 2019 basic approaches use rules to test data plausibility e g range and variability checks taylor and loescher 2013 and even studies with complex workflows initially implement rules based approaches e g leigh et al 2018 statistical approaches rely on the distribution of data to identify points outside of the expectation cook et al 2020 regression approaches estimate a value and compare it to the observation chandola et al 2009 feature based approaches apply numerous variables or features within one or more machine learning methods to determine if the data point should be grouped with valid or anomalous points talagala et al 2019 in approaching data driven methods for anomaly detection important considerations include data extent what duration of data are available some methods require data partitioned into separate groups for training and testing models data labels do sufficient data exist in which anomalies have been identified by an expert the availability of labeled data impacts which types of models can be used supervised model types require labeled data for training while unsupervised model types do not for all model types labeled data enable assessment of performance data quality do sufficient data exist in which anomalies have been corrected some methods require clean data that are free from anomalies for training models variables what variables are to be considered is a single variable sensor observed or are multiple variables measured do sensors at nearby sites provide additional information anomaly types what types of anomalies are of particular concern can rules based detection effectively detect some of these cases online offline detection does detection need to occur in real time online or is a retrospective offline approach acceptable in the following sections we provide a brief description of several approaches and methods for detecting and correcting anomalies in environmental sensor data we also illustrate gaps in the current state of practice for anomaly detection and correction in the quality control process a 1 data redundancy approaches various types of data redundancy including sensors people and models are used to detect anomalies in environmental sensor data the gold standard world meteorological organization 2008 mourad and bertrand krajewski 2002 compares data from multiple sensors requiring at least three sensors to determine which observation is erroneous increased cost maintenance power and data storage requirements challenge observational networks to implement redundant sensors furthermore multiple sensors may all exhibit the vagaries of environmental events sensor malfunctions and infrastructure failures complicating assessment and correction of data quality to improve the consistency of quality control jones et al 2018 suggest another form of data redundancy in which multiple technicians collaborate to review and correct data finally data redundancy may be achieved by modeling expected values for comparison with sensor measurements a physically based model could be used however model availability and uncertainty are barriers moatar et al 2001 given the relative simplicity of implementation ability to scale to large volumes of data few input requirements and potential for fast performance statistical and data driven techniques may be more appropriate thus we examined several classes of data driven techniques to model expected sensor behavior as data redundancy approaches a 2 univariate or multivariate approaches some predictive time series models are based on data from a single sensor independent of the condition of other co located sensors or data advantages of these univariate methods are that processing can be performed on multiple sensors independently and simultaneously and gaps or errors in data from one sensor will not impact data from other sensors hill and minsker 2010 however anomalies in one sensor stream may correspond to anomalies in a related sensor so approaches that utilize the information from multiple sensors provide multiple lines of evidence toward anomaly detection li et al 2017 furthermore when performing quality control post processing technicians regularly consult the record of other variables simultaneously recorded at the same site to check for internal consistency campbell et al 2013 and to inform corrective actions there is no clear best approach and even the same authors simultaneously promote a univariate detector hill and minsker 2010 and a multivariate approach hill et al 2009 either method may yield acceptable results although leigh et al 2018 report poor performance for multivariate time series regression compared to univariate the data in question will drive whether a univariate method is required or if additional power could be achieved with multiple variables in our work we considered both univariate and multivariate approaches and compared the benefits and drawbacks related to the data we examined a 3 spatial dependency external consistency refers to comparison with data from other locations campbell et al 2013 and some data driven approaches are based on relationships between sites in particular spatial dependencies between weather sensors have been used to identify anomalies galarus et al 2012 in another application data driven models used weighted data from neighboring stream monitoring sites to infill daily mean flow records giustarini et al 2016 one study included data at an upstream site offset by estimated travel time to detect anomalies in aquatic data conde 2011 spatial methods assume high correlation for a particular variable at sites having similar characteristics which may not be clearly established for the data of interest in this work we focused models on data at a single site of interest so that detection and correction could be applied to sites independently a 4 regression approaches regression models are a class of data driven anomaly detectors for time series that predict the next anticipated value based on previous data either univariate or multivariate to detect anomalies with regression the modeled value is compared to the observed and a range of acceptability is determined for the residuals such that points outside of that range are classed as anomalous and vice versa constant acceptability thresholds may be based on a user defined range or determined as a prediction interval based on the model results leigh et al 2018 thresholds may also be dynamic varying based on the range of the model residuals hundman et al 2018 for example in one study dereszynski and dietterich 2007 the threshold range for an observation varied based on the modeled state of the sensor i e a narrower range when the sensor was classed as good versus bad auto regressive integrated moving average arima is a regression technique that uses a combination of past data to forecast the next point arima has been successfully implemented to predict environmental data and subsequently detect anomalies hill and minsker 2010 leigh et al 2018 papacharalampous et al 2019 another regression technique based on a previous sequence of data is long short term memory lstm a class of artificial neural networks anns though applications to environmental data anomalies to date are limited lstm models have been used to reconstruct time series to detect anomalies in other fields hundman et al 2018 lindemann et al 2019 malhotra et al 2016 yin et al 2020 and other ann model types have been used for environmental anomaly detection hill and minsker 2010 russo et al 2020 other algorithms that show promise for time series regression include prophet a time series forecasting method developed by facebook with focus on business applications taylor and letham 2018 and hierarchical temporal memory htm ahmad et al 2017 another method that has been implemented for anomaly detection in environmental sensor data is dynamic bayesian networks which predict values in a time series based on assigned model states corresponding to temporal windows studies developed models based on a few previous points hill et al 2009 thousands of previous points hill and minsker 2006 and multiple past years of data to give an output based on the day of year and hour of day dereszynski and dietterich 2007 these models assume that temporal states can be definitively assigned as well as consistently applied and we did not attempt them due to complexity and obscurity of implementation because regression models produce an estimate they are well suited for both detection and correction of anomalous data the time series regression models we investigated were arima lstm and facebook prophet while arima has been commonly attempted for anomaly detection in time series data other techniques are emergent in this field e g lstm and there are few examples comparing multiple regression techniques for aquatic sensor data a 5 feature based approaches feature based methods comprise another class of anomaly detectors commonly used for discrete data tan et al 2019 which some authors have applied to environmental time series leigh et al 2018 russo et al 2020 talagala et al 2019 unlike regression methods feature based methods do not make a prediction of the observation anomalies are detected either based on a supervised model trained to data labels anomalous or valid russo et al 2020 or an unsupervised model that determines the likelihood of the point being anomalous based on distance to neighboring points these methods rely on multiple variables as model input features which in the case of aquatic sensor time series may correspond to variables measured concurrently by adjacent sensors past values of the variable of interest or transformations of the relationships between these variables particularly for data with temporal correlation it is not obvious which features should be selected and complex feature engineering may be required christ et al 2018 another challenge is selecting an appropriate data transformation a preprocessing step e g taking the first derivative of the data to highlight outlying points leigh et al 2018 talagala et al 2019 almost any feature based machine learning method may be applied to anomaly detection problems and approaches described in the literature include principal components analysis support vector machines tran et al 2019 hdoutliers leigh et al 2018 k nearest neighbor russo et al 2020 talagala et al 2019 clustering hill and minsker 2010 random forest russo et al 2020 xgboost and isolated forest smolyakov et al 2019 the success of feature based techniques in detecting anomalies from environmental sensor data is mixed hill and minsker 2010 leigh et al 2018 russo et al 2020 as they do not make predictions feature based approaches are not well suited to performing corrections given that our objectives were to both detect and correct anomalies we did not pursue feature based approaches in the work reported here a 6 anomaly types in most of the studies cited here the emphasis is on anomalies that are outliers where the value of the variable is outside of expected ranges or rates of change detection of gradual bias that may occur due to drift in the sensor or ongoing fouling has not been successfully reported the models implemented by dereszynski and dietterich 2007 identify some biases resulting from abrupt shifts in conditions however the authors acknowledge that complex anomalies are outside of the performance of their detector conde 2011 was unable to identify labeled anomalies with relatively small variation from the measured baseline leigh et al 2018 intentionally prioritized outliers in development of anomaly detection techniques for aquatic sensors given that existing methods have not addressed anomalies caused by drift and fouling there is significant room for improvement in methods for detecting these types of anomalies we examined both outliers and more subtle anomaly types in our methods and software implementation a 7 reproducibility although effectively implemented for specific case studies in the research realm none of the techniques described in the cited studies have been packaged as easily accessible software for broad application and dissemination without reusable code the specifics of the algorithms as implemented with environmental data cannot be examined further tested or applied to other datasets recent work in outlier detection was encapsulated in an r package talagala et al 2019 however a lack of documentation made it difficult to know how to install the package and apply the methods to our datasets provenance of data from raw field observations to quality controlled data products is vitally important yet rarely described in sufficient detail that the process used to arrive at final data products could be repeated horsburgh et al 2015 applying more automated techniques can help and reusable software tools can overcome barriers related to understanding and implementing complex algorithms for practical application rather than a model calibrated to a specific variable site combination practitioners need tools that can be applied to a broad suite of variables and or monitoring locations documented in a reusable and reproducible way thus we sought to package the tools we developed as open source software that could easily be deployed in a commonly available analytical environment a 8 anomaly correction various techniques and past studies developed functionality for detecting anomalies but few applied corrective actions which is an important and time consuming step in quality control post processing a handful of studies used modeled arima forecasts to directly replace anomalies that were detected by the same arima model termed anomaly detection and mitigation adam hill and minsker 2010 leigh et al 2018 however the objective of adam was to improve detection by ensuring that model input data did not include detected anomalies not to generate a corrected version of the dataset furthermore the success of adam was mixed and resulted in high rates of false positives leigh et al 2018 given the general lack of available methods for automated correction we explored new approaches for inclusion in the software package we developed appendix b list of files and functions this appendix provides a listing of each of the python files in the pyhydroqc package and describes the functionality that each provides more detailed documentation is found in the github repository and package documentation see the software availability section parameters py this file contains assignments of parameters for all steps of the anomaly detection workflow parameters are defined specific to each site and observed variable that are referenced in the detect script lstm parameters are consistent across sites and variables arima hyper parameters are specific to each site variable combination other parameters are used for rules based anomaly detection determining dynamic thresholds and for widening anomalous events anomaly utilities py contains functions for performing anomaly detection and correction get data retrieves and formats data retrieval is based on site observed variable and year to pass through subsequent steps the required format is a pandas data frame with columns corresponding to datetime as the index raw data corrected data and data labels anomalies identified by technicians anomaly events widens anomalies and indexes events or groups of anomalous data assign cm a helper function for resizing anomaly events to the original size for determining metrics compare events compares anomaly events detected by an algorithm to events labeled by a technician metrics determines performance metrics of the detections relative to labeled data event metrics determines performance metrics based on number of events rather than the number of data points print metrics prints the metrics to the console group bools indexes contiguous groups of anomalous and valid data to facilitate correction xfade uses a cross fade to blend forecasted and backcasted data over anomaly events for generating data correction set dynamic threshold creates a threshold that varies dynamically based on the model residuals set cons threshold creates a threshold of constant value detect anomalies uses model residuals and threshold values to classify anomalous data aggregate results combines the detections from multiple models to give a single output of anomaly detections plt threshold plots thresholds and model residuals plt results plots raw data model predictions detected and labeled anomalies modeling utilities py contains functions for building and training models pdq automatically determines the p d q hyperparameters of a time series for arima modeling build arima model lstm univar lstm multivar lstm univar bidir lstm multivar bidir wrappers that call other functions in the file to scale and reshape data for lstm models only create and train a model and output model predictions and residuals create scaler creates a scaler object for scaling and unscaling data create training dataset create bidir training dataset creates a training dataset based on a random selection of points from the dataset reshapes data to include the desired time steps for input to the lstm model the number of past data points to examine or past and future points bidirectional ensures that data already identified as anomalous i e by rules based detection are not used create sequenced dataset create bidir sequenced dataset reshapes all inputs into sequences that include time steps for input to the lstm model using either only past data points or past and future data points bidirectional used for testing or for applying the model to a full dataset create vanilla model create bidir model helper functions used to create single layer lstm models train model fits the model to training data uses a validation subset to monitor for improvements to ensure that training is not too long rules detect py contains functions for rules based anomaly detection and preprocessing depends on anomaly utilities py functions include range check scans for data points outside of user defined limits and marks the points as anomalous persistence scans for repeated values in the data and marks them as anomalous if the duration exceeds a user defined length group size determines the maximum length of anomalous groups identified by the previous steps interpolate corrects data points with linear interpolation a typical approach for short anomalous events add labels enables the addition of anomaly labels referring to anomalies previously identified by an expert in the case that labels may have been missed for corrected data that are nan or have been set to a no data value e g 9999 calibration py contains functions for identifying and correcting calibration events functions include calib edge detect identifies possible calibration event candidates by using edge filtering calib persist detect identifies possible calibration event candidates based on persistence of a user defined length calib overlap identifies possible calibration event candidates by finding concurrent events of multiple sensors from the calib persist detect function find gap determines a gap value for a calibration event based on the largest data difference within a time window around a datetime lin drift cor performs linear drift correction to address sensor drift given calibration dates and a gap value model workflow py contains functionality to build and train arima and lstm models apply the models to make predictions set thresholds detect anomalies widen anomalous events and determine metrics depends on anomaly utilities py modeling utilities py and rules detect py wrapper function names are arima detect lstm detect univar and lstm detect multivar lstm model workflows include options for vanilla or bidirectional within each wrapper function the full detection workflow is followed options allow for output of plots summaries and metrics arima correct py contains functionality to perform corrections and plot results using arima models depends on anomaly utilities py arima group ensures that the valid data surrounding anomalous data points and groups of data points are sufficient forecasting backcasting arima forecast creates predictions of data where anomalies occur generate corrections the primary function for determining corrections passes through data with anomalies and determines corrections using piecewise arima models corrections are determined by averaging together cross fade both a forecast and a backcast appendix c logan river observatory input parameters and settings table c1 input parameters for each time series persistence duration and window size refer to the number of time steps 20 5 h 30 7 5 h 40 10 h 45 11 25 h table c1 observed variable parameter franklin basin tony grove water lab main street mendon blacksmith fork temperature degrees c maximum range 13 20 18 20 28 28 minimum range 2 2 2 2 2 2 persistence duration 30 30 30 30 30 30 window size 30 30 30 30 30 30 alpha 1e 04 1e 05 1e 04 1e 05 1e 04 1e 04 threshold minimum 0 25 0 4 0 4 0 4 0 4 0 4 p d q 1 1 3 10 1 0 0 1 5 0 0 0 3 1 1 1 1 0 specific conductance μs cm maximum range 380 500 450 2700 800 900 minimum range 120 175 200 150 200 200 persistence duration 30 30 30 30 30 30 window size 30 40 40 40 40 20 alpha 1e 04 1e 05 1e 04 1e 06 1e 05 1e 02 threshold minimum 4 5 5 5 5 4 p d q 10 1 3 6 1 2 7 1 0 1 1 5 9 1 4 0 0 5 ph maximum range 9 2 9 9 2 9 5 9 9 2 minimum range 7 5 8 8 7 5 7 4 7 2 persistence duration 45 45 45 45 45 45 window size 30 40 40 20 20 30 alpha 1e 05 1e 05 1e 05 1e 04 1e 04 1e 05 threshold minimum 0 02 0 02 0 02 0 03 0 03 0 03 p d q 10 1 1 8 1 4 10 1 0 3 1 1 0 1 2 0 1 4 dissolved oxygen mg l maximum range 13 14 14 15 15 14 minimum range 8 7 7 5 3 2 persistence duration 45 45 45 45 45 45 window size 30 30 30 30 30 30 alpha 1e 04 1e 04 1e 05 1e 05 1e 03 1e 04 threshold minimum 0 15 0 15 0 15 0 25 0 15 0 15 p d q 0 1 5 10 1 0 1 1 1 1 1 1 10 1 3 0 0 5 table c2 lstm model parameters and settings selected for the lro case study defaults were used for all other settings and parameters not listed here see géron 2017 and keras development team n d for additional details table c2 parameter function setting details time steps model add 5 the number of past data considered as input for prediction for the lro data more time steps 10 15 20 biased results toward the mean reduced time steps 5 gave greater accuracy and improved computational time units cells model add 128 number of cells or nodes in the model architecture there is no rule for finding the perfect number of cells we chose a high number and used early stopping and dropout to prevent overfitting for processing purposes it is generally preferred to have network dimensions in multiples of 32 dropout model add 0 2 a fraction of cells that are randomly ignored during training using dropout improves the model by reducing overfitting but the number usually matters little 20 is often used to balance accuracy and overfitting optimizer model compile adam algorithm for training adam adaptive movement estimation is commonly selected for training lstm models for being computationally efficient requiring little memory and handling large amounts of data loss model compile mean absolute error the quantity to be minimized during training mean absolute error computes the mean of the difference between observations and predictions epochs model fit 100 the number of rounds to train the model we opted for a high number that is truncated by early stopping that ends training when the model is sufficiently fit validation split model fit 0 1 fraction of training data to be used as validation data on which the loss is evaluated at the end of each epoch callbacks model fit early stopping interrupts training when performance on the validation set drops patience model fit 6 number of epochs with no improvement after which training will be stopped shuffle model fit false whether to shuffle training data before each epoch set to false because the order of training data matters for these data appendix d anomaly detection and correction examples this appendix includes additional examples of anomaly detection and correction performed by the pyhydroqc workflow on lro case study data figure c1 illustrates anomaly detection false positives and true positives peaks and troughs in the data were considered anomalies by the model arima but only two of them 2017 12 18 and 2017 12 26 were labeled by the technician it is unclear why certain peaks were labeled by the technician while others were not although this example includes several false positives the algorithm behaved as expected fig c1 examples of anomalies detected using an arima model for specific conductance at main street fig c1 in some cases the apparent success of the model results may be an artifact of both the generalization of detections in the compare events function and the liberal application of labels by technicians some time series contain extensive periods of data labeled as anomalous that correspond to concerns with sensor validity or site conditions e g figure c2 when comparing events to determine confusion matrix categories any overlap in model detections results in all points of the anomalous period being identified as true positives this is an example where large events may bias the metrics toward true positives if any point in the event is detected or toward false negatives if the event goes undetected less likely this particular event contributes to the 13 000 true positives for this time series ph at main street fig c2 examples of anomalies detected using an lstm multivariate bidirectional model for ph at main street for of an extended period of data labeled as a sensor malfunction fig c2 we were interested in whether the models could detect calibration events for one time series ph at main street one model type lstm multivariate bidirectional detected approximately 20 of labeled calibration events we found that the master list of calibrations recorded in the field notes differs from what technicians labeled in the data some calibrations recorded in the field notes were not labeled by technicians in the data and other events labeled by technicians appeared to be calibrations but were not part of the master list derived from the field notes these discrepancies point to deficiencies in the labeled data the model predictions are erratic and do not track the observations at most calibration events figure c3a even if the threshold was not sensitive enough to result in detections in some cases calibration events were detected as anomalous by the model figure c3b but there was no mechanism to distinguish from other anomalies these examples illustrate the challenge of using the model based approach for detecting and correcting calibration events fig c3 examples of anomalies detected using an lstm multivariate bidirectional model on a ph sensor at main street with calibration events fig c3 a direct comparison of results from each model type illustrates model behaviors and associated detections for specific conductance at tony grove where there was variability in performance between model types see section 3 4 the arima and lstm multivariate vanilla models detected points at the edges of long duration labeled events improving their performance metrics relative to the other model types figure c4 further illustrates differences between model estimates and resulting detections for the first date range the estimates of both multivariate models deviate from the original data because they use other variables as input in the absence of this information only one univariate model detects an anomaly in the second date range models responded to the localized event in distinct ways and none resulted in a detection in the third date range estimates from the multivariate models exhibit spikes around the detections illustrating that information is coming from other variables it is likely that some of these labeled anomalies correspond to calibration events for which other variables exhibited greater shifts than did specific conductance fig c4 examples comparing model estimates and detected anomalies for all model types for specific conductance at tony grove fig c4 although the correction algorithm was capable of capturing diurnal oscillations in some cases data patterns did not translate and propagate through the corrections e g figure c5 because each correction is based on individual independent models trained for data immediately prior to and following an anomalous event the number of data points considered can vary even though the adjacent data used for input is limited by the maximum duration parameter some models may still overgeneralize i e a straight line other models may use so little data that a pattern is missed while still others are focused on a single dominant feature i e an oscillation or a curve furthermore a pattern may be damped over an extended time period explicitly incorporating seasonality into development of the arima models may result in more consistent output of oscillations however developing seasonal arima models is computationally demanding and the correction algorithm already requires significant computational resources the correction algorithm is directly dependent on identified anomalies in figure c5c an anomalous event 2018 06 19 2018 06 20 was detected by the model but even with widening the initial abrupt decrease was not labeled anomalous so it was considered valid data and it directly influenced the forecast for the correction algorithm to be effective anomalies should be reviewed and may need adjustment e g further widening fig c5 examples of problematic algorithm correction a and b dissolved oxygen at tony grove c specific conductance at mendon fig c5 
25605,sensors measuring environmental phenomena at high frequency commonly report anomalies related to fouling sensor drift and calibration and datalogging and transmission issues suitability of data for analyses and decision making often depends on manual review and adjustment of data machine learning techniques have potential to automate identification and correction of anomalies streamlining the quality control process we explored approaches for automating anomaly detection and correction of aquatic sensor data for implementation in a python package pyhydroqc we applied both classical and deep learning time series regression models that estimate values identify anomalies based on dynamic thresholds and offer correction estimates techniques were developed and performance assessed using data reviewed corrected and labeled by technicians in an aquatic monitoring use case auto regressive integrated moving average arima consistently performed best and aggregating results from multiple models improved detection pyhydroqc includes custom functions and a workflow for anomaly detection and correction keywords aquatic sensors quality control anomaly detection python data management software and data availability name of software pyhydroqc description a python package for automated detection and correction of anomalies in aquatic sensor data developer and contact information amber jones amber jones usu edu year first available 2021 program language python 3 7 hardware required personal computer running microsoft windows apple macos or linux software required pyhydroqc uses the following python packages all of which are available via the python package index pypi numpy 1 19 1 pandas 1 1 0 matplotlib 3 3 0 scipy 1 5 2 pmdarima 1 6 1 tensorflow 2 3 keras 2 4 3 statsmodels 0 11 1 scikit learn 0 23 2 os warnings pickle random software availability the pyhydroqc software is open source and is released under the berkeley software distribution version 3 bsd3 software license it can be installed within a python environment from the python package index pypi using the pip utility source code documentation and examples for the software are freely available in github at https github com ambersjones pyhydroqc and zenodo jones et al 2022 dataset availability a resource containing the input data processing scripts results and code to generate plots in this manuscript is described and stored in hydroshare jones et al 2022 additional documentation a resource containing an example jupyter notebook with instructions is described and stored in hydroshare jones 2022 all functions included in the package are documented here https ambersjones github io pyhydroqc 1 introduction observation of environmental phenomena using in situ sensors is increasingly common as sensors and related peripherals become more affordable and as cyberinfrastructure and expertise to support their operation have grown hart and martinez 2006 pellerin et al 2016 rode et al 2016 sensors are subject to environmental factors that affect measurements and their suitability for subsequent analyses data from environmental sensors include anomalous points and biases that are artifacts of instrument noise or drift power failures transmission errors or unusual ambient conditions horsburgh et al 2015 wagner et al 2006 protocols for ensuring quality of environmental sensor data quality assurance and mechanisms for performing data post processing quality control are challenges and key components of sensor network cyberinfrastructure campbell et al 2013 gries et al 2014 jones et al 2015 as the quantity of sensor data increases there is a commensurate need for practices that ensure resultant data are of high quality for subsequent analyses and exploration campbell et al 2013 gibert et al 2016 in current practice quality control post processing of sensor data is expensive and tedious tools exist to assist practitioners and technicians in reviewing data and performing corrections gries et al 2014 horsburgh et al 2015 sheldon 2008 however quality control remains a time consuming and manual process consisting of an interactive sequence of steps performing corrections generally requires expert knowledge about the sensor and the phenomena being observed as well as conditions at the monitoring location fiebrich et al 2010 white et al 2010 furthermore the quality control process involves subjectivity as individual technicians may make different correction decisions jones et al 2018 as a result it is difficult to transfer the institutional knowledge required to post process data and even for trained and experienced technicians quality control remains a daunting task as datasets grow in size and complexity for environmental observatories with ongoing data collection for one network a substantial delay of approximately six months between data collection and availability of reviewed and processed datasets allowed for thorough review and correction jones et al 2017 for cases where observations are used for real time decisions related to public health and water treatment the impacts of anomalous data are costly as sensor datasets continue to grow it is not tenable for scientists and technicians to manually perform quality control tasks gibert et al 2018 neither is it advisable to use or publish data without performing corrections to mitigate for errors as a result there is a recognized need for automating and improving quality control post processing for high frequency in situ sensor data in this vein automated data driven techniques to detect anomalies in streaming sensor data are documented in the realm of research hill and minsker 2010 leigh et al 2018 russo et al 2020 talagala et al 2019 however they are unfamiliar to practitioners generally lack robust and accessible software implementations and are not typically reproducible furthermore while basic checks and more complex algorithms may identify and flag potentially erroneous values e g dereszynski and dietterich 2007 hill et al 2009 taylor and loescher 2013 these procedures are generally not capable of applying corrective actions thus the specific questions we pursued with this research are 1 how can data driven methods be applied to automatically detect and correct anomalies in aquatic sensor data and 2 how can these methods be packaged into an overall workflow and reusable software for general application regression models are one class of data driven techniques that can be used as anomaly detectors for time series data by making a prediction based on previous data either univariate or multivariate and comparing the residual of the modeled and observed values to a threshold because regression models produce an estimate they are well suited for detection and correction of anomalous data although it is a substantial step in quality control post processing automated anomaly correction has not been widely examined a handful of studies replaced raw data with modeled forecasts to exclude anomalies from model input but did not generate a corrected version of the dataset hill and minsker 2010 leigh et al 2018 in this work we implemented and compared several regression models for anomaly detection and explored new approaches for anomaly correction although effectively implemented for specific case studies none of the techniques described in the cited studies have been packaged as accessible software for broad application and dissemination without reusable code the specifics of the algorithms as implemented with environmental data cannot be examined further tested or applied to other datasets rather than a model calibrated to a specific variable site combination practitioners need tools that can be applied to a broad suite of variables and or monitoring locations documented in a reusable and reproducible way thus we sought to package the tools we developed as open source software that could easily be deployed in a commonly available analytical environment in this paper we present a python package pyhydroqc that implements a set of methods for data driven anomaly detection and correction for aquatic sensor data observed with high frequency in time our approach includes machine learning algorithms for detection labeling and correction of anomalous points multiple years of aquatic monitoring data from the logan river observatory lro that have been reviewed and corrected by trained technicians were used as a case study for developing and testing automated detection and correction methods the algorithms are encapsulated in a python package that is publicly available and open source see software and data availability section example scripts are also shared as jupyter notebooks that can be run with case study data to demonstrate the functionality and performance of the tools we developed as there are many potential approaches to anomaly detection additional techniques can be incorporated by adding new functions to the package that can be intrgrated into the workflow thus the specific contributions of this work include 1 advancing the algorithms and methods for automated quality control of aquatic sensor data and 2 developing and demonstrating software tools that can make the process more approachable for data technicians and scientists we anticipate that this work will be of interest to researchers practitioners and technicians that maintain environmental monitoring networks the python package can be used in any python environment and potential users should be familiar with scripting in python or a similar language but do not need specific training or expertise section 2 outlines the methods we implemented for detecting anomalies and performing corrections in the context of the structure and design of the pyhydroqc python package including a description of the case study that drove the implementation in section 3 we report the performance of the techniques on case study data and offer recommendations for next steps followed by conclusions in section 4 appendix a contains related background including an overview of relevant literature and additional motivation for the work reported 2 methods 2 1 pyhydroqc software design and implementation this work implements methods for anomaly detection and correction for environmental time series data within a python based software package a subset of data driven regression models are situated within an overall workflow that includes practical steps to facilitate anomaly detection and correction the following sections describe the approaches for anomaly detection and correction including details of how the software supports the workflow while many classes of algorithms could be used for detecting anomalies in aquatic sensor data we selected time series regression models that were relatively straightforward to implement and that we anticipate will meet the needs and considerations of many applications specifically we investigated auto regressive integrated moving average arima several types of long short term memory lstm and facebook prophet arima has been successfully implemented to detect anomalies in environmental data hill and minsker 2010 leigh et al 2018 papacharalampous et al 2019 lstm is a class of artificial neural networks anns and though applications to environmental data anomalies are limited studies from other fields have detected anomalies with lstm models hundman et al 2018 lindemann et al 2019 malhotra et al 2016 yin et al 2020 prophet was investigated but not included in the python package because prophet is geared toward social media and business applications taylor and letham 2018 we found that its applicability to environmental data is insufficient it failed to capture seasonal shifts in the timing of daily cycles and model features did not represent environmental phenomena this paper focuses on a subset of models but the modular design of the python package allows for the implementation of additional techniques the software design and development were driven by the following steps as a workflow for anomaly detection and correction fig 1 and each is described in more detail in the sections that follow 1 import raw sensor data into a memory resident data structure 2 perform rules based anomaly detection and correction as a first pass at quality control including addressing sensor calibration 3 build one or more models for predicting observed values a determine model hyperparameters b transform and scale data if necessary c build and fit models d execute the model to determine model predictions and residuals 4 post process model results a determine dynamic thresholds based on model residuals and user defined parameters b detect anomalies where the absolute value of the model residual exceeds the defined threshold c widen and index anomalous events 5 compare technician labeled and detected anomalous events rules based and model based detections inclusive to assign confusion matrix categories and report metrics this step is only applicable if labeled data are available 6 combine detections identified by multiple models for an aggregate anomaly detection if rules based detection has been performed those detections are included 7 perform model based correction for points identified as anomalous in addition to performing the workflow steps requirements that drove our design included 1 open source software development to facilitate deployment and use by others 2 cross platform compatibility for use on windows macos and linux platforms 3 modular and extensible architecture that enables each workflow step to be executed independently along with integration of new additional functionality and 4 simple deployment a python package was selected as the platform for software implementation the python language meets the open source and cross platform requirements and existing tools and libraries in python support steps in the workflow including loading and manipulating large datasets and developing data driven models in a python package functions that comprise each step in the workflow can be called by scripts in a modular manner each of the steps can be performed independently facilitating flexibility in use a python package also supports extensibility as new functions can be added without impacting existing functionality finally python packages can be published to the python package index pypi https pypi org making deployment straightforward and ensuring that algorithms can be applied in any python coding environment the anomaly detection and correction workflow steps are encapsulated by functions in the pyhydroqc python package described in the following sections high level workflow wrapper functions arima detect lstm univar detect and lstm multivar detect call more granular functions specific to each data and model type to perform steps 2 7 fig 1 and generate objects of the modelworkflow class for clarity each function is named and described in this paper however most users will use the overarching workflow function calls example python scripts and jupyter notebooks see software availability section illustrate how the workflow functions are implemented for the data use case described in this paper a full list of functions with inputs and outputs is found in appendix b and with the package documentation 2 1 1 data format and import pyhydroqc operates on pandas data frames which are high performance two dimensional tabular data structures for representing data in memory pandas development team 2008 data frames can be created and saved or output as comma separated values csv files for pyhydroqc to perform anomaly detection and correction input data need to be formatted as a data frame for each variable of interest indexed by date time with a column of raw data if technician labels or corrections are available for determining anomaly detection metrics they are included as additional columns in the data frame in general most date time formats reported by sensor systems will be interpreted by pandas as date time objects in the rare case that the date time format is not supported some pre processing may be required pyhydroqc also supports environmental sensor data formatted as one table or file with a single date time column and multiple columns of measurements one for each sensor output for flat files with this structure the pyhydroqc get data function wraps the read csv function from the pandas library to import data into python and parse into separate pandas data frames for each variable as required by the anomaly detection and correction functions 2 1 2 rules based detection and correction rules based detection is an important precursor to detection using models leigh et al 2018 taylor and loescher 2013 and the results of this step contribute to the overall set of detected anomalies whether a result of sensor failure or another cause some anomalies are low hanging fruit that can be detected by rules based preprocessing that performs a first pass of the data preprocessing the data is motivated in part by the need to train models on a dataset absent of extreme outliers or artifacts that models cannot capture by applying rules based anomaly detection and correction a first degree of correction is made for subsequent input into data driven models we created python functions to detect and correct out of range and persistent data furthermore some aquatic sensors commonly exhibit drift which requires sensor calibration and subsequent data correction because calibration shift and the preceding drift are subtle and difficult for any type of model to detect we developed a rules based routine that attempts to identify these events basic correction methods for these anomaly types were also implemented as python functions 2 1 2 1 range and persistence checks the function range check adds a column to the data frame and populates it with an anomalous label if the observation is outside of user defined thresholds or a valid label if it is within the thresholds ranges should be determined specific to each sensor based on physics and the environment in which the sensor is deployed and can be refined based on site specific patterns data persistence refers to the sensor reporting a repeated value which is unlikely in natural systems although sensors may report repeated values due to limitations in resolution for the persistence function the user defines a minimum duration of repeated values for data to be considered anomalous if repeated values exceed that duration the points are classified as anomalous by populating the column from the range check function beyond these basic checks additional rules of increasing complexity could be added to the pyhydroqc package and the anomaly detection workflow examples include ranges that vary seasonally rate of change checks and differencing checks once anomalous points are identified by the python functions that implement these rules labels are carried through to the model based detection steps labeled points are omitted from model training either by logical exclusion or for models requiring an unbroken time series for training by interpolating between valid points linear interpolation is performed using the interpolate function over the entire time series as a preliminary correction step so that model input is more valid if the complete workflow is followed values initially corrected using linear interpolation are replaced by the model based correction described in section 2 1 9 2 1 2 2 calibration and drift correction environmental sensors commonly drift and many aquatic sensors specific conductance ph dissolved oxygen require regular calibration to known standards drift causes a gradual increasing or decreasing trend separate from daily and seasonal patterns and a calibration event manifests as a localized shift that corrects subsequent data up or down these trends and shifts can be subtle and difficult to identify without a detailed record of calibration dates in preliminary work the model based detectors described in subsequent sections were unable to consistently identify these data patterns detected shifts due to calibration events were undiscernible from other localized anomalies thus it is important to address calibration events early in the quality control process because it is preferable that model based detectors be trained on data that are free from drift for calibration and drift correction we implemented functions to mimic a typical manual workflow performing post processing correction for drift and calibration involves review of data comparison of field records to data shifts to identify points corresponding to calibrations and application of a drift correction that uses start and end points and the gap of the calibration shift to retroactively correct data between two calibrations in our experience calibration events are typically reviewed and corrected one at a time while recognizing the difficulty of definitively identifying calibration events in an automated way we designed functions for detection functions calib edge detect calib detect calib overlap and correction functions find gap lin drift cor of data affected by drift and calibration the algorithms take advantage of characteristics of calibration events specifically that events only occur during certain hours of the day they may involve a shift in observed data and that when returned to the water sensors may report the same values for several time steps until the sensor stabilizes two separate approaches identify calibration events 1 where there is a discernible shift in the data or 2 persistence occurs over a limited window of points both are restricted to hours and days when technicians would be in the field given dates of calibration a gap value needs to be specified for correcting past data a function find gap identifies the greatest shift for a given window of time to determine a gap value and the precise point that should be shifted while accounting for outlier spikes commonly associated with calibrations a function for linear drift correction lin drift cor corrects for drift and calibration events given start and end dates and a gap value of the calibration shift while the calibration event detectors may not adequately identify events requiring technician review or input this process is a step toward automation as it evaluates gap values according to a set of rules rather than arbitrary determination by technicians as illustrated in jones et al 2018 and allows for bulk correction of calibration events 2 1 3 model based detection using arima arima is a time series forecasting model where inputs correspond to past time steps of the variable of interest and the output is a predicted value for that variable at the next time step arima uses three parameters to define a linear model equation 1 1 y t i 1 p φ i y t i ε t i 1 q θ i ε t i where y t is the model output or the prediction for time step t p is the number of previous points in the series to be used in the model q is the number of moving average terms to include φ i are the fitted coefficients for auto regression θ i are fitted model coefficients for the moving average and ε t is the moving average error term not shown in the equation is the term d which is the order of differencing applied to the data y before this equation is evaluated the parameters p d q can be determined manually or automatically manual parameter determination involves time series decomposition and the review of auto correlation plots which is tedious for numerous data series automatic determination of the parameters is effective but can be computationally demanding pyhydroqc includes a function pdq for automated determination using the pmdarima package smith 2017 given p d q model training involves determining the values of the coefficients for the terms in the linear equation φ i and θ i based on actual data in pyhydroqc the function build arima model constructs and trains an arima model given input time series data and input parameters p d q it relies on the sarimax function from the statsmodel package seabold and perktold 2010 to fit an arima model based on equation 1 make model predictions for each time step and compare predictions to observations input data should be free from gaps so the anomaly detection workflow uses output of the rules based detection with linear interpolation of any identified anomalies as input for arima modeling 2 1 4 model based detection using lstm lstm is a type of neural network model architecture specifically designed for time dependent and sequenced data lstm models consist of recurrent cells or units each corresponding to one time step a cell uses gates to control the flow of information in and out of the cell and how much of the past data that the cell remembers for computing output to train an lstm model the weights of the connections within and between the gates are iteratively refined based on training data there are many variations of lstm architecture greff et al 2017 for our implementation we compared several lstm model types that are appropriate to time series data modeling for anomaly detection vanilla and bidirectional univariate and multivariate in contrast with other neural network architectures for which many layers are advised for fitting data more shallow lstm have been used because of the internal complexity of lstm cells géron 2017 greff et al 2017 hundman et al 2018 other model types could be constructed model layers and complexity could be added and the input parameters could be tuned to each time series parameters can be defined by users and can be adjusted to investigate sensitivity and we describe our approach for parameter selection in section 3 1 4 the objective of this work was not to achieve the best time series model but rather to detect anomalies so fine tuning models was not required or pursued instead comparisons were made between a few basic lstm variations with the same parameter settings as mentioned pyhydroqc workflow functions call multiple lower level functions for lstm models each type is implemented within the workflow function by an associated model wrapper function lstm univar lstm multivar lstm univar bidir lstm multivar bidir which calls functions specific to that model type for preprocessing model building model training and model evaluation shown in fig 1 and described in the jupyter notebook example script the model wrappers return objects of the class lstmmodelcontainer containing model predictions and residuals for each time step similar to the output of build arima model the model wrapper functions also include an option for saving lstm models for future use which is important because lstm model training and development is stochastic resulting in a new model each time the algorithm is run we developed models for a particular sensor deployed to a certain location so the models are variable and location specific and can be reused for that data series after training 2 1 4 1 vanilla and bidirectional lstm pyhydroqc implements the vanilla type of lstm model greff et al 2017 which consists of a single layer lstm in a sequence to one manner i e the model returns a single output based on a sequence of inputs given a user specified number of past time steps the model output is a single value for the next point in time bidirectional lstm models use observations both before and after the point of interest to provide information for model prediction by encoding a vanilla lstm model with a bidirectional wrapper input data are traversed both forward and backward in sequence and model output is the value to have occurred in the middle of the sequence in pyhydroqc parallel functions structure input data to contain a user specified number of time steps prior to the point of interest for vanilla lstm and prior to and following the point of interest for bidirectional lstm functions further described in section 2 1 5 3 2 1 4 2 univariate and multivariate lstm either univariate or multivariate input data may be used for vanilla and bidirectional lstm through the lstm workflow functions and model wrapper functions the workflow functions lstm detect univar and lstm detect multivar prepare data and report results for univariate or multivariate data and call the associated model wrapper functions lstm univar and lstm univar bidir for univariate lstm multivar bidir and lstm multivar for multivariate for multivariate data the models use data for all observed variables as input and output estimates of the same variables for the point of interest model errors are examined for each variable and independent thresholds are set for anomaly detection 2 1 4 3 lstm preprocessing model building and training the functions for preprocessing model building and model training are compiled as sequenced steps in the lstm model wrapper functions fig 1 preprocessing for lstm models involves scaling reshaping and ensuring that training data are valid which is facilitated by using the output of the rules based detection data must be scaled so that extreme values do not have an outsized impact on the model and pyhydroqc includes a function for scaling create scaler based on the standardscaler function from the scikitlearn package which subtracts the mean and divides by the standard deviation to scale the data pedregosa et al 2011 reshaping data creates a sequence of immediately previous points i e model input for each data value i e model output pyhydroqc functions create sequenced dataset and create bidir sequenced dataset reshape data based on a user defined number of past time steps to build a model structure the pyhydroqc functions create vanilla model and create bidir model use the sequential model from the keras package keras development team n d with model layers lstm dense and bidirectional and the suite of user specified hyperparameters accepted by the sequential model to train the model the functions create training dataset and create bidir training dataset select a subset of data based on a user defined number of random points ensuring that none were identified as anomalous by the rules based detection these points are reshaped and used for training the lstm model the function train model uses the keras early stopping feature so that model training ceases when the error of the test and validation sets randomly selected by the algorithm are approximately equal 2 1 5 post processing dynamic threshold determination and anomaly detection a key component of model based anomaly detection using regression approaches is determination of the threshold that regulates whether a point is marked as anomalous or valid aquatic data vary seasonally daily and with environmental events changes that may not be adequately captured by a model a dynamic threshold has the potential to improve detection accuracy by applying a narrower range i e higher sensitivity when the model predictions are more precise and a wider range when model predictions are more variable in particular by using a dynamic threshold we hoped to identify localized outliers that are within the absolute expected range of values but are relatively distinct for a narrower time window and which were undetectable with a constant threshold pyhydroqc implements a dynamic threshold following the format of confidence intervals and prediction intervals used in other studies hundman et al 2018 leigh et al 2018 for each data point a threshold is determined based on a moving window of points equation 2 2 t μ z α 2 σ i f z α 2 σ m i n μ m i n o t h e r w i s e where t is the threshold μ is the mean of the user defined moving window model residuals σ is the standard deviation of the moving window model residuals α is a user defined value to adjust the width of the threshold z α 2 is the α 2 quantile of a normal distribution and min is a user defined parameter for the minimum threshold value note that min may be set to zero having no effect or to a non zero value to prevent too many false positives i e detections that are not anomalies this can occur when model residuals are low over an extended period and the dynamic threshold is smaller than the resolution or uncertainty inherent in the sensor given a time series of model residuals the set dynamic threshold function in pyhydroqc determines upper and lower thresholds for each point in a series using equation 2 with a user defined moving window the number of points used to calculate μ and σ the detect anomalies function then compares the dynamic threshold values to the residuals for each time step to determine whether a point is anomalous if rules based detection was performed the anomalies detected in that step are propagated through the workflow and are included in the detections output by this step 2 1 6 post processing anomaly events and widening in comparing anomalies identified by the model based detectors to anomalies labeled by technicians we observed mismatches related to resolution and lags in model approximations related to model smoothing when an anomaly is identified either the technician or the algorithm must determine how many points to label to address this in a systematic way pyhydroqc generalizes anomalies into numbered events consisting of groups of anomalous points by widening the detection window to include points before and after anomalies detected by the algorithm as well as points labeled by the technician overlap between the two is more likely in pyhydroqc the anomaly events function groups contiguous anomalous points as events by adding a column to the data frame with incrementing numbers as an index for each anomalous event to perform widening for each anomalous event the function assigns the event s index to points before and after the event the number of points is user defined effectively adding those points to the event 2 1 7 performance metrics for data with technician labels the function compare events determines valid and invalid detections by comparing events detected by the algorithm to those labeled by the technician each point is classified as true positive true negative false positive or false negative when there is any overlap between detected events and labeled events i e any portion of a labeled event is detected all points are classed as true positives to indicate that the labeled event was detected for accuracy the points assigned as anomalous on the edges of events by widening are removed from the event as part of this step a confusion matrix compares model classifications to actual data to evaluate overall performance by reporting total true positives true negatives false positives and false negatives leigh et al 2018 tan et al 2019 additional metrics that are commonly reported include positive predictive value precision negative predictive value accuracy recall and f scores li et al 2017 in pyhydroqc the function metrics determines the performance metric outputs in table 1 as aggregates of precision and recall f scores combine true positives false positives and false negatives into a single assessment score to assess models cook et al 2020 the f1 score gives equal weight to false positives and false negatives while the f2 score gives greater weight to false negatives f scores range from 0 to 1 with 1 being the upper bound because anomalies are sparse relative to the total number of data points the datasets are considered imbalanced chandola et al 2009 counts of true negatives are overwhelming resulting in high accuracy which may make it difficult to compare between models tan et al 2019 as a result anomaly detection focuses on true positives false positives and false negatives anomaly detection requires a balance between increasing true positives while reducing both false negatives and false positives objectives that may be mutually exclusive and depend on model sensitivity our preferred approach is to err on the side of sensitivity in the detector to minimize false negatives along with maximizing true positives even at the expense of increased false positives the f2 score supports this aim by more heavily weighting false negatives while the f1 score equally weights true negatives and false negatives cook et al 2020 2 1 8 aggregate detections in applying multiple models rather than select the single best performing model a robust approach is to aggregate results so that a point identified by any of the models as anomalous is considered a detection to address this pyhydroqc includes a function aggregate results for combining anomalies detected by the different model types into a single column of detected anomalies the outcome of aggregation is that a point is classed as anomalous if it was detected by any of the considered models when a point is identified as anomalous by either rules based detection or by any of the models it is denoted with a boolean in columns of output data frames one that corresponds to each model so the source of the anomaly may be traced through the process because rules based detections are propagated through the workflow and are present in the detections associated with each model the aggregation automatically includes the rules based detections 2 1 9 model based correction a primary goal of this work was to suggest corrections for anomalous points which is enabled by using time series regression methods for anomaly detection while the model predictions used to determine anomalies could be simply substituted as corrections the prevalence of consecutive anomalous points means that anomalous points would be used to determine corrections to prevent this correction models were implemented at a more granular scale a function generate corrections was developed that implements piecewise arima models using the following steps 1 given a data frame of observations with anomalies detected assign consecutive points with either valid or anomalous labels to alternating groups the function group bools adds a column populated with 0 for valid points and assigns each anomalous event a unique integer 2 ensure that sets of valid data points are large enough to generate forecast predictions where valid data points are in between anomalous points and the duration is too small to use as model input the function arima group merges them with previous and subsequent anomalous points into one anomalous group by resetting the group s incrementing index 3 for each anomalous group beginning with the group of shortest duration and progressing in order of increasing duration develop 2 arima models one based on the preceding valid points and one based on subsequent valid points using a specified maximum number of points for model development use the piecewise models to make forecasts and backcasts and blend them using the function xfade to get a single correction estimate for each point in the anomalous group 4 in the data frame populate a new column with the correction estimates for points in anomalous groups and with the observations for the points in valid groups to blend the forecast and backcast the values are weighted according to the proximity to each end point of the anomalous event as shown in equation 3 which is encoded in the function xfade 3 y k a k n k n 1 b k k 1 n 1 where y k is the correction estimate for each time step k in the anomalous group n is the total number of data points in the anomalous group to be corrected k 0 n 1 and a k and b k are the arima forecasted and backcasted values respectively examples in section 3 4 illustrate this concept because the arima correction is based on points immediately proximate instead of using the hyperparameters and model generated for the dataset as a whole each forecast and backcast is an individual arima model with hyperparameters and model fit based on the window of valid data using more granular models allows models to be tuned to that local time window and helps prevent errors that might arise from not having enough valid data points to estimate a point e g if p 9 for the time series as a whole at least 9 valid data points are required to avoid overfitting and to conserve computational resources the generate corrections function includes a user defined limit on the duration of data used to develop and train piecewise models to generate the forecasts and backcasts instead of applying corrections sequentially the correction function first corrects the events of shortest length and then corrects events of increasing duration in this manner corrected estimates are available as model inputs when needed for correcting longer events this helps ensure that the period of valid data before or after an anomalous event is sufficient to capture patterns 2 2 experimental use case logan river observatory data the primary objective of this work was to advance automation of quality control post processing specifically for environmental sensor data as an extensive test case we used data collected within the lro where high frequency monitoring is conducted at several climate and aquatic sites within the logan river watershed in northern utah usa http lro usu edu neilson et al 2021 monitoring sites were established and infrastructure was originally deployed using protocols described by jones et al 2017 the lro is similar to many research sites throughout the world where in situ monitoring of aquatic climatic and terrestrial variables is performed in support of research activities utah state university manages the monitoring network including site maintenance and data dissemination available at http lrodata usu edu the upper logan river watershed consists of mountainous forest and rangeland with limited development while the lower watershed is agricultural and urban with multiple agricultural diversions hydrology is generally driven by snowmelt and the upper watershed is characterized by karst topography aquatic monitoring sites are located in both the upper mountain canyon and lower urban agricultural sections and include sensors for water level water temperature ph dissolved oxygen specific conductance and turbidity fig 2 raw sensor observations are recorded on field dataloggers streamed to a central base station and loaded to an operational database jones et al 2015 technicians perform quality control post processing on collected data using a suite of interactive tools to generate a quality controlled copy of data horsburgh et al 2015 in this process technicians review data and consult with the record of field activities to identify label and correct anomalous points or events in the data lro data exhibit a number of anomaly types including outliers artificial persistence drift and others described by horsburgh et al 2015 currently post processing consumes approximately half of a full time technician s time with additional support from hourly assistants we sought to move toward automated methods to reduce the time and resources required to perform quality control post processing to test pyhydroqc we used data from the six aquatic sites shown in fig 2 for four variables common to aquatic monitoring and measured at all lro sites temperature ph specific conductance and dissolved oxygen most of the sites include over 6 years of data at 15 min intervals with few to no gaps for both raw and labeled corrected data to assess performance we used the metrics implemented in pyhydroqc to compare automated anomaly detection with the manual results produced by technicians lro sensor data were exported from a relational database observations data model horsburgh et al 2008 to flat csv files corresponding to each site indexed by a single date time column with columns for the measurements output by each aquatic sensor the pyhydroqc get data function was used to read the csv files into individual pandas data frames for subsequent analyses testing the software against case study data was performed on a 2017 macbook pro laptop with 16 gb ram and a 3 1 ghz quad core intel core i7 processor 3 results and discussion 3 1 preprocessing and settings the following subsections present the parameters configuration and settings used by each anomaly detection and correction procedure anomalies detected by the combination of rules range and persistence and models with thresholds arima and lstm are reported together in section 3 3 3 1 1 rules based detection and correction range and persistence checks for the lro data range thresholds were determined specific to each sensor based on manufacturer reported ranges and were further refined according to past observations at each site table c1 the maximum allowable persistence durations were also based on review of raw observations and varied with sensor initially persistence durations were set lower 5 10 time steps however those durations resulted in many false positives as sensors regularly reported repeated values for more than 10 time steps we observed that repeated values are often caused by limitations in sensor resolution so persistence durations were increased 30 45 time steps table c1 anomalies detected by these functions retained labels through subsequent steps so the metrics resulting from rules based detection are reported with the overall anomaly detection results in section 3 3 anomalies detected by the range and persistence checks were initially corrected by linear interpolation which is identical to the lro protocol used by technicians to manually correct over short periods however in the pyhydroqc anomaly detection and correction workflow the linear interpolation correction is an intermediate step to facilitate more accurate model development these points retain an anomalous label through subsequent steps of the workflow and are eventually corrected using the model correction algorithm consequently the final correction is performed by the model overwriting the interpolated points in the final corrected dataset 3 1 2 rules based detection and correction calibration and drift correction results from the calibration detection algorithms were compared to calibration events identified and corrected by technicians for all sensors at one site main street the persistence functions calib detect and calib overlap identified about 25 of the calibration events with a high false positive rate 5x the persistence we observed following a calibration may be specific to the sensors used in the lro ysi multiparameter sondes and not broadly applicable the edge detection function calib edge detect identified about 40 of calibrations for ph but was less successful 10 for specific conductance and dissolved oxygen additional effort could be applied to improve calibration event detection and to refine the parameters of the edge detection function threshold and width in theory the model algorithms should identify these local shifts as anomalies however although the observed values may deviate from the modeled the residuals were often within the dynamic thresholds as defined in table c1 and so were not detected as anomalies adjusting threshold settings may identify more calibration events but cause oversensitivity furthermore the corrective action required for calibration events is different from that of other anomaly types so the detection step should be separate although calibration events were not automatically detected with high accuracy the function for finding gap values was effective at determining valid gap values and end times for calibration shifts in a review of the results of the find gap function out of 100 distinct calibrations the total for all variables at main street revision was made for only 6 instances with calibration dates and gap values as inputs the function for linear drift correction was executed for all calibrated sensors specific conductance ph dissolved oxygen for the main street site many of the automatically determined gap values approximated the values used by the technician for correction in which case the linear drift correction was comparable to the technician correction some automatically determined values were judged as preferable to the technician selected gap value e g fig 3 in our experience selecting a viable gap value and performing drift correction can be the most time consuming aspect of manual quality control so although the algorithms we designed were not successful in identifying a majority of calibration events technicians typically record the dates of calibration and automatically determining the gap value and performing drift correction in batch is a significant improvement furthermore using an algorithm for this step increases consistency the range of gap values selected by multiple technicians was the primary source of quality control subjectivity identified by jones et al 2018 based on our testing using the lro data our recommended workflow for addressing drift and calibration events is to 1 identify a list of calibration dates generally from field notes although the pyhydroqc functions may be useful 2 determine gap values and associated times using the find gap function 3 review those shifts and make any adjustments and 4 use the dates and gap values as inputs to the linear drift correction function code for performing these steps including generating plots of gap values for review are demonstrated in example notebooks 3 1 3 model based detection and correction threshold determination the dynamic threshold used to evaluate differences between simulated and observed values directly impacts which observations are detected as anomalous or valid for the lro data we used trial and error to settle on window sizes alpha values and minimum range values for determining thresholds table c1 the same threshold settings were used for all model types we found that moving windows longer than a single day resulted in too much smoothing to the threshold and introduced artifacts due to daily patterns in model residuals in general window sizes of 5 10 h corresponding to 20 40 time steps were selected to balance between over smoothing of longer windows and highly dynamic thresholds of shorter windows an added benefit of smaller window sizes is that fewer computational resources are required to determine thresholds relatively small alpha values were selected 0 001 0 00001 to create a sufficiently high threshold range with larger alpha values the narrow threshold range was overly sensitive resulting in too many false positives minimum values were similar for all sensors across sites with a few exceptions as illustrated in fig 4 the pattern of spread in thresholds tracks with the variability in model residuals and residuals that exceed the threshold are detected anomalies 3 1 4 model based detection and correction model parameters and settings to create arima models p d q were determined for each lro data series over the full duration of data using the pdq function table c1 to build compile and train lstm models consistent parameters and settings were used for all of the lro data series and the several varieties of lstm models table c2 default settings and commonly used parameters géron 2017 keras development team n d were selected with minimal tuning to achieve the goal of satisfactory rather than perfect models models were trained with a randomized subset of 20 000 data points from each data series corresponding to approximately 10 of the dataset this number of data points was determined to be robust given that increasing the number of training points did not change the overall results and training with 20 000 data points was less resource intensive compared with larger training sets anomalous events in both technician labeled data and model detected data were widened by a single point widening factor 1 this setting was used for all data series and all model types 3 2 anomaly detection example examples help demonstrate the performance of the workflow for both successful and unsuccessful anomaly detection fig 5 additional examples in appendix d on 2018 11 11 the arima model detected an event that was not labeled by the technician false positive although this is a false positive the model with a dynamic threshold behaved as designed in detecting a localized outlier the events on 2018 11 12 and 2018 11 13 consist of points both detected by the algorithm and labeled by the technician true positive not all points labeled by the technician were detected as anomalies by the model however performing widening and considering the overlapping sets of points as anomalous events resulted in true positives for all of these points the event on 2018 11 14 was not detected by the algorithm but was labeled by the technician false negative there is nothing in the original data to indicate that something was amiss so it is unclear why the points were labeled as anomalous by the technician the technician has expert knowledge or is following protocol that the algorithm is unable to discern in assessing algorithm performance we defer to technician labels as a benchmark however the quality control process is subjective jones et al 2018 and data are not perfectly labeled making reliance on technician labels as a gold standard problematic russo et al 2020 in the lro data we identified numerous cases where it was unclear why some data points were labeled and others were not see appendix d which may be due to multiple technicians and evolving protocols among other reasons 3 3 combined anomaly detection results the f2 scores for all time series table 2 combine true positives false positives and false negatives to indicate overall performance for each model type rules based detection and an aggregate of all models higher scores indicate better model performance f2 1 would be a perfect score fig 6 is a visual illustration of the confusion matrix where each panel corresponds to a time series and each bar to a model type the bottom portion of each bar light blue represents true positives the middle portion orange represents false negatives and the sum of those is equivalent to all technician labeled points the top portion of each bar purple represents false positives the dashed lines distinguish the proportion of anomalies identified by rules based detection true positives below the lower dashed line black were detected by rules while those above it were only detected by models likewise false positives below the upper dashed line gray were detected by rules and false positives above it were detected by only models anomalies detected by rules those below each line may have also been detected by models so there may be overlap the results illustrate some general trends regarding the performance of both rules based and model based detection 3 3 1 detections due to rules and threshold settings for several time series the rules based algorithm accounts for the majority of anomaly true positive detections e g temperature at several sites dissolved oxygen at franklin basin in these cases the model detection did not provide many additional detections in other cases e g temperature at tony grove all ph time series most specific conductance and dissolved oxygen time series the true positives are split between rules based and model based indicating that the models captured anomalous events that the rules based detection missed this demonstrates the value of using both approaches in tandem in some cases the success of the model s in detecting anomalies true positives is offset by a large number of false positives particularly high counts of false positives indicate oversensitivity due to either persistence durations that are too short or to thresholds that are too tight both of which may result in too many detections in particular dissolved oxygen at franklin basin and mendon and specific conductance at blacksmith fork exhibit high rates of false positives given that most are under the rules based line the false positives are attributable to oversensitivity in rules range check or persistence duration rather than inadequate threshold settings the similar rates of false positives between models for many time series indicates that using the same threshold settings for all model types is acceptable cases with a large portion of false negatives undetected anomalies across models indicate that the models were not sensitive enough e g temperature at main street and blacksmith fork better detection might occur with tighter thresholds or adjusted rules based settings practitioners need to consider the tradeoffs with model sensitivity in determining threshold settings under the assumption that anomalies identified by the algorithm would be further reviewed by a technician the thresholds can be set to capture more potential anomalies erring on the side of false positives however sensitivity must be balanced to avoid excessive false positives from narrow thresholds 3 3 2 model comparison the detections between all models were generally comparable e g temperature at most sites ph at most sites dissolved oxygen at several sites although for a few time series there were distinct variations in results between models e g specific conductance at franklin basin and tony grove dissolved oxygen at tony grove arima models gave the best average f2 score table 2 they generally outperformed lstm models for the cases with differences in model performance and were often slightly better than the lstm models for the time series with comparable results arima was generally more sensitive detecting more true positives than the lstm models at the expense of detecting more false positives results from the lstm models varied without a discernible pattern in one case the univariate bidirectional model excelled temperature at main street while in other cases the multivariate vanilla was preferred specific conductance at franklin basin and tony grove dissolved oxygen at the water lab for some of these series the more successful models detected a few points that were part of long events that were labeled anomalous by technicians which improved the performance of those model types although the multivariate and bidirectional models include more information in their input either with additional variables or additional points in time these models did not broadly outperform their simpler counterparts with regard to the multivariate model while there is some physical relationship between the variables of interest the lstm model has no explicit physical drivers we might expect that the behavior of other variables related to diurnal cycle or ambient events to offer some predictive information however most types of anomalies occur for a single variable independent of other sensors some conditions such as sediment or ice built up around a multi parameter sensing sonde or an issue with power to the sensors may impact more than one sensor simultaneously in these cases there is value in noticing concurrent anomalies in multiple sensors because not all studies measure all variables single sensor algorithms are more versatile and can perform well as shown in these results there may be systems in which variables are so physically related that a more directly dependent relationship model can be more reliable differences in anomaly detection between the model types could be due to several factors arima and lstm models have inherently different structures with distinct processes for hyperparameter tuning and model training arima models use a limited number of hyperparameters three which were tuned by automated optimization while lstm models include several hyperparameters for which minimal tuning was performed it is possible that lstm models could be improved with additional tuning however the process may not be worth the effort given that the objective of modeling was to detect anomalies rather than generate a perfect model as one example we observed lstm models consistently biased toward the overall time series mean which was reduced when developed with input sequences containing fewer previous data points 5 versus 10 another possible explanation for the poorer performance of lstm models is a result of the training process lstm models were trained on a randomized subset of available data due to the stochastic nature of training data selection and initialization of weights a new model is developed each time the algorithm is run although pyhydroqc can save models for future use if a distinct set of training data was used or learning converges to a local minimum it may cause the seemingly arbitrary failure of some lstm models on certain time series to test this lstm models were regenerated the resulting metrics were similar to those reported in table 2 this indicates that the size of the training sets is sufficient so that the strength of the model does not depend on the specific randomized subset of data used for training independently developing and training multiple models on the same time series is a straightforward check for training data robustness although we tested across a range of sites that span elevation land use and hydrologic regime within the lro these locations do not represent the full spectrum of sites across the world investigating the suitability of the algorithm to additional physical settings is an important next step more directly examining the performance of each model type related to physical characteristics of locations may help inform transferability of the techniques 3 3 3 model aggregation the comparability of most of the results suggests that using any one of the models may be acceptable however rather than select a single model aggregating detections by the multiple models may improve results f2 scores of aggregated anomaly detection table 2 indicate overall good performance for most time series f2 0 8 also illustrated by confusion matrix plots fig 7 for some time series the aggregation does not add high value presumably because the same points were detected by multiple models however for a few time series in particular aggregating detections of multiple models had a synergistic effect such that the aggregate f2 score is higher than that of any single model e g temperature at main street dissolved oxygen at tony grove lower f2 scores 0 8 that persist after aggregating model detections are a result of either high rates of false positives dissolved oxygen at franklin basin or false negatives temperature at blacksmith fork both of which could be addressed by tuning rules and threshold settings as described rather than perfecting models the results affirm that time series regression methods with dynamic thresholds and widening are an effective tool for automating anomaly detection and correction and implementing these techniques can streamline the quality control process without the models a technician would need to review 200 000 data points for each of the time series used in this case study by using the pyhydroqc anomaly detection workflow the number of data points for review referring to combined rules and model detections is reduced by at least an order of magnitude e g 20 000 for ph at franklin basin even for cases with high rates of false positives e g 4000 for dissolved oxygen at franklin basin 3 4 model based correction examples the model based anomaly correction implemented in pyhydroqc generally resulted in smooth data profiles without outstanding nonlinearities fig 8 the method offers a viable path for correcting many anomalous events although results varied depending on the duration the variable the season and the reliability of anomaly detection for shorter durations e g approximately 2 h fig 8a the model corrected data are similar to the technician correction i e linear interpolation for longer periods the blended forecasts and backcasts can estimate patterns diurnal cycles fig 8b and c that would not be practical for a technician to approximate in these cases technicians did not attempt corrections but set data to a no data value 9999 in other cases the model did not capture data patterns particularly for extended periods see appendix d for examples some models overgeneralized and missed patterns while others focused on a single dominant feature overall the correction algorithm better captured diurnal patterns in temperature and ph data while regular patterns in specific conductance and dissolved oxygen were less consistently approximated 3 5 combined correction results quantifying the overall performance of the correction algorithm for each time series is impractical because no gold standard exists for comparison algorithm corrected data cannot be quantitatively compared to technician corrected data because the technician corrected data are subjective contain correction and labeling errors and include many periods where the values were set to a designated no data value e g 9999 for the lro for correcting lro data technicians followed one of the following paths 1 linear interpolation for periods less than 4 h or 2 setting values to 9999 for longer periods where interpolation was deemed unreasonable technicians also performed linear drift correction between identified calibration events the model based correction algorithm is not designed to correct for drift which was performed as part of the rules based steps section 3 1 2 without a benchmark correction algorithm performance cannot be definitively measured for each time series leaving evaluation to be done qualitatively on a case by case basis section 3 4 and appendix d we considered simulating artificially introduced anomalies which are then corrected and compared to valid raw data however it is unclear what frequency and duration of artificial anomalies would be appropriate and how to propagate artificial anomalies through multiple concurrently measured variables i e in the case of multivariate models we determined that analysis to be outside the scope of this work in an attempt to assess the value of the correction algorithm in terms of relative accuracy we considered the total number of points in each series that were altered from the raw data by the technician or the algorithm and that were set to values outside of a valid range table 3 ranges specific to each time series were adopted from the range checks in rules based preprocessing table c1 to determine whether altered points were valid technician corrections resulting in invalid values generally correspond to data changed to the no data value of 9999 causes of invalid values produced by the correction algorithm may include periods where anomaly detection was not adequately inclusive so the points corrected by the algorithm were overly influenced by anomalous points that were not labeled as such figure c5 in another scenario anomalous data may be close to the range limits resulting in forecasts backcasts and corrections outside of the valid range e g the estimations of peaks in fig 8b exceed the upper limit for that time series for most cases the algorithm correction resulted in significantly fewer invalid values than the technician correction for 16 out of 24 time series most of the temperature specific conductance and ph series the number of invalid points produced by the algorithm correction was less than 100 out of 200 000 total points while the number of invalid points produced by the technician was significantly higher ranging from 22 to 8541 for five of the time series primarily dissolved oxygen the algorithm correction resulted in a higher number of invalid values for some of these series the anomaly detection was also less performant e g dissolved oxygen at franklin basin tony grove and mendon fig 7 these results highlight the need to review anomaly detections and refine settings to improve anomaly detection although the corrections classed as valid were within an acceptable range for that time series the correction may not have approximated observed data patterns so review of proposed algorithm corrections is necessary the overarching benefit of the correction in pyhydroqc is that the algorithm may capture diurnal patterns to suggest values that a technician could not estimate however anomalous events need review prior to correction as do correction suggestions adjacent data may be inadequate to generate correction estimates for the full duration of an anomalous event a more complete workflow could offer correction options for each anomalous event for review and selection by a technician 4 conclusions we developed a new python package pyhydroqc that enables application of rules based and time series regression techniques coupled with dynamic thresholds as part of a workflow to detect and correct anomalies in aquatic sensor data functions to implement the models and supporting steps in the workflow are contained in the python package and documented within the github repository available functions include rules based anomaly detection calibration detection and drift correction model development and estimation threshold determination anomaly detection and widening performance metrics reporting and model based correction although this workflow advances the automation of sensor data post processing and can be implemented in any python environment a python package and scripts may not be intuitive tools for some technicians a graphical user interface offering more interactive review could be built on top of the underlying functionality contained in pyhydroqc another potential next step is investigating execution of the software on an edge computing device to streamline the process and reduce reliance on centralized systems we tested the methods on 24 time series of aquatic data from the logan river in northern utah the case study sites varied in physical characteristics and spanned 5 6 years of high frequency data based on our case study the anomaly detection workflow enabled by pyhydroqc was successful with high detection rates arima models were most performant likely due to differences in model structure and development rather than using constant thresholds dynamic thresholds allowed for responsiveness to data variability adjusting threshold settings impacts the sensitivity of model detection we suggest erring on the side of oversensitivity and then reviewing detections a correction algorithm used blended forecasts and backcasts of local models to make correction estimates that follow data patterns for events of up to several days for some observed variables these approximations surpass a technician s ability to correct anomalous data but each corrected event needs review a rules based approach was successful in determining calibration gap values and performing linear drift correction with calibration dates as input though not completely automated this work helps to streamline the process of quality control related to sensor drift and calibration beyond the case study data applying the techniques to datasets from other locations and environmental variables outside of the aquatic domain is an important next step manual detection and correction performed by technicians is an extended process that overlaps with other tasks to perform quality control for 3 6 month durations of a single time series takes multiple days of dedicated effort in comparison implementing the complete pyhydroqc workflow for anomaly detection and correction for all variables at a single site for a single year of data takes a few hours to run in the background on a personal computer a technician will still need to review results however we submit that the package and workflow offer significant resource savings throughout this process the technician was treated as an oracle with technician labels dictating algorithm performance the subjectivity inherent in manual quality control and uneven application of labels by technicians highlight the need for improving consistency in quality control which is an important driver of automating post processing given that computers are not subjective in their decisions as the volume of environmental sensor data continues to increase so does the need for performing post processing quality control this work contributes tools and approaches that can be used to streamline and automate the quality control process to reduce the costs of manual quality control facilitate a post processing workflow that is reproducible defensible and consistent and provide reliable data for analysis and decision making declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this research was primarily funded by the united states national science foundation under grant number 1931297 any opinions findings and conclusions or recommendations expressed are those of the authors and do not necessarily reflect the views of the national science foundation additional funding support was provided by the utah water research laboratory at utah state university ongoing data collection and management in the lro is supported by funding from the utah state legislature administered by the utah division of water resources additional funding for the lro has been provided by utah state university logan city and the cache water district we gratefully acknowledge the work of the many technicians and students who have participated in maintaining and operating the lro instrumentation along with producing the quality controlled sensor data that were used in testing the software we developed appendix a background manual post processing by a technician remains the most commonly implemented approach for correcting anomalies in environmental sensor data software tools have been developed to assist technicians in performing quality control wherein anomalies are identified visually or using filters or rules that are implemented based on user input horsburgh et al 2015 sheldon 2008 while initially straightforward to implement manual post processing is resource intensive requires significant expertise and may be implemented unevenly within and between sensor networks additionally manual approaches may not be reproducible making it difficult to track the provenance of data from raw measurements to quality controlled products data driven anomaly detection has the potential to address the deficiencies of manual post processing by streamlining and standardizing the workflow numerous data driven approaches have been documented for anomaly detection chandola et al 2009 cook et al 2020 tan et al 2019 basic approaches use rules to test data plausibility e g range and variability checks taylor and loescher 2013 and even studies with complex workflows initially implement rules based approaches e g leigh et al 2018 statistical approaches rely on the distribution of data to identify points outside of the expectation cook et al 2020 regression approaches estimate a value and compare it to the observation chandola et al 2009 feature based approaches apply numerous variables or features within one or more machine learning methods to determine if the data point should be grouped with valid or anomalous points talagala et al 2019 in approaching data driven methods for anomaly detection important considerations include data extent what duration of data are available some methods require data partitioned into separate groups for training and testing models data labels do sufficient data exist in which anomalies have been identified by an expert the availability of labeled data impacts which types of models can be used supervised model types require labeled data for training while unsupervised model types do not for all model types labeled data enable assessment of performance data quality do sufficient data exist in which anomalies have been corrected some methods require clean data that are free from anomalies for training models variables what variables are to be considered is a single variable sensor observed or are multiple variables measured do sensors at nearby sites provide additional information anomaly types what types of anomalies are of particular concern can rules based detection effectively detect some of these cases online offline detection does detection need to occur in real time online or is a retrospective offline approach acceptable in the following sections we provide a brief description of several approaches and methods for detecting and correcting anomalies in environmental sensor data we also illustrate gaps in the current state of practice for anomaly detection and correction in the quality control process a 1 data redundancy approaches various types of data redundancy including sensors people and models are used to detect anomalies in environmental sensor data the gold standard world meteorological organization 2008 mourad and bertrand krajewski 2002 compares data from multiple sensors requiring at least three sensors to determine which observation is erroneous increased cost maintenance power and data storage requirements challenge observational networks to implement redundant sensors furthermore multiple sensors may all exhibit the vagaries of environmental events sensor malfunctions and infrastructure failures complicating assessment and correction of data quality to improve the consistency of quality control jones et al 2018 suggest another form of data redundancy in which multiple technicians collaborate to review and correct data finally data redundancy may be achieved by modeling expected values for comparison with sensor measurements a physically based model could be used however model availability and uncertainty are barriers moatar et al 2001 given the relative simplicity of implementation ability to scale to large volumes of data few input requirements and potential for fast performance statistical and data driven techniques may be more appropriate thus we examined several classes of data driven techniques to model expected sensor behavior as data redundancy approaches a 2 univariate or multivariate approaches some predictive time series models are based on data from a single sensor independent of the condition of other co located sensors or data advantages of these univariate methods are that processing can be performed on multiple sensors independently and simultaneously and gaps or errors in data from one sensor will not impact data from other sensors hill and minsker 2010 however anomalies in one sensor stream may correspond to anomalies in a related sensor so approaches that utilize the information from multiple sensors provide multiple lines of evidence toward anomaly detection li et al 2017 furthermore when performing quality control post processing technicians regularly consult the record of other variables simultaneously recorded at the same site to check for internal consistency campbell et al 2013 and to inform corrective actions there is no clear best approach and even the same authors simultaneously promote a univariate detector hill and minsker 2010 and a multivariate approach hill et al 2009 either method may yield acceptable results although leigh et al 2018 report poor performance for multivariate time series regression compared to univariate the data in question will drive whether a univariate method is required or if additional power could be achieved with multiple variables in our work we considered both univariate and multivariate approaches and compared the benefits and drawbacks related to the data we examined a 3 spatial dependency external consistency refers to comparison with data from other locations campbell et al 2013 and some data driven approaches are based on relationships between sites in particular spatial dependencies between weather sensors have been used to identify anomalies galarus et al 2012 in another application data driven models used weighted data from neighboring stream monitoring sites to infill daily mean flow records giustarini et al 2016 one study included data at an upstream site offset by estimated travel time to detect anomalies in aquatic data conde 2011 spatial methods assume high correlation for a particular variable at sites having similar characteristics which may not be clearly established for the data of interest in this work we focused models on data at a single site of interest so that detection and correction could be applied to sites independently a 4 regression approaches regression models are a class of data driven anomaly detectors for time series that predict the next anticipated value based on previous data either univariate or multivariate to detect anomalies with regression the modeled value is compared to the observed and a range of acceptability is determined for the residuals such that points outside of that range are classed as anomalous and vice versa constant acceptability thresholds may be based on a user defined range or determined as a prediction interval based on the model results leigh et al 2018 thresholds may also be dynamic varying based on the range of the model residuals hundman et al 2018 for example in one study dereszynski and dietterich 2007 the threshold range for an observation varied based on the modeled state of the sensor i e a narrower range when the sensor was classed as good versus bad auto regressive integrated moving average arima is a regression technique that uses a combination of past data to forecast the next point arima has been successfully implemented to predict environmental data and subsequently detect anomalies hill and minsker 2010 leigh et al 2018 papacharalampous et al 2019 another regression technique based on a previous sequence of data is long short term memory lstm a class of artificial neural networks anns though applications to environmental data anomalies to date are limited lstm models have been used to reconstruct time series to detect anomalies in other fields hundman et al 2018 lindemann et al 2019 malhotra et al 2016 yin et al 2020 and other ann model types have been used for environmental anomaly detection hill and minsker 2010 russo et al 2020 other algorithms that show promise for time series regression include prophet a time series forecasting method developed by facebook with focus on business applications taylor and letham 2018 and hierarchical temporal memory htm ahmad et al 2017 another method that has been implemented for anomaly detection in environmental sensor data is dynamic bayesian networks which predict values in a time series based on assigned model states corresponding to temporal windows studies developed models based on a few previous points hill et al 2009 thousands of previous points hill and minsker 2006 and multiple past years of data to give an output based on the day of year and hour of day dereszynski and dietterich 2007 these models assume that temporal states can be definitively assigned as well as consistently applied and we did not attempt them due to complexity and obscurity of implementation because regression models produce an estimate they are well suited for both detection and correction of anomalous data the time series regression models we investigated were arima lstm and facebook prophet while arima has been commonly attempted for anomaly detection in time series data other techniques are emergent in this field e g lstm and there are few examples comparing multiple regression techniques for aquatic sensor data a 5 feature based approaches feature based methods comprise another class of anomaly detectors commonly used for discrete data tan et al 2019 which some authors have applied to environmental time series leigh et al 2018 russo et al 2020 talagala et al 2019 unlike regression methods feature based methods do not make a prediction of the observation anomalies are detected either based on a supervised model trained to data labels anomalous or valid russo et al 2020 or an unsupervised model that determines the likelihood of the point being anomalous based on distance to neighboring points these methods rely on multiple variables as model input features which in the case of aquatic sensor time series may correspond to variables measured concurrently by adjacent sensors past values of the variable of interest or transformations of the relationships between these variables particularly for data with temporal correlation it is not obvious which features should be selected and complex feature engineering may be required christ et al 2018 another challenge is selecting an appropriate data transformation a preprocessing step e g taking the first derivative of the data to highlight outlying points leigh et al 2018 talagala et al 2019 almost any feature based machine learning method may be applied to anomaly detection problems and approaches described in the literature include principal components analysis support vector machines tran et al 2019 hdoutliers leigh et al 2018 k nearest neighbor russo et al 2020 talagala et al 2019 clustering hill and minsker 2010 random forest russo et al 2020 xgboost and isolated forest smolyakov et al 2019 the success of feature based techniques in detecting anomalies from environmental sensor data is mixed hill and minsker 2010 leigh et al 2018 russo et al 2020 as they do not make predictions feature based approaches are not well suited to performing corrections given that our objectives were to both detect and correct anomalies we did not pursue feature based approaches in the work reported here a 6 anomaly types in most of the studies cited here the emphasis is on anomalies that are outliers where the value of the variable is outside of expected ranges or rates of change detection of gradual bias that may occur due to drift in the sensor or ongoing fouling has not been successfully reported the models implemented by dereszynski and dietterich 2007 identify some biases resulting from abrupt shifts in conditions however the authors acknowledge that complex anomalies are outside of the performance of their detector conde 2011 was unable to identify labeled anomalies with relatively small variation from the measured baseline leigh et al 2018 intentionally prioritized outliers in development of anomaly detection techniques for aquatic sensors given that existing methods have not addressed anomalies caused by drift and fouling there is significant room for improvement in methods for detecting these types of anomalies we examined both outliers and more subtle anomaly types in our methods and software implementation a 7 reproducibility although effectively implemented for specific case studies in the research realm none of the techniques described in the cited studies have been packaged as easily accessible software for broad application and dissemination without reusable code the specifics of the algorithms as implemented with environmental data cannot be examined further tested or applied to other datasets recent work in outlier detection was encapsulated in an r package talagala et al 2019 however a lack of documentation made it difficult to know how to install the package and apply the methods to our datasets provenance of data from raw field observations to quality controlled data products is vitally important yet rarely described in sufficient detail that the process used to arrive at final data products could be repeated horsburgh et al 2015 applying more automated techniques can help and reusable software tools can overcome barriers related to understanding and implementing complex algorithms for practical application rather than a model calibrated to a specific variable site combination practitioners need tools that can be applied to a broad suite of variables and or monitoring locations documented in a reusable and reproducible way thus we sought to package the tools we developed as open source software that could easily be deployed in a commonly available analytical environment a 8 anomaly correction various techniques and past studies developed functionality for detecting anomalies but few applied corrective actions which is an important and time consuming step in quality control post processing a handful of studies used modeled arima forecasts to directly replace anomalies that were detected by the same arima model termed anomaly detection and mitigation adam hill and minsker 2010 leigh et al 2018 however the objective of adam was to improve detection by ensuring that model input data did not include detected anomalies not to generate a corrected version of the dataset furthermore the success of adam was mixed and resulted in high rates of false positives leigh et al 2018 given the general lack of available methods for automated correction we explored new approaches for inclusion in the software package we developed appendix b list of files and functions this appendix provides a listing of each of the python files in the pyhydroqc package and describes the functionality that each provides more detailed documentation is found in the github repository and package documentation see the software availability section parameters py this file contains assignments of parameters for all steps of the anomaly detection workflow parameters are defined specific to each site and observed variable that are referenced in the detect script lstm parameters are consistent across sites and variables arima hyper parameters are specific to each site variable combination other parameters are used for rules based anomaly detection determining dynamic thresholds and for widening anomalous events anomaly utilities py contains functions for performing anomaly detection and correction get data retrieves and formats data retrieval is based on site observed variable and year to pass through subsequent steps the required format is a pandas data frame with columns corresponding to datetime as the index raw data corrected data and data labels anomalies identified by technicians anomaly events widens anomalies and indexes events or groups of anomalous data assign cm a helper function for resizing anomaly events to the original size for determining metrics compare events compares anomaly events detected by an algorithm to events labeled by a technician metrics determines performance metrics of the detections relative to labeled data event metrics determines performance metrics based on number of events rather than the number of data points print metrics prints the metrics to the console group bools indexes contiguous groups of anomalous and valid data to facilitate correction xfade uses a cross fade to blend forecasted and backcasted data over anomaly events for generating data correction set dynamic threshold creates a threshold that varies dynamically based on the model residuals set cons threshold creates a threshold of constant value detect anomalies uses model residuals and threshold values to classify anomalous data aggregate results combines the detections from multiple models to give a single output of anomaly detections plt threshold plots thresholds and model residuals plt results plots raw data model predictions detected and labeled anomalies modeling utilities py contains functions for building and training models pdq automatically determines the p d q hyperparameters of a time series for arima modeling build arima model lstm univar lstm multivar lstm univar bidir lstm multivar bidir wrappers that call other functions in the file to scale and reshape data for lstm models only create and train a model and output model predictions and residuals create scaler creates a scaler object for scaling and unscaling data create training dataset create bidir training dataset creates a training dataset based on a random selection of points from the dataset reshapes data to include the desired time steps for input to the lstm model the number of past data points to examine or past and future points bidirectional ensures that data already identified as anomalous i e by rules based detection are not used create sequenced dataset create bidir sequenced dataset reshapes all inputs into sequences that include time steps for input to the lstm model using either only past data points or past and future data points bidirectional used for testing or for applying the model to a full dataset create vanilla model create bidir model helper functions used to create single layer lstm models train model fits the model to training data uses a validation subset to monitor for improvements to ensure that training is not too long rules detect py contains functions for rules based anomaly detection and preprocessing depends on anomaly utilities py functions include range check scans for data points outside of user defined limits and marks the points as anomalous persistence scans for repeated values in the data and marks them as anomalous if the duration exceeds a user defined length group size determines the maximum length of anomalous groups identified by the previous steps interpolate corrects data points with linear interpolation a typical approach for short anomalous events add labels enables the addition of anomaly labels referring to anomalies previously identified by an expert in the case that labels may have been missed for corrected data that are nan or have been set to a no data value e g 9999 calibration py contains functions for identifying and correcting calibration events functions include calib edge detect identifies possible calibration event candidates by using edge filtering calib persist detect identifies possible calibration event candidates based on persistence of a user defined length calib overlap identifies possible calibration event candidates by finding concurrent events of multiple sensors from the calib persist detect function find gap determines a gap value for a calibration event based on the largest data difference within a time window around a datetime lin drift cor performs linear drift correction to address sensor drift given calibration dates and a gap value model workflow py contains functionality to build and train arima and lstm models apply the models to make predictions set thresholds detect anomalies widen anomalous events and determine metrics depends on anomaly utilities py modeling utilities py and rules detect py wrapper function names are arima detect lstm detect univar and lstm detect multivar lstm model workflows include options for vanilla or bidirectional within each wrapper function the full detection workflow is followed options allow for output of plots summaries and metrics arima correct py contains functionality to perform corrections and plot results using arima models depends on anomaly utilities py arima group ensures that the valid data surrounding anomalous data points and groups of data points are sufficient forecasting backcasting arima forecast creates predictions of data where anomalies occur generate corrections the primary function for determining corrections passes through data with anomalies and determines corrections using piecewise arima models corrections are determined by averaging together cross fade both a forecast and a backcast appendix c logan river observatory input parameters and settings table c1 input parameters for each time series persistence duration and window size refer to the number of time steps 20 5 h 30 7 5 h 40 10 h 45 11 25 h table c1 observed variable parameter franklin basin tony grove water lab main street mendon blacksmith fork temperature degrees c maximum range 13 20 18 20 28 28 minimum range 2 2 2 2 2 2 persistence duration 30 30 30 30 30 30 window size 30 30 30 30 30 30 alpha 1e 04 1e 05 1e 04 1e 05 1e 04 1e 04 threshold minimum 0 25 0 4 0 4 0 4 0 4 0 4 p d q 1 1 3 10 1 0 0 1 5 0 0 0 3 1 1 1 1 0 specific conductance μs cm maximum range 380 500 450 2700 800 900 minimum range 120 175 200 150 200 200 persistence duration 30 30 30 30 30 30 window size 30 40 40 40 40 20 alpha 1e 04 1e 05 1e 04 1e 06 1e 05 1e 02 threshold minimum 4 5 5 5 5 4 p d q 10 1 3 6 1 2 7 1 0 1 1 5 9 1 4 0 0 5 ph maximum range 9 2 9 9 2 9 5 9 9 2 minimum range 7 5 8 8 7 5 7 4 7 2 persistence duration 45 45 45 45 45 45 window size 30 40 40 20 20 30 alpha 1e 05 1e 05 1e 05 1e 04 1e 04 1e 05 threshold minimum 0 02 0 02 0 02 0 03 0 03 0 03 p d q 10 1 1 8 1 4 10 1 0 3 1 1 0 1 2 0 1 4 dissolved oxygen mg l maximum range 13 14 14 15 15 14 minimum range 8 7 7 5 3 2 persistence duration 45 45 45 45 45 45 window size 30 30 30 30 30 30 alpha 1e 04 1e 04 1e 05 1e 05 1e 03 1e 04 threshold minimum 0 15 0 15 0 15 0 25 0 15 0 15 p d q 0 1 5 10 1 0 1 1 1 1 1 1 10 1 3 0 0 5 table c2 lstm model parameters and settings selected for the lro case study defaults were used for all other settings and parameters not listed here see géron 2017 and keras development team n d for additional details table c2 parameter function setting details time steps model add 5 the number of past data considered as input for prediction for the lro data more time steps 10 15 20 biased results toward the mean reduced time steps 5 gave greater accuracy and improved computational time units cells model add 128 number of cells or nodes in the model architecture there is no rule for finding the perfect number of cells we chose a high number and used early stopping and dropout to prevent overfitting for processing purposes it is generally preferred to have network dimensions in multiples of 32 dropout model add 0 2 a fraction of cells that are randomly ignored during training using dropout improves the model by reducing overfitting but the number usually matters little 20 is often used to balance accuracy and overfitting optimizer model compile adam algorithm for training adam adaptive movement estimation is commonly selected for training lstm models for being computationally efficient requiring little memory and handling large amounts of data loss model compile mean absolute error the quantity to be minimized during training mean absolute error computes the mean of the difference between observations and predictions epochs model fit 100 the number of rounds to train the model we opted for a high number that is truncated by early stopping that ends training when the model is sufficiently fit validation split model fit 0 1 fraction of training data to be used as validation data on which the loss is evaluated at the end of each epoch callbacks model fit early stopping interrupts training when performance on the validation set drops patience model fit 6 number of epochs with no improvement after which training will be stopped shuffle model fit false whether to shuffle training data before each epoch set to false because the order of training data matters for these data appendix d anomaly detection and correction examples this appendix includes additional examples of anomaly detection and correction performed by the pyhydroqc workflow on lro case study data figure c1 illustrates anomaly detection false positives and true positives peaks and troughs in the data were considered anomalies by the model arima but only two of them 2017 12 18 and 2017 12 26 were labeled by the technician it is unclear why certain peaks were labeled by the technician while others were not although this example includes several false positives the algorithm behaved as expected fig c1 examples of anomalies detected using an arima model for specific conductance at main street fig c1 in some cases the apparent success of the model results may be an artifact of both the generalization of detections in the compare events function and the liberal application of labels by technicians some time series contain extensive periods of data labeled as anomalous that correspond to concerns with sensor validity or site conditions e g figure c2 when comparing events to determine confusion matrix categories any overlap in model detections results in all points of the anomalous period being identified as true positives this is an example where large events may bias the metrics toward true positives if any point in the event is detected or toward false negatives if the event goes undetected less likely this particular event contributes to the 13 000 true positives for this time series ph at main street fig c2 examples of anomalies detected using an lstm multivariate bidirectional model for ph at main street for of an extended period of data labeled as a sensor malfunction fig c2 we were interested in whether the models could detect calibration events for one time series ph at main street one model type lstm multivariate bidirectional detected approximately 20 of labeled calibration events we found that the master list of calibrations recorded in the field notes differs from what technicians labeled in the data some calibrations recorded in the field notes were not labeled by technicians in the data and other events labeled by technicians appeared to be calibrations but were not part of the master list derived from the field notes these discrepancies point to deficiencies in the labeled data the model predictions are erratic and do not track the observations at most calibration events figure c3a even if the threshold was not sensitive enough to result in detections in some cases calibration events were detected as anomalous by the model figure c3b but there was no mechanism to distinguish from other anomalies these examples illustrate the challenge of using the model based approach for detecting and correcting calibration events fig c3 examples of anomalies detected using an lstm multivariate bidirectional model on a ph sensor at main street with calibration events fig c3 a direct comparison of results from each model type illustrates model behaviors and associated detections for specific conductance at tony grove where there was variability in performance between model types see section 3 4 the arima and lstm multivariate vanilla models detected points at the edges of long duration labeled events improving their performance metrics relative to the other model types figure c4 further illustrates differences between model estimates and resulting detections for the first date range the estimates of both multivariate models deviate from the original data because they use other variables as input in the absence of this information only one univariate model detects an anomaly in the second date range models responded to the localized event in distinct ways and none resulted in a detection in the third date range estimates from the multivariate models exhibit spikes around the detections illustrating that information is coming from other variables it is likely that some of these labeled anomalies correspond to calibration events for which other variables exhibited greater shifts than did specific conductance fig c4 examples comparing model estimates and detected anomalies for all model types for specific conductance at tony grove fig c4 although the correction algorithm was capable of capturing diurnal oscillations in some cases data patterns did not translate and propagate through the corrections e g figure c5 because each correction is based on individual independent models trained for data immediately prior to and following an anomalous event the number of data points considered can vary even though the adjacent data used for input is limited by the maximum duration parameter some models may still overgeneralize i e a straight line other models may use so little data that a pattern is missed while still others are focused on a single dominant feature i e an oscillation or a curve furthermore a pattern may be damped over an extended time period explicitly incorporating seasonality into development of the arima models may result in more consistent output of oscillations however developing seasonal arima models is computationally demanding and the correction algorithm already requires significant computational resources the correction algorithm is directly dependent on identified anomalies in figure c5c an anomalous event 2018 06 19 2018 06 20 was detected by the model but even with widening the initial abrupt decrease was not labeled anomalous so it was considered valid data and it directly influenced the forecast for the correction algorithm to be effective anomalies should be reviewed and may need adjustment e g further widening fig c5 examples of problematic algorithm correction a and b dissolved oxygen at tony grove c specific conductance at mendon fig c5 
25606,water systems modelers have developed multiple independent model and study area specific tools to store query visualize and share their data this fragmentation makes difficult comparisons and synthesis within or across study areas this paper identifies the common components of four existing tools for data storage web visualization and repository and relates them correctly in syntax and semantics connecting the tools in a state of the art open source software ecosystem allowed comparing the effects of population growth and water conservation in simulation and optimization models for the bear river watershed u s and monterrey mexico that have different spatial coverage and urban and agricultural representations the software ecosystem also publishes the models and their data for discovery the software ecosystem can help modelers tap the best features of individual software tools to answer a next generation of water management questions that seek to discover use reproduce extend compare or synthesize within or across study areas keywords systems analysis openagua wamdam hydra platform open source hydroshare 1 introduction over the last half century the water resources systems analysis community has improved the modeling of interrelated natural and built water resources infrastructure and helped inform decisions regarding system planning and management maass et al 1962 rosenberg and madani 2014 brown et al 2015 systems models represent mass balance interactions between supply and demand components and have been widely used to support water resource systems analysis despite modeling advances modelers face technical challenges in developing and using these models first systems modelers must manage and store input and output data and track metadata second they need to set up socio economic and infrastructure management scenarios and track differences in input and output data abdallah and rosenberg 2019 third modelers need to visualize water system components and their connectivity as nodes and links fourth modelers must plot input and output data to communicate model results and engage stakeholders with minimum technical difficulties brown et al 2015 fifth modelers are increasingly required by funding agencies and journals to publish the final modeling data code and results to support reproducible science rosenberg and watkins 2018 stagge et al 2019 rosenberg et al 2020 2021 currently most systems modelers use or develop separate model specific tools for each of these tasks often on a per project basis developing these tools is time consuming and requires programming experience modelers would benefit from generalized modular tools to store data visualize and compare results and publish data for many datasets and models these tools should be reusable independent of any specific software or model require minimal programming and be open source should users want to modify or extend software functions traditional water resources systems models such as the water evaluation and planning system weap yates et al 2005 riverware zagona et al 2001 hec ressim hec 2007 goldsim goldsim technology group llc 2014 ewater source welsh et al 2013 aquatool andreu et al 1996 epanet rossman 2000 realm perera et al 2005 and the stormwater management model swmm rossman 2010 provide data storage data visualization and results from computation features in tightly coupled software architecture for example weap and riverware store data using proprietary database methods such as comma separated values csv and or as data management interface files modelers often use the software s graphical user interface gui to manually enter and access data while a few models like weap offer an application programming interface api that allows programmatic access to data most models have their own model engine which is one or more simulation or optimization algorithms that operate on the input data loucks et al 2005 knox et al 2014 2018 meier et al 2014 tomlinson et al 2020 often traditional software tools are proprietary such as weap riverware hec ressim goldsim ewater source and aquatool and may require paid licenses to use other systems modeling software such as epanet rossman 2000 realm perera et al 2005 and the stormwater management model swmm rossman 2010 are open source and free to use but have specific user interfaces or input file formats no software system can publish standardized data and associated metadata to online repositories the tightly coupled models and their heterogeneity reveals why it is difficult to reuse any of their data storage visualization or computational components additionally sharing publishing or transferring data to another model may require significant effort to first understand the data structures and then write data export and import functions for each model abdallah and rosenberg 2019 further traditional software often requires installation on local machines which adds a barrier to engaging stakeholders like water resources managers installing software can be problematic due to often it restrictions on installing new software on managers computers managers and their staff often want to inspect model s input data network configurations and visualize results of interest alminagorta et al 2016a systems modelers and researchers also need to develop novel models with capabilities beyond traditional models lund et al 2013 alminagorta et al 2016a kok et al 2018 lee et al 2019 alafifi and rosenberg 2020 researchers often spend the time to prepare input data develop algorithms and recreate other data storage visualization and analysis features within their modeling environment even though other models support similar features these modelers often use simple methods to manage data like excel and text files sehlke and jacobson 2005 alminagorta et al 2016b using excel allows modelers to easily access data but often requires the author to help others query or interpret data values one reason for such difficulties in interpreting and reusing these model files is because the files have limited or no metadata are intended to be read by a computer rather than a person and are designed to be used as input for a specific model in a specific location this specificity can make model coupling and reuse in other locations difficult in the broader field of hydrology researchers have developed loosely coupled and interoperable software architectures such as openmi to couple hydrologic components such as snowmelt runoff and infiltration processes elag and goodall 2013 each modeling component exchanges inputs and outputs defined across space and time with the other components using a standardized data coupling interface shared vocabulary and data exchange functions moore and tindall 2005 the hydrocouple interface extends openmi to include geospatial data formats and support simulation on high performance computers moore and tindall 2005 buahin and horsburgh 2018 the community surface dynamics modeling system csdms peckham et al 2013 provides an environment to couple earth surface models using a common programming language interpreter and shared vocabulary others developed a web service approach to couple components of the topoflow spatially distributed hydrologic model jiang et al 2017 zhang et al 2019 introduced a service oriented wrapper system for geo analysis models for gridded modeling such as the soil and water assessment tool swat model and the unstructured grid finite volume community ocean model fvcom in such software or component coupling the output data of a model is used as input data for another where each model still uses its own model specific database most of these methods execute in sequence without archiving model data gan et al 2020 developed a web service by coupling a generic user interface application tethys swain et al 2016 with a distributed snowmelt model gichamo et al 2020 and then storing and sharing its model input and output in the online hydroshare repository horsburgh et al 2016 most hydrology models use gridded input data such as terrain topology soil cover properties and precipitation that are often available in centralized data services gan et al 2020 systems models represent reservoir diversion irrigation municipal hydropower return flow groundwater river reach and other components as a network of nodes with links brown et al 2015 in this paper we use the term network which refers to connections of nodes through links data for water systems are disparate and are held or provided by different entities in various formats and structures to generalize water resources systems modeling hydra platform team has developed a framework of a database and web service to transfer water systems model data between a client water resources models and visualization tools harou et al 2010 knox et al 2014 2019 this paper presents an interoperability approach that integrates four independently developed active existing open source software tools for water resources systems modeling these four software frameworks are 1 the water management data model wamdam offers a defined metadata and use of controlled vocabulary cvs to enable data query and comparisons across models and datasets abdallah and rosenberg 2019 2 hydra platform allows users to encode and communicate systems modeling data over the web using a web services approach knox et al 2014 2019 3 openagua provides a web based application that uses hydra platform to let users collaboratively visualize and edit model networks rheinheimer 2020 and 4 hydroshare enables researchers to publish and discover water related datasets and modeling data tarboton et al 2014 this fragmentation of capabilities makes difficult comparisons and synthesis within or across study areas this paper identifies the common components of four existing tools for data storage web visualization and repository and relates them correctly in syntax and semantics into a software ecosystem jansen et al 2009 the software ecosystem can help modelers perform the following three key tasks i organize and store water systems modeling data with metadata and controlled vocabularies ii visualize edit and compare networks datasets and scenarios in an online application and iii publish systems modeling data with contextual metadata to enable data discovery and analysis these tasks allow modelers to engage stakeholders reproduce analyses and meet journal and funders data management requirements the software ecosystem serves both existing proprietary and novel models communities below we define three use cases that motivate and illustrate the software ecosystem three subsequent sections describe the software ecosystem components their coupling and example applications in the bear river watershed usa and the monterrey metropolitan area mexico the final sections present use case results limitations recommendations and invite community involvement to grow the software ecosystem 2 methods this section describes use cases software ecosystem coupling and applications 2 1 use cases the software ecosystem components focus on supporting three steps that modelers commonly follow to develop models i organize and store input and output model data which includes parameter values initial boundary conditions input data timeseries output time series ii visualize networks and perform exploratory data analysis and model diagnostics and iii publish data code and instructions used to run it to a public repository to enable their discovery especially for academic studies three use case general questions guide the software ecosystem work and may apply to many models answering similar or different research questions 1 how are networks defined and how their data similar and different for two models overlapping in the same study area in this use case we seek to demonstrate the ecosystem s database and online visualization functionality to select and compare data in a study area originating from different models that were developed for different purposes 2 how do water management scenarios in two different models of the same study area compare in this use case we seek to demonstrate how users can use the ecosystem s interactive experience in openagua to define scenarios in graphical user interface and then serve input data into the two different models through the wamdam database and vice versal for results to be visualized back in openagua 3 how do the values for an input data compare in two published modeling datasets for different study areas in this use case we seek to demonstrate the value of publishing modeling datasets using wamdam database into hydroshare which enables their discovery and comparative analysis these questions represent modelers needs to visualize their model networks visualize system data verify data input and engage stakeholders the questions also address modelers needs to change model input data run alternative or competing models and then visualize output data across scenarios and models presently these steps are often manual and specific to the structure of a model s data input file 2 2 software ecosystem components here we describe four existing generic open source software components that provide some of key modeling features table 1 first wamdam is a well defined data and metadata management framework with software tools to load relate and compare data for many systems modeling datasets abdallah and rosenberg 2019 the wamdam wizard is an open source desktop based software written in python that helps users load and query data from a wamdam sqlite database abdallah and rosenberg 2019 second hydra platform is an open source python based api for systems water management data hydra platform provides a web service to transfer model data used between a client application i e custom scripts and applications that need to access hydra platform and hydra platform s database hydra platform uses a generic data storage system for networks and their data but does not require metadata e g source and method of data harou et al 2010 knox et al 2014 2019 in contrast wamdam does require those metadata to correctly interpret data and wamdam also offers controlled vocabularies to relate data across models abdallah and rosenberg 2019 third openagua is a web based application for collaborative modeling and visualization of water resources systems that uses hydra platform as a part of its web services and data storage system rheinheimer 2020 openagua generically manages data for models where users can optionally add metadata fourth hydroshare is the consortium of universities for the advancement of hydrologic science inc cuahsi online collaboration environment with web services for sharing and discovering data models and code tarboton et al 2014 hydroshare requires metadata according to the dublin core metadata initiative which describe digital resources i e files such as title owner coverage in space and time hydroshare also creates a digital object identifier doi for the published resources e g modeling data so resources can be easily cited in journal publications and other documents hydroshare allows their users to make models and data publicly available online without login while openagua requires a login to access its public networks and data 2 3 software ecosystem coupling we couple the components into a software ecosystem figure 1 together the coupled components allow users to reuse components to store visualize compare publish and discover modeling data across many different models key connections are fig 1 black arrows i move data from a weap model into the wamdam database ii export data from the wamdam database and upload data to hydra platform iii exchange data between hydra platform and openagua and iv export data from hydra platform into wamdam the oppostive direction of step ii final steps include v publish data from wamdam into hydroshare and vi use jupyter notebooks to query and analyze published datasets in hydroshare below we describe the coupling of each pair of components 2 3 1 import model datasets to wamdam modelers can already organize and store their water management data in a wamdam sqlite database using the wamdam wizard the wizard supports importing modeling datasets from a generic microsoft excel importer weap models and networks using weap s api abdallah and rosenberg 2019 stream discharge time series data from the consortium of universities for the advancement of hydrologic science inc cuahsi hydrologic information systems web services or u s bureau of reclamation reservoir storage and release time series data modelers can also export gams data for optimization models or any other system models into csv files and then use the generic excel importer to import the data into a wamdam database abdallah and rosenberg 2019 the data importer reads tabular data from an excel workbook which is available here https github com wamdamproject wamdam jupyternotebooks the workbook includes spreadsheets for metadata that match wamdam s data dictionary of required and optional fields for the following tables organizations people sources methods resource types e g models object types network scenarios models links numeric values categorical values seasonal numeric values time series and multi attribute series the data importer reads and validates the provided input data and metadata and their correct relationships in the database the data importer provides informative error messages if required data is missing and the relationships between data does not match each model dataset that is imported to the wamdam database can in turn be connected to the other ecosystem components as described in the following sub sections 2 3 2 export data from wamdam to hydra platform after importing modeling data into wamdam modelers may need to move data to other tools such as hydra platform to take advantage of online client applications such as openagua wamdam and hydra platform manage data for the same shared domain of water resources systems modeling however they have different motivating use cases and thus have different designs wamdam organizes data and metadata and uses controlled vocabularies to relate terms across many models and datasets whereas hydra platform provides a generic storage and web service approach that supports connecting from custom client applications e g guis and models to couple wamdam and hydra platform we first identified and mapped the equivalent common tables and contents between wamdam and hydra platform where each of them has different terms to describe the same metadata item table 2 hydra platform handles users login information and has the concept of a project where users can collaborate on one or many networks for the same model in contrast wamdam uses controlled vocabularies to relate synonymous object types attributes and instances across models and supports the reuse of explicit metadata of sources methods organizations and people that describe data hydra platform only allows changing data values among scenarios for the same network while wamdam allows changing both the network connectivity and data values as part of scenarios thus as a remedy two scenarios in wamdam with differences in nodes and links for the same master network will be stored in hydra platform as scenarios in two separate networks finally we wrote a python script to export wamdam data to hydra platform the script is run from the wamdam wizard and i connects to a wamdam sqlite database that is previously populated with data for systems models each model may have multiple networks and each network may have many scenarios then ii under the visualize and publish tab in the wizard the user clicks openagua and fills out login credentials to their hydra platform account as managed by openagua app users then iii upload wamdam data into an existing project in hydra platform or add a new project next users iv choose a resource type in the wamdam database e g model or dataset name to visualize its network and data then users v select a network for the model and vi choose one or more scenarios inside the network finally the user clicks upload the python script calls the hydra platform web service to add a project uses a sql script to query the wamdam database for the data to populate into each equivalent hydra platform table the script then calls the add attribute add template and add network methods to add new attributes add a new resource type i e a model which includes all object types their attributes and add the network which includes all nodes and links and their scenarios 2 3 3 visualize hydra platform data in openagua once the modeling data are successfully uploaded into hydra platform they automatically become available to hydra platform client applications such as openagua that use the hydra platform api to store manage and retrieve data the openagua web api is written in python javascript and html exposing to end users a graphical user interface uses and includes a wide range of functions that extend the core data management capabilities of hydra platform openagua site rheinheimer 2020 in review rheinheimer 2020 by design any network scenario data or metadata value that is created or entered in openagua is stored in and can be accessed using hydra platform s native web service functions hydra platform manages users and login credentials openagua extends this user management capability to allow projects and networks to be publicly visible to and discoverable by any openagua user 2 3 4 export from hydra platform to wamdam modelers may also want to export data hosted in hydra platform within openagua into wamdam to publish their data in hydroshare this transfer would allow further cross model data queries and analysis that are not possible in hydra platform or openagua to transfer data from hydra platform to wamdam open the wamdam wizard and under the import data to wamdam tab click the import from openagua button the user first provides their hydra platform account credentials created through openagua to connect to the hydra platform server next the user selects a project name resource type i e model name a network scenario and a directory on the local machine to import the hydra platform data into a wamdam template excel file when the user clicks the import button the wamdam wizard script calls four main hydra platform web service functions get template get network get scenarios and get all resource data these functions pull i a list of all the object types and their attributes ii a list of nodes and links iii all the scenarios in the selected network and their metadata of start and end dates and time steps and finally iv a list of all attributes for the nodes and links and their data values supported data types are time series array numeric seasonal and descriptors each call returns a javascript object notation json result parsed and mapped to the wamdam tables and their terminology modelers can then augment the excel workbook template with additional metadata that are required by wamdam and may not be available in hydra platform such as source method people and organizations for the nodes links and data values if metadata is not known modelers can define and reuse one generic metadata item e g source to all model data in wamdam modelers also can use controlled vocabularies in the workbook template to register native terms of datasets in hydra platform which allows the terms to be queried using controlled terms abdallah and rosenberg 2019 users then use the wamdam wizard to load the imported excel workbook into a wamdam sqlite database 2 3 5 connect wamdam and hydroshare modelers are increasingly required to publish their modeling data with contextual metadata that describe its content and coverage in space and time additionally there is increasing need to provide programmatic access to read query and analyze published data the wamdam database can contain modeling data from hydra platform hydra platform compliant applications like openagua model data sets such as for a weap model or other data sources to publish wamdam sqlite files into hydroshare as a composite resource we wrote a new python script and integrated it with the wamdam wizard the script is run from the wamdam wizard harvests the dublin core metadata from wamdam database and uploads the sqlite file with the model data into hydroshare more specifically the script uses the hs restclient python rest api library that allows programmatic access to publish and query hydroshare files and metadata hydroshare development team 2020 to use the script a user should open the wamdam wizard and connect to a wamdam sqlite database that contains the systems models networks and their data the wizard verifies that sqlite file complies with the wamdam schema including all tables and fields next under the visualize and publish tab in the wizard the user clicks hydroshare the user then provides their login credentials to their hydroshare account the user specifies a title abstract and author name s for the new dataset publication finally the user clicks publish this button executes a python script that auto queries the following generic and extended metadata from the sqlite file i temporal coverage from the modeling scenario s as the minimum start and maximum end dates ii spatial coverage box from the minimum and maximum latitude and longitude for nodes in the network s iii list of resources type s i e model names unique object types and attribute controlled vocabularies if they exist iv network and scenario name s and v list of sources methods people and organizations metadata the script adds three keywords to the created hydroshare resource wamdam systems models and water management these keywords allow hydroshare users to discover the published dataset and other prior published datasets next the script calls hydroshare s createresource method to upload the wamdam sqlite file and all the above metadata into a private resource in hydroshare where users can edit metadata share the resource with other hydroshare members or make the resource public finally hydroshare creates a doi for permanent publication once the user makes the published resource public modelers can use jupyter notebooks to programmatically access any of the published wamdam sqlite databases in hydroshare using wamdam s defined schema to query analyze and potentially reuse model data 2 4 additional coupling and testing coupling wamdam hydra platform and openagua required each developer to make minor changes to their software for example we added a field named layout property in wamdam for object types to visualize a shape for each object type e g reservoir icon as a triangle we adopted openagua s object type layout that encodes the icon shape and color in scalable vector graphics svg format we added the source and method metadata fields to hydra platform which can accommodate arbitrary metadata in json format we also changed the openagua gui to display wamdam metadata fields we made four scenario properties in wamdam required opposed to optional before so scenarios could be viewed in openagua scenariostartdate scenarioenddate timestep and timestepunitcv we also added two additional fields scenarioparentname and scenariotype to the scenarios table in wamdam scenarioparentname explicitly maps scenario inheritance among scenarios as supported in hydra platform and openagua scenariotype can take one of five potential values in openagua which affects where they are shown in the interface baseline option uncertainty portfolio and results the first four scenario types are used to organize and query changed input data while the fifth is used to query results input and results scenarios are shown in different areas of the application the root scenario is the main top level input scenario from which other option or uncertainty scenarios are derived each newly defined child option or uncertainty scenario in openagua references i e reuses the identical input data of its parent portfolio scenarios indicate a group of option scenarios in openagua users can edit the input scenario data using the basic data editor users can enter new values manually the new values will be unique to the child scenario the results scenario type stores output values for a modeling scenario this scenario type is used in openagua to visualize and compare output datasets in the results explorer finally wamdam adopted the list of units used in hydra platform as a common controlled vocabulary as openagua also uses hydra platform s units these above changes allow users to send wamdam data to hydra platform and use openagua to examine and edit scenarios online and import the data back to wamdam to run a model or publish the dataset we anticipate that the coupling is the beginning of a process where each software will continue to update to improve the user experience and accommodate more diverse use cases we validated the integrity of the import and export scripts to couple software ecosystem components by uploading the bear river 2017 rosenberg 2017 water allocation weap model abdallah 2019 from wamdam to hydra platform we confirmed the upload visually in openagua and then downloaded the model dataset back into wamdam we then used the wamdam wizard scenario comparison tool to verify that both scenarios the original in wamdam and the newly downloaded scenario from hydra platform were identical and no changes were unintentionally introduced in the upload or download mappings thus modelers can now upload wamdam modeling data into hydra platform and use the specific openagua set up to visualize and edit data online users can also import models from hydra platform into wamdam run the model and publish input or output results into hydroshare to enable data discovery analysis or serve data to other models 2 5 application we illustrate the numerous benefits of the software ecosystem with three use cases that include tasks to store visualize edit publish and compare modeling data for two models that overlap in part of the bear river watershed usa and a third model for the monterrey metropolitan area mexico the models address different research questions but are used to demonstrate the generality of the ecosystem tools and to draw comparisons among the models and their data the first model is the watershed area of suitable habitat wash optimization model that allocates water to maximize watershed habitat areas for the lower bear river watershed utah portion alafifi and rosenberg 2020 the wash model uses the general algebraic modeling system gams engine which has no user interface the second model is a weap simulation model that allocates water by water right priority within the bear river watershed utah and idaho portions weap has a proprietary database and does not support data publication the third model is a water allocation model for the monterrey metropolitan area mexico rheinheimer 2020 both the wash and weap models were developed from a predecessor 2010 utah division of water resources model for the lower bear river basin that had a plain text input file and fortran computational engine which was deprecated the wash model disaggregated irrigation demands within cache valley utah while the weap model extended the model domain upstream to idaho and bear lake the monterrey metropolitan area model is stored in hydra platform within openagua with no controlled vocabulary the use cases assume a modeler has used the wamdam wizard and loaded data for the three models into a wamdam sqlite database abdallah and rosenberg 2019 the user already has accounts for hydroshare and openagua free the first use case exported the weap and wash model data for the bear river watershed from wamdam to hydra platform then openagua was used to visually compare the similarities and differences in the model networks in an online web browser fig 2 running models within the openagua platform is outside the scope of this work an openagua manuscript will demonstrate that feature as part of its unique contribution the second use case created new weap and wash model scenarios in openagua that increased and decreased annual urban water demand in cache county utah by 25 from the base demand then exported the model data to hydro platform and on to wamdam to run the models using jupyter notebooks the model runs quantified the annual percent change in unmet demand at cache county weap model and change in the suitable watershed area for aquatic flood plain and wetlands habitat for native bonneville cutthroat trout fish cottonwoods and three indicator migratory bird species with differing needs for shallow medium and deep water habitat wash model more specifically scenario data were manually input and edited online in openagua appendix a figure a1 the wamdam wizard was used to download the new scenario data back to wamdam python scripts in jupyter notebooks abdallah and rosenberg 2019 were used to query the wamdam database for each model input the new scenario data into weap using its api write the gms wash input data file execute both models and read their results and store them in wamdam next the wamdam wizard was used to export scenario results for both models from wamdam to hydra platform finally the openaqua results explorer utility was used to plot and compare the annual unmet demand across the baseline conservation and growth scenarios fig 3 the third use case compares the magnitude and seasonality of agriculture water demand for the monterrey metropolitan area mexico and the bear river watershed in utah then publishes the datasets in hydroshare fig 4 for this use case we first uploaded each model dataset to hydroshare using the wamdam wizard while users may write their own custom python script to upload datasets to hydroshare the wizard offers an important role by extracting key metadata from the models data stored in wamdam sqlite database and auto populates hydroshare metadata fields for the user see section 2 2 5 we note here that the two datasets are for input data loaded into wamdam sqlite database from two different models using different software after the datasets were uploaded to hydroshare we then used a python script in the jupyter notebooks to access and download the published sqlite files query and compare the controlled terms delivery target for logan irrigation demand site in utah and delivered flow for agriculture demand for the dr bajo rio san juan site in mexico we choose sites in both models to compare the seasonality and magnitude of irrigated agriculture in two countries since they have comparable demand for irrigation 3 results we present use case results to manage visualize edit and publish water resources modeling data and results online 3 1 use case 1 how are the networks of the weap and wash models in the bear river watershed usa similar and different comparison of the two model networks table 3 and fig 5 shows 1 the weap model for the bear river supports more water system components such as flow requirement groundwater and streamflow gage which are not explicitly supported in wash the common resource types between the models that use the same controlled vocabularies are demand and dem reservoir and v and return flow and returnflowexist in weap and wash respectively wash used the general node resource type j for any other network connection while weap is specific about the types such as river headflow or diversion outflow these results show the similarities and differences in the two models capabilities and a potential for input data reuse or transfer between them in other watersheds e g populate a new wash model from an existing weap model 2 the weap model has a larger number of object instances and covers a larger area upstream into idaho for example the weap model includes 20 demand sites within the same lower basin area compared to 11 sites for wash more specifically the weap model includes three urban demand sites for cache county logan potable north cache potable and south cache potable while wash represents all of them in one node as j3 that has a controlled term of cache county m i the reader can view these sites in openagua fig 5 3 the weap model also includes specific upstream supply and demand and storage especially bear lake top half of the screenshot in the wash model this part of the system is aggregated into a river headflow 4 the bear river weap model simulates monthly demand reliability across 40 years of dry wet and average water years compared to the wash model which focuses on maximizing the watershed area for suitable habitat within a single year thus the weap model could be useful to quantify cooperation scenarios between the utah and idaho states where downstream users in utah could store water in bear lake in wet years and use it later in dry years both the bear river weap and wash models data are published in hydroshare abdallah 2020b appendix a figure a2 3 2 use case 2 what are the differences in weap and wash model s outputs in the face of water conservation and population growth scenarios in the bear river watershed results follow the general expected trend that increased demand increases shortages while water conservation reduces shortages fig 6 there are four years 1970 1976 1993 1996 where water conservation eliminates shortages while shortages persist for the base case and increased demand scenario in dryer years e g 1987 to 1992 and 2000 to 2004 where there is not enough water to meet site demand the conservation scenario reduces the magnitude of shortages compared to the baseline scenario these results are also available online in openagua for stakeholders to view and discuss for the wash model the watershed area for suitable habitat for native vegetation birds and fish in the baseline scenario 2003 hydrologic year is estimated at 121 526 acres reducing cache county urban demand by 25 would increase the wash area by 144 acres while a 25 increase in the site s demand would decrease the wash area by 142 acres this small increase or decrease in wash area is because of the small influence of cache county urban site which represents about 18 of the total annual agriculture and urban demand in this watershed of 415 million cubic meters 336 446 acre feet these results show the potential role of targeted urban water conservation and growth in improving or degrading suitable habitat areas in the watershed one very interesting comparison between the weap and wash models is that weap estimates 8 12 17 increase in demand shortage for cache county urban site in 2003 fig 6 red box while the wash model completely satisfies demands no shortages for all three demand scenarios if the wash model could not meet the demands constraints the model would return an infeasible solution alafifi and rosenberg 2020 this discrepancy between the two models to meet demand at the cache county urban site in 2003 is likely because of the two models different spatial extents and how they aggregate and disaggregate demand sites and upstream supplies see use case 1 results 3 3 use case 3 how do the magnitude and seasonality of agriculture water demands in monterrey metropolitan area mexico and utah compare results show on average that the monthly demand target for logan irrigation site in utah is 1 8 cubic meters per second cms black squares compared to 0 15 cms demand grey circles for dr bajo rio san juan agriculture demand site in monterrey mexico fig 7 agriculture demand i e crop growth in utah extends for six months and is much shorter than the 11 month irrigation season in mexico in utah agriculture demands begin in april peak in july and end in october in mexico agriculture demands begin in december peak in april and end in october it is unclear why the mexico demand from june to october has two steps of increase and decrease the two steps may represent switching to different crops or harvesting patterns this comparison between two different models and countries was possible because the software ecosystem can move data between models and software the monterrey mexico water allocation model data can be accessed in hydroshare abdallah 2020a all of the above results can be reproduced in jupyter notebooks abdallah 2020c while a weap software license is still required to run the bear river model stakeholders can use the openagua tool to examine the already uploaded model input data and select results without needing weap license 4 discussion connecting wamdam hydra platform openagua and hydroshare into a software ecosystem allows modelers to store edit run scenarios visualize and publish online water resources systems data the software ecosystem facilitates the export of model data from one component to another to allow users to access features available in other system components together these coupled features allow modelers to compare simulation and optimization models for the same modeling domain and different domains for example the weap bear river model supports more specific demand sites within the same area compared to wash and thus weap offers more decision support analysis for each demand site the weap bear river model represents specific upstream supply and demand and storage compared to aggregated river headflows in wash thus the weap model could be useful to quantify cooperation scenarios between utah and idaho the weap bear river model includes 40 years of monthly supply data compared to a single year in this wash model and thus the weap model would be more useful to simulate water allocations and potentially unmet demand under a spectrum of historic hydrologic years from dry to wet conditions reducing urban demand in cache county in the bear river weap model by 25 would reduce unmet demand relative to the base case including in dry years the same software ecosystem tools and steps were used for a different model wash to estimate the effect of decreases in cache county urban demand by 25 finally the ecosystem enabled systematic comparisons and insights from input data originating from two different models in utah and mexico comparing utah and mexico agricultural demands from two models showed both agriculture demand sites from two different models for utah and mexico share high seasonal variability but have different growth seasons the identified variability suggests the importan0ce of water storage for both sites at different times when demand is low winter to use water later when demand is high such as spring in mexico and summer in utah 4 1 advantages of using the software ecosystem tools no model or software tool can do all data storage scenario entry visualization comparison stakeholder access and publishing tasks equally well our described software ecosystem approach allows modelers to move their data to the software component that is best suited for the data or modeling task additionally the software ecosystem allows users to construct workflows for tasks that cannot be done in any of the individual software system components for example the software ecosystem allows users to visualize and compare model data for many models and scenarios without being limited to one set of core object types and attributes as in weap or riverware users can also define model scenarios online in openagua then move data to wamdam to run the model and publish results the ecosystem tools can help compare networks for the same basin in two different modeling software these comparisons are facilitated by consistent data storage with metadata and controlled vocabularies in wamdam hydra platform and consistent visualization in openagua our software ecosystem further allows each model and dataset to retain its native terms for object types and attributes this feature allows users to view model data in openagua and support broader stakeholder engagement stakeholders can inspect modeling networks and data using a web browser without needing to install software on local machines this online setup provides users greater access to create new scenarios edit and visualize input data using the openagua interface researchers who develop novel models can use the software ecosystem components to manage their data compare scenarios and identify differences in networks input and output data without need to develop their own data management online visualization or publication features comparing datasets across novel and existing models will help researchers undertake benchmarking studies and distinguish similarities and differences in input data a new code to add another model needs to be compatible with wamdam and then it will also work with the other software tools similarly if a new model is compatible with hydra platform then the model s data can be imported into wamdam the automated publishing of water resources systems models and their data will make it easier for researchers and stakeholders to discover use reproduce extend and build new models sharing and publishing these models and datasets helps researchers fulfill data management requirements established by the national science foundation https www nsf gov eng general dmp jsp and by journals rosenberg and watkins 2018 stagge et al 2019 rosenberg et al 2020 sharing model datasets can increase the potential for their reuse reduce the time to build models and increase the value of water resources models within and outside the discipline the use of the software ecosystem products by others can be measured by a simple discovery exercise in hydroshare search resources for the keyword wamdam currently hydroshare returns six published wamdam datasets that are part of this work 4 2 limitations there is much work to do to improve the software ecosystem tools and coupling there are still metadata and software specific configurations and parameters to support the wamdam wizard is currently implemented on a local machine deploying wamdam in a cloud setting with web services coupling similar to openagua hydra platform and hydroshare would make the wamdam wizard less dependent on local computer configurations openagua is used here to visualize networks edit scenarios data and plot results but not to run models in the cloud currently openagua users can only visually search for public models and networks which will become difficult as the number of projects and networks grow with time openagua needs a search function currently data and model discovery are limited to projects and networks in openagua or sqlite databases and their metadata in hydroshare hydroshare interface does not natively support data analysis on data within multiple sqlite files it is also not user friendly to search with hydroshare for specific network scenario node link or attribute data that are contained within many published sqlite files in hydroshare reproducing model results requires running the models running models may be difficult for models that must be installed on a desktop machine require a paid license and are operating system specific the current scenario parameters in wamdam and hydra platform use simple time steps such as day month or year the software ecosystem should support time steps in posix format to account for leap years and number of days in the month that are available in weap 4 3 future work future versions of the software ecosystem should support geospatial search for individual water management infrastructure its connectivity and data this feature can be added by building on the ability to search time series data e g hydrodesktop ames et al 2012 and hydroclient http data cuahsi org wamdam support for controlled vocabularies would be particularly useful to search across different native terminology used in models and by users this functionality could allow the cuahsi web services to search for reservoir bathymetry curves seasonal demand data or network connectivity for example which demand sites are supplied by a particular reservoir both the hydra platform and openagua development teams are currently exploring ways to integrate alternative database systems to accommodate big data that can result from many scenario analyses for large water networks instead of using user login credentials to connect to the hydra platform and hydroshare servers more secure authentication methods should be implemented including api key and javascript web token jwt methods to remove user s needs to install software locally future implementations of the wamdam wizard should build read and write web services to a server based database as an online application future work should provide tools to allow users to more easily provide metadata and register their native terms with existing controlled vocabularies besides the wamdam workbook excel template future work should allow instantaneous interaction through an api between data storage in wamdam visualization in openagua and many simulation or optimization model engines further work should use the software ecosystem with more applications and models and connect additional tools to expand the ecosystem further work must also keep up with all the modifications and improvements that model developers make software by nature needs updates to keep up with recent library versions and compatibilities in future work the wamdam wizard needs to be updated to the latest libraries of python 3x distribution finally we note the need for continual alignment of development efforts to help ensure ecosystem components remain inter compatible over the long term this continual alignment requires regular communications and code transparency between component projects even as each ecosystem component is developed independently addressing this challenge will require version control and strong documentation of respective tools 4 4 invitation for community involvement and feedback we invite water modelers analysts students faculty professionals managers and other members of the water resources systems community to use the software ecosystem provide feedback and help develop new tools to expand the ecosystem to improve model communication develop and use controlled vocabularies metadata and tools to describe relate and move model data these capabilities will also help visualize and compare model inputs and outputs the current ecosystem is the product years of work and feedback we received from collaborators colleagues workshop participants and audiences since the inception of hydra platform hydroshare and wamdam in 2013 and openagua in 2016 you can participate in multiple ways such as 1 use the wamdam keyword to search and discover water systems datasets and models in hydroshare 2 use the software ecosystem tools for your existing weap model 3 use the wamdam wizard to link native vocabulary for your data sets s and model s to controlled vocabulary 4 build exporters and importers to wamdam for your own custom model or dataset and 5 build other interoperable software tools that will further your work and the work of others 5 conclusions this paper addressed the problem of using many disconnected and often model specific software tools to store visualize edit run analyze and publish systems modeling data we contributed a description prototype and demonstration of an interoperable set of open source software tools wamdam hydra platform openagua and hydroshare that help modelers to i store and organize data with metadata and controlled vocabularies in wamdam ii visualize edit and compare model networks and their input and output data in an online application openagua and iii publish systems modeling data and metadata to support data discovery and analysis in hydroshare the value of this ecosystem is in enabling modelers to move water systems models data from database to a model to an online user interface and publish it to an online data repository moving data across the coupled tools allow comparisons between models in a common platform the software ecosystem offers generalized features beyond any standalone software this prototype of coupled software ecosystem aims to help modelers spend more time on modeling and less time to develop specific tools to store visualize and publish data and models we see the software ecosystem as a complement to existing models such as weap weap has a large user base that could benefit from this software ecosystem by visualizing its models online and publishing their data in hydroshare using the wamdam consistent schema model datasets published in hydroshare can be discovered with the keyword wamdam and can be reproduced and used in follow on applications the software ecosystem is demonstrated in three use cases for two models in utah and one model in mexico the use cases compare networks for the same basin in two different modeling software set up and run multiple scenarios and models from an online portal and automate the process to share and publish model data the software ecosystem can help modelers tap the best features of individual software tools to answer a next generation of water management questions that seek to discover use reproduce extend compare or synthesize within or across study areas future work should implement all components of the coupling software online to make the interface more user friendly and support use cases for instantaneous connection between wamdam and hydra platform using the software ecosystem to move data has a learning curve as it involves familiarity with wamdam and creating two accounts with openagua and hydroshare we invite water modelers analysts students faculty professionals managers and other members of the water resources systems community to use the software ecosystem provide feedback and help develop new tools to expand the ecosystem software availability name of software the wamdam wizard developer adel m abdallah contact adel m abdallah email adel m abdallah gmail com year first available 2020 required hardware and software the wamdam wizard executable is available for use with microsoft excel 2007 and later versions and sqlite3 on windows 64 bit computers hydra platform web services are hosted by openagua which is available online on any browser or as a web service hydroshare is available online through a browser and web service input data and directions documentation of all source code datasets use cases and instructions to use the ecosystem and replicate results are available on github jupyter notebooks can be executed on a local machine and many of them can be run on the cloud using mybinder service it is assumed the user is familiar with working in a python coding environment hadia akbar at utah state university used an earlier draft of the wamdam wizard openagua hydroshare and jupyter notebooks to reproduce results for each use case jiada li at the university of utah also used the final tools and reproduced use cases results https github com wamdamproject wamdam jupyternotebooks blob master 3 visualizepublish 00 wamdam directions and use cases ipynb programming languages python and structured query language sql cost and license free software and source code are released under the new berkeley software distribution bsd 3 clause license which allows for liberal reuse declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was supported by the national science foundation through the ci water project grant 1135482 iutah grant 1208732 and utah mineral lease funds any opinions findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the national science foundation the intermountain section of the american public works association the utah chapter of the american public works association and the utah water users association provided additional financial support david tarboton and scott black provided insights into connecting wamdam with hydroshare ayman affifi provided insights on using the wash model in gams hadia akbar at utah state university used an earlier draft of the wamdam wizard openagua hydroshare and jupyter notebooks to reproduce results for each use case jiada li at the university of utah also used the final tools and reproduced use cases results we thank the three anonymous reviewers whose comments and suggestions helped improve and clarify this manuscript appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105371 
25606,water systems modelers have developed multiple independent model and study area specific tools to store query visualize and share their data this fragmentation makes difficult comparisons and synthesis within or across study areas this paper identifies the common components of four existing tools for data storage web visualization and repository and relates them correctly in syntax and semantics connecting the tools in a state of the art open source software ecosystem allowed comparing the effects of population growth and water conservation in simulation and optimization models for the bear river watershed u s and monterrey mexico that have different spatial coverage and urban and agricultural representations the software ecosystem also publishes the models and their data for discovery the software ecosystem can help modelers tap the best features of individual software tools to answer a next generation of water management questions that seek to discover use reproduce extend compare or synthesize within or across study areas keywords systems analysis openagua wamdam hydra platform open source hydroshare 1 introduction over the last half century the water resources systems analysis community has improved the modeling of interrelated natural and built water resources infrastructure and helped inform decisions regarding system planning and management maass et al 1962 rosenberg and madani 2014 brown et al 2015 systems models represent mass balance interactions between supply and demand components and have been widely used to support water resource systems analysis despite modeling advances modelers face technical challenges in developing and using these models first systems modelers must manage and store input and output data and track metadata second they need to set up socio economic and infrastructure management scenarios and track differences in input and output data abdallah and rosenberg 2019 third modelers need to visualize water system components and their connectivity as nodes and links fourth modelers must plot input and output data to communicate model results and engage stakeholders with minimum technical difficulties brown et al 2015 fifth modelers are increasingly required by funding agencies and journals to publish the final modeling data code and results to support reproducible science rosenberg and watkins 2018 stagge et al 2019 rosenberg et al 2020 2021 currently most systems modelers use or develop separate model specific tools for each of these tasks often on a per project basis developing these tools is time consuming and requires programming experience modelers would benefit from generalized modular tools to store data visualize and compare results and publish data for many datasets and models these tools should be reusable independent of any specific software or model require minimal programming and be open source should users want to modify or extend software functions traditional water resources systems models such as the water evaluation and planning system weap yates et al 2005 riverware zagona et al 2001 hec ressim hec 2007 goldsim goldsim technology group llc 2014 ewater source welsh et al 2013 aquatool andreu et al 1996 epanet rossman 2000 realm perera et al 2005 and the stormwater management model swmm rossman 2010 provide data storage data visualization and results from computation features in tightly coupled software architecture for example weap and riverware store data using proprietary database methods such as comma separated values csv and or as data management interface files modelers often use the software s graphical user interface gui to manually enter and access data while a few models like weap offer an application programming interface api that allows programmatic access to data most models have their own model engine which is one or more simulation or optimization algorithms that operate on the input data loucks et al 2005 knox et al 2014 2018 meier et al 2014 tomlinson et al 2020 often traditional software tools are proprietary such as weap riverware hec ressim goldsim ewater source and aquatool and may require paid licenses to use other systems modeling software such as epanet rossman 2000 realm perera et al 2005 and the stormwater management model swmm rossman 2010 are open source and free to use but have specific user interfaces or input file formats no software system can publish standardized data and associated metadata to online repositories the tightly coupled models and their heterogeneity reveals why it is difficult to reuse any of their data storage visualization or computational components additionally sharing publishing or transferring data to another model may require significant effort to first understand the data structures and then write data export and import functions for each model abdallah and rosenberg 2019 further traditional software often requires installation on local machines which adds a barrier to engaging stakeholders like water resources managers installing software can be problematic due to often it restrictions on installing new software on managers computers managers and their staff often want to inspect model s input data network configurations and visualize results of interest alminagorta et al 2016a systems modelers and researchers also need to develop novel models with capabilities beyond traditional models lund et al 2013 alminagorta et al 2016a kok et al 2018 lee et al 2019 alafifi and rosenberg 2020 researchers often spend the time to prepare input data develop algorithms and recreate other data storage visualization and analysis features within their modeling environment even though other models support similar features these modelers often use simple methods to manage data like excel and text files sehlke and jacobson 2005 alminagorta et al 2016b using excel allows modelers to easily access data but often requires the author to help others query or interpret data values one reason for such difficulties in interpreting and reusing these model files is because the files have limited or no metadata are intended to be read by a computer rather than a person and are designed to be used as input for a specific model in a specific location this specificity can make model coupling and reuse in other locations difficult in the broader field of hydrology researchers have developed loosely coupled and interoperable software architectures such as openmi to couple hydrologic components such as snowmelt runoff and infiltration processes elag and goodall 2013 each modeling component exchanges inputs and outputs defined across space and time with the other components using a standardized data coupling interface shared vocabulary and data exchange functions moore and tindall 2005 the hydrocouple interface extends openmi to include geospatial data formats and support simulation on high performance computers moore and tindall 2005 buahin and horsburgh 2018 the community surface dynamics modeling system csdms peckham et al 2013 provides an environment to couple earth surface models using a common programming language interpreter and shared vocabulary others developed a web service approach to couple components of the topoflow spatially distributed hydrologic model jiang et al 2017 zhang et al 2019 introduced a service oriented wrapper system for geo analysis models for gridded modeling such as the soil and water assessment tool swat model and the unstructured grid finite volume community ocean model fvcom in such software or component coupling the output data of a model is used as input data for another where each model still uses its own model specific database most of these methods execute in sequence without archiving model data gan et al 2020 developed a web service by coupling a generic user interface application tethys swain et al 2016 with a distributed snowmelt model gichamo et al 2020 and then storing and sharing its model input and output in the online hydroshare repository horsburgh et al 2016 most hydrology models use gridded input data such as terrain topology soil cover properties and precipitation that are often available in centralized data services gan et al 2020 systems models represent reservoir diversion irrigation municipal hydropower return flow groundwater river reach and other components as a network of nodes with links brown et al 2015 in this paper we use the term network which refers to connections of nodes through links data for water systems are disparate and are held or provided by different entities in various formats and structures to generalize water resources systems modeling hydra platform team has developed a framework of a database and web service to transfer water systems model data between a client water resources models and visualization tools harou et al 2010 knox et al 2014 2019 this paper presents an interoperability approach that integrates four independently developed active existing open source software tools for water resources systems modeling these four software frameworks are 1 the water management data model wamdam offers a defined metadata and use of controlled vocabulary cvs to enable data query and comparisons across models and datasets abdallah and rosenberg 2019 2 hydra platform allows users to encode and communicate systems modeling data over the web using a web services approach knox et al 2014 2019 3 openagua provides a web based application that uses hydra platform to let users collaboratively visualize and edit model networks rheinheimer 2020 and 4 hydroshare enables researchers to publish and discover water related datasets and modeling data tarboton et al 2014 this fragmentation of capabilities makes difficult comparisons and synthesis within or across study areas this paper identifies the common components of four existing tools for data storage web visualization and repository and relates them correctly in syntax and semantics into a software ecosystem jansen et al 2009 the software ecosystem can help modelers perform the following three key tasks i organize and store water systems modeling data with metadata and controlled vocabularies ii visualize edit and compare networks datasets and scenarios in an online application and iii publish systems modeling data with contextual metadata to enable data discovery and analysis these tasks allow modelers to engage stakeholders reproduce analyses and meet journal and funders data management requirements the software ecosystem serves both existing proprietary and novel models communities below we define three use cases that motivate and illustrate the software ecosystem three subsequent sections describe the software ecosystem components their coupling and example applications in the bear river watershed usa and the monterrey metropolitan area mexico the final sections present use case results limitations recommendations and invite community involvement to grow the software ecosystem 2 methods this section describes use cases software ecosystem coupling and applications 2 1 use cases the software ecosystem components focus on supporting three steps that modelers commonly follow to develop models i organize and store input and output model data which includes parameter values initial boundary conditions input data timeseries output time series ii visualize networks and perform exploratory data analysis and model diagnostics and iii publish data code and instructions used to run it to a public repository to enable their discovery especially for academic studies three use case general questions guide the software ecosystem work and may apply to many models answering similar or different research questions 1 how are networks defined and how their data similar and different for two models overlapping in the same study area in this use case we seek to demonstrate the ecosystem s database and online visualization functionality to select and compare data in a study area originating from different models that were developed for different purposes 2 how do water management scenarios in two different models of the same study area compare in this use case we seek to demonstrate how users can use the ecosystem s interactive experience in openagua to define scenarios in graphical user interface and then serve input data into the two different models through the wamdam database and vice versal for results to be visualized back in openagua 3 how do the values for an input data compare in two published modeling datasets for different study areas in this use case we seek to demonstrate the value of publishing modeling datasets using wamdam database into hydroshare which enables their discovery and comparative analysis these questions represent modelers needs to visualize their model networks visualize system data verify data input and engage stakeholders the questions also address modelers needs to change model input data run alternative or competing models and then visualize output data across scenarios and models presently these steps are often manual and specific to the structure of a model s data input file 2 2 software ecosystem components here we describe four existing generic open source software components that provide some of key modeling features table 1 first wamdam is a well defined data and metadata management framework with software tools to load relate and compare data for many systems modeling datasets abdallah and rosenberg 2019 the wamdam wizard is an open source desktop based software written in python that helps users load and query data from a wamdam sqlite database abdallah and rosenberg 2019 second hydra platform is an open source python based api for systems water management data hydra platform provides a web service to transfer model data used between a client application i e custom scripts and applications that need to access hydra platform and hydra platform s database hydra platform uses a generic data storage system for networks and their data but does not require metadata e g source and method of data harou et al 2010 knox et al 2014 2019 in contrast wamdam does require those metadata to correctly interpret data and wamdam also offers controlled vocabularies to relate data across models abdallah and rosenberg 2019 third openagua is a web based application for collaborative modeling and visualization of water resources systems that uses hydra platform as a part of its web services and data storage system rheinheimer 2020 openagua generically manages data for models where users can optionally add metadata fourth hydroshare is the consortium of universities for the advancement of hydrologic science inc cuahsi online collaboration environment with web services for sharing and discovering data models and code tarboton et al 2014 hydroshare requires metadata according to the dublin core metadata initiative which describe digital resources i e files such as title owner coverage in space and time hydroshare also creates a digital object identifier doi for the published resources e g modeling data so resources can be easily cited in journal publications and other documents hydroshare allows their users to make models and data publicly available online without login while openagua requires a login to access its public networks and data 2 3 software ecosystem coupling we couple the components into a software ecosystem figure 1 together the coupled components allow users to reuse components to store visualize compare publish and discover modeling data across many different models key connections are fig 1 black arrows i move data from a weap model into the wamdam database ii export data from the wamdam database and upload data to hydra platform iii exchange data between hydra platform and openagua and iv export data from hydra platform into wamdam the oppostive direction of step ii final steps include v publish data from wamdam into hydroshare and vi use jupyter notebooks to query and analyze published datasets in hydroshare below we describe the coupling of each pair of components 2 3 1 import model datasets to wamdam modelers can already organize and store their water management data in a wamdam sqlite database using the wamdam wizard the wizard supports importing modeling datasets from a generic microsoft excel importer weap models and networks using weap s api abdallah and rosenberg 2019 stream discharge time series data from the consortium of universities for the advancement of hydrologic science inc cuahsi hydrologic information systems web services or u s bureau of reclamation reservoir storage and release time series data modelers can also export gams data for optimization models or any other system models into csv files and then use the generic excel importer to import the data into a wamdam database abdallah and rosenberg 2019 the data importer reads tabular data from an excel workbook which is available here https github com wamdamproject wamdam jupyternotebooks the workbook includes spreadsheets for metadata that match wamdam s data dictionary of required and optional fields for the following tables organizations people sources methods resource types e g models object types network scenarios models links numeric values categorical values seasonal numeric values time series and multi attribute series the data importer reads and validates the provided input data and metadata and their correct relationships in the database the data importer provides informative error messages if required data is missing and the relationships between data does not match each model dataset that is imported to the wamdam database can in turn be connected to the other ecosystem components as described in the following sub sections 2 3 2 export data from wamdam to hydra platform after importing modeling data into wamdam modelers may need to move data to other tools such as hydra platform to take advantage of online client applications such as openagua wamdam and hydra platform manage data for the same shared domain of water resources systems modeling however they have different motivating use cases and thus have different designs wamdam organizes data and metadata and uses controlled vocabularies to relate terms across many models and datasets whereas hydra platform provides a generic storage and web service approach that supports connecting from custom client applications e g guis and models to couple wamdam and hydra platform we first identified and mapped the equivalent common tables and contents between wamdam and hydra platform where each of them has different terms to describe the same metadata item table 2 hydra platform handles users login information and has the concept of a project where users can collaborate on one or many networks for the same model in contrast wamdam uses controlled vocabularies to relate synonymous object types attributes and instances across models and supports the reuse of explicit metadata of sources methods organizations and people that describe data hydra platform only allows changing data values among scenarios for the same network while wamdam allows changing both the network connectivity and data values as part of scenarios thus as a remedy two scenarios in wamdam with differences in nodes and links for the same master network will be stored in hydra platform as scenarios in two separate networks finally we wrote a python script to export wamdam data to hydra platform the script is run from the wamdam wizard and i connects to a wamdam sqlite database that is previously populated with data for systems models each model may have multiple networks and each network may have many scenarios then ii under the visualize and publish tab in the wizard the user clicks openagua and fills out login credentials to their hydra platform account as managed by openagua app users then iii upload wamdam data into an existing project in hydra platform or add a new project next users iv choose a resource type in the wamdam database e g model or dataset name to visualize its network and data then users v select a network for the model and vi choose one or more scenarios inside the network finally the user clicks upload the python script calls the hydra platform web service to add a project uses a sql script to query the wamdam database for the data to populate into each equivalent hydra platform table the script then calls the add attribute add template and add network methods to add new attributes add a new resource type i e a model which includes all object types their attributes and add the network which includes all nodes and links and their scenarios 2 3 3 visualize hydra platform data in openagua once the modeling data are successfully uploaded into hydra platform they automatically become available to hydra platform client applications such as openagua that use the hydra platform api to store manage and retrieve data the openagua web api is written in python javascript and html exposing to end users a graphical user interface uses and includes a wide range of functions that extend the core data management capabilities of hydra platform openagua site rheinheimer 2020 in review rheinheimer 2020 by design any network scenario data or metadata value that is created or entered in openagua is stored in and can be accessed using hydra platform s native web service functions hydra platform manages users and login credentials openagua extends this user management capability to allow projects and networks to be publicly visible to and discoverable by any openagua user 2 3 4 export from hydra platform to wamdam modelers may also want to export data hosted in hydra platform within openagua into wamdam to publish their data in hydroshare this transfer would allow further cross model data queries and analysis that are not possible in hydra platform or openagua to transfer data from hydra platform to wamdam open the wamdam wizard and under the import data to wamdam tab click the import from openagua button the user first provides their hydra platform account credentials created through openagua to connect to the hydra platform server next the user selects a project name resource type i e model name a network scenario and a directory on the local machine to import the hydra platform data into a wamdam template excel file when the user clicks the import button the wamdam wizard script calls four main hydra platform web service functions get template get network get scenarios and get all resource data these functions pull i a list of all the object types and their attributes ii a list of nodes and links iii all the scenarios in the selected network and their metadata of start and end dates and time steps and finally iv a list of all attributes for the nodes and links and their data values supported data types are time series array numeric seasonal and descriptors each call returns a javascript object notation json result parsed and mapped to the wamdam tables and their terminology modelers can then augment the excel workbook template with additional metadata that are required by wamdam and may not be available in hydra platform such as source method people and organizations for the nodes links and data values if metadata is not known modelers can define and reuse one generic metadata item e g source to all model data in wamdam modelers also can use controlled vocabularies in the workbook template to register native terms of datasets in hydra platform which allows the terms to be queried using controlled terms abdallah and rosenberg 2019 users then use the wamdam wizard to load the imported excel workbook into a wamdam sqlite database 2 3 5 connect wamdam and hydroshare modelers are increasingly required to publish their modeling data with contextual metadata that describe its content and coverage in space and time additionally there is increasing need to provide programmatic access to read query and analyze published data the wamdam database can contain modeling data from hydra platform hydra platform compliant applications like openagua model data sets such as for a weap model or other data sources to publish wamdam sqlite files into hydroshare as a composite resource we wrote a new python script and integrated it with the wamdam wizard the script is run from the wamdam wizard harvests the dublin core metadata from wamdam database and uploads the sqlite file with the model data into hydroshare more specifically the script uses the hs restclient python rest api library that allows programmatic access to publish and query hydroshare files and metadata hydroshare development team 2020 to use the script a user should open the wamdam wizard and connect to a wamdam sqlite database that contains the systems models networks and their data the wizard verifies that sqlite file complies with the wamdam schema including all tables and fields next under the visualize and publish tab in the wizard the user clicks hydroshare the user then provides their login credentials to their hydroshare account the user specifies a title abstract and author name s for the new dataset publication finally the user clicks publish this button executes a python script that auto queries the following generic and extended metadata from the sqlite file i temporal coverage from the modeling scenario s as the minimum start and maximum end dates ii spatial coverage box from the minimum and maximum latitude and longitude for nodes in the network s iii list of resources type s i e model names unique object types and attribute controlled vocabularies if they exist iv network and scenario name s and v list of sources methods people and organizations metadata the script adds three keywords to the created hydroshare resource wamdam systems models and water management these keywords allow hydroshare users to discover the published dataset and other prior published datasets next the script calls hydroshare s createresource method to upload the wamdam sqlite file and all the above metadata into a private resource in hydroshare where users can edit metadata share the resource with other hydroshare members or make the resource public finally hydroshare creates a doi for permanent publication once the user makes the published resource public modelers can use jupyter notebooks to programmatically access any of the published wamdam sqlite databases in hydroshare using wamdam s defined schema to query analyze and potentially reuse model data 2 4 additional coupling and testing coupling wamdam hydra platform and openagua required each developer to make minor changes to their software for example we added a field named layout property in wamdam for object types to visualize a shape for each object type e g reservoir icon as a triangle we adopted openagua s object type layout that encodes the icon shape and color in scalable vector graphics svg format we added the source and method metadata fields to hydra platform which can accommodate arbitrary metadata in json format we also changed the openagua gui to display wamdam metadata fields we made four scenario properties in wamdam required opposed to optional before so scenarios could be viewed in openagua scenariostartdate scenarioenddate timestep and timestepunitcv we also added two additional fields scenarioparentname and scenariotype to the scenarios table in wamdam scenarioparentname explicitly maps scenario inheritance among scenarios as supported in hydra platform and openagua scenariotype can take one of five potential values in openagua which affects where they are shown in the interface baseline option uncertainty portfolio and results the first four scenario types are used to organize and query changed input data while the fifth is used to query results input and results scenarios are shown in different areas of the application the root scenario is the main top level input scenario from which other option or uncertainty scenarios are derived each newly defined child option or uncertainty scenario in openagua references i e reuses the identical input data of its parent portfolio scenarios indicate a group of option scenarios in openagua users can edit the input scenario data using the basic data editor users can enter new values manually the new values will be unique to the child scenario the results scenario type stores output values for a modeling scenario this scenario type is used in openagua to visualize and compare output datasets in the results explorer finally wamdam adopted the list of units used in hydra platform as a common controlled vocabulary as openagua also uses hydra platform s units these above changes allow users to send wamdam data to hydra platform and use openagua to examine and edit scenarios online and import the data back to wamdam to run a model or publish the dataset we anticipate that the coupling is the beginning of a process where each software will continue to update to improve the user experience and accommodate more diverse use cases we validated the integrity of the import and export scripts to couple software ecosystem components by uploading the bear river 2017 rosenberg 2017 water allocation weap model abdallah 2019 from wamdam to hydra platform we confirmed the upload visually in openagua and then downloaded the model dataset back into wamdam we then used the wamdam wizard scenario comparison tool to verify that both scenarios the original in wamdam and the newly downloaded scenario from hydra platform were identical and no changes were unintentionally introduced in the upload or download mappings thus modelers can now upload wamdam modeling data into hydra platform and use the specific openagua set up to visualize and edit data online users can also import models from hydra platform into wamdam run the model and publish input or output results into hydroshare to enable data discovery analysis or serve data to other models 2 5 application we illustrate the numerous benefits of the software ecosystem with three use cases that include tasks to store visualize edit publish and compare modeling data for two models that overlap in part of the bear river watershed usa and a third model for the monterrey metropolitan area mexico the models address different research questions but are used to demonstrate the generality of the ecosystem tools and to draw comparisons among the models and their data the first model is the watershed area of suitable habitat wash optimization model that allocates water to maximize watershed habitat areas for the lower bear river watershed utah portion alafifi and rosenberg 2020 the wash model uses the general algebraic modeling system gams engine which has no user interface the second model is a weap simulation model that allocates water by water right priority within the bear river watershed utah and idaho portions weap has a proprietary database and does not support data publication the third model is a water allocation model for the monterrey metropolitan area mexico rheinheimer 2020 both the wash and weap models were developed from a predecessor 2010 utah division of water resources model for the lower bear river basin that had a plain text input file and fortran computational engine which was deprecated the wash model disaggregated irrigation demands within cache valley utah while the weap model extended the model domain upstream to idaho and bear lake the monterrey metropolitan area model is stored in hydra platform within openagua with no controlled vocabulary the use cases assume a modeler has used the wamdam wizard and loaded data for the three models into a wamdam sqlite database abdallah and rosenberg 2019 the user already has accounts for hydroshare and openagua free the first use case exported the weap and wash model data for the bear river watershed from wamdam to hydra platform then openagua was used to visually compare the similarities and differences in the model networks in an online web browser fig 2 running models within the openagua platform is outside the scope of this work an openagua manuscript will demonstrate that feature as part of its unique contribution the second use case created new weap and wash model scenarios in openagua that increased and decreased annual urban water demand in cache county utah by 25 from the base demand then exported the model data to hydro platform and on to wamdam to run the models using jupyter notebooks the model runs quantified the annual percent change in unmet demand at cache county weap model and change in the suitable watershed area for aquatic flood plain and wetlands habitat for native bonneville cutthroat trout fish cottonwoods and three indicator migratory bird species with differing needs for shallow medium and deep water habitat wash model more specifically scenario data were manually input and edited online in openagua appendix a figure a1 the wamdam wizard was used to download the new scenario data back to wamdam python scripts in jupyter notebooks abdallah and rosenberg 2019 were used to query the wamdam database for each model input the new scenario data into weap using its api write the gms wash input data file execute both models and read their results and store them in wamdam next the wamdam wizard was used to export scenario results for both models from wamdam to hydra platform finally the openaqua results explorer utility was used to plot and compare the annual unmet demand across the baseline conservation and growth scenarios fig 3 the third use case compares the magnitude and seasonality of agriculture water demand for the monterrey metropolitan area mexico and the bear river watershed in utah then publishes the datasets in hydroshare fig 4 for this use case we first uploaded each model dataset to hydroshare using the wamdam wizard while users may write their own custom python script to upload datasets to hydroshare the wizard offers an important role by extracting key metadata from the models data stored in wamdam sqlite database and auto populates hydroshare metadata fields for the user see section 2 2 5 we note here that the two datasets are for input data loaded into wamdam sqlite database from two different models using different software after the datasets were uploaded to hydroshare we then used a python script in the jupyter notebooks to access and download the published sqlite files query and compare the controlled terms delivery target for logan irrigation demand site in utah and delivered flow for agriculture demand for the dr bajo rio san juan site in mexico we choose sites in both models to compare the seasonality and magnitude of irrigated agriculture in two countries since they have comparable demand for irrigation 3 results we present use case results to manage visualize edit and publish water resources modeling data and results online 3 1 use case 1 how are the networks of the weap and wash models in the bear river watershed usa similar and different comparison of the two model networks table 3 and fig 5 shows 1 the weap model for the bear river supports more water system components such as flow requirement groundwater and streamflow gage which are not explicitly supported in wash the common resource types between the models that use the same controlled vocabularies are demand and dem reservoir and v and return flow and returnflowexist in weap and wash respectively wash used the general node resource type j for any other network connection while weap is specific about the types such as river headflow or diversion outflow these results show the similarities and differences in the two models capabilities and a potential for input data reuse or transfer between them in other watersheds e g populate a new wash model from an existing weap model 2 the weap model has a larger number of object instances and covers a larger area upstream into idaho for example the weap model includes 20 demand sites within the same lower basin area compared to 11 sites for wash more specifically the weap model includes three urban demand sites for cache county logan potable north cache potable and south cache potable while wash represents all of them in one node as j3 that has a controlled term of cache county m i the reader can view these sites in openagua fig 5 3 the weap model also includes specific upstream supply and demand and storage especially bear lake top half of the screenshot in the wash model this part of the system is aggregated into a river headflow 4 the bear river weap model simulates monthly demand reliability across 40 years of dry wet and average water years compared to the wash model which focuses on maximizing the watershed area for suitable habitat within a single year thus the weap model could be useful to quantify cooperation scenarios between the utah and idaho states where downstream users in utah could store water in bear lake in wet years and use it later in dry years both the bear river weap and wash models data are published in hydroshare abdallah 2020b appendix a figure a2 3 2 use case 2 what are the differences in weap and wash model s outputs in the face of water conservation and population growth scenarios in the bear river watershed results follow the general expected trend that increased demand increases shortages while water conservation reduces shortages fig 6 there are four years 1970 1976 1993 1996 where water conservation eliminates shortages while shortages persist for the base case and increased demand scenario in dryer years e g 1987 to 1992 and 2000 to 2004 where there is not enough water to meet site demand the conservation scenario reduces the magnitude of shortages compared to the baseline scenario these results are also available online in openagua for stakeholders to view and discuss for the wash model the watershed area for suitable habitat for native vegetation birds and fish in the baseline scenario 2003 hydrologic year is estimated at 121 526 acres reducing cache county urban demand by 25 would increase the wash area by 144 acres while a 25 increase in the site s demand would decrease the wash area by 142 acres this small increase or decrease in wash area is because of the small influence of cache county urban site which represents about 18 of the total annual agriculture and urban demand in this watershed of 415 million cubic meters 336 446 acre feet these results show the potential role of targeted urban water conservation and growth in improving or degrading suitable habitat areas in the watershed one very interesting comparison between the weap and wash models is that weap estimates 8 12 17 increase in demand shortage for cache county urban site in 2003 fig 6 red box while the wash model completely satisfies demands no shortages for all three demand scenarios if the wash model could not meet the demands constraints the model would return an infeasible solution alafifi and rosenberg 2020 this discrepancy between the two models to meet demand at the cache county urban site in 2003 is likely because of the two models different spatial extents and how they aggregate and disaggregate demand sites and upstream supplies see use case 1 results 3 3 use case 3 how do the magnitude and seasonality of agriculture water demands in monterrey metropolitan area mexico and utah compare results show on average that the monthly demand target for logan irrigation site in utah is 1 8 cubic meters per second cms black squares compared to 0 15 cms demand grey circles for dr bajo rio san juan agriculture demand site in monterrey mexico fig 7 agriculture demand i e crop growth in utah extends for six months and is much shorter than the 11 month irrigation season in mexico in utah agriculture demands begin in april peak in july and end in october in mexico agriculture demands begin in december peak in april and end in october it is unclear why the mexico demand from june to october has two steps of increase and decrease the two steps may represent switching to different crops or harvesting patterns this comparison between two different models and countries was possible because the software ecosystem can move data between models and software the monterrey mexico water allocation model data can be accessed in hydroshare abdallah 2020a all of the above results can be reproduced in jupyter notebooks abdallah 2020c while a weap software license is still required to run the bear river model stakeholders can use the openagua tool to examine the already uploaded model input data and select results without needing weap license 4 discussion connecting wamdam hydra platform openagua and hydroshare into a software ecosystem allows modelers to store edit run scenarios visualize and publish online water resources systems data the software ecosystem facilitates the export of model data from one component to another to allow users to access features available in other system components together these coupled features allow modelers to compare simulation and optimization models for the same modeling domain and different domains for example the weap bear river model supports more specific demand sites within the same area compared to wash and thus weap offers more decision support analysis for each demand site the weap bear river model represents specific upstream supply and demand and storage compared to aggregated river headflows in wash thus the weap model could be useful to quantify cooperation scenarios between utah and idaho the weap bear river model includes 40 years of monthly supply data compared to a single year in this wash model and thus the weap model would be more useful to simulate water allocations and potentially unmet demand under a spectrum of historic hydrologic years from dry to wet conditions reducing urban demand in cache county in the bear river weap model by 25 would reduce unmet demand relative to the base case including in dry years the same software ecosystem tools and steps were used for a different model wash to estimate the effect of decreases in cache county urban demand by 25 finally the ecosystem enabled systematic comparisons and insights from input data originating from two different models in utah and mexico comparing utah and mexico agricultural demands from two models showed both agriculture demand sites from two different models for utah and mexico share high seasonal variability but have different growth seasons the identified variability suggests the importan0ce of water storage for both sites at different times when demand is low winter to use water later when demand is high such as spring in mexico and summer in utah 4 1 advantages of using the software ecosystem tools no model or software tool can do all data storage scenario entry visualization comparison stakeholder access and publishing tasks equally well our described software ecosystem approach allows modelers to move their data to the software component that is best suited for the data or modeling task additionally the software ecosystem allows users to construct workflows for tasks that cannot be done in any of the individual software system components for example the software ecosystem allows users to visualize and compare model data for many models and scenarios without being limited to one set of core object types and attributes as in weap or riverware users can also define model scenarios online in openagua then move data to wamdam to run the model and publish results the ecosystem tools can help compare networks for the same basin in two different modeling software these comparisons are facilitated by consistent data storage with metadata and controlled vocabularies in wamdam hydra platform and consistent visualization in openagua our software ecosystem further allows each model and dataset to retain its native terms for object types and attributes this feature allows users to view model data in openagua and support broader stakeholder engagement stakeholders can inspect modeling networks and data using a web browser without needing to install software on local machines this online setup provides users greater access to create new scenarios edit and visualize input data using the openagua interface researchers who develop novel models can use the software ecosystem components to manage their data compare scenarios and identify differences in networks input and output data without need to develop their own data management online visualization or publication features comparing datasets across novel and existing models will help researchers undertake benchmarking studies and distinguish similarities and differences in input data a new code to add another model needs to be compatible with wamdam and then it will also work with the other software tools similarly if a new model is compatible with hydra platform then the model s data can be imported into wamdam the automated publishing of water resources systems models and their data will make it easier for researchers and stakeholders to discover use reproduce extend and build new models sharing and publishing these models and datasets helps researchers fulfill data management requirements established by the national science foundation https www nsf gov eng general dmp jsp and by journals rosenberg and watkins 2018 stagge et al 2019 rosenberg et al 2020 sharing model datasets can increase the potential for their reuse reduce the time to build models and increase the value of water resources models within and outside the discipline the use of the software ecosystem products by others can be measured by a simple discovery exercise in hydroshare search resources for the keyword wamdam currently hydroshare returns six published wamdam datasets that are part of this work 4 2 limitations there is much work to do to improve the software ecosystem tools and coupling there are still metadata and software specific configurations and parameters to support the wamdam wizard is currently implemented on a local machine deploying wamdam in a cloud setting with web services coupling similar to openagua hydra platform and hydroshare would make the wamdam wizard less dependent on local computer configurations openagua is used here to visualize networks edit scenarios data and plot results but not to run models in the cloud currently openagua users can only visually search for public models and networks which will become difficult as the number of projects and networks grow with time openagua needs a search function currently data and model discovery are limited to projects and networks in openagua or sqlite databases and their metadata in hydroshare hydroshare interface does not natively support data analysis on data within multiple sqlite files it is also not user friendly to search with hydroshare for specific network scenario node link or attribute data that are contained within many published sqlite files in hydroshare reproducing model results requires running the models running models may be difficult for models that must be installed on a desktop machine require a paid license and are operating system specific the current scenario parameters in wamdam and hydra platform use simple time steps such as day month or year the software ecosystem should support time steps in posix format to account for leap years and number of days in the month that are available in weap 4 3 future work future versions of the software ecosystem should support geospatial search for individual water management infrastructure its connectivity and data this feature can be added by building on the ability to search time series data e g hydrodesktop ames et al 2012 and hydroclient http data cuahsi org wamdam support for controlled vocabularies would be particularly useful to search across different native terminology used in models and by users this functionality could allow the cuahsi web services to search for reservoir bathymetry curves seasonal demand data or network connectivity for example which demand sites are supplied by a particular reservoir both the hydra platform and openagua development teams are currently exploring ways to integrate alternative database systems to accommodate big data that can result from many scenario analyses for large water networks instead of using user login credentials to connect to the hydra platform and hydroshare servers more secure authentication methods should be implemented including api key and javascript web token jwt methods to remove user s needs to install software locally future implementations of the wamdam wizard should build read and write web services to a server based database as an online application future work should provide tools to allow users to more easily provide metadata and register their native terms with existing controlled vocabularies besides the wamdam workbook excel template future work should allow instantaneous interaction through an api between data storage in wamdam visualization in openagua and many simulation or optimization model engines further work should use the software ecosystem with more applications and models and connect additional tools to expand the ecosystem further work must also keep up with all the modifications and improvements that model developers make software by nature needs updates to keep up with recent library versions and compatibilities in future work the wamdam wizard needs to be updated to the latest libraries of python 3x distribution finally we note the need for continual alignment of development efforts to help ensure ecosystem components remain inter compatible over the long term this continual alignment requires regular communications and code transparency between component projects even as each ecosystem component is developed independently addressing this challenge will require version control and strong documentation of respective tools 4 4 invitation for community involvement and feedback we invite water modelers analysts students faculty professionals managers and other members of the water resources systems community to use the software ecosystem provide feedback and help develop new tools to expand the ecosystem to improve model communication develop and use controlled vocabularies metadata and tools to describe relate and move model data these capabilities will also help visualize and compare model inputs and outputs the current ecosystem is the product years of work and feedback we received from collaborators colleagues workshop participants and audiences since the inception of hydra platform hydroshare and wamdam in 2013 and openagua in 2016 you can participate in multiple ways such as 1 use the wamdam keyword to search and discover water systems datasets and models in hydroshare 2 use the software ecosystem tools for your existing weap model 3 use the wamdam wizard to link native vocabulary for your data sets s and model s to controlled vocabulary 4 build exporters and importers to wamdam for your own custom model or dataset and 5 build other interoperable software tools that will further your work and the work of others 5 conclusions this paper addressed the problem of using many disconnected and often model specific software tools to store visualize edit run analyze and publish systems modeling data we contributed a description prototype and demonstration of an interoperable set of open source software tools wamdam hydra platform openagua and hydroshare that help modelers to i store and organize data with metadata and controlled vocabularies in wamdam ii visualize edit and compare model networks and their input and output data in an online application openagua and iii publish systems modeling data and metadata to support data discovery and analysis in hydroshare the value of this ecosystem is in enabling modelers to move water systems models data from database to a model to an online user interface and publish it to an online data repository moving data across the coupled tools allow comparisons between models in a common platform the software ecosystem offers generalized features beyond any standalone software this prototype of coupled software ecosystem aims to help modelers spend more time on modeling and less time to develop specific tools to store visualize and publish data and models we see the software ecosystem as a complement to existing models such as weap weap has a large user base that could benefit from this software ecosystem by visualizing its models online and publishing their data in hydroshare using the wamdam consistent schema model datasets published in hydroshare can be discovered with the keyword wamdam and can be reproduced and used in follow on applications the software ecosystem is demonstrated in three use cases for two models in utah and one model in mexico the use cases compare networks for the same basin in two different modeling software set up and run multiple scenarios and models from an online portal and automate the process to share and publish model data the software ecosystem can help modelers tap the best features of individual software tools to answer a next generation of water management questions that seek to discover use reproduce extend compare or synthesize within or across study areas future work should implement all components of the coupling software online to make the interface more user friendly and support use cases for instantaneous connection between wamdam and hydra platform using the software ecosystem to move data has a learning curve as it involves familiarity with wamdam and creating two accounts with openagua and hydroshare we invite water modelers analysts students faculty professionals managers and other members of the water resources systems community to use the software ecosystem provide feedback and help develop new tools to expand the ecosystem software availability name of software the wamdam wizard developer adel m abdallah contact adel m abdallah email adel m abdallah gmail com year first available 2020 required hardware and software the wamdam wizard executable is available for use with microsoft excel 2007 and later versions and sqlite3 on windows 64 bit computers hydra platform web services are hosted by openagua which is available online on any browser or as a web service hydroshare is available online through a browser and web service input data and directions documentation of all source code datasets use cases and instructions to use the ecosystem and replicate results are available on github jupyter notebooks can be executed on a local machine and many of them can be run on the cloud using mybinder service it is assumed the user is familiar with working in a python coding environment hadia akbar at utah state university used an earlier draft of the wamdam wizard openagua hydroshare and jupyter notebooks to reproduce results for each use case jiada li at the university of utah also used the final tools and reproduced use cases results https github com wamdamproject wamdam jupyternotebooks blob master 3 visualizepublish 00 wamdam directions and use cases ipynb programming languages python and structured query language sql cost and license free software and source code are released under the new berkeley software distribution bsd 3 clause license which allows for liberal reuse declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was supported by the national science foundation through the ci water project grant 1135482 iutah grant 1208732 and utah mineral lease funds any opinions findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the national science foundation the intermountain section of the american public works association the utah chapter of the american public works association and the utah water users association provided additional financial support david tarboton and scott black provided insights into connecting wamdam with hydroshare ayman affifi provided insights on using the wash model in gams hadia akbar at utah state university used an earlier draft of the wamdam wizard openagua hydroshare and jupyter notebooks to reproduce results for each use case jiada li at the university of utah also used the final tools and reproduced use cases results we thank the three anonymous reviewers whose comments and suggestions helped improve and clarify this manuscript appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105371 
25607,our objective in this work was to present a hierarchical bayesian spatiotemporal model that allowed us to make spatial predictions of air pollution levels effectively and with very few computational costs we specified a hierarchical spatiotemporal model using the stochastic partial differential equations of the integrated nested laplace approximations approximation this approach allowed us to spatially predict in the territory of catalonia spain the levels of the four pollutants for which there is the most evidence of an adverse health effect our model allowed us to make fairly accurate spatial predictions of both long term and short term exposure to air pollutants with a relatively low density of monitoring stations and at a much lower computation time the only requirements of our method are the minimum number of stations distributed throughout the territory where the predictions are to be made and that the spatial and temporal dimensions are either independent or separable graphical abstract image 1 keywords spatial predictions hierarchical bayesian spatiotemporal model stochastic partial differential equations spde integrated nested laplace approximations inla 1 introduction in studies assessing the health effects of exposure to air pollution there is the problem of how to estimate that exposure air pollution monitoring station locations do not usually coincide with where the majority of the subjects exposed to such pollution are found in fact the air pollution monitoring stations are not often distributed homogeneously in the territory under study and it is quite usual that large areas even some densely populated ones do not have any stations at all many studies use the measurements observed in the geographical region of the study to estimate by means of point estimators the exposure levels for that entire region the estimators most widely used are the inverse distance weighted average and the arithmetic mean of the values of the air pollutant observed in several monitoring stations although sometimes the values of the pollutants observed in the nearest monitoring station are also used as estimators the problem as wannemuehler et al 2009 pointed out is that when air pollution levels exhibit spatial variation across the study region using these point estimators leads to a bias as a consequence of ignoring the spatial structure i e spatial dependence of the data furthermore when that biased estimated level is related to a health variable this leads to an underestimation of the health effect of interest wannemuehler et al 2009 while there are numerous studies that propose models to estimate the levels of air pollutants explicitly incorporating both spatial and temporal dependence cameletti et al 2011 2013 pirani et al 2013 shaddick et al 2013 liang et al 2015 2016 calculli et al 2015 cheam et al 2017 mukhopadhyay and sahu 2018 chen et al 2018 clifford et al 2019 nicolis et al 2019 wan et al 2021 to refer to only some of those that have appeared in the last ten years there are however fewer studies that attempt to predict air pollution levels in locations where there is no monitoring station i e spatial prediction and even fewer that evaluate the predictive capacity of the models they propose with no intention of supplying an exhaustive list among them we will cite the studies of cameletti et al 2011 2013 pirani et al 2013 shaddick et al 2013 mukhopadhyay and sahu 2018 nicolis et al 2019 wan et al 2021 and fiovaranti et al 2021 the spatial domain of these studies ranges from cities santiago de chile nicolis et al 2019 beijing wan et al 2021 to countries eu 15 countries shaddick et al 2013 italy fiovaranti et al 2021 through to metropolitan areas greater london pirani et al 2013 and regions po valley northern italy cameletti et al 2011 2013 england and wales mukhopadhyay and sahu 2018 the pollutants that are predicted in these studies are coarse particles pm10 with a diameter of 10 μm μm or less cameletti et al 2011 2013 pirani et al 2013 mukhopadhyay and sahu 2018 fiovaranti et al 2021 fine particles pm2 5 with a diameter of 2 5 μm or less mukhopadhyay and sahu 2018 nicolis et al 2019 wan et al 2021 nitrogen dioxide no2 shaddick et al 2013 mukhopadhyay and sahu 2018 and ozone o3 mukhopadhyay and sahu 2018 regarding the frequency at which pollutants are observed daily data cameletti et al 2011 2013 pirani et al 2013 mukhopadhyay and sahu 2018 fiovaranti et al 2021 dominate although hourly data nicolis et al 2019 wan et al 2021 and annual data shaddick et al 2013 are also used the models used in most of these articles in addition to incorporating spatial and temporal dependencies include explanatory variables among which appear in decreasing order of the number of studies meteorological variables cameletti et al 2011 2013 pirani et al 2013 shaddick et al 2013 nicolis et al 2019 wan et al 2021 fiovaranti et al 2021 other pollutants different from the one predicted cameletti et al 2011 2013 topographical variables altitude cameletti et al 2013 wan et al 2021 and distances to sea and roads shaddick et al 2013 fiovaranti et al 2021 and to mountains wan et al 2021 site types pirani et al 2013 mukhopadhyay and sahu 2018 and land use variables shaddick et al 2013 with one exception wan et al 2021 the studies use a bayesian approach because it is the one that best allows the uncertainty of complex space time data to be incorporated most of the studies that use the bayesian approach perform the inference using the monte carlo markov chain mcmc cameletti et al 2011 pirani et al 2013 shaddick et al 2013 mukhopadhyay and sahu 2018 nicolis et al 2019 only two use the stochastic partial differential equations spde representation of the inla approximation cameletti et al 2013 fiovaranti et al 2021 using mcmc implies a high computational model complexity that in some cases prevents the practical application of the methods proposed by these studies as an exception it is worth mentioning nicolis et al 2019 who use the sptimer package bakar and sahu 2015 this package which uses mcmc allows large space time data sets to be handled with fast computation and very good data processing capacity the inla approach is much more computationally effective than mcmc producing accurate approximations to posterior distributions even for very complex models lindgren and rue 2015 these few studies that provide methods for spatial prediction use a relatively large number of monitoring stations in this study however we intend to present an equally effective model that allows the use of information from a small number of monitoring stations furthermore we intend to make spatial predictions at a much lower computational cost than existing methods in fact we consider that our contributions do consist of providing a method that allows spatial predictions to be made in territories with a low density of monitoring stations and with a much shorter computational time than other alternative methods specifically our objective in this work was to present a hierarchical bayesian spatiotemporal model that allowed us to make effective spatial predictions of air pollution levels with very few computational costs in this work we used the spde representation of the inla approximation to spatially predict in the territory of catalonia spain the levels of the four pollutants for which there is the most evidence of an adverse health effect pm10 no2 o3 and pm2 5 we performed the spatial predictions at a point level defined by its utm coordinates allowing them to be aggregated later into any spatial unit required we were especially interested in the long term exposure to air pollutants that is by living in a certain area an individual is exposed to a mix of pollutants that have lasting effects on their health we also considered the performance of our method to spatially predict short term exposure to air pollutants which has more temporary effects on health 2 methods 2 1 data we obtained information on the hourly levels of air pollution for 2011 2020 from the 143 monitoring stations from the catalan network for pollution control and prevention xvpca open data departament de territori i sostenibilitat generalitat de catalunya 2021 located throughout catalonia fig s1 in supplementary material and that were active during that period the pollutants we were interested in for making spatial predictions were pm10 no2 o3 and pm2 5 all of them expressed as μm m3 air pollutants of interest hereinafter nevertheless the monitoring stations also measured other pollutants nitrogen monoxide no sulphur dioxide so2 carbon monoxide co benzene c6h6 hydrogen sulphide h2s dichloride cl2 and heavy metals mercury arsenic nickel and lead we have used these other pollutants as covariates not all pollutants of interest were measured at all the monitoring stations thus during the entire 2011 2020 period pm10 was measured at 122 stations no2 at 77 stations o3 at 62 stations and pm2 5 at 42 stations as can be seen in fig 1 most of the monitoring stations were located in the city of barcelona and in its metropolitan area in the rest of the territory the stations were located in cities especially those that measure no2 and pm2 5 and in the case of o3 also in rural areas on the other hand in 2020 which we used to spatially predict short term exposure the number of air pollution monitoring stations dropped considerably from 143 to 78 in particular the number of stations measuring particles dropped dramatically in the case of pm2 5 from 42 to 3 92 88 fewer pm10 from 122 to 36 70 49 fewer the number of stations measuring o3 went from 62 to 50 19 35 fewer stations and no2 from 77 to 67 12 99 fewer table 1 as our primary interest was in spatially predicting long term exposure to air pollutants we used the monthly averages after obtaining the daily averages from the hourly data from january 2011 to december 2019 to make the spatial predictions of the short term exposure we used the daily averages from january 1 2020 to november 29 2020 we carried out the spatial predictions at a point level with the centroids being basic health areas abs for its acronym in catalan from here on catalan health planning defines an abs as the elementary territorial unit through which primary health care services are organized atenció primària girona institut català de la salut 2021 the abss are either made up of neighbourhoods or districts in urban areas or by one or more municipalities in rural areas their delimitation is determined by geographical demographic social and epidemiological factors and in particular based on the accessibility the population has to services and the efficiency in the organization of health resources atenció primària girona 2021 catalonia is divided into 376 abss with populations between 371 and 72 321 inhabitants mean 20 266 inhabitants standard deviation 13 391 median 18 457 inhabitants first quartile q1 10 554 third quartile q3 27 529 the population density was in the range of 0 31 34 590 58 inhabitants km2 mean 3 486 36 standard deviation 6 719 23 median 309 18 q1 44 83 q3 3 752 54 in catalonia 769 of the 947 of the municipalities belong to a single abs of the 178 remaining 46 were divided into more than one abs 37 of them into a maximum of five abss eight between six and 14 abss and one the city of barcelona into 67 abss idescat 2021 less than a third of the abss have at least one air pollution monitoring station 105 from a total of 376 that is one abs has five monitoring stations six have three stations 22 have two stations and the remaining 76 have only one station as covariates we included the altitude of the air pollution monitoring station in m and the area of the abs in km2 the altitude as well as other information related to the monitoring station such as its latitude and longitude were obtained from the departament de territori i sostenibilitat 2021 we transformed the geographic coordinates latitude and longitude to utm coordinates in km using the r package rgdal bivand et al 2021 the areas of the abs as well as the utm coordinates of their centroids were calculated using qgis version 2 18 from the digitized cartography of the abs information of 2018 open data departament de salut 2021 it is known that at least in the short term exposure to air pollution is correlated with various meteorological variables for this reason in the case of spatial prediction of short term exposure we also included several meteorological variables as covariates most of them such as temperature in c relative humidity in wind speed at 10 m in m s and atmospheric pressure hpa influence the dispersion of the pollutant although some also influence its formation for instance global solar radiation w m2 o3 is a secondary pollutant formed when the two atoms that make up oxygen gas dissociate under the action of light solar the sources of the data were the stations in the network of automatic meteorological stations xema of the meteorological service of catalonia meteocat open data we also used the daily data from the state meteorological agency s aemet automatic stations the meteorological stations albeit not as much as the air pollutant monitoring stations are also dispersed throughout the territory catalonia has 217 meteorological stations 188 belonging to meteocat and 29 to aemet all of them measured all the meteorological variables every day in this case we had a spatial misalignment problem given that the meteorological station locations do not match the air pollutant monitoring locations to address this problem and along the lines of wright et al 2021 we jointly model air pollutants and meteorological variables further details can be found in barceló et al 2016 2 2 model specification we specified a hierarchical spatiotemporal model as follows at the top of the hierarchy 1 z s i t y s i t ε s i t where i denoted the air pollution monitoring station where the pollutant was observed t was the time unit s i the location of the station y the spatiotemporal process the realization of which corresponded to the pollutant measurements at station i and time unit t and ε the measurement error defined by a gaussian white noise process i e spatially and temporally uncorrelated the variance of the measurement error σ ε 2 was the nugget effect the spatiotemporal process y was an independent in time gaussian field gf with zero mean and a matérn covariance function 2 c o v η s i t η s i t σ 2 2 ν 1 γ ν κ s i s i ν κ ν κ s i s i where η denotes a spatiotemporal process κ ν is the modified bessel function of the second type and order ν 0 ν is a parameter controlling the smoothness of the gf σ 2 is the variance and κ 0 is a scaling parameter related to the range ρ the distance to which the spatial correlation becomes small we used ρ 8 ν κ where ρ corresponded to the distance where the spatial correlation is close to 0 1 for each ν lindgren et al 2011 κ 2 φ ν where φ is a parameter controlling the rate of decay of the spatial correlation as the distance s i s i increases at the next level we specified the following measurement equation 3 y s i t μ s i t η s i t where y is the realization of the spatiotemporal process y μ denoted the large scale component depending on the covariates and η was a spatiotemporal process due to its computational problems we chose to represent the gf as a gaussian markov random field gmrf rue et al 2009 gmrfs are defined by a precision matrix with a sparse structure allowing inference to be performed in a computationally effective way we linked the gf and gmrf through the stochastic partial differential equations spde approach lindgren et al 2011 the spde allowed us to find a gmrf with local neighbourhood and sparse precision matrix instead of spatiotemporal covariance function and the dense covariance matrix of a gf respectively that best represented the matérn field further details can be found in lindgren et al 2011 and in cameletti et al 2013 we specified the large scale component μ as a generalized linear mixed model glmm with response from the gaussian family specifically for each of the pollutants of interest pm10 no2 o3 and pm2 5 we specified two glmms one for long term exposure and the other for short term exposure long term exposure μ i t β 0 j 1 14 β j p o l l u t a n t j i t β 15 a l t i t u d e i β 16 a r e a i s d y i y e a r ω i τ m o n t h short term exposure μ i t β 0 j 1 14 β j p o l l u t a n t j i t β 15 t e m p e r a t u r e i t β 16 r e l a t i v e h u m i d i t y i t β 17 w i n d s p e e d i t β 18 a t m o s p h e r i c p r e s s u r e i t β 19 s o l a r r a d i a t i o n i t β 20 a l t i t u d e i β 21 a r e a i s d y i w e e k ω i τ d a y where i denoted the air pollution monitoring station where the pollutant was observed i 1 2 143 t was the time unit month in the case of long term exposure day in the case of short term exposure μ i t e y i t y i t denoted the air pollutants of interest pm10 no2 o3 and pm2 5 p o l l u t a n t j i t corresponded to the pollutant j measurements at station i and time unit t pollutants considered were first the pollutants of interest other than the pollutant for which the spatial prediction was made and second the rest of the pollutants i e no so2 co c6h6 h2s cl2 mercury arsenic nickel and lead a r e a i was the area of the abs i s d y i η i and τ denoted random effects in the models we included s d y i y e a r s d y i w e e k structured random effects indexed on a standard deviation of the air pollutant that was being predicted in the abs i during a particular year 2011 2018 and a particular week of 2020 weeks 1 37 respectively we chose a random walk of order one rw1 as the structure of the random effect in the integrated nested laplace approximations inla approach rue et al 2009 2017 the random walk of order 1 for the gaussian vector x is constructed assuming independent increments r inla project 2021a δ x i x i x i 1 n 0 σ x 2 following the inla approach when as in our case the random effects are indexed on a continuous variable such as s d y i y e a r s d y i w e e k τ m o n t h and τ d a y they can be used as smoothers to model non linear dependency on covariates in the linear predictor ω i denoted a random effect indexed on the air pollution monitoring station this random effect was unstructured independent and identically distributed random effects and captured individual heterogeneity i e unobserved confounders specific to the station and invariant in time we also included τ m o n t h and τ d a y structured random effects indexed on time in order to control the temporal dependency associated to possible seasonal effects throughout the year long term exposure and throughout the week short term exposure in this case a model for seasonal variation with periodicity m 12 for long term exposure seven for short term exposure for the random vector x1 x2 xn n m was obtained assuming that the sums were independent gaussian with a precision τ the density for x is derived from the n m 1 increments r inla project 2021b τ n m 1 2 e τ 2 x i x i 1 x i m 1 2 2 3 inference inferences for gmrfs were made following a bayesian perspective using the inla approach rue et al 2009 2017 we started from the spde representation which uses a finite element representation to define the matérn field as a linear combination of basis functions defined on a triangulation of the domain mesh hereinafter this consists of subdividing the domain into a set of non intersecting triangles meeting in at most a common edge or corner lindgren et al 2011 cameletti et al 2013 then instead of projecting the subsequent mean of the random field onto mesh nodes to target locations where we do not have observed data we performed the spatial prediction of the random field jointly with the parameter estimation process for this we projected the mesh into those locations with no air pollutants observed and then we jointly computed the posterior means at all the locations with observed and unobserved air pollutants measurements krainski et al 2020 we separately estimated each year long term exposure and each week short term exposure and then merged every year and every week we used priors that penalize complexity pc priors these priors are robust in that they do not have an impact on the results and furthermore they have an epidemiological interpretation simpson et al 2017 all analyses were carried out using the free software r version 4 0 3 through the inla package rue et al 2009 2017 r inla project 2021c the maps were represented using the leaflet package cheng et al 2019 2 4 measures of predictive performance the predictive performance of each model was assessed by cross validation considering a training set 2011 2018 for long term exposure weeks 1 36 january 1 to september 8 2020 for short term exposure and a test set 2019 for long term exposure and weeks 37 september 9 to 48 november 29 2020 for short term exposure the prediction accuracy was assessed by mean absolute percentage error mape m a p e 1 n i t y s i t y ˆ s i t y ˆ s i t 100 where n was the total number of available observations in the test set y s i t were the pollutant measurements at station i and time unit t at the test set and y ˆ s i t were the posterior means root mean square error rmse r m s e 1 n i t y s i t y ˆ s i t 2 correlation coefficient r i t y s i t y s i t y ˆ s i t y ˆ s i t i t y s i t y s i t 2 i t y ˆ s i t y ˆ s i t 2 1 2 actual coverage of the 95 prediction intervals 2 5 sensitivity analysis we conducted two sensitivity analyses first we carried out a new cross validation and then we changed the spatiotemporal model in both cases we consider the spatial prediction of long term exposure to no2 as regards cross validation we considered five random samples from the monitoring stations in which no2 was measured during the entire period 2011 2019 as training sets specifically we considered random samples of approximately 75 of the stations 58 out of a total of 77 stations of 70 55 stations 50 41 stations 45 35 stations and 20 18 stations as a test set we considered the rest of the stations 19 22 36 42 and 59 remaining stations respectively next we calculated the measures prediction accuracy explained previously with respect to the spatiotemporal model above we considered an independent in time gaussian field gf and following camelleti et al 2013 we assumed a spatiotemporal gaussian field that changes in time according to an autoregressive of order one ar 1 returning the measurement equation 2 2a y s i t μ s i t η s i t the realization of the spatiotemporal process η was specified as 4 η s i t φ η s i t 1 ω s i t where φ 1 here it was ω s i t which was assumed to be a zero mean gaussian and a matérn covariance function equation 3 in the addition in the glmm specification of the large scale component μ in the linear predictor we included structured random effects indexed on year τ y e a r to capture the long term trend μ i t β 0 j 1 14 β j p o l l u t a n t j i t β 15 a l t i t u d e i β 16 a r e a i s d y i y e a r η i τ m o n t h τ y e a r with this analysis our objective was to compare not only the predictive performance of the model 1 2 4 5 with the one specified above 1 3 but above all to compare the computation time in the inference of both specifications 3 results descriptive results are shown in table 1 regarding long term exposure we observed that apart from o3 the daily averages of pollutants decreased in 2019 pm2 5 22 39 less no2 12 88 less and pm10 8 48 less in contrast the daily average of o3 increased by 3 97 in 2019 compared to 2011 2018 with regard to short term exposure the levels of no2 and pm10 were higher from september 9 week 37 23 11 and 4 87 respectively conversely the levels of o3 and pm2 5 although in this case only measured in three stations were lower than the levels before september 9 19 34 and 9 81 when we excluded the lockdown in spain this took place from march 14 week 11 to june 21 week 25 both 2020 the variations from september 9 changed sign for pm10 it was 7 67 lower they were moderated for no2 which was 3 64 higher and o3 12 48 lower while they were increased in the case of pm2 5 13 74 lower in the supplementary material we provide some results of fitting the models for long term exposure in table s1 we show the results for the hyperparameters of the model as well as various measures of goodness of fit and complexity of the model in fig s2 the posterior distribution of the betas of the fixed effects and in fig s3 the posterior means of the fitted values of the long term exposure to air pollutants by abs vs the observed levels of the air pollutants in each abs as can be seen the fit was quite good in all cases in table 2 we show some descriptions of the spatial predictions in particular we would like to point out the symmetry of the predictions means and medians are similar in all cases and the non existence of negative values in the predictions the measures of predictive performance are shown in table 3 except for pm2 5 the results for long term exposure were quite good achieved coverages of the 95 credibility intervals for predictions were greater than 90 correlation coefficients were greater than 0 80 and mapes less than 10 furthermore if table 3 is compared with table 1 it is observed that the reduction in the variability of the spatial prediction measured between the ratio of the rmse and the standard deviations of the pollutants observed was at most one third of the standard deviations of the pollutants during the period 2011 2018 19 40 for no2 25 71 for o3 and 33 89 for pm10 again with the exception of pm2 5 the rmse in this case was 59 92 of the standard deviation in the period 2011 2018 therefore except for pm2 5 our method managed to significantly reduce the variability of the spatial prediction around fairly accurate predictions although quite good note that in relative terms the results for pm10 were somewhat worse than for gaseous pollutants no2 and o3 the poor results obtained for pm2 5 are because of its smaller sample size although it is true that in the period as a whole up to 42 stations measured pm2 5 the year with the lowest number of active stations was 2018 31 stations with the rest of the years ranging between 33 and 35 active stations no year fell below 40 active stations for the rest of pollutants the year with the lowest number of stations measuring pm10 was 2018 with 94 stations while the other years ranged between 100 and 107 stations in the cases of no2 and o3 it was 2015 with 59 and 44 stations respectively with the other years oscillating between 62 and 66 and 45 and 57 respectively regarding the short term exposure first the predictive performance was worse when we did not exclude the lockdown period which took place in spain from march 14 to june 21 2020 than when we did in fact note that the predictive performance measures were much better for gaseous pollutants no2 and o3 the results for the coarse particles pm10 were quite poor we did not interpret the results for pm2 5 as it was measured in only three stations this was likely due to the lower number of stations where pm10 was measured 36 stations versus 67 for no2 and 50 for o3 see table 1 the variability of the spatial prediction was reduced by much less than in long term exposure especially for no2 the rmses were between 36 09 for o3 and 54 86 for no2 of the standard deviations of the pollutants excluding lockdown the results of the sensitivity analyses when the number of stations in the training set was greater than 40 and when the spatiotemporal gaussian field changed in time according to an ar 1 model 1 2 4 5 were quite similar to the results for the spatiotemporal process independent in time gaussian field model 1 3 and all the stations were included in the training set table 4 when we varied the number of stations in the training set but used every year 2011 2019 the predictive performance seemed to depend on the size of the sample the more stations the training set had the better the results were in other words dramatically deteriorating with a small sample size in fact the cut off appears to be 40 stations below this the predictive performance measures were poor although the predictive performance of the spatiotemporal gaussian field model changed in time according to an ar 1 model 1 2 4 5 was very similar to that of the spatiotemporal process independent in time gaussian field model 1 3 perhaps somewhat worse in relative terms the computation time was much longer using a 6 core intel core i9 2 9 ghz 32 gb ram while the model inference 1 3 required on average 0 05 s per observation a total of 569 s on average the model 1 2 4 5 required 0 354 s a total of 3 947 s that is seven times more computing time the maps of the posterior means and the posterior standard deviations for 2019 in quintiles of the spatiotemporal process independent in time gaussian field model 1 3 for the long term exposure of pm10 no2 and o3 are shown in fig 2 we decided not to represent the posterior means for pm2 5 because of its poor predictive performance the spatial distributions of the subsequent means of pm10 and no2 were quite similar although in the high levels of no2 fourth and fifth quintiles there was somewhat more spatial variation note that unlike pm10 and no2 the lowest levels of o3 first and second quintiles occurred in the urban areas as expected the uncertainty as measured by the posterior standard deviations was in general higher in those areas with few or no monitoring stations note however that higher levels of air pollutants do not always coincide with higher standard deviations 4 discussion our results were quite good in terms of predictive performance at least for those pollutants that were observed in more than 40 collecting stations pm10 no2 and o3 in long term exposure and no2 and o3 in short term exposure the current coverage of the spatial predictions of these pollutants are in line with similar studies using the same model and the same data pm10 but applying two different methods for the inference camelleti et al find coverage between 0 95 and 0 97 using mcmc cameletti et al 2011 and 0 897 using inla spde cameletti et al 2013 mukhopadhyay and sahu 2018 find coverage between 0 91 and 0 92 for the spatial predictions for o3 in our case 0 89 for short term exposure and 0 945 for long term exposure between 0 89 and 0 90 for pm10 in our case 0 917 for long term exposure and between 0 95 and 0 965 for no2 in our case 0 905 for short term exposure and 0 963 for long term exposure note we have preferred not to comment on the results in which we found poor predictive performance our coverage could also be comparable to those provided by pirani et al 2013 for the spatial predictions for pm10 between 0 87 and 0 93 although it should be noted that these show the coverage at 90 the correlation coefficients between the observed levels of air pollutants and the subsequent means of the spatial predictions are in the range reported in fiovaranti et al 2021 for pm10 0 79 0 91 in our case they were higher than in camelleti et al 0 863 when the inferences were made with mcmc cameletti et al 2011 and 0 702 when they were made with inla spde cameletti et al 2013 compared to 0 917 in our case or than in pirani et al 2013 between 0 73 and 0 78 in both cases for pm10 however they were somewhat lower than in mukhopadhyay and sahu 2018 0 88 0 89 for pm10 0 92 0 94 for no2 and 0 93 0 94 for o3 it should be said nonetheless that the number of observations in mukhopadhyay and sahu range between 56 625 for pm10 and 100 138 for no2 while in our case we had 11 157 observations the reduction in the variability of the spatial prediction can only be compared with mukhopadhyay and sahu 2018 since they are the only ones who show these standard deviations in this sense both mukhopadhyay and sahu and ourselves achieved a similar reduction in the variability of the spatial prediction although good the results of the predictive performance were less so for the spatial prediction of long term exposure to pm10 despite being observed in the largest number of collecting stations see table 1 and for the short term exposure for gaseous pollutants no2 and o3 regarding the spatial prediction of long term exposure to pm10 we believe that it is a consequence of the location of the monitoring stations the stations that measure pm10 although more abundant in urban areas are also located in rural areas while those that measure no2 are located almost exclusively in urban areas in the city of barcelona even though 13 of no2 is generated outside the municipality it is 71 in the case of pm10 barcelona city council 2015 saez et al 2020 it is not unreasonable to suppose that these figures can be extrapolated to the entire barcelona metropolitan area which comprises 41 75 of the total population of catalonia and where the majority of pm10 and no2 monitoring stations are located in other words while no2 monitoring stations measured almost all the no2 pollution pm10 monitoring stations did not collect all the pm10 pollution data this could also explain why the posterior means of the pm10 predictions exhibited less spatial variability than the no2 predictions fig 2 a and b with regard to the spatial predictions of short term exposure the reduction in the number of monitoring stations during 2020 could have led to a deterioration in the predictive performance however we believe it could also be due to the data behaviour during 2020 as a consequence of the lockdown to flatten the covid 19 pandemic curve mobility was greatly reduced in 2020 specifically mobility was reduced by 40 on average compared to pre covid 19 levels during the lockdown and did not fully recover in the september november 2020 period being 5 15 lower depending on the area of catalonia 26 we are sure that this anomalous behaviour would have influenced the predictive performance of the spatial predictions of short term exposure the predictive performance of our model depends on the number of stations where pollutants are measured we have found that with fewer than 40 stations probably spread throughout the territory although not necessarily homogeneously the predictive performance deteriorates considerably our method is quite similar to that of camelleti et al 2013 however as we show with the sensitivity analysis our method in which we perform the inference year by year or week by week and then merge the subsequent ones has a much shorter computation time in addition to somewhat better results even for such an atypical year such as 2020 in our study we preferred to use a hierarchical bayesian model rather than for example atmospheric chemistry models there reasons why we chose our approach are two fold first and most importantly we are interested in predicting at a higher spatio temporal resolution than is usually handled by atmospheric chemistry models up to one hundredth of a degree by one hundredth of a degree and second these models do not usually take into account temporal and spatial dependencies which hierarchical bayesian models do we believe that several points warrant further investigation first we are convinced that our results might not be as good if the spatial and temporal dimensions were dependent and not separable that is if the spatial dependence varied over time fortunately the spatial dependence of air pollutants does not vary over time even during 2020 although air pollution levels decreased as a consequence of the reduction in mobility the spatial dependence was more or less similar to previous years for spatial dependence to vary over time major changes in infrastructures or likewise lasting limitations in mobility that were not homogeneous throughout the territory be produced of course other types of spatiotemporal data could imply other results spatial prediction when spatial and temporal dependencies are non separable requires other more complex methods in line with krainski s non separable space time models 2018 which are a derivation of the iterated heat equation with spatially correlated driving noise second air pollution exposure misclassification due to between area mobility and within area variation should be mitigated using for example the methods proposed by richmond bryant and long 2020 to help mitigate the impact of measurement error another problem that deserves further investigation is non stationarity in variance for this we will start from the works of ossandón et al 2021 and verdin et al 2019 who model the spatial process by adjusting a hierarchical model at each location and apply a spatial process on the betas this enables us to also model the variance spatially 5 conclusion in this work we have shown a hierarchical bayesian spatiotemporal model that has allowed us to make fairly accurate spatial predictions with a low computational cost our model provides predictions of both long term and short term exposure the only requirements of the method that we propose lie in a minimum number of stations being distributed throughout the territory where the prediction is to be made and that the spatial and temporal dimensions are either independent or separable software and data availability we used open data with free access from these sources air pollutants departament de territori i sostenibilitat generalitat de catalunya available at https analisi transparenciacatalunya cat en medi ambient qualitat de l aire als punts de mesurament autom t tasf thgu last accessed on march 14 2021 meteorological variables meteocat generalitat de catalunya meteorological data from xema available at https analisi transparenciacatalunya cat en medi ambient dades meteorol giques de la xema nzvn apee last accessed on march 14 2021 aemet aemet open data in spanish available at http www aemet es es datos abiertos aemet opendata last accessed on march 14 2021 digitized cartography of the abs departament de salut cartography available at https salutweb gencat cat ca el departament estadistiques sanitaries cartografia accessed on march 14 2021 code will be available at www researchprojects es authorship ms had the original idea for the paper and designed the study the bibliographic search and the writing of the introduction were carried out by ms and mab the methods and statistical analysis were chosen and performed by ms mab created the tables and figures ms and mab wrote the results and the discussion the writing and final editing was done by all authors ms and mab reviewed and approved the manuscript funding this work was partially financed by the supera covid19 fund from saun santander universidades crue and csic and by the covid 19 competitive grant program from pfizer global medical grants it also received funding in the form of a free transfer of data from aemet the funding sources did not participate in the design or conduct of the study the collection management analysis or interpretation of the data or the preparation review or approval of the manuscript ethics not applicable declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this study was carried out within the cohort real world data subprogram of ciber of epidemiology and public health ciberesp we appreciate the comments of three anonymous reviewers of a previous version of this work who without doubt helped us to improve our work the usual disclaimer applies appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105369 
25607,our objective in this work was to present a hierarchical bayesian spatiotemporal model that allowed us to make spatial predictions of air pollution levels effectively and with very few computational costs we specified a hierarchical spatiotemporal model using the stochastic partial differential equations of the integrated nested laplace approximations approximation this approach allowed us to spatially predict in the territory of catalonia spain the levels of the four pollutants for which there is the most evidence of an adverse health effect our model allowed us to make fairly accurate spatial predictions of both long term and short term exposure to air pollutants with a relatively low density of monitoring stations and at a much lower computation time the only requirements of our method are the minimum number of stations distributed throughout the territory where the predictions are to be made and that the spatial and temporal dimensions are either independent or separable graphical abstract image 1 keywords spatial predictions hierarchical bayesian spatiotemporal model stochastic partial differential equations spde integrated nested laplace approximations inla 1 introduction in studies assessing the health effects of exposure to air pollution there is the problem of how to estimate that exposure air pollution monitoring station locations do not usually coincide with where the majority of the subjects exposed to such pollution are found in fact the air pollution monitoring stations are not often distributed homogeneously in the territory under study and it is quite usual that large areas even some densely populated ones do not have any stations at all many studies use the measurements observed in the geographical region of the study to estimate by means of point estimators the exposure levels for that entire region the estimators most widely used are the inverse distance weighted average and the arithmetic mean of the values of the air pollutant observed in several monitoring stations although sometimes the values of the pollutants observed in the nearest monitoring station are also used as estimators the problem as wannemuehler et al 2009 pointed out is that when air pollution levels exhibit spatial variation across the study region using these point estimators leads to a bias as a consequence of ignoring the spatial structure i e spatial dependence of the data furthermore when that biased estimated level is related to a health variable this leads to an underestimation of the health effect of interest wannemuehler et al 2009 while there are numerous studies that propose models to estimate the levels of air pollutants explicitly incorporating both spatial and temporal dependence cameletti et al 2011 2013 pirani et al 2013 shaddick et al 2013 liang et al 2015 2016 calculli et al 2015 cheam et al 2017 mukhopadhyay and sahu 2018 chen et al 2018 clifford et al 2019 nicolis et al 2019 wan et al 2021 to refer to only some of those that have appeared in the last ten years there are however fewer studies that attempt to predict air pollution levels in locations where there is no monitoring station i e spatial prediction and even fewer that evaluate the predictive capacity of the models they propose with no intention of supplying an exhaustive list among them we will cite the studies of cameletti et al 2011 2013 pirani et al 2013 shaddick et al 2013 mukhopadhyay and sahu 2018 nicolis et al 2019 wan et al 2021 and fiovaranti et al 2021 the spatial domain of these studies ranges from cities santiago de chile nicolis et al 2019 beijing wan et al 2021 to countries eu 15 countries shaddick et al 2013 italy fiovaranti et al 2021 through to metropolitan areas greater london pirani et al 2013 and regions po valley northern italy cameletti et al 2011 2013 england and wales mukhopadhyay and sahu 2018 the pollutants that are predicted in these studies are coarse particles pm10 with a diameter of 10 μm μm or less cameletti et al 2011 2013 pirani et al 2013 mukhopadhyay and sahu 2018 fiovaranti et al 2021 fine particles pm2 5 with a diameter of 2 5 μm or less mukhopadhyay and sahu 2018 nicolis et al 2019 wan et al 2021 nitrogen dioxide no2 shaddick et al 2013 mukhopadhyay and sahu 2018 and ozone o3 mukhopadhyay and sahu 2018 regarding the frequency at which pollutants are observed daily data cameletti et al 2011 2013 pirani et al 2013 mukhopadhyay and sahu 2018 fiovaranti et al 2021 dominate although hourly data nicolis et al 2019 wan et al 2021 and annual data shaddick et al 2013 are also used the models used in most of these articles in addition to incorporating spatial and temporal dependencies include explanatory variables among which appear in decreasing order of the number of studies meteorological variables cameletti et al 2011 2013 pirani et al 2013 shaddick et al 2013 nicolis et al 2019 wan et al 2021 fiovaranti et al 2021 other pollutants different from the one predicted cameletti et al 2011 2013 topographical variables altitude cameletti et al 2013 wan et al 2021 and distances to sea and roads shaddick et al 2013 fiovaranti et al 2021 and to mountains wan et al 2021 site types pirani et al 2013 mukhopadhyay and sahu 2018 and land use variables shaddick et al 2013 with one exception wan et al 2021 the studies use a bayesian approach because it is the one that best allows the uncertainty of complex space time data to be incorporated most of the studies that use the bayesian approach perform the inference using the monte carlo markov chain mcmc cameletti et al 2011 pirani et al 2013 shaddick et al 2013 mukhopadhyay and sahu 2018 nicolis et al 2019 only two use the stochastic partial differential equations spde representation of the inla approximation cameletti et al 2013 fiovaranti et al 2021 using mcmc implies a high computational model complexity that in some cases prevents the practical application of the methods proposed by these studies as an exception it is worth mentioning nicolis et al 2019 who use the sptimer package bakar and sahu 2015 this package which uses mcmc allows large space time data sets to be handled with fast computation and very good data processing capacity the inla approach is much more computationally effective than mcmc producing accurate approximations to posterior distributions even for very complex models lindgren and rue 2015 these few studies that provide methods for spatial prediction use a relatively large number of monitoring stations in this study however we intend to present an equally effective model that allows the use of information from a small number of monitoring stations furthermore we intend to make spatial predictions at a much lower computational cost than existing methods in fact we consider that our contributions do consist of providing a method that allows spatial predictions to be made in territories with a low density of monitoring stations and with a much shorter computational time than other alternative methods specifically our objective in this work was to present a hierarchical bayesian spatiotemporal model that allowed us to make effective spatial predictions of air pollution levels with very few computational costs in this work we used the spde representation of the inla approximation to spatially predict in the territory of catalonia spain the levels of the four pollutants for which there is the most evidence of an adverse health effect pm10 no2 o3 and pm2 5 we performed the spatial predictions at a point level defined by its utm coordinates allowing them to be aggregated later into any spatial unit required we were especially interested in the long term exposure to air pollutants that is by living in a certain area an individual is exposed to a mix of pollutants that have lasting effects on their health we also considered the performance of our method to spatially predict short term exposure to air pollutants which has more temporary effects on health 2 methods 2 1 data we obtained information on the hourly levels of air pollution for 2011 2020 from the 143 monitoring stations from the catalan network for pollution control and prevention xvpca open data departament de territori i sostenibilitat generalitat de catalunya 2021 located throughout catalonia fig s1 in supplementary material and that were active during that period the pollutants we were interested in for making spatial predictions were pm10 no2 o3 and pm2 5 all of them expressed as μm m3 air pollutants of interest hereinafter nevertheless the monitoring stations also measured other pollutants nitrogen monoxide no sulphur dioxide so2 carbon monoxide co benzene c6h6 hydrogen sulphide h2s dichloride cl2 and heavy metals mercury arsenic nickel and lead we have used these other pollutants as covariates not all pollutants of interest were measured at all the monitoring stations thus during the entire 2011 2020 period pm10 was measured at 122 stations no2 at 77 stations o3 at 62 stations and pm2 5 at 42 stations as can be seen in fig 1 most of the monitoring stations were located in the city of barcelona and in its metropolitan area in the rest of the territory the stations were located in cities especially those that measure no2 and pm2 5 and in the case of o3 also in rural areas on the other hand in 2020 which we used to spatially predict short term exposure the number of air pollution monitoring stations dropped considerably from 143 to 78 in particular the number of stations measuring particles dropped dramatically in the case of pm2 5 from 42 to 3 92 88 fewer pm10 from 122 to 36 70 49 fewer the number of stations measuring o3 went from 62 to 50 19 35 fewer stations and no2 from 77 to 67 12 99 fewer table 1 as our primary interest was in spatially predicting long term exposure to air pollutants we used the monthly averages after obtaining the daily averages from the hourly data from january 2011 to december 2019 to make the spatial predictions of the short term exposure we used the daily averages from january 1 2020 to november 29 2020 we carried out the spatial predictions at a point level with the centroids being basic health areas abs for its acronym in catalan from here on catalan health planning defines an abs as the elementary territorial unit through which primary health care services are organized atenció primària girona institut català de la salut 2021 the abss are either made up of neighbourhoods or districts in urban areas or by one or more municipalities in rural areas their delimitation is determined by geographical demographic social and epidemiological factors and in particular based on the accessibility the population has to services and the efficiency in the organization of health resources atenció primària girona 2021 catalonia is divided into 376 abss with populations between 371 and 72 321 inhabitants mean 20 266 inhabitants standard deviation 13 391 median 18 457 inhabitants first quartile q1 10 554 third quartile q3 27 529 the population density was in the range of 0 31 34 590 58 inhabitants km2 mean 3 486 36 standard deviation 6 719 23 median 309 18 q1 44 83 q3 3 752 54 in catalonia 769 of the 947 of the municipalities belong to a single abs of the 178 remaining 46 were divided into more than one abs 37 of them into a maximum of five abss eight between six and 14 abss and one the city of barcelona into 67 abss idescat 2021 less than a third of the abss have at least one air pollution monitoring station 105 from a total of 376 that is one abs has five monitoring stations six have three stations 22 have two stations and the remaining 76 have only one station as covariates we included the altitude of the air pollution monitoring station in m and the area of the abs in km2 the altitude as well as other information related to the monitoring station such as its latitude and longitude were obtained from the departament de territori i sostenibilitat 2021 we transformed the geographic coordinates latitude and longitude to utm coordinates in km using the r package rgdal bivand et al 2021 the areas of the abs as well as the utm coordinates of their centroids were calculated using qgis version 2 18 from the digitized cartography of the abs information of 2018 open data departament de salut 2021 it is known that at least in the short term exposure to air pollution is correlated with various meteorological variables for this reason in the case of spatial prediction of short term exposure we also included several meteorological variables as covariates most of them such as temperature in c relative humidity in wind speed at 10 m in m s and atmospheric pressure hpa influence the dispersion of the pollutant although some also influence its formation for instance global solar radiation w m2 o3 is a secondary pollutant formed when the two atoms that make up oxygen gas dissociate under the action of light solar the sources of the data were the stations in the network of automatic meteorological stations xema of the meteorological service of catalonia meteocat open data we also used the daily data from the state meteorological agency s aemet automatic stations the meteorological stations albeit not as much as the air pollutant monitoring stations are also dispersed throughout the territory catalonia has 217 meteorological stations 188 belonging to meteocat and 29 to aemet all of them measured all the meteorological variables every day in this case we had a spatial misalignment problem given that the meteorological station locations do not match the air pollutant monitoring locations to address this problem and along the lines of wright et al 2021 we jointly model air pollutants and meteorological variables further details can be found in barceló et al 2016 2 2 model specification we specified a hierarchical spatiotemporal model as follows at the top of the hierarchy 1 z s i t y s i t ε s i t where i denoted the air pollution monitoring station where the pollutant was observed t was the time unit s i the location of the station y the spatiotemporal process the realization of which corresponded to the pollutant measurements at station i and time unit t and ε the measurement error defined by a gaussian white noise process i e spatially and temporally uncorrelated the variance of the measurement error σ ε 2 was the nugget effect the spatiotemporal process y was an independent in time gaussian field gf with zero mean and a matérn covariance function 2 c o v η s i t η s i t σ 2 2 ν 1 γ ν κ s i s i ν κ ν κ s i s i where η denotes a spatiotemporal process κ ν is the modified bessel function of the second type and order ν 0 ν is a parameter controlling the smoothness of the gf σ 2 is the variance and κ 0 is a scaling parameter related to the range ρ the distance to which the spatial correlation becomes small we used ρ 8 ν κ where ρ corresponded to the distance where the spatial correlation is close to 0 1 for each ν lindgren et al 2011 κ 2 φ ν where φ is a parameter controlling the rate of decay of the spatial correlation as the distance s i s i increases at the next level we specified the following measurement equation 3 y s i t μ s i t η s i t where y is the realization of the spatiotemporal process y μ denoted the large scale component depending on the covariates and η was a spatiotemporal process due to its computational problems we chose to represent the gf as a gaussian markov random field gmrf rue et al 2009 gmrfs are defined by a precision matrix with a sparse structure allowing inference to be performed in a computationally effective way we linked the gf and gmrf through the stochastic partial differential equations spde approach lindgren et al 2011 the spde allowed us to find a gmrf with local neighbourhood and sparse precision matrix instead of spatiotemporal covariance function and the dense covariance matrix of a gf respectively that best represented the matérn field further details can be found in lindgren et al 2011 and in cameletti et al 2013 we specified the large scale component μ as a generalized linear mixed model glmm with response from the gaussian family specifically for each of the pollutants of interest pm10 no2 o3 and pm2 5 we specified two glmms one for long term exposure and the other for short term exposure long term exposure μ i t β 0 j 1 14 β j p o l l u t a n t j i t β 15 a l t i t u d e i β 16 a r e a i s d y i y e a r ω i τ m o n t h short term exposure μ i t β 0 j 1 14 β j p o l l u t a n t j i t β 15 t e m p e r a t u r e i t β 16 r e l a t i v e h u m i d i t y i t β 17 w i n d s p e e d i t β 18 a t m o s p h e r i c p r e s s u r e i t β 19 s o l a r r a d i a t i o n i t β 20 a l t i t u d e i β 21 a r e a i s d y i w e e k ω i τ d a y where i denoted the air pollution monitoring station where the pollutant was observed i 1 2 143 t was the time unit month in the case of long term exposure day in the case of short term exposure μ i t e y i t y i t denoted the air pollutants of interest pm10 no2 o3 and pm2 5 p o l l u t a n t j i t corresponded to the pollutant j measurements at station i and time unit t pollutants considered were first the pollutants of interest other than the pollutant for which the spatial prediction was made and second the rest of the pollutants i e no so2 co c6h6 h2s cl2 mercury arsenic nickel and lead a r e a i was the area of the abs i s d y i η i and τ denoted random effects in the models we included s d y i y e a r s d y i w e e k structured random effects indexed on a standard deviation of the air pollutant that was being predicted in the abs i during a particular year 2011 2018 and a particular week of 2020 weeks 1 37 respectively we chose a random walk of order one rw1 as the structure of the random effect in the integrated nested laplace approximations inla approach rue et al 2009 2017 the random walk of order 1 for the gaussian vector x is constructed assuming independent increments r inla project 2021a δ x i x i x i 1 n 0 σ x 2 following the inla approach when as in our case the random effects are indexed on a continuous variable such as s d y i y e a r s d y i w e e k τ m o n t h and τ d a y they can be used as smoothers to model non linear dependency on covariates in the linear predictor ω i denoted a random effect indexed on the air pollution monitoring station this random effect was unstructured independent and identically distributed random effects and captured individual heterogeneity i e unobserved confounders specific to the station and invariant in time we also included τ m o n t h and τ d a y structured random effects indexed on time in order to control the temporal dependency associated to possible seasonal effects throughout the year long term exposure and throughout the week short term exposure in this case a model for seasonal variation with periodicity m 12 for long term exposure seven for short term exposure for the random vector x1 x2 xn n m was obtained assuming that the sums were independent gaussian with a precision τ the density for x is derived from the n m 1 increments r inla project 2021b τ n m 1 2 e τ 2 x i x i 1 x i m 1 2 2 3 inference inferences for gmrfs were made following a bayesian perspective using the inla approach rue et al 2009 2017 we started from the spde representation which uses a finite element representation to define the matérn field as a linear combination of basis functions defined on a triangulation of the domain mesh hereinafter this consists of subdividing the domain into a set of non intersecting triangles meeting in at most a common edge or corner lindgren et al 2011 cameletti et al 2013 then instead of projecting the subsequent mean of the random field onto mesh nodes to target locations where we do not have observed data we performed the spatial prediction of the random field jointly with the parameter estimation process for this we projected the mesh into those locations with no air pollutants observed and then we jointly computed the posterior means at all the locations with observed and unobserved air pollutants measurements krainski et al 2020 we separately estimated each year long term exposure and each week short term exposure and then merged every year and every week we used priors that penalize complexity pc priors these priors are robust in that they do not have an impact on the results and furthermore they have an epidemiological interpretation simpson et al 2017 all analyses were carried out using the free software r version 4 0 3 through the inla package rue et al 2009 2017 r inla project 2021c the maps were represented using the leaflet package cheng et al 2019 2 4 measures of predictive performance the predictive performance of each model was assessed by cross validation considering a training set 2011 2018 for long term exposure weeks 1 36 january 1 to september 8 2020 for short term exposure and a test set 2019 for long term exposure and weeks 37 september 9 to 48 november 29 2020 for short term exposure the prediction accuracy was assessed by mean absolute percentage error mape m a p e 1 n i t y s i t y ˆ s i t y ˆ s i t 100 where n was the total number of available observations in the test set y s i t were the pollutant measurements at station i and time unit t at the test set and y ˆ s i t were the posterior means root mean square error rmse r m s e 1 n i t y s i t y ˆ s i t 2 correlation coefficient r i t y s i t y s i t y ˆ s i t y ˆ s i t i t y s i t y s i t 2 i t y ˆ s i t y ˆ s i t 2 1 2 actual coverage of the 95 prediction intervals 2 5 sensitivity analysis we conducted two sensitivity analyses first we carried out a new cross validation and then we changed the spatiotemporal model in both cases we consider the spatial prediction of long term exposure to no2 as regards cross validation we considered five random samples from the monitoring stations in which no2 was measured during the entire period 2011 2019 as training sets specifically we considered random samples of approximately 75 of the stations 58 out of a total of 77 stations of 70 55 stations 50 41 stations 45 35 stations and 20 18 stations as a test set we considered the rest of the stations 19 22 36 42 and 59 remaining stations respectively next we calculated the measures prediction accuracy explained previously with respect to the spatiotemporal model above we considered an independent in time gaussian field gf and following camelleti et al 2013 we assumed a spatiotemporal gaussian field that changes in time according to an autoregressive of order one ar 1 returning the measurement equation 2 2a y s i t μ s i t η s i t the realization of the spatiotemporal process η was specified as 4 η s i t φ η s i t 1 ω s i t where φ 1 here it was ω s i t which was assumed to be a zero mean gaussian and a matérn covariance function equation 3 in the addition in the glmm specification of the large scale component μ in the linear predictor we included structured random effects indexed on year τ y e a r to capture the long term trend μ i t β 0 j 1 14 β j p o l l u t a n t j i t β 15 a l t i t u d e i β 16 a r e a i s d y i y e a r η i τ m o n t h τ y e a r with this analysis our objective was to compare not only the predictive performance of the model 1 2 4 5 with the one specified above 1 3 but above all to compare the computation time in the inference of both specifications 3 results descriptive results are shown in table 1 regarding long term exposure we observed that apart from o3 the daily averages of pollutants decreased in 2019 pm2 5 22 39 less no2 12 88 less and pm10 8 48 less in contrast the daily average of o3 increased by 3 97 in 2019 compared to 2011 2018 with regard to short term exposure the levels of no2 and pm10 were higher from september 9 week 37 23 11 and 4 87 respectively conversely the levels of o3 and pm2 5 although in this case only measured in three stations were lower than the levels before september 9 19 34 and 9 81 when we excluded the lockdown in spain this took place from march 14 week 11 to june 21 week 25 both 2020 the variations from september 9 changed sign for pm10 it was 7 67 lower they were moderated for no2 which was 3 64 higher and o3 12 48 lower while they were increased in the case of pm2 5 13 74 lower in the supplementary material we provide some results of fitting the models for long term exposure in table s1 we show the results for the hyperparameters of the model as well as various measures of goodness of fit and complexity of the model in fig s2 the posterior distribution of the betas of the fixed effects and in fig s3 the posterior means of the fitted values of the long term exposure to air pollutants by abs vs the observed levels of the air pollutants in each abs as can be seen the fit was quite good in all cases in table 2 we show some descriptions of the spatial predictions in particular we would like to point out the symmetry of the predictions means and medians are similar in all cases and the non existence of negative values in the predictions the measures of predictive performance are shown in table 3 except for pm2 5 the results for long term exposure were quite good achieved coverages of the 95 credibility intervals for predictions were greater than 90 correlation coefficients were greater than 0 80 and mapes less than 10 furthermore if table 3 is compared with table 1 it is observed that the reduction in the variability of the spatial prediction measured between the ratio of the rmse and the standard deviations of the pollutants observed was at most one third of the standard deviations of the pollutants during the period 2011 2018 19 40 for no2 25 71 for o3 and 33 89 for pm10 again with the exception of pm2 5 the rmse in this case was 59 92 of the standard deviation in the period 2011 2018 therefore except for pm2 5 our method managed to significantly reduce the variability of the spatial prediction around fairly accurate predictions although quite good note that in relative terms the results for pm10 were somewhat worse than for gaseous pollutants no2 and o3 the poor results obtained for pm2 5 are because of its smaller sample size although it is true that in the period as a whole up to 42 stations measured pm2 5 the year with the lowest number of active stations was 2018 31 stations with the rest of the years ranging between 33 and 35 active stations no year fell below 40 active stations for the rest of pollutants the year with the lowest number of stations measuring pm10 was 2018 with 94 stations while the other years ranged between 100 and 107 stations in the cases of no2 and o3 it was 2015 with 59 and 44 stations respectively with the other years oscillating between 62 and 66 and 45 and 57 respectively regarding the short term exposure first the predictive performance was worse when we did not exclude the lockdown period which took place in spain from march 14 to june 21 2020 than when we did in fact note that the predictive performance measures were much better for gaseous pollutants no2 and o3 the results for the coarse particles pm10 were quite poor we did not interpret the results for pm2 5 as it was measured in only three stations this was likely due to the lower number of stations where pm10 was measured 36 stations versus 67 for no2 and 50 for o3 see table 1 the variability of the spatial prediction was reduced by much less than in long term exposure especially for no2 the rmses were between 36 09 for o3 and 54 86 for no2 of the standard deviations of the pollutants excluding lockdown the results of the sensitivity analyses when the number of stations in the training set was greater than 40 and when the spatiotemporal gaussian field changed in time according to an ar 1 model 1 2 4 5 were quite similar to the results for the spatiotemporal process independent in time gaussian field model 1 3 and all the stations were included in the training set table 4 when we varied the number of stations in the training set but used every year 2011 2019 the predictive performance seemed to depend on the size of the sample the more stations the training set had the better the results were in other words dramatically deteriorating with a small sample size in fact the cut off appears to be 40 stations below this the predictive performance measures were poor although the predictive performance of the spatiotemporal gaussian field model changed in time according to an ar 1 model 1 2 4 5 was very similar to that of the spatiotemporal process independent in time gaussian field model 1 3 perhaps somewhat worse in relative terms the computation time was much longer using a 6 core intel core i9 2 9 ghz 32 gb ram while the model inference 1 3 required on average 0 05 s per observation a total of 569 s on average the model 1 2 4 5 required 0 354 s a total of 3 947 s that is seven times more computing time the maps of the posterior means and the posterior standard deviations for 2019 in quintiles of the spatiotemporal process independent in time gaussian field model 1 3 for the long term exposure of pm10 no2 and o3 are shown in fig 2 we decided not to represent the posterior means for pm2 5 because of its poor predictive performance the spatial distributions of the subsequent means of pm10 and no2 were quite similar although in the high levels of no2 fourth and fifth quintiles there was somewhat more spatial variation note that unlike pm10 and no2 the lowest levels of o3 first and second quintiles occurred in the urban areas as expected the uncertainty as measured by the posterior standard deviations was in general higher in those areas with few or no monitoring stations note however that higher levels of air pollutants do not always coincide with higher standard deviations 4 discussion our results were quite good in terms of predictive performance at least for those pollutants that were observed in more than 40 collecting stations pm10 no2 and o3 in long term exposure and no2 and o3 in short term exposure the current coverage of the spatial predictions of these pollutants are in line with similar studies using the same model and the same data pm10 but applying two different methods for the inference camelleti et al find coverage between 0 95 and 0 97 using mcmc cameletti et al 2011 and 0 897 using inla spde cameletti et al 2013 mukhopadhyay and sahu 2018 find coverage between 0 91 and 0 92 for the spatial predictions for o3 in our case 0 89 for short term exposure and 0 945 for long term exposure between 0 89 and 0 90 for pm10 in our case 0 917 for long term exposure and between 0 95 and 0 965 for no2 in our case 0 905 for short term exposure and 0 963 for long term exposure note we have preferred not to comment on the results in which we found poor predictive performance our coverage could also be comparable to those provided by pirani et al 2013 for the spatial predictions for pm10 between 0 87 and 0 93 although it should be noted that these show the coverage at 90 the correlation coefficients between the observed levels of air pollutants and the subsequent means of the spatial predictions are in the range reported in fiovaranti et al 2021 for pm10 0 79 0 91 in our case they were higher than in camelleti et al 0 863 when the inferences were made with mcmc cameletti et al 2011 and 0 702 when they were made with inla spde cameletti et al 2013 compared to 0 917 in our case or than in pirani et al 2013 between 0 73 and 0 78 in both cases for pm10 however they were somewhat lower than in mukhopadhyay and sahu 2018 0 88 0 89 for pm10 0 92 0 94 for no2 and 0 93 0 94 for o3 it should be said nonetheless that the number of observations in mukhopadhyay and sahu range between 56 625 for pm10 and 100 138 for no2 while in our case we had 11 157 observations the reduction in the variability of the spatial prediction can only be compared with mukhopadhyay and sahu 2018 since they are the only ones who show these standard deviations in this sense both mukhopadhyay and sahu and ourselves achieved a similar reduction in the variability of the spatial prediction although good the results of the predictive performance were less so for the spatial prediction of long term exposure to pm10 despite being observed in the largest number of collecting stations see table 1 and for the short term exposure for gaseous pollutants no2 and o3 regarding the spatial prediction of long term exposure to pm10 we believe that it is a consequence of the location of the monitoring stations the stations that measure pm10 although more abundant in urban areas are also located in rural areas while those that measure no2 are located almost exclusively in urban areas in the city of barcelona even though 13 of no2 is generated outside the municipality it is 71 in the case of pm10 barcelona city council 2015 saez et al 2020 it is not unreasonable to suppose that these figures can be extrapolated to the entire barcelona metropolitan area which comprises 41 75 of the total population of catalonia and where the majority of pm10 and no2 monitoring stations are located in other words while no2 monitoring stations measured almost all the no2 pollution pm10 monitoring stations did not collect all the pm10 pollution data this could also explain why the posterior means of the pm10 predictions exhibited less spatial variability than the no2 predictions fig 2 a and b with regard to the spatial predictions of short term exposure the reduction in the number of monitoring stations during 2020 could have led to a deterioration in the predictive performance however we believe it could also be due to the data behaviour during 2020 as a consequence of the lockdown to flatten the covid 19 pandemic curve mobility was greatly reduced in 2020 specifically mobility was reduced by 40 on average compared to pre covid 19 levels during the lockdown and did not fully recover in the september november 2020 period being 5 15 lower depending on the area of catalonia 26 we are sure that this anomalous behaviour would have influenced the predictive performance of the spatial predictions of short term exposure the predictive performance of our model depends on the number of stations where pollutants are measured we have found that with fewer than 40 stations probably spread throughout the territory although not necessarily homogeneously the predictive performance deteriorates considerably our method is quite similar to that of camelleti et al 2013 however as we show with the sensitivity analysis our method in which we perform the inference year by year or week by week and then merge the subsequent ones has a much shorter computation time in addition to somewhat better results even for such an atypical year such as 2020 in our study we preferred to use a hierarchical bayesian model rather than for example atmospheric chemistry models there reasons why we chose our approach are two fold first and most importantly we are interested in predicting at a higher spatio temporal resolution than is usually handled by atmospheric chemistry models up to one hundredth of a degree by one hundredth of a degree and second these models do not usually take into account temporal and spatial dependencies which hierarchical bayesian models do we believe that several points warrant further investigation first we are convinced that our results might not be as good if the spatial and temporal dimensions were dependent and not separable that is if the spatial dependence varied over time fortunately the spatial dependence of air pollutants does not vary over time even during 2020 although air pollution levels decreased as a consequence of the reduction in mobility the spatial dependence was more or less similar to previous years for spatial dependence to vary over time major changes in infrastructures or likewise lasting limitations in mobility that were not homogeneous throughout the territory be produced of course other types of spatiotemporal data could imply other results spatial prediction when spatial and temporal dependencies are non separable requires other more complex methods in line with krainski s non separable space time models 2018 which are a derivation of the iterated heat equation with spatially correlated driving noise second air pollution exposure misclassification due to between area mobility and within area variation should be mitigated using for example the methods proposed by richmond bryant and long 2020 to help mitigate the impact of measurement error another problem that deserves further investigation is non stationarity in variance for this we will start from the works of ossandón et al 2021 and verdin et al 2019 who model the spatial process by adjusting a hierarchical model at each location and apply a spatial process on the betas this enables us to also model the variance spatially 5 conclusion in this work we have shown a hierarchical bayesian spatiotemporal model that has allowed us to make fairly accurate spatial predictions with a low computational cost our model provides predictions of both long term and short term exposure the only requirements of the method that we propose lie in a minimum number of stations being distributed throughout the territory where the prediction is to be made and that the spatial and temporal dimensions are either independent or separable software and data availability we used open data with free access from these sources air pollutants departament de territori i sostenibilitat generalitat de catalunya available at https analisi transparenciacatalunya cat en medi ambient qualitat de l aire als punts de mesurament autom t tasf thgu last accessed on march 14 2021 meteorological variables meteocat generalitat de catalunya meteorological data from xema available at https analisi transparenciacatalunya cat en medi ambient dades meteorol giques de la xema nzvn apee last accessed on march 14 2021 aemet aemet open data in spanish available at http www aemet es es datos abiertos aemet opendata last accessed on march 14 2021 digitized cartography of the abs departament de salut cartography available at https salutweb gencat cat ca el departament estadistiques sanitaries cartografia accessed on march 14 2021 code will be available at www researchprojects es authorship ms had the original idea for the paper and designed the study the bibliographic search and the writing of the introduction were carried out by ms and mab the methods and statistical analysis were chosen and performed by ms mab created the tables and figures ms and mab wrote the results and the discussion the writing and final editing was done by all authors ms and mab reviewed and approved the manuscript funding this work was partially financed by the supera covid19 fund from saun santander universidades crue and csic and by the covid 19 competitive grant program from pfizer global medical grants it also received funding in the form of a free transfer of data from aemet the funding sources did not participate in the design or conduct of the study the collection management analysis or interpretation of the data or the preparation review or approval of the manuscript ethics not applicable declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this study was carried out within the cohort real world data subprogram of ciber of epidemiology and public health ciberesp we appreciate the comments of three anonymous reviewers of a previous version of this work who without doubt helped us to improve our work the usual disclaimer applies appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105369 
25608,high frequency water quality data measured by in situ sensors allowed the development of auto calibration methods for water quality simulation programs however these methods consider static parameter values which may be unrealistic for microorganism activities an alternative technique is to use data assimilation methods this paper presents a first application of a particle filter that assimilates dissolved oxygen do data into the hydro ecological model prose it demonstrates the capability of the approach for simulating do concentrations and characterizing time varying physiological properties of living communities in contrasted trophic contexts do concentrations are better estimated especially during algal bloom when phytoplankton physiological parameters match the ones reported in the literature despite the simulation capabilities related to phytoplankton further improvements related to low flow periods can still be achieved especially concerning the heterotrophic bacteria properties as well as a finer description of the biodegradable component of the organic matter flux at the system s boundaries keywords data assimilation dissolved oxygen particle filter parameter estimation water quality modeling 1 introduction since the first water quality model on dissolved oxygen do developed by streeter and phelps 1925 water quality modeling takes into account more and more complex biogeochemical cycles in river systems numerous river water quality simulation softwares have been developed over the past decades such as the qual series park and lee 2002 pelletier et al 2006 aquatox park et al 1974 1982 mike11 mike hydro river dhi 2007 2017b quasar whitehead et al 1997 rwqm shanahan et al 2001 reichert et al 2001 vanrolleghem et al 2001 riverstrahler billen et al 1994 garnier et al 1995 ruelland et al 2007 thieu et al 2009 raimonet et al 2018 and prose even et al 1998 2007b flipo et al 2004 vilmin et al 2015b complex biogeochemical processes are represented by physico chemical equations using a high number of parameters related to microorganisms activities growth respiration mortality photosynthesis etc and physical processes such as oxygen reaeration all those parameter values need to be determined as much as possible based on laboratory experiments a numerical adjustment is required for the remaining parameters this step is called calibration and is based on the comparison between the output of the forward model and measured data finally the calibrated model is validated over another period of time which ends the two step fitting procedure the calibration procedure was formerly achieved following a trial error method like the first applications of the rive model billen et al 1994 garnier et al 1995 even et al 1998 in the seine basin or with more elaborate approaches derived from learning strategies vilmin et al 2015b to reduce the bias due to user subjectivity auto calibration procedures have been implemented into a few river water quality simulation programs such as in simcat warn 1987 quasar whitehead et al 1997 qual2kw pelletier et al 2006 and mike hydro river dhi 2017a however the concept of an optimal parameter set having a biophysical meaning is questionable as illustrated by the equifinality where different parameter sets can produce equally good results beven 1989 polus et al 2011 moreover a static optimal parameter over time is not realistic for the description of microorganism activities such as phytoplankton dynamics mao et al 2009 huang et al 2013 as well as phosphorus dynamics huang and gao 2017 the recent study of wang et al 2019 confirmed this statement and demonstrated the need for a dynamic representation of the most influential parameters of water quality models in situ sensors allow for a continuous monitoring of river water quality like do concentrations measured every 15 min by the meseine network of siaap the public sewage company of the greater paris area and by the carboseine project in the seine river system groleau et al 2013 these measurements have been used to calibrate and validate the water quality program prose in the seine river system vilmin et al 2016 2018 however the simulation of do concentrations during low water periods remains difficult particularly for dry years and algal bloom periods vilmin et al 2018 an alternative approach to combine the high frequency water quality data with the water quality model is to use a data assimilation technique data assimilation methods enable to transform deterministic models into stochastic models with dynamic model parameters as recommended by reichert and mieleitner 2009 and kattwinkel and reichert 2017 for ecological and environmental research data assimilation methods have been widely applied in meteorology courtier et al 1994 1998 kalnay et al 1996 gauthier et al 2007 kleist and ide 2015 yucel et al 2015 and hydrology modeling ottlé and vidal madjar 1994 rodell et al 2004 moradkhani et al 2005a b weerts and el serafy 2006 andreadis et al 2007 salamon and feyen 2009 moradkhani et al 2012 plaza et al 2012 paiva et al 2013 vrugt et al 2013 abbaszadeh et al 2018 although the use of data assimilation based on an extended kalman filter dates back from the late 1970 s early 1980 s with improved versions of streeter phelps like i e do bod models beck and young 1976 beck 1978 whitehead 1978 whitehead et al 1981 whitehead and hornberger 1984 cosby and hornberger 1984 guo 2003 its appropriation by the freshwater community remains very limited with the advances in computational power and high frequency measurements the application of data assimilation methods recently expended in biogeochemical modeling the ensemble based kalman filter has been tested in biogeochemical oceanography modeling soetaert and gregoire 2011 simon and bertino 2012 simon et al 2012 gharamti et al 2017 yu et al 2018 for water quality modeling after having used mostly extended kalman filter bowles and grenney 1978 cosby et al 1984 ennola et al 1998 pastres et al 2003 mao et al 2009 recent studies focus on the forecast of algal bloom dynamics in river systems or lakes using ensemble based kalman filter beck and halfon 1991 huang et al 2013 kim et al 2014 page et al 2018 chen et al 2019 loos et al 2020 which became the most popular assimilation technique carrassi et al 2018 cho et al 2020 however this method assumes a gaussian distribution of the observational and simulation errors a bayesian method as the particle filter is required to evaluate forward model parameters distributions carrassi et al 2018 such an application for freshwater modeling was also recommended by huang et al 2013 and for the first time in the freshwater community implemented by wang et al 2019 on a synthetic case study the main interest of a synthetic case study is that the parameter values are prescribed so that the efficiency of the particle filter can be evaluated properly this first study on the pf applied to water quality issue demonstrated the interest of such an approach to identify community switch in river systems as well as the most important bio physical processes over time moreover the study lead to technical conclusions on the number of particles required and how to handle parameters which are useful for the real oxygen data assimilation in the current paper we present a first application of the particle filter implemented into the prose program even et al 1998 2007b flipo et al 2004 vilmin et al 2015b called hereafter prose pa wang et al 2019 assimilating high frequency measured do data the study aims at proving the capacity of particle filter to simulate do concentrations and to characterize the physiological properties of phytoplankton and heterotrophic bacteria represented by model parameters in contrasted hydrological and trophic contexts low flow high flow autotrophic heterotrophic within the seine river system a real and more complex river system compared with the synthetic case study wang et al 2019 the manuscript is organized as follows sections 2 1 and 2 2 present briefly the prose pa program and the particle filter for easy reading one can consult the previous publications for the detailed description of prose pa program and the particle filtering algorithm wang et al 2018 2019 the study area simulation period hydrological conditions model input data and available do measurements are presented in section 2 3 the simulations statistical criteria to evaluate the model performance and numerical settings are described in section 2 4 then the performances of the prose model on the simulated do concentrations with and without data assimilation framework are evaluated separately section 3 1 following the simulated do concentrations the phytoplanktonic and bacterial parameters identifiability is successively illustrated in section 3 2 the uncertainties of input bdom data biodegradable dissolved organic matters and the capacity of particle filter to identify phytoplanktonic parameters values are discussed in section 4 to finish the last section gives briefly the conclusions section 5 2 material and methods our goal being the improvement of the do concentrations in water as well as the identification of dynamic micro organisms parameters we present hereafter the tools the numerical set up and the details of the simulations 2 1 the prose pa software the prose pa software wang et al 2019 is the most recent version of the prose software even et al 1998 2007b flipo et al 2004 vilmin et al 2015b which was used extensively to study the biogeochemical functioning of the seine river system even et al 1998 2004 2007a flipo et al 2004 2007 polus et al 2011 raimonet et al 2015 vilmin et al 2015b a 2016 2018 it couples prose with a particle filtering algorithm in order to assimilate high frequency do concentrations in a river system and to characterize the most influential model parameters fig 1 prose pa is composed of three independent c libraries a hydrodynamic library a transport library and a biogeochemical library and simulates in parallel the hydro biogeochemical functioning of a river system with a bayesian data assimilation algorithm particle filter fig 1 the hydrodynamic library calculates water heights and discharges by solving the 1d shallow water equations fig 1a the transport library uses the calculated hydraulic data to simulate the advection and dispersion of both particulate and dissolved matters fig 1a the biogeochemical library c rive is based on the community centered rive model billen et al 1994 garnier et al 1995 the rive model simulates the cycles of nutrients carbon and dissolved oxygen in both the water column and an unconsolidated sediment layer fig 1b biogeochemical processes related to microorganisms such as growth mortality photosynthesis and respiration are explicitly formulated the governing equations of c rive are given in wang et al 2018 2 2 data assimilation framework 2 2 1 particle filter sequential data assimilation aims at integrating observations y t at each time step t into the forward model by conditioning the values of the state variable z t namely do concentrations and model parameters x t to the observed quantities f z t y 1 t based on the bayes theorem bayes 1763 and the markov property markov 1906 it can be summed up by the bayesian filtering equation 1 f z t y 1 t f y t z t f z t y 1 t 1 f y t y 1 t 1 f y t z t f z t z t 1 y 1 t 1 f z t 1 y 1 t 1 f y t y 1 t 1 where y 1 t represents the observation vector up to time t f z t y 1 t is the conditional probability of the state variables knowing the observations up to time t f y t z t is the probability to observe y t knowing z t namely the likelihood f z t y 1 t 1 is the conditional probability of the state variables knowing the observations up to time t 1 prior knowledge while f y t y 1 t 1 is a normalization constant that does not depend on z t therefore to condition the state variables z t to the observations up to time t we need to know how to compute the likelihood of the observations given the current values of the parameters f y t z t the term f z t y 1 t 1 is obtained by propagating f z t 1 y 1 t 1 through the forward model however there is no analytical formulation for these probability distributions particle filtering and in particular the sampling importance resampling algorithm sir doucet et al 2000 liu 2001 approximates the conditional distributions f z t y 1 t by n weighted realizations of the state variables the particles using an importance distribution proposal distribution π z t y 1 t the weight update formula for each particle eq 2 is 2 ω t i f z t i y 1 t π z t i y 1 t f y t z t i f z t i z t 1 i y 1 t 1 f z t 1 i y 1 t 1 π z t i z t 1 i y t π z t 1 i y 1 t 1 where f y t z t i is the likelihood function which indicates how y t is likely to be observed given z t i at time t typically the standard importance distribution π z t i z t 1 i y t f z t i z t 1 i y 1 t 1 is used doucet et al 2001 särkkä 2013 in this case equation 2 can be transformed to 3 ω t i f y t z t i ω t 1 i ω t i ω t i ω t i where ω t i and ω t 1 i represent the posterior and prior importance weights at time t ω t i is the normalised importance weight the filtering distribution f z t y 1 t is then approximated by the weighted particles eq 4 doucet et al 2001 4 f z t y 1 t i 1 n ω t i δ z t i where δ is the dirac function and n is the number of particles assuming that the observation error is gaussian the likelihood of each particle i is given by the pdf of the multivariate normal distribution 5 ln l y t z t i m 2 ln 2 π 1 2 ln σ 1 2 y t h z t i t σ 1 y t h z t i f y t z t i l y t z t i i 1 n l y t z t i where m is the number of monitoring stations and h denotes the observation operator which extracts the simulated dissolved oxygen concentrations at the monitoring sites from z t i to the observation y t σ is the error covariance matrix of the observations considering that the observation errors at the monitoring stations are mutually independent σ is a diagonal matrix since we assumed that the observation error follows a normal distribution the diagonal terms correspond to the variance of the measurement errors 2 2 2 resampling procedure a well known and important problem when applying particle filtering is the degeneracy of the weights which means that almost all the particles have a near zero importance weight after a simulation time and only a few particles have a high importance weight the discrete probability densities then cannot approximate adequately the filtering posterior pdf f z t y 1 t eq 4 to reduce the degeneracy effect a series of resampling methods were proposed in the literature and reviewed by li et al 2015 resampling aims at eliminating particles with small importance weights and duplicating particles with high importance weights the systematic resampling technique kitagawa 1996 moradkhani et al 2005a li et al 2015 has been implemented in prose pa the criterion for performing resampling is based on the variation of the importance weights which indicates the degree of particle degeneracy the effective sample size n eff defined by kong et al 1994 allows for monitoring the degree of particle degeneracy and is approximated by n e ff eq 6 6 n e ff 1 i 1 n ω t i 2 n e ff ranges from 1 to n the number of particles in practice the resampling procedure is performed only when n e ff falls below a user defined threshold n thres α n with α 0 3 wang et al 2019 after resampling all the importance weights are reset to 1 n as mentioned above the particles with high importance weights may be duplicated many times which results in sample impoverishment to restore the diversity of the particles a random perturbation is added to the parameter values after the resampling procedure eq 7 7 x t 1 i x t r e s a m p l i n g i η t i η t i n 0 s φ i 2 where s is a user defined perturbation proportional to the parameter ranges φ i more details about the mathematical formulations of prose pa approach and a flowchart can be found in wang et al 2019 2 3 study area and available data 2 3 1 study area and simulation period the study area starts from the upstream of paris city up to the entrance of the seine estuary at poses fig 2 this area consists of 220 km stretch of the seine river and 25 km of the marne river four tributaries of the seine river oise mauldre vaucouleurs and epte are taken into account in the model as lateral boundary conditions bearing almost 16 million inhabitants the study area is highly impacted by human activities five waste water treatment plants wwtps managed by the siaap are located in the modeled area only two major wwtps seine aval sav and seine centre sc located downstream of paris city are displayed in fig 2 sav the largest wwtp of europe treats the effluents of over 6 5 million equivalent inhabitants rocher et al 2011 it has a mean water discharge of 17 m3 s in summer which contributes to almost one third of the discharge of the seine river at the gauging station for 2011 during large rain events the water collected by the combined sewer system exceeds the treatment capacity of the wwtps and overflows directly into the seine river through many stormwater discharge pipes more than 150 these water are called csos combined sewer overflows that carry large amounts of suspended solids organic matters and nutrients into the seine river even et al 2007b the duration of the csos varies from 1 h to 21 h and 90 of the csos last less than 10 h csos induce fast do concentration drops by promoting bacterial activities fig 3 two major csos outlets are presented in fig 2 the seine river is submitted to temperate climate conditions it has a maximum discharge in winter and a minimum discharge in summer the hydro biogeochemical functioning of the seine river is simulated for the year 2011 2011 01 01 to 2011 12 31 the year 2011 is a relatively dry year with a mean daily discharge of 226 6 m3 s a maximum daily discharge of 1020 m3 s and a minimum daily discharge of 66 2 m3 s at the gauging station obtained from banque hydro database on 2020 06 23 www hydro eaufrance fr fig 3 even though algal blooms almost disappeared in the seine river garnier et al 2020 three successive algal blooms occurred in the seine river system in 2011 vilmin et al 2016 the year 2011 therefore offers a large panel of hydrological and trophic contexts which is challenging for the first test of the particle filter for mimicking a real river system 2 3 2 input data and available oxygen data as for previous published studies vilmin et al 2015b a 2016 2018 daily water quality data nutrients organic matters suspended solids bacteria biomass of the seine marne and oise rivers are provided by the public drinking water company of the paris urban area sedif they are applied as lateral and upstream boundary conditions for primary producers weekly chl a measurements provided by siaap are used as the upstream boundary condition for seine river while 15 min chl a concentrations provided by sedif define the upstream boundary condition of the marne river for smaller tributaries mauldre vaucouleurs and epte lower frequency water quality data from the national river monitoring network rcs are available river daily discharge are obtained from the national banque hydro database www hydro eaufrance fr the daily water flow and quality data of the five wwtps provided by siaap 151 csos and 26 small permanent effluents are also considered as lateral boundary conditions eight monitoring stations managed by siaap provide 15 min time step do concentrations in 2011 fig 2 and table 1 the differences in the number of measurements are related to sensor dysfunctioning four monitoring stations from rcs monitoring network provide low frequency do measurements fig 2 and table 1 these high frequency measurements are assimilated by prose pa wang et al 2019 to improve the simulation of the biogeochemical functioning of the seine river system and to characterize the most influential model parameters that were identified by wang et al 2018 beforehand 2 4 simulations and numerical setting 2 4 1 simulation types and statistical criteria to assess the performances of the particle filter a cross validation was realized that means we run 8 simulations with data assimilation for each simulation only the observed data at 7 high frequency monitoring stations are assimilated in order to predict the do concentrations at the 8th monitoring station to evaluate the potential of prose pa for the simulation of do concentrations at the four low frequency monitoring stations rcs all the high frequency observed do data are assimilated the results of a classic static parameters prose simulation without data assimilation is systematically displayed for the sake of comparison with the dynamic parameters simulation provided by the particle filter calibrated parameters of the prose model garnier et al 1995 vilmin 2014 vilmin et al 2015b are used for the simulation without data assimilation the predicted do concentrations are compared with the observations subsequently two statistical criteria namely the root mean square error rmse and the kling gupta efficiency kge kling et al 2012 are calculated to evaluate the model performances kge is composed of three terms the correlation coefficient the model bias and the coefficient of variation eq 9 kge ranges from inf to 1 essentially the closer to 1 the kge the more accurate the model 8 rmse k 1 n o b s y s i m k y o b s k 2 n o b s 9 kge 1 r 1 2 β 1 2 γ 1 2 where n obs number of observations y sim k and y obs k simulated and observed do concentrations r correlation coefficient β model bias β μ s i m μ o b s with μ the mean of the do concentrations γ coefficient of variation γ σ s i m μ s i m σ o b s μ o b s with σ the standard deviation of the do concentrations 2 4 2 parameters considered for data assimilation and numerical settings the twelve parameters considered for data assimilation in prose pa were selected based on a previous sensitivity analysis of the c rive model wang et al 2018 and on the subsequent first proof of concept of a particle filter applied to water quality issues wang et al 2019 following the recommendations of wang et al 2019 the number of particles used in this study is set to 500 the boundary conditions namely the water quality data the river discharge the water flow and quality data of the wwtps and the csos are considered perfectly known although this may be a strong assumption it allows for a first quantitative estimation of the particle filter performances for the simulation of real river systems especially the share of organic matter between its biodegradable and refractory fractions was calibrated using the forward model of vilmin et al 2018 configured with static parameters resulting in an overall better simulation of do figs 4 and 5 after calibration the bdom accounts for 47 of total dom carried by seine marne and oise rivers while this bdom portion was 30 before calibration the observation error is assumed to follow a gaussian distribution with a mean zero and a standard deviation of 10 of the measured do concentrations 0 1 y t which is commonly acknowledged by the water quality community using ekf and enkf mao et al 2009 kim et al 2014 this relatively large observation error integrates both the measurement error itself and error due to uncertainties in sensor positioning the perturbation of the model parameters after the resampling procedure is randomly generated following a normal distribution with a mean zero and a standard deviation proportional to the parameter ranges based on the previous study wang et al 2019 the value of s proportion of the parameter range in equation 7 is set to 0 1 eventually 5 min time step simulations are run using 20 processors intel r xeon r cpu e5 2640 v4 2 40 ghz the simulation with data assimilation takes 3 9 days for a 1 year simulation period 365 days 3 results do simulation and model parameter characterization 3 1 improved simulation of do concentrations with particle filter the ensemble weighted average of the simulated do concentrations and the 95 confidence intervals are calculated at the eight high frequency monitoring stations figs 4 and 5 rmses and kges are calculated for the whole year of 2011 the algal bloom periods and the low flow periods without algal bloom from 2011 07 20 to 2011 12 06 the particle filter improves the simulation of do concentrations significantly table 3 indeed rmses obtained with prose pa range from 0 71 mgo2 l to 1 32 mgo2 l while those obtained with forward model prose vary from 0 75 mgo2 l to 1 61 mgo2 l for the whole year of 2011 rmses for prose pa decrease at all monitoring stations especially at bougival 47 0 74 mgo2 l vs 1 40 mgo2 l table 3 sartrouville 33 1 14 mgo2 l vs 1 69 mgo2 l table 3 and meulan 29 0 70 mgo2 l vs 0 99 mgo2 l table 3 moreover kges obtained with prose pa are closer to 1 except for the sartrouville station 0 46 vs 0 53 at all stations the simulated do concentrations by prose pa are more correlated with the measured do concentrations see r in table 3 in terms of model bias for the full year 2011 β prose pa and prose produce similar results both prose pa and prose tend to overestimate do concentrations β 1 table 3 the maximum bias is estimated at the sartrouville station with an overestimation of 12 for prose pa and 19 for prose respectively table 3 the coefficient of variation γ explains the relative dispersion of the do concentrations around the average of do concentration the results show that prose pa evaluates more correctly the dispersion of observed do concentrations where the coefficients of variation γ are much closer to 1 table 3 except for the sartrouville 0 51 vs 0 78 and meulan 0 60 vs 0 77 stations however both prose pa and prose underestimate the amplitude of do concentrations γ 1 despite a lower kge at sartrouville station for the simulations of prose pa the results show the capacity of particle filter for predicting cross validation the do concentrations in river systems higher rmse at all stations besides improving the estimation of do concentrations the particle filter captures algae dynamics accurately table 3 algal bloom period the impact of algal blooms on do concentrations is very difficult to simulate using the forward model prose with static parameters with prose six values of kges are lower than 0 5 among which three values are negative during algal bloom period table 3 while only two kge values are below 0 5 for prose pa the simulations at the four low frequency stations show also the improvement of algae dynamics simulation fig 6 in a lesser extent prose pa also improves the simulation of do concentrations during low flow period without algal bloom but as for forward simulations with prose the do concentrations estimated with prose pa are still overestimated which is related to large uncertainties on the organic matter composition as further discussed in the discussion section 3 2 dynamic parameters characterization to evaluate the performances of prose pa on parameters characterization the temporal evolution of the posterior pdf of the model parameters are displayed in a set of image plots figs 7 and 9 high frequency oxygen data at eight stations are assimilated to characterize the most influential parameters the posterior pdfs figs 8 and 10 are estimated by weighted kernel density estimation using the normalised importance weights eq 2 the posterior pdf can then give the parameter value at which the posterior pdf has a maximum density this parameter value is called the mode of the posterior pdf which is described as mode hereafter to compare the parameter values estimated by data assimilation with those reported in the literature the average of the modes of a given parameter is calculated for specific periods 3 2 1 phytoplanktonic parameters three successive algal blooms occurred in the seine river system in 2011 vilmin et al 2016 these periods are represented by the line shaded polygons fig 7 chl a 12 μg l phytoplanktonic parameters affect the do concentrations only during algal blooms wang et al 2019 therefore the results of each algal bloom are described independently bloom of march and april first line shaded polygon from 2011 03 04 to 2011 04 30 the results illustrate that the optimal temperature for the growth of phytoplankton t opt pp increases after april fig 7 before april from 2011 03 04 to 2011 03 31 low optimal temperatures 12 c 20 c with an average mode of 15 7 c is estimated while moderate optimal temperatures 22 c 28 c with an average of 24 7 c are obtained from 2011 04 07 to 2011 04 18 prose pa identifies then two optimal temperatures representing two different physiological properties during the bloom of march and april fig 7 the bloom seems therefore divided into two phases relative high values of the respiration of maintenance r m pp ranging from 0 012 h 1 to 0 018 h 1 fig 8 are estimated in the early stage of the bloom 2011 03 04 to 2011 03 15 an average of 0 015 h 1 is obtained over this period then the mode of r m pp moves towards 0 006 h 1 fig 8 at the end of the bloom the modes vary within the parameter range fig 7 this behaviour can be observed for the light extinction coefficient by algal self shading η chla pp and the ratio of chlorophyll a to carbon chla c pp also while no clear switch of the maximum photosynthesis rate p max pp can be observed before 2011 04 15 an average of 0 39 h 1 for p max pp during this period is estimated after 2011 04 15 the maximum photosynthesis rate of phytoplankton p max pp decreases with an average of 0 27 h 1 the posterior distributions of the photosynthetic capacity α pp from 2011 03 16 to 2011 04 05 are stable with modes ranging from 0 013 h 1 µe m 2 s 1 1 to 0 016 h 1 µe m 2 s 1 1 an average of 0 0015 h 1 µe m 2 s 1 1 is calculated for this period bloom of may second line shaded polygon from 2011 05 06 to 2011 06 02 the posterior pdfs of the model parameters vary during the bloom in may fig 7 the modes fluctuate widely even if the posterior pdfs of the phytoplanktonic parameters are unstable during this period the simulated peaks of do concentrations by prose pa are well synchronized with the observed peaks which can not be correctly reproduced using prose static parameter setting figs 4 and 5 the results indicate that a time varying parameter setting is necessary to reproduce the algae dynamics as mentioned by mao et al 2009 huang et al 2013 and wang et al 2019 bloom of july third line shaded polygon from 2011 06 30 to 2011 07 13 during the bloom of july the optimal temperature for the growth of phytoplankton t opt pp is well determined with an average mode of 22 4 c fig 7 high values of r m pp and η chla pp corresponding to the maximum of the posterior densities are identified 0 015 h 1 and 0 034 m 1 μgchla l 1 1 fig 7 a switch of the phytoplanktonic properties α pp and p max pp can be observed at the end of the bloom which indicates that the particle filter is able to represent the mortality of phytoplankton linked to its senescence by a decrease in the maximum photosynthesis rate p max pp and the photosynthetic capacity α pp 3 2 2 bacterial parameters the bacterial parameters are mostly influential out of algal bloom periods and with moderate to high water temperature t 6 c wang et al 2018 prose pa identifies bacteria parameters before the bloom of march figs 9 and 10 the averages of the modes are estimated at 0 026 h 1 for the maximum growth rate μ max hb 0 29 for the bacterial growth yield y hb and 31 0 c for the optimal temperature for growth of bacteria t opt hb fig 10 2011 02 06 to 2011 02 20 between the bloom of may and the bloom of july the bacterial activities are characterized by a high maximum growth rate μ max hb 0 112 h 1 and a low bacterial growth yield y hb 0 07 fig 9 the optimal temperature for the growth of bacteria t opt pp is estimated at 20 7 c after the bloom of july before december the physiological properties of the bacteria determined by particle filter are similar to those identified between the bloom of may and the bloom of july fig 9 at the end of the simulation the bacterial properties identified by particle filter are similar to those before the bloom of march fig 9 4 discussion 4 1 possible origin of mismatches between simulation and observation during critical periods the presented rmses of 0 71 1 32 mgo2 l are relatively high cross validation when comparing the measured and simulated do concentrations table 3 as demonstrated in section 3 1 both prose pa and prose overestimate do concentrations between the bloom of may and july figs 4 and 5 2011 06 03 to 2011 06 29 at five of the eight high frequency monitoring stations colombes chatou sartrouville and in a lesser extent bougival and mericourt slighter discrepancies of the same type are also observed in september and in october during dry periods especially at sartrouville and mericourt stations those mismatches may have two origins more or less independent from one another uncertainties of bdom concentrations in input data and particle filter degeneracy 4 1 1 particule filter degeneracy periods when the mismatch between simulation and observation is large lead to the degeneracy of the particle filter section 2 2 2 meaning that during those periods of time almost all particles have a zero or small normalised importance weight the filter degeneracy is particularly important between the bloom of may and july fig 11 and in a lesser extent in september and in october during dry periods for any reason the degeneracy effect indicates that the particle filter cannot correct the simulated do concentrations by perturbing the model parameter values only the constant n eff for the three critical periods fig 11 illustrates that the differences between the simulated do concentrations and the observations are so important that all importance weights are close to zero eq 5 to maintain the functioning of the particle filter when all importance weights goes to zero the observation values at these moments are practically ignored it technically explains why high rmses occur during these critical periods as a matter of fact the fact that the particle filter degenerates during critical periods of time invalidates one of our main hypothesis that is that model inputs or boundary conditions are perfectly known and points out the need for further research dedicated to the integration of uncertainties on these variables in the assimilation framework and especially the quantification of the biodegradable fraction of the organic matter 4 1 2 description of bdom in input data during the critical periods mentioned above although almost all the biodegradable organic matter is consumed by heterotrophic bacteria the particle filter cannot mitigate the overestimation of oxygen before and after the bloom of july it reaches its limit indeed by increasing μ max hb maximal growth rate value and decreasing y hb value growth yield fig 9 high maximum growth rate μ max hb 0 112 h 1 and low bacterial growth yield y hb 0 07 increase oxygen consumption by heterotrophic bacteria and decrease do concentrations in water column but not sufficiently to reduce the gap between simulation and observation figs 4 and 5 furthermore the 95 confidence intervals of the do concentrations simulated by prose pa are very narrow during these periods see for instance at sartrouville station which indicates that the model parameters have a low influence on do concentrations it seems that the farther from an organic matter source the monitoring station is located the larger the discrepancies between simulation and observation this is true for the suresnes to sartrouville monitoring stations prose pa is able to simulate do concentrations at andresy station properly a rmse of 0 53 mgo2 l when using eight stations for data assimilation this station is located downstream of two important organic matter sources the oise river and the sav wwtp fig 2 which enrich the seine river in bdom fig 12 further downstream when all the biodegradable organic matter has been consumed see the mismatch at the mericourt station the particle filter degenerates again the overestimation of the do concentrations during the critical periods can therefore be explained by a deficit of bdom in the water column as illustrated by the longitudinal profile of mean bdom concentration from 2011 09 07 to 2011 09 27 fig 12 the bdom are shortly degraded downstream of sav wwtp and the oise river simulated bdom concentrations are less than 0 02 mgc l at meulan and mericourt stations which limits the bacterial activities fig 12 this is the reason why the simulations of prose pa and prose are similar during low flow periods without algal bloom prose pa simulates almost the same oxygen level at meulan and mericourt stations in september and october however the depletion of measured do is observed at mericourt station fig 5 it means that supplementary bdom is required to produce this depletion at mericourt station the same phenomenon can also be observed from colombes station to bougival station the simulated do concentrations varying from 7 mgo 2 l to 8 mgo 2 l are similar at colombes chatou and bougival stations from 2011 09 07 to 2011 09 27 the do concentrations are well simulated by prose pa at colombes and bougival stations at this period but the depletion of do concentrations 6 mgo 2 l fig 4 during this period are not reproduced at chatou station located at a right reach of the seine river this depletion of oxygen results from the lateral inflows containing bdom overflow of csos wwtps before chatou station this indicates that some unknown overflow sources are not considered on the right reach of the seine river before chatou station to conclude the biodegradable organic matter carried by rivers csos towards the seine river is quite uncertain and dismiss the hypothesis of perfectly known boundary conditions the uncertainties of the biodegradable portion of dom have been reported in numerous studies meyer et al 1987 servais et al 1987 1995 1999 søndergaard and middelboe 1995 wang et al 2010 he et al 2010 kang and mitchell 2013 this portion depends on the hydrological and trophic contexts of a river therefore varying temporally generally the range of bdom portion of dom appears to be 0 05 0 54 hasanyar et al 2020 the portion of bdom used in this paper is 0 47 for rivers which seems a rather good estimate most of the time but need to be refined when it leads to improper organic matter depletion 4 2 synthesis of the phytoplanktonic parameters values characterized by prose pa particle filter to interpret the identification of the phytoplanktonic parameters by prose pa particle filter the averages of the posterior pdfs modes are calculated for the algal blooms of march and july table 4 two optimal temperatures for the growth of phytoplankton t opt pp are estimated for the algal bloom of march 15 7 c vs 24 7 c respectively a t opt pp of 22 4 c is identified for the bloom of july the different physiological properties identified by prose pa reflect different phytoplanktonic communities in the seine river the algal species of the seine river have been investigated for the last 30 years by the piren seine program groleau et al 2014 revealed that the phytoplanktonic species identified in march 2011 are synedra ulna cyclotella sp nitzschia sp and the specie identified in july is nitzschia sp garnier et al 1995 highlighted that the spring algal bloom in seine river is mainly composed of a species of centric diatoms stephanodiscus hantzschii descy et al 2012 mentioned also that the stephanodiscus sp grows normally in early spring while the nitzschia sp grows at the end of spring or in summer this confirms the consistence of t opt pp values identified for the blooms of march april and july 24 7 c vs 22 4 c probably nitzschia sp moreover descy et al 2012 used 11 c and 17 c to model the different phytoplanktonic communities growing in spring stephanodiscus sp and in summer nitzschia sp for the loire river while vilmin 2014 obtained the calibrated values of 10 c and 23 c for the seine river these studies are in agreement with the observed switch of t opt phy of phytoplankton during the algal bloom of march april to summarize the parameter values identified by prose pa are in agreement with those reported by descy et al 2012 vilmin 2014 and garnier et al 1995 for some periods and with the prior knowledge of phytoplanktonic communities on seine river groleau et al 2014 the different parameter values indicate the necessity of a time varying parameter setting for simulating algal bloom dynamics in river systems properly 5 conclusions a particle filter is applied for the first time to assimilate high frequency dissolved oxygen data in river system and to characterize the physiological properties of living communities represented by model parameter values under contrasted hydrological and trophic contexts in spite of the assumption that the boundary conditions input water quality data are perfectly known this article shows that the particle filter improves significantly the simulation of the do concentrations in the seine river particularly particle filter allows a more accurate simulation of the algal bloom dynamics which is very difficult to reproduce using forward model configured with static parameter the different physiological properties of phytoplankton can be characterized using particle filter the identified phytoplanktonic parameter values during the algal blooms are consistent with the values reported in the literature and with the prior knowledge on the phytoplanktonic communities in the seine river system this paper demonstrates the efficiency of the particle filter to predict the dissolved oxygen concentrations and to capture the variability of phytoplanktonic parameters during algal blooms which cannot be described by a forward model that uses static parameters only this study demonstrates the necessity of a time varying parameter setting for simulating microorganism activities nevertheless further improvement of do concentration simulated by the particle filter can be achieved by introducing a time varying bdom content in the boundary conditions represented by rivers csos and wwtps such an advance should also improve the capability of prose pa to characterize heterotrophic bacteria properties parameters during low flow without algae bloom software availability name of software prose pa contact address nicolas flipo mines paristech fr year first available 2019 program language ansi c operating system linux software access gitlab deposit under discussion availability and cost open source licence open licence under discussion declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work is a contribution to the piren seine research program part of the french long term socio ecological research ltser site zones ateliers seine we would like to thank the data providers the high frequency oxygen observations from meseine network were provided by the public sewage company of the greater paris area siaap and the low frequency oxygen data from the national river monitoring network rcs was supplied by the drinking water company of the paris urban area sedif 
25608,high frequency water quality data measured by in situ sensors allowed the development of auto calibration methods for water quality simulation programs however these methods consider static parameter values which may be unrealistic for microorganism activities an alternative technique is to use data assimilation methods this paper presents a first application of a particle filter that assimilates dissolved oxygen do data into the hydro ecological model prose it demonstrates the capability of the approach for simulating do concentrations and characterizing time varying physiological properties of living communities in contrasted trophic contexts do concentrations are better estimated especially during algal bloom when phytoplankton physiological parameters match the ones reported in the literature despite the simulation capabilities related to phytoplankton further improvements related to low flow periods can still be achieved especially concerning the heterotrophic bacteria properties as well as a finer description of the biodegradable component of the organic matter flux at the system s boundaries keywords data assimilation dissolved oxygen particle filter parameter estimation water quality modeling 1 introduction since the first water quality model on dissolved oxygen do developed by streeter and phelps 1925 water quality modeling takes into account more and more complex biogeochemical cycles in river systems numerous river water quality simulation softwares have been developed over the past decades such as the qual series park and lee 2002 pelletier et al 2006 aquatox park et al 1974 1982 mike11 mike hydro river dhi 2007 2017b quasar whitehead et al 1997 rwqm shanahan et al 2001 reichert et al 2001 vanrolleghem et al 2001 riverstrahler billen et al 1994 garnier et al 1995 ruelland et al 2007 thieu et al 2009 raimonet et al 2018 and prose even et al 1998 2007b flipo et al 2004 vilmin et al 2015b complex biogeochemical processes are represented by physico chemical equations using a high number of parameters related to microorganisms activities growth respiration mortality photosynthesis etc and physical processes such as oxygen reaeration all those parameter values need to be determined as much as possible based on laboratory experiments a numerical adjustment is required for the remaining parameters this step is called calibration and is based on the comparison between the output of the forward model and measured data finally the calibrated model is validated over another period of time which ends the two step fitting procedure the calibration procedure was formerly achieved following a trial error method like the first applications of the rive model billen et al 1994 garnier et al 1995 even et al 1998 in the seine basin or with more elaborate approaches derived from learning strategies vilmin et al 2015b to reduce the bias due to user subjectivity auto calibration procedures have been implemented into a few river water quality simulation programs such as in simcat warn 1987 quasar whitehead et al 1997 qual2kw pelletier et al 2006 and mike hydro river dhi 2017a however the concept of an optimal parameter set having a biophysical meaning is questionable as illustrated by the equifinality where different parameter sets can produce equally good results beven 1989 polus et al 2011 moreover a static optimal parameter over time is not realistic for the description of microorganism activities such as phytoplankton dynamics mao et al 2009 huang et al 2013 as well as phosphorus dynamics huang and gao 2017 the recent study of wang et al 2019 confirmed this statement and demonstrated the need for a dynamic representation of the most influential parameters of water quality models in situ sensors allow for a continuous monitoring of river water quality like do concentrations measured every 15 min by the meseine network of siaap the public sewage company of the greater paris area and by the carboseine project in the seine river system groleau et al 2013 these measurements have been used to calibrate and validate the water quality program prose in the seine river system vilmin et al 2016 2018 however the simulation of do concentrations during low water periods remains difficult particularly for dry years and algal bloom periods vilmin et al 2018 an alternative approach to combine the high frequency water quality data with the water quality model is to use a data assimilation technique data assimilation methods enable to transform deterministic models into stochastic models with dynamic model parameters as recommended by reichert and mieleitner 2009 and kattwinkel and reichert 2017 for ecological and environmental research data assimilation methods have been widely applied in meteorology courtier et al 1994 1998 kalnay et al 1996 gauthier et al 2007 kleist and ide 2015 yucel et al 2015 and hydrology modeling ottlé and vidal madjar 1994 rodell et al 2004 moradkhani et al 2005a b weerts and el serafy 2006 andreadis et al 2007 salamon and feyen 2009 moradkhani et al 2012 plaza et al 2012 paiva et al 2013 vrugt et al 2013 abbaszadeh et al 2018 although the use of data assimilation based on an extended kalman filter dates back from the late 1970 s early 1980 s with improved versions of streeter phelps like i e do bod models beck and young 1976 beck 1978 whitehead 1978 whitehead et al 1981 whitehead and hornberger 1984 cosby and hornberger 1984 guo 2003 its appropriation by the freshwater community remains very limited with the advances in computational power and high frequency measurements the application of data assimilation methods recently expended in biogeochemical modeling the ensemble based kalman filter has been tested in biogeochemical oceanography modeling soetaert and gregoire 2011 simon and bertino 2012 simon et al 2012 gharamti et al 2017 yu et al 2018 for water quality modeling after having used mostly extended kalman filter bowles and grenney 1978 cosby et al 1984 ennola et al 1998 pastres et al 2003 mao et al 2009 recent studies focus on the forecast of algal bloom dynamics in river systems or lakes using ensemble based kalman filter beck and halfon 1991 huang et al 2013 kim et al 2014 page et al 2018 chen et al 2019 loos et al 2020 which became the most popular assimilation technique carrassi et al 2018 cho et al 2020 however this method assumes a gaussian distribution of the observational and simulation errors a bayesian method as the particle filter is required to evaluate forward model parameters distributions carrassi et al 2018 such an application for freshwater modeling was also recommended by huang et al 2013 and for the first time in the freshwater community implemented by wang et al 2019 on a synthetic case study the main interest of a synthetic case study is that the parameter values are prescribed so that the efficiency of the particle filter can be evaluated properly this first study on the pf applied to water quality issue demonstrated the interest of such an approach to identify community switch in river systems as well as the most important bio physical processes over time moreover the study lead to technical conclusions on the number of particles required and how to handle parameters which are useful for the real oxygen data assimilation in the current paper we present a first application of the particle filter implemented into the prose program even et al 1998 2007b flipo et al 2004 vilmin et al 2015b called hereafter prose pa wang et al 2019 assimilating high frequency measured do data the study aims at proving the capacity of particle filter to simulate do concentrations and to characterize the physiological properties of phytoplankton and heterotrophic bacteria represented by model parameters in contrasted hydrological and trophic contexts low flow high flow autotrophic heterotrophic within the seine river system a real and more complex river system compared with the synthetic case study wang et al 2019 the manuscript is organized as follows sections 2 1 and 2 2 present briefly the prose pa program and the particle filter for easy reading one can consult the previous publications for the detailed description of prose pa program and the particle filtering algorithm wang et al 2018 2019 the study area simulation period hydrological conditions model input data and available do measurements are presented in section 2 3 the simulations statistical criteria to evaluate the model performance and numerical settings are described in section 2 4 then the performances of the prose model on the simulated do concentrations with and without data assimilation framework are evaluated separately section 3 1 following the simulated do concentrations the phytoplanktonic and bacterial parameters identifiability is successively illustrated in section 3 2 the uncertainties of input bdom data biodegradable dissolved organic matters and the capacity of particle filter to identify phytoplanktonic parameters values are discussed in section 4 to finish the last section gives briefly the conclusions section 5 2 material and methods our goal being the improvement of the do concentrations in water as well as the identification of dynamic micro organisms parameters we present hereafter the tools the numerical set up and the details of the simulations 2 1 the prose pa software the prose pa software wang et al 2019 is the most recent version of the prose software even et al 1998 2007b flipo et al 2004 vilmin et al 2015b which was used extensively to study the biogeochemical functioning of the seine river system even et al 1998 2004 2007a flipo et al 2004 2007 polus et al 2011 raimonet et al 2015 vilmin et al 2015b a 2016 2018 it couples prose with a particle filtering algorithm in order to assimilate high frequency do concentrations in a river system and to characterize the most influential model parameters fig 1 prose pa is composed of three independent c libraries a hydrodynamic library a transport library and a biogeochemical library and simulates in parallel the hydro biogeochemical functioning of a river system with a bayesian data assimilation algorithm particle filter fig 1 the hydrodynamic library calculates water heights and discharges by solving the 1d shallow water equations fig 1a the transport library uses the calculated hydraulic data to simulate the advection and dispersion of both particulate and dissolved matters fig 1a the biogeochemical library c rive is based on the community centered rive model billen et al 1994 garnier et al 1995 the rive model simulates the cycles of nutrients carbon and dissolved oxygen in both the water column and an unconsolidated sediment layer fig 1b biogeochemical processes related to microorganisms such as growth mortality photosynthesis and respiration are explicitly formulated the governing equations of c rive are given in wang et al 2018 2 2 data assimilation framework 2 2 1 particle filter sequential data assimilation aims at integrating observations y t at each time step t into the forward model by conditioning the values of the state variable z t namely do concentrations and model parameters x t to the observed quantities f z t y 1 t based on the bayes theorem bayes 1763 and the markov property markov 1906 it can be summed up by the bayesian filtering equation 1 f z t y 1 t f y t z t f z t y 1 t 1 f y t y 1 t 1 f y t z t f z t z t 1 y 1 t 1 f z t 1 y 1 t 1 f y t y 1 t 1 where y 1 t represents the observation vector up to time t f z t y 1 t is the conditional probability of the state variables knowing the observations up to time t f y t z t is the probability to observe y t knowing z t namely the likelihood f z t y 1 t 1 is the conditional probability of the state variables knowing the observations up to time t 1 prior knowledge while f y t y 1 t 1 is a normalization constant that does not depend on z t therefore to condition the state variables z t to the observations up to time t we need to know how to compute the likelihood of the observations given the current values of the parameters f y t z t the term f z t y 1 t 1 is obtained by propagating f z t 1 y 1 t 1 through the forward model however there is no analytical formulation for these probability distributions particle filtering and in particular the sampling importance resampling algorithm sir doucet et al 2000 liu 2001 approximates the conditional distributions f z t y 1 t by n weighted realizations of the state variables the particles using an importance distribution proposal distribution π z t y 1 t the weight update formula for each particle eq 2 is 2 ω t i f z t i y 1 t π z t i y 1 t f y t z t i f z t i z t 1 i y 1 t 1 f z t 1 i y 1 t 1 π z t i z t 1 i y t π z t 1 i y 1 t 1 where f y t z t i is the likelihood function which indicates how y t is likely to be observed given z t i at time t typically the standard importance distribution π z t i z t 1 i y t f z t i z t 1 i y 1 t 1 is used doucet et al 2001 särkkä 2013 in this case equation 2 can be transformed to 3 ω t i f y t z t i ω t 1 i ω t i ω t i ω t i where ω t i and ω t 1 i represent the posterior and prior importance weights at time t ω t i is the normalised importance weight the filtering distribution f z t y 1 t is then approximated by the weighted particles eq 4 doucet et al 2001 4 f z t y 1 t i 1 n ω t i δ z t i where δ is the dirac function and n is the number of particles assuming that the observation error is gaussian the likelihood of each particle i is given by the pdf of the multivariate normal distribution 5 ln l y t z t i m 2 ln 2 π 1 2 ln σ 1 2 y t h z t i t σ 1 y t h z t i f y t z t i l y t z t i i 1 n l y t z t i where m is the number of monitoring stations and h denotes the observation operator which extracts the simulated dissolved oxygen concentrations at the monitoring sites from z t i to the observation y t σ is the error covariance matrix of the observations considering that the observation errors at the monitoring stations are mutually independent σ is a diagonal matrix since we assumed that the observation error follows a normal distribution the diagonal terms correspond to the variance of the measurement errors 2 2 2 resampling procedure a well known and important problem when applying particle filtering is the degeneracy of the weights which means that almost all the particles have a near zero importance weight after a simulation time and only a few particles have a high importance weight the discrete probability densities then cannot approximate adequately the filtering posterior pdf f z t y 1 t eq 4 to reduce the degeneracy effect a series of resampling methods were proposed in the literature and reviewed by li et al 2015 resampling aims at eliminating particles with small importance weights and duplicating particles with high importance weights the systematic resampling technique kitagawa 1996 moradkhani et al 2005a li et al 2015 has been implemented in prose pa the criterion for performing resampling is based on the variation of the importance weights which indicates the degree of particle degeneracy the effective sample size n eff defined by kong et al 1994 allows for monitoring the degree of particle degeneracy and is approximated by n e ff eq 6 6 n e ff 1 i 1 n ω t i 2 n e ff ranges from 1 to n the number of particles in practice the resampling procedure is performed only when n e ff falls below a user defined threshold n thres α n with α 0 3 wang et al 2019 after resampling all the importance weights are reset to 1 n as mentioned above the particles with high importance weights may be duplicated many times which results in sample impoverishment to restore the diversity of the particles a random perturbation is added to the parameter values after the resampling procedure eq 7 7 x t 1 i x t r e s a m p l i n g i η t i η t i n 0 s φ i 2 where s is a user defined perturbation proportional to the parameter ranges φ i more details about the mathematical formulations of prose pa approach and a flowchart can be found in wang et al 2019 2 3 study area and available data 2 3 1 study area and simulation period the study area starts from the upstream of paris city up to the entrance of the seine estuary at poses fig 2 this area consists of 220 km stretch of the seine river and 25 km of the marne river four tributaries of the seine river oise mauldre vaucouleurs and epte are taken into account in the model as lateral boundary conditions bearing almost 16 million inhabitants the study area is highly impacted by human activities five waste water treatment plants wwtps managed by the siaap are located in the modeled area only two major wwtps seine aval sav and seine centre sc located downstream of paris city are displayed in fig 2 sav the largest wwtp of europe treats the effluents of over 6 5 million equivalent inhabitants rocher et al 2011 it has a mean water discharge of 17 m3 s in summer which contributes to almost one third of the discharge of the seine river at the gauging station for 2011 during large rain events the water collected by the combined sewer system exceeds the treatment capacity of the wwtps and overflows directly into the seine river through many stormwater discharge pipes more than 150 these water are called csos combined sewer overflows that carry large amounts of suspended solids organic matters and nutrients into the seine river even et al 2007b the duration of the csos varies from 1 h to 21 h and 90 of the csos last less than 10 h csos induce fast do concentration drops by promoting bacterial activities fig 3 two major csos outlets are presented in fig 2 the seine river is submitted to temperate climate conditions it has a maximum discharge in winter and a minimum discharge in summer the hydro biogeochemical functioning of the seine river is simulated for the year 2011 2011 01 01 to 2011 12 31 the year 2011 is a relatively dry year with a mean daily discharge of 226 6 m3 s a maximum daily discharge of 1020 m3 s and a minimum daily discharge of 66 2 m3 s at the gauging station obtained from banque hydro database on 2020 06 23 www hydro eaufrance fr fig 3 even though algal blooms almost disappeared in the seine river garnier et al 2020 three successive algal blooms occurred in the seine river system in 2011 vilmin et al 2016 the year 2011 therefore offers a large panel of hydrological and trophic contexts which is challenging for the first test of the particle filter for mimicking a real river system 2 3 2 input data and available oxygen data as for previous published studies vilmin et al 2015b a 2016 2018 daily water quality data nutrients organic matters suspended solids bacteria biomass of the seine marne and oise rivers are provided by the public drinking water company of the paris urban area sedif they are applied as lateral and upstream boundary conditions for primary producers weekly chl a measurements provided by siaap are used as the upstream boundary condition for seine river while 15 min chl a concentrations provided by sedif define the upstream boundary condition of the marne river for smaller tributaries mauldre vaucouleurs and epte lower frequency water quality data from the national river monitoring network rcs are available river daily discharge are obtained from the national banque hydro database www hydro eaufrance fr the daily water flow and quality data of the five wwtps provided by siaap 151 csos and 26 small permanent effluents are also considered as lateral boundary conditions eight monitoring stations managed by siaap provide 15 min time step do concentrations in 2011 fig 2 and table 1 the differences in the number of measurements are related to sensor dysfunctioning four monitoring stations from rcs monitoring network provide low frequency do measurements fig 2 and table 1 these high frequency measurements are assimilated by prose pa wang et al 2019 to improve the simulation of the biogeochemical functioning of the seine river system and to characterize the most influential model parameters that were identified by wang et al 2018 beforehand 2 4 simulations and numerical setting 2 4 1 simulation types and statistical criteria to assess the performances of the particle filter a cross validation was realized that means we run 8 simulations with data assimilation for each simulation only the observed data at 7 high frequency monitoring stations are assimilated in order to predict the do concentrations at the 8th monitoring station to evaluate the potential of prose pa for the simulation of do concentrations at the four low frequency monitoring stations rcs all the high frequency observed do data are assimilated the results of a classic static parameters prose simulation without data assimilation is systematically displayed for the sake of comparison with the dynamic parameters simulation provided by the particle filter calibrated parameters of the prose model garnier et al 1995 vilmin 2014 vilmin et al 2015b are used for the simulation without data assimilation the predicted do concentrations are compared with the observations subsequently two statistical criteria namely the root mean square error rmse and the kling gupta efficiency kge kling et al 2012 are calculated to evaluate the model performances kge is composed of three terms the correlation coefficient the model bias and the coefficient of variation eq 9 kge ranges from inf to 1 essentially the closer to 1 the kge the more accurate the model 8 rmse k 1 n o b s y s i m k y o b s k 2 n o b s 9 kge 1 r 1 2 β 1 2 γ 1 2 where n obs number of observations y sim k and y obs k simulated and observed do concentrations r correlation coefficient β model bias β μ s i m μ o b s with μ the mean of the do concentrations γ coefficient of variation γ σ s i m μ s i m σ o b s μ o b s with σ the standard deviation of the do concentrations 2 4 2 parameters considered for data assimilation and numerical settings the twelve parameters considered for data assimilation in prose pa were selected based on a previous sensitivity analysis of the c rive model wang et al 2018 and on the subsequent first proof of concept of a particle filter applied to water quality issues wang et al 2019 following the recommendations of wang et al 2019 the number of particles used in this study is set to 500 the boundary conditions namely the water quality data the river discharge the water flow and quality data of the wwtps and the csos are considered perfectly known although this may be a strong assumption it allows for a first quantitative estimation of the particle filter performances for the simulation of real river systems especially the share of organic matter between its biodegradable and refractory fractions was calibrated using the forward model of vilmin et al 2018 configured with static parameters resulting in an overall better simulation of do figs 4 and 5 after calibration the bdom accounts for 47 of total dom carried by seine marne and oise rivers while this bdom portion was 30 before calibration the observation error is assumed to follow a gaussian distribution with a mean zero and a standard deviation of 10 of the measured do concentrations 0 1 y t which is commonly acknowledged by the water quality community using ekf and enkf mao et al 2009 kim et al 2014 this relatively large observation error integrates both the measurement error itself and error due to uncertainties in sensor positioning the perturbation of the model parameters after the resampling procedure is randomly generated following a normal distribution with a mean zero and a standard deviation proportional to the parameter ranges based on the previous study wang et al 2019 the value of s proportion of the parameter range in equation 7 is set to 0 1 eventually 5 min time step simulations are run using 20 processors intel r xeon r cpu e5 2640 v4 2 40 ghz the simulation with data assimilation takes 3 9 days for a 1 year simulation period 365 days 3 results do simulation and model parameter characterization 3 1 improved simulation of do concentrations with particle filter the ensemble weighted average of the simulated do concentrations and the 95 confidence intervals are calculated at the eight high frequency monitoring stations figs 4 and 5 rmses and kges are calculated for the whole year of 2011 the algal bloom periods and the low flow periods without algal bloom from 2011 07 20 to 2011 12 06 the particle filter improves the simulation of do concentrations significantly table 3 indeed rmses obtained with prose pa range from 0 71 mgo2 l to 1 32 mgo2 l while those obtained with forward model prose vary from 0 75 mgo2 l to 1 61 mgo2 l for the whole year of 2011 rmses for prose pa decrease at all monitoring stations especially at bougival 47 0 74 mgo2 l vs 1 40 mgo2 l table 3 sartrouville 33 1 14 mgo2 l vs 1 69 mgo2 l table 3 and meulan 29 0 70 mgo2 l vs 0 99 mgo2 l table 3 moreover kges obtained with prose pa are closer to 1 except for the sartrouville station 0 46 vs 0 53 at all stations the simulated do concentrations by prose pa are more correlated with the measured do concentrations see r in table 3 in terms of model bias for the full year 2011 β prose pa and prose produce similar results both prose pa and prose tend to overestimate do concentrations β 1 table 3 the maximum bias is estimated at the sartrouville station with an overestimation of 12 for prose pa and 19 for prose respectively table 3 the coefficient of variation γ explains the relative dispersion of the do concentrations around the average of do concentration the results show that prose pa evaluates more correctly the dispersion of observed do concentrations where the coefficients of variation γ are much closer to 1 table 3 except for the sartrouville 0 51 vs 0 78 and meulan 0 60 vs 0 77 stations however both prose pa and prose underestimate the amplitude of do concentrations γ 1 despite a lower kge at sartrouville station for the simulations of prose pa the results show the capacity of particle filter for predicting cross validation the do concentrations in river systems higher rmse at all stations besides improving the estimation of do concentrations the particle filter captures algae dynamics accurately table 3 algal bloom period the impact of algal blooms on do concentrations is very difficult to simulate using the forward model prose with static parameters with prose six values of kges are lower than 0 5 among which three values are negative during algal bloom period table 3 while only two kge values are below 0 5 for prose pa the simulations at the four low frequency stations show also the improvement of algae dynamics simulation fig 6 in a lesser extent prose pa also improves the simulation of do concentrations during low flow period without algal bloom but as for forward simulations with prose the do concentrations estimated with prose pa are still overestimated which is related to large uncertainties on the organic matter composition as further discussed in the discussion section 3 2 dynamic parameters characterization to evaluate the performances of prose pa on parameters characterization the temporal evolution of the posterior pdf of the model parameters are displayed in a set of image plots figs 7 and 9 high frequency oxygen data at eight stations are assimilated to characterize the most influential parameters the posterior pdfs figs 8 and 10 are estimated by weighted kernel density estimation using the normalised importance weights eq 2 the posterior pdf can then give the parameter value at which the posterior pdf has a maximum density this parameter value is called the mode of the posterior pdf which is described as mode hereafter to compare the parameter values estimated by data assimilation with those reported in the literature the average of the modes of a given parameter is calculated for specific periods 3 2 1 phytoplanktonic parameters three successive algal blooms occurred in the seine river system in 2011 vilmin et al 2016 these periods are represented by the line shaded polygons fig 7 chl a 12 μg l phytoplanktonic parameters affect the do concentrations only during algal blooms wang et al 2019 therefore the results of each algal bloom are described independently bloom of march and april first line shaded polygon from 2011 03 04 to 2011 04 30 the results illustrate that the optimal temperature for the growth of phytoplankton t opt pp increases after april fig 7 before april from 2011 03 04 to 2011 03 31 low optimal temperatures 12 c 20 c with an average mode of 15 7 c is estimated while moderate optimal temperatures 22 c 28 c with an average of 24 7 c are obtained from 2011 04 07 to 2011 04 18 prose pa identifies then two optimal temperatures representing two different physiological properties during the bloom of march and april fig 7 the bloom seems therefore divided into two phases relative high values of the respiration of maintenance r m pp ranging from 0 012 h 1 to 0 018 h 1 fig 8 are estimated in the early stage of the bloom 2011 03 04 to 2011 03 15 an average of 0 015 h 1 is obtained over this period then the mode of r m pp moves towards 0 006 h 1 fig 8 at the end of the bloom the modes vary within the parameter range fig 7 this behaviour can be observed for the light extinction coefficient by algal self shading η chla pp and the ratio of chlorophyll a to carbon chla c pp also while no clear switch of the maximum photosynthesis rate p max pp can be observed before 2011 04 15 an average of 0 39 h 1 for p max pp during this period is estimated after 2011 04 15 the maximum photosynthesis rate of phytoplankton p max pp decreases with an average of 0 27 h 1 the posterior distributions of the photosynthetic capacity α pp from 2011 03 16 to 2011 04 05 are stable with modes ranging from 0 013 h 1 µe m 2 s 1 1 to 0 016 h 1 µe m 2 s 1 1 an average of 0 0015 h 1 µe m 2 s 1 1 is calculated for this period bloom of may second line shaded polygon from 2011 05 06 to 2011 06 02 the posterior pdfs of the model parameters vary during the bloom in may fig 7 the modes fluctuate widely even if the posterior pdfs of the phytoplanktonic parameters are unstable during this period the simulated peaks of do concentrations by prose pa are well synchronized with the observed peaks which can not be correctly reproduced using prose static parameter setting figs 4 and 5 the results indicate that a time varying parameter setting is necessary to reproduce the algae dynamics as mentioned by mao et al 2009 huang et al 2013 and wang et al 2019 bloom of july third line shaded polygon from 2011 06 30 to 2011 07 13 during the bloom of july the optimal temperature for the growth of phytoplankton t opt pp is well determined with an average mode of 22 4 c fig 7 high values of r m pp and η chla pp corresponding to the maximum of the posterior densities are identified 0 015 h 1 and 0 034 m 1 μgchla l 1 1 fig 7 a switch of the phytoplanktonic properties α pp and p max pp can be observed at the end of the bloom which indicates that the particle filter is able to represent the mortality of phytoplankton linked to its senescence by a decrease in the maximum photosynthesis rate p max pp and the photosynthetic capacity α pp 3 2 2 bacterial parameters the bacterial parameters are mostly influential out of algal bloom periods and with moderate to high water temperature t 6 c wang et al 2018 prose pa identifies bacteria parameters before the bloom of march figs 9 and 10 the averages of the modes are estimated at 0 026 h 1 for the maximum growth rate μ max hb 0 29 for the bacterial growth yield y hb and 31 0 c for the optimal temperature for growth of bacteria t opt hb fig 10 2011 02 06 to 2011 02 20 between the bloom of may and the bloom of july the bacterial activities are characterized by a high maximum growth rate μ max hb 0 112 h 1 and a low bacterial growth yield y hb 0 07 fig 9 the optimal temperature for the growth of bacteria t opt pp is estimated at 20 7 c after the bloom of july before december the physiological properties of the bacteria determined by particle filter are similar to those identified between the bloom of may and the bloom of july fig 9 at the end of the simulation the bacterial properties identified by particle filter are similar to those before the bloom of march fig 9 4 discussion 4 1 possible origin of mismatches between simulation and observation during critical periods the presented rmses of 0 71 1 32 mgo2 l are relatively high cross validation when comparing the measured and simulated do concentrations table 3 as demonstrated in section 3 1 both prose pa and prose overestimate do concentrations between the bloom of may and july figs 4 and 5 2011 06 03 to 2011 06 29 at five of the eight high frequency monitoring stations colombes chatou sartrouville and in a lesser extent bougival and mericourt slighter discrepancies of the same type are also observed in september and in october during dry periods especially at sartrouville and mericourt stations those mismatches may have two origins more or less independent from one another uncertainties of bdom concentrations in input data and particle filter degeneracy 4 1 1 particule filter degeneracy periods when the mismatch between simulation and observation is large lead to the degeneracy of the particle filter section 2 2 2 meaning that during those periods of time almost all particles have a zero or small normalised importance weight the filter degeneracy is particularly important between the bloom of may and july fig 11 and in a lesser extent in september and in october during dry periods for any reason the degeneracy effect indicates that the particle filter cannot correct the simulated do concentrations by perturbing the model parameter values only the constant n eff for the three critical periods fig 11 illustrates that the differences between the simulated do concentrations and the observations are so important that all importance weights are close to zero eq 5 to maintain the functioning of the particle filter when all importance weights goes to zero the observation values at these moments are practically ignored it technically explains why high rmses occur during these critical periods as a matter of fact the fact that the particle filter degenerates during critical periods of time invalidates one of our main hypothesis that is that model inputs or boundary conditions are perfectly known and points out the need for further research dedicated to the integration of uncertainties on these variables in the assimilation framework and especially the quantification of the biodegradable fraction of the organic matter 4 1 2 description of bdom in input data during the critical periods mentioned above although almost all the biodegradable organic matter is consumed by heterotrophic bacteria the particle filter cannot mitigate the overestimation of oxygen before and after the bloom of july it reaches its limit indeed by increasing μ max hb maximal growth rate value and decreasing y hb value growth yield fig 9 high maximum growth rate μ max hb 0 112 h 1 and low bacterial growth yield y hb 0 07 increase oxygen consumption by heterotrophic bacteria and decrease do concentrations in water column but not sufficiently to reduce the gap between simulation and observation figs 4 and 5 furthermore the 95 confidence intervals of the do concentrations simulated by prose pa are very narrow during these periods see for instance at sartrouville station which indicates that the model parameters have a low influence on do concentrations it seems that the farther from an organic matter source the monitoring station is located the larger the discrepancies between simulation and observation this is true for the suresnes to sartrouville monitoring stations prose pa is able to simulate do concentrations at andresy station properly a rmse of 0 53 mgo2 l when using eight stations for data assimilation this station is located downstream of two important organic matter sources the oise river and the sav wwtp fig 2 which enrich the seine river in bdom fig 12 further downstream when all the biodegradable organic matter has been consumed see the mismatch at the mericourt station the particle filter degenerates again the overestimation of the do concentrations during the critical periods can therefore be explained by a deficit of bdom in the water column as illustrated by the longitudinal profile of mean bdom concentration from 2011 09 07 to 2011 09 27 fig 12 the bdom are shortly degraded downstream of sav wwtp and the oise river simulated bdom concentrations are less than 0 02 mgc l at meulan and mericourt stations which limits the bacterial activities fig 12 this is the reason why the simulations of prose pa and prose are similar during low flow periods without algal bloom prose pa simulates almost the same oxygen level at meulan and mericourt stations in september and october however the depletion of measured do is observed at mericourt station fig 5 it means that supplementary bdom is required to produce this depletion at mericourt station the same phenomenon can also be observed from colombes station to bougival station the simulated do concentrations varying from 7 mgo 2 l to 8 mgo 2 l are similar at colombes chatou and bougival stations from 2011 09 07 to 2011 09 27 the do concentrations are well simulated by prose pa at colombes and bougival stations at this period but the depletion of do concentrations 6 mgo 2 l fig 4 during this period are not reproduced at chatou station located at a right reach of the seine river this depletion of oxygen results from the lateral inflows containing bdom overflow of csos wwtps before chatou station this indicates that some unknown overflow sources are not considered on the right reach of the seine river before chatou station to conclude the biodegradable organic matter carried by rivers csos towards the seine river is quite uncertain and dismiss the hypothesis of perfectly known boundary conditions the uncertainties of the biodegradable portion of dom have been reported in numerous studies meyer et al 1987 servais et al 1987 1995 1999 søndergaard and middelboe 1995 wang et al 2010 he et al 2010 kang and mitchell 2013 this portion depends on the hydrological and trophic contexts of a river therefore varying temporally generally the range of bdom portion of dom appears to be 0 05 0 54 hasanyar et al 2020 the portion of bdom used in this paper is 0 47 for rivers which seems a rather good estimate most of the time but need to be refined when it leads to improper organic matter depletion 4 2 synthesis of the phytoplanktonic parameters values characterized by prose pa particle filter to interpret the identification of the phytoplanktonic parameters by prose pa particle filter the averages of the posterior pdfs modes are calculated for the algal blooms of march and july table 4 two optimal temperatures for the growth of phytoplankton t opt pp are estimated for the algal bloom of march 15 7 c vs 24 7 c respectively a t opt pp of 22 4 c is identified for the bloom of july the different physiological properties identified by prose pa reflect different phytoplanktonic communities in the seine river the algal species of the seine river have been investigated for the last 30 years by the piren seine program groleau et al 2014 revealed that the phytoplanktonic species identified in march 2011 are synedra ulna cyclotella sp nitzschia sp and the specie identified in july is nitzschia sp garnier et al 1995 highlighted that the spring algal bloom in seine river is mainly composed of a species of centric diatoms stephanodiscus hantzschii descy et al 2012 mentioned also that the stephanodiscus sp grows normally in early spring while the nitzschia sp grows at the end of spring or in summer this confirms the consistence of t opt pp values identified for the blooms of march april and july 24 7 c vs 22 4 c probably nitzschia sp moreover descy et al 2012 used 11 c and 17 c to model the different phytoplanktonic communities growing in spring stephanodiscus sp and in summer nitzschia sp for the loire river while vilmin 2014 obtained the calibrated values of 10 c and 23 c for the seine river these studies are in agreement with the observed switch of t opt phy of phytoplankton during the algal bloom of march april to summarize the parameter values identified by prose pa are in agreement with those reported by descy et al 2012 vilmin 2014 and garnier et al 1995 for some periods and with the prior knowledge of phytoplanktonic communities on seine river groleau et al 2014 the different parameter values indicate the necessity of a time varying parameter setting for simulating algal bloom dynamics in river systems properly 5 conclusions a particle filter is applied for the first time to assimilate high frequency dissolved oxygen data in river system and to characterize the physiological properties of living communities represented by model parameter values under contrasted hydrological and trophic contexts in spite of the assumption that the boundary conditions input water quality data are perfectly known this article shows that the particle filter improves significantly the simulation of the do concentrations in the seine river particularly particle filter allows a more accurate simulation of the algal bloom dynamics which is very difficult to reproduce using forward model configured with static parameter the different physiological properties of phytoplankton can be characterized using particle filter the identified phytoplanktonic parameter values during the algal blooms are consistent with the values reported in the literature and with the prior knowledge on the phytoplanktonic communities in the seine river system this paper demonstrates the efficiency of the particle filter to predict the dissolved oxygen concentrations and to capture the variability of phytoplanktonic parameters during algal blooms which cannot be described by a forward model that uses static parameters only this study demonstrates the necessity of a time varying parameter setting for simulating microorganism activities nevertheless further improvement of do concentration simulated by the particle filter can be achieved by introducing a time varying bdom content in the boundary conditions represented by rivers csos and wwtps such an advance should also improve the capability of prose pa to characterize heterotrophic bacteria properties parameters during low flow without algae bloom software availability name of software prose pa contact address nicolas flipo mines paristech fr year first available 2019 program language ansi c operating system linux software access gitlab deposit under discussion availability and cost open source licence open licence under discussion declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work is a contribution to the piren seine research program part of the french long term socio ecological research ltser site zones ateliers seine we would like to thank the data providers the high frequency oxygen observations from meseine network were provided by the public sewage company of the greater paris area siaap and the low frequency oxygen data from the national river monitoring network rcs was supplied by the drinking water company of the paris urban area sedif 
25609,model uncertainties are generally integrated in environmental long running numerical simulators via a categorical variable by focusing on gaussian process gp models we show how different categorical kernel models exchangeable ordinal group etc can bring valuable insights into the correlation of the simulator output values computed for different levels of the categorical variable i e the interlevel dependence structure supported by two real case applications cyclone induced waves and reservoir modeling we have proposed a cross validation approach to select the most appropriate kernel by finding a trade off between predictability explainability and stability of the covariance coefficients this approach can be used effectively to support some physical assumptions regarding the categorical variable through comparison to tree based techniques we show that gp models can be considered a satisfactory compromise when only a few model runs 100 are available by presenting a high predictability and a concise and graphical way to map the interlevel dependence structure keywords categorical variables computationally intensive simulator metamodel kriging model selection abbreviations cs compound symmetry dt decision tree e expert based gen general gp gaussian process lr low rank o ordinal rf random forest 1 introduction high resolution numerical simulators are key components of environmental science that help to obtain deeper insights into the behavior of natural systems some examples are veeck et al 2020 for hydrologic modeling zhao et al 2013 for agricultural modeling vandromme et al 2020 for landslide modeling abily et al 2016 for urban flooding and idier et al 2020 for marine flooding to model the natural system these simulators all have in common to involve a large spectrum of assumptions related to the choice in the structure form of the model e g 1d versus 2d modeling leandro et al 2009 the selection of the physical processes regarded as relevant and prominent e g account for spatial heterogeneity liu et al 2017 the use of alternative physical laws e g different soil water retention curves silva ursulino et al 2019 the system s future evolution e g future gas emission scenarios le cozannet et al 2015 or land use change mishra et al 2018 etc depending on the modeling assumptions the simulation results can differ hence resulting in model uncertainty termed structural uncertainty e g hill et al 2013 some of these modeling assumptions can be modeled by means of continuous variables such as geotechnical properties of a given soil formation or time series of rainfall conditions at a given location etc some of them involve categorical variables i e multilevel indicators that take up a finite number of discrete values each discrete level of the categorical variable is associated with a different modeling assumption e g level a is associated with modeling assumption a some real case applications are provided in the domain of safety analysis of radioactive waste disposal by storlie et al 2013 earthquake risk assessments by rohmer et al 2014 marine flooding induced by sea level rise by le cozannet et al 2015 reservoir engineering for co2 geological storage by manceau and rohmer 2016 pollution risk analysis and management by lauvernet and helbert 2020 etc quantifying the correlation of the simulator output values computed for two levels i e two modeling assumptions of the categorical variable whatever the values of the other input variables is of high interest to measure the impact of structural uncertainty because it informs whether each level should be treated equivalently with respect to the numerically simulated variable of interest or if there is any dependence among the levels like a group structure this type of structure named the interlevel dependence structure in this study can be useful to identify modeling assumptions that should be considered a priority in complementary simulation based studies for instance a level showing pronounced impact on the variable of interest suggests where to focus the simulation based exploration and a group structure suggests simplifying the analysis by restricting the analysis to a single member of the group beyond structural uncertainty such information may be of interest to support the analysis of deep uncertainty based on scenario discovery kwakkel and jaxa rozen 2016 our objective is to develop a statistical procedure to learn the interlevel dependence structure due to the high computation time cost of numerical environmental simulations we aim to learn the dependence structure with only a few model runs on the order of 100 by relying on the design and analysis of computer experiments santner et al 2003 the key element of the proposed procedure is the use of gaussian process models denoted gp models williams and rasmussen 2006 with covariance functions also known as kernels adapted to handle mixed continuous categorical inputs and combined by tensor products e g roustant et al 2020 qian et al 2008 zhang et al 2020 the kernels specify how similar i e correlated two instances of the variable of interest e g y and y 0 are expected to be at two input values u and u 0 i e in our case at two levels of the categorical input this similarity function can be encoded in different manners depending on the assumptions regarding the categorical variable nominal ordinal interdependence between the levels interactions between given levels etc see for instance roustant et al 2020 and lauvernet and helbert 2020 once fitted the resulting correlation matrix provides the mapping of the dependence structure to illustrate the type of results that can be derived fig 1 a depicts an unknown relationship between a continuous and a categorical input variable with 5 levels i e five modeling assumptions fig 1b depicts the correlation matrix derived from the gp based analysis given 25 model runs a group of highly correlated levels are identified for u 1 4 as well as the decreasing correlation of u 5 with the others from 25 to 15 considering u 4 to u 1 these observations are consistent with the test function if the functional relationship in fig 1a had been known this result would have been straightforward but here the structure is unknown and can be learned only with a limited number of numerical results here with only 25 model runs this result that is further discussed in sect 4 2 depends on how the kernel model is defined which raises the question of model selection that is addressed in the present study by relying on a multicriterion analysis the paper is organized as follows section 2 describes the different steps of the proposed procedure as well as the statistical methods in this latter we provide the formal definition of the interlevel dependence structure related to the gp correlation matrix sect 2 2 by relying on the tensor product of kernels for continuous and categorical variables sect 2 3 a multicriterion approach for selecting the categorical covariance kernel model is also detailed sect 2 4 section 3 describes the application cases i e the synthetic case described in fig 1 and two real cases namely cyclone induced wave numerical modeling rohmer et al 2016 and reservoir modeling of co2 storage manceau and rohmer 2016 both real cases are representative of two distinct situations the cyclone case illustrates a situation where a physical intuition on an a priori influence of the categorical variable is available whereas the reservoir case illustrates the opposite situation where the physical intuition is harder to give the procedure is then applied to each of these cases in section 4 in section 5 we further discuss the results by comparing the gp based procedure to a popular alternative approach based on tree based techniques on this basis practical recommendations are then defined in section 6 2 methods 2 1 description of the procedure the proposed procedure is as follows step 1 we follow the approach of design and analysis of computer experiments santner et al 2003 a series of runs of the expensive to evaluate environmental simulator were performed by considering a limited number of randomly selected input variable configurations step 2 using the set of random computer experiments training dataset different hypotheses regarding the structure of the considered input categorical variable are tested and modeled by means of different kernel covariance formulations see further details in sect 2 2 and 2 3 step 3 the question of selecting the most appropriate kernel is examined by analyzing different aspects i e by considering different criteria as described in sect 2 4 the objective is to select the resulting gp model that can achieve a trade off between the different criteria step 4 since the practitioner is preferably interested in the dependence structure between the modeling assumptions the correlation matrix derived from the covariance matrix is analyzed see further explanation in sect 2 2 this result can be confronted with some a priori physically based interpretation of the categorical variable influence that the practitioner may have before analyzing the computer experiments 2 2 gaussian process for mixed continuous and categorical inputs let us consider the set of d continuous input variables x x 1 x d and the set of j categorical inputs u u 1 u j with n l 1 n l j levels that represent the categorical inputs the output y is then computed using the numerical environmental simulator f as y f x u f w in the context of gaussian process gp modeling also named kriging williams and rasmussen 2006 the function f is assumed to be a realization of a gp y w with a constant mean μ and a covariance function k named kernel that can be written as follows 1 w w k w w cov y w y w let us denote w 1 w n the training samples and y y 1 f w 1 y n f w n denote the corresponding results the prediction at a new observation w is given by the kriging mean y ˆ w as follows 2 y ˆ w e y w y w 1 y 1 y w n y n μ c w t c 1 y μ i where c is the covariance matrix between the points y w 1 y w n whose element is c i j k w i w j c w is the vector composed of the covariance between y w and the points y w 1 y w n and i is the identity vector of length n the prediction at w can be associated with an error estimate provided by the kriging variance σ ˆ 2 given by 3 σ ˆ 2 w var y w y w 1 y 1 y w n y n c w w c w t c 1 c w using the gp mean and variance equations 2 and 3 the prediction interval pi α at the given confidence level α e g α 95 can be computed as follows 4 p i α w y ˆ w σ ˆ w q n 0 1 1 α 2 y ˆ w σ ˆ w q n 0 1 1 α 2 where q n 0 1 is the quantile of order 1 α 2 of the standard normal distribution accounting for a mixture of input variable types continuous or categorical ordinal or nominal is made via the covariance function k w w here it is assumed to be the tensor product of the covariance function for the continuous inputs k cont x x and the ones for the categorical inputs k cat u u as k w w k cont x x i 1 j k cat i u i u i other combination approaches are possible as discussed by roustant et al 2020 the covariance function k cont can be described by kernel models that are commonly used in the computer experiment community in the present study we restrict the analysis to the stationary twice differentiable in the mean square matérn 5 2 model williams and rasmussen 2006 the categorical covariance functions k cat i i 1 j can be described in different manners depending on the assumption related to the categorical input as described in sect 2 3 in practice k cat i can be interpreted under the homoscedastic assumption as the kernel up to a multiplicative constant of the 1d section u i y x u i u i where x and u i u 1 u i 1 u i 1 u j are fixed in practice we prefer using the scaled form of the covariance i e the correlation to ease the interpretation of the dependencies of the simulator output on interactions between the levels of the categorical variable i e of the modeling assumptions with the same notations the correlation kernel derived from k cat i is interpreted as the correlation of the 1d section u i y x u i u i regardless of x and u i thus the inspection of k c a t i reveals the correlation of the simulator output explained by the ith categorical input the others being fixed note that such a correlation does not depend on the remaining inputs which is a result of constructing k by tensor product 2 3 covariance kernel models for categorical inputs table 1 summarizes the different options for defining a categorical covariance function their interpretation and their practical implementation the interested reader can refer to roustant et al 2020 and references therein for a more formal presentation in particular with an analysis of the positive definiteness of covariance matrices note that we restrict the presentation to the case of a single input with n l levels which is hereafter denoted by u the generalization to multiple categorical inputs can be done by following the tensor product formulation described in sect 2 2 when the practitioner assumes that no preference can be given to the n l levels i e all considered scenarios are considered to have the same influence k cat can be described by an exchangeable covariance qian et al 2008 also named the compound symmetry denoted cs function as follows 5 k c a t c s u u σ 2 if u u ρ σ 2 if u u where ρ is a unique correlation coefficient satisfying ρ 1 n l 1 1 when the practitioner assumes that the variable of interest will act differently depending on the considered level but without excluding some dependencies between these different responses k cat can then be described by the most general and complex dependence structure where each pairwise coefficient can take a different value depending on the considered levels u u the covariance function reads as follows 6 k cat gen u u c u u if u u v u if u u the latter structure can be simplified by adding a priori information on the dependence between the levels for instance by relying on expert based information a possible option is to assume that some levels perform similarly and that they can be grouped assume that the n l levels of u are partitioned into g groups and denote g u as the group number corresponding to a given level u then the covariance function can be written as roustant et al 2020 7 k cat e u u k cat gen g u g u c g u g u if g u g u v g u if g u g u where for all i j 1 g the terms c i i v i are within group correlations and c i j v i v j i j are between group correlations the structure can be simplified by assuming that the correlation value for each pair of groups is unique by means of a compound symmetry matrix pinheiro and bates 2006 instead of deriving the groups based on expert information a possible option is to explain the level dependencies by a few key latent continuous variables using a low rank approximation roustant et al 2020 so that the covariance matrix k cat lr can be defined as 8 k cat lr q q t where the matrix q is of size n l q where q is low the higher q is the more likely the danger of overfitting in practice typical values of q are 2 or 3 a value of 1 is not recommended see zhang et al 2020 when the practitioner assumes that the levels can be ordered this means that the categorical variable can be described by an ordinal continuous variable that is not directly observed i e it is said to be latent and the levels are seen as discretized values of this ordinal variable qian et al 2008 the corresponding covariance function can be defined by taking advantage of the tools available for the continuous variables as follows 9 k cat o u u k cont f u f u where k cont is a one dimensional continuous kernel such as the matérn 5 2 model and f is a one dimensional nondecreasing function also called warping f 1 n l r the warping function can be modeled via a parametric model e g the cdf of a flexible probability distribution such as normal or beta or a nonparametric model e g a piecewise linear or a quadratic spline as detailed by roustant et al 2020 2 4 kernel model selection as mentioned above different kernel modeling choices can be made to represent the categorical variable raising the question of selecting the most appropriate kernel covariance model depending on the modeling objective different approaches exist for selecting an optimal model with respect to a specific criterion burnham and anderson 2002 for instance a model that satisfactorily represents i e explains the relationships between inputs and outputs might not necessarily perform as well for prediction therefore we propose to examine different viewpoints on the problem of kernel selection by analyzing different criteria this multicriterion approach shares similarities with the data science framework proposed by yu and kumbier 2020 who advocate analyzing three core principles predictability computability and stability to select the most appropriate kernel model given the training dataset we investigate whether the considered gp model is capable of predictability this concept is related to whether the gp model is capable of predicting yet unseen input configurations i e samples that have not been used for training which can be assessed by using independent test samples bootstrap or cross validation approaches e g hastie et al 2009 two indicators are estimated the first indicator denoted q 2 measures the deviation from the true output value given a test set t q 2 is defined as follows 10 q 2 1 i t y i y ˆ i 2 i t y i y ˆ i 2 where y ˆ i is the ith gp based prediction of the model output y i and y 1 t i t y i is the average value for the test set a coefficient q 2 close to 1 0 indicates that the gp model is successful in matching the new observations that have not been used for the training for the sake of comparability between the different criteria we consider 1 q 2 the second criterion is related to the coverage denoted ca of the prediction intervals pi α eq 4 at the given confidence level α calculated on the test set t defined by 11 c a 1 t i t 1 y i p i α w i where 1 a is the indicator function ca evaluates whether the model output y i is within the bounds of the prediction interval the gp derived pi α is optimal when ca is close to the theoretical value of α for the sake of comparability between the different criteria we consider the error in coverage defined as 1 ca explainability and simplicity the former concept relates to whether the considered gp model is capable of representing the data for instance by analyzing the likelihood l however adding more model parameters results in increasing the explainability to counterbalance this tendency related to overfitting a penalty term is generally introduced see e g höge et al 2018 to select a simpler model here simplicity refers to the number of gp model parameters but alternative definitions exist see for instance rougier and priebe 2020 who introduce the concept of flexibility by assuming that the true gp model exists and that it is among the set of candidate gp models we propose relying on the bayesian information criterion bic schwarz 1978 described as follows 12 b i c 2 log l k log n where k is the number of parameters and n is the number of observations stability we explore to what extent the kernel correlation matrix derived from the covariance matrix is stable to the perturbations in the training dataset we evaluate an error measure between the correlation matrices r 0 ˆ estimated using the gp model fitted with the whole training dataset and r j ˆ estimated at the jth iteration of the n cv fold cross validation procedure for instance when n cv 5 the training dataset corresponds to the whole training dataset from which 20 of the observations have been randomly removed the error measure is defined by 13 e r r 1 n cv k 1 n c v r 0 ˆ r k ˆ f 2 where f is the frobenius matrix norm by restricting to the nondiagonal elements in the upper triangular part of r ˆ eq 13 is calculated as 1 n cv k 1 n c v 1 n p i 1 n p r ˆ i 0 r ˆ i k 2 where r ˆ i 0 is the ith coefficient of r 0 ˆ read in column order for instance r ˆ i k is the ith coefficient of r k ˆ and n p is the number of terms of r ˆ note that alternative formulations could also be proposed to better account for the correlation matrix structure such as the diagonally weighted matrix norm proposed by cressie and hardouin 2019 3 description of the application cases in this section we describe the application cases namely a synthetic test function sect 3 1 and two real cases in the domain of cyclone induced wave modeling sect 3 2 and reservoir engineering sect 3 3 3 1 synthetic case we first consider a synthetic test function using a modified version of the two dimensional branin function 1 1 http www sfu ca ssurjano branin html where one continuous variable u is assumed to reach only discrete values as follows 14 y h x 20 if u 1 h x 10 if u 2 h x 7 5 if u 3 h x 5 0 if u 4 5 h x 20 if u 5 where h x z z 5 4 π 2 x 2 5 π x 6 10 1 1 8 π cos x 10 with x 5 10 by construction levels 1 4 are highly correlated see fig 1a we consider the different categorical kernels defined in table 1 with the following assumptions expert based groups two experts have given their opinions the first expert denoted e indicates a realistic grouping of levels i e consistent with the true function and the second one denoted ew indicates an unrealistic grouping i e u 1 u 3 and u 2 u 4 u 5 low rank approximation a matrix with rank of q 2 is tested denoted lr2 ordinal variable the levels are ordered by following the level index for the expert based grouping we assume that another expert does not know the correct ordering and assumes an unrealistic order denoted ow i e u 4 u 2 u 5 u 3 u 1 the training dataset is defined through random sampling by considering m x points per u level with m 4 5 and 6 i e with different training dataset sizes of 20 25 and 30 3 2 real case application 1 cyclone induced waves the first real case is based on rohmer et al 2016 and deals with the modeling of waves induced by cyclones at sainte suzanne city located in the northeast of reunion island fig 2 b the aim is to analyze the evolution of the significant wave height h s maximum value over time as a function of the cyclone characteristics these parameters are modeled by means of five scalar continuous input parameters namely the maximum wind speed the radius of maximum winds i e the distance from the cyclone eye at which the maximum wind intensity is reached the shift around the central pressure the forward speed defined as the translation speed of the cyclone eye and the landfall position which both characterize the minimum distance and the relative position of the track to the studied site a set of seven historical cyclone tracks are considered these tracks are randomly shifted via the continuous input variable modeling the relative position of the track from their original track so that they cross the center of reunion island see fig 2b a categorical variable is defined here with each level corresponding to a given track a series of 100 computer experiments were performed by randomly sampling the inputs using a latin hypercube sampling approach combined with a maximin criterion johnson et al 1990 the design of the experiments is presented in appendix a accounting for the spatial variability of the track as illustrated in fig 2b requires integrating each cyclone s spatial position along the track as an input variable of the gp models which is hardly feasible in practice an a priori physical interpretation of the track influence speculates that h s is strongly related to the angle of approach of the cyclone in the vicinity of the studied site which increases from 0 east direction to 180 west direction 90 is north confirmed by the sensitivity analysis of rohmer et al 2016 this means that the track effect can be summarized by a single scalar continuous input i e in relation to the angle of approach the analysis of the boxplots in fig 2a seems to support this hypothesis in particular the median value of the maximum h s appears to increase as the angle of approach increases from 0 to 90 from gael to dumile cyclone however the tendency from 90 to 180 from dumile to banzi cyclone is less clear especially for banzi the difficulty in the interpretation may here be related to the complexity of this cyclone track compared to the quasilinear shape of the others fig 2b to support the evidence of the influence of the angle of approach a more rigorous analysis is needed here the validity of this hypothesis is further investigated using a gp model with different categorical kernels as described in table 1 with the following assumptions expert based groups a first assumption relies on the selection of two groups one composed of the 4 cyclones gael giovanna hollanda and dumile coming from the northeastern ne quadrant and another group composed of 3 tracks bejisa haliba and banzi coming from the northwestern nw quadrant fig 2 a second assumption based on three groups is also tested by differentiating the track whose angle is almost at 90 north namely the dumile cyclone fig 2 in addition two assumptions are made regarding the link between the groups by specifying a general assumption e2 and e3 or a compound symmetry covariance assumption e2cs and e3cs low rank approximation ranks of q 2 and q 3 are tested kernels denoted lr2 and lr3 respectively ordinal variable the levels are ordered following the angle of approach 3 3 real case application 2 co2 geological storage the second real case application corresponds to the modeling of the long term fate of stored co2 in a deep aquifer on a potential project in the paris basin france as described by manceau and rohmer 2016 the injection of 30 mt of co2 over 30 years in the lower triassic sandstone formation at a depth of approximately 1000 m was numerically simulated the evolution of the quantity of mobile co2 for a time period of 150 years after the injection stopped was investigated as a function of two continuous input variables namely the porosity and the intrinsic permeability of the aquifer rock formation four categorical input variables related to the assumptions for permeability anisotropy minor medium and large regional hydraulic gradient absence and activated capillary effect absence and activated and the choice in the physical law used to model the relative permeability as a function of co2 saturation ten choices as depicted in fig 3 due to the importance of the latter parameter as shown by manceau and rohmer 2016 the following analysis focuses on this variable a series of 100 computer experiments was performed by randomly sampling the inputs using a latin hypercube sampling approach combined with a maximin criterion a base 10 logarithm transformation was applied to the quantity of mobile co2 due to the large asymmetry of its distribution the design of the experiments is presented in appendix a unlike the case described in sect 3 2 it is harder to give a physical intuition on an a priori influence of the categorical variable related to the relative permeability laws this lack of intuition is related to the richness of the information associated with the process of residual trapping that is related to different aspects 1 the capacity of the porous medium to allow the flow of the gaseous phase in the presence of another phase called relative permeability is represented as a function of the gas saturation in the porous medium see examples in fig 3c this capacity is associated with a potential hysteretic effect resulting in a nonunique dependence during the flow over time see further details in juanes et al 2006 2 the capacity of the alternative phase water is represented by another function of the saturation fig 3d and 3 the considered phase gaseous or aqueous progressively becomes isolated when its saturation decreases in a porous medium leading to saturation that cannot be reduced these specific situations are called residual saturations for the gaseous phase fig 3b and irreducible saturations for the aqueous phase to help formulate a physically based assumption about the interdependencies the boxplots in fig 3a allow us to identify some specific law behaviors especially for law 1 some tendency can also be noticed when ordering the laws in a specific order with respect to the median values of the variable of interest to obtain a clearer picture we test the validity of these observations via the proposed gp based approach by considering the different categorical kernels described in table 1 with the following assumptions expert based groups the grouping should account for the three facets of the co2 flow in porous media i e by integrating the three pieces of information depicted in fig 3b d dissimilarities in both the imbibition drainage curve shape and residual trapping model on this basis the following grouping is proposed law 2 law 10 law 6 9 law 1 law 3 law 4 law 5 in addition an assumption is made regarding the link between the groups by specifying a general or a compound symmetry covariance assumption denoted e and ecs respectively low rank approximation a rank of q 2 and of q 6 i e of the same number of expert based groups are tested kernels denoted lr2 and lr6 ordinal variable the levels are ordered with respect to the value of the maximum co2 residual saturation fig 3b the three other categorical variables are modeled as follows the regional hydraulic gradient and the capillary effect are both assigned a compound symmetric kernel an ordinal kernel with spline based warping is defined for the permeability anisotropy because there is a natural ordering of the levels 4 results 4 1 implementation procedure in this section we apply the gp based procedure described in sect 2 to the cases described in sect 3 to ensure identifiability of the model defined with k we treat k c o n t as a covariance kernel and each k cat j as a correlation kernel j 1 j for all gp models we consider a matérn 5 2 covariance matrix for the continuous variables for the ordinal kernel k cat o a matérn 5 2 continuous kernel is combined with a quadratic nondecreasing spline warping function f see eq 9 we consider gp models with constant trends and fit them using the r package kergp deville et al 2018 by applying a prescaling and centering of the continuous input variables and of the variable of interest the covariance parameters are estimated via a maximum likelihood approach using the derivative free constrained optimizer by linear approximations named cobyla developed by powell 1994 with 250 randomly selected initial starts the predictability and stability were assessed via a 5 fold cross validation procedure repeated 25 times 4 2 application to the synthetic case considering the synthetic test case fig 4 depicts the gp derived correlation matrices for each of the kernel assumptions described in sect 3 1 using the training dataset of intermediate size m 5 the application of the expert based and ordinal kernel assumptions fig 4c e shows some consistent structures among the levels namely the highly correlated group for u 1 to u 4 and the particular behavior of u 5 the magnitude of the interdependencies between the identified group and u 5 differs slightly assumption e indicates a low to moderate correlation 10 whereas assumption o indicates a decreasing correlation from 25 to 15 for u 4 1 the structure of interlevel dependencies is richer for the general fig 4a and lr2 based gp models fig 4b for these correlation matrices the interpretation is also less straightforward than for e or o for which some group structures are more easily identified the visual inspection of both matrices suggests however some interesting features i e the increasing correlation between u 1 4 for gen and the particular behavior of u 5 as well as a possible grouping of u 2 4 for lr2 capturing the correlation structure can be hard for these cases given the size of the training dataset of 25 this is further discussed below when analyzing the stability criterion in this case the exchangeable assumption i e compound symmetry cs leads to a low to moderate intercorrelation coefficient of 40 not shown the four criteria for kernel model selection are examined in fig 5 for the sake of comparability between the different assumptions for m we preferably plot the difference of bic with the minimum value over the whole experiment named δbic several observations can be made explainability measured by bic fig 5a appears efficient here to exclude the unrealistic assumptions ow and ew they present very large bic differences regardless of m for instance burnham and anderson 2002 suggested a difference of the considered information criterion relative to the minimum value of at least 10 to support the ranking between model candidates with confidence bic appears to be very informative to discriminate the kernel assumptions and clearly selects the ordinal assumption as the most appropriate the q 2 criterion is commonly used in the computer experiment community to rank different models with respect to their predictive capability in our case basing the analysis on this unique criterion is difficult at low size of the training dataset m 4 models e cs and o all minimize 1 q 2 and show similar performance see median values in fig 5b selecting one of them is thus hardly achievable furthermore the model associated with the unrealistic expert based grouping ew has a satisfactory predictive capability at low m values although the width of the confidence interval is larger than the others for larger m value here m 6 i e with the largest size of the training dataset the q 2 criterion allows us to identify model o as the most appropriate model with respect to the predictive capability log10 1 q 2 is the lowest in fig 5b the other facet of predictability related to the coverage of the prediction intervals suggests discarding kernel assumptions gen and lr2 because ca is here far larger than the level of the prediction interval but hardly allows differentiating the other assumptions regardless of m fig 5c the stability criterion fig 5d tends to suffer from the same sensitivity as q 2 to the size of the training dataset but has a higher discriminative power at low m values m 4 both kernel models cs and o are selected as very stable because of low log10 err values in fig 5d however a high stability is also reached for ew for a large m value the ordinal assumption is then clearly selected as the assumption leading to the most stable correlation matrix from this analysis we can conclude that the ordinal assumption allows us to successfully fulfill three of the criteria explainability predictability and stability especially at sufficiently high m values m 5 for coverage the ordinal assumption is not ranked first but ca appears of a reasonable order of magnitude median value of 85 i e with moderate deviation from the level of the 95 predictive interval for the small size of the training dataset m 4 unambiguously rejecting selecting ew and cs becomes difficult if the explainability criterion bic criterion is not considered which can partly be explained by the analysis of the ew based correlation matrix fig 4d which reveals a quasi homogeneous structure with correlation coefficients ranging from 44 to 56 i e of the same order of magnitude as the cs assumption of 40 hence indicating that both gp models should perform similarly suggesting that at a low number of training points the cs assumption remains the most reasonable assumption when the goal is the joint maximization of predictability stability and explainability 4 3 application to the real case application 1 considering the cyclone test case fig 6 depicts the gp derived correlation matrices for each of the kernel assumptions described in sect 3 2 fig 6a reflects the hypothesis of a single group without difference in effect on maximum significant wave height between the tracks the derived correlation appears to be high 80 the application of alternative kernel assumptions shows some consistent structures among the cyclone tracks two groups of cyclones appear to be highly correlated coefficient 75 namely those coming from ne and those coming from nw as shown in fig 6b general formulation and fig 6 e f e2 and e2cs the high correlation among these groups is also indicated by the other assumptions 1 the low rank approximation lr2 and lr3 fig 6c and d but these assumptions lead to a richer interdependency structure 2 to a lesser extent the expert based assumption e3cs fig 6 h however we note that there is ambiguity regarding the dumile cyclone see in particular fig 6g which is highly correlated with either ne cyclones e2 e2cs or nw cyclones lr3 e3cs the different assumptions all suggest a moderate correlation on the order of 40 60 between groups of cyclones coming from ne and nw these observations are consistent with the ordinal assumption fig 6h which indicates a decreasing correlation from gael the track with the lowest angle of approach to the banzi cyclone the track with the largest angle of approach and a central role of dumile with a decreasing correlation either with the ne cyclone or with the nw cyclone the four criteria of kernel model selection are examined in fig 7 regarding explainability and simplicity the ordinal assumption o appears to be the most appropriate the derived gp model presents the minimum bic value and the differences with the alternative models are large here they are larger than 20 regarding predictability the ordinal assumption also leads to the best model regarding this criterion however the predictability of the expert based model candidates e2 e2cs e3 e3cs remains of moderate to high degree with a median value of q 2 of approximately 90 regarding ca the use of e3cs allows us to reach the level of the 95 prediction interval but alternative assumptions e2 e2cs e3cs cs and o lead to satisfactory coverage of the prediction intervals as well finally fig 6d indicates the poor stability of the gen and lr3 kernels i e the high sensitivity of the estimates of the correlation coefficients which may be related to the high number of coefficients to estimate 21 and 11 respectively also showing the satisfactory stability of two alternative assumptions i e the expert based ones and the ordinal one on this basis we can conclude that the ordinal assumption allows us to reach a satisfactory trade off between the four criteria which appears to be consistent with the aforementioned physical intuition the latent continuous variable here is related to the angle of approach see sect 3 2 fig 8 a summarizes this dependence via the spline based warping function f used to set up the ordinal covariance kernel model k cat o see eq 9 this shows a strong link between ne cyclones gael giovanna and to a lesser extent for hollanda as well and a quasilinear increasing influence to banzi this warping is the basis for computing the correlation matrix fig 6i to ease the interpretation let us focus on a single row of fig 6i i e the pairwise correlation between the gael track and the others fig 8b provides a clear indication of a highly correlated group of cyclones coming from ne within the red colored envelope with a correlation coefficient exceeding 80 and a decreasing correlation with those coming from nw the analysis of the correlations derived from the cross validation iterations for each repetition gray lines in fig 8b confirms this result more than 75 of the cross validation derived results show high correlation 75 of dumile with the ne cyclones hence in agreement with e2 assumption fig 6e compared to o the analysis of e2 performance criteria shows that this assumption can also be considered reasonable with very satisfactory stability of the correlation coefficients fig 7d although the criterion values appear to be higher the stability criterion appears to be lower than the one for o which may be related to the lower number of correlation coefficients to be estimated of 2 for the e2 assumption and of 7 for the o assumption 4 4 application to the real case application 2 considering the co2 geological storage test case fig 9 depicts the gp derived correlation matrices for each of the kernel assumptions described in set 3 3 some consistent structures can be noticed regarding law 1 which appears to be anticorrelated with the others as shown in fig 9a general assumption and in fig 9 b c low rank approximation lr2 and lr6 the specificity of law 1 is also outlined by the expert based grouping in fig 9d which indicates here a moderate positive correlation of 35 40 in agreement with the ordinal assumption fig 9f which indicates that the pairwise correlation coefficients of law 1 with the others see last row of fig 9f rapidly decrease although disagreeing on the correlation magnitude the lr2 gen and e models all suggest a moderate correlation among all laws except for law 1 the assumption lr6 also suggests the particular behavior of law 5 which goes in the same direction as the expert based clustering of considering it as belonging to a single group the assumption ecs i e using a cs between group covariance assumption leads to a less complex correlation structure and clearly highlights the grouping of laws 6 9 as suggested by the experts see also the laws outlined in dark blue in fig 3 which is in agreement with the group of highly correlated laws as outlined by fig 9f though the size of the group is larger and includes laws 2 and 4 as well to summarize the inspection of the correlation matrices is more difficult here than for the cyclonic application case sect 4 3 where all assumptions more or less agree regarding the information supplied by the modelers nevertheless this inspection highlights the specificities of law 1 and law 5 lr6 assumption which both strongly differ from the other laws this is suggested by the irreducible water saturation yellow and medium blue curve in fig 3c and d law 1 is even more different with an irreducible water saturation associated with a large maximum gas residual saturation which seems to explain why the model behaves so specifically when this law is accounted for the four criteria for kernel model selection are examined in fig 10 for the considered case we show that the expert based ecs i e with the simplified correlation structure between the groups and the cs assumption both lead to gp models that satisfactorily fulfill the four criteria with a slightly higher performance for the simpler structure of cs where the expert based grouping of laws in particular laws 6 9 in dark blue in fig 8 is informative in the sense that it leads to a competitive gp model but only makes a slight difference with the simpler structure especially regarding explainability with bic difference 10 and stability due to the lowest number of cs correlation coefficients namely 1 note that due to the high number of covariance parameters relative to the 100 simulation runs gen and lr6 score poorly here from this analysis we can conclude that given the 100 simulation results there is only mild evidence supporting the assumption of the structure associated with the permeability laws that was intuited from the analysis of the boxplots in fig 3a and of the curve similarities fig 3b d similar to the synthetic case additional simulation results should be performed to unambiguously discriminate the most appropriate kernel assumption 5 comparison to tree based methods in practice gp models are not the first statistical modeling option that comes to mind when addressing the problem of categorical variables a popular approach relies on tree based methods such as regression decision trees denoted dt breiman 1984 and random forest regression denoted rf breiman 2001 which both natively handle categorical predictors without having to first transform them e g by using feature engineering techniques because they are based on binary recursive partitioning examples of real case applications are provided by jaxa rozen and kwakkel 2018 and rohmer et al 2018 from a practical viewpoint the advantage of dt is to provide the structure of interlevel dependence as well as the interactions with the other input variables with a graphical presentation of the results in the form of a tree which greatly eases the interpretation for instance in the cyclone real case application fig 11 a gives the tree structure derived from the analysis of the cyclone real case however from the analysis of fig 11 we note several differences between the structure constructed using the dt trained using the whole dataset fig 11a and the ones at each iteration of the cross validation procedure i e using three dt model setups with a perturbed training database fig 11b d in particular for the leftmost part related to the track variable outlined by a green rectangle the grouping of tracks is similar for figs 11a and c but differs for fig 11b and is even absent for fig 11d this high sensitivity of the derived structure to the changes in the training dataset has already been identified in the literature breiman 2001 in addition to bringing some confusion regarding the dependencies between levels the drawback is also a poorer predictability this is shown by the low q 2 values for each test case in table 2 however rf achieves a higher predictive capability by adding a random character to the dt construction process at two levels 1 each tree is constructed using a different bootstrap sample 2 each node is split using the best among a subset of mtry input parameters randomly chosen at that node as confirmed by table 2 however high predictability comes at the expense of losing some interpretability i e the ability to represent the structure via the easily understandable tree representation rf being an ensemble of randomized dt models although some developments are available to extract some meaningful rules from rf see e g fokkema 2020 this comparison exercise could be improved regarding different aspects in particular we have used default parametrizations of the tested tree based methods first tuning rf hyperparameters namely mtry and the minimum node size which are respectively set up to the root square of the total number of input variables and to 5 is expected to improve the results probst et al 2019 second we used the default splitting rule based on squared residuals minimization by breiman 2001 this splitting rule is known to favor the selection of variables with many possible splits continuous variables or categorical variables with many levels over variables with few splits as shown by strobl et al 2007 for the estimates of variable importance measures improvements can either rely on some processing of the categorical inputs see an extended discussion by wright and könig 2019 or on alternative splitting rules e g based on the randomized algorithm named extra trees by geurts et al 2006 as tested by jaxa rozen and kwakkel 2018 finally it should be emphasized that one reason for the higher performance of gp models is that they have the ability to exploit the smoothness with respect to the continuous inputs further work may include recent developments on rf for smooth nonlinear relations friedberg et al 2020 6 concluding remarks recommendations and further work model uncertainties related to the structure form of the model or to the unambiguous choice of appropriate physical laws are generally analyzed in computer based studies by defining a categorical variable i e a multilevel indicator to obtain deeper insights into how the simulator output values computed for two levels of the categorical variable correlate the interlevel dependence structure is learned from a series of computer experiments 100 200 by means of a gp based correlation matrix table 3 summarizes the key aspects of the gp approach for three test cases together with a comparison to tree based methods showing that the gp based approach can be seen as a satisfactory compromise because 1 it clearly achieves higher predictability with a q 2 value 90 given a moderate size of the training dataset typically 100 200 whatever the considered case table 2 while reaching high performance with respect to different criteria explainability simplicity stability and 2 the correlation matrices provide a concise and graphical way of interlevel dependence structure using the interpretation provided in sect 2 2 this matrix estimated for three cases in figs 4 6 and 9 is useful to design complementary simulation based studies by focusing on some levels that present some distinct behaviors such as level u 5 for the synthetic case or the end members of the cyclone tracks or by confirming that all levels should be considered such as in the reservoir case the gp revealed interlevel dependence structure should also provide key elements to support the scenario discovery process in particular to nuance the uniformity and independence assumptions of the scenarios quinn et al 2020 bringing the combined gp scenario discovery to an operational level necessitates further investigations the cornerstone of the gp approach is however the careful selection of the kernel model that can take different forms exchangeable ordinal group etc depending on the physically based assumptions for the categorical variable table 1 provides the different options that the environmental modeler can implement a multicriterion selection approach is proposed to question the different kernel options within a transparent procedure this flexibility is well shown in the cyclone application case sect 3 2 where the proposed procedure allows us to confront an a priori physically based assumption i e the cyclone track effect can be summarized by a scalar ordinal variable to alternative views on the dependence structure and to support the evidence of the a priori assumption in the absence of a physical based assumption an optimization search procedure can be envisioned for instance by searching for all subsets of groups or for the rank dimension value of the lr kernel that minimizes all four proposed criteria within a multifold cross validation approach one limitation of the proposed gp based approach is that it does not ensure that a unique kernel model is selected multiple assumptions may eventually turn out to be valid with respect to the four selection criteria or a trade off may be difficult to find in the reservoir case the application sect 3 3 only moderately supports the evidence of some dependence structure the compound symmetric and expert based kernel models perform similarly with respect to the four selection criteria although this result is informative per se in particular in situations where the practitioner is preferably interested in explaining the numerical results additional investigations are necessary to confirm this conclusion which is shown in the synthetic case sect 3 1 where the ordinal assumption was also successfully identified provided that a minimum number of training samples are available on the one hand if additional model runs are computationally affordable a possible option is to rely on an adaptive sampling strategy however this question deserves further investigation in the presence of a mixture of continuous and categorical variables and could be based on recent advances in the context of optimization by pelamatti et al 2019 and munoz zuniga and sinoquet 2020 on the other hand if additional model runs are not possible an option is to aggregate the information provided by the plausible gp models i e the ones that satisfactorily fulfill the criteria while accounting for some weight reflecting their plausibility with respect to the selection criteria this option can take advantage of adequate averaging techniques developed for instance within the bayesian framework as proposed by zhang and taflanidis 2019 for uncertainty quantification and by ginsbourger et al 2008 for optimization problems software and data availability software name kergp developers yves deville david ginsbourger olivier roustant contributors nicolas durrande maintainer olivier roustant roustant insa toulouse fr system requirements windows linux mac program language r availability https cran r project org web packages kergp index html license gpl 3 0 documentation https cran r project org web packages kergp kergp pdf reproducible code a jupyter notebook is available on zenodo at rohmer 2022 it showcases the implementation of gaussian process models using mixed continuous categorical inputs variables with an application on the real case in the domain of marine flooding application case 1 described in sect 3 2 and results discussed in sect 4 3 the design of numerical experiments and the simulation results for application case 1 are provided to re use these numerical simulation results please contact jeremy rohmer brgm j rohmer brgm fr the notebook provides the r scripts to perform the fitting and the performance assessment which can easily be adapted to any other application case declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this research was conducted within the frame of the chair in applied mathematics oquaido gathering partners in technological research brgm cea ifpen irsn safran storengy and academia cnrs ecole centrale de lyon mines saint etienne university of grenoble university of nice university of toulouse around advanced methods for computer experiments data used for real case 1 are based on work funded by the french national research agency within the spicy project anr 14 ce03 0013 data used for real case 2 are based on ultimate co2 project funded by the european commission s seventh framework program fp7 2007 2013 under grant agreement n 281196 we are grateful to prof j rougier as well as to two anonymous reviewers for their comments that led to improvements of the article appendix a design of experiments for the real cases fig a1 design of the experiment for the real cyclone case described in sect 3 2 track indicates the categorical variable related to the selection of the cyclone track depicted in fig 2 the continuous variables normalized between 0 and 1 are the landfall position x o the forward speed v f the radius of maximum winds radius the shift of the maximum wind speed v m and the shift around the central pressure p c the first row provides the boxplots of the continuous variable given each level of track the last column provides the scatter plot of the variable of interest i e the maximum significant wave height max h s versus the continuous variables fig a1 fig a2 design of the experiment for the reservoir real case described in sect 3 3 the categorical variables are the regional hydraulic gradient grad the capillary effect capi the permeability anisotropy and the physical laws for the relative permeability k r the last column provides the boxplot of the variable of interest i e the log10 of the quantity of mobile co2 denoted m co2 given each level of the categorical variables and the scatter plot for the continuous variables normalized between 0 and 1 fig a2 
25609,model uncertainties are generally integrated in environmental long running numerical simulators via a categorical variable by focusing on gaussian process gp models we show how different categorical kernel models exchangeable ordinal group etc can bring valuable insights into the correlation of the simulator output values computed for different levels of the categorical variable i e the interlevel dependence structure supported by two real case applications cyclone induced waves and reservoir modeling we have proposed a cross validation approach to select the most appropriate kernel by finding a trade off between predictability explainability and stability of the covariance coefficients this approach can be used effectively to support some physical assumptions regarding the categorical variable through comparison to tree based techniques we show that gp models can be considered a satisfactory compromise when only a few model runs 100 are available by presenting a high predictability and a concise and graphical way to map the interlevel dependence structure keywords categorical variables computationally intensive simulator metamodel kriging model selection abbreviations cs compound symmetry dt decision tree e expert based gen general gp gaussian process lr low rank o ordinal rf random forest 1 introduction high resolution numerical simulators are key components of environmental science that help to obtain deeper insights into the behavior of natural systems some examples are veeck et al 2020 for hydrologic modeling zhao et al 2013 for agricultural modeling vandromme et al 2020 for landslide modeling abily et al 2016 for urban flooding and idier et al 2020 for marine flooding to model the natural system these simulators all have in common to involve a large spectrum of assumptions related to the choice in the structure form of the model e g 1d versus 2d modeling leandro et al 2009 the selection of the physical processes regarded as relevant and prominent e g account for spatial heterogeneity liu et al 2017 the use of alternative physical laws e g different soil water retention curves silva ursulino et al 2019 the system s future evolution e g future gas emission scenarios le cozannet et al 2015 or land use change mishra et al 2018 etc depending on the modeling assumptions the simulation results can differ hence resulting in model uncertainty termed structural uncertainty e g hill et al 2013 some of these modeling assumptions can be modeled by means of continuous variables such as geotechnical properties of a given soil formation or time series of rainfall conditions at a given location etc some of them involve categorical variables i e multilevel indicators that take up a finite number of discrete values each discrete level of the categorical variable is associated with a different modeling assumption e g level a is associated with modeling assumption a some real case applications are provided in the domain of safety analysis of radioactive waste disposal by storlie et al 2013 earthquake risk assessments by rohmer et al 2014 marine flooding induced by sea level rise by le cozannet et al 2015 reservoir engineering for co2 geological storage by manceau and rohmer 2016 pollution risk analysis and management by lauvernet and helbert 2020 etc quantifying the correlation of the simulator output values computed for two levels i e two modeling assumptions of the categorical variable whatever the values of the other input variables is of high interest to measure the impact of structural uncertainty because it informs whether each level should be treated equivalently with respect to the numerically simulated variable of interest or if there is any dependence among the levels like a group structure this type of structure named the interlevel dependence structure in this study can be useful to identify modeling assumptions that should be considered a priority in complementary simulation based studies for instance a level showing pronounced impact on the variable of interest suggests where to focus the simulation based exploration and a group structure suggests simplifying the analysis by restricting the analysis to a single member of the group beyond structural uncertainty such information may be of interest to support the analysis of deep uncertainty based on scenario discovery kwakkel and jaxa rozen 2016 our objective is to develop a statistical procedure to learn the interlevel dependence structure due to the high computation time cost of numerical environmental simulations we aim to learn the dependence structure with only a few model runs on the order of 100 by relying on the design and analysis of computer experiments santner et al 2003 the key element of the proposed procedure is the use of gaussian process models denoted gp models williams and rasmussen 2006 with covariance functions also known as kernels adapted to handle mixed continuous categorical inputs and combined by tensor products e g roustant et al 2020 qian et al 2008 zhang et al 2020 the kernels specify how similar i e correlated two instances of the variable of interest e g y and y 0 are expected to be at two input values u and u 0 i e in our case at two levels of the categorical input this similarity function can be encoded in different manners depending on the assumptions regarding the categorical variable nominal ordinal interdependence between the levels interactions between given levels etc see for instance roustant et al 2020 and lauvernet and helbert 2020 once fitted the resulting correlation matrix provides the mapping of the dependence structure to illustrate the type of results that can be derived fig 1 a depicts an unknown relationship between a continuous and a categorical input variable with 5 levels i e five modeling assumptions fig 1b depicts the correlation matrix derived from the gp based analysis given 25 model runs a group of highly correlated levels are identified for u 1 4 as well as the decreasing correlation of u 5 with the others from 25 to 15 considering u 4 to u 1 these observations are consistent with the test function if the functional relationship in fig 1a had been known this result would have been straightforward but here the structure is unknown and can be learned only with a limited number of numerical results here with only 25 model runs this result that is further discussed in sect 4 2 depends on how the kernel model is defined which raises the question of model selection that is addressed in the present study by relying on a multicriterion analysis the paper is organized as follows section 2 describes the different steps of the proposed procedure as well as the statistical methods in this latter we provide the formal definition of the interlevel dependence structure related to the gp correlation matrix sect 2 2 by relying on the tensor product of kernels for continuous and categorical variables sect 2 3 a multicriterion approach for selecting the categorical covariance kernel model is also detailed sect 2 4 section 3 describes the application cases i e the synthetic case described in fig 1 and two real cases namely cyclone induced wave numerical modeling rohmer et al 2016 and reservoir modeling of co2 storage manceau and rohmer 2016 both real cases are representative of two distinct situations the cyclone case illustrates a situation where a physical intuition on an a priori influence of the categorical variable is available whereas the reservoir case illustrates the opposite situation where the physical intuition is harder to give the procedure is then applied to each of these cases in section 4 in section 5 we further discuss the results by comparing the gp based procedure to a popular alternative approach based on tree based techniques on this basis practical recommendations are then defined in section 6 2 methods 2 1 description of the procedure the proposed procedure is as follows step 1 we follow the approach of design and analysis of computer experiments santner et al 2003 a series of runs of the expensive to evaluate environmental simulator were performed by considering a limited number of randomly selected input variable configurations step 2 using the set of random computer experiments training dataset different hypotheses regarding the structure of the considered input categorical variable are tested and modeled by means of different kernel covariance formulations see further details in sect 2 2 and 2 3 step 3 the question of selecting the most appropriate kernel is examined by analyzing different aspects i e by considering different criteria as described in sect 2 4 the objective is to select the resulting gp model that can achieve a trade off between the different criteria step 4 since the practitioner is preferably interested in the dependence structure between the modeling assumptions the correlation matrix derived from the covariance matrix is analyzed see further explanation in sect 2 2 this result can be confronted with some a priori physically based interpretation of the categorical variable influence that the practitioner may have before analyzing the computer experiments 2 2 gaussian process for mixed continuous and categorical inputs let us consider the set of d continuous input variables x x 1 x d and the set of j categorical inputs u u 1 u j with n l 1 n l j levels that represent the categorical inputs the output y is then computed using the numerical environmental simulator f as y f x u f w in the context of gaussian process gp modeling also named kriging williams and rasmussen 2006 the function f is assumed to be a realization of a gp y w with a constant mean μ and a covariance function k named kernel that can be written as follows 1 w w k w w cov y w y w let us denote w 1 w n the training samples and y y 1 f w 1 y n f w n denote the corresponding results the prediction at a new observation w is given by the kriging mean y ˆ w as follows 2 y ˆ w e y w y w 1 y 1 y w n y n μ c w t c 1 y μ i where c is the covariance matrix between the points y w 1 y w n whose element is c i j k w i w j c w is the vector composed of the covariance between y w and the points y w 1 y w n and i is the identity vector of length n the prediction at w can be associated with an error estimate provided by the kriging variance σ ˆ 2 given by 3 σ ˆ 2 w var y w y w 1 y 1 y w n y n c w w c w t c 1 c w using the gp mean and variance equations 2 and 3 the prediction interval pi α at the given confidence level α e g α 95 can be computed as follows 4 p i α w y ˆ w σ ˆ w q n 0 1 1 α 2 y ˆ w σ ˆ w q n 0 1 1 α 2 where q n 0 1 is the quantile of order 1 α 2 of the standard normal distribution accounting for a mixture of input variable types continuous or categorical ordinal or nominal is made via the covariance function k w w here it is assumed to be the tensor product of the covariance function for the continuous inputs k cont x x and the ones for the categorical inputs k cat u u as k w w k cont x x i 1 j k cat i u i u i other combination approaches are possible as discussed by roustant et al 2020 the covariance function k cont can be described by kernel models that are commonly used in the computer experiment community in the present study we restrict the analysis to the stationary twice differentiable in the mean square matérn 5 2 model williams and rasmussen 2006 the categorical covariance functions k cat i i 1 j can be described in different manners depending on the assumption related to the categorical input as described in sect 2 3 in practice k cat i can be interpreted under the homoscedastic assumption as the kernel up to a multiplicative constant of the 1d section u i y x u i u i where x and u i u 1 u i 1 u i 1 u j are fixed in practice we prefer using the scaled form of the covariance i e the correlation to ease the interpretation of the dependencies of the simulator output on interactions between the levels of the categorical variable i e of the modeling assumptions with the same notations the correlation kernel derived from k cat i is interpreted as the correlation of the 1d section u i y x u i u i regardless of x and u i thus the inspection of k c a t i reveals the correlation of the simulator output explained by the ith categorical input the others being fixed note that such a correlation does not depend on the remaining inputs which is a result of constructing k by tensor product 2 3 covariance kernel models for categorical inputs table 1 summarizes the different options for defining a categorical covariance function their interpretation and their practical implementation the interested reader can refer to roustant et al 2020 and references therein for a more formal presentation in particular with an analysis of the positive definiteness of covariance matrices note that we restrict the presentation to the case of a single input with n l levels which is hereafter denoted by u the generalization to multiple categorical inputs can be done by following the tensor product formulation described in sect 2 2 when the practitioner assumes that no preference can be given to the n l levels i e all considered scenarios are considered to have the same influence k cat can be described by an exchangeable covariance qian et al 2008 also named the compound symmetry denoted cs function as follows 5 k c a t c s u u σ 2 if u u ρ σ 2 if u u where ρ is a unique correlation coefficient satisfying ρ 1 n l 1 1 when the practitioner assumes that the variable of interest will act differently depending on the considered level but without excluding some dependencies between these different responses k cat can then be described by the most general and complex dependence structure where each pairwise coefficient can take a different value depending on the considered levels u u the covariance function reads as follows 6 k cat gen u u c u u if u u v u if u u the latter structure can be simplified by adding a priori information on the dependence between the levels for instance by relying on expert based information a possible option is to assume that some levels perform similarly and that they can be grouped assume that the n l levels of u are partitioned into g groups and denote g u as the group number corresponding to a given level u then the covariance function can be written as roustant et al 2020 7 k cat e u u k cat gen g u g u c g u g u if g u g u v g u if g u g u where for all i j 1 g the terms c i i v i are within group correlations and c i j v i v j i j are between group correlations the structure can be simplified by assuming that the correlation value for each pair of groups is unique by means of a compound symmetry matrix pinheiro and bates 2006 instead of deriving the groups based on expert information a possible option is to explain the level dependencies by a few key latent continuous variables using a low rank approximation roustant et al 2020 so that the covariance matrix k cat lr can be defined as 8 k cat lr q q t where the matrix q is of size n l q where q is low the higher q is the more likely the danger of overfitting in practice typical values of q are 2 or 3 a value of 1 is not recommended see zhang et al 2020 when the practitioner assumes that the levels can be ordered this means that the categorical variable can be described by an ordinal continuous variable that is not directly observed i e it is said to be latent and the levels are seen as discretized values of this ordinal variable qian et al 2008 the corresponding covariance function can be defined by taking advantage of the tools available for the continuous variables as follows 9 k cat o u u k cont f u f u where k cont is a one dimensional continuous kernel such as the matérn 5 2 model and f is a one dimensional nondecreasing function also called warping f 1 n l r the warping function can be modeled via a parametric model e g the cdf of a flexible probability distribution such as normal or beta or a nonparametric model e g a piecewise linear or a quadratic spline as detailed by roustant et al 2020 2 4 kernel model selection as mentioned above different kernel modeling choices can be made to represent the categorical variable raising the question of selecting the most appropriate kernel covariance model depending on the modeling objective different approaches exist for selecting an optimal model with respect to a specific criterion burnham and anderson 2002 for instance a model that satisfactorily represents i e explains the relationships between inputs and outputs might not necessarily perform as well for prediction therefore we propose to examine different viewpoints on the problem of kernel selection by analyzing different criteria this multicriterion approach shares similarities with the data science framework proposed by yu and kumbier 2020 who advocate analyzing three core principles predictability computability and stability to select the most appropriate kernel model given the training dataset we investigate whether the considered gp model is capable of predictability this concept is related to whether the gp model is capable of predicting yet unseen input configurations i e samples that have not been used for training which can be assessed by using independent test samples bootstrap or cross validation approaches e g hastie et al 2009 two indicators are estimated the first indicator denoted q 2 measures the deviation from the true output value given a test set t q 2 is defined as follows 10 q 2 1 i t y i y ˆ i 2 i t y i y ˆ i 2 where y ˆ i is the ith gp based prediction of the model output y i and y 1 t i t y i is the average value for the test set a coefficient q 2 close to 1 0 indicates that the gp model is successful in matching the new observations that have not been used for the training for the sake of comparability between the different criteria we consider 1 q 2 the second criterion is related to the coverage denoted ca of the prediction intervals pi α eq 4 at the given confidence level α calculated on the test set t defined by 11 c a 1 t i t 1 y i p i α w i where 1 a is the indicator function ca evaluates whether the model output y i is within the bounds of the prediction interval the gp derived pi α is optimal when ca is close to the theoretical value of α for the sake of comparability between the different criteria we consider the error in coverage defined as 1 ca explainability and simplicity the former concept relates to whether the considered gp model is capable of representing the data for instance by analyzing the likelihood l however adding more model parameters results in increasing the explainability to counterbalance this tendency related to overfitting a penalty term is generally introduced see e g höge et al 2018 to select a simpler model here simplicity refers to the number of gp model parameters but alternative definitions exist see for instance rougier and priebe 2020 who introduce the concept of flexibility by assuming that the true gp model exists and that it is among the set of candidate gp models we propose relying on the bayesian information criterion bic schwarz 1978 described as follows 12 b i c 2 log l k log n where k is the number of parameters and n is the number of observations stability we explore to what extent the kernel correlation matrix derived from the covariance matrix is stable to the perturbations in the training dataset we evaluate an error measure between the correlation matrices r 0 ˆ estimated using the gp model fitted with the whole training dataset and r j ˆ estimated at the jth iteration of the n cv fold cross validation procedure for instance when n cv 5 the training dataset corresponds to the whole training dataset from which 20 of the observations have been randomly removed the error measure is defined by 13 e r r 1 n cv k 1 n c v r 0 ˆ r k ˆ f 2 where f is the frobenius matrix norm by restricting to the nondiagonal elements in the upper triangular part of r ˆ eq 13 is calculated as 1 n cv k 1 n c v 1 n p i 1 n p r ˆ i 0 r ˆ i k 2 where r ˆ i 0 is the ith coefficient of r 0 ˆ read in column order for instance r ˆ i k is the ith coefficient of r k ˆ and n p is the number of terms of r ˆ note that alternative formulations could also be proposed to better account for the correlation matrix structure such as the diagonally weighted matrix norm proposed by cressie and hardouin 2019 3 description of the application cases in this section we describe the application cases namely a synthetic test function sect 3 1 and two real cases in the domain of cyclone induced wave modeling sect 3 2 and reservoir engineering sect 3 3 3 1 synthetic case we first consider a synthetic test function using a modified version of the two dimensional branin function 1 1 http www sfu ca ssurjano branin html where one continuous variable u is assumed to reach only discrete values as follows 14 y h x 20 if u 1 h x 10 if u 2 h x 7 5 if u 3 h x 5 0 if u 4 5 h x 20 if u 5 where h x z z 5 4 π 2 x 2 5 π x 6 10 1 1 8 π cos x 10 with x 5 10 by construction levels 1 4 are highly correlated see fig 1a we consider the different categorical kernels defined in table 1 with the following assumptions expert based groups two experts have given their opinions the first expert denoted e indicates a realistic grouping of levels i e consistent with the true function and the second one denoted ew indicates an unrealistic grouping i e u 1 u 3 and u 2 u 4 u 5 low rank approximation a matrix with rank of q 2 is tested denoted lr2 ordinal variable the levels are ordered by following the level index for the expert based grouping we assume that another expert does not know the correct ordering and assumes an unrealistic order denoted ow i e u 4 u 2 u 5 u 3 u 1 the training dataset is defined through random sampling by considering m x points per u level with m 4 5 and 6 i e with different training dataset sizes of 20 25 and 30 3 2 real case application 1 cyclone induced waves the first real case is based on rohmer et al 2016 and deals with the modeling of waves induced by cyclones at sainte suzanne city located in the northeast of reunion island fig 2 b the aim is to analyze the evolution of the significant wave height h s maximum value over time as a function of the cyclone characteristics these parameters are modeled by means of five scalar continuous input parameters namely the maximum wind speed the radius of maximum winds i e the distance from the cyclone eye at which the maximum wind intensity is reached the shift around the central pressure the forward speed defined as the translation speed of the cyclone eye and the landfall position which both characterize the minimum distance and the relative position of the track to the studied site a set of seven historical cyclone tracks are considered these tracks are randomly shifted via the continuous input variable modeling the relative position of the track from their original track so that they cross the center of reunion island see fig 2b a categorical variable is defined here with each level corresponding to a given track a series of 100 computer experiments were performed by randomly sampling the inputs using a latin hypercube sampling approach combined with a maximin criterion johnson et al 1990 the design of the experiments is presented in appendix a accounting for the spatial variability of the track as illustrated in fig 2b requires integrating each cyclone s spatial position along the track as an input variable of the gp models which is hardly feasible in practice an a priori physical interpretation of the track influence speculates that h s is strongly related to the angle of approach of the cyclone in the vicinity of the studied site which increases from 0 east direction to 180 west direction 90 is north confirmed by the sensitivity analysis of rohmer et al 2016 this means that the track effect can be summarized by a single scalar continuous input i e in relation to the angle of approach the analysis of the boxplots in fig 2a seems to support this hypothesis in particular the median value of the maximum h s appears to increase as the angle of approach increases from 0 to 90 from gael to dumile cyclone however the tendency from 90 to 180 from dumile to banzi cyclone is less clear especially for banzi the difficulty in the interpretation may here be related to the complexity of this cyclone track compared to the quasilinear shape of the others fig 2b to support the evidence of the influence of the angle of approach a more rigorous analysis is needed here the validity of this hypothesis is further investigated using a gp model with different categorical kernels as described in table 1 with the following assumptions expert based groups a first assumption relies on the selection of two groups one composed of the 4 cyclones gael giovanna hollanda and dumile coming from the northeastern ne quadrant and another group composed of 3 tracks bejisa haliba and banzi coming from the northwestern nw quadrant fig 2 a second assumption based on three groups is also tested by differentiating the track whose angle is almost at 90 north namely the dumile cyclone fig 2 in addition two assumptions are made regarding the link between the groups by specifying a general assumption e2 and e3 or a compound symmetry covariance assumption e2cs and e3cs low rank approximation ranks of q 2 and q 3 are tested kernels denoted lr2 and lr3 respectively ordinal variable the levels are ordered following the angle of approach 3 3 real case application 2 co2 geological storage the second real case application corresponds to the modeling of the long term fate of stored co2 in a deep aquifer on a potential project in the paris basin france as described by manceau and rohmer 2016 the injection of 30 mt of co2 over 30 years in the lower triassic sandstone formation at a depth of approximately 1000 m was numerically simulated the evolution of the quantity of mobile co2 for a time period of 150 years after the injection stopped was investigated as a function of two continuous input variables namely the porosity and the intrinsic permeability of the aquifer rock formation four categorical input variables related to the assumptions for permeability anisotropy minor medium and large regional hydraulic gradient absence and activated capillary effect absence and activated and the choice in the physical law used to model the relative permeability as a function of co2 saturation ten choices as depicted in fig 3 due to the importance of the latter parameter as shown by manceau and rohmer 2016 the following analysis focuses on this variable a series of 100 computer experiments was performed by randomly sampling the inputs using a latin hypercube sampling approach combined with a maximin criterion a base 10 logarithm transformation was applied to the quantity of mobile co2 due to the large asymmetry of its distribution the design of the experiments is presented in appendix a unlike the case described in sect 3 2 it is harder to give a physical intuition on an a priori influence of the categorical variable related to the relative permeability laws this lack of intuition is related to the richness of the information associated with the process of residual trapping that is related to different aspects 1 the capacity of the porous medium to allow the flow of the gaseous phase in the presence of another phase called relative permeability is represented as a function of the gas saturation in the porous medium see examples in fig 3c this capacity is associated with a potential hysteretic effect resulting in a nonunique dependence during the flow over time see further details in juanes et al 2006 2 the capacity of the alternative phase water is represented by another function of the saturation fig 3d and 3 the considered phase gaseous or aqueous progressively becomes isolated when its saturation decreases in a porous medium leading to saturation that cannot be reduced these specific situations are called residual saturations for the gaseous phase fig 3b and irreducible saturations for the aqueous phase to help formulate a physically based assumption about the interdependencies the boxplots in fig 3a allow us to identify some specific law behaviors especially for law 1 some tendency can also be noticed when ordering the laws in a specific order with respect to the median values of the variable of interest to obtain a clearer picture we test the validity of these observations via the proposed gp based approach by considering the different categorical kernels described in table 1 with the following assumptions expert based groups the grouping should account for the three facets of the co2 flow in porous media i e by integrating the three pieces of information depicted in fig 3b d dissimilarities in both the imbibition drainage curve shape and residual trapping model on this basis the following grouping is proposed law 2 law 10 law 6 9 law 1 law 3 law 4 law 5 in addition an assumption is made regarding the link between the groups by specifying a general or a compound symmetry covariance assumption denoted e and ecs respectively low rank approximation a rank of q 2 and of q 6 i e of the same number of expert based groups are tested kernels denoted lr2 and lr6 ordinal variable the levels are ordered with respect to the value of the maximum co2 residual saturation fig 3b the three other categorical variables are modeled as follows the regional hydraulic gradient and the capillary effect are both assigned a compound symmetric kernel an ordinal kernel with spline based warping is defined for the permeability anisotropy because there is a natural ordering of the levels 4 results 4 1 implementation procedure in this section we apply the gp based procedure described in sect 2 to the cases described in sect 3 to ensure identifiability of the model defined with k we treat k c o n t as a covariance kernel and each k cat j as a correlation kernel j 1 j for all gp models we consider a matérn 5 2 covariance matrix for the continuous variables for the ordinal kernel k cat o a matérn 5 2 continuous kernel is combined with a quadratic nondecreasing spline warping function f see eq 9 we consider gp models with constant trends and fit them using the r package kergp deville et al 2018 by applying a prescaling and centering of the continuous input variables and of the variable of interest the covariance parameters are estimated via a maximum likelihood approach using the derivative free constrained optimizer by linear approximations named cobyla developed by powell 1994 with 250 randomly selected initial starts the predictability and stability were assessed via a 5 fold cross validation procedure repeated 25 times 4 2 application to the synthetic case considering the synthetic test case fig 4 depicts the gp derived correlation matrices for each of the kernel assumptions described in sect 3 1 using the training dataset of intermediate size m 5 the application of the expert based and ordinal kernel assumptions fig 4c e shows some consistent structures among the levels namely the highly correlated group for u 1 to u 4 and the particular behavior of u 5 the magnitude of the interdependencies between the identified group and u 5 differs slightly assumption e indicates a low to moderate correlation 10 whereas assumption o indicates a decreasing correlation from 25 to 15 for u 4 1 the structure of interlevel dependencies is richer for the general fig 4a and lr2 based gp models fig 4b for these correlation matrices the interpretation is also less straightforward than for e or o for which some group structures are more easily identified the visual inspection of both matrices suggests however some interesting features i e the increasing correlation between u 1 4 for gen and the particular behavior of u 5 as well as a possible grouping of u 2 4 for lr2 capturing the correlation structure can be hard for these cases given the size of the training dataset of 25 this is further discussed below when analyzing the stability criterion in this case the exchangeable assumption i e compound symmetry cs leads to a low to moderate intercorrelation coefficient of 40 not shown the four criteria for kernel model selection are examined in fig 5 for the sake of comparability between the different assumptions for m we preferably plot the difference of bic with the minimum value over the whole experiment named δbic several observations can be made explainability measured by bic fig 5a appears efficient here to exclude the unrealistic assumptions ow and ew they present very large bic differences regardless of m for instance burnham and anderson 2002 suggested a difference of the considered information criterion relative to the minimum value of at least 10 to support the ranking between model candidates with confidence bic appears to be very informative to discriminate the kernel assumptions and clearly selects the ordinal assumption as the most appropriate the q 2 criterion is commonly used in the computer experiment community to rank different models with respect to their predictive capability in our case basing the analysis on this unique criterion is difficult at low size of the training dataset m 4 models e cs and o all minimize 1 q 2 and show similar performance see median values in fig 5b selecting one of them is thus hardly achievable furthermore the model associated with the unrealistic expert based grouping ew has a satisfactory predictive capability at low m values although the width of the confidence interval is larger than the others for larger m value here m 6 i e with the largest size of the training dataset the q 2 criterion allows us to identify model o as the most appropriate model with respect to the predictive capability log10 1 q 2 is the lowest in fig 5b the other facet of predictability related to the coverage of the prediction intervals suggests discarding kernel assumptions gen and lr2 because ca is here far larger than the level of the prediction interval but hardly allows differentiating the other assumptions regardless of m fig 5c the stability criterion fig 5d tends to suffer from the same sensitivity as q 2 to the size of the training dataset but has a higher discriminative power at low m values m 4 both kernel models cs and o are selected as very stable because of low log10 err values in fig 5d however a high stability is also reached for ew for a large m value the ordinal assumption is then clearly selected as the assumption leading to the most stable correlation matrix from this analysis we can conclude that the ordinal assumption allows us to successfully fulfill three of the criteria explainability predictability and stability especially at sufficiently high m values m 5 for coverage the ordinal assumption is not ranked first but ca appears of a reasonable order of magnitude median value of 85 i e with moderate deviation from the level of the 95 predictive interval for the small size of the training dataset m 4 unambiguously rejecting selecting ew and cs becomes difficult if the explainability criterion bic criterion is not considered which can partly be explained by the analysis of the ew based correlation matrix fig 4d which reveals a quasi homogeneous structure with correlation coefficients ranging from 44 to 56 i e of the same order of magnitude as the cs assumption of 40 hence indicating that both gp models should perform similarly suggesting that at a low number of training points the cs assumption remains the most reasonable assumption when the goal is the joint maximization of predictability stability and explainability 4 3 application to the real case application 1 considering the cyclone test case fig 6 depicts the gp derived correlation matrices for each of the kernel assumptions described in sect 3 2 fig 6a reflects the hypothesis of a single group without difference in effect on maximum significant wave height between the tracks the derived correlation appears to be high 80 the application of alternative kernel assumptions shows some consistent structures among the cyclone tracks two groups of cyclones appear to be highly correlated coefficient 75 namely those coming from ne and those coming from nw as shown in fig 6b general formulation and fig 6 e f e2 and e2cs the high correlation among these groups is also indicated by the other assumptions 1 the low rank approximation lr2 and lr3 fig 6c and d but these assumptions lead to a richer interdependency structure 2 to a lesser extent the expert based assumption e3cs fig 6 h however we note that there is ambiguity regarding the dumile cyclone see in particular fig 6g which is highly correlated with either ne cyclones e2 e2cs or nw cyclones lr3 e3cs the different assumptions all suggest a moderate correlation on the order of 40 60 between groups of cyclones coming from ne and nw these observations are consistent with the ordinal assumption fig 6h which indicates a decreasing correlation from gael the track with the lowest angle of approach to the banzi cyclone the track with the largest angle of approach and a central role of dumile with a decreasing correlation either with the ne cyclone or with the nw cyclone the four criteria of kernel model selection are examined in fig 7 regarding explainability and simplicity the ordinal assumption o appears to be the most appropriate the derived gp model presents the minimum bic value and the differences with the alternative models are large here they are larger than 20 regarding predictability the ordinal assumption also leads to the best model regarding this criterion however the predictability of the expert based model candidates e2 e2cs e3 e3cs remains of moderate to high degree with a median value of q 2 of approximately 90 regarding ca the use of e3cs allows us to reach the level of the 95 prediction interval but alternative assumptions e2 e2cs e3cs cs and o lead to satisfactory coverage of the prediction intervals as well finally fig 6d indicates the poor stability of the gen and lr3 kernels i e the high sensitivity of the estimates of the correlation coefficients which may be related to the high number of coefficients to estimate 21 and 11 respectively also showing the satisfactory stability of two alternative assumptions i e the expert based ones and the ordinal one on this basis we can conclude that the ordinal assumption allows us to reach a satisfactory trade off between the four criteria which appears to be consistent with the aforementioned physical intuition the latent continuous variable here is related to the angle of approach see sect 3 2 fig 8 a summarizes this dependence via the spline based warping function f used to set up the ordinal covariance kernel model k cat o see eq 9 this shows a strong link between ne cyclones gael giovanna and to a lesser extent for hollanda as well and a quasilinear increasing influence to banzi this warping is the basis for computing the correlation matrix fig 6i to ease the interpretation let us focus on a single row of fig 6i i e the pairwise correlation between the gael track and the others fig 8b provides a clear indication of a highly correlated group of cyclones coming from ne within the red colored envelope with a correlation coefficient exceeding 80 and a decreasing correlation with those coming from nw the analysis of the correlations derived from the cross validation iterations for each repetition gray lines in fig 8b confirms this result more than 75 of the cross validation derived results show high correlation 75 of dumile with the ne cyclones hence in agreement with e2 assumption fig 6e compared to o the analysis of e2 performance criteria shows that this assumption can also be considered reasonable with very satisfactory stability of the correlation coefficients fig 7d although the criterion values appear to be higher the stability criterion appears to be lower than the one for o which may be related to the lower number of correlation coefficients to be estimated of 2 for the e2 assumption and of 7 for the o assumption 4 4 application to the real case application 2 considering the co2 geological storage test case fig 9 depicts the gp derived correlation matrices for each of the kernel assumptions described in set 3 3 some consistent structures can be noticed regarding law 1 which appears to be anticorrelated with the others as shown in fig 9a general assumption and in fig 9 b c low rank approximation lr2 and lr6 the specificity of law 1 is also outlined by the expert based grouping in fig 9d which indicates here a moderate positive correlation of 35 40 in agreement with the ordinal assumption fig 9f which indicates that the pairwise correlation coefficients of law 1 with the others see last row of fig 9f rapidly decrease although disagreeing on the correlation magnitude the lr2 gen and e models all suggest a moderate correlation among all laws except for law 1 the assumption lr6 also suggests the particular behavior of law 5 which goes in the same direction as the expert based clustering of considering it as belonging to a single group the assumption ecs i e using a cs between group covariance assumption leads to a less complex correlation structure and clearly highlights the grouping of laws 6 9 as suggested by the experts see also the laws outlined in dark blue in fig 3 which is in agreement with the group of highly correlated laws as outlined by fig 9f though the size of the group is larger and includes laws 2 and 4 as well to summarize the inspection of the correlation matrices is more difficult here than for the cyclonic application case sect 4 3 where all assumptions more or less agree regarding the information supplied by the modelers nevertheless this inspection highlights the specificities of law 1 and law 5 lr6 assumption which both strongly differ from the other laws this is suggested by the irreducible water saturation yellow and medium blue curve in fig 3c and d law 1 is even more different with an irreducible water saturation associated with a large maximum gas residual saturation which seems to explain why the model behaves so specifically when this law is accounted for the four criteria for kernel model selection are examined in fig 10 for the considered case we show that the expert based ecs i e with the simplified correlation structure between the groups and the cs assumption both lead to gp models that satisfactorily fulfill the four criteria with a slightly higher performance for the simpler structure of cs where the expert based grouping of laws in particular laws 6 9 in dark blue in fig 8 is informative in the sense that it leads to a competitive gp model but only makes a slight difference with the simpler structure especially regarding explainability with bic difference 10 and stability due to the lowest number of cs correlation coefficients namely 1 note that due to the high number of covariance parameters relative to the 100 simulation runs gen and lr6 score poorly here from this analysis we can conclude that given the 100 simulation results there is only mild evidence supporting the assumption of the structure associated with the permeability laws that was intuited from the analysis of the boxplots in fig 3a and of the curve similarities fig 3b d similar to the synthetic case additional simulation results should be performed to unambiguously discriminate the most appropriate kernel assumption 5 comparison to tree based methods in practice gp models are not the first statistical modeling option that comes to mind when addressing the problem of categorical variables a popular approach relies on tree based methods such as regression decision trees denoted dt breiman 1984 and random forest regression denoted rf breiman 2001 which both natively handle categorical predictors without having to first transform them e g by using feature engineering techniques because they are based on binary recursive partitioning examples of real case applications are provided by jaxa rozen and kwakkel 2018 and rohmer et al 2018 from a practical viewpoint the advantage of dt is to provide the structure of interlevel dependence as well as the interactions with the other input variables with a graphical presentation of the results in the form of a tree which greatly eases the interpretation for instance in the cyclone real case application fig 11 a gives the tree structure derived from the analysis of the cyclone real case however from the analysis of fig 11 we note several differences between the structure constructed using the dt trained using the whole dataset fig 11a and the ones at each iteration of the cross validation procedure i e using three dt model setups with a perturbed training database fig 11b d in particular for the leftmost part related to the track variable outlined by a green rectangle the grouping of tracks is similar for figs 11a and c but differs for fig 11b and is even absent for fig 11d this high sensitivity of the derived structure to the changes in the training dataset has already been identified in the literature breiman 2001 in addition to bringing some confusion regarding the dependencies between levels the drawback is also a poorer predictability this is shown by the low q 2 values for each test case in table 2 however rf achieves a higher predictive capability by adding a random character to the dt construction process at two levels 1 each tree is constructed using a different bootstrap sample 2 each node is split using the best among a subset of mtry input parameters randomly chosen at that node as confirmed by table 2 however high predictability comes at the expense of losing some interpretability i e the ability to represent the structure via the easily understandable tree representation rf being an ensemble of randomized dt models although some developments are available to extract some meaningful rules from rf see e g fokkema 2020 this comparison exercise could be improved regarding different aspects in particular we have used default parametrizations of the tested tree based methods first tuning rf hyperparameters namely mtry and the minimum node size which are respectively set up to the root square of the total number of input variables and to 5 is expected to improve the results probst et al 2019 second we used the default splitting rule based on squared residuals minimization by breiman 2001 this splitting rule is known to favor the selection of variables with many possible splits continuous variables or categorical variables with many levels over variables with few splits as shown by strobl et al 2007 for the estimates of variable importance measures improvements can either rely on some processing of the categorical inputs see an extended discussion by wright and könig 2019 or on alternative splitting rules e g based on the randomized algorithm named extra trees by geurts et al 2006 as tested by jaxa rozen and kwakkel 2018 finally it should be emphasized that one reason for the higher performance of gp models is that they have the ability to exploit the smoothness with respect to the continuous inputs further work may include recent developments on rf for smooth nonlinear relations friedberg et al 2020 6 concluding remarks recommendations and further work model uncertainties related to the structure form of the model or to the unambiguous choice of appropriate physical laws are generally analyzed in computer based studies by defining a categorical variable i e a multilevel indicator to obtain deeper insights into how the simulator output values computed for two levels of the categorical variable correlate the interlevel dependence structure is learned from a series of computer experiments 100 200 by means of a gp based correlation matrix table 3 summarizes the key aspects of the gp approach for three test cases together with a comparison to tree based methods showing that the gp based approach can be seen as a satisfactory compromise because 1 it clearly achieves higher predictability with a q 2 value 90 given a moderate size of the training dataset typically 100 200 whatever the considered case table 2 while reaching high performance with respect to different criteria explainability simplicity stability and 2 the correlation matrices provide a concise and graphical way of interlevel dependence structure using the interpretation provided in sect 2 2 this matrix estimated for three cases in figs 4 6 and 9 is useful to design complementary simulation based studies by focusing on some levels that present some distinct behaviors such as level u 5 for the synthetic case or the end members of the cyclone tracks or by confirming that all levels should be considered such as in the reservoir case the gp revealed interlevel dependence structure should also provide key elements to support the scenario discovery process in particular to nuance the uniformity and independence assumptions of the scenarios quinn et al 2020 bringing the combined gp scenario discovery to an operational level necessitates further investigations the cornerstone of the gp approach is however the careful selection of the kernel model that can take different forms exchangeable ordinal group etc depending on the physically based assumptions for the categorical variable table 1 provides the different options that the environmental modeler can implement a multicriterion selection approach is proposed to question the different kernel options within a transparent procedure this flexibility is well shown in the cyclone application case sect 3 2 where the proposed procedure allows us to confront an a priori physically based assumption i e the cyclone track effect can be summarized by a scalar ordinal variable to alternative views on the dependence structure and to support the evidence of the a priori assumption in the absence of a physical based assumption an optimization search procedure can be envisioned for instance by searching for all subsets of groups or for the rank dimension value of the lr kernel that minimizes all four proposed criteria within a multifold cross validation approach one limitation of the proposed gp based approach is that it does not ensure that a unique kernel model is selected multiple assumptions may eventually turn out to be valid with respect to the four selection criteria or a trade off may be difficult to find in the reservoir case the application sect 3 3 only moderately supports the evidence of some dependence structure the compound symmetric and expert based kernel models perform similarly with respect to the four selection criteria although this result is informative per se in particular in situations where the practitioner is preferably interested in explaining the numerical results additional investigations are necessary to confirm this conclusion which is shown in the synthetic case sect 3 1 where the ordinal assumption was also successfully identified provided that a minimum number of training samples are available on the one hand if additional model runs are computationally affordable a possible option is to rely on an adaptive sampling strategy however this question deserves further investigation in the presence of a mixture of continuous and categorical variables and could be based on recent advances in the context of optimization by pelamatti et al 2019 and munoz zuniga and sinoquet 2020 on the other hand if additional model runs are not possible an option is to aggregate the information provided by the plausible gp models i e the ones that satisfactorily fulfill the criteria while accounting for some weight reflecting their plausibility with respect to the selection criteria this option can take advantage of adequate averaging techniques developed for instance within the bayesian framework as proposed by zhang and taflanidis 2019 for uncertainty quantification and by ginsbourger et al 2008 for optimization problems software and data availability software name kergp developers yves deville david ginsbourger olivier roustant contributors nicolas durrande maintainer olivier roustant roustant insa toulouse fr system requirements windows linux mac program language r availability https cran r project org web packages kergp index html license gpl 3 0 documentation https cran r project org web packages kergp kergp pdf reproducible code a jupyter notebook is available on zenodo at rohmer 2022 it showcases the implementation of gaussian process models using mixed continuous categorical inputs variables with an application on the real case in the domain of marine flooding application case 1 described in sect 3 2 and results discussed in sect 4 3 the design of numerical experiments and the simulation results for application case 1 are provided to re use these numerical simulation results please contact jeremy rohmer brgm j rohmer brgm fr the notebook provides the r scripts to perform the fitting and the performance assessment which can easily be adapted to any other application case declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this research was conducted within the frame of the chair in applied mathematics oquaido gathering partners in technological research brgm cea ifpen irsn safran storengy and academia cnrs ecole centrale de lyon mines saint etienne university of grenoble university of nice university of toulouse around advanced methods for computer experiments data used for real case 1 are based on work funded by the french national research agency within the spicy project anr 14 ce03 0013 data used for real case 2 are based on ultimate co2 project funded by the european commission s seventh framework program fp7 2007 2013 under grant agreement n 281196 we are grateful to prof j rougier as well as to two anonymous reviewers for their comments that led to improvements of the article appendix a design of experiments for the real cases fig a1 design of the experiment for the real cyclone case described in sect 3 2 track indicates the categorical variable related to the selection of the cyclone track depicted in fig 2 the continuous variables normalized between 0 and 1 are the landfall position x o the forward speed v f the radius of maximum winds radius the shift of the maximum wind speed v m and the shift around the central pressure p c the first row provides the boxplots of the continuous variable given each level of track the last column provides the scatter plot of the variable of interest i e the maximum significant wave height max h s versus the continuous variables fig a1 fig a2 design of the experiment for the reservoir real case described in sect 3 3 the categorical variables are the regional hydraulic gradient grad the capillary effect capi the permeability anisotropy and the physical laws for the relative permeability k r the last column provides the boxplot of the variable of interest i e the log10 of the quantity of mobile co2 denoted m co2 given each level of the categorical variables and the scatter plot for the continuous variables normalized between 0 and 1 fig a2 
