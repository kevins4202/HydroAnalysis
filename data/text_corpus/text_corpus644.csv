index,text
3220,real time precipitation forecasts analysis 4 4 1 overall forecast accuracy 4 4 2 extreme precipitation forecast example 5 discussion 6 conclusion acknowledgements appendix a supplementary data alanazi 2012 64 76 a barnes 2007 1140 1147 l batuwita 2013 83 99 r imbalancedlearningfoundationsalgorithmsapplications classimbalancelearningmethodsforsupportvectormachines bergstra 2012 j bertacchiuvo 2001 259 271 c burger 2013 3429 3449 g cannon 2015 6938 6959 a cattoen 2020 1655 1673 c chen 2019 982 994 h chen 2013 4187 4205 j chivers 2020 125126 b elshorbagy 2010 1943 1961 a esmaeilbeigi 2020 492 502 m famien 2018 313 338 a gagne 2014 1024 1043 d he 2016 8217 8237 x heredia 2018 2021 2040 m hirose 2006 61 63 n hirose 2019 689 710 h huang 2020 124687 s iida 2006 n a n a y jose 2022 1 25 d kalra 2012 a kato 2006 129 153 t kato 2018 846 859 r kubota 2009 203 222 t kumar 2019 2221 a lafon 2013 1367 1381 t li 2010 h ma 2021 w makihara 1996 751 762 y makihara 2000 63 111 y maurya 2020 e1915 r michelangeli 2009 p moghim 2017 1867 1884 s moghim 2017 621 636 s nagata 2011 37 50 k najafi 2011 650 664 m ngounguelangue 2021 893 912 c pierce 2015 2421 2442 d polson 2011 1 23 n sachindra 2018 240 258 d saito 2006 1266 1298 k schultz 2010 249 254 d sugimoto 2020 17 22 s takido 2016 185 195 k teutschbein 2012 12 29 c tsuji 2020 859 876 h vandal 2019 557 570 t 1995 naturestatisticallearningtheory vapnik 1998 v statisticallearningtheory whan 2018 3651 3673 k wood 2002 acl 6 a xiao 2022 151679 s xu 2019 601 615 l yamamoto 2021 1 15 k yang 2018 609 623 x yoshikane 2022 e0000016 t yoshimura 2008 22 26 k yu 2017 92 104 p zarei 2021 677 689 m yinx2022x128125 yinx2022x128125xg full 2022 06 29t03 03 58z fundingbody japan le tracking http creativecommons org licenses by nc nd 4 0 nr justice publishacceptedmanuscriptindexable http www elsevier com open access userlicense 1 0 2024 07 01t00 00 00 000z http creativecommons org licenses by nc nd 4 0 this is an open access article under the cc by nc nd license 2022 the author s published by elsevier b v 2022 07 07t06 30 41 078z http vtw elsevier com data voc addontypes 50 7 aggregated refined 0 item s0022 1694 22 00700 4 s0022169422007004 1 s2 0 s0022169422007004 10 1016 j jhydrol 2022 128125 271842 2022 12 08t09 03 37 529035z 2022 09 01 2022 09 30 unlimited japanletr 1 s2 0 s0022169422007004 main pdf https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169422007004 main application pdf 4b0285b3edd7313ce2564528366eaf7c main pdf main pdf pdf true 8001194 main 13 1 s2 0 s0022169422007004 main 1 png https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169422007004 preview image png 4123380e7d190d515cbb4afac18f24f9 main 1 png main 1 png png 60146 849 656 image web pdf 1 1 s2 0 s0022169422007004 gr2 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169422007004 gr2 downsampled image jpeg 4be90c6e80f97c866ec10430382d9485 gr2 jpg gr2 gr2 jpg jpg 32953 199 533 image downsampled 1 s2 0 s0022169422007004 gr1 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169422007004 gr1 downsampled image jpeg 0cbe6895767ec5fc46ba1c4995287918 gr1 jpg gr1 gr1 jpg jpg 42264 261 533 image downsampled 1 s2 0 s0022169422007004 gr4 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169422007004 gr4 downsampled image jpeg a712b50ded2387e8831a266f8a83c11a gr4 jpg gr4 gr4 jpg jpg 62340 333 696 image downsampled 1 s2 0 s0022169422007004 gr3 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169422007004 gr3 downsampled image jpeg 21a5eb1573d81c2a3a3cd83d87bafbdb gr3 jpg gr3 gr3 jpg jpg 94138 397 734 image downsampled 1 s2 0 s0022169422007004 gr6 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169422007004 gr6 downsampled image jpeg f099388119a031b99a59bbe17721a6ca gr6 jpg gr6 gr6 jpg jpg 91897 390 711 image downsampled 1 s2 0 s0022169422007004 gr5 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169422007004 gr5 downsampled image jpeg d01367a395335d8ebe2273b5357efb78 gr5 jpg gr5 gr5 jpg jpg 72801 479 685 image downsampled 1 s2 0 s0022169422007004 gr8 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169422007004 gr8 downsampled image jpeg 1df1aed63369aad69dbb6746103101e9 gr8 jpg gr8 gr8 jpg jpg 95887 582 711 image downsampled 1 s2 0 s0022169422007004 gr7 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169422007004 gr7 downsampled image jpeg c5bfdfe2474a5d90cdca8ac954fe6aa8 gr7 jpg gr7 gr7 jpg jpg 101191 573 711 image downsampled 1 s2 0 s0022169422007004 gr9 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169422007004 gr9 downsampled image jpeg 270ac0d3049fca74282b1b86e808332c gr9 jpg gr9 gr9 jpg jpg 120307 645 624 image downsampled 1 s2 0 s0022169422007004 gr2 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169422007004 gr2 thumbnail image gif d899ae00437fd1ee1614921d05503712 gr2 sml gr2 gr2 sml sml 5846 82 219 image thumbnail 1 s2 0 s0022169422007004 gr1 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169422007004 gr1 thumbnail image gif c82a4fa83717a73ab13de2304850fd80 gr1 sml gr1 gr1 sml sml 9923 107 219 image thumbnail 1 s2 0 s0022169422007004 gr4 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169422007004 gr4 thumbnail image gif 7b799e16f33536cdfac0cedf23adccfd gr4 sml gr4 gr4 sml sml 7079 105 219 image thumbnail 1 s2 0 s0022169422007004 gr3 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169422007004 gr3 thumbnail image gif c81111bee83ad31b2fe0db0ee689f62a gr3 sml gr3 gr3 sml sml 11143 118 219 image thumbnail 1 s2 0 s0022169422007004 gr6 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169422007004 gr6 thumbnail image gif b22b4de68100b52b3d4c41cff7bd0cc8 gr6 sml gr6 gr6 sml sml 11447 120 219 image thumbnail 1 s2 0 s0022169422007004 gr5 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169422007004 gr5 thumbnail image gif 7d889dfb840abe4cf890dd222f037e14 gr5 sml gr5 gr5 sml sml 10236 153 219 image thumbnail 1 s2 0 s0022169422007004 gr8 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169422007004 gr8 thumbnail image gif f77ab5aacd4e60199ae8f0d13251bde3 gr8 sml gr8 gr8 sml sml 10224 164 200 image thumbnail 1 s2 0 s0022169422007004 gr7 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169422007004 gr7 thumbnail image gif cc9ba09dede2d35590fb3ee9587cea01 gr7 sml gr7 gr7 sml sml 11164 164 203 image thumbnail 1 s2 0 s0022169422007004 gr9 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169422007004 gr9 thumbnail image gif 403660a3b6670a7c23ea5e9134d4e399 gr9 sml gr9 gr9 sml sml 10841 163 158 image thumbnail 1 s2 0 s0022169422007004 gr2 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169422007004 gr2 highres image jpeg d89b8f5a1a0168b36bd759cd5b5e9679 gr2 lrg jpg gr2 gr2 lrg jpg jpg 244806 880 2362 image high res 1 s2 0 s0022169422007004 gr1 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169422007004 gr1 highres image jpeg 3c68050ae85904be1c867c20e76a77f0 gr1 lrg jpg gr1 gr1 lrg jpg jpg 286398 1158 2362 image high res 1 s2 0 s0022169422007004 gr4 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169422007004 gr4 highres image jpeg a573ff7379eb073fba309300cc70b16a gr4 lrg jpg gr4 gr4 lrg jpg jpg 413622 1476 3083 image high res 1 s2 0 s0022169422007004 gr3 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169422007004 gr3 highres image jpeg 58533051b04b413a75b1621743c58c87 gr3 lrg jpg gr3 gr3 lrg jpg jpg 657095 1756 3248 image high res 1 s2 0 s0022169422007004 gr6 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169422007004 gr6 highres image jpeg 87f42e6f8f26e3a04e287ba0d2d224ec gr6 lrg jpg gr6 gr6 lrg jpg jpg 661149 1728 3149 image high res 1 s2 0 s0022169422007004 gr5 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169422007004 gr5 highres image jpeg 1dcbcdf45094e5c1117200aa5abe3aa0 gr5 lrg jpg gr5 gr5 lrg jpg jpg 522032 2119 3031 image high res 1 s2 0 s0022169422007004 gr8 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169422007004 gr8 highres image jpeg 139a376ab69bee8e1d3268ef9f67d550 gr8 lrg jpg gr8 gr8 lrg jpg jpg 644875 2580 3150 image high res 1 s2 0 s0022169422007004 gr7 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169422007004 gr7 highres image jpeg a6afd7d44a125cdb65505092a78bf705 gr7 lrg jpg gr7 gr7 lrg jpg jpg 709561 2538 3150 image high res 1 s2 0 s0022169422007004 gr9 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169422007004 gr9 highres image jpeg 54b9dc773a579ba68aec9f1166de19cb gr9 lrg jpg gr9 gr9 lrg jpg jpg 817106 2857 2764 image high res 1 s2 0 s0022169422007004 mmc1 docx https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169422007004 mmc1 main application vnd openxmlformats officedocument wordprocessingml document 2d826eb5de20cbb6f5b589ab546ae2fe mmc1 docx mmc1 mmc1 docx docx 33366037 application 1 s2 0 s0022169422007004 am pdf https s3 eu west 1 amazonaws com prod ucs content store eu west content egi 10vsvh6d14t main application pdf 811875113b5cee2306b9fe850880ca5a am pdf am am pdf pdf false 2123428 aam pdf hydrol 128125 128125 s0022 1694 22 00700 4 10 1016 j jhydrol 2022 128125 the author s fig 1 study area of southwestern japan including kyushu shikoku and chugoku fig 2 illustration of a training data preparation for each year and month the first 3 hour forecasts from each 39 hour forecast were collected and combined as modeled data blue observed precipitation from radar amedas was used as predictand green b in the svm regression data in the training years were collected every three hours red marks but the prediction was made for all hours in the validation year gray marks for interpretation of the references to colour in this figure legend the reader is referred to the web version of this article fig 3 averaged hourly precipitation in january from a radar amedas b msm gpv c qm and d cdft e svm f svm qm and g svm cdft the statistical metrics evaluated against radar amedas were shown in each subplot and the units for r rmse and bias are unitless mm hr and mm hr respectively fig 4 taylor diagram of estimated hourly precipitation in a january and b july derived from msm gpv gray cross svm blue pentagon qm orange square cdft green triangle svm qm red diamond and svm cdft purple circle the black star on the x axis labeled as reference represents observations from radar amedas for interpretation of the references to colour in this figure legend the reader is referred to the web version of this article fig 5 receiver operating characteristic roc curves in a january and c july derived from msm gpv gray svm blue qm orange cdft green svm qm red and svm cdft purple the black box area in a and c are enlarged as shown in b and d respectively for interpretation of the references to colour in this figure legend the reader is referred to the web version of this article fig 6 averaged hourly precipitation in july from a radar amedas b msm gpv c qm and d cdft e svm f svm qm and g svm cdft the performance statistics evaluated against radar amedas were shown in each subplot and the unit for r rmse and bias are unitless mm hr and mm hr respectively fig 7 forecast accuracy of a r b rmse c pod and d far versus lead time in january 2020 dot solid lines represent averaged statistics of the 248 members of 39 hour precipitation forecasts from msm gpv black svm orange svm qm blue and svm cdft red corresponding dashed lines represent the standard deviation sd of the 248 members for interpretation of the references to colour in this figure legend the reader is referred to the web version of this article fig 8 forecast accuracy of a r b rmse c pod and d far versus lead time in july 2020 dot solid lines represent averaged statistics of the 248 members of 39 hour precipitation forecasts from msm gpv black svm orange svm qm blue and svm cdft red corresponding dashed lines represent the standard deviation sd of the 248 members for interpretation of the references to colour in this figure legend the reader is referred to the web version of this article fig 9 precipitation at 4 a m on july 5th gmt 9 from a radar amedas b d msm gpv e g svm h j svm qm and k m svm cdft the second third and fourth columns represent the precipitation forecast of the target hour at a lead time of 37 h 19 h and 1 h respectively table 1 experiment cases and corresponding descriptions in the study cases description 1 msm gpv modeled hourly precipitation from msm gpv 2 svm post processed modeled hourly precipitation using svm 3 qm post processed modeled hourly precipitation using qm 4 cdft post processed modeled hourly precipitation using cdft 5 svm qm post processed modeled hourly precipitation using svm and qm 6 svm cdft post processed modeled hourly precipitation using svm and cdft table 2 evaluation metrics and indices used in the study index index name equation or definition unit r correlation coefficient r i 1 n x o i x o i 2 x m i x m i 2 i 1 n x o i x o i 2 i 1 n x m i x m i 2 rmse root mean square error rmse 1 n i 1 n x m i x o i 2 mm bias bias bias i 1 n x m i x o i n mm ard absolute relative difference ard s m s o s 0 pod probability of detection pod tp tp f n far false alarm ratio far fp fp t n r10 number of heavy precipitation hours monthly count of hourly precipitation 10 mm hours rx1h max 1 hour precipitation monthly maximum 1 hour precipitation mm rx3h max 3 hour precipitation monthly maximum 3 hour precipitation mm notation x o i and x m i observed and estimated hourly precipitation x o i and x m i average observed and estimated hourly precipitation n number of precipitation data sm and so statistics from estimated and observed precipitation tp true positive observed rain correctly detected fn false negative observed rain not detected fp false positive rain detected but not observed tn true negative rain not detected and not observed table 3 evaluation metrics of estimated hourly precipitation in all examined experiment cases during january and july in the cross validation experiment the absolute relative differences for the mean ardm standard deviation ardsd skewness ardsk and kurtosis ardkr of the hourly precipitation distribution were examined r rmse mm bias mm ardm ardsd ardsk ardkr january msm gpv 0 387 0 246 0 022 0 930 0 533 0 758 4 186 svm 0 490 0 213 0 037 0 742 0 630 0 748 2 797 qm 0 385 0 250 0 023 1 131 0 546 0 695 3 376 cdft 0 397 0 256 0 009 1 149 0 567 0 708 3 524 svm qm 0 460 0 232 0 006 0 931 0 535 0 759 2 647 svm cdft 0 475 0 228 0 001 0 869 0 544 0 744 2 731 july msm gpv 0 235 1 387 0 109 1 633 0 892 0 528 1 407 svm 0 296 1 234 0 197 4 994 0 793 0 644 1 054 qm 0 228 1 502 0 045 1 710 0 969 0 553 1 720 cdft 0 234 1 545 0 021 2 202 1 069 0 543 1 441 svm qm 0 282 1 435 0 098 1 670 0 826 0 663 1 389 svm cdft 0 280 1 378 0 033 1 653 0 816 0 668 1 337 table 4 statistics of extreme indices derived from all experiment cases against observations in january and july the units of rmse and bias for r10 rx1h and rx3h are hour mm hr and mm 3hr respectively january july r rmse bias r rmse bias r10 msm gpv 0 332 0 456 0 001 0 606 5 342 2 922 svm 0 176 0 447 0 146 0 481 8 048 6 365 qm 0 302 0 517 0 031 0 631 5 281 0 455 cdft 0 295 0 571 0 086 0 647 5 258 0 337 svm qm 0 338 0 537 0 032 0 635 6 837 1 940 svm cdft 0 299 0 501 0 020 0 637 5 682 0 172 rx1h msm gpv 0 550 2 881 0 032 0 357 16 460 5 609 svm 0 538 3 443 2 203 0 418 22 680 19 104 qm 0 495 3 477 0 017 0 348 17 394 0 018 cdft 0 503 3 478 0 481 0 357 16 872 1 585 svm qm 0 511 3 081 0 201 0 358 15 743 1 948 svm cdft 0 516 3 152 0 228 0 352 16 044 2 536 rx3h msm gpv 0 606 5 199 0 104 0 457 29 949 12 088 svm 0 605 5 989 3 579 0 495 39 797 31 116 qm 0 570 5 956 0 333 0 455 30 469 1 504 cdft 0 576 6 008 0 829 0 452 30 825 2 256 svm qm 0 570 5 846 0 873 0 435 32 306 4 602 svm cdft 0 581 5 747 0 565 0 434 31 056 1 022 research papers a support vector machine based method for improving real time hourly precipitation forecast in japan gaohong yin a takao yoshikane a kosuke yamamoto b takuji kubota b kei yoshimura a b a institute of industrial science the university of tokyo kashiwa japan institute of industrial science the university of tokyo kashiwa japan institute of industrial science the university of tokyo kashiwa japan b earth observation research center japan aerospace exploration agency tsukuba japan earth observation research center japan aerospace exploration agency tsukuba japan earth observation research center japan aerospace exploration agency tsukuba japan corresponding authors at institute of industrial science the university of tokyo 5 1 5 kashiwanoha kashiwa chiba 277 8574 japan gaohong yin institute of industrial science the university of tokyo 5 1 5 kashiwanoha kashiwa chiba 277 8574 japan this manuscript was handled by andras bardossy editor in chief with the assistance of fi john chang associate editor real time precipitation forecast facilitates water management and water associated disaster early warning however numerical weather prediction nwp models provide precipitation forecasts with bias this study proposed to combine support vector machine svm regression with quantile based bias correction method to improve real time 39 hour precipitation forecasts in japan five methods were compared and evaluated against observations which include svm regression quantile mapping qm cumulative distribution function transform cdft and the combination of svm and qm or cdft results indicated that the combination of svm and cdft i e svm cdft generally provided the highest accuracy with good computational efficiency svm alone improved the spatial representation of hourly precipitation with a correlation coefficient increased from 0 387 to 0 490 in january and from 0 235 to 0 296 in july in the cross validation experiment however svm underestimated the variability of hourly precipitation and heavy precipitation events qm and cdft perform well in correcting the bias in modeled precipitation while they have limited capability in correcting the rainband location combining svm and quantile based method took advantage of both approaches providing a more consistent variability with observations and better predicted extreme precipitation events although overestimation of rainfall area was witnessed the simple concept high computational efficiency as well as evident improvement in forecast accuracy make the combined cases especially the svm cdft method beneficial for real time precipitation forecast and flood early warning keywords precipitation real time forecasting support vector machine regression quantile mapping cdf transform bias correction 1 introduction japan is a country largely influenced by extreme precipitation natural disasters associated with extreme precipitation cause great loss in economics and human lives according to the ministry of land infrastructure transport and tourism mlit of japan mlit 2020 the frequency of short duration intense precipitation increased approximately 1 4 times within the recent 30 years the number of rivers exceeding dangerous water levels increased from 83 in 2014 to 403 in 2019 the economic damage caused by floods in japan increased from approximately 200 billion jpy in 2010 to over 2 15 trillion jpy in 2019 therefore an accurate and effective real time forecast of hazards associated with precipitation is crucial to enhance land surface hydrological prediction and further improve flood forecasts a real time operating system today s earth te https www eorc jaxa jp water index html was developed jointly by the japan aerospace exploration agency jaxa and the university of tokyo yoshimura et al 2008 ma et al 2021 te is a terrestrial surface simulation system aiming at forecasting land surface and river conditions with high resolution in both space and time as a regional version of te today s earth japan te japan has demonstrated its capability to capture the location and timing of extreme floods with satisfactory accuracy such as during the typhoon hagibis in 2019 ma et al 2021 by using the mesoscale model grid point value msm gpv saito et al 2006 atmospheric analysis data from the japan meteorological agency jma as forcing data te japan is capable of providing real time flood forecasts with a lead time of 39 h msm is an operational non hydrostatic numerical weather prediction nwp model to support disaster prevention saito et al 2006 non hydrostatic models outperform common dynamic nwp models by explicitly resolving deep convection for precipitation forecasts and providing a finer horizontal resolution whan and schmeits 2018 however due to the high computational cost these high resolution models usually provide deterministic forecasts without uncertainty estimates cattoën et al 2020 and contain errors in estimating the location of small scale extreme events whan and schmeits 2018 thus a careful evaluation and post processing of model based precipitation forecasts is still needed maurya et al 2020 yamamoto and sayama 2021 statistical methods are often used to correct bias in precipitation forecasts by establishing a statistical relationship between model outputs and observations moghim and bras 2017 one common statistical method is the quantile based method which adjusts the simulated cumulative distribution function cdf with observations heredia et al 2018 different quantile based methods have been developed and used for precipitation bias correction such as the quantile mapping qm panofsky et al 1958 wood et al 2002 teutschbein and seibert 2012 chen et al 2013 cannon et al 2015 equidistant cdf edcdf li et al 2010 moghim et al 2017 yang et al 2018 and the cdf transform cdft michelangeli et al 2009 pierce et al 2015 heredia et al 2018 previous studies found that quantile based methods outperformed simpler bias correction methods that only correct the mean and variance of simulated precipitation machine learning ml has been increasingly used for the downscaling and bias correction of precipitation xu et al 2019 kumar et al 2019 jose et al 2022 xiao et al 2022 the ml technique can solve high dimensional nonlinear and multivariate complex systems without explicitly constructing a physical or statistical model he et al 2016 moghim and bras 2017 vandal et al 2019 moghim and bras 2017 developed an artificial neural network ann model to bias correct monthly precipitation from a climate model they found that ann reduced estimation error and improved correlation even when the temporal consistency between modeled and observed data sets was weak xu et al 2019 implemented the wavelet support vector machine wsvm and wavelet random forest wrf methods for the bias correction and downscaling of multi model ensemble precipitation forecasts at a monthly scale results demonstrated the benefits of ml based methods in downscaling while in terms of correction the uncertainties of precipitation forecasts in summer and extreme events were still considerable zarei et al 2021 compared rf and qm methods in correcting the bias of daily ensemble precipitation forecasts and found that rf performed better concerning probabilistic evaluations previous studies usually implemented ml for precipitation bias correction on a daily or monthly scale the potential of ml to correct hourly precipitation forecast has not been well investigated especially for the real time operation purpose which seeks not only accuracy but also computational efficiency moreover most studies focused on correcting the bias of ensemble forecasts of precipitation from climate models with coarse spatial resolution the effectiveness of the bias correction method to improve high resolution deterministic precipitation forecast from non hydrostatic models is less studied the objective of the study is to improve the real time 39 hour precipitation forecast simulated from msm gpv in order to enhance flood forecast accuracy over japan a combined strategy by using svm regression and quantile based method i e qm and cdft jointly was proposed in the study the paper is organized as follows section 2 described the study area and data sets used in this study section 3 introduced the experiment design and evaluation metrics section 4 presented the results from the cross validation and real time forecast experiments section 5 provides a discussion of the results followed by the findings of this study summarized in section 6 2 study area and datasets 2 1 study area the study area is southwestern japan including kyushu shikoku and chugoku with a geographic boundary of 29 n to 37 34 n and 127 8 e to 136 14e fig 1 southwestern japan is located in the asian monsoon zone with hot and humid summers and moderately cold winters it is one of the regions in japan having the largest amount of annual precipitation especially in kyushu and shikoku kato 2006 sugimoto 2020 tsuji et al 2020 due to the frequent occurrence of extreme precipitation this region is vulnerable to flooding disasters such as the series of fatal floods in kyushu in july 2020 a large portion 30 of precipitation falls during the summer rainy season which is known as baiu bertacchi uvo et al 2001 about 90 of the torrential rainfall events in southern japan are associated with linear precipitation systems that result in narrow rainbands kato et al 2018 during the winter winter monsoon dominates the climate hirose and fukudome 2006 prevailing northwesterly winds bring snow to the sea of japan side of chugoku upstream of mountains and sometimes to the shikoku mountains 2 2 data sets 2 2 1 nwp based hourly precipitation mesoscale model grid point value msm gpv data set from the nonhydrostatic mesoscale model developed at jma was used as the modeled precipitation saito et al 2006 msm gpv provides a 39 hour precipitation forecast every three hours i e eight times a day covering japan and its surrounding area with a spatial resolution of 0 05x 0 0625 in the study msm gpv hourly precipitation forecasts from 2007 to 2020 were acquired and rescaled onto a grid with a resolution of 0 06x 0 06for convenience 2 2 2 observed hourly precipitation radar automated meteorological data acquisition system amedas makihara et al 1996 makihara 2000 from jma was used to evaluate the modeled precipitation radar amedas is a radar rain gauge analyzed precipitation product that calibrates radar data with ground based rain gauge data and provides hourly rainfall with a 1 km resolution every 30 min nagata 2011 radar amedas precipitation data provides high accuracy with slight underestimation relative to rain gauge nagata 2011 and overestimation relative to amedas measurements makihara et al 1996 it has been widely used as a referential dataset for heavy rainfall warning takido et al 2016 and precipitation product evaluation iida et al 2006 kubota et al 2009 hirose et al 2019 in japan the study used radar amedas hourly precipitation initiated at the beginning of each hour from 2007 to 2020 i e data issued at the middle of each hour was not used afterward data was upscaled to the same grid as the regridded msm gpv with a spatial resolution of 0 06 0 06 3 methods 3 1 support vector machine svm regression svm regression is a supervised ml method that has been widely used in hydrologic and climate change studies elshorbagy et al 2010 sachindra et al 2018 chen et al 2019 huang et al 2020 the basic idea of svm regression is to find the optimal hyperplane and boundary layers to minimize a defined error function chivers et al 2020 svm employs the structural risk minimization principle and reduces the problem of overfitting yu et al 2017 additionally svm shows the advantage of not being trapped in local minimums najafi et al 2011 and can be used with a relatively small sample size al anazi and gates 2012 a brief introduction of svm regression is provided in the section and more details can be found in vapnik 1995 1998 consider a given training data set x i y i i 1 n where x i is the input vector i e model estimates and y i is the corresponding output i e observation and n is the size of the data set the svm regression function can be written as 1 f x w φ x b where w is the weight vector b is the bias and φ is the nonlinear transformation function that maps the input space into a higher dimensional feature space the objective of svm regression is to find the w and b that minimize the empirical risk 2 r empirical 1 n i 1 n y i f x i where y i f x i is defined as the vapnik s ε insensitive loss function with errors less than the tolerance ε ignored 3 y i f x i 0 f o r y i f x i ε y i f x i ε f o r y i f x i ε after reformulation the svm regression problem can be summarized as an optimization problem 4 min 1 2 w 2 c i 1 n ξ i ξ i s u b j e c t t o y i f x i ε ξ i f x i y i ε ξ i ξ i 0 ξ i 0 where c is a positive constant determining the degree of penalized loss ξ i and ξ i are slack variables specifying the upper and lower training errors subject to the error tolerance ε the optimization problem can be solved by using a series of lagrange multipliers α i and α i and the svm regression function can be rewritten as 5 f x i 1 n α i α i k x x i b where k x x i φ x i φ x i and is known as the kernel function in the study a nonlinear regression based on the gaussian radial basis function rbf was used where γ is a parameter associated with the width of the kernel 6 k x x i e x p γ x x i 2 in the study both input i e predictor and output i e predictand variables of the svm model are hourly precipitation instead of establishing a point to point relationship between modeled and observed precipitation the concept of feature vector range was used to relate observed precipitation at the target grid with neighboring grids in model simulations for each observed precipitation yj t at target grid j and time t modeled precipitation in a 21x21 grid points domain i e 1 26x1 26 centered at the target grid at time t was used as x j t j 1 441 svm model was trained to learn the relationship between modeled precipitation located inside the feature vector range x j t and the observed precipitation at the target grid yj t the feature vector range was utilized based on the assumption that the spatial distribution of modeled precipitation over a range of areas is connected with observed precipitation at the target location through weather patterns yoshikane et al 2022 the use of a feature vector range also provides the potential to correct the location of narrow rainfall bands that are mistakenly simulated the hyperparameters aforementioned including c γ and ε determine the error tolerance level and the accuracy of an svm model smets et al 2007 shi 2020 following yoshikane et al 2022 a random search approach bergstra and bengio 2012 on a regular grid layout i e use grids with an interval of 10 grids along both latitude and longitude directions was used to select the hyperparameters the optimal hyperparameters were determined based on the combination that provided the minimum root mean square error rmse 3 2 quantile mapping qm qm is a widely used statistical method for the bias correction and downscaling of model simulations panofsky et al 1958 wood et al 2002 teutschbein and seibert 2012 chen et al 2013 cannon et al 2015 the fundamental idea is to equate the cumulative distribution functions cdfs of modeled and observed variables during the historical period as shown in equation 7 7 f o h x o h f m h x m h x o h f o h 1 f m h x m h where f o h and f m h represent the cdfs of observed data x o h and modeled data x m h in a historical period h respectively f m h 1 is the inverse function of f m h and is defined in the range of 0 1 the qm method assumes that model bias is stationary which indicates the characteristics in the historical period do not change in the future in the study the qm on a grid cell basis was conducted 3 3 cdf transform cdft the qm method only considers the distribution of historical data without taking the future modeled data into account therefore an extended method cdf transform hereafter cdft michelangeli et al 2009 was developed based on the assumption that there is a transform t allowing to translate the cdf of modeled historical data into the cdf of observed historical data 8 t f m h x f o h x assuming u f m h x u 0 1 equation 8 can be written as 9 t u f o h f m h 1 u further assuming that the relationship in equation 9 persists into the future i e t f m f x f o f x replacing u in equation 9 with f m f x the target cdf i e cdf of observed future data is obtained as 10 f o f x f o h f m h 1 f m f x where f o f and f m f represent the cdfs of observed data and modeled data in a future period f the cdft method takes both historical and future data into consideration and thus better captures the characteristics associated with climate change the cdft method has been implemented for statistical downscaling and bias correction of climate model simulations for future projections bürger et al 2013 pierce et al 2015 famien et al 2018 however the potential of the cdft method to correct bias in forecast data at seasonal or diurnal scales was seldom investigated ngoungue et al 2021 therefore cdft was included in the study to investigate its plausibility in correcting bias in short range forecasts 3 4 experiment design following the typical framework to evaluate an ml model the experiment consisted of a cross validation experiment and a 39 hour forecast testing experiment the study period from 2007 to 2020 was divided into the training and validation period i e 2007 2019 and the testing period i e the year 2020 experiments were conducted for each month and results in january and july were shown in the study results for other months are available in the supplementary 3 4 1 cross validation experiment a leave one year out cross validation experiment was conducted for the validation period from 2007 to 2019 to evaluate the overall performance of all examined experiment cases instead of using all 39 hour precipitation forecasts from msm gpv only the first 3 hour forecasts from each 39 hour forecast were collected and combined as the modeled data blue line as shown in fig 2 a observed hourly precipitation from radar amedas was used as the predictand green line only the first 3 hour forecasts from msm gpv were used because random errors in the forecasts increase rapidly with lead time which may deteriorate the performance of bias correction methods the study evaluated six experiment cases with descriptions summarized in table 1 results from 1 modeled precipitation msm gpv and post processed precipitation based on 2 svm 3 qm 4 cdft 5 the combination of svm and qm svm qm and 6 the combination of svm and cdft svm cdft were evaluated with radar amedas in the svm regression training data was selected every three hours in each training year and the prediction was made for all hours in the validation year fig 2b for the sake of computational efficiency a trial experiment showed that using all training data to train the svm model did not yield substantially different results but the computational time was considerably increased additionally due to the significant skewness in the hourly precipitation i e large number of no rain and light rain events a commonly used undersampling strategy was adopted to reduce the ratio of no rain versus rain data to 1 1 prior to applying svm regression hyperparameters used for both january and july precipitation were selected as c 16 γ 5 10 6 ε 0 01 in the cdft case due to the limited number of forecasts in the future period especially in the real time 39 hour forecast experiment section 3 4 2 the cdf of modeled future data f m f was derived by using both modeled historical i e training period and future i e validation or testing period data using the cross validation for july 2007 as an example f m h and f m f were acquired using modeled data in july from 2008 to 2019 and 2007 to 2019 respectively in the combination cases i e svm qm and svm cdft qm or cdft was used as an additional step after svm regression to adjust the cdf of post processed precipitation from svm with observations results from svm qm and svm cdft were compared with svm qm and cdft to investigate if the combination strategy is superior to the individual method 3 4 2 39 hour forecast experiment the real time precipitation forecast experiment used pre processed data in section 3 4 1 from 2007 to 2019 as training data and the 39 hour forecasts in 2020 were tested based on the cross validation results only svm based experiment cases i e svm svm qm and svm cdft were examined in the testing experiment again training data were selected every three hours for svm model training considering the computational efficiency the trained svm model was subsequently used for precipitation forecasts in 2020 results from svm based experiment cases were evaluated against radar amedas 3 5 evaluation metrics various metrics were used to evaluate the post processed modeled precipitation as shown in table 2 statistical metrics including correlation coefficient r root mean square error rmse and bias were used to quantify the consistency between observed and estimated precipitation additionally the absolute relative difference ard was used to evaluate the effectiveness and robustness of the examined bias correction method most bias correction methods correct only the first two moments i e mean and standard deviation of the target variable but often introduce additional uncertainties especially to higher order moments lafon et al 2013 therefore in the study the ard of four statistical moments including mean ardm standard deviation ardsd skewness ardsk and kurtosis ardkr were evaluated closer to zero ard value indicates a better correction result additionally contingency statistical metrics including the probability of detection pod and false alarm ratio far were calculated to evaluate the capability of estimated precipitation to detect rainfall events correctly by selecting different thresholds to distinguish wet and dry conditions a series of pod and far values can be calculated and displayed as a receiver operating characteristic roc curve the upper left point of the roc curve i e pod 1 and far 0 indicates a perfect classifier curves below the 1 1 line indicate a worse than random classifier performance moreover extreme precipitation indices were also examined to evaluate the capability of modeled precipitation to estimate heavy rainfall events which include the monthly count of hourly precipitation no less than 10 mm hr r10 monthly maximum 1 hour rx1h and 3 hour rx3h precipitation 4 results 4 1 cross validation results in january 4 1 1 average precipitation in january averaged hourly precipitation in january during the validation period i e 2007 2019 was calculated as shown in fig 3 msm gpv captured the spatial variation of long term averaged precipitation with high rainfall intensity witnessed in northern chugoku fig 3b however the area with average rainfall intensity larger than 0 4 mm hr was underestimated qm showed similar performance to msm gpv while cdft provided a better consistency with observations rcdft 0 984 and biascdft 0 009 mm hr and the high rainfall intensity area was better captured svm improved the spatial representation of precipitation in january relative to msm gp with a large r of 0 976 however the large rainfall rate in northern chugoku was not captured leading to the largest bias 0 037 mm hr among all examined cases svm cdft provided the highest agreement with observations with r close to 1 and bias close to zero the close to unity r value in the svm cdft case is attributed to both svm and cdft as svm enhanced the spatial representation of hourly precipitation and cdft further adjusted the temporal distribution of each validation year to match with the long term distribution it is also noted that except for the qm case all other examined cases provided a negative bias indicating an overall underestimation of precipitation relative to radar amedas in january 4 1 2 evaluation of hourly precipitation in january the spatial statistics of each hour in each validation year were calculated afterward validation year averaged hourly statistics were computed in table 3 svm based cases improved the spatial correlation and reduced rmse relative to msm gpv with the svm case yielding the largest r of 0 490 cdft case provided a larger r but also increased the rmse value rcdft 0 397 and rmsecdft 0 256 mm hr relative to msm gpv as for qm it slightly degraded all examined statistics at an hourly scale the improved correlation in svm based cases suggested that the use of feature vector range i e neighboring grid cells in svm regression was effective however the svm case failed to capture heavy precipitation events resulting in a large negative bias the combination of svm and qm or cdft took advantage of each method yielding the highest statistics with high r values and small bias particularly in the svm cdft case rsvm cdft 0 475 and biassvm cdft 0 001 mm hr ard values were used to indicate the bias of distribution moments of estimated hourly precipitation as shown in table 3 closer to zero ard value indicates a better bias correction performance results suggested that all examined experiment cases effectively reduced the bias for high order moments i e skewness and kurtosis except that svm qm provided a slightly larger ardsk than msm gpv the bias correction performance for the first two moments i e mean and standard deviation varied with experiment cases both qm and cdft yielded relatively larger ardm and ardsd than msm gpv whereas svm based cases tended to provide smaller ardm but larger ardsd the relatively poor performance of qm and cdft at the first two moments may be because they are conducted on a grid cell basis and thus failed to capture the spatial characteristics of hourly rainfall svm cdft generally provided the highest accuracy with a reduced or similar ard value for all examined moments relative to msm gpv collecting all hourly precipitation at each grid in january during the validation years from 2007 to 2019 the taylor diagram was shown in fig 4 a to evaluate the similarity between estimated and observed precipitation a smaller distance to the reference i e observation represents a better capability to reproduce observation results showed that svm based cases i e svm svm qm and svm cdft provided slightly larger r and smaller centered rmse relative to msm gpv however the svm case largely underestimated the variability of hourly precipitation relative to observations with a normalized standard deviation of about 0 53 qm and cdft were located close to the msm gpv case with slightly smaller r and larger centered rmse than msm gpv svm cdft overall yielded the highest consistency between estimated and observed hourly precipitation furthermore roc curves were used to illustrate the diagnostic ability of different experiment cases to distinguish between wet and dry conditions fig 5 a b a range of threshold values from 0 1 mm hr to 50 mm hr were used to calculate pod and far values all examined cases showed better skill than the random classifier i e above the 1 1 line as the threshold value increases both pod and far decrease svm qm and svm cdft generally provided superior accuracy in classifying wet and dry conditions with larger pod values especially the svm qm case however when the threshold is over 20 mm hr qm and cdft provided slightly higher pod relative to svm qm and svm cdft fig 5b suggesting their better capability of capturing heavy rainfall events in january 4 2 cross validation results in july 4 2 1 average precipitation in july averaged hourly precipitation in july from 2007 to 2019 was computed as shown in fig 6 during the summer monsoon period short duration intense precipitation occurs more frequently resulting in more extensive average precipitation than in january and the largest rainfall intensity was witnessed in central kyushu and southern shikoku msm gpv represented the spatial pattern of long term average precipitation in july however underestimation was found in most areas and overestimation was occasionally detected in areas such as yakushima island to the south of kyushu island both qm and cdft improved the spatial consistency between the estimated and observed precipitation with larger r and smaller bias similar to the findings in january all svm based cases provided a higher spatial r relative to msm gpv indicating their capability to capture the spatial characteristics of precipitation however the underestimation of heavy precipitation in the svm case is more severe than in january with a negative bias of 0 197 mm hr svm cdft yielded the largest r of 0 994 among all examined cases but it slightly overestimated the precipitation in central kyushu 4 2 2 evaluation of hourly precipitation in july the statistical skill of post processed hourly precipitation in july is inferior to january due to the dramatically increased spatio temporal variation of precipitation during the summer monsoon season table 3 similar to january results the svm case provided the highest r rsvm 0 296 but the largest bias biassvm 0 197 mm hr among all cases as for qm and cdft they largely reduced the bias of estimated precipitation relative to msm gpv whereas larger rmse and slightly smaller r values were witnessed again svm cdft yielded the highest overall accuracy with improved spatial correlation rsvm cdft 0 280 and reduced errors rmsesvm cdft 1 378 mm hr biassvm cdft 0 033 mm hr in terms of ard qm and cdft failed to reduce the bias of distribution moments and yielded larger ard values than msm gpv while svm based cases showed smaller ard values only for kurtosis ardkr and standard deviation ardsd the taylor diagram fig 4b showed more scattered results among experiment cases than in january msm gpv evidently underestimated the variability of hourly precipitation i e normalized standard deviation less than 1 in july svm greatly improved the correlation and reduced the centered rmse whereas it caused an even larger underestimation of precipitation variability when compared to radar amedas cdft and svm qm tended to increase precipitation variability while qm and svm cdft provided the highest agreement of spatio temporal variability with observations regarding the findings from roc curves fig 5c d the conclusions from july are similar to january in that svm qm and svm cdft generally provided the highest accuracy in identifying wet and dry conditions qm and cdft showed larger pod when the threshold value equals 50 mm hr svm case showed improvement relative to msm gpv only for light rainfall events due to the underestimation issue in svm regression 4 3 extreme precipitation analysis extreme precipitation prediction at an hourly scale is crucial but challenging especially for rainfall events associated with severe convection the capability of all examined cases to capture extreme precipitation events was examined for each validation year the extreme indices derived from all experiment cases and observations were acquired afterward the spatial statistics of extreme indices from each experiment case and observations were calculated and the validation year averaged statistics were shown in table 4 in january msm gpv consistently outperformed other experiment cases with larger r and smaller rmse and bias for all examined extreme indices in all examined bias correction cases improvement of extreme precipitation estimates in january was not witnessed while during july except for svm all other examined bias correction cases showed a larger r and smaller bias than msm gpv for r10 regarding the maximum hourly and 3 hourly precipitation svm showed the largest spatial correlation 0 418 and 0 495 for rx1h and rx3h with large negative bias all other bias correction cases provided reduced bias of rx1h and rx3h although improvements in spatial correlation and rmse were not clear 4 4 real time precipitation forecasts analysis cross validation analysis demonstrated the capability of svm based methods to improve the spatial representation of hourly precipitation which is critically essential for flood forecasts therefore the performance of svm based methods for real time precipitation forecast in 2020 was further investigated in this section as msm gpv forecasts were issued every three hours totally 248 members of 39 hour precipitation forecast were initiated in both january and july i e eight forecasts per day for 31 days the overall accuracy of the precipitation forecasts versus lead time is discussed in this section moreover an extreme precipitation event in july was used as an example to evaluate the performance of svm based bias correction methods 4 4 1 overall forecast accuracy for each 39 hour precipitation forecast member the statistics skill i e r rmse pod and far of each experiment case at every hour were computed subsequently the averaged statistics of all 248 members at different lead times were calculated figs 7 and 8 displayed that all experiment cases yielded decreased r and increased rmse when lead time increased suggesting the deterioration of forecast accuracy at a longer lead time in terms of contingency statistics all experiment cases exhibited a decreased accuracy in pod threshold 2 5 mm hr as lead time increased whereas the performance of far varies in january and july in january fig 7d the averaged far values were relatively stable until about 25 h ahead followed by an increasing trend until the end of the forecast time while in july fig 8d the far values increased with lead time until about 10 h ahead and then maintained relatively stable comparing the performance of different experiment cases in january fig 7 all svm based cases provided larger r and smaller rmse than msm gpv particularly svm yielded the largest r and smallest rmse across the 39 hour forecast time however svm showed the lowest pod and far due to the limited dynamic range of precipitation estimated by svm svm qm and svm cdft were also slightly inferior to msm gpv in correctly identifying rainfall events with a threshold of 2 5 mm hr similar to the results in january the svm case provided the largest r and smallest rmse values during the 39 hour forecast time in july fig 8 however the performance of svm qm and svm cdft are distinct from the january results both svm qm and svm cdft provided larger r but larger rmse values than results from msm gpv throughout the 39 forecast hours additionally svm qm and svm cdft performed better in capturing rainfall events with higher pod values despite larger far values the standard deviation of metrics from the 248 members increased with lead time especially for rmse results also suggested that svm qm and svm cdft tended to add variability i e larger standard deviation to hourly precipitation forecast such increased variability is more apparent in july than in january when msm gpv significantly underestimated the variability of hourly precipitation 4 4 2 extreme precipitation forecast example precipitation forecast for july 5th at 4 a m gmt 9 was used as an example to assess the capability of svm based cases to forecast extreme precipitation this particular hour is selected because it experienced the largest domain averaged hourly precipitation in july 2020 based on observed precipitation from radar amedas jma also reported extreme precipitation over kyushu during this period especially in kumamoto and kagoshima https www data jma go jp obd stats data bosai report index 1989 html using 4 a m on july 5th as the target hour 13 forecasts with different lead times can be acquired with the shortest lead time of 1 h initiated at 3 a m of the same day observed and modeled precipitation at the target hour with a lead time of 37 h 19 h and 1 h are shown in fig 9 for all experiment cases as the lead time became shorter forecast accuracy was enhanced with the spatial variation of precipitation better represented similar to cross validation results the svm case consistently underestimated extreme precipitation all experiment cases failed to forecast the extreme precipitation 30 mm hr in southern kyushu 37 h ahead of the target time fig 9b e h k with a precipitation intensity of approximately 10 mm hr reported when the lead time was 19 h fig 9c f i l both svm qm and svm cdft better predicted the intensity and location of the extreme precipitation than msm gpv but they still underestimated the intensity when forecasting precipitation 1 h ahead of the target hour all cases represented the spatial characteristics of precipitation with higher intensity in mid to southern kyushu and southwestern shikoku msm gpv fig 9d showed a slight location mismatch of rainband in kyushu and regions that experienced extreme rainfall were largely underestimated results from svm qm fig 9j and svm cdft fig 9m are similar they both successfully predicted the large area of the extreme precipitation event at the target hour however both svm qm and svm cdft slightly overestimated extreme precipitation areas compared to the observation while from the perspective of hazard mitigation and flood warning such overestimation of extreme precipitation area is often preferable relative to the largely underestimated estimates from msm gpv barnes et al 2007 schultz et al 2010 5 discussion results of examined experiment cases in different months exhibited similar findings whereas differences were also witnessed these differences are mainly attributed to the distinctive spatio temporal characteristics of precipitation in different seasons in the study area july is one of the wettest months in southwestern japan which has more frequent extreme precipitation events with large variability in space and time while in january the winter monsoon brings dry and cold air leading to low intensity precipitation with a less spatio temporal variation therefore the overall accuracy of estimated precipitation in july is inferior to in january due to the significant dynamic variation of hourly precipitation in july that failed to be represented by msm gpv the benefit of the proposed method to improve precipitation forecast is often more profound than in january while in january the characteristics of precipitation are better represented by msm gpv and thus the improvement that can be achieved by bias correction is limited it is found that the svm case improved the spatial representation of precipitation in both january and july however it failed to capture extreme precipitation leading to a large negative bias in the estimated precipitation such underestimation was also found in previous studies and explained by the non gaussian outliers in the training data kalra and ahmad 2012 in the study hourly precipitation is studied which is highly skewed with a large number of non precipitation and light precipitation events with the highly imbalanced data svm regression attempts to minimize the penalty loss c i 1 n ξ i ξ i in equation 4 with the use of a soft margin leading to a bias toward the majority batuwita and palade 2013 such bias caused by data imbalance is prevalent for most ml algorithms and various methods have been proposed to mitigate the impact of this problem following previous work gagne et al 2014 this study undersampled non precipitation events prior to svm regression which alleviated the overwhelming signal of zeros however the effect of data imbalance cannot be entirely eliminated by undersampling yielding still underestimated precipitation applying a quantile based method upon svm results i e svm qm and svm cdft satisfactorily adjusted the variability of estimated precipitation derived from svm meanwhile maintaining the improved spatial representation of precipitation in the study qm and cdft were conducted on a grid cell basis i e using modeled precipitation at the target grid only while svm used information from neighborhood grids within the feature vector range a trial experiment results not shown here of conducting qm and cdft with the concept of feature vector range i e using identical data as svm found that they performed worse than the current qm and cdft strategy additionally unlike in the svm case the improvement of spatial pattern can hardly be detected in the trial experiment therefore the combination strategy used in the study took full advantage of each method providing the highest accuracy of post processed precipitation forecasts as a post processing method for a real time operating system not only accuracy but also computational efficiency is essential the examined experiment cases in the study are all simple in concept with high computation efficiency for example for the svm cdft case which provided the highest overall accuracy it spent less than 1 5 h to finish one year cross validation over a 140 140 grid on a linux operating system with a xeon 2 8 ghz processor and 16 gb of ram once the classifiers during the historical period i e 2007 2019 are prepared the processing time for one 39 hour forecast using svm cdft is approximately 5 min which is highly efficient a preliminary investigation on using more complicated strategies suggested that they provided a similar level of accuracy but the computational time also greatly increased for example a two step analysis using random forecast rf classification to identify rain or non rain cases followed by an svm regression to provide quantitative estimates of precipitation was tested the overall statistics from rf svm were not obviously better than the svm cdft case whereas the time spent for one year cross validation was almost tripled about 4 5 h 6 conclusion an accurate precipitation forecast is crucial for the early warning of associated disasters the study proposed to combine svm regression with a quantile based bias correction method to improve the real time hourly precipitation forecast from msm gpv results showed that svm effectively improved the spatial representation of hourly precipitation in both january and july however due to the high skewness of hourly precipitation data and the inherent characteristics of the svm regression svm failed to capture heavy precipitation events and underestimated the variability of precipitation especially during the wet month i e july qm and cdft effectively correct the bias in estimated precipitation caused by errors in convective parameterizations but they usually cannot correct bias associated with unrealistic large scale variability a combination of svm and qm or cdft took advantage of both methods providing post processed hourly precipitation with high spatial correlation low bias and high hit rate particularly in the svm cdft case the svm cdft method is simple without requiring additional information from other hydrological or meteorological variables overall the high accuracy and the efficient computational time outstand svm cdft to be used to improve real time hourly precipitation forecasts despite the encouraging results from svm cdft issues associated with the proposed method were also noticed it is found that svm cdft tended to overestimate rainfall coverage area especially during heavy precipitation events leading to a large rmse of estimated precipitation additionally the capability of svm cdft to improve extreme precipitation forecast is limited therefore further improvement of the current method is demanded moreover the selection of feature vector size is critically important in determining the performance of svm a preliminary investigation of feature vector size suggested that a smaller feature vector size might deteriorate the overall accuracy of precipitation forecast but accordingly improve local extreme precipitation forecast a non constant setting of parameters in svm may be beneficial and requires further exploration additionally the empirical svm model used in the study does not allow an uncertainty analysis of post processed precipitation forecasts therefore a probabilistic version of svm such as the bayesian svm polson and scott 2011 and kernel based svm esmaeilbeigi et al 2020 should be explored furthermore the potential of other ml based methods in improving precipitation forecast also requires more investigation in the following study declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements we gratefully appreciate dr misako kachi and dr riko oki from earth observation research center japan aerospace exploration agency for the support to this work this work was supported by earth observation research center japan aerospace exploration agency jaxa eorc grant jx pspc 533980 the environmental restoration and conservation agency of japan via the environment research and technology development fund s 20 grant jpmeerf21s12020 the council for science technology and innovation csti via cross ministerial strategic innovation promotion programs sip enhancement of national resilience against natural disasters funding agency nied the japan society for the promotion of science jsps grants 21h05002 18h03794 and 21 k20443 government of japan ministry of education culture sports science and technology via the integrated research program for advancing climate models tougou grant jpmxd0717935457 arcs ii grant jpmxd1420318865 and dias grant jpmxd0716808979 we thank the japan meteorological agency jma and data integration and analysis system program dias for providing the msm gpv precipitation forecast product http apps diasjp net gpv data we appreciate the japan meteorological business support center jmbsc for providing radar amedas data http www jmbsc or jp en contact html radar amedas data can be acquired by sending request to the international service officer jmbsc ab jmbsc or jp appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2022 128125 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 
3220,real time precipitation forecasts analysis 4 4 1 overall forecast accuracy 4 4 2 extreme precipitation forecast example 5 discussion 6 conclusion acknowledgements appendix a supplementary data alanazi 2012 64 76 a barnes 2007 1140 1147 l batuwita 2013 83 99 r imbalancedlearningfoundationsalgorithmsapplications classimbalancelearningmethodsforsupportvectormachines bergstra 2012 j bertacchiuvo 2001 259 271 c burger 2013 3429 3449 g cannon 2015 6938 6959 a cattoen 2020 1655 1673 c chen 2019 982 994 h chen 2013 4187 4205 j chivers 2020 125126 b elshorbagy 2010 1943 1961 a esmaeilbeigi 2020 492 502 m famien 2018 313 338 a gagne 2014 1024 1043 d he 2016 8217 8237 x heredia 2018 2021 2040 m hirose 2006 61 63 n hirose 2019 689 710 h huang 2020 124687 s iida 2006 n a n a y jose 2022 1 25 d kalra 2012 a kato 2006 129 153 t kato 2018 846 859 r kubota 2009 203 222 t kumar 2019 2221 a lafon 2013 1367 1381 t li 2010 h ma 2021 w makihara 1996 751 762 y makihara 2000 63 111 y maurya 2020 e1915 r michelangeli 2009 p moghim 2017 1867 1884 s moghim 2017 621 636 s nagata 2011 37 50 k najafi 2011 650 664 m ngounguelangue 2021 893 912 c pierce 2015 2421 2442 d polson 2011 1 23 n sachindra 2018 240 258 d saito 2006 1266 1298 k schultz 2010 249 254 d sugimoto 2020 17 22 s takido 2016 185 195 k teutschbein 2012 12 29 c tsuji 2020 859 876 h vandal 2019 557 570 t 1995 naturestatisticallearningtheory vapnik 1998 v statisticallearningtheory whan 2018 3651 3673 k wood 2002 acl 6 a xiao 2022 151679 s xu 2019 601 615 l yamamoto 2021 1 15 k yang 2018 609 623 x yoshikane 2022 e0000016 t yoshimura 2008 22 26 k yu 2017 92 104 p zarei 2021 677 689 m yinx2022x128125 yinx2022x128125xg full 2022 06 29t03 03 58z fundingbody japan le tracking http creativecommons org licenses by nc nd 4 0 nr justice publishacceptedmanuscriptindexable http www elsevier com open access userlicense 1 0 2024 07 01t00 00 00 000z http creativecommons org licenses by nc nd 4 0 this is an open access article under the cc by nc nd license 2022 the author s published by elsevier b v 2022 07 07t06 30 41 078z http vtw elsevier com data voc addontypes 50 7 aggregated refined 0 item s0022 1694 22 00700 4 s0022169422007004 1 s2 0 s0022169422007004 10 1016 j jhydrol 2022 128125 271842 2022 12 08t09 03 37 529035z 2022 09 01 2022 09 30 unlimited japanletr 1 s2 0 s0022169422007004 main pdf https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169422007004 main application pdf 4b0285b3edd7313ce2564528366eaf7c main pdf main pdf pdf true 8001194 main 13 1 s2 0 s0022169422007004 main 1 png https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169422007004 preview image png 4123380e7d190d515cbb4afac18f24f9 main 1 png main 1 png png 60146 849 656 image web pdf 1 1 s2 0 s0022169422007004 gr2 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169422007004 gr2 downsampled image jpeg 4be90c6e80f97c866ec10430382d9485 gr2 jpg gr2 gr2 jpg jpg 32953 199 533 image downsampled 1 s2 0 s0022169422007004 gr1 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169422007004 gr1 downsampled image jpeg 0cbe6895767ec5fc46ba1c4995287918 gr1 jpg gr1 gr1 jpg jpg 42264 261 533 image downsampled 1 s2 0 s0022169422007004 gr4 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169422007004 gr4 downsampled image jpeg a712b50ded2387e8831a266f8a83c11a gr4 jpg gr4 gr4 jpg jpg 62340 333 696 image downsampled 1 s2 0 s0022169422007004 gr3 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169422007004 gr3 downsampled image jpeg 21a5eb1573d81c2a3a3cd83d87bafbdb gr3 jpg gr3 gr3 jpg jpg 94138 397 734 image downsampled 1 s2 0 s0022169422007004 gr6 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169422007004 gr6 downsampled image jpeg f099388119a031b99a59bbe17721a6ca gr6 jpg gr6 gr6 jpg jpg 91897 390 711 image downsampled 1 s2 0 s0022169422007004 gr5 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169422007004 gr5 downsampled image jpeg d01367a395335d8ebe2273b5357efb78 gr5 jpg gr5 gr5 jpg jpg 72801 479 685 image downsampled 1 s2 0 s0022169422007004 gr8 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169422007004 gr8 downsampled image jpeg 1df1aed63369aad69dbb6746103101e9 gr8 jpg gr8 gr8 jpg jpg 95887 582 711 image downsampled 1 s2 0 s0022169422007004 gr7 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169422007004 gr7 downsampled image jpeg c5bfdfe2474a5d90cdca8ac954fe6aa8 gr7 jpg gr7 gr7 jpg jpg 101191 573 711 image downsampled 1 s2 0 s0022169422007004 gr9 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169422007004 gr9 downsampled image jpeg 270ac0d3049fca74282b1b86e808332c gr9 jpg gr9 gr9 jpg jpg 120307 645 624 image downsampled 1 s2 0 s0022169422007004 gr2 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169422007004 gr2 thumbnail image gif d899ae00437fd1ee1614921d05503712 gr2 sml gr2 gr2 sml sml 5846 82 219 image thumbnail 1 s2 0 s0022169422007004 gr1 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169422007004 gr1 thumbnail image gif c82a4fa83717a73ab13de2304850fd80 gr1 sml gr1 gr1 sml sml 9923 107 219 image thumbnail 1 s2 0 s0022169422007004 gr4 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169422007004 gr4 thumbnail image gif 7b799e16f33536cdfac0cedf23adccfd gr4 sml gr4 gr4 sml sml 7079 105 219 image thumbnail 1 s2 0 s0022169422007004 gr3 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169422007004 gr3 thumbnail image gif c81111bee83ad31b2fe0db0ee689f62a gr3 sml gr3 gr3 sml sml 11143 118 219 image thumbnail 1 s2 0 s0022169422007004 gr6 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169422007004 gr6 thumbnail image gif b22b4de68100b52b3d4c41cff7bd0cc8 gr6 sml gr6 gr6 sml sml 11447 120 219 image thumbnail 1 s2 0 s0022169422007004 gr5 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169422007004 gr5 thumbnail image gif 7d889dfb840abe4cf890dd222f037e14 gr5 sml gr5 gr5 sml sml 10236 153 219 image thumbnail 1 s2 0 s0022169422007004 gr8 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169422007004 gr8 thumbnail image gif f77ab5aacd4e60199ae8f0d13251bde3 gr8 sml gr8 gr8 sml sml 10224 164 200 image thumbnail 1 s2 0 s0022169422007004 gr7 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169422007004 gr7 thumbnail image gif cc9ba09dede2d35590fb3ee9587cea01 gr7 sml gr7 gr7 sml sml 11164 164 203 image thumbnail 1 s2 0 s0022169422007004 gr9 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169422007004 gr9 thumbnail image gif 403660a3b6670a7c23ea5e9134d4e399 gr9 sml gr9 gr9 sml sml 10841 163 158 image thumbnail 1 s2 0 s0022169422007004 gr2 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169422007004 gr2 highres image jpeg d89b8f5a1a0168b36bd759cd5b5e9679 gr2 lrg jpg gr2 gr2 lrg jpg jpg 244806 880 2362 image high res 1 s2 0 s0022169422007004 gr1 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169422007004 gr1 highres image jpeg 3c68050ae85904be1c867c20e76a77f0 gr1 lrg jpg gr1 gr1 lrg jpg jpg 286398 1158 2362 image high res 1 s2 0 s0022169422007004 gr4 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169422007004 gr4 highres image jpeg a573ff7379eb073fba309300cc70b16a gr4 lrg jpg gr4 gr4 lrg jpg jpg 413622 1476 3083 image high res 1 s2 0 s0022169422007004 gr3 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169422007004 gr3 highres image jpeg 58533051b04b413a75b1621743c58c87 gr3 lrg jpg gr3 gr3 lrg jpg jpg 657095 1756 3248 image high res 1 s2 0 s0022169422007004 gr6 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169422007004 gr6 highres image jpeg 87f42e6f8f26e3a04e287ba0d2d224ec gr6 lrg jpg gr6 gr6 lrg jpg jpg 661149 1728 3149 image high res 1 s2 0 s0022169422007004 gr5 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169422007004 gr5 highres image jpeg 1dcbcdf45094e5c1117200aa5abe3aa0 gr5 lrg jpg gr5 gr5 lrg jpg jpg 522032 2119 3031 image high res 1 s2 0 s0022169422007004 gr8 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169422007004 gr8 highres image jpeg 139a376ab69bee8e1d3268ef9f67d550 gr8 lrg jpg gr8 gr8 lrg jpg jpg 644875 2580 3150 image high res 1 s2 0 s0022169422007004 gr7 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169422007004 gr7 highres image jpeg a6afd7d44a125cdb65505092a78bf705 gr7 lrg jpg gr7 gr7 lrg jpg jpg 709561 2538 3150 image high res 1 s2 0 s0022169422007004 gr9 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169422007004 gr9 highres image jpeg 54b9dc773a579ba68aec9f1166de19cb gr9 lrg jpg gr9 gr9 lrg jpg jpg 817106 2857 2764 image high res 1 s2 0 s0022169422007004 mmc1 docx https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169422007004 mmc1 main application vnd openxmlformats officedocument wordprocessingml document 2d826eb5de20cbb6f5b589ab546ae2fe mmc1 docx mmc1 mmc1 docx docx 33366037 application 1 s2 0 s0022169422007004 am pdf https s3 eu west 1 amazonaws com prod ucs content store eu west content egi 10vsvh6d14t main application pdf 811875113b5cee2306b9fe850880ca5a am pdf am am pdf pdf false 2123428 aam pdf hydrol 128125 128125 s0022 1694 22 00700 4 10 1016 j jhydrol 2022 128125 the author s fig 1 study area of southwestern japan including kyushu shikoku and chugoku fig 2 illustration of a training data preparation for each year and month the first 3 hour forecasts from each 39 hour forecast were collected and combined as modeled data blue observed precipitation from radar amedas was used as predictand green b in the svm regression data in the training years were collected every three hours red marks but the prediction was made for all hours in the validation year gray marks for interpretation of the references to colour in this figure legend the reader is referred to the web version of this article fig 3 averaged hourly precipitation in january from a radar amedas b msm gpv c qm and d cdft e svm f svm qm and g svm cdft the statistical metrics evaluated against radar amedas were shown in each subplot and the units for r rmse and bias are unitless mm hr and mm hr respectively fig 4 taylor diagram of estimated hourly precipitation in a january and b july derived from msm gpv gray cross svm blue pentagon qm orange square cdft green triangle svm qm red diamond and svm cdft purple circle the black star on the x axis labeled as reference represents observations from radar amedas for interpretation of the references to colour in this figure legend the reader is referred to the web version of this article fig 5 receiver operating characteristic roc curves in a january and c july derived from msm gpv gray svm blue qm orange cdft green svm qm red and svm cdft purple the black box area in a and c are enlarged as shown in b and d respectively for interpretation of the references to colour in this figure legend the reader is referred to the web version of this article fig 6 averaged hourly precipitation in july from a radar amedas b msm gpv c qm and d cdft e svm f svm qm and g svm cdft the performance statistics evaluated against radar amedas were shown in each subplot and the unit for r rmse and bias are unitless mm hr and mm hr respectively fig 7 forecast accuracy of a r b rmse c pod and d far versus lead time in january 2020 dot solid lines represent averaged statistics of the 248 members of 39 hour precipitation forecasts from msm gpv black svm orange svm qm blue and svm cdft red corresponding dashed lines represent the standard deviation sd of the 248 members for interpretation of the references to colour in this figure legend the reader is referred to the web version of this article fig 8 forecast accuracy of a r b rmse c pod and d far versus lead time in july 2020 dot solid lines represent averaged statistics of the 248 members of 39 hour precipitation forecasts from msm gpv black svm orange svm qm blue and svm cdft red corresponding dashed lines represent the standard deviation sd of the 248 members for interpretation of the references to colour in this figure legend the reader is referred to the web version of this article fig 9 precipitation at 4 a m on july 5th gmt 9 from a radar amedas b d msm gpv e g svm h j svm qm and k m svm cdft the second third and fourth columns represent the precipitation forecast of the target hour at a lead time of 37 h 19 h and 1 h respectively table 1 experiment cases and corresponding descriptions in the study cases description 1 msm gpv modeled hourly precipitation from msm gpv 2 svm post processed modeled hourly precipitation using svm 3 qm post processed modeled hourly precipitation using qm 4 cdft post processed modeled hourly precipitation using cdft 5 svm qm post processed modeled hourly precipitation using svm and qm 6 svm cdft post processed modeled hourly precipitation using svm and cdft table 2 evaluation metrics and indices used in the study index index name equation or definition unit r correlation coefficient r i 1 n x o i x o i 2 x m i x m i 2 i 1 n x o i x o i 2 i 1 n x m i x m i 2 rmse root mean square error rmse 1 n i 1 n x m i x o i 2 mm bias bias bias i 1 n x m i x o i n mm ard absolute relative difference ard s m s o s 0 pod probability of detection pod tp tp f n far false alarm ratio far fp fp t n r10 number of heavy precipitation hours monthly count of hourly precipitation 10 mm hours rx1h max 1 hour precipitation monthly maximum 1 hour precipitation mm rx3h max 3 hour precipitation monthly maximum 3 hour precipitation mm notation x o i and x m i observed and estimated hourly precipitation x o i and x m i average observed and estimated hourly precipitation n number of precipitation data sm and so statistics from estimated and observed precipitation tp true positive observed rain correctly detected fn false negative observed rain not detected fp false positive rain detected but not observed tn true negative rain not detected and not observed table 3 evaluation metrics of estimated hourly precipitation in all examined experiment cases during january and july in the cross validation experiment the absolute relative differences for the mean ardm standard deviation ardsd skewness ardsk and kurtosis ardkr of the hourly precipitation distribution were examined r rmse mm bias mm ardm ardsd ardsk ardkr january msm gpv 0 387 0 246 0 022 0 930 0 533 0 758 4 186 svm 0 490 0 213 0 037 0 742 0 630 0 748 2 797 qm 0 385 0 250 0 023 1 131 0 546 0 695 3 376 cdft 0 397 0 256 0 009 1 149 0 567 0 708 3 524 svm qm 0 460 0 232 0 006 0 931 0 535 0 759 2 647 svm cdft 0 475 0 228 0 001 0 869 0 544 0 744 2 731 july msm gpv 0 235 1 387 0 109 1 633 0 892 0 528 1 407 svm 0 296 1 234 0 197 4 994 0 793 0 644 1 054 qm 0 228 1 502 0 045 1 710 0 969 0 553 1 720 cdft 0 234 1 545 0 021 2 202 1 069 0 543 1 441 svm qm 0 282 1 435 0 098 1 670 0 826 0 663 1 389 svm cdft 0 280 1 378 0 033 1 653 0 816 0 668 1 337 table 4 statistics of extreme indices derived from all experiment cases against observations in january and july the units of rmse and bias for r10 rx1h and rx3h are hour mm hr and mm 3hr respectively january july r rmse bias r rmse bias r10 msm gpv 0 332 0 456 0 001 0 606 5 342 2 922 svm 0 176 0 447 0 146 0 481 8 048 6 365 qm 0 302 0 517 0 031 0 631 5 281 0 455 cdft 0 295 0 571 0 086 0 647 5 258 0 337 svm qm 0 338 0 537 0 032 0 635 6 837 1 940 svm cdft 0 299 0 501 0 020 0 637 5 682 0 172 rx1h msm gpv 0 550 2 881 0 032 0 357 16 460 5 609 svm 0 538 3 443 2 203 0 418 22 680 19 104 qm 0 495 3 477 0 017 0 348 17 394 0 018 cdft 0 503 3 478 0 481 0 357 16 872 1 585 svm qm 0 511 3 081 0 201 0 358 15 743 1 948 svm cdft 0 516 3 152 0 228 0 352 16 044 2 536 rx3h msm gpv 0 606 5 199 0 104 0 457 29 949 12 088 svm 0 605 5 989 3 579 0 495 39 797 31 116 qm 0 570 5 956 0 333 0 455 30 469 1 504 cdft 0 576 6 008 0 829 0 452 30 825 2 256 svm qm 0 570 5 846 0 873 0 435 32 306 4 602 svm cdft 0 581 5 747 0 565 0 434 31 056 1 022 research papers a support vector machine based method for improving real time hourly precipitation forecast in japan gaohong yin a takao yoshikane a kosuke yamamoto b takuji kubota b kei yoshimura a b a institute of industrial science the university of tokyo kashiwa japan institute of industrial science the university of tokyo kashiwa japan institute of industrial science the university of tokyo kashiwa japan b earth observation research center japan aerospace exploration agency tsukuba japan earth observation research center japan aerospace exploration agency tsukuba japan earth observation research center japan aerospace exploration agency tsukuba japan corresponding authors at institute of industrial science the university of tokyo 5 1 5 kashiwanoha kashiwa chiba 277 8574 japan gaohong yin institute of industrial science the university of tokyo 5 1 5 kashiwanoha kashiwa chiba 277 8574 japan this manuscript was handled by andras bardossy editor in chief with the assistance of fi john chang associate editor real time precipitation forecast facilitates water management and water associated disaster early warning however numerical weather prediction nwp models provide precipitation forecasts with bias this study proposed to combine support vector machine svm regression with quantile based bias correction method to improve real time 39 hour precipitation forecasts in japan five methods were compared and evaluated against observations which include svm regression quantile mapping qm cumulative distribution function transform cdft and the combination of svm and qm or cdft results indicated that the combination of svm and cdft i e svm cdft generally provided the highest accuracy with good computational efficiency svm alone improved the spatial representation of hourly precipitation with a correlation coefficient increased from 0 387 to 0 490 in january and from 0 235 to 0 296 in july in the cross validation experiment however svm underestimated the variability of hourly precipitation and heavy precipitation events qm and cdft perform well in correcting the bias in modeled precipitation while they have limited capability in correcting the rainband location combining svm and quantile based method took advantage of both approaches providing a more consistent variability with observations and better predicted extreme precipitation events although overestimation of rainfall area was witnessed the simple concept high computational efficiency as well as evident improvement in forecast accuracy make the combined cases especially the svm cdft method beneficial for real time precipitation forecast and flood early warning keywords precipitation real time forecasting support vector machine regression quantile mapping cdf transform bias correction 1 introduction japan is a country largely influenced by extreme precipitation natural disasters associated with extreme precipitation cause great loss in economics and human lives according to the ministry of land infrastructure transport and tourism mlit of japan mlit 2020 the frequency of short duration intense precipitation increased approximately 1 4 times within the recent 30 years the number of rivers exceeding dangerous water levels increased from 83 in 2014 to 403 in 2019 the economic damage caused by floods in japan increased from approximately 200 billion jpy in 2010 to over 2 15 trillion jpy in 2019 therefore an accurate and effective real time forecast of hazards associated with precipitation is crucial to enhance land surface hydrological prediction and further improve flood forecasts a real time operating system today s earth te https www eorc jaxa jp water index html was developed jointly by the japan aerospace exploration agency jaxa and the university of tokyo yoshimura et al 2008 ma et al 2021 te is a terrestrial surface simulation system aiming at forecasting land surface and river conditions with high resolution in both space and time as a regional version of te today s earth japan te japan has demonstrated its capability to capture the location and timing of extreme floods with satisfactory accuracy such as during the typhoon hagibis in 2019 ma et al 2021 by using the mesoscale model grid point value msm gpv saito et al 2006 atmospheric analysis data from the japan meteorological agency jma as forcing data te japan is capable of providing real time flood forecasts with a lead time of 39 h msm is an operational non hydrostatic numerical weather prediction nwp model to support disaster prevention saito et al 2006 non hydrostatic models outperform common dynamic nwp models by explicitly resolving deep convection for precipitation forecasts and providing a finer horizontal resolution whan and schmeits 2018 however due to the high computational cost these high resolution models usually provide deterministic forecasts without uncertainty estimates cattoën et al 2020 and contain errors in estimating the location of small scale extreme events whan and schmeits 2018 thus a careful evaluation and post processing of model based precipitation forecasts is still needed maurya et al 2020 yamamoto and sayama 2021 statistical methods are often used to correct bias in precipitation forecasts by establishing a statistical relationship between model outputs and observations moghim and bras 2017 one common statistical method is the quantile based method which adjusts the simulated cumulative distribution function cdf with observations heredia et al 2018 different quantile based methods have been developed and used for precipitation bias correction such as the quantile mapping qm panofsky et al 1958 wood et al 2002 teutschbein and seibert 2012 chen et al 2013 cannon et al 2015 equidistant cdf edcdf li et al 2010 moghim et al 2017 yang et al 2018 and the cdf transform cdft michelangeli et al 2009 pierce et al 2015 heredia et al 2018 previous studies found that quantile based methods outperformed simpler bias correction methods that only correct the mean and variance of simulated precipitation machine learning ml has been increasingly used for the downscaling and bias correction of precipitation xu et al 2019 kumar et al 2019 jose et al 2022 xiao et al 2022 the ml technique can solve high dimensional nonlinear and multivariate complex systems without explicitly constructing a physical or statistical model he et al 2016 moghim and bras 2017 vandal et al 2019 moghim and bras 2017 developed an artificial neural network ann model to bias correct monthly precipitation from a climate model they found that ann reduced estimation error and improved correlation even when the temporal consistency between modeled and observed data sets was weak xu et al 2019 implemented the wavelet support vector machine wsvm and wavelet random forest wrf methods for the bias correction and downscaling of multi model ensemble precipitation forecasts at a monthly scale results demonstrated the benefits of ml based methods in downscaling while in terms of correction the uncertainties of precipitation forecasts in summer and extreme events were still considerable zarei et al 2021 compared rf and qm methods in correcting the bias of daily ensemble precipitation forecasts and found that rf performed better concerning probabilistic evaluations previous studies usually implemented ml for precipitation bias correction on a daily or monthly scale the potential of ml to correct hourly precipitation forecast has not been well investigated especially for the real time operation purpose which seeks not only accuracy but also computational efficiency moreover most studies focused on correcting the bias of ensemble forecasts of precipitation from climate models with coarse spatial resolution the effectiveness of the bias correction method to improve high resolution deterministic precipitation forecast from non hydrostatic models is less studied the objective of the study is to improve the real time 39 hour precipitation forecast simulated from msm gpv in order to enhance flood forecast accuracy over japan a combined strategy by using svm regression and quantile based method i e qm and cdft jointly was proposed in the study the paper is organized as follows section 2 described the study area and data sets used in this study section 3 introduced the experiment design and evaluation metrics section 4 presented the results from the cross validation and real time forecast experiments section 5 provides a discussion of the results followed by the findings of this study summarized in section 6 2 study area and datasets 2 1 study area the study area is southwestern japan including kyushu shikoku and chugoku with a geographic boundary of 29 n to 37 34 n and 127 8 e to 136 14e fig 1 southwestern japan is located in the asian monsoon zone with hot and humid summers and moderately cold winters it is one of the regions in japan having the largest amount of annual precipitation especially in kyushu and shikoku kato 2006 sugimoto 2020 tsuji et al 2020 due to the frequent occurrence of extreme precipitation this region is vulnerable to flooding disasters such as the series of fatal floods in kyushu in july 2020 a large portion 30 of precipitation falls during the summer rainy season which is known as baiu bertacchi uvo et al 2001 about 90 of the torrential rainfall events in southern japan are associated with linear precipitation systems that result in narrow rainbands kato et al 2018 during the winter winter monsoon dominates the climate hirose and fukudome 2006 prevailing northwesterly winds bring snow to the sea of japan side of chugoku upstream of mountains and sometimes to the shikoku mountains 2 2 data sets 2 2 1 nwp based hourly precipitation mesoscale model grid point value msm gpv data set from the nonhydrostatic mesoscale model developed at jma was used as the modeled precipitation saito et al 2006 msm gpv provides a 39 hour precipitation forecast every three hours i e eight times a day covering japan and its surrounding area with a spatial resolution of 0 05x 0 0625 in the study msm gpv hourly precipitation forecasts from 2007 to 2020 were acquired and rescaled onto a grid with a resolution of 0 06x 0 06for convenience 2 2 2 observed hourly precipitation radar automated meteorological data acquisition system amedas makihara et al 1996 makihara 2000 from jma was used to evaluate the modeled precipitation radar amedas is a radar rain gauge analyzed precipitation product that calibrates radar data with ground based rain gauge data and provides hourly rainfall with a 1 km resolution every 30 min nagata 2011 radar amedas precipitation data provides high accuracy with slight underestimation relative to rain gauge nagata 2011 and overestimation relative to amedas measurements makihara et al 1996 it has been widely used as a referential dataset for heavy rainfall warning takido et al 2016 and precipitation product evaluation iida et al 2006 kubota et al 2009 hirose et al 2019 in japan the study used radar amedas hourly precipitation initiated at the beginning of each hour from 2007 to 2020 i e data issued at the middle of each hour was not used afterward data was upscaled to the same grid as the regridded msm gpv with a spatial resolution of 0 06 0 06 3 methods 3 1 support vector machine svm regression svm regression is a supervised ml method that has been widely used in hydrologic and climate change studies elshorbagy et al 2010 sachindra et al 2018 chen et al 2019 huang et al 2020 the basic idea of svm regression is to find the optimal hyperplane and boundary layers to minimize a defined error function chivers et al 2020 svm employs the structural risk minimization principle and reduces the problem of overfitting yu et al 2017 additionally svm shows the advantage of not being trapped in local minimums najafi et al 2011 and can be used with a relatively small sample size al anazi and gates 2012 a brief introduction of svm regression is provided in the section and more details can be found in vapnik 1995 1998 consider a given training data set x i y i i 1 n where x i is the input vector i e model estimates and y i is the corresponding output i e observation and n is the size of the data set the svm regression function can be written as 1 f x w φ x b where w is the weight vector b is the bias and φ is the nonlinear transformation function that maps the input space into a higher dimensional feature space the objective of svm regression is to find the w and b that minimize the empirical risk 2 r empirical 1 n i 1 n y i f x i where y i f x i is defined as the vapnik s ε insensitive loss function with errors less than the tolerance ε ignored 3 y i f x i 0 f o r y i f x i ε y i f x i ε f o r y i f x i ε after reformulation the svm regression problem can be summarized as an optimization problem 4 min 1 2 w 2 c i 1 n ξ i ξ i s u b j e c t t o y i f x i ε ξ i f x i y i ε ξ i ξ i 0 ξ i 0 where c is a positive constant determining the degree of penalized loss ξ i and ξ i are slack variables specifying the upper and lower training errors subject to the error tolerance ε the optimization problem can be solved by using a series of lagrange multipliers α i and α i and the svm regression function can be rewritten as 5 f x i 1 n α i α i k x x i b where k x x i φ x i φ x i and is known as the kernel function in the study a nonlinear regression based on the gaussian radial basis function rbf was used where γ is a parameter associated with the width of the kernel 6 k x x i e x p γ x x i 2 in the study both input i e predictor and output i e predictand variables of the svm model are hourly precipitation instead of establishing a point to point relationship between modeled and observed precipitation the concept of feature vector range was used to relate observed precipitation at the target grid with neighboring grids in model simulations for each observed precipitation yj t at target grid j and time t modeled precipitation in a 21x21 grid points domain i e 1 26x1 26 centered at the target grid at time t was used as x j t j 1 441 svm model was trained to learn the relationship between modeled precipitation located inside the feature vector range x j t and the observed precipitation at the target grid yj t the feature vector range was utilized based on the assumption that the spatial distribution of modeled precipitation over a range of areas is connected with observed precipitation at the target location through weather patterns yoshikane et al 2022 the use of a feature vector range also provides the potential to correct the location of narrow rainfall bands that are mistakenly simulated the hyperparameters aforementioned including c γ and ε determine the error tolerance level and the accuracy of an svm model smets et al 2007 shi 2020 following yoshikane et al 2022 a random search approach bergstra and bengio 2012 on a regular grid layout i e use grids with an interval of 10 grids along both latitude and longitude directions was used to select the hyperparameters the optimal hyperparameters were determined based on the combination that provided the minimum root mean square error rmse 3 2 quantile mapping qm qm is a widely used statistical method for the bias correction and downscaling of model simulations panofsky et al 1958 wood et al 2002 teutschbein and seibert 2012 chen et al 2013 cannon et al 2015 the fundamental idea is to equate the cumulative distribution functions cdfs of modeled and observed variables during the historical period as shown in equation 7 7 f o h x o h f m h x m h x o h f o h 1 f m h x m h where f o h and f m h represent the cdfs of observed data x o h and modeled data x m h in a historical period h respectively f m h 1 is the inverse function of f m h and is defined in the range of 0 1 the qm method assumes that model bias is stationary which indicates the characteristics in the historical period do not change in the future in the study the qm on a grid cell basis was conducted 3 3 cdf transform cdft the qm method only considers the distribution of historical data without taking the future modeled data into account therefore an extended method cdf transform hereafter cdft michelangeli et al 2009 was developed based on the assumption that there is a transform t allowing to translate the cdf of modeled historical data into the cdf of observed historical data 8 t f m h x f o h x assuming u f m h x u 0 1 equation 8 can be written as 9 t u f o h f m h 1 u further assuming that the relationship in equation 9 persists into the future i e t f m f x f o f x replacing u in equation 9 with f m f x the target cdf i e cdf of observed future data is obtained as 10 f o f x f o h f m h 1 f m f x where f o f and f m f represent the cdfs of observed data and modeled data in a future period f the cdft method takes both historical and future data into consideration and thus better captures the characteristics associated with climate change the cdft method has been implemented for statistical downscaling and bias correction of climate model simulations for future projections bürger et al 2013 pierce et al 2015 famien et al 2018 however the potential of the cdft method to correct bias in forecast data at seasonal or diurnal scales was seldom investigated ngoungue et al 2021 therefore cdft was included in the study to investigate its plausibility in correcting bias in short range forecasts 3 4 experiment design following the typical framework to evaluate an ml model the experiment consisted of a cross validation experiment and a 39 hour forecast testing experiment the study period from 2007 to 2020 was divided into the training and validation period i e 2007 2019 and the testing period i e the year 2020 experiments were conducted for each month and results in january and july were shown in the study results for other months are available in the supplementary 3 4 1 cross validation experiment a leave one year out cross validation experiment was conducted for the validation period from 2007 to 2019 to evaluate the overall performance of all examined experiment cases instead of using all 39 hour precipitation forecasts from msm gpv only the first 3 hour forecasts from each 39 hour forecast were collected and combined as the modeled data blue line as shown in fig 2 a observed hourly precipitation from radar amedas was used as the predictand green line only the first 3 hour forecasts from msm gpv were used because random errors in the forecasts increase rapidly with lead time which may deteriorate the performance of bias correction methods the study evaluated six experiment cases with descriptions summarized in table 1 results from 1 modeled precipitation msm gpv and post processed precipitation based on 2 svm 3 qm 4 cdft 5 the combination of svm and qm svm qm and 6 the combination of svm and cdft svm cdft were evaluated with radar amedas in the svm regression training data was selected every three hours in each training year and the prediction was made for all hours in the validation year fig 2b for the sake of computational efficiency a trial experiment showed that using all training data to train the svm model did not yield substantially different results but the computational time was considerably increased additionally due to the significant skewness in the hourly precipitation i e large number of no rain and light rain events a commonly used undersampling strategy was adopted to reduce the ratio of no rain versus rain data to 1 1 prior to applying svm regression hyperparameters used for both january and july precipitation were selected as c 16 γ 5 10 6 ε 0 01 in the cdft case due to the limited number of forecasts in the future period especially in the real time 39 hour forecast experiment section 3 4 2 the cdf of modeled future data f m f was derived by using both modeled historical i e training period and future i e validation or testing period data using the cross validation for july 2007 as an example f m h and f m f were acquired using modeled data in july from 2008 to 2019 and 2007 to 2019 respectively in the combination cases i e svm qm and svm cdft qm or cdft was used as an additional step after svm regression to adjust the cdf of post processed precipitation from svm with observations results from svm qm and svm cdft were compared with svm qm and cdft to investigate if the combination strategy is superior to the individual method 3 4 2 39 hour forecast experiment the real time precipitation forecast experiment used pre processed data in section 3 4 1 from 2007 to 2019 as training data and the 39 hour forecasts in 2020 were tested based on the cross validation results only svm based experiment cases i e svm svm qm and svm cdft were examined in the testing experiment again training data were selected every three hours for svm model training considering the computational efficiency the trained svm model was subsequently used for precipitation forecasts in 2020 results from svm based experiment cases were evaluated against radar amedas 3 5 evaluation metrics various metrics were used to evaluate the post processed modeled precipitation as shown in table 2 statistical metrics including correlation coefficient r root mean square error rmse and bias were used to quantify the consistency between observed and estimated precipitation additionally the absolute relative difference ard was used to evaluate the effectiveness and robustness of the examined bias correction method most bias correction methods correct only the first two moments i e mean and standard deviation of the target variable but often introduce additional uncertainties especially to higher order moments lafon et al 2013 therefore in the study the ard of four statistical moments including mean ardm standard deviation ardsd skewness ardsk and kurtosis ardkr were evaluated closer to zero ard value indicates a better correction result additionally contingency statistical metrics including the probability of detection pod and false alarm ratio far were calculated to evaluate the capability of estimated precipitation to detect rainfall events correctly by selecting different thresholds to distinguish wet and dry conditions a series of pod and far values can be calculated and displayed as a receiver operating characteristic roc curve the upper left point of the roc curve i e pod 1 and far 0 indicates a perfect classifier curves below the 1 1 line indicate a worse than random classifier performance moreover extreme precipitation indices were also examined to evaluate the capability of modeled precipitation to estimate heavy rainfall events which include the monthly count of hourly precipitation no less than 10 mm hr r10 monthly maximum 1 hour rx1h and 3 hour rx3h precipitation 4 results 4 1 cross validation results in january 4 1 1 average precipitation in january averaged hourly precipitation in january during the validation period i e 2007 2019 was calculated as shown in fig 3 msm gpv captured the spatial variation of long term averaged precipitation with high rainfall intensity witnessed in northern chugoku fig 3b however the area with average rainfall intensity larger than 0 4 mm hr was underestimated qm showed similar performance to msm gpv while cdft provided a better consistency with observations rcdft 0 984 and biascdft 0 009 mm hr and the high rainfall intensity area was better captured svm improved the spatial representation of precipitation in january relative to msm gp with a large r of 0 976 however the large rainfall rate in northern chugoku was not captured leading to the largest bias 0 037 mm hr among all examined cases svm cdft provided the highest agreement with observations with r close to 1 and bias close to zero the close to unity r value in the svm cdft case is attributed to both svm and cdft as svm enhanced the spatial representation of hourly precipitation and cdft further adjusted the temporal distribution of each validation year to match with the long term distribution it is also noted that except for the qm case all other examined cases provided a negative bias indicating an overall underestimation of precipitation relative to radar amedas in january 4 1 2 evaluation of hourly precipitation in january the spatial statistics of each hour in each validation year were calculated afterward validation year averaged hourly statistics were computed in table 3 svm based cases improved the spatial correlation and reduced rmse relative to msm gpv with the svm case yielding the largest r of 0 490 cdft case provided a larger r but also increased the rmse value rcdft 0 397 and rmsecdft 0 256 mm hr relative to msm gpv as for qm it slightly degraded all examined statistics at an hourly scale the improved correlation in svm based cases suggested that the use of feature vector range i e neighboring grid cells in svm regression was effective however the svm case failed to capture heavy precipitation events resulting in a large negative bias the combination of svm and qm or cdft took advantage of each method yielding the highest statistics with high r values and small bias particularly in the svm cdft case rsvm cdft 0 475 and biassvm cdft 0 001 mm hr ard values were used to indicate the bias of distribution moments of estimated hourly precipitation as shown in table 3 closer to zero ard value indicates a better bias correction performance results suggested that all examined experiment cases effectively reduced the bias for high order moments i e skewness and kurtosis except that svm qm provided a slightly larger ardsk than msm gpv the bias correction performance for the first two moments i e mean and standard deviation varied with experiment cases both qm and cdft yielded relatively larger ardm and ardsd than msm gpv whereas svm based cases tended to provide smaller ardm but larger ardsd the relatively poor performance of qm and cdft at the first two moments may be because they are conducted on a grid cell basis and thus failed to capture the spatial characteristics of hourly rainfall svm cdft generally provided the highest accuracy with a reduced or similar ard value for all examined moments relative to msm gpv collecting all hourly precipitation at each grid in january during the validation years from 2007 to 2019 the taylor diagram was shown in fig 4 a to evaluate the similarity between estimated and observed precipitation a smaller distance to the reference i e observation represents a better capability to reproduce observation results showed that svm based cases i e svm svm qm and svm cdft provided slightly larger r and smaller centered rmse relative to msm gpv however the svm case largely underestimated the variability of hourly precipitation relative to observations with a normalized standard deviation of about 0 53 qm and cdft were located close to the msm gpv case with slightly smaller r and larger centered rmse than msm gpv svm cdft overall yielded the highest consistency between estimated and observed hourly precipitation furthermore roc curves were used to illustrate the diagnostic ability of different experiment cases to distinguish between wet and dry conditions fig 5 a b a range of threshold values from 0 1 mm hr to 50 mm hr were used to calculate pod and far values all examined cases showed better skill than the random classifier i e above the 1 1 line as the threshold value increases both pod and far decrease svm qm and svm cdft generally provided superior accuracy in classifying wet and dry conditions with larger pod values especially the svm qm case however when the threshold is over 20 mm hr qm and cdft provided slightly higher pod relative to svm qm and svm cdft fig 5b suggesting their better capability of capturing heavy rainfall events in january 4 2 cross validation results in july 4 2 1 average precipitation in july averaged hourly precipitation in july from 2007 to 2019 was computed as shown in fig 6 during the summer monsoon period short duration intense precipitation occurs more frequently resulting in more extensive average precipitation than in january and the largest rainfall intensity was witnessed in central kyushu and southern shikoku msm gpv represented the spatial pattern of long term average precipitation in july however underestimation was found in most areas and overestimation was occasionally detected in areas such as yakushima island to the south of kyushu island both qm and cdft improved the spatial consistency between the estimated and observed precipitation with larger r and smaller bias similar to the findings in january all svm based cases provided a higher spatial r relative to msm gpv indicating their capability to capture the spatial characteristics of precipitation however the underestimation of heavy precipitation in the svm case is more severe than in january with a negative bias of 0 197 mm hr svm cdft yielded the largest r of 0 994 among all examined cases but it slightly overestimated the precipitation in central kyushu 4 2 2 evaluation of hourly precipitation in july the statistical skill of post processed hourly precipitation in july is inferior to january due to the dramatically increased spatio temporal variation of precipitation during the summer monsoon season table 3 similar to january results the svm case provided the highest r rsvm 0 296 but the largest bias biassvm 0 197 mm hr among all cases as for qm and cdft they largely reduced the bias of estimated precipitation relative to msm gpv whereas larger rmse and slightly smaller r values were witnessed again svm cdft yielded the highest overall accuracy with improved spatial correlation rsvm cdft 0 280 and reduced errors rmsesvm cdft 1 378 mm hr biassvm cdft 0 033 mm hr in terms of ard qm and cdft failed to reduce the bias of distribution moments and yielded larger ard values than msm gpv while svm based cases showed smaller ard values only for kurtosis ardkr and standard deviation ardsd the taylor diagram fig 4b showed more scattered results among experiment cases than in january msm gpv evidently underestimated the variability of hourly precipitation i e normalized standard deviation less than 1 in july svm greatly improved the correlation and reduced the centered rmse whereas it caused an even larger underestimation of precipitation variability when compared to radar amedas cdft and svm qm tended to increase precipitation variability while qm and svm cdft provided the highest agreement of spatio temporal variability with observations regarding the findings from roc curves fig 5c d the conclusions from july are similar to january in that svm qm and svm cdft generally provided the highest accuracy in identifying wet and dry conditions qm and cdft showed larger pod when the threshold value equals 50 mm hr svm case showed improvement relative to msm gpv only for light rainfall events due to the underestimation issue in svm regression 4 3 extreme precipitation analysis extreme precipitation prediction at an hourly scale is crucial but challenging especially for rainfall events associated with severe convection the capability of all examined cases to capture extreme precipitation events was examined for each validation year the extreme indices derived from all experiment cases and observations were acquired afterward the spatial statistics of extreme indices from each experiment case and observations were calculated and the validation year averaged statistics were shown in table 4 in january msm gpv consistently outperformed other experiment cases with larger r and smaller rmse and bias for all examined extreme indices in all examined bias correction cases improvement of extreme precipitation estimates in january was not witnessed while during july except for svm all other examined bias correction cases showed a larger r and smaller bias than msm gpv for r10 regarding the maximum hourly and 3 hourly precipitation svm showed the largest spatial correlation 0 418 and 0 495 for rx1h and rx3h with large negative bias all other bias correction cases provided reduced bias of rx1h and rx3h although improvements in spatial correlation and rmse were not clear 4 4 real time precipitation forecasts analysis cross validation analysis demonstrated the capability of svm based methods to improve the spatial representation of hourly precipitation which is critically essential for flood forecasts therefore the performance of svm based methods for real time precipitation forecast in 2020 was further investigated in this section as msm gpv forecasts were issued every three hours totally 248 members of 39 hour precipitation forecast were initiated in both january and july i e eight forecasts per day for 31 days the overall accuracy of the precipitation forecasts versus lead time is discussed in this section moreover an extreme precipitation event in july was used as an example to evaluate the performance of svm based bias correction methods 4 4 1 overall forecast accuracy for each 39 hour precipitation forecast member the statistics skill i e r rmse pod and far of each experiment case at every hour were computed subsequently the averaged statistics of all 248 members at different lead times were calculated figs 7 and 8 displayed that all experiment cases yielded decreased r and increased rmse when lead time increased suggesting the deterioration of forecast accuracy at a longer lead time in terms of contingency statistics all experiment cases exhibited a decreased accuracy in pod threshold 2 5 mm hr as lead time increased whereas the performance of far varies in january and july in january fig 7d the averaged far values were relatively stable until about 25 h ahead followed by an increasing trend until the end of the forecast time while in july fig 8d the far values increased with lead time until about 10 h ahead and then maintained relatively stable comparing the performance of different experiment cases in january fig 7 all svm based cases provided larger r and smaller rmse than msm gpv particularly svm yielded the largest r and smallest rmse across the 39 hour forecast time however svm showed the lowest pod and far due to the limited dynamic range of precipitation estimated by svm svm qm and svm cdft were also slightly inferior to msm gpv in correctly identifying rainfall events with a threshold of 2 5 mm hr similar to the results in january the svm case provided the largest r and smallest rmse values during the 39 hour forecast time in july fig 8 however the performance of svm qm and svm cdft are distinct from the january results both svm qm and svm cdft provided larger r but larger rmse values than results from msm gpv throughout the 39 forecast hours additionally svm qm and svm cdft performed better in capturing rainfall events with higher pod values despite larger far values the standard deviation of metrics from the 248 members increased with lead time especially for rmse results also suggested that svm qm and svm cdft tended to add variability i e larger standard deviation to hourly precipitation forecast such increased variability is more apparent in july than in january when msm gpv significantly underestimated the variability of hourly precipitation 4 4 2 extreme precipitation forecast example precipitation forecast for july 5th at 4 a m gmt 9 was used as an example to assess the capability of svm based cases to forecast extreme precipitation this particular hour is selected because it experienced the largest domain averaged hourly precipitation in july 2020 based on observed precipitation from radar amedas jma also reported extreme precipitation over kyushu during this period especially in kumamoto and kagoshima https www data jma go jp obd stats data bosai report index 1989 html using 4 a m on july 5th as the target hour 13 forecasts with different lead times can be acquired with the shortest lead time of 1 h initiated at 3 a m of the same day observed and modeled precipitation at the target hour with a lead time of 37 h 19 h and 1 h are shown in fig 9 for all experiment cases as the lead time became shorter forecast accuracy was enhanced with the spatial variation of precipitation better represented similar to cross validation results the svm case consistently underestimated extreme precipitation all experiment cases failed to forecast the extreme precipitation 30 mm hr in southern kyushu 37 h ahead of the target time fig 9b e h k with a precipitation intensity of approximately 10 mm hr reported when the lead time was 19 h fig 9c f i l both svm qm and svm cdft better predicted the intensity and location of the extreme precipitation than msm gpv but they still underestimated the intensity when forecasting precipitation 1 h ahead of the target hour all cases represented the spatial characteristics of precipitation with higher intensity in mid to southern kyushu and southwestern shikoku msm gpv fig 9d showed a slight location mismatch of rainband in kyushu and regions that experienced extreme rainfall were largely underestimated results from svm qm fig 9j and svm cdft fig 9m are similar they both successfully predicted the large area of the extreme precipitation event at the target hour however both svm qm and svm cdft slightly overestimated extreme precipitation areas compared to the observation while from the perspective of hazard mitigation and flood warning such overestimation of extreme precipitation area is often preferable relative to the largely underestimated estimates from msm gpv barnes et al 2007 schultz et al 2010 5 discussion results of examined experiment cases in different months exhibited similar findings whereas differences were also witnessed these differences are mainly attributed to the distinctive spatio temporal characteristics of precipitation in different seasons in the study area july is one of the wettest months in southwestern japan which has more frequent extreme precipitation events with large variability in space and time while in january the winter monsoon brings dry and cold air leading to low intensity precipitation with a less spatio temporal variation therefore the overall accuracy of estimated precipitation in july is inferior to in january due to the significant dynamic variation of hourly precipitation in july that failed to be represented by msm gpv the benefit of the proposed method to improve precipitation forecast is often more profound than in january while in january the characteristics of precipitation are better represented by msm gpv and thus the improvement that can be achieved by bias correction is limited it is found that the svm case improved the spatial representation of precipitation in both january and july however it failed to capture extreme precipitation leading to a large negative bias in the estimated precipitation such underestimation was also found in previous studies and explained by the non gaussian outliers in the training data kalra and ahmad 2012 in the study hourly precipitation is studied which is highly skewed with a large number of non precipitation and light precipitation events with the highly imbalanced data svm regression attempts to minimize the penalty loss c i 1 n ξ i ξ i in equation 4 with the use of a soft margin leading to a bias toward the majority batuwita and palade 2013 such bias caused by data imbalance is prevalent for most ml algorithms and various methods have been proposed to mitigate the impact of this problem following previous work gagne et al 2014 this study undersampled non precipitation events prior to svm regression which alleviated the overwhelming signal of zeros however the effect of data imbalance cannot be entirely eliminated by undersampling yielding still underestimated precipitation applying a quantile based method upon svm results i e svm qm and svm cdft satisfactorily adjusted the variability of estimated precipitation derived from svm meanwhile maintaining the improved spatial representation of precipitation in the study qm and cdft were conducted on a grid cell basis i e using modeled precipitation at the target grid only while svm used information from neighborhood grids within the feature vector range a trial experiment results not shown here of conducting qm and cdft with the concept of feature vector range i e using identical data as svm found that they performed worse than the current qm and cdft strategy additionally unlike in the svm case the improvement of spatial pattern can hardly be detected in the trial experiment therefore the combination strategy used in the study took full advantage of each method providing the highest accuracy of post processed precipitation forecasts as a post processing method for a real time operating system not only accuracy but also computational efficiency is essential the examined experiment cases in the study are all simple in concept with high computation efficiency for example for the svm cdft case which provided the highest overall accuracy it spent less than 1 5 h to finish one year cross validation over a 140 140 grid on a linux operating system with a xeon 2 8 ghz processor and 16 gb of ram once the classifiers during the historical period i e 2007 2019 are prepared the processing time for one 39 hour forecast using svm cdft is approximately 5 min which is highly efficient a preliminary investigation on using more complicated strategies suggested that they provided a similar level of accuracy but the computational time also greatly increased for example a two step analysis using random forecast rf classification to identify rain or non rain cases followed by an svm regression to provide quantitative estimates of precipitation was tested the overall statistics from rf svm were not obviously better than the svm cdft case whereas the time spent for one year cross validation was almost tripled about 4 5 h 6 conclusion an accurate precipitation forecast is crucial for the early warning of associated disasters the study proposed to combine svm regression with a quantile based bias correction method to improve the real time hourly precipitation forecast from msm gpv results showed that svm effectively improved the spatial representation of hourly precipitation in both january and july however due to the high skewness of hourly precipitation data and the inherent characteristics of the svm regression svm failed to capture heavy precipitation events and underestimated the variability of precipitation especially during the wet month i e july qm and cdft effectively correct the bias in estimated precipitation caused by errors in convective parameterizations but they usually cannot correct bias associated with unrealistic large scale variability a combination of svm and qm or cdft took advantage of both methods providing post processed hourly precipitation with high spatial correlation low bias and high hit rate particularly in the svm cdft case the svm cdft method is simple without requiring additional information from other hydrological or meteorological variables overall the high accuracy and the efficient computational time outstand svm cdft to be used to improve real time hourly precipitation forecasts despite the encouraging results from svm cdft issues associated with the proposed method were also noticed it is found that svm cdft tended to overestimate rainfall coverage area especially during heavy precipitation events leading to a large rmse of estimated precipitation additionally the capability of svm cdft to improve extreme precipitation forecast is limited therefore further improvement of the current method is demanded moreover the selection of feature vector size is critically important in determining the performance of svm a preliminary investigation of feature vector size suggested that a smaller feature vector size might deteriorate the overall accuracy of precipitation forecast but accordingly improve local extreme precipitation forecast a non constant setting of parameters in svm may be beneficial and requires further exploration additionally the empirical svm model used in the study does not allow an uncertainty analysis of post processed precipitation forecasts therefore a probabilistic version of svm such as the bayesian svm polson and scott 2011 and kernel based svm esmaeilbeigi et al 2020 should be explored furthermore the potential of other ml based methods in improving precipitation forecast also requires more investigation in the following study declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements we gratefully appreciate dr misako kachi and dr riko oki from earth observation research center japan aerospace exploration agency for the support to this work this work was supported by earth observation research center japan aerospace exploration agency jaxa eorc grant jx pspc 533980 the environmental restoration and conservation agency of japan via the environment research and technology development fund s 20 grant jpmeerf21s12020 the council for science technology and innovation csti via cross ministerial strategic innovation promotion programs sip enhancement of national resilience against natural disasters funding agency nied the japan society for the promotion of science jsps grants 21h05002 18h03794 and 21 k20443 government of japan ministry of education culture sports science and technology via the integrated research program for advancing climate models tougou grant jpmxd0717935457 arcs ii grant jpmxd1420318865 and dias grant jpmxd0716808979 we thank the japan meteorological agency jma and data integration and analysis system program dias for providing the msm gpv precipitation forecast product http apps diasjp net gpv data we appreciate the japan meteorological business support center jmbsc for providing radar amedas data http www jmbsc or jp en contact html radar amedas data can be acquired by sending request to the international service officer jmbsc ab jmbsc or jp appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2022 128125 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 
3221,in urban environment there exist numerous pre existing underground structures which act as underground barriers to block the groundwater flow and soil movement i e water blocking wb and soil blocking sb effects making the responses of groundwater and soil to dewatering different from that without nearby structure in this study dewatering inside a foundation pit with a nearby metro station was simulated in a coupled hydro mechanical numerical model after validating the model by measured results from a field pumping test extensive numerical simulations were conducted to investigate the behaviours of groundwater and soil during dewatering under wb and sb effects results show that the two effects would induce opposite soil response either aggravating or restricting the dewatering induced soil settlement under the combined actions of the two barrier effects the soil responses depend on which effect would play a dominant role when the distance between the metro station and foundation pit d is small the sb effect plays a leading role restricting the dewatering induced soil settlement as d increases both the sb and wb effects get weakened while sb effect weakens more rapidly when d surpasses a critical value the wb effect in turn plays a dominant role aggravating the dewatering induced soil settlement keywords dewatering groundwater drawdown ground settlement water blocking effect soil blocking effect data availability data will be made available on request nomenclature c effective cohesion d distance between the station and the foundation pit e elastic modulus of soil e 0 initial void ratio h d dewatered depth inside the foundation pit k soil permeability k h horizontal hydraulic conductivity k v vertical hydraulic conductivity l 0 distance of influence l vm1 distance of the maximum settlement position to the excavation l vm2 distance of the maximum settlement position behind the metro station to the metro station q discharge rate of single pumping well r influence range of the pumping s w groundwater drawdown s s specific storage coefficient s 1 maximum drawdown outside excavation without metro station nearby s 2 maximum drawdown outside excavation with metro station nearby γ unit weight γ w unit weight of water η water blocking intensity factor δs difference of drawdowns on both sides of metro station δ hm maximum enclosure wall deflection δ hm1 maximum wall deflection for the case without metro station outside δ hm2 maximum wall deflection for the case with metro station outside δ vm maximum surface settlement δ vm1 maximum surface settlement for the case without metro station outside δ vm2 maximum surface settlement for the case with metro station outside δ vm3 maximum surface settlement behind metro station φ effective friction angle 1 introduction with the rapid urbanization in many regions a huge demand exists for the construction of underground infrastructures ni et al 2011 attard et al 2017 pujades et al 2017 zhang et al 2017 zeng et al 2019c zheng et al 2019a li et al 2021 shen et al 2021 zeng et al 2021d during the underground construction in water rich strata dewatering is normally performed inside a foundation pit surrounded by waterproof curtains to reduce groundwater level creating a dry working environment to avoid groundwater related disaster pujades et al 2014 wu et al 2016 wang et al 2017 wu et al 2017 wang et al 2018 zeng et al 2018 wang et al 2019 xu et al 2019 li et al 2020 wu et al 2020b zeng et al 2022 however if the waterproof curtains are incapable of cutting off the hydraulic connection between inside and outside of a foundation pit the dewatering will potentially incur the groundwater drawdown outside the pit further causing severe land subsidence chi and reilinger 1984 roy and robinson 2009 wang et al 2012 feng and zhan 2015 pujades et al 2016 figueroa miranda et al 2018 feng and zhan 2019 pan and fu 2020 wu et al 2020a lyu et al 2021 in particular when a foundation pit is closely adjacent to a pre existing deep buried structure the structure would play a role in blocking the groundwater flow called as water blocking effect making the dewatering induced water level distribution outside the pit different from that without underground structure pujades et al 2012 xu et al 2014 font capo et al 2015 attard et al 2016 de caro et al 2020 pujades et al 2012 pointed out that the subsurface structures penetrating any aquifers would reduce the water crossing section of the aquifers thus changing the groundwater flow pattern which would typically lead to a more significant water level drawdown at the outflow side of the structure i e the downgradient side concerning this issue pujades et al 2012 and paris et al 2010 quantified the water level distributions on both sides of an underground structure by analytical methods and numerical calculations respectively xu et al 2012 and ma et al 2014 numerically and experimentally investigated the impact of the water blocking effect of urban underground structures on land subsidence caused by pumping of groundwater they stressed that the pre existing underground structures significantly aggravate the water level decline and land subsidence on the downgradient side in addition to the water blocking effect the pre existing underground structure will also act as an underground barrier to directly hinder soil movement through soil structure interaction called as soil blocking effect liao et al 2016 numerically analyzed the soil blocking effect of a pre existing metro station on the response of soil to foundation pit excavation they stated that the pre existing station could restrict strata movements around the station leading to the reduction of land subsidence outside the pit since there were no water level drawdowns outside the pit in liao s simulation the reported results only reflected the influence of the soil blocking effect on strata movements indicating that this kind of barrier effect could inhibit the land subsidence the above studies prove that a pre existing underground structure would bring about both the water blocking effect and soil blocking effect to the environment technically the two barrier effects would cause opposite results the former one could aggravate the water level decline and land subsidence at the outflow side or downgradient side of the structure while the latter one would reduce the strata deformation or land subsidence around the underground structure the current studies only focus on either of the two barrier effects and do not investigate the combined actions of both the effects however in practical engineering because it is quite normal to find a foundation pit with hydraulic connection with surroundings the above two barrier effects would normally work simultaneously on this occasion the responses of groundwater and soil to foundation pit dewatering would depend on which barrier effect plays a predominant role however the current investigations have not given the answer of this issue the objective of this study is on one hand to reveal the behaviours of groundwater and soil during dewatering construction considering the combined actions of the two barrier effects and meanwhile to ascertain the conditions that should be met when either of the barrier effect play a leading role a common scenario that a foundation pit is dewatered in the vicinity of a pre existing metro station was modelled in a coupled hydro mechanical numerical model after validating the model by measured results from a field pumping test a parametric study was conducted by changing two typical parameters in the model i e the distance between the station and the foundation pit d and the dewatered depth inside the foundation pit h d to seek the behaviours of groundwater and soil during pumping under barrier effects of varying intensity the results of this study could provide insights to better understand the impact of barrier effects on the environment which would facilitate the future optimization design of dewatering 2 numerical analysis 2 1 problem setup fig 1 presents a sketch showing the typical positional relation among a foundation pit a pre existing metro station and strata in tianjin china zeng et al 2018 zeng et al 2019b based on a practical project i e the pumping test used for the model validation see section 2 2 3 the foundation pit was set 40 m wide and 155 m long only half of the pit was shown in fig 1 due to symmetry the retaining wall was set 0 8 m thick and 32 5 m deep a total of 24 dewatering wells with a length of 24 5 m were set inside the foundation pit the metro station was set 20 m wide and 19 m deep the retaining structure for constructing the station was set 35 m deep the dewatered depth inside the foundation pit h d and the distance between the station and the foundation pit d were set as two changing parameters in the numerical analysis table 1 summarizes the specified values for h d and d there are a total of 28 scenarios including those with no metro station outside the foundation pit i e d for comparisons this study focused on the foundation pit behaviour during dewatering construction or specifically during the pre excavation dewatering thus the soil bulk remained unexcavated during the simulation according to zeng et al 2021a and zeng et al 2021c only pumping of groundwater is able to incur considerable retaining wall movement and thus the first level of strut reinforced concrete was installed on the wall crown before the pumping construction which is also a typical practice in real construction the soil classification and aquifer distribution shown in fig 1 are from a practical project site i e the pumping test site used for the model validation see section 2 2 3 which are also the typical shallow strata conditions in tianjin zeng et al 2019c ha et al 2020 there were five aquifers aq0 aqiv and four aquitards adi adiv within 50 m below the ground level bgl the layers of silts and silty sands were regarded as aquifers and the layers of silty clays were treated as aquitards the initial water levels of the aquifers were 2 0 m 2 7 m 3 0 m 3 2 m and 3 7 m bgl from upper to lower layers the main parameters describing the soil properties obtained from site investigation and laboratory test are shown in fig 2 because the enclosure wall of the foundation pit did not totally cut off aqii the dewatering construction inside the pit was expected to incur groundwater drawdown in aqii outside the pit further causing drawdowns in aqi and aq0 due to groundwater leakage on this occasion the adjacent metro station would cause water blocking effect in aqii aqi and aq0 and meanwhile hinder soil movement around the station due to its soil blocking effect 2 2 model setup 2 2 1 general description of the modelling numerical modelling was conducted in the finite element package abaqus to analyze the barrier effect problem proposed in section 2 1 the model is able to reflect hydro mechanical coupling based on a series of fundamental theories e g darcy s law fluid mass balance theory and biot consolidation theory the detailed introductions regarding to the governing equations of the hydro mechanical coupling method can be seen in li et al 2020 the horizontal size of the model depends on the influence range of the pumping r supposing a maximum groundwater drawdown s w of 35 5 m in aqii sichardt s formula r 10 s w k gives r 615 m say the soil permeability k 3 m d bear 1979 for this the lateral boundary was set 800 m behind the enclosure wall of the foundation pit the vertical size of the model was set 50 m involving 12 soil layers as shown in fig 1 fig 3 presents the mesh of the model showing the dimensions of each part the symmetry property was used to minish the model size as shown in fig 3 the structures of pumping well enclosure wall first level of struts and metro station were modelled shell element was used to simulate pumping well beam element was used to model strut solid element was employed to model enclosure wall and metro station which were assumed to be impermeable the behaviours of all the structures were assumed to obey the elastic stress strain relation the elastic modulus of pumping well was set 210 gpa while that of the rest structures was set 30 gpa according to the design code china academy of building research 2010 the soil structure interaction was assumed to obey the coulomb s friction law and the coefficient of friction was set 0 3 according to zeng et al 2018 2 2 2 boundary condition and pumping simulation at the model bottom and lateral planes asymmetric ones soil movements in all directions were restrained while at the symmetric planes only the movement perpendicular to the plane was restrained in addition both the model bottom and the lateral plane symmetric one were applied an impervious boundary while at the asymmetric vertical surfaces a fixed water head boundary was applied to keep the initial head assumed at model surface unchanged throughout the pumping as to the simulation of pumping process considering the water level inside the pumping well would drop rapidly and then be kept unchanged until further operations e g termination of pumping or increase of pumping depth zeng et al 2021a a fixed water head boundary was set on the soil nodes contacting with the structure of the pumping well as illustrated in fig 4 for example as to the scenario with pumping depth of 10 m i e a water level decline of 10 m inside the pumping well zero water head boundary was applied on the soil well interface within 10 m and a constant head boundary was set on the soil well interface from 10 m to 24 5 m bottom of the pumping well in the model to build a hydrostatic pressure distribution in this depth range 2 2 3 soil parameters and model validation in most coastland in china e g tianjin the response of soil to water level change is commonly simulated by elastic model or mohr coulomb mc model wang et al 2019 zeng et al 2019a li et al 2020 zeng et al 2021b the reason is that the water level experienced a dramatic decline tens of meters in the past while rose substantially in recent decades owing to a series of policies to limit the groundwater withdrawal additionally recent field tests in tianjin demonstrated that the soils typically those with sand particles or the sandy soil basically deform elastically under general water level decline zheng et al 2019a zheng et al 2019b thus the mc constitutive model with a linear elastic perfectly plastic stress strain relation was used to simulate the response of soil in this study table 2 shows the main soil parameters used for the mc model most parameters were obtained from laboratory test while the elastic modulus e specific storage coefficient s s and hydraulic conductivity k h and k v were acquired from an inversion calculation on the basis of a practical pumping test the above mentioned pumping test was conducted inside a foundation pit the same pit as that described in section 2 1 after the enclosure wall and the struts at wall crown had been casted during the test a total of 22 pumping wells were opened to withdraw groundwater for about 77 h the rest unopened pumping wells were served as observation wells and a final water level decline of 15 m was found inside the foundation pit the groundwater drawdown and ground settlement outside the pit were also measured in the pumping test see figs 5 and 7 during the numerical inversion the measured results were finally fitted by the computed results based on which parameters e k h and k v were acquired the detailed inversion procedure can be referred to in zeng et al 2021b based on the obtained e one could calculate s s based on the equation 1 according to bear 1979 where γ w unit weight of water 1 s s γ w e fig 5 a shows the computed and measured time history curves of groundwater drawdown inside and outside the foundation pit the statistics of both the computed and measured steady drawdown are presented in fig 5 b the measured drawdowns inside the foundation pit were acquired from two unopened pumping wells j1 and j2 which were screened in the range of 0 24 5 m bgl the measured drawdowns outside the foundation pit were obtained from four observation wells g1 1 g1 2 g2 1 and g2 2 which were installed approximately 2 m from the foundation pit where g1 1 and g1 2 were screened through the aquifers aq0 and aqi i e 0 18 m bgl to monitor the water level change in aq0 and aqi and g2 1 and g2 2 were screened through aqii i e 24 30 m bgl to observe the water level change in aqii it shows that the computed drawdowns on both side of the pit are able to well match the measured ones especially for the steady drawdown calculation error of within 10 this demonstrates the good performance of the established model in simulating the groundwater seepage in soils one should note that a relatively large calculation error appears in the first few hours of pumping the reason should be the simplified pumping simulation method employed in this study overestimates the drawdown initially fig 6 shows the computed and measured time history curves of discharge rate of single pumping well the measured data are from shen et al 2015 who also conducted pumping tests in tianjin china the same city to this study the strata condition aquifer distribution dewatering well structure and operation of dewatering were similar in both shen s study and this study however the dewatering well in shen s study was 18 m deep a bit shallower than that in this study 24 5 m deep because the measured water level decline in pumping well of 13 m in shen s study is a bit smaller than that in this study approximately 14 m a parameter specific discharge rate q s w is defined as the ratio of the discharge rate of single pumping well q to the water level decline in the pumping well s w fig 6 compares the measured q s w in shen s study with the computed q s w in this study note in shen s study only the steady data of q s w was shown the computed steady q s w are close to the measured q s w in shen s study which however shows a slightly smaller result the reason might be the above mentioned difference in dewatering well length in both studies fig 7 a shows the computed and measured time history curves of ground surface settlement outside the foundation pit the statistics of both the computed and measured final surface settlement are presented in fig 7 b it shows that the computed settlements are able to reflect the general trend of the measured settlement development with time although an overestimation of the settlement occurs initially and a underestimation of settlement appears finally the cause of the calculation error basically within 30 should be the difference between the simplified model setup e g uniform soil layer thickness and soil properties and the complicated actual field conditions e g non uniform soil layer thickness and soil properties 3 analyses of barrier effect 3 1 groundwater drawdown taking the scenarios of h d 22 m as examples fig 8 presents the distributions of groundwater drawdown in aqi and aqii outside the foundation pit note the drawdown curves in aq0 are similar to those in aqi as to the cases with metro station outside the pit the drawdown curves in all the aquifers are divided into several sections due to the barrier effect of the metro station on these aquifers for the section in front of the metro station i e the section between the station and the foundation pit more obvious drawdown appears compared to the case without metro station while for the section behind the metro station relatively small drawdown exists this is the typical result of water blocking effect and the underlying working mechanism should be the groundwater flow path rotation and seepage path extension around the barrier in aquifers in addition with the increase of d i e the distance between foundation pit and metro station the maximum drawdown becomes smaller and the drawdown distribution tends to approach the condition without metro station indicating that the water blocking effect gets weakened as d increases one should note that the drawdowns under the base slab of the station as shown in fig 8 b are generally smaller than those without consideration of the station structure appearing this finding indicates that the current design theory would overestimate the water head decline under the metro station the more apparent as d reduces because the current theory could only calculate the dewatering induced water head change without underground barrier outside the pit 3 2 maximum groundwater drawdown and differential drawdown the above analyses indicate that the intensity of water blocking effect is related to the distance between the foundation pit and metro station d to better evaluate the water blocking effect wb effect in each aquifer an index the wb intensity factor η is defined as s 2 s 1 where s 2 and s 1 represent the maximum drawdown outside the pit considering and without considering the existence of metro station respectively η 1 means no wb effect η 1 means the wb effect takes effect among which greater η means more apparent wb effect fig 9 plots the variation of η with d under different h d in each aquifer it shows that η decreases nonlinearly as d increases in all the related aquifers in both aq0 and aqi the attenuation laws of η are almost the same while in aqii the attenuation of η is more apparent and faster especially when d is within 40 m this may be due to the difference in groundwater recharge condition in each aquifer e g the groundwater leakage from aqiii to aqii would help release the wb effect in aqii besides because the influence radius of water level decline in aquifer is usually large the wb effect would still take effect even though d is great however in general the wb effect has already been relatively weak when d reaches 100 m additionally the difference of drawdowns on both sides of the metro station δs is rather large see fig 8 which would cause the station structure to be subject to great asymmetric water pressure to better clarify the variation of δs under different pumping conditions δs was normalized as δs h d fig 10 presents the relationship between δs h d and d in each aquifer based on all the calculations it shows that δs h d is the largest in aqii followed by that in aqi and relatively small δs h d appears in aq0 in the same aquifer δs h d decreases with the increase of d the asymmetric water pressure on the station structure would worsen the loading condition of the metro station and aggravate the uneven deformation of the station structure 3 3 ground surface settlement taking the scenarios of h d 22 m as examples fig 11 presents the distributions of ground surface settlement under different d three main points can be obtained from fig 11 1 as to the cases with metro station outside the foundation pit the distribution of surface settlement is discontinuous which is divided into several sections by the metro station two apparent settlement troughs appear on both sides of the station different from the distribution pattern of single settlement trough for the case without metro station 2 as to the settlement trough in front of the metro station when d is small e g d 5 m and d 10 m the surface settlement is less apparent than that without station outside with the increase of d this part of settlement increases continuously and exceeds the maximum settlement of the case without station nearby as d is greater than 20 m however after d is over 40 m the maximum settlement gradually decreases and tends to approach the situation without station outside this deformation law is the result of the interaction between the water blocking wb effect and soil blocking sb effect of the adjacent metro station specifically when d is small the sb effect would play a leading role making the surface settlement smaller than that without station with the increase of d however both the sb and wb effects continuously get weakened and the sb effect weakens faster causing the surface settlement in front of the station to increase gradually when d is increased to a certain extent e g d 20 m the wb effect would in turn play a dominant role making the surface settlement between the station and foundation pit greater than that without station with the further increase of d i e larger than 40 m the sb effect would no longer work while the wb effect continues to weaken causing the surface settlement in front of the station to decrease and approach the condition without station outside 3 the settlement trough behind the metro station is caused by the tractive effect of the metro station when it moves towards the foundation pit during the dewatering in general the surface settlement behind the metro station is smaller than that without station outside but with the increase of d the settlement difference between the conditions with and without station gets smaller this deformation law is closely related to the groundwater response behind the station as indicated in section 3 1 3 4 maximum surface settlement the above analyses show that the dewatering induced maximum surface settlement for the case with metro station outside δ vm2 could either smaller or greater than that without station nearby δ vm1 depending on the distance between the station and the foundation pit d to further reveal the relationship between δ vm2 and δ vm1 under different d fig 12 plots the variation of δ vm2 δ vm1 with d regardless of the dewatered depth inside the foundation pit the relation between δ vm2 δ vm1 and d follows a uniform nonlinear variation when d is less than approximately 15 m δ vm2 δ vm1 increases rapidly with d but its value is always smaller than 1 indicating that the soil blocking effect of adjacent station plays a dominant role in such a condition and thus the dewatering induced soil settlements outside the foundation pit are restricted however as d increases the soil blocking effect weakens so rapidly that when d is greater than approximately 15 m the water blocking effect in turn plays a dominant role and thus δ vm2 δ vm1 becomes greater than 1 as d is greater than approximately 40 m δ vm2 δ vm1 gets smaller continuously and approaches to 1 indicating that the soil blocking effect no longer works while the water blocking effect continues to weaken and tends to be invalid in the design of foundation pit the influence of both the soil blocking and water blocking effects of adjacent underground structures should be considered the deformation analysis should be performed according to different positions of underground structures to realize the optimal design additionally as described in section 3 3 a settlement trough also appears behind the adjacent metro station by developing a mathematical relation between the maximum surface settlement behind the station δ vm3 and the maximum surface settlement in front of the station δ vm2 one can conveniently evaluate δ vm3 based on obtained δ vm2 note δ vm2 can be evaluated based on δ vm1 according to fig 12 and δ vm1 can be computed according to the traditional design method fig 13 shows the relationship between δ vm3 δ vm2 and d δ vm3 is smaller than δ vm2 regardless of the dewatered depth and the relation between δ vm3 δ vm2 and d follows a uniform nonlinear variation when d is less than approximately 15 m δ vm3 δ vm2 reduces sharply with the increase of d this is because the rapid weakening of soil blocking effect leads to the fast increase of δ vm2 when d is greater than approximately 15 m because both the soil blocking and water blocking effects are tending to become weak at a similar rate although the water blocking effect plays a dominant role at this time both the changes of δ vm3 and δ vm2 are relatively small and thus the variation of δ vm3 δ vm2 with d is not apparent 3 5 position of the maximum surface settlement the description in section 3 3 indicates that the distance between the maximum settlement position and the enclosure wall l vm1 varies with the change of d see fig 11 to make clear the variation of l vm1 with d fig 14 plots the relation between l vm1 and d regardless of the dewatered depth inside the foundation pit the relation between l vm1 and d follows a uniform nonlinear variation a unified mathematical equation could be used to represent the relation of l vm1 with d facilitating the approximation of l vm1 under different d additionally l vm1 gets greater with the increase of d however when d is greater than 40 m l vm1 is basically equal to that under dewatering without adjacent metro station although a small fluctuation of l vm1 appears this indicates that both the soil blocking and water blocking effects on this occasion are rather weak likewise another parameter l vm2 is defined to represent the distance between the maximum settlement position behind the metro station and the metro station fig 15 shows the relation between l vm2 and d regardless of the dewatered depth inside the foundation pit l vm2 basically increases linearly with d and a unified mathematical equation could also be used to represent the relation of l vm2 with d facilitating the approximation of l vm2 under different d 4 concluding remarks in this study a series of hydro mechanical numerical simulations were conducted to reveal the responses of groundwater and surrounding soil to foundation pit dewatering considering the barrier effect of pre existing metro station on aquifers two types of barrier effect i e water blocking wb and soil blocking sb effects were found to work simultaneously and the interaction between them were clarified based on this study the following main conclusions can be drawn 1 the wb effect could induce more obvious groundwater drawdown in front of the metro station and relatively small drawdown behind the station which further causes the station structure to be subject to great asymmetric water pressure worsening the loading condition of the station and possibly aggravating the uneven deformation of the station structure however the wb effect would weaken rapidly as d distance from station to foundation pit increases in an aquifer with better groundwater recharge condition the reduction of wb effect with d is rather fast in general the wb effect is quite weak when d reaches 100 m 2 the wb and sb effects would induce opposite soil response either aggravating or restricting the dewatering induced soil settlement under the combined actions of the two barrier effects the soil responses depend on which effect would play a dominant role when d is small e g smaller than 15 m in this study the sb effect plays a leading role restricting the dewatering induced soil settlement as d increases both the sb and wb effects get weakened while sb effect weakens more rapidly when d surpasses a critical value e g larger than 15 m the wb effect in turn plays a dominant role aggravating the dewatering induced soil settlement 3 the wb and sb effects could synthetically limit the dewatering induced lateral movement of the enclosure wall when d is around 2h d the maximum enclosure wall deflection could be reduced by up to 10 in the design of foundation pit with adjacent structures the considering of barrier effect would yield a more reasonable scheme of retaining structure 4 several empirical relations were established to approximate the dewatering induced maximum surface settlement 0 043 h d 0 085 h d the maximum enclosure wall deflection 0 023 h d to 0 06 h d and the maximum settlement positions on both side of the pre existing metro station the relation between the maximum surface settlements for the cases with and without the metro station outside was also developed facilitating the evaluation of the soil response to dewatering under barrier effect based on the soil response without barrier effect finally apart from the distance from the pit the insertion ratio of the station structure into the aquifer and the hydraulic conductivity of the aquifers or hydraulic connection among aquifers are also the importing parameters affecting the intensity of barrier effect further investigations could be conducted to focus on those issues additionally all the results of this study are on the basis of the strata with soft to stiff silty clays silts and silty sands alternately appearing in the site thus the obtained results especially the developed empirical relations could be used to evaluate the barrier effect under similar strata condition whereas the applicability of the results to analyze the barrier effect in other strata e g sandy gravel needs to be further verified declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was supported by the national natural science foundation of china grant numbers 51978261 51708206 and 41877283 the natural science foundation of hunan province grant number 2022jj20023 the research foundation of education bureau of hunan province grant numbers 21a0290 and 20a190 and the science and technology innovation program of hunan province grant number 2021rc4037 finally we deeply appreciate for the warm and efficient work by editors and reviewers 
3221,in urban environment there exist numerous pre existing underground structures which act as underground barriers to block the groundwater flow and soil movement i e water blocking wb and soil blocking sb effects making the responses of groundwater and soil to dewatering different from that without nearby structure in this study dewatering inside a foundation pit with a nearby metro station was simulated in a coupled hydro mechanical numerical model after validating the model by measured results from a field pumping test extensive numerical simulations were conducted to investigate the behaviours of groundwater and soil during dewatering under wb and sb effects results show that the two effects would induce opposite soil response either aggravating or restricting the dewatering induced soil settlement under the combined actions of the two barrier effects the soil responses depend on which effect would play a dominant role when the distance between the metro station and foundation pit d is small the sb effect plays a leading role restricting the dewatering induced soil settlement as d increases both the sb and wb effects get weakened while sb effect weakens more rapidly when d surpasses a critical value the wb effect in turn plays a dominant role aggravating the dewatering induced soil settlement keywords dewatering groundwater drawdown ground settlement water blocking effect soil blocking effect data availability data will be made available on request nomenclature c effective cohesion d distance between the station and the foundation pit e elastic modulus of soil e 0 initial void ratio h d dewatered depth inside the foundation pit k soil permeability k h horizontal hydraulic conductivity k v vertical hydraulic conductivity l 0 distance of influence l vm1 distance of the maximum settlement position to the excavation l vm2 distance of the maximum settlement position behind the metro station to the metro station q discharge rate of single pumping well r influence range of the pumping s w groundwater drawdown s s specific storage coefficient s 1 maximum drawdown outside excavation without metro station nearby s 2 maximum drawdown outside excavation with metro station nearby γ unit weight γ w unit weight of water η water blocking intensity factor δs difference of drawdowns on both sides of metro station δ hm maximum enclosure wall deflection δ hm1 maximum wall deflection for the case without metro station outside δ hm2 maximum wall deflection for the case with metro station outside δ vm maximum surface settlement δ vm1 maximum surface settlement for the case without metro station outside δ vm2 maximum surface settlement for the case with metro station outside δ vm3 maximum surface settlement behind metro station φ effective friction angle 1 introduction with the rapid urbanization in many regions a huge demand exists for the construction of underground infrastructures ni et al 2011 attard et al 2017 pujades et al 2017 zhang et al 2017 zeng et al 2019c zheng et al 2019a li et al 2021 shen et al 2021 zeng et al 2021d during the underground construction in water rich strata dewatering is normally performed inside a foundation pit surrounded by waterproof curtains to reduce groundwater level creating a dry working environment to avoid groundwater related disaster pujades et al 2014 wu et al 2016 wang et al 2017 wu et al 2017 wang et al 2018 zeng et al 2018 wang et al 2019 xu et al 2019 li et al 2020 wu et al 2020b zeng et al 2022 however if the waterproof curtains are incapable of cutting off the hydraulic connection between inside and outside of a foundation pit the dewatering will potentially incur the groundwater drawdown outside the pit further causing severe land subsidence chi and reilinger 1984 roy and robinson 2009 wang et al 2012 feng and zhan 2015 pujades et al 2016 figueroa miranda et al 2018 feng and zhan 2019 pan and fu 2020 wu et al 2020a lyu et al 2021 in particular when a foundation pit is closely adjacent to a pre existing deep buried structure the structure would play a role in blocking the groundwater flow called as water blocking effect making the dewatering induced water level distribution outside the pit different from that without underground structure pujades et al 2012 xu et al 2014 font capo et al 2015 attard et al 2016 de caro et al 2020 pujades et al 2012 pointed out that the subsurface structures penetrating any aquifers would reduce the water crossing section of the aquifers thus changing the groundwater flow pattern which would typically lead to a more significant water level drawdown at the outflow side of the structure i e the downgradient side concerning this issue pujades et al 2012 and paris et al 2010 quantified the water level distributions on both sides of an underground structure by analytical methods and numerical calculations respectively xu et al 2012 and ma et al 2014 numerically and experimentally investigated the impact of the water blocking effect of urban underground structures on land subsidence caused by pumping of groundwater they stressed that the pre existing underground structures significantly aggravate the water level decline and land subsidence on the downgradient side in addition to the water blocking effect the pre existing underground structure will also act as an underground barrier to directly hinder soil movement through soil structure interaction called as soil blocking effect liao et al 2016 numerically analyzed the soil blocking effect of a pre existing metro station on the response of soil to foundation pit excavation they stated that the pre existing station could restrict strata movements around the station leading to the reduction of land subsidence outside the pit since there were no water level drawdowns outside the pit in liao s simulation the reported results only reflected the influence of the soil blocking effect on strata movements indicating that this kind of barrier effect could inhibit the land subsidence the above studies prove that a pre existing underground structure would bring about both the water blocking effect and soil blocking effect to the environment technically the two barrier effects would cause opposite results the former one could aggravate the water level decline and land subsidence at the outflow side or downgradient side of the structure while the latter one would reduce the strata deformation or land subsidence around the underground structure the current studies only focus on either of the two barrier effects and do not investigate the combined actions of both the effects however in practical engineering because it is quite normal to find a foundation pit with hydraulic connection with surroundings the above two barrier effects would normally work simultaneously on this occasion the responses of groundwater and soil to foundation pit dewatering would depend on which barrier effect plays a predominant role however the current investigations have not given the answer of this issue the objective of this study is on one hand to reveal the behaviours of groundwater and soil during dewatering construction considering the combined actions of the two barrier effects and meanwhile to ascertain the conditions that should be met when either of the barrier effect play a leading role a common scenario that a foundation pit is dewatered in the vicinity of a pre existing metro station was modelled in a coupled hydro mechanical numerical model after validating the model by measured results from a field pumping test a parametric study was conducted by changing two typical parameters in the model i e the distance between the station and the foundation pit d and the dewatered depth inside the foundation pit h d to seek the behaviours of groundwater and soil during pumping under barrier effects of varying intensity the results of this study could provide insights to better understand the impact of barrier effects on the environment which would facilitate the future optimization design of dewatering 2 numerical analysis 2 1 problem setup fig 1 presents a sketch showing the typical positional relation among a foundation pit a pre existing metro station and strata in tianjin china zeng et al 2018 zeng et al 2019b based on a practical project i e the pumping test used for the model validation see section 2 2 3 the foundation pit was set 40 m wide and 155 m long only half of the pit was shown in fig 1 due to symmetry the retaining wall was set 0 8 m thick and 32 5 m deep a total of 24 dewatering wells with a length of 24 5 m were set inside the foundation pit the metro station was set 20 m wide and 19 m deep the retaining structure for constructing the station was set 35 m deep the dewatered depth inside the foundation pit h d and the distance between the station and the foundation pit d were set as two changing parameters in the numerical analysis table 1 summarizes the specified values for h d and d there are a total of 28 scenarios including those with no metro station outside the foundation pit i e d for comparisons this study focused on the foundation pit behaviour during dewatering construction or specifically during the pre excavation dewatering thus the soil bulk remained unexcavated during the simulation according to zeng et al 2021a and zeng et al 2021c only pumping of groundwater is able to incur considerable retaining wall movement and thus the first level of strut reinforced concrete was installed on the wall crown before the pumping construction which is also a typical practice in real construction the soil classification and aquifer distribution shown in fig 1 are from a practical project site i e the pumping test site used for the model validation see section 2 2 3 which are also the typical shallow strata conditions in tianjin zeng et al 2019c ha et al 2020 there were five aquifers aq0 aqiv and four aquitards adi adiv within 50 m below the ground level bgl the layers of silts and silty sands were regarded as aquifers and the layers of silty clays were treated as aquitards the initial water levels of the aquifers were 2 0 m 2 7 m 3 0 m 3 2 m and 3 7 m bgl from upper to lower layers the main parameters describing the soil properties obtained from site investigation and laboratory test are shown in fig 2 because the enclosure wall of the foundation pit did not totally cut off aqii the dewatering construction inside the pit was expected to incur groundwater drawdown in aqii outside the pit further causing drawdowns in aqi and aq0 due to groundwater leakage on this occasion the adjacent metro station would cause water blocking effect in aqii aqi and aq0 and meanwhile hinder soil movement around the station due to its soil blocking effect 2 2 model setup 2 2 1 general description of the modelling numerical modelling was conducted in the finite element package abaqus to analyze the barrier effect problem proposed in section 2 1 the model is able to reflect hydro mechanical coupling based on a series of fundamental theories e g darcy s law fluid mass balance theory and biot consolidation theory the detailed introductions regarding to the governing equations of the hydro mechanical coupling method can be seen in li et al 2020 the horizontal size of the model depends on the influence range of the pumping r supposing a maximum groundwater drawdown s w of 35 5 m in aqii sichardt s formula r 10 s w k gives r 615 m say the soil permeability k 3 m d bear 1979 for this the lateral boundary was set 800 m behind the enclosure wall of the foundation pit the vertical size of the model was set 50 m involving 12 soil layers as shown in fig 1 fig 3 presents the mesh of the model showing the dimensions of each part the symmetry property was used to minish the model size as shown in fig 3 the structures of pumping well enclosure wall first level of struts and metro station were modelled shell element was used to simulate pumping well beam element was used to model strut solid element was employed to model enclosure wall and metro station which were assumed to be impermeable the behaviours of all the structures were assumed to obey the elastic stress strain relation the elastic modulus of pumping well was set 210 gpa while that of the rest structures was set 30 gpa according to the design code china academy of building research 2010 the soil structure interaction was assumed to obey the coulomb s friction law and the coefficient of friction was set 0 3 according to zeng et al 2018 2 2 2 boundary condition and pumping simulation at the model bottom and lateral planes asymmetric ones soil movements in all directions were restrained while at the symmetric planes only the movement perpendicular to the plane was restrained in addition both the model bottom and the lateral plane symmetric one were applied an impervious boundary while at the asymmetric vertical surfaces a fixed water head boundary was applied to keep the initial head assumed at model surface unchanged throughout the pumping as to the simulation of pumping process considering the water level inside the pumping well would drop rapidly and then be kept unchanged until further operations e g termination of pumping or increase of pumping depth zeng et al 2021a a fixed water head boundary was set on the soil nodes contacting with the structure of the pumping well as illustrated in fig 4 for example as to the scenario with pumping depth of 10 m i e a water level decline of 10 m inside the pumping well zero water head boundary was applied on the soil well interface within 10 m and a constant head boundary was set on the soil well interface from 10 m to 24 5 m bottom of the pumping well in the model to build a hydrostatic pressure distribution in this depth range 2 2 3 soil parameters and model validation in most coastland in china e g tianjin the response of soil to water level change is commonly simulated by elastic model or mohr coulomb mc model wang et al 2019 zeng et al 2019a li et al 2020 zeng et al 2021b the reason is that the water level experienced a dramatic decline tens of meters in the past while rose substantially in recent decades owing to a series of policies to limit the groundwater withdrawal additionally recent field tests in tianjin demonstrated that the soils typically those with sand particles or the sandy soil basically deform elastically under general water level decline zheng et al 2019a zheng et al 2019b thus the mc constitutive model with a linear elastic perfectly plastic stress strain relation was used to simulate the response of soil in this study table 2 shows the main soil parameters used for the mc model most parameters were obtained from laboratory test while the elastic modulus e specific storage coefficient s s and hydraulic conductivity k h and k v were acquired from an inversion calculation on the basis of a practical pumping test the above mentioned pumping test was conducted inside a foundation pit the same pit as that described in section 2 1 after the enclosure wall and the struts at wall crown had been casted during the test a total of 22 pumping wells were opened to withdraw groundwater for about 77 h the rest unopened pumping wells were served as observation wells and a final water level decline of 15 m was found inside the foundation pit the groundwater drawdown and ground settlement outside the pit were also measured in the pumping test see figs 5 and 7 during the numerical inversion the measured results were finally fitted by the computed results based on which parameters e k h and k v were acquired the detailed inversion procedure can be referred to in zeng et al 2021b based on the obtained e one could calculate s s based on the equation 1 according to bear 1979 where γ w unit weight of water 1 s s γ w e fig 5 a shows the computed and measured time history curves of groundwater drawdown inside and outside the foundation pit the statistics of both the computed and measured steady drawdown are presented in fig 5 b the measured drawdowns inside the foundation pit were acquired from two unopened pumping wells j1 and j2 which were screened in the range of 0 24 5 m bgl the measured drawdowns outside the foundation pit were obtained from four observation wells g1 1 g1 2 g2 1 and g2 2 which were installed approximately 2 m from the foundation pit where g1 1 and g1 2 were screened through the aquifers aq0 and aqi i e 0 18 m bgl to monitor the water level change in aq0 and aqi and g2 1 and g2 2 were screened through aqii i e 24 30 m bgl to observe the water level change in aqii it shows that the computed drawdowns on both side of the pit are able to well match the measured ones especially for the steady drawdown calculation error of within 10 this demonstrates the good performance of the established model in simulating the groundwater seepage in soils one should note that a relatively large calculation error appears in the first few hours of pumping the reason should be the simplified pumping simulation method employed in this study overestimates the drawdown initially fig 6 shows the computed and measured time history curves of discharge rate of single pumping well the measured data are from shen et al 2015 who also conducted pumping tests in tianjin china the same city to this study the strata condition aquifer distribution dewatering well structure and operation of dewatering were similar in both shen s study and this study however the dewatering well in shen s study was 18 m deep a bit shallower than that in this study 24 5 m deep because the measured water level decline in pumping well of 13 m in shen s study is a bit smaller than that in this study approximately 14 m a parameter specific discharge rate q s w is defined as the ratio of the discharge rate of single pumping well q to the water level decline in the pumping well s w fig 6 compares the measured q s w in shen s study with the computed q s w in this study note in shen s study only the steady data of q s w was shown the computed steady q s w are close to the measured q s w in shen s study which however shows a slightly smaller result the reason might be the above mentioned difference in dewatering well length in both studies fig 7 a shows the computed and measured time history curves of ground surface settlement outside the foundation pit the statistics of both the computed and measured final surface settlement are presented in fig 7 b it shows that the computed settlements are able to reflect the general trend of the measured settlement development with time although an overestimation of the settlement occurs initially and a underestimation of settlement appears finally the cause of the calculation error basically within 30 should be the difference between the simplified model setup e g uniform soil layer thickness and soil properties and the complicated actual field conditions e g non uniform soil layer thickness and soil properties 3 analyses of barrier effect 3 1 groundwater drawdown taking the scenarios of h d 22 m as examples fig 8 presents the distributions of groundwater drawdown in aqi and aqii outside the foundation pit note the drawdown curves in aq0 are similar to those in aqi as to the cases with metro station outside the pit the drawdown curves in all the aquifers are divided into several sections due to the barrier effect of the metro station on these aquifers for the section in front of the metro station i e the section between the station and the foundation pit more obvious drawdown appears compared to the case without metro station while for the section behind the metro station relatively small drawdown exists this is the typical result of water blocking effect and the underlying working mechanism should be the groundwater flow path rotation and seepage path extension around the barrier in aquifers in addition with the increase of d i e the distance between foundation pit and metro station the maximum drawdown becomes smaller and the drawdown distribution tends to approach the condition without metro station indicating that the water blocking effect gets weakened as d increases one should note that the drawdowns under the base slab of the station as shown in fig 8 b are generally smaller than those without consideration of the station structure appearing this finding indicates that the current design theory would overestimate the water head decline under the metro station the more apparent as d reduces because the current theory could only calculate the dewatering induced water head change without underground barrier outside the pit 3 2 maximum groundwater drawdown and differential drawdown the above analyses indicate that the intensity of water blocking effect is related to the distance between the foundation pit and metro station d to better evaluate the water blocking effect wb effect in each aquifer an index the wb intensity factor η is defined as s 2 s 1 where s 2 and s 1 represent the maximum drawdown outside the pit considering and without considering the existence of metro station respectively η 1 means no wb effect η 1 means the wb effect takes effect among which greater η means more apparent wb effect fig 9 plots the variation of η with d under different h d in each aquifer it shows that η decreases nonlinearly as d increases in all the related aquifers in both aq0 and aqi the attenuation laws of η are almost the same while in aqii the attenuation of η is more apparent and faster especially when d is within 40 m this may be due to the difference in groundwater recharge condition in each aquifer e g the groundwater leakage from aqiii to aqii would help release the wb effect in aqii besides because the influence radius of water level decline in aquifer is usually large the wb effect would still take effect even though d is great however in general the wb effect has already been relatively weak when d reaches 100 m additionally the difference of drawdowns on both sides of the metro station δs is rather large see fig 8 which would cause the station structure to be subject to great asymmetric water pressure to better clarify the variation of δs under different pumping conditions δs was normalized as δs h d fig 10 presents the relationship between δs h d and d in each aquifer based on all the calculations it shows that δs h d is the largest in aqii followed by that in aqi and relatively small δs h d appears in aq0 in the same aquifer δs h d decreases with the increase of d the asymmetric water pressure on the station structure would worsen the loading condition of the metro station and aggravate the uneven deformation of the station structure 3 3 ground surface settlement taking the scenarios of h d 22 m as examples fig 11 presents the distributions of ground surface settlement under different d three main points can be obtained from fig 11 1 as to the cases with metro station outside the foundation pit the distribution of surface settlement is discontinuous which is divided into several sections by the metro station two apparent settlement troughs appear on both sides of the station different from the distribution pattern of single settlement trough for the case without metro station 2 as to the settlement trough in front of the metro station when d is small e g d 5 m and d 10 m the surface settlement is less apparent than that without station outside with the increase of d this part of settlement increases continuously and exceeds the maximum settlement of the case without station nearby as d is greater than 20 m however after d is over 40 m the maximum settlement gradually decreases and tends to approach the situation without station outside this deformation law is the result of the interaction between the water blocking wb effect and soil blocking sb effect of the adjacent metro station specifically when d is small the sb effect would play a leading role making the surface settlement smaller than that without station with the increase of d however both the sb and wb effects continuously get weakened and the sb effect weakens faster causing the surface settlement in front of the station to increase gradually when d is increased to a certain extent e g d 20 m the wb effect would in turn play a dominant role making the surface settlement between the station and foundation pit greater than that without station with the further increase of d i e larger than 40 m the sb effect would no longer work while the wb effect continues to weaken causing the surface settlement in front of the station to decrease and approach the condition without station outside 3 the settlement trough behind the metro station is caused by the tractive effect of the metro station when it moves towards the foundation pit during the dewatering in general the surface settlement behind the metro station is smaller than that without station outside but with the increase of d the settlement difference between the conditions with and without station gets smaller this deformation law is closely related to the groundwater response behind the station as indicated in section 3 1 3 4 maximum surface settlement the above analyses show that the dewatering induced maximum surface settlement for the case with metro station outside δ vm2 could either smaller or greater than that without station nearby δ vm1 depending on the distance between the station and the foundation pit d to further reveal the relationship between δ vm2 and δ vm1 under different d fig 12 plots the variation of δ vm2 δ vm1 with d regardless of the dewatered depth inside the foundation pit the relation between δ vm2 δ vm1 and d follows a uniform nonlinear variation when d is less than approximately 15 m δ vm2 δ vm1 increases rapidly with d but its value is always smaller than 1 indicating that the soil blocking effect of adjacent station plays a dominant role in such a condition and thus the dewatering induced soil settlements outside the foundation pit are restricted however as d increases the soil blocking effect weakens so rapidly that when d is greater than approximately 15 m the water blocking effect in turn plays a dominant role and thus δ vm2 δ vm1 becomes greater than 1 as d is greater than approximately 40 m δ vm2 δ vm1 gets smaller continuously and approaches to 1 indicating that the soil blocking effect no longer works while the water blocking effect continues to weaken and tends to be invalid in the design of foundation pit the influence of both the soil blocking and water blocking effects of adjacent underground structures should be considered the deformation analysis should be performed according to different positions of underground structures to realize the optimal design additionally as described in section 3 3 a settlement trough also appears behind the adjacent metro station by developing a mathematical relation between the maximum surface settlement behind the station δ vm3 and the maximum surface settlement in front of the station δ vm2 one can conveniently evaluate δ vm3 based on obtained δ vm2 note δ vm2 can be evaluated based on δ vm1 according to fig 12 and δ vm1 can be computed according to the traditional design method fig 13 shows the relationship between δ vm3 δ vm2 and d δ vm3 is smaller than δ vm2 regardless of the dewatered depth and the relation between δ vm3 δ vm2 and d follows a uniform nonlinear variation when d is less than approximately 15 m δ vm3 δ vm2 reduces sharply with the increase of d this is because the rapid weakening of soil blocking effect leads to the fast increase of δ vm2 when d is greater than approximately 15 m because both the soil blocking and water blocking effects are tending to become weak at a similar rate although the water blocking effect plays a dominant role at this time both the changes of δ vm3 and δ vm2 are relatively small and thus the variation of δ vm3 δ vm2 with d is not apparent 3 5 position of the maximum surface settlement the description in section 3 3 indicates that the distance between the maximum settlement position and the enclosure wall l vm1 varies with the change of d see fig 11 to make clear the variation of l vm1 with d fig 14 plots the relation between l vm1 and d regardless of the dewatered depth inside the foundation pit the relation between l vm1 and d follows a uniform nonlinear variation a unified mathematical equation could be used to represent the relation of l vm1 with d facilitating the approximation of l vm1 under different d additionally l vm1 gets greater with the increase of d however when d is greater than 40 m l vm1 is basically equal to that under dewatering without adjacent metro station although a small fluctuation of l vm1 appears this indicates that both the soil blocking and water blocking effects on this occasion are rather weak likewise another parameter l vm2 is defined to represent the distance between the maximum settlement position behind the metro station and the metro station fig 15 shows the relation between l vm2 and d regardless of the dewatered depth inside the foundation pit l vm2 basically increases linearly with d and a unified mathematical equation could also be used to represent the relation of l vm2 with d facilitating the approximation of l vm2 under different d 4 concluding remarks in this study a series of hydro mechanical numerical simulations were conducted to reveal the responses of groundwater and surrounding soil to foundation pit dewatering considering the barrier effect of pre existing metro station on aquifers two types of barrier effect i e water blocking wb and soil blocking sb effects were found to work simultaneously and the interaction between them were clarified based on this study the following main conclusions can be drawn 1 the wb effect could induce more obvious groundwater drawdown in front of the metro station and relatively small drawdown behind the station which further causes the station structure to be subject to great asymmetric water pressure worsening the loading condition of the station and possibly aggravating the uneven deformation of the station structure however the wb effect would weaken rapidly as d distance from station to foundation pit increases in an aquifer with better groundwater recharge condition the reduction of wb effect with d is rather fast in general the wb effect is quite weak when d reaches 100 m 2 the wb and sb effects would induce opposite soil response either aggravating or restricting the dewatering induced soil settlement under the combined actions of the two barrier effects the soil responses depend on which effect would play a dominant role when d is small e g smaller than 15 m in this study the sb effect plays a leading role restricting the dewatering induced soil settlement as d increases both the sb and wb effects get weakened while sb effect weakens more rapidly when d surpasses a critical value e g larger than 15 m the wb effect in turn plays a dominant role aggravating the dewatering induced soil settlement 3 the wb and sb effects could synthetically limit the dewatering induced lateral movement of the enclosure wall when d is around 2h d the maximum enclosure wall deflection could be reduced by up to 10 in the design of foundation pit with adjacent structures the considering of barrier effect would yield a more reasonable scheme of retaining structure 4 several empirical relations were established to approximate the dewatering induced maximum surface settlement 0 043 h d 0 085 h d the maximum enclosure wall deflection 0 023 h d to 0 06 h d and the maximum settlement positions on both side of the pre existing metro station the relation between the maximum surface settlements for the cases with and without the metro station outside was also developed facilitating the evaluation of the soil response to dewatering under barrier effect based on the soil response without barrier effect finally apart from the distance from the pit the insertion ratio of the station structure into the aquifer and the hydraulic conductivity of the aquifers or hydraulic connection among aquifers are also the importing parameters affecting the intensity of barrier effect further investigations could be conducted to focus on those issues additionally all the results of this study are on the basis of the strata with soft to stiff silty clays silts and silty sands alternately appearing in the site thus the obtained results especially the developed empirical relations could be used to evaluate the barrier effect under similar strata condition whereas the applicability of the results to analyze the barrier effect in other strata e g sandy gravel needs to be further verified declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was supported by the national natural science foundation of china grant numbers 51978261 51708206 and 41877283 the natural science foundation of hunan province grant number 2022jj20023 the research foundation of education bureau of hunan province grant numbers 21a0290 and 20a190 and the science and technology innovation program of hunan province grant number 2021rc4037 finally we deeply appreciate for the warm and efficient work by editors and reviewers 
3222,obtaining accurate point estimates and reliable interval prediction results for rainfall and runoff series is important to aid in water resource decision making and planning management in a changing environment in this paper we propose a two stage hybrid model p cvee ldbnn the model uses the complete ensemble empirical mode decomposition with adaptive noise ceemdan and variational mode decomposition vmd methods for data preprocessing and a bayesian neural network ldbnn based on langevin dynamics as a prediction model then a new dataset partitioning method based on a particle swarm optimization algorithm which is different from the traditional dataset partitioning method is applied a one stage hybrid model t cee ldbnn that integrates the ceemdan method and the traditional method of partitioning datasets a one stage hybrid model p cee ldbnn that integrates the ceemdan method and the new method of partitioning datasets and the two stage hybrid p cvee ldbnn model were compared these models were applied to monthly runoff and monthly precipitation series from seven hydrological and meteorological stations in the yellow river basin the mean absolute error mae absolute root mean squared error rmse nash sutcliffe efficiency nse and correlation coefficient r were used to evaluate the predictive ability of the models the containment ratio cr the average bandwidth iw and the average asymmetry degree s were used to evaluate the interval prediction results of the models the results show that 1 the subset obtained with the new method of data partitioning considering statistical properties is more favorable for model prediction 2 the second decomposition approach based on vmd is a reliable method that can significantly improve the prediction accuracy of the final model 3 compared with traditional neural networks that can only obtain deterministic point prediction results bayesian methods can provide intervals with prediction results making the results of the two stage hybrid p cvee ldbnn model highly reliable keywords two stage hybrid model langevin dynamics bayesian neural networks data portioning particle swarm optimization algorithm yellow river basin data availability the data that has been used is confidential 1 introduction influenced by human activities including the construction of dams excessive reclamation and changing of land use and pollution of water and climate change rainfall and runoff series are characterized by high degrees of nonlinearity and complexity and different hydrological series exhibit considerable differences javadinejad et al 2021 eslamian et al 2018 fatahi nafchi et al 2021 kaveh et al 2017 these differences increase the difficulty of accurately forecasting rainfall and runoff series the deterministic point predictions obtained by hydrological models are often nonideal for water resource planning and risk assessments if they are underestimated or overestimated compared with the observed flows gao et al 2020b rational estimates of rainfall and runoff prediction intervals can provide a basis for optimal water resource allocation and hydrological forecasting decisions khan and coulibaly 2006 liu and gupta 2007 therefore one objective is to obtain improved point forecasting results and provide a reliable interval forecast in the context of the changing environment unlike physically driven models that need to consider the underlying physical processes in forecasting fang et al 2017 romagnoli et al 2017 wu and lin 2017 machine learning and deep learning models can directly capture the complex nonlinear relationships between input and output variables gao et al 2020b jeong and park 2019 lin et al 2009 ostad ali askari and shayan 2021 wu and chau 2011 zhang et al 2021 there are studies that have proven machine learning and deep learning models to be effective tools in hydrological studies chang et al 2014 kao et al 2021 kao et al 2020 zhou et al 2019 in particular anns are reliable hydrological modeling tools that are widely used for rainfall and runoff modeling jain and srinivasulu 2004 johny et al 2020 rajurkar et al 2002 anns include the input layer hidden layer and output layer ostad ali askari et al 2017 ostad ali askari and shayannejad 2021 however traditional anns have the following two main limitations 1 anns update parameters by error backpropagation and gradient descent methods which provide good fits for training period data well and may produce relatively large errors in predictions based on new data and 2 only deterministic point prediction results can be obtained chandra et al 2017 the main differences between neural networks based on bayesian inference methods and traditional neural networks are as follows neal 1996 first the bayesian approach treats the parameters network weights as random variables that obey a certain probability distribution rather than unknown constants notably the markov chain monte carlo mcmc method can capture parameter uncertainty welling and teh 2011 according to bayes theorem the posterior distribution of the parameters is obtained from the prior distribution of the parameters and the likelihood function the posterior distribution of the parameters contains information about the parameters and provides reliable estimation results second the predicted outcome is obtained from a probability distribution bayesian point prediction results are obtained based on the mean of the predicted distribution bayesian methods consider the uncertainty of parameters network weights and model outputs which can mitigate overfitting li et al 2016 additionally bayesian neural networks outperform traditional anns in terms of prediction accuracy kingston et al 2005 incorporating the uncertainty of parameters into forecasts can improve the point forecasting results kasiviswanathan et al 2013 and yield reliable interval forecasts khan and coulibaly 2006 bayesian neural networks have been widely used for hydrological modeling li et al 2021 zhan et al 2020 zhang et al 2011 the mcmc sampling method can provide estimates of the posterior distribution of data gamerman and lopes 2006 however the application of bayesian methods in neural networks is impeded by the limitations of mcmc methods in terms of convergence and scalability chandra et al 2019 in previous studies a class of mcmc techniques based on langevin dynamics was considered to solve this problem these methods combine gradient concepts and gaussian noise in the parameter updating scheme thus allowing the markov chain to converge to the full posterior distribution neal 2011 welling and teh 2011 chandra et al 2017 applied a method based on langevin dynamics and a neural network for chaotic time series prediction and showed that the method improved most of the problems of the mcmc random walk algorithm however due to the highly nonstationary and complex nature of hydrological series in changing environments individual models often fail to adequately predict rainfall and runoff series when meteorological observations are unavailable zuo et al 2020 the empirical mode decomposition emd huang et al 1998 ensemble empirical mode decomposition eemd wu and huang 2009 and complete ensemble empirical mode decomposition with adaptive noise ceemdan maría et al 2011 methods were developed to address nonstationary time series for enhanced analysis and forecasting eemd was proposed to solve the mode mixing problem that occurs in emd however the reconstructed signal contains residual noise that may generate new modes wen et al 2019 the ceemdan method as a variation of the eemd algorithm can provide a more accurate construction result than prior methods maría et al 2011 hybrid models based on ceemdan decomposition are also widely used for forecasting studies in several fields lin et al 2021 integrated the ceemdan approach with long short term memory lstm for stock index prediction gao et al 2020a integrated the ceemdan method with a cnn lstm for hourly solar irradiance forecasting and wang et al 2021 integrated the ceemdan method with a gated recurrent unit network gru for forecasting natural gas prices however to our knowledge no previous studies have used the ceemdan method coupled with a bayesian neural network based on langevin dynamics to predict rainfall and runoff series guo et al 2012 and huang et al 2014 noted that high frequency e g imf1 components can highly disrupt predictions and are the most difficult to accurately model additionally the prediction accuracy of the developed models was improved by removing imf1 components imf1 components may contain high frequency information for rainfall and runoff series which complicates the prediction process wen et al 2019 a two stage decomposition method was proposed to solve this problem and it uses variational mode decomposition vmd dragomiretskiy and zosso 2014 for the decomposition of high frequencies imf1 fijani et al 2019 rahimpour et al 2021 wang et al 2017 the two stage decomposition hybrid model performs better than traditional models and can significantly improve the model prediction accuracy to test the predictive performance of a model the study data are usually partitioned into training sets and test sets and the test sets are used to evaluate the performance of the model in some previous studies the first 80 of the data were used to establish the training set and the remaining 20 formed the test sets we deem this strategy the traditional approach guo et al 2021 yin et al 2021 yue et al 2020 however other studies have shown that the length of the training data series has an impact on the model prediction results guo et al 2020 tokar and johnson 1999 for limited datasets the length of the training data series obtained will vary depending on subset partitioning moreover the prediction performance of a model may improve when the difference between the mean and variance of the training and test sets is relatively small gao et al 2020a obtaining subsets with similar statistical properties mean and standard deviation can ensure that each subset represents the same statistical population masters 1993 however the nonlinear nonstationary and complex characteristics of rainfall and runoff series in changing environments may be challenging to model and the corresponding statistical characteristics can be inconsistent if the same partitioning criterion as in the traditional method of partitioning datasets is used to partition different hydrological series large differences may arise in the statistical properties of the obtained training and test sets statistical differences between these two datasets may lead to unstable predictions thus the traditional method of dividing the dataset may not be applicable in many cases in previous studies researchers used different approaches to ensure that the statistical properties are similar between subsets but no more detail on how to achieve this is available campolo et al 1999 ray and klindworth 2000 kohonen 1982 and bowden et al 2002 divided the data using a self organizing map som making the data in different subsets representative however this approach performs dimensionality reduction clustering on high dimensional input and output data for the purpose of dividing the data into different subsets furthermore no guidelines exist for determining the optimal size and shape of the som bowden et al 2002 proposed a genetic algorithm to determine the optimal arrangement of the dataset into fixed size subsets with similar statistical properties and ensure that the differences in means and standard deviations between the subsets were minimized although this approach was successful the proportions of subsets to be divided had to be selected in advance furthermore these approaches actually change the time order of the data for time series such as rainfall and runoff temporal dependence relations exist and the order of data in the test set should not be changed therefore we propose a new method of partitioning the data using a particle swarm optimization pso algorithm j and r 1995 to minimize the differences in the mean and variance between subsets this approach does not change the order of the data and does not require the proportions of the training and test sets to be predetermined instead the optimal partitions are selected we believe that for different rainfall and runoff series different ratios should be used to partition the training and test sets the main objective of this study is twofold to develop a data partitioning method based on pso and to construct a prediction model based on a two stage hybrid model p cvee ldbnn that integrates ceemdan vmd langevin dynamics and a bayesian neural network one objective of this study is 1 to determine the effects of traditional data partitioning methods and the proposed partitioning method on model prediction results a one stage hybrid model cee ldbnn that integrates ceemdan langevin dynamics and bayesian neural network methods is constructed to simulate and predict the training and test sets obtained with the two methods of data partitioning the one stage hybrid cee ldbnn model that uses the proposed partitioning method is called the p cee ldbnn model and the one stage hybrid cee ldbnn model that uses the traditional dataset partitioning method is called the t cee ldbnn model the other objectives of this study are 2 to test the prediction performance of the two stage hybrid p cvee ldbnn model and the one stage hybrid t cee ldbnn and p cee ldbnn models and 3 to compare and analyze the interval evaluation index results to assess the interval prediction capability of the models the data selected in this paper are rainfall and runoff series from seven hydrological and meteorological stations in the yellow river basin t cee ldbnn a one stage hybrid model that integrates ceemdan langevin dynamics bayesian neural network and traditional methods for partitioning datasets p cee ldbnn one stage hybrid model that integrates ceemdan langevin dynamics and bayesian neural network methods with the proposed method for partitioning datasets p cvee ldbnn two stage hybrid model that integrates ceemdan vmd langevin dynamics and bayesian neural network methods with the newly proposed method for partitioning datasets mh minghe hydrological stationlm longmen hydrological stationtdg toudaoguai hydrological stationhx huaxian hydrological stationqs qishan meteorological stationys yongshou meteorological stationly linyou meteorological station 2 materials and methods 2 1 complete ensemble empirical mode decomposition with adaptive noise ceemdan emd is a method used to analyze nonlinear and nonstationary signals huang et al 1998 eemd wu and huang 2009 is used to solve the modal mixing problem of emd by adding white noise to the original signal however some new issues may be created zhang et al 2017b found that as noise cannot be completely removed new and undesirable modes are often created to overcome these problems the ceemdan method was proposed in this approach adaptive white noise is added to each eemd step and this can reduce the number of calculations needed improve the decomposition efficiency and accurately reconstruct the original signal maría et al 2011 for more details on the ceemdan algorithm see the study by maría et al 2011 2 2 variational mode decomposition vmd vmd is a new variational method for signal decomposition proposed by dragomiretskiy and zosso 2014 it is a fully nonrecursive variational mode decomposition algorithm and extracts the modes concurrently vmd is superior to the previously proposed decomposition model emd in terms of noise robustness dragomiretskiy and zosso 2014 in this paper we use the vmd algorithm to perform a second decomposition of the imf1 components obtained from the ceemdan algorithm decomposition 1 construction of variational problems 1 min μ k ω k k 1 k t δ t j π t μ k t e j w k t 2 2 s t k 1 k μ k t f where μ k μ 1 μ 2 μ k and ω k ω 1 ω 2 ω k are sets of k finite bandwidth imfs obtained by decomposition and the corresponding center frequencies respectively 2 solutions to variational problems to solve a constrained variational problem the quadratic penalty term α and the lagrangian operator λ t are introduced and the augmented lagrangian expression can be obtained as follows 2 l μ k ω k λ α k 1 k t δ t j π t μ k t e j w k t 2 2 f t k 1 k μ k t 2 2 λ t f t k 1 k μ k t the solution of the original minimization problem 1 is the saddle point of the augmented lagrangian expression notably the saddle point of formula 2 can be obtained by using the alternating direction of multipliers method admm for more details on the vmd algorithm see dragomiretskiy and zosso 2014 2 3 langevin dynamics for bayesian neural networks bayesian neural networks combine artificial neural networks with bayesian inference methods which account for the uncertainty of the model parameters and the model prediction results and can obtain interval prediction results langevin dynamics is an mcmc algorithm that integrates gradient and gaussian noise in updating the parameters of bayesian neural networks enabling faster convergence of markov chains to the posterior distribution 2 3 1 model and prior let a denote a single variable time series that is generated by a signal plus noise model 3 y t f y t ε t w h e r e t 1 2 n where f y t is a neural network y t y t 1 y t 2 y t i is a vector of lagged values of y t i represents the number of inputs to the feedforward neural network and ε t is the noise component with ε t n 0 τ 2 t a feedforward neural network with one hidden layer is used to calculate f y t which is defined as follows 4 f y t a ξ o h 1 h φ h a ξ h i 1 i γ ih y t i where a represents the activation function the sigmoid function is used in this paper ξ h and ξ o represent the biases of the hidden layer and the output layer respectively φ h represents the weight set used to map the hidden layer to the output layer and γ ih represents the weight set used to map the input layer to the hidden layer the network contains a total of l i h 2 h o 1 parameters where i is the number of neurons in the input layer and h is the number of neurons in the hidden layer let θ γ φ ξ τ 2 γ and φ are the parameter vectors mapped from the input layer to the hidden layer and the hidden layer to the output layer respectively ξ is the parameter vector of the biases and τ is the variance of the noise assuming that the elements of θ are independent the weights and biases have normal prior distributions with a mean of zero and variance σ 2 additionally τ 2 is assumed to be an inverse gamma prior distribution with parameters v 1 and v 2 the joint prior distribution of the parameters can be obtained by the product of the prior distributions of the parameters chandra et al 2017 the likelihood function is given by the following 5 l y θ 2 π τ 2 n 2 e x p 1 2 τ 2 y t f y t 2 according to bayes theorem the posterior distribution of parameters is as follows 6 p θ y l y θ π θ 2 3 2 bayesian point and interval forecasts the mean of the predicted posterior distribution is chosen as the bayesian point estimate 7 e y n 1 1 m m 1 m y n 1 m where m is the number of samples the 95 credible interval obtained from the predicted posterior distribution is used as the result of bayesian interval estimation f y n 1 is defined as the cumulative distribution function of y n 1 8 p y y upper f y upper 0 95 9 p y y lower 1 f y lower 0 05 if y upper and y lower satisfy the above conditions then y lower y upper is the bayesian interval prediction result for y n 1 2 3 3 algorithm bayesian inference is used to calculate the parameters of the proposed model and the posterior probability distribution is usually used to represent uncertainty the statistical data for the parameters in the above formula are usually difficult to directly obtain therefore these parameters must be calculated by sampling from the posterior distribution the metropolis hastings algorithm is a widely used mcmc algorithm the proposed mcmc algorithm combines langevin dynamics and a single metropolis hastings algorithm the new value of θ is as follows 10 θ q n θ m σ θ where 11 θ m θ m r e y θ m e y θ m y t f y t m 2 e y θ m e θ 1 e θ l r represents the learning rate σ θ σ θ 2 i l and i l is an l l identity matrix then θ q can be expressed based on two steps a weight updating based on gradient descent with equation 22 b the addition of a certain amount of gaussian noise from n 0 σ θ the langevin dynamics for bayesian neural networks ldbnn algorithm is given in algorithm 1 the ldbnn algorithm involves gradients with gaussian noise in the parameter updating step compared with the random walk algorithm the ldbnn approach provides more accurate estimates of parameters algorithm 1 langevin dynamics for bayesian neural networks result obtain the posterior distribution of the parameters set max samples k repeat until k k 1 set hyperparameters σ v 1 v 2 r 2 obtain μ from n 0 σ μ 3 calculate the gradient θ m by eq 22 4 propose θ θ m θ m μ 5 calculate the acceptance probability α m i n 1 p θ p y q θ m θ q p θ m y q θ q θ m 6 obtain u from a standard normal distribution if α u accept and set θ m 1 θ else θ m 1 θ m end end 2 4 a new method of partitioning datasets the statistical distributions of various rainfall and runoff series are different due to factors such as climate change and human activities partitioning different datasets based on the same ratio may result in significant differences between the obtained training and test sets thus affecting the accuracy of predictions therefore we propose a new method of partitioning datasets based on a pso algorithm we regard the locations of the partitioned training and test sets as unknown parameters and apply pso to optimize the parameters our aim is to minimize the difference in mean and variance between the training and test sets obtained by partitioning the data the target function of the pso algorithm is the sum of the absolute value of the difference of the mean and the absolute value of the difference of the variance between the training set and the test set the population size was set to 40 the maximum number of iterations was 150 to ensure that the length of the training set is larger than that of the test set we set the parameter optimization range as 0 5 length data length data to search for the global optimal solution the specific details of the above process are shown in algorithm 2 algorithm 2 particle swarm optimization for partitioning datasets data univariate hydrological series result obtain the optimal solution for unknown parameters training sets data 0 x test sets data x length data a and b are the mean and variance of the training sets respectively c and d are the mean and variance of the test sets respectively f abs a c abs b d set target function f set unknown parameter x set scope of the parameter 0 5 length data length data set population size n set max iteration m procedure pso for each particle i initialize velocity v i and position x i for particle i calculate f and set pbest i x i gbest m i n pbest i m 0 while m m for i 1 to n update the velocity and position of particle i calculate f if f x i f p b e s t i pbest i x i if f pbest i f g b e s t gbest pbest i end end x g b e s t print x 3 case study 3 1 study area and data the yellow river basin originates in the bayan krai mountains on the tibetan plateau and finally flows into the bohai sea yang et al 2021 the yellow river is the second largest river in china after the yangtze river with a total length of approximately 5 500 km and a basin area of approximately 753 000 square kilometers the yellow river basin is 4 675 m above sea level with an average flow of 1774 5 m3 s the dividing point between the upper and middle reaches of the yellow river basin is in the town of hekou in inner mongolia and the dividing point between the middle and lower reaches is in peach valley in henan province throughout chinese history the yellow river and its coastal watershed have been the primary foundations of the chinese nation and the yellow river is known as the mother river the climate varies significantly in different areas of the basin from the origin of the yellow river to the bohai sea and the climatic zone changes from cold to temperate the yellow river basin receives abundant sunshine generally reaching 2 000 to 3 300 h throughout the year since the 1970s the annual precipitation in the yellow river basin has exhibited a decreasing trend xu 2001 because the yellow river basin supplies water to 12 of china s population zhang et al 2017a accurate and reliable forecasting is of great importance in this study monthly runoff data from four hydrological stations and monthly precipitation data from three meteorological stations in the yellow river basin were collected fig 1 the monthly runoff series lengths for the xh tdg mh and lm hydrological stations were 480 648 852 and 732 respectively the length of the monthly precipitation series was the same for the qs ys and ly meteorological stations all with 576 3 2 model development due to the limitations of neural networks most studies divide datasets into training sets validation sets and test sets validation sets are used for network adjustment to prevent overfitting bayesian neural networks are usually relatively robust to overfitting because they obtain the predictive distribution by integrating over the parameters rather than choosing a single point estimate chandra et al 2017 bayesian methods do not require the use of cross validation to avoid overfitting khan and coulibaly 2006 we only divided the dataset into training and test sets we propose a new method of dividing datasets based on the pso algorithm the two stage hybrid p cvee ldbnn model can be used to predict rainfall and runoff series the main modeling process of the two stage hybrid p cvee ldbnn model is as follows 1 the original monthly runoff and monthly precipitation data are partitioned using our proposed new dataset partitioning method to obtain training and test sets of different lengths 2 in the first stage the ceemdan algorithm is utilized to decompose the data series into distinct imfs with corresponding low high frequency and residual res components 3 in the second stage the vmd method is applied to decompose imf1 data into different variational modes vms 4 an ldbnn is used to predict imfs and res subsequences where the predicted imf1 component is obtained by summing the different vm subsequences 5 the prediction results of all subcomponents are superimposed to obtain the final prediction results to compare the effects of traditional methods and our proposed method of data partitioning based on the subsequent model prediction results a one stage hybrid cee ldbnn model was used to model and predict training and test sets of different lengths obtained from the two partitioning methods a two stage hybrid p cvee ldbnn model was then used to evaluate the impact of the vmd approach on the model prediction results the reliability of the credible intervals obtained from the t cee ldbnn model p cee ldbnn model and p cvee ldbnn model was assessed using the credible interval evaluation index fig 2 shows the flow chart of the above modeling process many studies have shown that the prediction accuracy of a hybrid model based on decomposition is higher than that of individual models abda and chettih 2018 chen et al 2021 he et al 2020 li et al 2018 due to the highly nonlinear nonstationary and strongly fluctuating characteristics of rainfall and runoff series the prediction accuracy of the standalone ldbnn model considered in this paper is poor and the model is not qualified for analysis tasks therefore the standalone ldbnn model is not used for comparison and its results are not shown in the analysis later in the paper in this paper a one step ahead prediction strategy was adopted during the testing period the ldbnn model was constructed using a three layer neuron arrangement i e input layer hidden layer and output layer the pacf method was applied to the decomposed submodes and the appropriate input sequence length was selected peng et al 2021 if the number of neurons in the input layer was 1 the number of neurons in the corresponding hidden layer was set to 5 otherwise the number of neurons in the hidden layer was set to 10 then the input and target data were normalized in the range of 0 1 a sigmoid function was selected as the neuron activation function for the ldbnn model our proposed method of partitioning datasets was applied to data from seven hydrological and meteorological stations and the results obtained after partitioning are shown in fig 3 for comparison the training and testing periods obtained with the traditional partitioning method are also shown in fig 3 3 3 performance evaluation the nash sutcliffe efficiency coefficient nse correlation coefficient r root mean square error rmse and mean absolute error mae are used as evaluation metrics to assess the accuracy of the mean of the posterior distribution of the bayesian point prediction results and they can be described as follows 12 nse 1 i 1 n o i p i 2 i 1 n o i o i 2 13 r i 1 n o i o i p i p i i 1 n o i o i 2 i 1 n p i p i 2 14 mae 1 n i 1 n p i o i 15 rmse 1 n i 1 n o i p i 2 where o i and p i represent observed and predicted values respectively o i and p i represent the averages of the observed and predicted values respectively and n is the length of the data sequence the containment ratio cr the average asymmetry degree s and the average bandwidth iw are used to assess the reliability of credible intervals at a credibility level of 95 for the bayesian interval prediction results xiong et al 2009 the cr indicates the ratio of the number of observations in the prediction interval to the total number of observations and can be expressed as a percentage the value of cr ranges from 0 to 1 an ideal prediction interval yields a cr equal to 1 or 100 therefore the higher the cr value is the greater the proportion of observed values within the forecasted interval s is used to describe the geometric structure of the prediction bounds the ideal prediction interval is symmetric and optimally the difference between the upper limit of the interval and the observed value will be approximately equal to the difference between the lower limit of the interval and the observed value iw indicates the average width of the prediction interval 16 iw i 1 n p up i p low i n 17 cr i 1 n c o i n c o i 1 p low i o i p up i 0 e l s e 18 s 1 n i 1 n p up i o i p up i p low i 0 5 where p up i and p low i are the upper and lower prediction bounds respectively 4 results and discussion we compared the proposed method of partitioning data and the traditional partitioning method based on the prediction results to simplify the process we used a one stage cee ldbnn model to simulate and forecast monthly runoff and precipitation data table 1 shows the prediction results of the cee ldbnn model based on the proposed method of partitioning datasets p cee ldbnn and these results were compared with those of a cee ldbnn model based on the traditional partitioning method t cee ldbnn in the testing stage in general the performance of the p cee ldbnn model far surpasses that of the t cee ldbnn model for runoff and precipitation data for the datasets from meteorological stations the p cee ldbnn model yielded nse values of approximately 0 696 qs 0 642 ys and 0 752 ly which are approximately 0 187 qs 0 229 ys and 0 107 ly higher than the nse values produced by the t cee ldbnn model respectively for the monthly precipitation data from the three meteorological stations the r values obtained by the p cee ldbnn model were higher than the corresponding values obtained by the t cee ldbnn model this result supports the relatively low rmse and mae values the forecasting accuracy of the p cee ldbnn model can be easily concluded to be higher than that of the t cee ldbnn model for the datasets from the hydrological stations the nse and r values obtained for the p cee ldbnn model were better than the corresponding values obtained for the t cee ldbnn model this result is consistent with the conclusions drawn above notably we found that the rmse and mae values obtained from the t cee ldbnn model were slightly smaller than the corresponding values obtained from the p cee ldbnn model at the hx tdg and mh hydrological stations notably we believe that the rmse and mae as error indicators are influenced by peak values fig 3 depicts the lengths of the training and test sets derived from different partitioning methods for the data from seven hydrological and meteorological stations as shown in fig 3 the datasets from the hx hydrological station were divided using the proposed method and the test set data obtained were from october 1981 to december 1999 spanning a total of 219 months the traditional method was used to divide the datasets from the same hydrological station and the test set data obtained were from february 1992 to december 1999 spanning a total of 95 months the period from october 1981 to january 1992 a total of 124 months was the prediction period considered in the p cee ldbnn model but the period of no predictions was considered in the t cee ldbnn model the p cee ldbnn model provided predictions over 314 tdg and 230 mh months and the t cee ldbnn model provided predictions for only 129 tdg and 169 mh months the lengths of the test sets obtained by the proposed method were longer than those obtained by the traditional method thus for all three sites predictions in some periods were obtained with the p cee ldbnn model but not with the t cee ldbnn model due to the comparatively longer prediction time of the p cee ldbnn model we believe that some points with large flow values points with large flows in a given period predicted by the p cee ldbnn model but not considered by the t cee ldbnn model may deviate from the true values leading to slightly higher rmse and mae values this result is supported by the formulas for mae equation 29 and rmse equation 30 fig 4 shows scatter plots of observed runoff values and the value predicted by the t cee ldbnn model and p cee ldbnn model in the testing phase our focus was on analyzing the effects of the two dataset partitioning methods for data from the hx tdg and mh hydrographic stations therefore only scatter plots for these three hydrographic stations are shown in fig 4 for the hx fig 4 a and tdg fig 4 b hydrological stations the results of the t cee ldbnn model during the test period were concentrated in the lower left corner of the scatter plot inside the black dotted line indicating that the observed flow values in the test set were small however for the p cee ldbnn model some large observed flow values during the test period phase were not appropriately predicted resulting in underestimated prediction results we believe that this is the reason why the rmse and mae values obtained for the p cee ldbnn model were larger than the corresponding values obtained for the t cee ldbnn model in fig 4 c mh hydrological station more scatter points located outside the black dotted line with observed flow values greater than 90 were found in the test sets of the p cee ldbnn model than in the test sets of the t cee ldbnn model some peaks points with high flows between november 1991 and november 1996 were observed and the predicted values of the p cee ldbnn model were comparatively low for the mh hydrological station the rmse and mae values obtained for the p cee ldbnn model were only 0 59 and 0 94 higher respectively than the corresponding values obtained for the t cee ldbnn model in table 1 this deviation was smaller than that observed at the hx and tdg hydrological stations although the peak flows were larger at the mh hydrological station compared with the peaks in the test sets obtained with the traditional method larger peaks were present in the test sets obtained with the proposed method the rmse and mae are most influenced by peak values thus the p cee ldbnn model yielded slightly larger rmse and mae values than does the t cee ldbnn model the nse and r metrics are widely used to verify the prediction results of models and they can reflect model performance the nse is also the best objective metric for reflecting the overall fit of a hydrological curve legates and mccabe 1999 based on these metrics the performance of the p cee ldbnn model was better than that of the t cee ldbnn model and our proposed new method of partitioning datasets can improve the performance of prediction models to some extent as shown in table 1 the prediction accuracy of the t cee ldbnn model and p cee ldbnn model for the datasets from the tdg hydrological station was lower than that for the datasets from other stations for further analysis we list the hydrological statistics for the hydrological and meteorological station data used in this paper in table 2 due to the use of different division methods for the datasets the lengths of the training and test sets were different fig 3 and the corresponding mean and standard deviation of the training and test sets were also different as shown in table 2 the significant differences between the training and test sets affected the prediction results of the models with large values of d mean and d std the tdg hydrological station had the largest values of d mean and d std therefore the accuracy was lower than that of the other stations the values of d mean and d std obtained by partitioning the datasets using the traditional method were higher than the corresponding values obtained by the proposed method this result is consistent with the above conclusions the p cee ldbnn model performed better than the t cee ldbnn model and the differences between the training and test sets obtained with the proposed method of data partitioning were comparatively smaller which improved the performance of the model d mean indicates the difference between the means of the training and test sets d std indicates the difference between the standard deviations of the training and test sets the new method of data partitioning based on pso appears to perform better than the traditional method of partitioning therefore we applied the two stage hybrid p cvee ldbnn model to simulate and forecast the training and test sets based on pso the nse r rmse and mae values were applied to assess the predictive performance of the model and the evaluation index values are listed in table 3 compared with the values in table 1 the prediction accuracy of the two stage hybrid p cvee ldbnn model shown in table 3 was greatly improved the nse values obtained with the two stage hybrid p cvee ldbnn model were all above 0 9 and significantly higher than the nse values obtained for the t cee ldbnn model and p cee ldbnn model in table 1 this finding is consistent with the high r values and low rmse and mae values obtained in the analyses above the imf1 components obtained by ceemdan decomposition may be characterized by non stationarity and strong volatility thus using vmd to decompose imf1 components can greatly improve the accuracy of the forecasting results notably for the tdg hydrographic station the nse value increased from 0 202 to 0 916 an increase of more than 0 7 with this approach to intuitively show the results of the models for comparison the nse r rmse and mae values of the seven datasets are illustrated using radial column charts the evaluation index values of the three models for the seven datasets in one step ahead prediction are shown in fig 5 specifically the performance of the models is visually illustrated and the results are consistent with those of the previous analyses in this paper fig 6 a b and c show the scatter plots of the observed and predicted flows of the three models in the test period the scatter diagrams are helpful for analyzing the correspondence between observed and predicted flows the coefficient of determination r2 ranges from to 1 the coefficient of determination r2 is also referred to as the goodness of fit the larger the goodness of fit is the denser the scatter around the regression line conversely this indicates that more data points are scattered far from the regression line which means that observations cannot be accurately reproduced by the model in fig 6 a the t cee ldbnn model yielded coefficient of determination values of 0 38 and 0 44 at the tdg and ys stations respectively in fig 6 b the p cee ldbnn model yielded a coefficient of determination of 0 44 at the tdg hydrological station they yielded values of the coefficient of determination that are smaller therefore the scatter is farther from the regression line in fig 6 c the p cvee ldbnn model yielded coefficient of determination values that all reached above 0 9 and the scatter is densely distributed around the regression line fig 6 shows that the r2 value obtained for the p cvee ldbnn model was the highest followed by that obtained for the p cee ldbnn model additionally the value obtained for the t cee ldbnn model was the lowest therefore the p cvee ldbnn model can accurately reproduce the observations accurate and reliable peak value prediction is important for flood prevention and warning water resource planning and management and safety risk analysis comparing the points in fig 6 a and fig 6 b the points in fig 6 c are closer to the y x line indicating that the predicted values obtained with the p cvee ldbnn model are closer to the true values compared with the p cvee ldbnn model the p cee ldbnn model and the t cee ldbnn model predicted peak values relatively farther away from the y x line table 4 shows the absolute relative error values are yielded by the p cee ldbnn model and the p cvee ldbnn model for predicting ten peak values under one step ahead prediction note that we do not give the are values yielded by the t cee ldbnn model here the reasons are twofold 1 the length of the test set obtained by the traditional dataset partitioning method and the length of the test set obtained by the new dataset partitioning method were not the same and therefore the ten peak values were not the same we cannot directly calculate the comparison 2 as seen in table 1 and table 3 the p cvee ldbnn model had much better predictive power than the t cee ldbnn model moreover the t cee ldbnn model in fig 6 yielded peak values prediction points farther away from the best fit line we can visualize from fig 6 that the prediction ability of the peak values of the p cvee ldbnn model was better than that of the peak values of the t cee ldbnn model the are value is calculated by the observed peak value and the corresponding predicted peak value xie et al 2019 the p cvee ldbnn model yielded the largest are values of 14 6 hx 9 2 tdg 8 9 mh 10 lm 17 6 qs 17 ys and 16 1 ly furthermore the prediction errors of the peak values of the p cvee ldbnn model were less than 20 the t cee ldbnn model on the other hand yielded widely fluctuating are values and had six stations where the maximum are value exceeded 40 hence the p cvee ldbnn model yielded the least error for predicting peak values thus the two stage hybrid p cvee ldbnn model can significantly improve the peak value prediction accuracy the ldbnn approach can yield predictive distributions of model parameters and model outputs and the corresponding credible intervals can be calculated the term credible interval here is different from the classic statistical term confidence interval a 95 credible interval indicates that the posterior probability that the predicted outcome is within the credible interval is 95 compared with deterministic prediction models the ldbnn model can provide a prediction interval to obtain probabilistic forecasts of rainfall and runoff series fig 7 shows the hydrographs of observed runoff and the values predicted with the proposed model p cvee ldbnn model during the testing phase in addition the 95 credible intervals for the predicted runoff are illustrated table 5 shows the cr and s values of the proposed model and the other two models t cee ldbnn model and p cee ldbnn model for comparison the cr and s values were based on the results obtained for 95 credible intervals credible intervals can represent accuracy and reliability sahlin 2015 the larger the cr value is the greater the number of observations contained in the credible interval according to formula 32 the closer the value of s is to 0 the higher the degree of symmetry of the credible interval used for prediction the more symmetrical a credible interval is the more reliable the prediction results will be therefore the greater the number of observations contained in the credible interval the more symmetrical the upper and lower bounds of the credible interval will be with respect to the observations leading to high model accuracy and reliable prediction results a comparison of the t cee ldbnn model and the p cee ldbnn model suggests that the p cee ldbnn model yields higher cr values than does the t cee ldbnn model at the hx tdg mh lm qs ys and ly hydrological and meteorological stations the cr values obtained with the p cee ldbnn model were 73 4 76 4 43 7 51 8 69 4 52 5 and 57 2 respectively the cr values obtained with the p cee ldbnn model were 7 4 20 9 0 8 13 7 15 0 19 8 and 13 8 higher than those of the t cee ldbnn model respectively for all hydrological and meteorological stations in this paper the s values obtained for the p cee ldbnn model were lower than the corresponding values obtained for the t cee ldbnn model thus the p cee ldbnn model yielded more accurate and reliable prediction results than the t cee ldbnn model this finding is consistent with the conclusions obtained earlier in this paper although the p cee ldbnn model yielded higher cr values and lower s values than the t cee ldbnn model the highest cr value obtained for the p cee ldbnn model was 73 4 hx and the lowest cr value was only 34 37 mh compared with those for the p cvee ldbnn model the credible intervals obtained for the t cee ldbnn model and the p cee ldbnn model contain fewer observations making the results unreliable the s value obtained for the p cvee ldbnn model was between 0 15 and 0 29 which was significantly lower than the corresponding values obtained for the t cee ldbnn model and the p cee ldbnn model therefore the prediction results obtained with the p cvee ldbnn model were more reliable the average width of the credible interval is also an important indicator of the reliability of the prediction interval the iw values for the seven stations calculated with the p cvee ldbnn model are listed in table 6 when the cr values of two reliable intervals are relatively close the smaller the iw value is the more reliable the interval prediction result zhang et al 2009 however the cr values obtained by the t cee ldbnn model and the p cee ldbnn model were much lower than the corresponding values obtained by the p cvee ldbnn model and the cr values of some stations are less than 50 a value of s less than 0 5 indicates that on average the hydrograph lies within the credible interval xiong et al 2009 however except for those for the hx hydrological station the s values of the t cee ldbnn model were all greater than 0 5 at more than half of the stations the p cee ldbnn model yielded s values greater than 0 5 which was not what we expected we believe that the comparison of model interval widths is not practically meaningful in the case of large differences in cr values therefore we do not list the iw values of the t cee ldbnn model and the p cee ldbnn model here the cr values obtained by the p cvee ldbnn model were comparatively higher the s values were less than 0 5 and the reliability of the prediction results was high 5 conclusions accurate prediction and reliable interval forecasts of rainfall and runoff series can provide a basis and reference for decision makers in water resource management and risk assessment in a changing environment in this study we constructed a two stage hybrid p cvee ldbnn model based on a new method of partitioning datasets the model first decomposes hydrological series into different imf components using the ceemdan method and then performs a second decomposition of the imf1 components using the vmd method each component is predicted separately by applying the ldbnn model and the sum of the prediction results of the components is the final prediction result of the model our proposed two stage hybrid p cvee ldbnn model was applied to four hydrological stations and three meteorological stations in the yellow river basin for forecasting with a one month ahead period the p cee ldbnn and the t cee ldbnn models were used as benchmark models hydrological series often display nonlinear nonstationary and strong fluctuations and significant differences exist among different hydrological series if the same ratio is used to partition different hydrological sequences large significant differences maybe emerge between the obtained training and test sets which can lead to unstable prediction results in most previous studies the same ratio 80 training to 20 test was used to partition datasets and we call this approach the traditional method we propose a new method of partitioning datasets based on a pso algorithm to partition hydrological sequences with different statistical properties into different proportions for training and testing the new method of data partitioning and the traditional method were used to partition datasets from seven hydrological and meteorological stations selected in this paper the one stage cee ldbnn model was applied for simulation and prediction based on the training and test sets obtained with the two methods by comparing and analyzing the nse r rmse and mae values obtained for the t cee ldbnn model and p cee ldbnn model we can draw the first conclusion the p cee ldbnn model performs better than the t cee ldbnn model relatively large significant differences between the training and test sets can affect the ability of a model to make predictions our proposed method of data partitioning based on pso can improve the model prediction accuracy to some extent to address the problem of low accuracy in predicting the imf1 components obtained by decomposition in the ceemdan method based on the first conclusion a second decomposition of imf1 components was performed with the vmd method based on the p cee ldbnn model to obtain the p cvee ldbnn model the evaluation metrics for the t cee ldbnn model and p cee ldbnn model and the corresponding scatter plots support the second conclusion the prediction accuracy of the two stage hybrid p cvee ldbnn model is significantly higher than that of the t cee ldbnn model and p cee ldbnn model and the p cvee ldbnn model provides more accurate prediction results for peak flows vmd is a reliable method that can significantly improve the accuracy of model predictions bayesian methods can provide credible intervals for obtaining predictions by comparing the t cee ldbnn model p cee ldbnn model and p cvee ldbnn model we identified a third conclusion the credible intervals obtained with the p cvee ldbnn model are more symmetric with respect to the observed flows and include higher proportions of observed flows than those of the other models in summary the two stage hybrid p cvee ldbnn model yields reliable interval prediction results and excellent point prediction results thus providing a basis for water resource planning management and risk assessment moreover the two stage hybrid model is more advantageous for areas with limited data on the initial conditions needed to construct the physical model in future studies multiscale e g 3 or 6 months ahead or shorter term e g daily or hourly rainfall and runoff forecasts can be considered because these types of series are important for hydrological series forecasting research credit authorship contribution statement hanbing xu conceptualization methodology visualization software writing original draft songbai song conceptualization methodology data curation writing review editing supervision validation tianli guo writing review editing investigation huimin wang writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work is jointly supported by national natural science foundation of china grant nos 51479171 52079110 the authors also wish to express their cordial gratitude to the editors and anonymous reviewers for their illuminating comments which have greatly helped improve the quality of this manuscript 
3222,obtaining accurate point estimates and reliable interval prediction results for rainfall and runoff series is important to aid in water resource decision making and planning management in a changing environment in this paper we propose a two stage hybrid model p cvee ldbnn the model uses the complete ensemble empirical mode decomposition with adaptive noise ceemdan and variational mode decomposition vmd methods for data preprocessing and a bayesian neural network ldbnn based on langevin dynamics as a prediction model then a new dataset partitioning method based on a particle swarm optimization algorithm which is different from the traditional dataset partitioning method is applied a one stage hybrid model t cee ldbnn that integrates the ceemdan method and the traditional method of partitioning datasets a one stage hybrid model p cee ldbnn that integrates the ceemdan method and the new method of partitioning datasets and the two stage hybrid p cvee ldbnn model were compared these models were applied to monthly runoff and monthly precipitation series from seven hydrological and meteorological stations in the yellow river basin the mean absolute error mae absolute root mean squared error rmse nash sutcliffe efficiency nse and correlation coefficient r were used to evaluate the predictive ability of the models the containment ratio cr the average bandwidth iw and the average asymmetry degree s were used to evaluate the interval prediction results of the models the results show that 1 the subset obtained with the new method of data partitioning considering statistical properties is more favorable for model prediction 2 the second decomposition approach based on vmd is a reliable method that can significantly improve the prediction accuracy of the final model 3 compared with traditional neural networks that can only obtain deterministic point prediction results bayesian methods can provide intervals with prediction results making the results of the two stage hybrid p cvee ldbnn model highly reliable keywords two stage hybrid model langevin dynamics bayesian neural networks data portioning particle swarm optimization algorithm yellow river basin data availability the data that has been used is confidential 1 introduction influenced by human activities including the construction of dams excessive reclamation and changing of land use and pollution of water and climate change rainfall and runoff series are characterized by high degrees of nonlinearity and complexity and different hydrological series exhibit considerable differences javadinejad et al 2021 eslamian et al 2018 fatahi nafchi et al 2021 kaveh et al 2017 these differences increase the difficulty of accurately forecasting rainfall and runoff series the deterministic point predictions obtained by hydrological models are often nonideal for water resource planning and risk assessments if they are underestimated or overestimated compared with the observed flows gao et al 2020b rational estimates of rainfall and runoff prediction intervals can provide a basis for optimal water resource allocation and hydrological forecasting decisions khan and coulibaly 2006 liu and gupta 2007 therefore one objective is to obtain improved point forecasting results and provide a reliable interval forecast in the context of the changing environment unlike physically driven models that need to consider the underlying physical processes in forecasting fang et al 2017 romagnoli et al 2017 wu and lin 2017 machine learning and deep learning models can directly capture the complex nonlinear relationships between input and output variables gao et al 2020b jeong and park 2019 lin et al 2009 ostad ali askari and shayan 2021 wu and chau 2011 zhang et al 2021 there are studies that have proven machine learning and deep learning models to be effective tools in hydrological studies chang et al 2014 kao et al 2021 kao et al 2020 zhou et al 2019 in particular anns are reliable hydrological modeling tools that are widely used for rainfall and runoff modeling jain and srinivasulu 2004 johny et al 2020 rajurkar et al 2002 anns include the input layer hidden layer and output layer ostad ali askari et al 2017 ostad ali askari and shayannejad 2021 however traditional anns have the following two main limitations 1 anns update parameters by error backpropagation and gradient descent methods which provide good fits for training period data well and may produce relatively large errors in predictions based on new data and 2 only deterministic point prediction results can be obtained chandra et al 2017 the main differences between neural networks based on bayesian inference methods and traditional neural networks are as follows neal 1996 first the bayesian approach treats the parameters network weights as random variables that obey a certain probability distribution rather than unknown constants notably the markov chain monte carlo mcmc method can capture parameter uncertainty welling and teh 2011 according to bayes theorem the posterior distribution of the parameters is obtained from the prior distribution of the parameters and the likelihood function the posterior distribution of the parameters contains information about the parameters and provides reliable estimation results second the predicted outcome is obtained from a probability distribution bayesian point prediction results are obtained based on the mean of the predicted distribution bayesian methods consider the uncertainty of parameters network weights and model outputs which can mitigate overfitting li et al 2016 additionally bayesian neural networks outperform traditional anns in terms of prediction accuracy kingston et al 2005 incorporating the uncertainty of parameters into forecasts can improve the point forecasting results kasiviswanathan et al 2013 and yield reliable interval forecasts khan and coulibaly 2006 bayesian neural networks have been widely used for hydrological modeling li et al 2021 zhan et al 2020 zhang et al 2011 the mcmc sampling method can provide estimates of the posterior distribution of data gamerman and lopes 2006 however the application of bayesian methods in neural networks is impeded by the limitations of mcmc methods in terms of convergence and scalability chandra et al 2019 in previous studies a class of mcmc techniques based on langevin dynamics was considered to solve this problem these methods combine gradient concepts and gaussian noise in the parameter updating scheme thus allowing the markov chain to converge to the full posterior distribution neal 2011 welling and teh 2011 chandra et al 2017 applied a method based on langevin dynamics and a neural network for chaotic time series prediction and showed that the method improved most of the problems of the mcmc random walk algorithm however due to the highly nonstationary and complex nature of hydrological series in changing environments individual models often fail to adequately predict rainfall and runoff series when meteorological observations are unavailable zuo et al 2020 the empirical mode decomposition emd huang et al 1998 ensemble empirical mode decomposition eemd wu and huang 2009 and complete ensemble empirical mode decomposition with adaptive noise ceemdan maría et al 2011 methods were developed to address nonstationary time series for enhanced analysis and forecasting eemd was proposed to solve the mode mixing problem that occurs in emd however the reconstructed signal contains residual noise that may generate new modes wen et al 2019 the ceemdan method as a variation of the eemd algorithm can provide a more accurate construction result than prior methods maría et al 2011 hybrid models based on ceemdan decomposition are also widely used for forecasting studies in several fields lin et al 2021 integrated the ceemdan approach with long short term memory lstm for stock index prediction gao et al 2020a integrated the ceemdan method with a cnn lstm for hourly solar irradiance forecasting and wang et al 2021 integrated the ceemdan method with a gated recurrent unit network gru for forecasting natural gas prices however to our knowledge no previous studies have used the ceemdan method coupled with a bayesian neural network based on langevin dynamics to predict rainfall and runoff series guo et al 2012 and huang et al 2014 noted that high frequency e g imf1 components can highly disrupt predictions and are the most difficult to accurately model additionally the prediction accuracy of the developed models was improved by removing imf1 components imf1 components may contain high frequency information for rainfall and runoff series which complicates the prediction process wen et al 2019 a two stage decomposition method was proposed to solve this problem and it uses variational mode decomposition vmd dragomiretskiy and zosso 2014 for the decomposition of high frequencies imf1 fijani et al 2019 rahimpour et al 2021 wang et al 2017 the two stage decomposition hybrid model performs better than traditional models and can significantly improve the model prediction accuracy to test the predictive performance of a model the study data are usually partitioned into training sets and test sets and the test sets are used to evaluate the performance of the model in some previous studies the first 80 of the data were used to establish the training set and the remaining 20 formed the test sets we deem this strategy the traditional approach guo et al 2021 yin et al 2021 yue et al 2020 however other studies have shown that the length of the training data series has an impact on the model prediction results guo et al 2020 tokar and johnson 1999 for limited datasets the length of the training data series obtained will vary depending on subset partitioning moreover the prediction performance of a model may improve when the difference between the mean and variance of the training and test sets is relatively small gao et al 2020a obtaining subsets with similar statistical properties mean and standard deviation can ensure that each subset represents the same statistical population masters 1993 however the nonlinear nonstationary and complex characteristics of rainfall and runoff series in changing environments may be challenging to model and the corresponding statistical characteristics can be inconsistent if the same partitioning criterion as in the traditional method of partitioning datasets is used to partition different hydrological series large differences may arise in the statistical properties of the obtained training and test sets statistical differences between these two datasets may lead to unstable predictions thus the traditional method of dividing the dataset may not be applicable in many cases in previous studies researchers used different approaches to ensure that the statistical properties are similar between subsets but no more detail on how to achieve this is available campolo et al 1999 ray and klindworth 2000 kohonen 1982 and bowden et al 2002 divided the data using a self organizing map som making the data in different subsets representative however this approach performs dimensionality reduction clustering on high dimensional input and output data for the purpose of dividing the data into different subsets furthermore no guidelines exist for determining the optimal size and shape of the som bowden et al 2002 proposed a genetic algorithm to determine the optimal arrangement of the dataset into fixed size subsets with similar statistical properties and ensure that the differences in means and standard deviations between the subsets were minimized although this approach was successful the proportions of subsets to be divided had to be selected in advance furthermore these approaches actually change the time order of the data for time series such as rainfall and runoff temporal dependence relations exist and the order of data in the test set should not be changed therefore we propose a new method of partitioning the data using a particle swarm optimization pso algorithm j and r 1995 to minimize the differences in the mean and variance between subsets this approach does not change the order of the data and does not require the proportions of the training and test sets to be predetermined instead the optimal partitions are selected we believe that for different rainfall and runoff series different ratios should be used to partition the training and test sets the main objective of this study is twofold to develop a data partitioning method based on pso and to construct a prediction model based on a two stage hybrid model p cvee ldbnn that integrates ceemdan vmd langevin dynamics and a bayesian neural network one objective of this study is 1 to determine the effects of traditional data partitioning methods and the proposed partitioning method on model prediction results a one stage hybrid model cee ldbnn that integrates ceemdan langevin dynamics and bayesian neural network methods is constructed to simulate and predict the training and test sets obtained with the two methods of data partitioning the one stage hybrid cee ldbnn model that uses the proposed partitioning method is called the p cee ldbnn model and the one stage hybrid cee ldbnn model that uses the traditional dataset partitioning method is called the t cee ldbnn model the other objectives of this study are 2 to test the prediction performance of the two stage hybrid p cvee ldbnn model and the one stage hybrid t cee ldbnn and p cee ldbnn models and 3 to compare and analyze the interval evaluation index results to assess the interval prediction capability of the models the data selected in this paper are rainfall and runoff series from seven hydrological and meteorological stations in the yellow river basin t cee ldbnn a one stage hybrid model that integrates ceemdan langevin dynamics bayesian neural network and traditional methods for partitioning datasets p cee ldbnn one stage hybrid model that integrates ceemdan langevin dynamics and bayesian neural network methods with the proposed method for partitioning datasets p cvee ldbnn two stage hybrid model that integrates ceemdan vmd langevin dynamics and bayesian neural network methods with the newly proposed method for partitioning datasets mh minghe hydrological stationlm longmen hydrological stationtdg toudaoguai hydrological stationhx huaxian hydrological stationqs qishan meteorological stationys yongshou meteorological stationly linyou meteorological station 2 materials and methods 2 1 complete ensemble empirical mode decomposition with adaptive noise ceemdan emd is a method used to analyze nonlinear and nonstationary signals huang et al 1998 eemd wu and huang 2009 is used to solve the modal mixing problem of emd by adding white noise to the original signal however some new issues may be created zhang et al 2017b found that as noise cannot be completely removed new and undesirable modes are often created to overcome these problems the ceemdan method was proposed in this approach adaptive white noise is added to each eemd step and this can reduce the number of calculations needed improve the decomposition efficiency and accurately reconstruct the original signal maría et al 2011 for more details on the ceemdan algorithm see the study by maría et al 2011 2 2 variational mode decomposition vmd vmd is a new variational method for signal decomposition proposed by dragomiretskiy and zosso 2014 it is a fully nonrecursive variational mode decomposition algorithm and extracts the modes concurrently vmd is superior to the previously proposed decomposition model emd in terms of noise robustness dragomiretskiy and zosso 2014 in this paper we use the vmd algorithm to perform a second decomposition of the imf1 components obtained from the ceemdan algorithm decomposition 1 construction of variational problems 1 min μ k ω k k 1 k t δ t j π t μ k t e j w k t 2 2 s t k 1 k μ k t f where μ k μ 1 μ 2 μ k and ω k ω 1 ω 2 ω k are sets of k finite bandwidth imfs obtained by decomposition and the corresponding center frequencies respectively 2 solutions to variational problems to solve a constrained variational problem the quadratic penalty term α and the lagrangian operator λ t are introduced and the augmented lagrangian expression can be obtained as follows 2 l μ k ω k λ α k 1 k t δ t j π t μ k t e j w k t 2 2 f t k 1 k μ k t 2 2 λ t f t k 1 k μ k t the solution of the original minimization problem 1 is the saddle point of the augmented lagrangian expression notably the saddle point of formula 2 can be obtained by using the alternating direction of multipliers method admm for more details on the vmd algorithm see dragomiretskiy and zosso 2014 2 3 langevin dynamics for bayesian neural networks bayesian neural networks combine artificial neural networks with bayesian inference methods which account for the uncertainty of the model parameters and the model prediction results and can obtain interval prediction results langevin dynamics is an mcmc algorithm that integrates gradient and gaussian noise in updating the parameters of bayesian neural networks enabling faster convergence of markov chains to the posterior distribution 2 3 1 model and prior let a denote a single variable time series that is generated by a signal plus noise model 3 y t f y t ε t w h e r e t 1 2 n where f y t is a neural network y t y t 1 y t 2 y t i is a vector of lagged values of y t i represents the number of inputs to the feedforward neural network and ε t is the noise component with ε t n 0 τ 2 t a feedforward neural network with one hidden layer is used to calculate f y t which is defined as follows 4 f y t a ξ o h 1 h φ h a ξ h i 1 i γ ih y t i where a represents the activation function the sigmoid function is used in this paper ξ h and ξ o represent the biases of the hidden layer and the output layer respectively φ h represents the weight set used to map the hidden layer to the output layer and γ ih represents the weight set used to map the input layer to the hidden layer the network contains a total of l i h 2 h o 1 parameters where i is the number of neurons in the input layer and h is the number of neurons in the hidden layer let θ γ φ ξ τ 2 γ and φ are the parameter vectors mapped from the input layer to the hidden layer and the hidden layer to the output layer respectively ξ is the parameter vector of the biases and τ is the variance of the noise assuming that the elements of θ are independent the weights and biases have normal prior distributions with a mean of zero and variance σ 2 additionally τ 2 is assumed to be an inverse gamma prior distribution with parameters v 1 and v 2 the joint prior distribution of the parameters can be obtained by the product of the prior distributions of the parameters chandra et al 2017 the likelihood function is given by the following 5 l y θ 2 π τ 2 n 2 e x p 1 2 τ 2 y t f y t 2 according to bayes theorem the posterior distribution of parameters is as follows 6 p θ y l y θ π θ 2 3 2 bayesian point and interval forecasts the mean of the predicted posterior distribution is chosen as the bayesian point estimate 7 e y n 1 1 m m 1 m y n 1 m where m is the number of samples the 95 credible interval obtained from the predicted posterior distribution is used as the result of bayesian interval estimation f y n 1 is defined as the cumulative distribution function of y n 1 8 p y y upper f y upper 0 95 9 p y y lower 1 f y lower 0 05 if y upper and y lower satisfy the above conditions then y lower y upper is the bayesian interval prediction result for y n 1 2 3 3 algorithm bayesian inference is used to calculate the parameters of the proposed model and the posterior probability distribution is usually used to represent uncertainty the statistical data for the parameters in the above formula are usually difficult to directly obtain therefore these parameters must be calculated by sampling from the posterior distribution the metropolis hastings algorithm is a widely used mcmc algorithm the proposed mcmc algorithm combines langevin dynamics and a single metropolis hastings algorithm the new value of θ is as follows 10 θ q n θ m σ θ where 11 θ m θ m r e y θ m e y θ m y t f y t m 2 e y θ m e θ 1 e θ l r represents the learning rate σ θ σ θ 2 i l and i l is an l l identity matrix then θ q can be expressed based on two steps a weight updating based on gradient descent with equation 22 b the addition of a certain amount of gaussian noise from n 0 σ θ the langevin dynamics for bayesian neural networks ldbnn algorithm is given in algorithm 1 the ldbnn algorithm involves gradients with gaussian noise in the parameter updating step compared with the random walk algorithm the ldbnn approach provides more accurate estimates of parameters algorithm 1 langevin dynamics for bayesian neural networks result obtain the posterior distribution of the parameters set max samples k repeat until k k 1 set hyperparameters σ v 1 v 2 r 2 obtain μ from n 0 σ μ 3 calculate the gradient θ m by eq 22 4 propose θ θ m θ m μ 5 calculate the acceptance probability α m i n 1 p θ p y q θ m θ q p θ m y q θ q θ m 6 obtain u from a standard normal distribution if α u accept and set θ m 1 θ else θ m 1 θ m end end 2 4 a new method of partitioning datasets the statistical distributions of various rainfall and runoff series are different due to factors such as climate change and human activities partitioning different datasets based on the same ratio may result in significant differences between the obtained training and test sets thus affecting the accuracy of predictions therefore we propose a new method of partitioning datasets based on a pso algorithm we regard the locations of the partitioned training and test sets as unknown parameters and apply pso to optimize the parameters our aim is to minimize the difference in mean and variance between the training and test sets obtained by partitioning the data the target function of the pso algorithm is the sum of the absolute value of the difference of the mean and the absolute value of the difference of the variance between the training set and the test set the population size was set to 40 the maximum number of iterations was 150 to ensure that the length of the training set is larger than that of the test set we set the parameter optimization range as 0 5 length data length data to search for the global optimal solution the specific details of the above process are shown in algorithm 2 algorithm 2 particle swarm optimization for partitioning datasets data univariate hydrological series result obtain the optimal solution for unknown parameters training sets data 0 x test sets data x length data a and b are the mean and variance of the training sets respectively c and d are the mean and variance of the test sets respectively f abs a c abs b d set target function f set unknown parameter x set scope of the parameter 0 5 length data length data set population size n set max iteration m procedure pso for each particle i initialize velocity v i and position x i for particle i calculate f and set pbest i x i gbest m i n pbest i m 0 while m m for i 1 to n update the velocity and position of particle i calculate f if f x i f p b e s t i pbest i x i if f pbest i f g b e s t gbest pbest i end end x g b e s t print x 3 case study 3 1 study area and data the yellow river basin originates in the bayan krai mountains on the tibetan plateau and finally flows into the bohai sea yang et al 2021 the yellow river is the second largest river in china after the yangtze river with a total length of approximately 5 500 km and a basin area of approximately 753 000 square kilometers the yellow river basin is 4 675 m above sea level with an average flow of 1774 5 m3 s the dividing point between the upper and middle reaches of the yellow river basin is in the town of hekou in inner mongolia and the dividing point between the middle and lower reaches is in peach valley in henan province throughout chinese history the yellow river and its coastal watershed have been the primary foundations of the chinese nation and the yellow river is known as the mother river the climate varies significantly in different areas of the basin from the origin of the yellow river to the bohai sea and the climatic zone changes from cold to temperate the yellow river basin receives abundant sunshine generally reaching 2 000 to 3 300 h throughout the year since the 1970s the annual precipitation in the yellow river basin has exhibited a decreasing trend xu 2001 because the yellow river basin supplies water to 12 of china s population zhang et al 2017a accurate and reliable forecasting is of great importance in this study monthly runoff data from four hydrological stations and monthly precipitation data from three meteorological stations in the yellow river basin were collected fig 1 the monthly runoff series lengths for the xh tdg mh and lm hydrological stations were 480 648 852 and 732 respectively the length of the monthly precipitation series was the same for the qs ys and ly meteorological stations all with 576 3 2 model development due to the limitations of neural networks most studies divide datasets into training sets validation sets and test sets validation sets are used for network adjustment to prevent overfitting bayesian neural networks are usually relatively robust to overfitting because they obtain the predictive distribution by integrating over the parameters rather than choosing a single point estimate chandra et al 2017 bayesian methods do not require the use of cross validation to avoid overfitting khan and coulibaly 2006 we only divided the dataset into training and test sets we propose a new method of dividing datasets based on the pso algorithm the two stage hybrid p cvee ldbnn model can be used to predict rainfall and runoff series the main modeling process of the two stage hybrid p cvee ldbnn model is as follows 1 the original monthly runoff and monthly precipitation data are partitioned using our proposed new dataset partitioning method to obtain training and test sets of different lengths 2 in the first stage the ceemdan algorithm is utilized to decompose the data series into distinct imfs with corresponding low high frequency and residual res components 3 in the second stage the vmd method is applied to decompose imf1 data into different variational modes vms 4 an ldbnn is used to predict imfs and res subsequences where the predicted imf1 component is obtained by summing the different vm subsequences 5 the prediction results of all subcomponents are superimposed to obtain the final prediction results to compare the effects of traditional methods and our proposed method of data partitioning based on the subsequent model prediction results a one stage hybrid cee ldbnn model was used to model and predict training and test sets of different lengths obtained from the two partitioning methods a two stage hybrid p cvee ldbnn model was then used to evaluate the impact of the vmd approach on the model prediction results the reliability of the credible intervals obtained from the t cee ldbnn model p cee ldbnn model and p cvee ldbnn model was assessed using the credible interval evaluation index fig 2 shows the flow chart of the above modeling process many studies have shown that the prediction accuracy of a hybrid model based on decomposition is higher than that of individual models abda and chettih 2018 chen et al 2021 he et al 2020 li et al 2018 due to the highly nonlinear nonstationary and strongly fluctuating characteristics of rainfall and runoff series the prediction accuracy of the standalone ldbnn model considered in this paper is poor and the model is not qualified for analysis tasks therefore the standalone ldbnn model is not used for comparison and its results are not shown in the analysis later in the paper in this paper a one step ahead prediction strategy was adopted during the testing period the ldbnn model was constructed using a three layer neuron arrangement i e input layer hidden layer and output layer the pacf method was applied to the decomposed submodes and the appropriate input sequence length was selected peng et al 2021 if the number of neurons in the input layer was 1 the number of neurons in the corresponding hidden layer was set to 5 otherwise the number of neurons in the hidden layer was set to 10 then the input and target data were normalized in the range of 0 1 a sigmoid function was selected as the neuron activation function for the ldbnn model our proposed method of partitioning datasets was applied to data from seven hydrological and meteorological stations and the results obtained after partitioning are shown in fig 3 for comparison the training and testing periods obtained with the traditional partitioning method are also shown in fig 3 3 3 performance evaluation the nash sutcliffe efficiency coefficient nse correlation coefficient r root mean square error rmse and mean absolute error mae are used as evaluation metrics to assess the accuracy of the mean of the posterior distribution of the bayesian point prediction results and they can be described as follows 12 nse 1 i 1 n o i p i 2 i 1 n o i o i 2 13 r i 1 n o i o i p i p i i 1 n o i o i 2 i 1 n p i p i 2 14 mae 1 n i 1 n p i o i 15 rmse 1 n i 1 n o i p i 2 where o i and p i represent observed and predicted values respectively o i and p i represent the averages of the observed and predicted values respectively and n is the length of the data sequence the containment ratio cr the average asymmetry degree s and the average bandwidth iw are used to assess the reliability of credible intervals at a credibility level of 95 for the bayesian interval prediction results xiong et al 2009 the cr indicates the ratio of the number of observations in the prediction interval to the total number of observations and can be expressed as a percentage the value of cr ranges from 0 to 1 an ideal prediction interval yields a cr equal to 1 or 100 therefore the higher the cr value is the greater the proportion of observed values within the forecasted interval s is used to describe the geometric structure of the prediction bounds the ideal prediction interval is symmetric and optimally the difference between the upper limit of the interval and the observed value will be approximately equal to the difference between the lower limit of the interval and the observed value iw indicates the average width of the prediction interval 16 iw i 1 n p up i p low i n 17 cr i 1 n c o i n c o i 1 p low i o i p up i 0 e l s e 18 s 1 n i 1 n p up i o i p up i p low i 0 5 where p up i and p low i are the upper and lower prediction bounds respectively 4 results and discussion we compared the proposed method of partitioning data and the traditional partitioning method based on the prediction results to simplify the process we used a one stage cee ldbnn model to simulate and forecast monthly runoff and precipitation data table 1 shows the prediction results of the cee ldbnn model based on the proposed method of partitioning datasets p cee ldbnn and these results were compared with those of a cee ldbnn model based on the traditional partitioning method t cee ldbnn in the testing stage in general the performance of the p cee ldbnn model far surpasses that of the t cee ldbnn model for runoff and precipitation data for the datasets from meteorological stations the p cee ldbnn model yielded nse values of approximately 0 696 qs 0 642 ys and 0 752 ly which are approximately 0 187 qs 0 229 ys and 0 107 ly higher than the nse values produced by the t cee ldbnn model respectively for the monthly precipitation data from the three meteorological stations the r values obtained by the p cee ldbnn model were higher than the corresponding values obtained by the t cee ldbnn model this result supports the relatively low rmse and mae values the forecasting accuracy of the p cee ldbnn model can be easily concluded to be higher than that of the t cee ldbnn model for the datasets from the hydrological stations the nse and r values obtained for the p cee ldbnn model were better than the corresponding values obtained for the t cee ldbnn model this result is consistent with the conclusions drawn above notably we found that the rmse and mae values obtained from the t cee ldbnn model were slightly smaller than the corresponding values obtained from the p cee ldbnn model at the hx tdg and mh hydrological stations notably we believe that the rmse and mae as error indicators are influenced by peak values fig 3 depicts the lengths of the training and test sets derived from different partitioning methods for the data from seven hydrological and meteorological stations as shown in fig 3 the datasets from the hx hydrological station were divided using the proposed method and the test set data obtained were from october 1981 to december 1999 spanning a total of 219 months the traditional method was used to divide the datasets from the same hydrological station and the test set data obtained were from february 1992 to december 1999 spanning a total of 95 months the period from october 1981 to january 1992 a total of 124 months was the prediction period considered in the p cee ldbnn model but the period of no predictions was considered in the t cee ldbnn model the p cee ldbnn model provided predictions over 314 tdg and 230 mh months and the t cee ldbnn model provided predictions for only 129 tdg and 169 mh months the lengths of the test sets obtained by the proposed method were longer than those obtained by the traditional method thus for all three sites predictions in some periods were obtained with the p cee ldbnn model but not with the t cee ldbnn model due to the comparatively longer prediction time of the p cee ldbnn model we believe that some points with large flow values points with large flows in a given period predicted by the p cee ldbnn model but not considered by the t cee ldbnn model may deviate from the true values leading to slightly higher rmse and mae values this result is supported by the formulas for mae equation 29 and rmse equation 30 fig 4 shows scatter plots of observed runoff values and the value predicted by the t cee ldbnn model and p cee ldbnn model in the testing phase our focus was on analyzing the effects of the two dataset partitioning methods for data from the hx tdg and mh hydrographic stations therefore only scatter plots for these three hydrographic stations are shown in fig 4 for the hx fig 4 a and tdg fig 4 b hydrological stations the results of the t cee ldbnn model during the test period were concentrated in the lower left corner of the scatter plot inside the black dotted line indicating that the observed flow values in the test set were small however for the p cee ldbnn model some large observed flow values during the test period phase were not appropriately predicted resulting in underestimated prediction results we believe that this is the reason why the rmse and mae values obtained for the p cee ldbnn model were larger than the corresponding values obtained for the t cee ldbnn model in fig 4 c mh hydrological station more scatter points located outside the black dotted line with observed flow values greater than 90 were found in the test sets of the p cee ldbnn model than in the test sets of the t cee ldbnn model some peaks points with high flows between november 1991 and november 1996 were observed and the predicted values of the p cee ldbnn model were comparatively low for the mh hydrological station the rmse and mae values obtained for the p cee ldbnn model were only 0 59 and 0 94 higher respectively than the corresponding values obtained for the t cee ldbnn model in table 1 this deviation was smaller than that observed at the hx and tdg hydrological stations although the peak flows were larger at the mh hydrological station compared with the peaks in the test sets obtained with the traditional method larger peaks were present in the test sets obtained with the proposed method the rmse and mae are most influenced by peak values thus the p cee ldbnn model yielded slightly larger rmse and mae values than does the t cee ldbnn model the nse and r metrics are widely used to verify the prediction results of models and they can reflect model performance the nse is also the best objective metric for reflecting the overall fit of a hydrological curve legates and mccabe 1999 based on these metrics the performance of the p cee ldbnn model was better than that of the t cee ldbnn model and our proposed new method of partitioning datasets can improve the performance of prediction models to some extent as shown in table 1 the prediction accuracy of the t cee ldbnn model and p cee ldbnn model for the datasets from the tdg hydrological station was lower than that for the datasets from other stations for further analysis we list the hydrological statistics for the hydrological and meteorological station data used in this paper in table 2 due to the use of different division methods for the datasets the lengths of the training and test sets were different fig 3 and the corresponding mean and standard deviation of the training and test sets were also different as shown in table 2 the significant differences between the training and test sets affected the prediction results of the models with large values of d mean and d std the tdg hydrological station had the largest values of d mean and d std therefore the accuracy was lower than that of the other stations the values of d mean and d std obtained by partitioning the datasets using the traditional method were higher than the corresponding values obtained by the proposed method this result is consistent with the above conclusions the p cee ldbnn model performed better than the t cee ldbnn model and the differences between the training and test sets obtained with the proposed method of data partitioning were comparatively smaller which improved the performance of the model d mean indicates the difference between the means of the training and test sets d std indicates the difference between the standard deviations of the training and test sets the new method of data partitioning based on pso appears to perform better than the traditional method of partitioning therefore we applied the two stage hybrid p cvee ldbnn model to simulate and forecast the training and test sets based on pso the nse r rmse and mae values were applied to assess the predictive performance of the model and the evaluation index values are listed in table 3 compared with the values in table 1 the prediction accuracy of the two stage hybrid p cvee ldbnn model shown in table 3 was greatly improved the nse values obtained with the two stage hybrid p cvee ldbnn model were all above 0 9 and significantly higher than the nse values obtained for the t cee ldbnn model and p cee ldbnn model in table 1 this finding is consistent with the high r values and low rmse and mae values obtained in the analyses above the imf1 components obtained by ceemdan decomposition may be characterized by non stationarity and strong volatility thus using vmd to decompose imf1 components can greatly improve the accuracy of the forecasting results notably for the tdg hydrographic station the nse value increased from 0 202 to 0 916 an increase of more than 0 7 with this approach to intuitively show the results of the models for comparison the nse r rmse and mae values of the seven datasets are illustrated using radial column charts the evaluation index values of the three models for the seven datasets in one step ahead prediction are shown in fig 5 specifically the performance of the models is visually illustrated and the results are consistent with those of the previous analyses in this paper fig 6 a b and c show the scatter plots of the observed and predicted flows of the three models in the test period the scatter diagrams are helpful for analyzing the correspondence between observed and predicted flows the coefficient of determination r2 ranges from to 1 the coefficient of determination r2 is also referred to as the goodness of fit the larger the goodness of fit is the denser the scatter around the regression line conversely this indicates that more data points are scattered far from the regression line which means that observations cannot be accurately reproduced by the model in fig 6 a the t cee ldbnn model yielded coefficient of determination values of 0 38 and 0 44 at the tdg and ys stations respectively in fig 6 b the p cee ldbnn model yielded a coefficient of determination of 0 44 at the tdg hydrological station they yielded values of the coefficient of determination that are smaller therefore the scatter is farther from the regression line in fig 6 c the p cvee ldbnn model yielded coefficient of determination values that all reached above 0 9 and the scatter is densely distributed around the regression line fig 6 shows that the r2 value obtained for the p cvee ldbnn model was the highest followed by that obtained for the p cee ldbnn model additionally the value obtained for the t cee ldbnn model was the lowest therefore the p cvee ldbnn model can accurately reproduce the observations accurate and reliable peak value prediction is important for flood prevention and warning water resource planning and management and safety risk analysis comparing the points in fig 6 a and fig 6 b the points in fig 6 c are closer to the y x line indicating that the predicted values obtained with the p cvee ldbnn model are closer to the true values compared with the p cvee ldbnn model the p cee ldbnn model and the t cee ldbnn model predicted peak values relatively farther away from the y x line table 4 shows the absolute relative error values are yielded by the p cee ldbnn model and the p cvee ldbnn model for predicting ten peak values under one step ahead prediction note that we do not give the are values yielded by the t cee ldbnn model here the reasons are twofold 1 the length of the test set obtained by the traditional dataset partitioning method and the length of the test set obtained by the new dataset partitioning method were not the same and therefore the ten peak values were not the same we cannot directly calculate the comparison 2 as seen in table 1 and table 3 the p cvee ldbnn model had much better predictive power than the t cee ldbnn model moreover the t cee ldbnn model in fig 6 yielded peak values prediction points farther away from the best fit line we can visualize from fig 6 that the prediction ability of the peak values of the p cvee ldbnn model was better than that of the peak values of the t cee ldbnn model the are value is calculated by the observed peak value and the corresponding predicted peak value xie et al 2019 the p cvee ldbnn model yielded the largest are values of 14 6 hx 9 2 tdg 8 9 mh 10 lm 17 6 qs 17 ys and 16 1 ly furthermore the prediction errors of the peak values of the p cvee ldbnn model were less than 20 the t cee ldbnn model on the other hand yielded widely fluctuating are values and had six stations where the maximum are value exceeded 40 hence the p cvee ldbnn model yielded the least error for predicting peak values thus the two stage hybrid p cvee ldbnn model can significantly improve the peak value prediction accuracy the ldbnn approach can yield predictive distributions of model parameters and model outputs and the corresponding credible intervals can be calculated the term credible interval here is different from the classic statistical term confidence interval a 95 credible interval indicates that the posterior probability that the predicted outcome is within the credible interval is 95 compared with deterministic prediction models the ldbnn model can provide a prediction interval to obtain probabilistic forecasts of rainfall and runoff series fig 7 shows the hydrographs of observed runoff and the values predicted with the proposed model p cvee ldbnn model during the testing phase in addition the 95 credible intervals for the predicted runoff are illustrated table 5 shows the cr and s values of the proposed model and the other two models t cee ldbnn model and p cee ldbnn model for comparison the cr and s values were based on the results obtained for 95 credible intervals credible intervals can represent accuracy and reliability sahlin 2015 the larger the cr value is the greater the number of observations contained in the credible interval according to formula 32 the closer the value of s is to 0 the higher the degree of symmetry of the credible interval used for prediction the more symmetrical a credible interval is the more reliable the prediction results will be therefore the greater the number of observations contained in the credible interval the more symmetrical the upper and lower bounds of the credible interval will be with respect to the observations leading to high model accuracy and reliable prediction results a comparison of the t cee ldbnn model and the p cee ldbnn model suggests that the p cee ldbnn model yields higher cr values than does the t cee ldbnn model at the hx tdg mh lm qs ys and ly hydrological and meteorological stations the cr values obtained with the p cee ldbnn model were 73 4 76 4 43 7 51 8 69 4 52 5 and 57 2 respectively the cr values obtained with the p cee ldbnn model were 7 4 20 9 0 8 13 7 15 0 19 8 and 13 8 higher than those of the t cee ldbnn model respectively for all hydrological and meteorological stations in this paper the s values obtained for the p cee ldbnn model were lower than the corresponding values obtained for the t cee ldbnn model thus the p cee ldbnn model yielded more accurate and reliable prediction results than the t cee ldbnn model this finding is consistent with the conclusions obtained earlier in this paper although the p cee ldbnn model yielded higher cr values and lower s values than the t cee ldbnn model the highest cr value obtained for the p cee ldbnn model was 73 4 hx and the lowest cr value was only 34 37 mh compared with those for the p cvee ldbnn model the credible intervals obtained for the t cee ldbnn model and the p cee ldbnn model contain fewer observations making the results unreliable the s value obtained for the p cvee ldbnn model was between 0 15 and 0 29 which was significantly lower than the corresponding values obtained for the t cee ldbnn model and the p cee ldbnn model therefore the prediction results obtained with the p cvee ldbnn model were more reliable the average width of the credible interval is also an important indicator of the reliability of the prediction interval the iw values for the seven stations calculated with the p cvee ldbnn model are listed in table 6 when the cr values of two reliable intervals are relatively close the smaller the iw value is the more reliable the interval prediction result zhang et al 2009 however the cr values obtained by the t cee ldbnn model and the p cee ldbnn model were much lower than the corresponding values obtained by the p cvee ldbnn model and the cr values of some stations are less than 50 a value of s less than 0 5 indicates that on average the hydrograph lies within the credible interval xiong et al 2009 however except for those for the hx hydrological station the s values of the t cee ldbnn model were all greater than 0 5 at more than half of the stations the p cee ldbnn model yielded s values greater than 0 5 which was not what we expected we believe that the comparison of model interval widths is not practically meaningful in the case of large differences in cr values therefore we do not list the iw values of the t cee ldbnn model and the p cee ldbnn model here the cr values obtained by the p cvee ldbnn model were comparatively higher the s values were less than 0 5 and the reliability of the prediction results was high 5 conclusions accurate prediction and reliable interval forecasts of rainfall and runoff series can provide a basis and reference for decision makers in water resource management and risk assessment in a changing environment in this study we constructed a two stage hybrid p cvee ldbnn model based on a new method of partitioning datasets the model first decomposes hydrological series into different imf components using the ceemdan method and then performs a second decomposition of the imf1 components using the vmd method each component is predicted separately by applying the ldbnn model and the sum of the prediction results of the components is the final prediction result of the model our proposed two stage hybrid p cvee ldbnn model was applied to four hydrological stations and three meteorological stations in the yellow river basin for forecasting with a one month ahead period the p cee ldbnn and the t cee ldbnn models were used as benchmark models hydrological series often display nonlinear nonstationary and strong fluctuations and significant differences exist among different hydrological series if the same ratio is used to partition different hydrological sequences large significant differences maybe emerge between the obtained training and test sets which can lead to unstable prediction results in most previous studies the same ratio 80 training to 20 test was used to partition datasets and we call this approach the traditional method we propose a new method of partitioning datasets based on a pso algorithm to partition hydrological sequences with different statistical properties into different proportions for training and testing the new method of data partitioning and the traditional method were used to partition datasets from seven hydrological and meteorological stations selected in this paper the one stage cee ldbnn model was applied for simulation and prediction based on the training and test sets obtained with the two methods by comparing and analyzing the nse r rmse and mae values obtained for the t cee ldbnn model and p cee ldbnn model we can draw the first conclusion the p cee ldbnn model performs better than the t cee ldbnn model relatively large significant differences between the training and test sets can affect the ability of a model to make predictions our proposed method of data partitioning based on pso can improve the model prediction accuracy to some extent to address the problem of low accuracy in predicting the imf1 components obtained by decomposition in the ceemdan method based on the first conclusion a second decomposition of imf1 components was performed with the vmd method based on the p cee ldbnn model to obtain the p cvee ldbnn model the evaluation metrics for the t cee ldbnn model and p cee ldbnn model and the corresponding scatter plots support the second conclusion the prediction accuracy of the two stage hybrid p cvee ldbnn model is significantly higher than that of the t cee ldbnn model and p cee ldbnn model and the p cvee ldbnn model provides more accurate prediction results for peak flows vmd is a reliable method that can significantly improve the accuracy of model predictions bayesian methods can provide credible intervals for obtaining predictions by comparing the t cee ldbnn model p cee ldbnn model and p cvee ldbnn model we identified a third conclusion the credible intervals obtained with the p cvee ldbnn model are more symmetric with respect to the observed flows and include higher proportions of observed flows than those of the other models in summary the two stage hybrid p cvee ldbnn model yields reliable interval prediction results and excellent point prediction results thus providing a basis for water resource planning management and risk assessment moreover the two stage hybrid model is more advantageous for areas with limited data on the initial conditions needed to construct the physical model in future studies multiscale e g 3 or 6 months ahead or shorter term e g daily or hourly rainfall and runoff forecasts can be considered because these types of series are important for hydrological series forecasting research credit authorship contribution statement hanbing xu conceptualization methodology visualization software writing original draft songbai song conceptualization methodology data curation writing review editing supervision validation tianli guo writing review editing investigation huimin wang writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work is jointly supported by national natural science foundation of china grant nos 51479171 52079110 the authors also wish to express their cordial gratitude to the editors and anonymous reviewers for their illuminating comments which have greatly helped improve the quality of this manuscript 
3223,climate change and increasing urbanization have worsened urban floods and other water problems and it is generally accepted that the optimization of the spatial layout of low impact development lid practices is a promising solution existing studies have focused on coupling the hydrological model with optimization methods but little is known about the effects of the spatial discretization of the model on the optimization results in this study the scale effects were examined in a case study in guangzhou china four models at various spatial discretization levels namely models r1 r2 r3 and r4 with an average unit size of 0 022 104 0 405 104 2 43 104 and 4 865 104 m2 respectively were constructed to investigate the performance difference caused by delineation scales the results show that the highest resolution model r1 one unit represented one type of land cover provides more cost effective layout schemes but during a heavy rainstorm the solution set provided by a coarser model are very similar to that generated by the finest model r1 a coarser model can provide wider solution sets but most of these schemes overshadow some types of lid practices such rain garden and bio retention cell which also have a critical role in urban heat mitigation air quality improvement and so on due to the cumbersome work of catchment subdividing and time consuming optimization process a coarse model would be a good substitute to a large area these findings provide new insights on how to achieve better performance by subdividing the catchment at a proper scale when optimizing the spatial layout of lid practices keywords low impact development layout optimization spatial discretization scale effects data availability data will be made available on request 1 introduction rapid urbanization not only alters the natural hydrologic cycle but also changes the surface conditions for example replacing vegetation with anthropogenic materials such as concrete asphalt brickwork and metal jacobson 2011 these impermeable materials hinder water infiltration into the soil increase the formation of runoff and the potential for flooding and worsen the contamination of water runoff bonneau et al 2017 li et al 2020 meanwhile it is projected that the impact of global warming will be more severe on urban areas resulting in more frequent and intense extreme rainfall events lai et al 2020 javadinejad et al 2021 ostad ali askari et al 2020 papalexiou and montanari 2019 yilmaz et al 2014 zhang 2020 which will further complicate urban water management marlow et al 2013 traditional centralized measures have the strength of removing more surface runoff rapidly than nature based solutions but the construction and renovation work of grey infrastructures are high priced and it is difficult to offer multiple benefits such as pollution load degradation heat down biodiversity and urban amenity alves et al 2019 as such nature based strategies have gradually been implemented in many countries such as low impact development lid in the usa the sustainable urban drainage system suds in the uk water sensitive urban design wsud in australia and sponge city in china fletcher et al 2015 these solutions have appellations but share the same goal that is to recover or mimic the natural water cycle in urban areas through nature based technologies chan et al 2018 one of the most widely used solutions is lid practices which includes green roofs rain gardens vegetative swales and porous pavements dietz 2007 considering their multiple benefits policymakers prefer lid measures to control surface runoff at its source and mitigate the impacts of urbanization the variety of lid facilities is considered to be limited dietz 2007 while the number of possible spatial combinations is deemed to be unlimited furthermore the budget constraints increase the complexity of the selection and placement of lid facilities as it is expected to achieve higher level of runoff reduction at lower cost there is an urgent need to seek cost effective lid layout schemes by balancing conflicting hydrological goal and economic concern chui et al 2016 kuller et al 2017 zhang and chui 2018 the exhaustive approach is unlikely to identify the optimal spatial layout for it will take a very long time in most cases to achieve convergence instead with advancements in computational abilities simulation optimization approaches emerged and multi objective optimization algorithms mooas have been widely used to identify the optimal solution set with several competing objectives damodaram and zechman 2013 deb 2014 gunantara 2018 javadinejad et al 2019 ostad ali askari et al 2017 presently these mooas have been successfully coupled with hydrologic hydraulic models to optimize lid spatial layouts islam et al 2021 pour et al 2020 a classic case is the application of the system for urban stormwater treatment and integration sustain embedded in arcgis with general applicability for providing cost efficient planning schemes under defined constraints jia et al 2015 lai et al 2007 lee et al 2012 mao et al 2017 the essence of this model is the coupling between the storm water management model swmm hydrologic simulation program fortran hspf and the non dominated sorting genetic algorithm nsga ii lee et al 2012 however one inescapable limitation for utilizing this model is that when it is applied in a large region with complicated pipe networks its computational burden may be enormous easily resulting in a program crash to cope with this deficiency many researchers have attempted to embed hydrological models into the mooa framework by coding according to their own research needs for example analogous to sustain the linking of swmm and nsga ii was widely applied xu et al 2017 oraei zare et al 2012 zhang et al 2013 zeng et al 2020 furthermore other algorithms combined with hydrological models have been developed to improve the optimization performance for example analogous to sustain the linking of swmm and nsga ii was widely applied xu et al 2017 oraei zare et al 2012 zhang et al 2013 zeng et al 2020 furthermore other algorithms combined with hydrological models have been developed to improve the optimization performance for instance the long term hydrologic impact assessment low impact development 2 1 l thia lid 2 1 model was coupled with a multi algorithm genetically adaptive multi objective approach liu et al 2016 the swmm was linked to the multi objective antlion optimization algorithm mani et al 2019 the modified particle swarm optimization algorithm duan et al 2016 li et al 2019 the multi objective shuffled frog leaping algorithm liu et al 2019 marginal cost based greedy strategy xu et al 2018 or the third evolution step of generalized differential evolution gde3 li et al 2022 when it comes to the optimization of lid layout configuration most researchers have highlighted the effects of different mooas on the optimization performance and conducted them at coarse scale without considering the impact of modelling resolution it is well known that swmm is often applied to simulate the hydrological effects of lid facilities elliott and trowsdale 2007 qin et al 2013 wu et al 2017 yazdi et al 2019 the basic computing units of the runoff component of swmm are usually the smaller sub catchments divided from the study area and the main principle of the subdivision is the spatial variabilities of the landscape such as the distribution of land use types slopes and drainage features gironás et al 2010 rossman and huber 2015 sometimes there is no uniform standard for the delineation of sub catchments and they are mainly determined by modelers so that the scale may be coarse or fine the uncertainty due to the spatial discretization may affect the surface runoff calculation and the lid layout optimization many studies have examined the difference in output from swmm at various spatial scales defined here as the resolution of the sub catchments in terms of the smaller catchments it was found that the runoff volumes are relatively insensitive to the spatial resolution goldstein et al 2016 shaneyfelt et al 2021 zeng et al 2022 for larger watersheds sun et al 2014 observed that the model parameters calibrated based on a fine discretization diminished the uncertainty of the overflow forecasts compared to those of the coarse delineation and chang et al 2019 found that the coarsening of the scale led to a decrease in the total and peak runoff similarly the modeling scale may impact the optimization process of the spatial allocation of lid practices as the decision variables change with the spatial discretization methods as lid devices are usually scattered across various sites to mitigate runoff a fine scale primarily that a sub catchment only represents one type of land cover rossman 2010 can reduce the uncertainty of the runoff reduction effects between the actual engineering design and the model output it is also helpful for the translation of the optimization results into practical measures randall et al 2019 but modeling at finer scales flat pitched rooftop pavements parking lot etc involves cumbersome work in subdividing data processing and computation chang et al 2019 accordingly the scale issue of the lid layout optimization is a crucial problem in the acquisition of an ideal solution however as discussed earlier previous studies conducted by simulation optimization approaches mostly employed a coarser model subdivision that is lumping diverse land use types into single sub catchment for simplification to alleviate the computation burden in the optimization process exceptionally randall et al 2019 subdivided the study area into 85 937 sub catchments by each separate land cover polygons for the convenience of lid allocation but this study merely examined the hydrological effects under four different lid model scenarios bach et al 2013 quantified the impact of block size on the selection of decentralised stormwater management options using a planning algorithm instead of hydrological simulation to the best of our knowledge there is no peer reviewed literature that investigates the impact of the modeling resolution on the hydrological performance of spatial configurations of lid practices optimized by mooa to bridge the research gap described above this study aims to explore how the optimal cost effective curves i e the distribution between investment and runoff reduction rate change with the scale of the model subdivision under various rainfall conditions as such swmm was selected for modeling at four spatial discretization levels and then coupled with the gde3 finally we ran them under three design storms with return periods of 0 5 5 and 20 years by understanding the modeling scale effects the findings of this study can provide guidance to modelers and planners hoping to achieve a more cost effective lid layout which can reduce more runoff at a lower cost 2 materials and methods 2 1 study area and data source the changban industrial campus cic in guangzhou city 23 8 n 113 16 e southern china with a total area of 9 7 ha fig 1 was selected as the study area the area was affected by the subtropical monsoon and the annual average precipitation was more than 1720 mm with heavy rainfall events occurring commonly chen et al 2021 there is a mountain outside the study area which is located to the north of the study area li et al 2022 and its runoff formation process is rapid and rich during periods of intense rainfall furthermore the cic is located at a low lying region with massive impervious surface where large runoff volumes would easily accumulate and the load of rainwater was prone to exceed the drainage capacity this highly urbanized area featured commercial buildings with flat or pitched roofs roads parking lots and unexplored territories as such this area was prone to flooding due to the synthetic action of substantial rainfall significant inflow runoff low lying terrain high impervious rate and low drainage capacity for example on 10 may 2016 a heavy rain hit this area causing severe flooding destroying the neighboring subway station several roadway sections and some low rise buildings and causing extensive property damage therefore response options and mitigation measures for reducing the loss caused by flooding is urgently needed in the cic considering the vulnerability of such areas to flooding the current land use type the existing building layout and the topography it is important to adopt the lid layout optimization to such areas to lessen the load of rainwater the data required for modeling included the land use type as well as a digital elevation model dem with a resolution of 5 m and a drainage network layer lastly updated in 2010 all of which came from the local land and resources bureau and water supplies bureau an automatic rain gauge and a flow meter were installed at the outfall of the watershed see fig 1 to record rainfall and runoff processes for calibration and validation of the swmm model 2 2 overview and scenarios in this study we developed four swmm models with sub catchments delineated at four different spatial resolutions including a high resolution model r1 and three progressively coarser low resolution models r2 r3 and r4 the storm events with return periods of 0 5 5 and 20 years were also input into each model as the rainfall series respectively thereafter inspired by li et al 2022 who pointed out that the third evolution step of generalized differential evolution gde3 performs better these models with various rainfall events and discretization scales were embedded to gde3 kukkonen and lampinen 2005 zelinka et al 2012 to optimize the lid layout finally the data was evolved for 200 and 500 generations respectively the straightforward coupling method and the optimization procedure are shown in fig 2 2 3 swmm model 2 3 1 model setup at four resolutions the us epa storm water management model swmm composed of runoff and routing modules has been widely used for dynamic rainfall runoff simulations in urban areas there is a lid control module for evaluating the effects of lid practices on runoff reduction rosa et al 2015 as such we selected the swmm to carry out the lid layout optimization and eight types of commonly used lid techniques including the rain barrel green roof rooftop disconnection bio retention cell rain garden porous pavement infiltration trench and vegetated swale one for many and one for one methods were used to deploy the lid control rossman 2010 the one for many method assigns one or more controls to an existing sub catchment and the lid practices act in parallel under this option the one for one method occupies the whole sub catchment by one type of the lid control and it also allows series connection of lid practices in diverse sub catchments shoemaker et al 2009 has proved that the differences in the simulation results between the distributed and aggregated lids were extremely smaller if the area of sub catchment less than 1 035 106 m2 as the total area of the study area is only 9 7 ha the impact of the connection method of lid measures on its efficacy can be neglected the selection of method mainly depended on the delineation approach of the sub catchments when we subdivided the study area at a fine scale with each sub catchment representing one land use type the one for one method was suitable for allocating lid practices conversely a sub catchment delineated at a coarse scale was always comprised of multiple land use types which suited the one for many method to place the lid facilities it is worth noting that the decision variables of mooa which were defined as the number area of each type of lid facility in each sub catchment as well as their constraints varied with partitioning methods and the distribution of the optimal solution sets changed accordingly thus we developed four versions of swmm model at different spatial resolution levels namely models r1 r2 r3 and r4 from fine to coarse scale to investigate the impacts of spatial scale on the allocation optimization of lid practice there are two common approaches applied in the urban sub catchments delineation geometric method based on nodes and artificial subdivision huang and jin 2019 the former one automatically generates sub catchments from the manholes of the urban drainage network by voronoi thiessen polygon it is time saving but impractical for example a building may be divided into two different sub catchments the latter one involves a manual sub catchment delineation with a comprehensive consideration of the distribution of land cover elevation and pipeline network which is time consuming but more realistic as such we selected the latter method to carry out the subdivision of sub catchments and finely divided model r1 into 436 sub catchments at the microscale according to landscape architects space requirements and design specifications bach et al 2018 as shown in fig 3 a one unit in model r1 always represented a specific land use type and the one for one method was used to deploy the lid practices spatially table s1 supplementary material shows the types of lid practices suitable for the specified land cover and a final list of suitable lid types for each unit would be determined following the field study based on the discretization of model r1 these micro sub catchments were merged into 24 medium sub catchments to form model r2 which were then combined into four large sub catchments to shape model r3 and were integrated into two larger sub catchments to structure model r4 fig 3b as the sub catchments in models r2 r3 and r4 always consisted of diverse land use types the one for many method was the best choice for deploying the lid controls in swmm the suitable types of lid practices for each sub catchment were summarized based on those in model r1 the spatial scales of these four models are different but they have the same pipe network system that is comprised of 169 junctions one outfall and 167 conduits fig 1 the flow direction of each sub catchment was assigned to a manhole located in the main drainage channel or near the outlet of the sub catchment 2 3 2 design storms this study also considered the effects of the various rainfall conditions on the layout optimization of the lid practices as lid practices are always designed to resist a low return period storm event and the efficiency of lid practices decreased significantly when the return period of the design storm is greater than 20 years mei et al 2018 three rainfall events with 0 5 5 20 yr return periods were selected to simulate the hydrological performance according to intensity duration frequency idf relationships in guangzhou the basic formula of rainfall intensities could be described by formula 1 based on formula 1 the rainfall intensities of a two hour duration and the return period of 1 in 0 5 5 and 20 years were selected to simulate the probable flooding situations as summarized in table 1 1 q 167 a t b n where q is the rainfall intensity t is the rainfall duration and a b and n are the constants determined by the location of the research region and the return period of the rainfall we selected the rainfall pattern with unimodal shapes middle peak keifer and chu 1957 marsalek and watt 1984 which is widely applied in current engineering practices to distribute the rainfall intensity temporally liu et al 2015 the time to peak ratio could be expressed as r t p t d where t p is the time before the peak value and t d is the duration of total rainfall the value r is an essential parameter in design storms and it is empirically fixed at 0 48 in guangzhou according to historical storms the temporal resolution is 1 min and the equations of the design storm could be expressed as formula 2 2 i t a τ ρ b n 1 n τ τ ρ b where τ t p t and ρ r for t t p τ t t p and ρ 1 r for t t p i t is the rainfall intensity at time t and the constants a b and n correspond to the constants a b and n in formula 1 which differ with the various return periods table 1 2 3 3 calibration and validation there are two types of parameters namely measured and empirical parameters in the swmm model the measured parameters are the area width slope and impervious percentage of each unit and they are determined by the topography and land cover type in this case the area and slope were calculated automatically in arcgis the width of the sub catchment was calculated by taking the square root of the sub catchment area assuming each sub catchment to be square in shape bisht et al 2016 and the impervious percentage was computed based on the distribution of land use as the sub catchments in model r1 had the highest resolution the impervious percentage of model r1 could be determined quickly and accurately and the impervious percentages of models r2 r3 and r4 were calculated based on that of model r1 the invert elevation and the maximum depth of each junction were other measured parameters that were extracted from the municipal pipe network database as for the empirical parameters the manning value of the surface the storage depth and the horton based infiltration indicators were initially established by referring to the adjacent catchment area chen et al 2018 zeng et al 2019 and the swmm manual rossman and huber 2015 the recommended ranges and values of calibrated parameters for swmm are shown in table s2 the infiltration rate is very important in model calibration as the soil in guangzhou is relatively moist during the rainy season from june to october consequently the manning value the depth of depression storage on impervious pervious area and maximum minimum infiltration rate within the recommended ranges of parameters were adjusted to improve the model accuracy generally one parameter was modified at a time when other parameters were fixed and we repeated this process until the requirements of nse values were met as the model was well calibrated after these steps it was not necessary to adjust the characteristics width the observed rainfall event from 2 10 of jun 7 to 19 55 of jun 8 in 2018 was used to calibrated model r3 and then another measured event from 12 25 of aug 28 to 5 35 of aug 29 in 2018 was applied to validate the nash sutcliffe efficiency nse value was greater than 0 7 for both calibration and validation periods and it is generally acceptable of nse greater than 0 5 moriasi et al 2007 nash and sutcliffe 1970 the shape and timing of the simulated hydrograph showed good agreement with the observed data with very slight differences at the peaks as shown in fig 4 next the validated parameter values of the low resolution model r3 were used to calibrate and validate the model r1 r2 and r4 it was found that the simulated hydrographs of models r1 r2 and r4 almost coincided with that of model r3 fig 4 and all nse values were greater than 0 7 table 2 this indicates that the level of sub catchment discretization has little impact on the overall model performance which is supported by previous studies on the scale effects shaneyfelt et al 2021 goldstein et al 2016 ghosh and hellweger 2012 as such models with various spatial resolutions perform well in the hydrological simulation and they can be considered for further study in scale effects the calibration parameter values were set as follows the manning s value and depression storage s depth of the impervious area were 0 011 and 0 5 mm and those of the pervious parts were 0 24 and 1 mm respectively the maximum and minimum infiltration rates were 10 mm h and 1 25 mm h respectively and the decay constant value was 4 and manning s value of the channel was between 0 01 and 0 01 2 4 multi objective optimization algorithm gde3 2 4 1 definition of optimization problem in this study we adopted the gde3 to investigate the scale effect on the distribution of the optimal solution sets of the lid layout the multi objective optimization problem deb 2014 considered here consisted of finding a set of decision variables x x 1 x 2 x n corresponding to the number area of each lid practice type in each sub catchment which minimized the total annual budget f 1 x and maximized the runoff reduction rate f 2 x expressed as formula 3 improvements in hydrological performance always result in increased investment so the optimal solution does not exist and the mooa is always employed to balance these conflicting goals to get a set of optimal solutions min f 1 x j 1 m a 1 j a 2 j and max f 2 x runof f baseline r u n o f f lid runof f baseline where a 1 j and a 2 j are the annualized construction cost and the annual maintenance cost huang et al 2018 of the lid practice j respectively m is the number of varieties of the lid facilities that can be used at a sub catchment runof f baseline and runof f lid are the simulated runoff in the scenario without and with the lid layout respectively once the construction of the lid practice was completed yearly maintenance was required over its entire life for the regular operation of the lid facility as the lifetime varied with the type of lid practice table 3 we adopted the total annual budget as an economic performance indicator the initial construction cost p j was uniformly transformed into the annualized construction cost a 1 j over a n j year project lifetime at an annual real interest rate of i which could be expressed as equation 4 4 a 1 j p j i 1 i n j 1 i n j 1 the one for one method to place lid practices rossman 2010 was always applied in models with fine resolution sub catchments and landscape architects and urban planners were invited to identify the suitable lid sites and subdivide the study area into small units based on the site feasibility table s1 landscape architecture and local construction guidelines bach et al 2018 there were several alternative types of lid implementation on a site but only one could be selected at one run accordingly the decision variables x x 1 x 2 x n referred to the selected type of the lid practice in each unit the type of lid practice selected for the n th sub catchment is x n and it was chosen from a list of types of lid practices that were suitable for the n th sub catchment correspondingly the range of x n was one of the values in 0 1 t where 0 represents no lid facility was built in the n th sub catchment t is the sum of the types of lid practices that can be placed in sub catchment n and the codes from 1 to t represents different types of lid practices such as in terms of the sub catchment made up of a flat roof x n 0 1 2 in which 1 represented green roof and 2 represented rain barrel this means that the lower bounds of all decision variables were set to 0 while the upper bounds depended on the number of the types of lid practices that were feasible to build in the corresponding sub catchment however the one for many method was more suitable for the model with coarse resolution sub catchments there were several varieties of land cover in a sub catchment so it could simultaneously contain multiple types of lid facilities as such the decision variable was altered to the area of each type of lid practice at each sub catchment expressed as x x 11 x 12 x 1 k x m 1 x m 2 x mk where k is the type of lid practice m is the sub catchment number there are two constraints one is that the value of x mk must not be greater than the suitable area in the m th sub catchment for the lid practice k and the other is that the sum of the area of each type of the lid practice allocated at the m th sub catchment x m 1 x m 2 x mk must be less than or equal to the total area of sub catchment m 2 4 2 development of gde3 numerous studies have demonstrated that there is no single algorithm outperforming the others huo et al 2016 keshavarzzadeh and ahmadi 2019 quresh et al 2019 vargas et al 2021 and the preferred algorithm depends on the definition of the optimization problem such as the number of decision variables and constraints since the goal of this study is to examine the scale effect based on the study conducted by li et al 2022 the third evolution step of generalized differential evolution gde3 was chosen to optimize the placement of lid practices due to its prominent mutation and crossover operators the overall structure can be expressed as follows 1 initialize the parent population of the size np randomly with values from the specific range as the decision variables in model r1 changed within a small number of discrete values and there was no constraint the initial values of parent population was generated randomly for models r2 r3 and r4 the ranges of decision variables consists of a large number of discrete values more importantly there were many solutions that did not satisfy the defined constraints as such values that located at one quarter or one fifth of the ranges while meeting the restrictions were used as initial solution to ensure the smooth progress of the optimization 2 repeat until the termination condition is satisfied a generate offspring population after repeating np times i select a point from the parent population ii generate a new offspring point with mutation and crossover iii update the impervious rate and width for a new offspring point rossman 2010 b combine the parent and offspring populations c select the best np points for the subsequent evolution according to fast non dominated and crowding distance sorting the outstanding characteristic of the gde3 is its mutation and crossover operators which act on each element of each solution through internal loops the gde3 is an extension of the differential evolution de algorithm whose crossover and mutation operations are determined by the crossover rate cr 0 1 and the mutation factor f r p g is a population of np solution vectors sets of decision variables x i g in a generation g and i 1 2 3 n p is a vector index each x i g is an n dimensional vector and x j i g is its j th element i 1 2 3 n three solution vectors namely x p 1 g x p 2 g and x p 3 g that are mutually different and different from x i g are randomly selected from the parent population x j p 1 g x j p 2 g and x j p 3 g were the j th elements of x p 1 g x p 2 g and x p 3 g respectively a trial element y j i g is generated by crossover operations and a trial vector y i g is obtained after the loop see fig s1 in the supplementary material 3 results 3 1 performance differences between generations in this study a population size of 200 was evolved for generations 200 and 500 to examine how generation affected optimization results of models at various resolutions as the solution sets were discrete the solutions under various scenarios did not always have the same cost or reduction rate for the convenience of data analysis when calculating the difference in the reduction rate between the different scenarios the two points with the closest cost were selected for comparison the average percent differences in the reduction rate with respect to 200 generations for 500 generations avpd ge values are listed in table 4 for model r4 with the lowest resolution the avpd ge values are relatively minor less than0 38 indicating that the front curve remains stable when the generation reached 200 and further evolution is unnecessary the avpd ge values rise with the increase in resolution the avpd ge values for higher resolution models r1 and r2 range from 1 62 to 5 79 which indicates that models at finer scale are more sensitive to generations the number of decision variables for models r1 r2 r3 and r4 are 430 192 32 and 16 respectively as a result this may be explained by the fact that models at finer scales have more decision variables requiring more generations to yield a stable optimal solution set additionally it is worthwhile to note that as shown in table 5 it takes significantly longer to optimize higher resolution models however while model r1 has twice as many decision variables as model r2 the difference in processing time is relatively small this is because for model r2 the range of decision variables is much boarder and there is a requirement to select solutions that satisfy the constraints during the optimization process 3 2 scale effects on the lid layout optimization following the results in section 3 1 it is hypothesized that the solution sets reach a more stable state after 500 generations these results were then used to analyze the effects of modeling scales on the lid layout optimization under the rainfall events of the return periods of 0 5 5 and 20 years as shown in fig 5 it is evident that the distribution ranges of solutions produced by models r1 and r2 are the narrowest regardless of the type of rainfall event but most runoff reduction rates computed by model r1 are higher than those computed by other models particularly for the 0 5 yr and 5 yr rainfall events it is suggested that for the low intensity rainfall events the layout schemes of lid practices optimized by model r1 in which one for one methods are applied to deploy lid practices spatially could reduce runoff more effectively at the same cost point in addition the front curve for model 4 has the greatest coverage and most cost effective schemes followed by model 3 and finally model r2 this result indicates that when applying the one for more method to place lid practices the coarser the sub catchment resolution is the better the performance of optimizing lid layouts would be it might also be related to the variation in the number of decision variables since the number of decision variables decrease as the spatial scale become coarser more importantly as the return period of rainfall events increase the differences between the front curves of models r1 r2 r3 and r4 become smaller and tend to merge together that is the modelling scale has less impact on optimization layout of lid practices under high intensity rainfall events based on each solution on the front curve see fig 5 the total area occupied by each type of lid practice was calculated and plotted against the resolution of the sub catchment discretization as shown in fig 6 outliers are solutions that were numerically out of the ordinary apart from outliers the area of lid practice generally varies within the interquartile range for instance under the 0 5 yr rainfall event the total area of green roofs ranges from 0 to 1 3 104 m2 when optimizing model r1 this spread however changes to 0 to 0 4 104 m2 0 to 2 2 104 m2 0 to 2 3 104 m2 when optimizing model r2 r3 and r4 it can be seen that the variation range of areas occupied by each type of lid facility for model r1 is smaller in comparison with other models this is in agreement with the fact that the spread of optimal solution sets of model r1 is also smaller besides for model r1 there is few rain barrel and rooftop disconnection applied in the layout design and with the increase of return period of rainfall event the variation range of rain garden and bio retention cell is low whereas the distributions of area of green roof vegetated swale infiltration trench and porous pavement change dramatically in the model r2 the area covered by rainfall barrels rooftop disconnection rain gardens and bio retention cells is little and most of the space is reserved for infiltration trenches and porous pavement for models r3 and r4 the variation ranges of green roof vegetated swale infiltration trenches and porous pavement is broad and more rooftop disconnections and rain garden were used in the layout design the results indicate that models with varying resolutions prefer different type of lid practices to optimize their layout schemes 3 3 lid layout at a specific target as can be seen in fig 5 the maximum runoff reduction rate for model r1 is about 60 for the purpose of ensuring a clear understanding of lid layout schemes a 60 runoff reduction was selected to analyze the difference of the spatial distribution of lid facilities for models r1 r2 r3 and r4 as illustrated in fig 7 in each sub catchment a pie chart was created to illustrate the relative proportion of each type of lid practice from the layout map of model r1 it is clear what type of lid practice should be located where in comparison with other models the layout generated by model r1 can serve as a design scheme for implementation in a practical setting nevertheless from the layout of model r2 r3 and r4 we can only estimate the proportions of each type of lid practice to be deployed within each sub catchment additionally layouts for a 20 yr rainfall event were applied to two real rainfall events described in section 2 3 3 and and a 50 yr design rainfall event in order to analyze differences in hydrological responses between optimized models at various discretization scale table 6 shows that the differences in the reduction rate between the schemes of each model are very small less than 1 46 during a 50 yr rainstorm while the difference during two real rainstorms varies within 6 33 the scheme optimized by model r4 performs the worst regardless of the type of rainfall besides the layouts generated by models r1 and r2 are able to capture more runoff during the rainfall events of jun 7 and aug 28 respectively it can be found that the reduction rates of model r2 are relative high under these two ocnditions but its budget is also the highest at 61 25 104 usd in other words the optimal layout of model r2 achieves a similar runoff control target 60 at a higher cost than other models under a 20 yr rainstorm but it can capture more runoff under the rainfall events with different shapes this indicates that the hydrological performance of lid layout are sensitive to the patterns of rainfall events 4 discussion 4 1 implication and cause analysis as mentioned previously most studies have focused on embedding the swmm model into various mooas to improve the performance of the layout optimization of lid facilities while ignoring the scale effects caused by the spatial delineation of the swmm model by contrast the present study reveals the impact of sub catchment resolutions on the layout optimization of the lid facilities during rainfall events with varying intensities which could facilitate the selection of an appropriate spatial discretization scale to optimize the layout of lid facilities the results show that for a given cost runoff control effectiveness of the layout optimized by each model decreases with the increase of return periods of rainfall event which is in line with previous studies that the performance of lid practices decreases with the growth of rainfall intensity lee et al 2013 qin et al 2013 yin et al 2020 this indicates that the main purpose of lid facilities is to resist small storm events it is worth noting that this decline is more pronounced in model r1 in the beginning it was suspected that this was related to the inadequacy of the evolution generations and we then used the gde3 to optimize model r1 under a 20 yr rainstorm for 1000 generations however the increase in generations only slightly enhances the performance of the layout optimization so the effect of generation on the distribution of optimal solution sets can be disregarded it was also observed that the coarser the spatial resolution the broader the spread of optimal cost effective solutions the finest model r1 could provide better lid layout schemes at a lower cost especially for low intensity rainfall events but for the models r2 r3 and r4 the cost effectiveness of solutions decreases with increasing spatial resolution this can be explained by the different number of decision variables and their ranges present in each model since the data i e elevation land use and drainage network used for swmm modelling were the same a finer model always has a higher number of decision variables like 436 in model r1 192 in model r2 32 in model r3 and 16 in model r4 however despite having a greater number of decision variables the ranges of decision variables in model r1 are the narrowest only consisting of a list of types of lid practices when applying one for more method to place lid facilities the ranges of decision variables increase with the decline of the number of decision variables differences caused by spatial resolutions such as the number of decision variables and their ranges collectively affect optimization performance another significant advantage of model r1 is that its solution set can take into account high cost lid practices such as rain gardens and bio retention cells to achieve the flood control yet models r2 r3 and r4 prefer low cost alternatives such as porous pavement vegetated swales and infiltration trenches as such in spite of similar performance on runoff control solutions provided by r1 have addition benefit in conserving biodiversity mitigating urban heat and improving air quality however constructing model r1 with units representing one type of land cover requires researchers to conduct a field study to determine which type of lid practice can be deployed and subdivide the catchment carefully and it is also time consuming to optimize model r1 as such it is not recommended to apply the subdividing method like model r1 to a large area it can be found that when applying one for more method to distribute lid facilities coarser model can provide more cost effective layout schemes but it does not mean that a coarser model always performs better as the results showe that model with a coarser resolution may perform poorly under other rainfall patterns this suggests that further testing is required to understand the trade offs between spatial scale rainfall pattern and the optimal solution sets another important finding is that during the high intensity rainfall event p 20a the distributions of optimal sets for models at various scales are very similar perhaps this is due to the fact that a coarser model has the ability to increase the proportion of lid practices with lower costs such as porous pavement infiltration trench and vegetative swales then when the study area is large it is recommended to delineate sub catchments at a moderate scale for optimization in order to save time in processing data whilst achieving better front curves 4 2 advantage and deficiency of the study as opposed to previous studies li et al 2022 liu et al 2019 which aimed to improve the performance of lid layout by applying different mooas the objective of this study is to examine the scale effects on the optimization of lid deployment one of the most significant findings is that when the one for more method is adopted to place the lid practices the study area could be coarsely delineated to reduce runtime without comprising performance another finding is that the fine model deploying lid practices by the one for one method could provide more cost effective solutions with multi type lid practices but the preliminary subdividing work and optimization process is much more cumbersome and the spread of optimal cost effective solutions is narrow the results can guide modelers in selecting appropriate spatial delineation methods which has not yet been explored in previous studies in the case of a small urban area a fine model with one for one method would be preferred as its layout could offer multiple functions and could be directly applied in practice thus reducing budget waste resulting from the gap between research and application to a large area it is recommended to obtain an initial optimization result at a coarse scale as a guideline for urban planning and then refine a fine scale layout design for a specific area when applied to engineering practice therefore it is worth taking the time to conduct this research however some deficiencies remained in this study for example due to limited data availability only one study area 9 7 ha was applied to examine the scale effects of the layout optimization so it is more suitable to apply the conclusion into a urban catchment with similar size more general conclusions should be drawn by taking larger or smaller areas as examples based on the previous study conducted by li et al 2022 only one mooa the gde3 was selected to analyze the optimization performance of the lid deployment the results were privately compared to those optimized by the commonly used nsga ii and the comparison shows that the performance differences between these two mooas could be ignored optimal results might be improved if an algorithm is tailored specifically to the layout problem of this study such as a finer scale model with a narrow solution set was proposed as discussed in section 4 1 the layout optimization of lid practices may also be influenced by patterns of rainfall events such as bimodal uniform and unimodal with early peak and late peak shapes zhang et al 2021 it is also a challenging undertaking to determine which rainfall event should be used to optimize the model furthermore lid practices serve multiple functions in urban design such as cooling amenity sanitation and biodiversity and these factors should be taken into account for the layout optimization of lid practices in the future study 5 conclusion in this research we took the cic in guangzhou china as the case study to investigate the effects of the modeling scales on the lid layout optimization the swmm model was used to simulate the hydrological response of each layout which involved two types of lid deployment approaches namely one for one and one for many when applying one for one method the model r1 with the finest resolution can provide more cost effective layout schemes consisting multiple types of lid practices but this advantage is diminished during a 20 yr rainstorm the solution set of the finest model r1 is the narrowest and it can not provide layout schemes with higher runoff reduction rate 60 additionally considerable time and effort must be invested in the sub catchment delineation and optimization process for the finest model r1 which did not suit a large area however the coarser models r2 r3 and r4 which applied one for more method to place lid practices can achieve a wider solution set while yielding similar benefits during a heavy rainfall event therefore if a high intensity rainfall event is selected to carry out swmm model it is more appropriate to apply a coarser model at the planning stage to ensure the investment and runoff control targets whereas the finest model is better suited to be applied at the implementation stage to provide a layout that covers a variety of lid practices and their corresponding sites in summary our findings provide new perspectives on the selection of spatial discretion resolutions in the lid layout optimization process however rainfall pattern and regional differences should also be noted when examined the scale effect of lid layout optimization for future studies it is suggested that various rainstorm patterns and case studies should be considered so as to minimize uncertainties as much as possible and achieve more reliable results credit authorship contribution statement zhaoli wang conceptualization project administration funding acquisition resources writing original draft shanshan li methodology validation formal analysis writing original draft xiaoqing wu guangsi lin writing review editing chengguang lai conceptualization resources writing review editing supervision funding acquisition declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the research acquire financial or data support by the national key r d program of china 2021yfc3001002 the national natural science foundation of china 51879107 the science and technology planning project of guangdong province in china 2020a0505100009 the water resource science and technology innovation program of guangdong province 2020 28 and the open fund of state key laboratory of subtropical building science 2021zb23 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2022 128113 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 
3223,climate change and increasing urbanization have worsened urban floods and other water problems and it is generally accepted that the optimization of the spatial layout of low impact development lid practices is a promising solution existing studies have focused on coupling the hydrological model with optimization methods but little is known about the effects of the spatial discretization of the model on the optimization results in this study the scale effects were examined in a case study in guangzhou china four models at various spatial discretization levels namely models r1 r2 r3 and r4 with an average unit size of 0 022 104 0 405 104 2 43 104 and 4 865 104 m2 respectively were constructed to investigate the performance difference caused by delineation scales the results show that the highest resolution model r1 one unit represented one type of land cover provides more cost effective layout schemes but during a heavy rainstorm the solution set provided by a coarser model are very similar to that generated by the finest model r1 a coarser model can provide wider solution sets but most of these schemes overshadow some types of lid practices such rain garden and bio retention cell which also have a critical role in urban heat mitigation air quality improvement and so on due to the cumbersome work of catchment subdividing and time consuming optimization process a coarse model would be a good substitute to a large area these findings provide new insights on how to achieve better performance by subdividing the catchment at a proper scale when optimizing the spatial layout of lid practices keywords low impact development layout optimization spatial discretization scale effects data availability data will be made available on request 1 introduction rapid urbanization not only alters the natural hydrologic cycle but also changes the surface conditions for example replacing vegetation with anthropogenic materials such as concrete asphalt brickwork and metal jacobson 2011 these impermeable materials hinder water infiltration into the soil increase the formation of runoff and the potential for flooding and worsen the contamination of water runoff bonneau et al 2017 li et al 2020 meanwhile it is projected that the impact of global warming will be more severe on urban areas resulting in more frequent and intense extreme rainfall events lai et al 2020 javadinejad et al 2021 ostad ali askari et al 2020 papalexiou and montanari 2019 yilmaz et al 2014 zhang 2020 which will further complicate urban water management marlow et al 2013 traditional centralized measures have the strength of removing more surface runoff rapidly than nature based solutions but the construction and renovation work of grey infrastructures are high priced and it is difficult to offer multiple benefits such as pollution load degradation heat down biodiversity and urban amenity alves et al 2019 as such nature based strategies have gradually been implemented in many countries such as low impact development lid in the usa the sustainable urban drainage system suds in the uk water sensitive urban design wsud in australia and sponge city in china fletcher et al 2015 these solutions have appellations but share the same goal that is to recover or mimic the natural water cycle in urban areas through nature based technologies chan et al 2018 one of the most widely used solutions is lid practices which includes green roofs rain gardens vegetative swales and porous pavements dietz 2007 considering their multiple benefits policymakers prefer lid measures to control surface runoff at its source and mitigate the impacts of urbanization the variety of lid facilities is considered to be limited dietz 2007 while the number of possible spatial combinations is deemed to be unlimited furthermore the budget constraints increase the complexity of the selection and placement of lid facilities as it is expected to achieve higher level of runoff reduction at lower cost there is an urgent need to seek cost effective lid layout schemes by balancing conflicting hydrological goal and economic concern chui et al 2016 kuller et al 2017 zhang and chui 2018 the exhaustive approach is unlikely to identify the optimal spatial layout for it will take a very long time in most cases to achieve convergence instead with advancements in computational abilities simulation optimization approaches emerged and multi objective optimization algorithms mooas have been widely used to identify the optimal solution set with several competing objectives damodaram and zechman 2013 deb 2014 gunantara 2018 javadinejad et al 2019 ostad ali askari et al 2017 presently these mooas have been successfully coupled with hydrologic hydraulic models to optimize lid spatial layouts islam et al 2021 pour et al 2020 a classic case is the application of the system for urban stormwater treatment and integration sustain embedded in arcgis with general applicability for providing cost efficient planning schemes under defined constraints jia et al 2015 lai et al 2007 lee et al 2012 mao et al 2017 the essence of this model is the coupling between the storm water management model swmm hydrologic simulation program fortran hspf and the non dominated sorting genetic algorithm nsga ii lee et al 2012 however one inescapable limitation for utilizing this model is that when it is applied in a large region with complicated pipe networks its computational burden may be enormous easily resulting in a program crash to cope with this deficiency many researchers have attempted to embed hydrological models into the mooa framework by coding according to their own research needs for example analogous to sustain the linking of swmm and nsga ii was widely applied xu et al 2017 oraei zare et al 2012 zhang et al 2013 zeng et al 2020 furthermore other algorithms combined with hydrological models have been developed to improve the optimization performance for example analogous to sustain the linking of swmm and nsga ii was widely applied xu et al 2017 oraei zare et al 2012 zhang et al 2013 zeng et al 2020 furthermore other algorithms combined with hydrological models have been developed to improve the optimization performance for instance the long term hydrologic impact assessment low impact development 2 1 l thia lid 2 1 model was coupled with a multi algorithm genetically adaptive multi objective approach liu et al 2016 the swmm was linked to the multi objective antlion optimization algorithm mani et al 2019 the modified particle swarm optimization algorithm duan et al 2016 li et al 2019 the multi objective shuffled frog leaping algorithm liu et al 2019 marginal cost based greedy strategy xu et al 2018 or the third evolution step of generalized differential evolution gde3 li et al 2022 when it comes to the optimization of lid layout configuration most researchers have highlighted the effects of different mooas on the optimization performance and conducted them at coarse scale without considering the impact of modelling resolution it is well known that swmm is often applied to simulate the hydrological effects of lid facilities elliott and trowsdale 2007 qin et al 2013 wu et al 2017 yazdi et al 2019 the basic computing units of the runoff component of swmm are usually the smaller sub catchments divided from the study area and the main principle of the subdivision is the spatial variabilities of the landscape such as the distribution of land use types slopes and drainage features gironás et al 2010 rossman and huber 2015 sometimes there is no uniform standard for the delineation of sub catchments and they are mainly determined by modelers so that the scale may be coarse or fine the uncertainty due to the spatial discretization may affect the surface runoff calculation and the lid layout optimization many studies have examined the difference in output from swmm at various spatial scales defined here as the resolution of the sub catchments in terms of the smaller catchments it was found that the runoff volumes are relatively insensitive to the spatial resolution goldstein et al 2016 shaneyfelt et al 2021 zeng et al 2022 for larger watersheds sun et al 2014 observed that the model parameters calibrated based on a fine discretization diminished the uncertainty of the overflow forecasts compared to those of the coarse delineation and chang et al 2019 found that the coarsening of the scale led to a decrease in the total and peak runoff similarly the modeling scale may impact the optimization process of the spatial allocation of lid practices as the decision variables change with the spatial discretization methods as lid devices are usually scattered across various sites to mitigate runoff a fine scale primarily that a sub catchment only represents one type of land cover rossman 2010 can reduce the uncertainty of the runoff reduction effects between the actual engineering design and the model output it is also helpful for the translation of the optimization results into practical measures randall et al 2019 but modeling at finer scales flat pitched rooftop pavements parking lot etc involves cumbersome work in subdividing data processing and computation chang et al 2019 accordingly the scale issue of the lid layout optimization is a crucial problem in the acquisition of an ideal solution however as discussed earlier previous studies conducted by simulation optimization approaches mostly employed a coarser model subdivision that is lumping diverse land use types into single sub catchment for simplification to alleviate the computation burden in the optimization process exceptionally randall et al 2019 subdivided the study area into 85 937 sub catchments by each separate land cover polygons for the convenience of lid allocation but this study merely examined the hydrological effects under four different lid model scenarios bach et al 2013 quantified the impact of block size on the selection of decentralised stormwater management options using a planning algorithm instead of hydrological simulation to the best of our knowledge there is no peer reviewed literature that investigates the impact of the modeling resolution on the hydrological performance of spatial configurations of lid practices optimized by mooa to bridge the research gap described above this study aims to explore how the optimal cost effective curves i e the distribution between investment and runoff reduction rate change with the scale of the model subdivision under various rainfall conditions as such swmm was selected for modeling at four spatial discretization levels and then coupled with the gde3 finally we ran them under three design storms with return periods of 0 5 5 and 20 years by understanding the modeling scale effects the findings of this study can provide guidance to modelers and planners hoping to achieve a more cost effective lid layout which can reduce more runoff at a lower cost 2 materials and methods 2 1 study area and data source the changban industrial campus cic in guangzhou city 23 8 n 113 16 e southern china with a total area of 9 7 ha fig 1 was selected as the study area the area was affected by the subtropical monsoon and the annual average precipitation was more than 1720 mm with heavy rainfall events occurring commonly chen et al 2021 there is a mountain outside the study area which is located to the north of the study area li et al 2022 and its runoff formation process is rapid and rich during periods of intense rainfall furthermore the cic is located at a low lying region with massive impervious surface where large runoff volumes would easily accumulate and the load of rainwater was prone to exceed the drainage capacity this highly urbanized area featured commercial buildings with flat or pitched roofs roads parking lots and unexplored territories as such this area was prone to flooding due to the synthetic action of substantial rainfall significant inflow runoff low lying terrain high impervious rate and low drainage capacity for example on 10 may 2016 a heavy rain hit this area causing severe flooding destroying the neighboring subway station several roadway sections and some low rise buildings and causing extensive property damage therefore response options and mitigation measures for reducing the loss caused by flooding is urgently needed in the cic considering the vulnerability of such areas to flooding the current land use type the existing building layout and the topography it is important to adopt the lid layout optimization to such areas to lessen the load of rainwater the data required for modeling included the land use type as well as a digital elevation model dem with a resolution of 5 m and a drainage network layer lastly updated in 2010 all of which came from the local land and resources bureau and water supplies bureau an automatic rain gauge and a flow meter were installed at the outfall of the watershed see fig 1 to record rainfall and runoff processes for calibration and validation of the swmm model 2 2 overview and scenarios in this study we developed four swmm models with sub catchments delineated at four different spatial resolutions including a high resolution model r1 and three progressively coarser low resolution models r2 r3 and r4 the storm events with return periods of 0 5 5 and 20 years were also input into each model as the rainfall series respectively thereafter inspired by li et al 2022 who pointed out that the third evolution step of generalized differential evolution gde3 performs better these models with various rainfall events and discretization scales were embedded to gde3 kukkonen and lampinen 2005 zelinka et al 2012 to optimize the lid layout finally the data was evolved for 200 and 500 generations respectively the straightforward coupling method and the optimization procedure are shown in fig 2 2 3 swmm model 2 3 1 model setup at four resolutions the us epa storm water management model swmm composed of runoff and routing modules has been widely used for dynamic rainfall runoff simulations in urban areas there is a lid control module for evaluating the effects of lid practices on runoff reduction rosa et al 2015 as such we selected the swmm to carry out the lid layout optimization and eight types of commonly used lid techniques including the rain barrel green roof rooftop disconnection bio retention cell rain garden porous pavement infiltration trench and vegetated swale one for many and one for one methods were used to deploy the lid control rossman 2010 the one for many method assigns one or more controls to an existing sub catchment and the lid practices act in parallel under this option the one for one method occupies the whole sub catchment by one type of the lid control and it also allows series connection of lid practices in diverse sub catchments shoemaker et al 2009 has proved that the differences in the simulation results between the distributed and aggregated lids were extremely smaller if the area of sub catchment less than 1 035 106 m2 as the total area of the study area is only 9 7 ha the impact of the connection method of lid measures on its efficacy can be neglected the selection of method mainly depended on the delineation approach of the sub catchments when we subdivided the study area at a fine scale with each sub catchment representing one land use type the one for one method was suitable for allocating lid practices conversely a sub catchment delineated at a coarse scale was always comprised of multiple land use types which suited the one for many method to place the lid facilities it is worth noting that the decision variables of mooa which were defined as the number area of each type of lid facility in each sub catchment as well as their constraints varied with partitioning methods and the distribution of the optimal solution sets changed accordingly thus we developed four versions of swmm model at different spatial resolution levels namely models r1 r2 r3 and r4 from fine to coarse scale to investigate the impacts of spatial scale on the allocation optimization of lid practice there are two common approaches applied in the urban sub catchments delineation geometric method based on nodes and artificial subdivision huang and jin 2019 the former one automatically generates sub catchments from the manholes of the urban drainage network by voronoi thiessen polygon it is time saving but impractical for example a building may be divided into two different sub catchments the latter one involves a manual sub catchment delineation with a comprehensive consideration of the distribution of land cover elevation and pipeline network which is time consuming but more realistic as such we selected the latter method to carry out the subdivision of sub catchments and finely divided model r1 into 436 sub catchments at the microscale according to landscape architects space requirements and design specifications bach et al 2018 as shown in fig 3 a one unit in model r1 always represented a specific land use type and the one for one method was used to deploy the lid practices spatially table s1 supplementary material shows the types of lid practices suitable for the specified land cover and a final list of suitable lid types for each unit would be determined following the field study based on the discretization of model r1 these micro sub catchments were merged into 24 medium sub catchments to form model r2 which were then combined into four large sub catchments to shape model r3 and were integrated into two larger sub catchments to structure model r4 fig 3b as the sub catchments in models r2 r3 and r4 always consisted of diverse land use types the one for many method was the best choice for deploying the lid controls in swmm the suitable types of lid practices for each sub catchment were summarized based on those in model r1 the spatial scales of these four models are different but they have the same pipe network system that is comprised of 169 junctions one outfall and 167 conduits fig 1 the flow direction of each sub catchment was assigned to a manhole located in the main drainage channel or near the outlet of the sub catchment 2 3 2 design storms this study also considered the effects of the various rainfall conditions on the layout optimization of the lid practices as lid practices are always designed to resist a low return period storm event and the efficiency of lid practices decreased significantly when the return period of the design storm is greater than 20 years mei et al 2018 three rainfall events with 0 5 5 20 yr return periods were selected to simulate the hydrological performance according to intensity duration frequency idf relationships in guangzhou the basic formula of rainfall intensities could be described by formula 1 based on formula 1 the rainfall intensities of a two hour duration and the return period of 1 in 0 5 5 and 20 years were selected to simulate the probable flooding situations as summarized in table 1 1 q 167 a t b n where q is the rainfall intensity t is the rainfall duration and a b and n are the constants determined by the location of the research region and the return period of the rainfall we selected the rainfall pattern with unimodal shapes middle peak keifer and chu 1957 marsalek and watt 1984 which is widely applied in current engineering practices to distribute the rainfall intensity temporally liu et al 2015 the time to peak ratio could be expressed as r t p t d where t p is the time before the peak value and t d is the duration of total rainfall the value r is an essential parameter in design storms and it is empirically fixed at 0 48 in guangzhou according to historical storms the temporal resolution is 1 min and the equations of the design storm could be expressed as formula 2 2 i t a τ ρ b n 1 n τ τ ρ b where τ t p t and ρ r for t t p τ t t p and ρ 1 r for t t p i t is the rainfall intensity at time t and the constants a b and n correspond to the constants a b and n in formula 1 which differ with the various return periods table 1 2 3 3 calibration and validation there are two types of parameters namely measured and empirical parameters in the swmm model the measured parameters are the area width slope and impervious percentage of each unit and they are determined by the topography and land cover type in this case the area and slope were calculated automatically in arcgis the width of the sub catchment was calculated by taking the square root of the sub catchment area assuming each sub catchment to be square in shape bisht et al 2016 and the impervious percentage was computed based on the distribution of land use as the sub catchments in model r1 had the highest resolution the impervious percentage of model r1 could be determined quickly and accurately and the impervious percentages of models r2 r3 and r4 were calculated based on that of model r1 the invert elevation and the maximum depth of each junction were other measured parameters that were extracted from the municipal pipe network database as for the empirical parameters the manning value of the surface the storage depth and the horton based infiltration indicators were initially established by referring to the adjacent catchment area chen et al 2018 zeng et al 2019 and the swmm manual rossman and huber 2015 the recommended ranges and values of calibrated parameters for swmm are shown in table s2 the infiltration rate is very important in model calibration as the soil in guangzhou is relatively moist during the rainy season from june to october consequently the manning value the depth of depression storage on impervious pervious area and maximum minimum infiltration rate within the recommended ranges of parameters were adjusted to improve the model accuracy generally one parameter was modified at a time when other parameters were fixed and we repeated this process until the requirements of nse values were met as the model was well calibrated after these steps it was not necessary to adjust the characteristics width the observed rainfall event from 2 10 of jun 7 to 19 55 of jun 8 in 2018 was used to calibrated model r3 and then another measured event from 12 25 of aug 28 to 5 35 of aug 29 in 2018 was applied to validate the nash sutcliffe efficiency nse value was greater than 0 7 for both calibration and validation periods and it is generally acceptable of nse greater than 0 5 moriasi et al 2007 nash and sutcliffe 1970 the shape and timing of the simulated hydrograph showed good agreement with the observed data with very slight differences at the peaks as shown in fig 4 next the validated parameter values of the low resolution model r3 were used to calibrate and validate the model r1 r2 and r4 it was found that the simulated hydrographs of models r1 r2 and r4 almost coincided with that of model r3 fig 4 and all nse values were greater than 0 7 table 2 this indicates that the level of sub catchment discretization has little impact on the overall model performance which is supported by previous studies on the scale effects shaneyfelt et al 2021 goldstein et al 2016 ghosh and hellweger 2012 as such models with various spatial resolutions perform well in the hydrological simulation and they can be considered for further study in scale effects the calibration parameter values were set as follows the manning s value and depression storage s depth of the impervious area were 0 011 and 0 5 mm and those of the pervious parts were 0 24 and 1 mm respectively the maximum and minimum infiltration rates were 10 mm h and 1 25 mm h respectively and the decay constant value was 4 and manning s value of the channel was between 0 01 and 0 01 2 4 multi objective optimization algorithm gde3 2 4 1 definition of optimization problem in this study we adopted the gde3 to investigate the scale effect on the distribution of the optimal solution sets of the lid layout the multi objective optimization problem deb 2014 considered here consisted of finding a set of decision variables x x 1 x 2 x n corresponding to the number area of each lid practice type in each sub catchment which minimized the total annual budget f 1 x and maximized the runoff reduction rate f 2 x expressed as formula 3 improvements in hydrological performance always result in increased investment so the optimal solution does not exist and the mooa is always employed to balance these conflicting goals to get a set of optimal solutions min f 1 x j 1 m a 1 j a 2 j and max f 2 x runof f baseline r u n o f f lid runof f baseline where a 1 j and a 2 j are the annualized construction cost and the annual maintenance cost huang et al 2018 of the lid practice j respectively m is the number of varieties of the lid facilities that can be used at a sub catchment runof f baseline and runof f lid are the simulated runoff in the scenario without and with the lid layout respectively once the construction of the lid practice was completed yearly maintenance was required over its entire life for the regular operation of the lid facility as the lifetime varied with the type of lid practice table 3 we adopted the total annual budget as an economic performance indicator the initial construction cost p j was uniformly transformed into the annualized construction cost a 1 j over a n j year project lifetime at an annual real interest rate of i which could be expressed as equation 4 4 a 1 j p j i 1 i n j 1 i n j 1 the one for one method to place lid practices rossman 2010 was always applied in models with fine resolution sub catchments and landscape architects and urban planners were invited to identify the suitable lid sites and subdivide the study area into small units based on the site feasibility table s1 landscape architecture and local construction guidelines bach et al 2018 there were several alternative types of lid implementation on a site but only one could be selected at one run accordingly the decision variables x x 1 x 2 x n referred to the selected type of the lid practice in each unit the type of lid practice selected for the n th sub catchment is x n and it was chosen from a list of types of lid practices that were suitable for the n th sub catchment correspondingly the range of x n was one of the values in 0 1 t where 0 represents no lid facility was built in the n th sub catchment t is the sum of the types of lid practices that can be placed in sub catchment n and the codes from 1 to t represents different types of lid practices such as in terms of the sub catchment made up of a flat roof x n 0 1 2 in which 1 represented green roof and 2 represented rain barrel this means that the lower bounds of all decision variables were set to 0 while the upper bounds depended on the number of the types of lid practices that were feasible to build in the corresponding sub catchment however the one for many method was more suitable for the model with coarse resolution sub catchments there were several varieties of land cover in a sub catchment so it could simultaneously contain multiple types of lid facilities as such the decision variable was altered to the area of each type of lid practice at each sub catchment expressed as x x 11 x 12 x 1 k x m 1 x m 2 x mk where k is the type of lid practice m is the sub catchment number there are two constraints one is that the value of x mk must not be greater than the suitable area in the m th sub catchment for the lid practice k and the other is that the sum of the area of each type of the lid practice allocated at the m th sub catchment x m 1 x m 2 x mk must be less than or equal to the total area of sub catchment m 2 4 2 development of gde3 numerous studies have demonstrated that there is no single algorithm outperforming the others huo et al 2016 keshavarzzadeh and ahmadi 2019 quresh et al 2019 vargas et al 2021 and the preferred algorithm depends on the definition of the optimization problem such as the number of decision variables and constraints since the goal of this study is to examine the scale effect based on the study conducted by li et al 2022 the third evolution step of generalized differential evolution gde3 was chosen to optimize the placement of lid practices due to its prominent mutation and crossover operators the overall structure can be expressed as follows 1 initialize the parent population of the size np randomly with values from the specific range as the decision variables in model r1 changed within a small number of discrete values and there was no constraint the initial values of parent population was generated randomly for models r2 r3 and r4 the ranges of decision variables consists of a large number of discrete values more importantly there were many solutions that did not satisfy the defined constraints as such values that located at one quarter or one fifth of the ranges while meeting the restrictions were used as initial solution to ensure the smooth progress of the optimization 2 repeat until the termination condition is satisfied a generate offspring population after repeating np times i select a point from the parent population ii generate a new offspring point with mutation and crossover iii update the impervious rate and width for a new offspring point rossman 2010 b combine the parent and offspring populations c select the best np points for the subsequent evolution according to fast non dominated and crowding distance sorting the outstanding characteristic of the gde3 is its mutation and crossover operators which act on each element of each solution through internal loops the gde3 is an extension of the differential evolution de algorithm whose crossover and mutation operations are determined by the crossover rate cr 0 1 and the mutation factor f r p g is a population of np solution vectors sets of decision variables x i g in a generation g and i 1 2 3 n p is a vector index each x i g is an n dimensional vector and x j i g is its j th element i 1 2 3 n three solution vectors namely x p 1 g x p 2 g and x p 3 g that are mutually different and different from x i g are randomly selected from the parent population x j p 1 g x j p 2 g and x j p 3 g were the j th elements of x p 1 g x p 2 g and x p 3 g respectively a trial element y j i g is generated by crossover operations and a trial vector y i g is obtained after the loop see fig s1 in the supplementary material 3 results 3 1 performance differences between generations in this study a population size of 200 was evolved for generations 200 and 500 to examine how generation affected optimization results of models at various resolutions as the solution sets were discrete the solutions under various scenarios did not always have the same cost or reduction rate for the convenience of data analysis when calculating the difference in the reduction rate between the different scenarios the two points with the closest cost were selected for comparison the average percent differences in the reduction rate with respect to 200 generations for 500 generations avpd ge values are listed in table 4 for model r4 with the lowest resolution the avpd ge values are relatively minor less than0 38 indicating that the front curve remains stable when the generation reached 200 and further evolution is unnecessary the avpd ge values rise with the increase in resolution the avpd ge values for higher resolution models r1 and r2 range from 1 62 to 5 79 which indicates that models at finer scale are more sensitive to generations the number of decision variables for models r1 r2 r3 and r4 are 430 192 32 and 16 respectively as a result this may be explained by the fact that models at finer scales have more decision variables requiring more generations to yield a stable optimal solution set additionally it is worthwhile to note that as shown in table 5 it takes significantly longer to optimize higher resolution models however while model r1 has twice as many decision variables as model r2 the difference in processing time is relatively small this is because for model r2 the range of decision variables is much boarder and there is a requirement to select solutions that satisfy the constraints during the optimization process 3 2 scale effects on the lid layout optimization following the results in section 3 1 it is hypothesized that the solution sets reach a more stable state after 500 generations these results were then used to analyze the effects of modeling scales on the lid layout optimization under the rainfall events of the return periods of 0 5 5 and 20 years as shown in fig 5 it is evident that the distribution ranges of solutions produced by models r1 and r2 are the narrowest regardless of the type of rainfall event but most runoff reduction rates computed by model r1 are higher than those computed by other models particularly for the 0 5 yr and 5 yr rainfall events it is suggested that for the low intensity rainfall events the layout schemes of lid practices optimized by model r1 in which one for one methods are applied to deploy lid practices spatially could reduce runoff more effectively at the same cost point in addition the front curve for model 4 has the greatest coverage and most cost effective schemes followed by model 3 and finally model r2 this result indicates that when applying the one for more method to place lid practices the coarser the sub catchment resolution is the better the performance of optimizing lid layouts would be it might also be related to the variation in the number of decision variables since the number of decision variables decrease as the spatial scale become coarser more importantly as the return period of rainfall events increase the differences between the front curves of models r1 r2 r3 and r4 become smaller and tend to merge together that is the modelling scale has less impact on optimization layout of lid practices under high intensity rainfall events based on each solution on the front curve see fig 5 the total area occupied by each type of lid practice was calculated and plotted against the resolution of the sub catchment discretization as shown in fig 6 outliers are solutions that were numerically out of the ordinary apart from outliers the area of lid practice generally varies within the interquartile range for instance under the 0 5 yr rainfall event the total area of green roofs ranges from 0 to 1 3 104 m2 when optimizing model r1 this spread however changes to 0 to 0 4 104 m2 0 to 2 2 104 m2 0 to 2 3 104 m2 when optimizing model r2 r3 and r4 it can be seen that the variation range of areas occupied by each type of lid facility for model r1 is smaller in comparison with other models this is in agreement with the fact that the spread of optimal solution sets of model r1 is also smaller besides for model r1 there is few rain barrel and rooftop disconnection applied in the layout design and with the increase of return period of rainfall event the variation range of rain garden and bio retention cell is low whereas the distributions of area of green roof vegetated swale infiltration trench and porous pavement change dramatically in the model r2 the area covered by rainfall barrels rooftop disconnection rain gardens and bio retention cells is little and most of the space is reserved for infiltration trenches and porous pavement for models r3 and r4 the variation ranges of green roof vegetated swale infiltration trenches and porous pavement is broad and more rooftop disconnections and rain garden were used in the layout design the results indicate that models with varying resolutions prefer different type of lid practices to optimize their layout schemes 3 3 lid layout at a specific target as can be seen in fig 5 the maximum runoff reduction rate for model r1 is about 60 for the purpose of ensuring a clear understanding of lid layout schemes a 60 runoff reduction was selected to analyze the difference of the spatial distribution of lid facilities for models r1 r2 r3 and r4 as illustrated in fig 7 in each sub catchment a pie chart was created to illustrate the relative proportion of each type of lid practice from the layout map of model r1 it is clear what type of lid practice should be located where in comparison with other models the layout generated by model r1 can serve as a design scheme for implementation in a practical setting nevertheless from the layout of model r2 r3 and r4 we can only estimate the proportions of each type of lid practice to be deployed within each sub catchment additionally layouts for a 20 yr rainfall event were applied to two real rainfall events described in section 2 3 3 and and a 50 yr design rainfall event in order to analyze differences in hydrological responses between optimized models at various discretization scale table 6 shows that the differences in the reduction rate between the schemes of each model are very small less than 1 46 during a 50 yr rainstorm while the difference during two real rainstorms varies within 6 33 the scheme optimized by model r4 performs the worst regardless of the type of rainfall besides the layouts generated by models r1 and r2 are able to capture more runoff during the rainfall events of jun 7 and aug 28 respectively it can be found that the reduction rates of model r2 are relative high under these two ocnditions but its budget is also the highest at 61 25 104 usd in other words the optimal layout of model r2 achieves a similar runoff control target 60 at a higher cost than other models under a 20 yr rainstorm but it can capture more runoff under the rainfall events with different shapes this indicates that the hydrological performance of lid layout are sensitive to the patterns of rainfall events 4 discussion 4 1 implication and cause analysis as mentioned previously most studies have focused on embedding the swmm model into various mooas to improve the performance of the layout optimization of lid facilities while ignoring the scale effects caused by the spatial delineation of the swmm model by contrast the present study reveals the impact of sub catchment resolutions on the layout optimization of the lid facilities during rainfall events with varying intensities which could facilitate the selection of an appropriate spatial discretization scale to optimize the layout of lid facilities the results show that for a given cost runoff control effectiveness of the layout optimized by each model decreases with the increase of return periods of rainfall event which is in line with previous studies that the performance of lid practices decreases with the growth of rainfall intensity lee et al 2013 qin et al 2013 yin et al 2020 this indicates that the main purpose of lid facilities is to resist small storm events it is worth noting that this decline is more pronounced in model r1 in the beginning it was suspected that this was related to the inadequacy of the evolution generations and we then used the gde3 to optimize model r1 under a 20 yr rainstorm for 1000 generations however the increase in generations only slightly enhances the performance of the layout optimization so the effect of generation on the distribution of optimal solution sets can be disregarded it was also observed that the coarser the spatial resolution the broader the spread of optimal cost effective solutions the finest model r1 could provide better lid layout schemes at a lower cost especially for low intensity rainfall events but for the models r2 r3 and r4 the cost effectiveness of solutions decreases with increasing spatial resolution this can be explained by the different number of decision variables and their ranges present in each model since the data i e elevation land use and drainage network used for swmm modelling were the same a finer model always has a higher number of decision variables like 436 in model r1 192 in model r2 32 in model r3 and 16 in model r4 however despite having a greater number of decision variables the ranges of decision variables in model r1 are the narrowest only consisting of a list of types of lid practices when applying one for more method to place lid facilities the ranges of decision variables increase with the decline of the number of decision variables differences caused by spatial resolutions such as the number of decision variables and their ranges collectively affect optimization performance another significant advantage of model r1 is that its solution set can take into account high cost lid practices such as rain gardens and bio retention cells to achieve the flood control yet models r2 r3 and r4 prefer low cost alternatives such as porous pavement vegetated swales and infiltration trenches as such in spite of similar performance on runoff control solutions provided by r1 have addition benefit in conserving biodiversity mitigating urban heat and improving air quality however constructing model r1 with units representing one type of land cover requires researchers to conduct a field study to determine which type of lid practice can be deployed and subdivide the catchment carefully and it is also time consuming to optimize model r1 as such it is not recommended to apply the subdividing method like model r1 to a large area it can be found that when applying one for more method to distribute lid facilities coarser model can provide more cost effective layout schemes but it does not mean that a coarser model always performs better as the results showe that model with a coarser resolution may perform poorly under other rainfall patterns this suggests that further testing is required to understand the trade offs between spatial scale rainfall pattern and the optimal solution sets another important finding is that during the high intensity rainfall event p 20a the distributions of optimal sets for models at various scales are very similar perhaps this is due to the fact that a coarser model has the ability to increase the proportion of lid practices with lower costs such as porous pavement infiltration trench and vegetative swales then when the study area is large it is recommended to delineate sub catchments at a moderate scale for optimization in order to save time in processing data whilst achieving better front curves 4 2 advantage and deficiency of the study as opposed to previous studies li et al 2022 liu et al 2019 which aimed to improve the performance of lid layout by applying different mooas the objective of this study is to examine the scale effects on the optimization of lid deployment one of the most significant findings is that when the one for more method is adopted to place the lid practices the study area could be coarsely delineated to reduce runtime without comprising performance another finding is that the fine model deploying lid practices by the one for one method could provide more cost effective solutions with multi type lid practices but the preliminary subdividing work and optimization process is much more cumbersome and the spread of optimal cost effective solutions is narrow the results can guide modelers in selecting appropriate spatial delineation methods which has not yet been explored in previous studies in the case of a small urban area a fine model with one for one method would be preferred as its layout could offer multiple functions and could be directly applied in practice thus reducing budget waste resulting from the gap between research and application to a large area it is recommended to obtain an initial optimization result at a coarse scale as a guideline for urban planning and then refine a fine scale layout design for a specific area when applied to engineering practice therefore it is worth taking the time to conduct this research however some deficiencies remained in this study for example due to limited data availability only one study area 9 7 ha was applied to examine the scale effects of the layout optimization so it is more suitable to apply the conclusion into a urban catchment with similar size more general conclusions should be drawn by taking larger or smaller areas as examples based on the previous study conducted by li et al 2022 only one mooa the gde3 was selected to analyze the optimization performance of the lid deployment the results were privately compared to those optimized by the commonly used nsga ii and the comparison shows that the performance differences between these two mooas could be ignored optimal results might be improved if an algorithm is tailored specifically to the layout problem of this study such as a finer scale model with a narrow solution set was proposed as discussed in section 4 1 the layout optimization of lid practices may also be influenced by patterns of rainfall events such as bimodal uniform and unimodal with early peak and late peak shapes zhang et al 2021 it is also a challenging undertaking to determine which rainfall event should be used to optimize the model furthermore lid practices serve multiple functions in urban design such as cooling amenity sanitation and biodiversity and these factors should be taken into account for the layout optimization of lid practices in the future study 5 conclusion in this research we took the cic in guangzhou china as the case study to investigate the effects of the modeling scales on the lid layout optimization the swmm model was used to simulate the hydrological response of each layout which involved two types of lid deployment approaches namely one for one and one for many when applying one for one method the model r1 with the finest resolution can provide more cost effective layout schemes consisting multiple types of lid practices but this advantage is diminished during a 20 yr rainstorm the solution set of the finest model r1 is the narrowest and it can not provide layout schemes with higher runoff reduction rate 60 additionally considerable time and effort must be invested in the sub catchment delineation and optimization process for the finest model r1 which did not suit a large area however the coarser models r2 r3 and r4 which applied one for more method to place lid practices can achieve a wider solution set while yielding similar benefits during a heavy rainfall event therefore if a high intensity rainfall event is selected to carry out swmm model it is more appropriate to apply a coarser model at the planning stage to ensure the investment and runoff control targets whereas the finest model is better suited to be applied at the implementation stage to provide a layout that covers a variety of lid practices and their corresponding sites in summary our findings provide new perspectives on the selection of spatial discretion resolutions in the lid layout optimization process however rainfall pattern and regional differences should also be noted when examined the scale effect of lid layout optimization for future studies it is suggested that various rainstorm patterns and case studies should be considered so as to minimize uncertainties as much as possible and achieve more reliable results credit authorship contribution statement zhaoli wang conceptualization project administration funding acquisition resources writing original draft shanshan li methodology validation formal analysis writing original draft xiaoqing wu guangsi lin writing review editing chengguang lai conceptualization resources writing review editing supervision funding acquisition declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the research acquire financial or data support by the national key r d program of china 2021yfc3001002 the national natural science foundation of china 51879107 the science and technology planning project of guangdong province in china 2020a0505100009 the water resource science and technology innovation program of guangdong province 2020 28 and the open fund of state key laboratory of subtropical building science 2021zb23 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2022 128113 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 
3224,flood events triggered by heavy rain such as pluvial and flash floods are a common threat worldwide however it is usually not known which areas and cities are particularly vulnerable to flooding caused by heavy rain to enable regional scale susceptibility assessment we developed a novel methodology based on tree based classifiers that considers both spatially distributed and catchment related influencing factors the performance of the developed methodology was evaluated using the region of bavaria germany for the case study area 70 500 km2 we trained a random forest rf a gradient boosting decision tree gbdt and a catboost model cb using 1 864 flood and non flood locations and 11 spatially distributed and six catchment related influencing factors regarding performance metrics all three models performed equally well cb auc 0 819 rf auc 0 816 gbdt auc 0 813 with the catboost model performing best when modeling large areas it proved critical to account for low sample density by ensuring i a homogeneous spatial coverage of the study area and ii the representation of the major landscapes in the training and test set in addition we propose an overall susceptibility score for cities based on the susceptibility map generated which can be used to prioritize cities for detailed investigations those responsible for spatial planning and flood risk management can apply the proposed methodology to obtain a pluvial and flash flood susceptibility assessment for large territories keywords flash flood machine learning catboost ensemble method shapley additive explanations data availability the authors do not have permission to share data 1 introduction since flash floods pose a deadly threat to humans researchers are looking for ways to improve flash flood forecasting and warning yet protecting against the natural hazard is challenging as flash floods are difficult to predict and can occur anywhere nonetheless researchers have succeeded in establishing early warning systems using weather radar hydrologic and hydrodynamic models and drainage network monitoring e g smith et al 2007 javelle et al 2010 looper and vieux 2012 bartos et al 2018 hofmann and schüttrumpf 2020 for warning purposes researchers identify flash flood susceptible areas using various modeling techniques such as gis hydrodynamic models or data driven models e g iosub et al 2020 nguyen et al 2020 li et al 2019 for several years researchers have increasingly used machine learning ml models instead of hydrodynamic or physical models in flood modeling according to mosavi et al 2018 the increasing popularity is because ml models can describe the flood events nonlinearity based on historical data alone without having to consider physical processes in addition ml models are less costly and faster to set up than hydrodynamic models wagenaar et al 2020 meanwhile researchers have successfully derived flood and flash flood susceptibility maps using ml models e g bui et al 2019a chen et al 2019 tien bui et al 2020 the ml algorithms applied in flash flood susceptibility modeling are diverse logistic regression e g costache 2019a decision trees e g costache and tien bui 2019 support vector machines e g tehrany et al 2014 naïve bayes e g khosravi et al 2019 ensemble methods e g bui et al 2019b artificial neural networks e g ngo et al 2018 to further improve prediction performance researchers have hybridized ml algorithms with bivariate models e g frequency ratio weights of evidence shannon s entropy multi criteria decision methods or gis techniques e g wang et al 2019 khosravi et al 2016 costache et al 2020c to identify pluvial and flash flood susceptible areas at regional level we need a modeling technique that is affordable quick and can process a large study area since the setup of hydrodynamic models is time consuming and the modeling is computationally intensive the application of hydrodynamic models to large regions is not practical however the application of data driven models in the literature is usually limited to individual catchments with sizes between 200 and 4 000 km2 e g youssef et al 2016 costache et al 2020b khosravi et al 2018 with the exception of ma et al 2019 ml algorithms have not yet been applied to vast regions with natural heterogeneity to take full advantage of data driven models for assessing flash floods we need to explore the challenges of modeling heterogeneous areas of several 10 000 km2 in this paper we illustrate how to generate a pluvial and flash flood susceptibility map for the region of bavaria germany using tree based ensemble models and appropriate explanatory factors first we review the influencing factors applied in flash flood susceptibility studies section 2 this is followed by a description of the study area section 3 we then introduce the datasets and methods used section 4 and present the study results section 5 finally we discuss our findings from susceptibility map generation at regional scale using tree based algorithms section 6 and provide concluding remarks section 7 2 influencing factors on flash flood occurrence to set up a good predictive model it is crucial to choose robust explanatory features several studies have proven that it is possible to predict whether a location was affected by a flash flood in the past based on area characteristics bui et al 2019a nguyen et al 2020 pham et al 2020a to find out the most frequently applied explanatory features we compared 23 recent studies on flash flood modeling that applied machine learning algorithms for direct comparison we listed the explanatory features used in each of these studies in table 1 we summarized the influencing factors in the categories of topography soil and geology land cover river network precipitation characteristics and anthropogenic factors in the studies examined the number and type of influencing factors applied varied we identified a total of 37 different influencing factors 12 of which were topography related the explanatory factors applied cover a broad spectrum and range from elevation and soil type through the fraction of vegetation cover and river density to the sealing degree the studies reviewed used between 6 and 15 influencing factors for flash flood modeling with 10 factors representing the median topographic parameters were the most frequently used explanatory factors in these studies table 1 the studies examined applied between 2 and 7 topographic parameters 5 parameters on average the slope as the only explanatory factor was used by each of the 23 studies reviewed other frequently considered topographic parameters were aspect elevation and the topographic wetness index twi which were applied in 16 15 and 14 studies respectively the most frequently used influencing factor after slope was lithology which was considered in 19 of the 23 studies in addition to the lithology soil properties were described by parameters such as soil type hydrological soil group and soil depth usually one or two parameters of the category soil and geology were used in four studies however no soil properties were considered in addition to lithology we attributed five explanatory factors to the category land cover namely normalized difference vegetation index ndvi fraction of vegetation cover land use curve number a combination of soil type and land use and surface roughness usually one of these five land cover parameters was used to explain the flash flood occurrence in 61 of the studies the explanatory factor land use was chosen which makes it the fifth most frequently used parameter besides twi in the studies investigated six parameters were used to describe the characteristics of the river network most often the parameter river density was applied followed by the distance to the river and the convergence index the flow direction in contrast was only considered in the study by pham et al 2020b in three studies a maximum of three river network parameters was considered simultaneously while the study by tang et al 2019 completely disregarded parameters of the river network in its modeling precipitation characteristics were considered in 14 studies anthropogenic factors in only three studies we counted population sealing degree gross domestic product and flash flood prevention investments among the anthropogenic factors the studies in which precipitation was considered generally used information on the mean of annual maximum precipitation amount for different durations e g 3h 24h and the precipitation amount of a specific event that triggered flash floods according to our literature review it is at least necessary to describe an area s topography lithology land use and river network to explain the occurrence of flash flooding with a ml model since topography has a significant influence on the occurrence of a flash flood the topography of an area must be comprehensively described by several parameters e g slope aspect elevation twi in the ml model the comparison of the studies also suggests that lithology is an essential parameter in explaining flash flooding often used in combination with soil type or hydrological soil group the characteristics of the river network should also be considered in the ml model e g via river density distance to the river convergence index although no river network parameter such as slope or lithology has prevailed in previous studies in addition to the predictive power of the influencing factor the availability of the underlying dataset certainly plays a role in the choice of influencing factors how well the chosen parameters can explain flash flood occurrence depends not least on the combination of parameters the quality of the underlying datasets and the characteristics of the study area the investigated studies were carried out in six different countries vietnam bui et al 2019b ngo et al 2018 nguyen et al 2020 pham et al 2020b tien bui et al 2019 tien bui et al 2020 romania costache and tien bui 2020 costache 2019b 2019a costache and zaharia 2017 costache et al 2019 costache et al 2020a costache et al 2020b costache et al 2020c iran hosseini et al 2020 janizadeh et al 2019 khosravi et al 2016 chapi et al 2017 pham et al 2020a khosravi et al 2018 china tang et al 2019 ma et al 2019 chen et al 2019 saudi arabia youssef et al 2016 and greece diakakis et al 2016 of the 23 studies 9 studies were performed in europe and 14 at study sites in asia although the studies cover different climate zones there is no discernible difference in the choice of the influencing factors based on the site location however the influencing factors may have had different feature importances in the models of the different regions which we have not examined 3 description of the study area the region of bavaria is located in southern germany and has an area of 70 542 km2 fig 1 bavaria can be divided into four major landscapes the alps the alpine foothills the eastern and south western low mountain range fig 2 the alps in southern bavaria are the smallest major landscape regarding area the bavarian alps are characterized by high mountains with heights between 1 500 and almost 3 000 m the alpine foothills elevation range 300 800 m describe the region north of the alps and south of the danube which is characterized by glacial deposits south of the danube there is fertile hilly country to which an area with numerous lakes connects further in the south the eastern low mountain range elevation range 300 1 500 m is located in the northeast of bavaria and the south western low mountain range elevation range 300 700 m is found in north west bavaria the low mountain ranges are mountainous areas with rounded wooded ridges and elongated valleys bavaria is characterized by a warm moderate climate as it is located in the transition zone between the maritime climate of western europe and the continental climate of eastern europe bavaria s climate is influenced by the different altitudes and the structure of the low mountain ranges the alpine foothills and the bavarian alps stmuv 2015 the altitudes range from 100 m in the northwestern part of bavaria to nearly 3 000 m in the alps differences between the seasons are clear but not extreme in bavaria in the period 1971 2000 the annual average temperature in bavaria was 7 8 c the average summer temperature in bavaria was 16 2 c the average winter temperature 0 5 c in the reference period 1971 2000 july was the warmest month and january the coldest due to the altitude dependence however the annual average temperatures vary greatly over bavaria stmuv 2015 the average annual precipitation amount in bavaria is 945 mm but regional differences are large with an average of 600 to 700 mm per year it rains the least in central and northwest bavaria the highest average annual precipitation amount with 1 800 mm is recorded in the alpine region with 111 mm on average july is the month with the most precipitation whereas february is the month with the fewest precipitation 56 mm in bavaria stmuv 2015 overall the warmer dry northwest contrasts the precipitation rich regions of the south and east of bavaria the pluvial and flash flood season in germany is summer with july being the month with the most events the region of bavaria also experiences heavy rain induced flood events in spring with 59 recorded events until 2017 munich was the second most affected city after berlin 65 kaiser et al 2021 bavaria is a water rich region with about 100 000 km of flowing waters large waters of national importance account for 4 200 km another 4 800 km are waters of regional importance lfu 2017c about 91 000 km are small waters that considerably influence the formation of flash floods a main european water divide runs through bavaria separating the danube rhine and elbe river basins fig 1 the danube is bavaria s largest river which is characterized by its water rich tributaries originating from the alps and the bavarian forest in the eastern low mountain range stmuv 2012 the danube drains most of bavaria from the south while the main drains the north west of bavaria agriculture and forestry are an important economic sector in bavaria more than half of the region 53 4 is used for agriculture 65 of which is arable land and 35 is grassland bbv 2020 forests and near natural areas account for 37 9 and are mainly found in the alpine region and the low mountain ranges in east and south western bavaria wetland and water bodies sum up to 1 2 of the area the built up area makes up 7 5 of the region in 2015 the degree of soil sealing in bavarian municipalities was between 27 and 75 in relation to bavaria s total settlement and traffic area the proportion of sealed surfaces amounts to 50 9 üreyen and thiel 2017 based on their runoff potential the bavarian soils can be divided into the four hydrological soil groups a b c and d the hydrological soil group a which corresponds to sand loamy sand or sandy loam types of soils occurs on 13 of the bavarian area the hydrological soil groups b silt loam or loam and c sandy clay loam have an area share of 14 and 49 respectively the hydrological soil group d which are soils of clay loam silty clay loam sandy clay silty clay or clay covers 24 of bavaria the soils of group d are characterized by very low infiltration rates and thus favor the generation of surface runoff in addition because soils of group c and d are characterized by high water retention capacity they can limit subsurface flow these soils with high runoff potential are mainly found in the alpine region and the south western low mountain ranges overall the soils with low and very low infiltration rates c and d account for 73 of the total area to sum up the study area has a large extent and includes a variety of different natural landscapes bavaria combines a great diversity regarding land use soil vegetation morphometry and climatic conditions therefore low mountain ranges and cultural landscapes stand for bavaria as much as large forests a multitude of rivers and lakes and the alpine region however the large natural heterogeneity of bavaria poses a challenge for the ml model which is typical for modeling vast areas and can therefore be considered exemplary to still achieve high model performance the selected explanatory factors must comprehensively describe the different characteristics of the major landscapes in the study area in addition a sufficient amount of training data in the different major landscapes is necessary to learn the relationships in a sound way 4 data and methods 4 1 methodical approach we recommend performing the steps shown in fig 3 to generate a pluvial and flash flood susceptibility map for a very large area the overall workflow consists of the following steps i feature selection based on literature review multicollinearity analysis and feature importance ii feature preparation including handling of missing data standardization and weight of evidence encoding iii construction of the training and test datasets iv model training and validation using performance statistics v final model selection and vi generation of the pluvial and flash flood susceptibility map in the following sections we describe each step in detail using bavaria as an example 4 2 flood inventory training and test dataset the derivation of the flash flood susceptibility for a given area using machine learning models requires the knowledge of formerly affected and unaffected locations based on the affected and unaffected locations the model learns which area characteristics and conditions favor the occurrence of flash floods in this study we used the pluvial and flash flood dataset generated within the framework of the hios project kaiser et al 2020 created a dataset of past german pluvial and flash flood events triggered by heavy rain using a variety of sources ranging from agencies over mission archives and insurance companies to media reports for a flash flood event to be documented the triggering heavy rain event must have lasted between 30 min and 24 h and must have exceeded a given minimum precipitation amount according to essl 2014 1 p 2 5 d where p is the precipitation amount in mm and d is the duration in min in addition the event must have occurred in a catchment smaller than 500 km2 and in the period between april and october if the heavy precipitation amount is unknown kaiser et al 2020 apply a short flooding rise time and or little to no warning time and a small watershed size as proxy indicators a pluvial flood event must meet the same thresholds to be documented however in the absence of a rainfall measurement the heavy rain event must have triggered flooding causing extreme impacts e g flooded cellars interruption of public traffic closed roads etc within the flood inventory no distinction is made between pluvial and flash floods because both flood types can occur simultaneously and the available event information does not allow a clear differentiation kaiser et al 2021 in the flood inventory an event can be uniquely identified by the affected city and the occurrence date kaiser et al 2021 the term city in this context refers to any size of settlement from village to city that can still be uniquely identified there are more than 10 000 uniquely identifiable villages towns and cities in bavaria due to the events spatial reference to settlements the flood inventory focuses on pluvial and flash flood events that occurred in populated areas because damaging flood events are more likely to be reported than non damaging events a bias toward damaging events in the flood inventory is likely according to the flood inventory by kaiser et al 2020 932 bavarian settlements and cities were affected by one or more pluvial and flash flood events by 2017 since the location of the flooding within the cities was generally unknown the center of the city was used as a uniform approximation the affected and unaffected cities are represented in the classification model by their city center point location which was converted to a raster pixel with a resolution of 25 m to prevent introducing bias into the model we chose the same number of unaffected cities as affected cities we randomly selected 932 cities from the remaining bavarian cities assuming that we know all affected cities and that the remaining cities have therefore not been affected to ensure the representativeness of the major landscapes we selected as many unaffected cities in each major landscape as affected cities were known subsequently we split the generated pluvial and flash flood dataset of 1 864 samples into a training and test dataset for the training dataset we randomly selected 80 of the dataset the remaining 20 of the dataset was used as a test dataset fig 2 since this is a binary classification problem the values 1 and 0 were assigned to the flood and non flood samples respectively at each sample point we also extracted the values of the influencing factors which were initially computed in a raster format with a 25 m resolution we calculated the catchment related factors for 12 964 catchments whose average size was 5 km2 and their median size was 2 km2 since flash floods mainly occur in small catchments we chose a catchment scale that can finely represent the catchment characteristics of bavaria in the case of catchment related influencing factors each sample point is assigned the value of the catchment in which it is located 4 3 feature selection based on the literature study and data availability we preselected possible influencing factors table 2 we investigated 42 influencing factors with respect to their suitability to explain the occurrence of heavy rain induced floods in bavaria the 42 selected influencing factors describe topography 11 soil and geology 6 land cover 3 river network 6 catchment properties 11 precipitation characteristics 2 anthropogenic factors 2 and spatial location 1 among the preselected features some are similar in that they describe the same area property but use different datasets or are derived differently this applies for example to the preselected features form factor and elongation ratio both of which describe the shape of a catchment we used multicollinearity analysis and the classifiers feature importance to reduce the 42 explanatory factors to the most influential ones an iterative trial and error approach was applied to determine the best combination and selection of influencing factors the lower limit for the importance of a feature was arbitrarily set at 2 and features contributing less were removed from the model we applied the indicators variance inflation factor vif and tolerance tol to evaluate collinearity among the predictors in the literature vif 10 and tol 0 1 are often considered as thresholds for the presence of multicollinearity dormann et al 2013 features that were very similar were characterized by high collinearity such as the features sediment transport index and stream power index topographic wetness index and slope elongation ratio and form factor of these collinear features the one that had the higher feature importance was selected using multicollinearity analysis and feature importance we gradually reduced the number of influencing factors to 17 while reducing the input features iteratively we took care to retain explanatory variables from all domains that can explain the occurrence of pluvial and flash floods thus we retained features describing topography land use soil and geology river network heavy precipitation anthropogenic factors and catchment properties overall we selected the following influencing factors elevation slope landforms land use sealing degree distance to river height above the nearest drainage the permeability of the upper aquifer saturated hydraulic conductivity the product of x and y coordinates representing the spatial location heavy rain hours and the catchment related variables relief melton number elongation ratio length area relation built up area share and agricultural area share table 3 lists the sources resolution and original format of the datasets used to derive the influencing factors for the study we converted the influencing factors into raster of 25 m resolution although tree based models are insensitive to collinear predictors we reduced the number of influencing factors as much as possible we aimed at reducing the model complexity to make them easier to understand to ensure the acceptance of the flash flood susceptibility map it is important to use models that are as simple and comprehensible as possible table 4 shows that all 17 selected influencing factors except the product of the x and y coordinate meet the vif and tol thresholds defined by dormann et al 2013 however we do not consider this to be a problem since on the one hand tree based classifiers are tolerant of multicollinearity and on the other hand the high vif was caused by the inclusion of products of two variables furthermore the predictor x y coordinate is a combined predictor that implicitly considers two predictors simultaneously and thus adding value to the model 4 4 description of the chosen influencing factors topography controls the hydrological processes of an area to a large extent researchers consider the slope to be the most important factor influencing the occurrence of flash floods tehrany et al 2013 vaezi et al 2017 diakakis et al 2016 the higher the slope the less the soil infiltration and the higher the surface runoff while high slopes promote runoff concentration flat areas favor flooding in addition to slope elevation is a good explanatory factor for flash flooding since water follows the gradient the lower lying areas of a catchment tend to be more often affected by flooding than the higher lying areas tehrany et al 2015 chapi et al 2017 for our study we derived the slope and elevation using the 25 m digital elevation model dem using arcgis pro fig 4 to assess inundation areas evaluating the overall topographic setting is crucial since the surface shape influences the flow accumulation macmillan and shary 2009 the division into landforms offers the possibility of a quantitative classification into different landscapes landform types can be determined by the characteristic terrain pattern which manifests through variation of the geomorphic features in shape size and scale macmillan and shary 2009 we applied the approach by weiss 2001 in which landforms are differentiated based on discrete slope position classes using the standard deviation of the topographic position index tpi to determine the tpi the elevation of each cell of a dem is compared to the mean elevations in the predefined vicinity of the cell weiss 2001 weiss 2001 distinguishes 10 landform types canyons midslope drainages upland drainages u shape valleys plains open slopes upper slopes local ridges midslope ridges and mountain tops we derived the landforms using the arcgis topography toolbox by dilts 2015 fig 5 land use influences the formation of runoff depending on the type of land use the nature of the topsoil changes and with it the infiltration capacity and surface roughness e g chandler et al 2018 sofia et al 2019 studies have shown that forests reduce the runoff volume and lower the peak discharge e g hundecha and bárdossy 2004 hümann et al 2011 while urbanization leads to an increase in peak flows e g miller and hess 2017 pumo et al 2017 niehoff et al 2002 have further proven that the impact of land use on the runoff generation depends on the precipitation characteristics accordingly the influence of land use on storm runoff generation is greater for convective storm events than for advective ones niehoff et al 2002 bronstert et al 2002 for the influencing factor land use the corine land cover dataset served as the data basis which was aggregated to the following classes built up area agricultural area forests and near natural areas wetlands water surfaces fig 4 as a supplement to land use we also considered the sealing degree in our models because the higher the degree of sealing the less water can infiltrate and the more surface runoff forms diakakis et al 2016 found that the sealing degree and the slope were the most important factors in explaining flash flooding in athens greece for our models we used the dataset of imperviousness density from copernicus 2018 which gives the percentage of soil sealing for each pixel fig 4 the porosity and permeability of the soils and rocks are decisive for the infiltration performance since the soil texture influences the hydraulic conductivity to account for the influence of the soil we incorporated the saturated hydraulic conductivity of the topsoil in the models in bavaria the saturated hydraulic conductivity varies between 0 and 156 cm d and averages 20 cm d fig 4 in this study we used the soil information contained in the soil map übk25 1 25 000 by lfu 2017b to derive the saturated hydraulic conductivity of the topsoil the übk25 uses among other things data from a geological map 1 25 000 forestry site maps and soil estimates each of the 614 map sheets of the übk25 was checked and supplemented by 300 to 800 boreholes and soil profiles in the field lfu 2017b for each soil profile detailed information about the soil layers and their properties is available in the übk25 profile database since the übk25 database does not contain information on saturated hydraulic conductivity we derived it as a function of soil texture and dry bulk density class each class combination of soil texture and dry bulk density was assigned a saturated hydraulic conductivity in cm d according to eckelmann 2005 we further averaged the saturated hydraulic conductivities according to their area fractions on each reference profile however due to the extrapolation of the soil profiles to the area the saturated hydraulic conductivity is subject to uncertainties bouvier et al 2021 have shown for a mediterranean catchment that subsurface flows can be predominant during flash floods in this context bedrock fractures and fracture orientation can influence the occurrence and direction of subsurface flow in catchments scaini et al 2018 bouvier et al 2021 therefore to reflect the runoff contribution of subsurface flows during flash floods we considered the permeability of the upper aquifer in our models the hydrogeological map hük 200 1 200 000 by bgr and sgd 2016 served as the data basis the hydrogeological map divides the upper aquifers of bavaria into 10 permeability classes ranging from high 10 2 10 3 m s to extremely low 10 9 m s fig 4 the hük 200 is derived from a geological map 1 200 000 and contains hydrogeological information on the upper large scale contiguous aquifer the permeability of the upper aquifer is based primarily on the evaluation of hydraulic tests performed on thousands of boreholes wells and gauges in areas where these hydraulic tests were not available hydrogeological units were assigned permeabilities by analogy bgr and sgd 2016 data uncertainties are particularly present in areas where complex geological hydrogeological conditions occur for example when the bedding conditions vary over small areas this was taken into account by assigning overarching permeability classes see table 5 the distance to a river is a good proxy of the probability of being affected by a flash flood and is thus a frequently chosen explanatory factor janizadeh et al 2019 hosseini et al 2020 khosravi et al 2018 because regions near streams are more likely affected by flooding than regions far away from any river however regions remote from streams can also be flooded by concentrated surface runoff we calculated the distance to the nearest stream using the euclidean distance tool of arcgis pro and the official river network of bavaria fig 5 in addition to the distance to a river we considered the metric height above the nearest drainage hand the hand value returns the hydrologic height by calculating the height of each catchment cell above the nearest river into which it flows nobre et al 2011 in summary the hand value normalizes the terrain heights with regard to the river network the hand value is often used to map susceptibility to flooding and gully erosion arabameri et al 2020 carvalho et al 2020 olorunfemi et al 2020 garousi nejad et al 2019 all in all the hand value is a valuable predictor for susceptibility to flash flooding that supplements the feature distance to a river we derived the hand metric using the arcgis toolbox by dilts 2015 the dem and the official river network of bavaria fig 5 heavy rain is the trigger of a flash flood and is therefore an important influencing factor hapuarachchi et al 2011 generally short and high intensity rains are associated with the occurrence of flash floods gaume et al 2009 borga et al 2011 lengfeld et al 2019 have shown based on radar measurements of 16 years that hourly precipitation events in germany are significantly less influenced by the orography than daily precipitation events to incorporate the rainfall in our models we used the dataset heavy rain hours from gdv and dwd 2018 fig 5 this dataset sums up all hours of heavy rain that occurred in germany since 2001 per zip code area to qualify as heavy rain the rain event had to exceed 25 l m2 in 1 h or 35 l m2 in 6 h gdv and dwd 2018 since there is a relationship between the catchment geomorphology and its hydrologic response costa 1987 harlin 1984 patnaik et al 2015 it is crucial to consider catchment properties as influencing factors we thus incorporated the catchment characteristics regarding shape topography and land use in our models it is known that the catchment shape affects the time of runoff concentration costache 2019b a rounded catchment will have a shorter concentration time than an elongated catchment schumm 1956 due to the shorter concentration time and the simultaneous drainage to the catchment center round catchments represent the greater risk for high peak discharges for each catchment we determined the elongation ratio which is a dimensionless parameter that quantifies the basin shape fig 6 according to schumm 1956 the elongation ratio e of a catchment is defined as follows 2 e 2 a π l b where a is the catchment area in km2 and l b is the basin length in km the elongation ratio of a catchment can be classified as follows more elongated 0 5 elongated 0 5 0 7 less elongated 0 7 0 8 oval 0 8 0 9 circular 0 9 schumm 1956 for further description of the catchment properties we delineated the length area relation using hack s law which is an empirical scaling law for river networks sassolas serrayet et al 2018 fig 6 based on fractal mathematics hack 1957 described the relationship between catchment size and mainstream length as a power function assuming that the mainstream length scales with the catchment size hack s law has been intensively studied and applied since its publication e g maritan et al 1996 reis 2006 sassolas serrayet et al 2018 hack s law is defined as follows 3 l 1 4 a 0 6 where a is the catchment area in km2 and l is the mainstream length in km using the given catchment size and hack s law we calculated the mainstream length in addition to the shape the topography of the catchment also plays a role in the runoff formation the basin relief allows conclusions about the geomorphological and hydrological characteristics of a catchment we used the relief to describe the terrain of the catchments fig 6 the catchment relief r is computed as the difference of the maximum catchment height h max and the minimum catchment height h min schumm 1956 4 r h max h min as another topography indicator we applied the melton ruggedness number fig 6 the melton ruggedness number is a slope index describing the relief conditions of a catchment in geomorphology the melton ruggedness number is mostly used to assess sediment transport processes within basins e g de scally and owens 2004 marchi and dalla fontana 2005 to quantify the ruggedness of a basin melton 1965 proposed the following dimensionless ratio 5 m r a where a is the catchment area in km2 and r is the basin relief in km the melton number can vary from zero to some large number although it rarely exceeds one melton numbers between 2 and 3 indicate a very rugged area for a first order catchment melton 1965 to further describe the catchment response to heavy rain we considered the land use distribution within the catchments to do so we included the percentage of built up area and agricultural area of a catchment derived from the corine land cover dataset fig 6 the spatial relationship between the sample points plays an important role because as we know from tobler s first law of geography everything is related to everything else but near things are more related than distant things tobler 1970 to account for the spatial dimension in the model we included the x and y coordinate as explanatory features however to reduce the number of features and minimize collinearity as well as to increase the informative value of the predictor we used the product of the x and y coordinates 4 5 data uncertainties and assumptions the hios dataset consists mainly of quality controlled heavy rain induced flood events two thirds of the events are fully verified meaning that an expert has not only confirmed the event but also investigated the nature and impact of the flood event more than a quarter of the events were verified by a trusted source such as the german weather service only 6 of the events have a low quality level as these events are considered plausible only due to the general meteorological situation in the affected region and period kaiser et al 2020 the data basis for hydrologic or hydrodynamic modeling and ml based modeling differs in some aspects to reconstruct a flash flood event for example by means of hydrological modeling highly accurate spatiotemporal information on precipitation and runoff is required this event information is often collected in post event surveys in the form of radar images station measurements discharge estimations videos photos and witness statements this information is then used to simulate the event runoff using a hydrological or hydrodynamic model based on the event precipitation since ml modeling is performed event independent precise knowledge of the spatiotemporal distribution of the event precipitation or runoff is not required for modeling nonetheless an event must meet the criteria explained in section 4 2 to be considered in the flood inventory including exceeding a specified precipitation amount during a specified time therefore although it is important for an ml driven approach to know the exact location of the event and the area characteristics the spatial and temporal resolution of the event information can be lower than required for a hydrologic hydrodynamic simulation the ml based derivation of the pluvial and flash flood susceptibility is based solely on static and event independent influencing factors thus the machine learning approach presented here differs fundamentally from hydrological or hydrodynamic modeling which is performed applying an event or scenario specific precipitation input in contrast the susceptibility derived using machine learning methods results solely from the properties of an area consequently it is possible that areas classified as having low susceptibility to heavy rain induced flooding could experience a flash flood under unfavorable conditions such as high antecedent soil moisture and extreme rainfall amounts furthermore we ensured that the spatial and temporal resolution of the underlying datasets of the influencing factors was as uniform as possible most of the datasets used are at a resolution of 1 25 000 and are from 2014 to 2020 table 3 since most of the 17 influencing factors are static e g slope elevation landforms we estimate the influence of temporal changes in the datasets on the modeling result to be minor only the influencing factors land use as well as the land use distribution in the catchments and the sealing degree are subject to temporal changes because most flood events cover a similar period as the datasets used namely 2006 to 2017 kaiser et al 2021 we assume that inaccuracies in the datasets due to temporal changes are small 4 6 feature preparation we prepared the influencing factors with the gis software arcgis pro 2 6 for further feature processing we used the python library scikit learn pedregosa et al 2011 to avoid losing valuable samples we replaced missing values in the dataset for categorical features we used the function simpleimputer to replace missing values with the most frequent feature value missing numerical values were replaced using the mean value from the five nearest neighbors in the dataset that have a value for the feature knnimputer we transformed the three categorical features i e landforms land use permeability of the upper aquifer into numbers as required by machine learning algorithms using weight of evidence woe encoding table 4 the woe method is a bivariate bayesian statistical approach that has been widely used in flood susceptibility studies e g tehrany et al 2014 hong et al 2018 costache 2019a 2019b using the woe method we can measure the predictive power of an independent variable an influencing factor regarding the dependent variable pluvial and flash flood occurrence in the case of flood modeling the woe coefficient is computed based on the relationship between the non occurrence and occurrence of pluvial and flash floods within each class of the categorical feature the woe coefficient for each categorical class is calculated as follows 6 woe ln b a b a where b indicates the number of flood pixels in the given class and a the total number of flood pixels of all classes accordingly the number of non flood pixels within the investigated class is given by b and the number of non flood pixels of all classes is represented by a we scaled all features using standardization standardscaler which is less affected by outliers than min max scaling in some flood susceptibility studies the numerical features are discretized using the natural breaks or quantile method e g costache 2019a 2019b tien bui et al 2019 tang et al 2020 however we did not group the continuous features as tree based classifiers are good at determining complex relationships between the independent and dependent variables 4 7 classifiers applied in this study we compare the ensemble methods random forest gradient boosting decision tree and catboost before choosing these three classifiers we tried several different machine learning algorithms e g naïve bayes support vector machines neural networks adaboost decision tree without optimizing their hyperparameters based on the first classification results we selected the three most promising of all classifiers in the following subsections we briefly describe the three classifiers used in this study a comprehensive description of the underlying principles can be found in the literature e g kuhn and johnson 2016 bentéjac et al 2020 bonaccorso 2020 4 7 1 random forest in 2001 leo breiman first introduced the random forest rf model breiman 2001 a random forest model consists of a large number of decision trees that are as uncorrelated as possible to generate an uncorrelated forest of trees the random forest uses bagging and feature randomness in the formation of the decision trees this means that each tree receives a random sample that is drawn from the training set with replacement bootstrapping each tree also only receives a random subset of the features due to this implemented randomness the prediction of the random forest model is often more accurate than the predictions of any individual decision tree breiman 2001 the random forest classifier is one of the most frequently used classification algorithms this is probably because random forest models are powerful ensemble methods that are not sensitive to multicollinearity and can handle missing and unbalanced data random forests have been applied in many studies of flood susceptibility mapping and achieved high predictive accuracies chen et al 2020 costache et al 2020a hosseini et al 2020 hong et al 2018 tang et al 2020 in this study we used the randomforestclassifier from the machine learning library scikit learn pedregosa et al 2011 for the python programming language unlike the original publication by breiman 2001 the scikit learn implementation averages the probabilistic prediction of the decision trees soft voting rather than using the majority decision of the classifier votes hard voting pedregosa et al 2011 compared to hard voting soft voting often achieves higher performance because it accounts for the uncertainty of each classifier in the final decision géron 2017 4 7 2 gradient boosting decision tree boosting is a method to create ensemble models under boosting we understand the training of several weak learners in a sequential and adaptive manner to obtain a stronger learner kuhn and johnson 2016 at first one initial model is fitted to the data then a second model is trained concentrating on improving the shortcomings of the previous model and so on the underlying assumption is that the combination of the models is better than one model alone since each subsequent model tries to improve the shortcomings of the combined ensemble model many boosting algorithms exist however adaboost and gradient boosting are among the most popular géron 2017 the gradient boosting algorithm was first presented by breiman 1997 and further developed by friedman 2001 gradient boosting fits the new learner to the residual errors of its predecessor the predecessor s shortcomings are identified by the gradient e g residual of an exponential loss function after the current model is added to the previous model a new model is fit to the residuals to minimize the loss function and so on kuhn and johnson 2016 in our study we applied the class gradientboostingclassifier of scikit learn pedregosa et al 2011 that uses decision trees as base estimators by default since each decision tree of the ensemble model uses a different feature subset to select the node s best split the individual decision trees differ from each other and thus can capture different signals from the data the gradient boosting algorithm has not yet been used in a flood susceptibility study however bui et al 2019b already applied the boosting algorithms adaboost and logitboost for flash flood susceptibility modeling 4 7 3 catboost the third classifier applied in this study also uses gradient boosting on decision trees and is called catboost for categorical boosting prokhorenkova et al 2018 as with the gradientboostingclassifier the decision trees of the catboost model differ from each other because they use different feature subsets the catboost classifier is available as an open source library for python provided by the technology and internet services company yandex the catboost algorithm differs from classical gradient boosting in two ways first catboost applies ordered boosting which is a permutation driven alternative to the standard gradient boosting algorithm prokhorenkova et al 2018 at each training step catboost uses the independent permuted historical samples and thus achieves unbiased boosting second catboost uses ordered target statistics to encode categorical features hancock and khoshgoftaar 2020 both techniques were implemented to combat target leakage a specific type of overfitting that occurs in all classical gradient boosting algorithms dorogush et al 2018 prokhorenkova et al 2018 due to the two algorithmic advances catboost has advantages over other implementations of gradient boosted decision trees dorogush et al 2018 proved that catboost outperforms other gradient boosting algorithms such as xgboost lightgbm or h2o on popular datasets besides catboost handles heterogeneous datasets with categorical features well and is easy to use hancock and khoshgoftaar 2020 in geosciences the catboost algorithm has recently been used to classify formation lithology dev and eden 2019 and to estimate reference evapotranspiration huang et al 2019 zhang et al 2020 kang et al 2020 also applied catboost to calculate an hourly wildfire risk index in a comparative study of four gradient boosting algorithms for landslide susceptibility mapping catboost achieved the highest predictive ability sahin 2020 recently hancock and khoshgoftaar 2020 reviewed studies from various disciplines that employed the catboost algorithm to analyze its effectiveness and shortcomings 4 8 model tuning to avoid overfitting we used 5 fold cross validation to tune the hyperparameters of the three ensemble models for this purpose the training set is divided into five distinct subsets folds one subset is used as the validation set and the remaining four subsets are used as the training set then the model is fitted on the training set and validated against the validation set this procedure is repeated for each of the five subsets using 5 fold cross validation the best combination of hyperparameter values for the three models was determined and applied to the training set depending on the model type different regularization parameters were optimized the models were constrained by limiting the size of their decision trees by the maximum depth of the tree max depth the number of features to consider when looking for the best split max features the minimum number of samples required to be at a leaf node min samples leaf the minimum number of samples required to split an internal node min samples split in addition the number of decision trees was limited n estimators for the gradient boosted algorithms the contribution of each tree was regulated by the learning rate the values of the regularization parameters are shown in table 6 the performance evaluation of the three classifiers was performed on the unseen test set 4 9 performance measures we assessed the predictive ability of the selected classifiers using various performance metrics the receiver operating characteristic roc curve is used to assess binary classifiers altman and bland 1994 brown and davis 2006 fawcett 2006 the roc curve plots the false positive rate fpr on the x axis against the true positive rate tpr on the y axis the true positive rate also called recall or sensitivity is the ratio of positive samples correctly identified by the classifier 7 tpr tp tp f n where tp is the number of true positive samples and fn is the number of false negative samples conversely the false positive rate describes the ratio of negative samples that are incorrectly identified as positive 8 fpr fp fp t n 1 t n r where tn is the number of true negative samples and fp is the number of false positive samples the fpr is equal to one minus the true negative rate tnr which is also called specificity the tnr is the proportion of negative samples correctly classified as negative 9 tnr tn tn f p the positive predictive rate ppr also called precision returns the accuracy of the classifiers positive predictions and is defined as follows 10 ppr tp tp f p conversely the negative predictive rate npr describes the accuracy of the classifiers negative predictions 11 npr tn tn f n the roc curve displays the classifier s ability to discriminate between a flood and a non flood event the diagonal of the roc curve represents a random classifier the aim is to obtain a classifier that is as far away from the diagonal as possible however there is a trade off as higher true positive rates come along with more false positive predictions for visual comparison the classifiers roc curves are superimposed graphically to compare the classifiers roc curves quantitatively we used the area under the roc curve auc see fig 7 the auc value measures the two dimensional area underneath the roc curve and thus aggregates the classifier s performance for all possible candidate thresholds possible auc values range from 0 to 1 the higher the auc value the better the predictive power of the classifier while an auc value of 1 represents the perfect classifier when comparing the models the most effective model is the one with the largest area under the roc curve we further evaluated the classifiers performances using the metrics accuracy and cohen s kappa also called kappa statistic the accuracy measures the correctly identified samples and is defined as follows 12 accuracy tp t n tp f p t n f n however the disadvantage of the accuracy metric is that it does not differentiate between the types of error being made cohen s kappa in contrast considers the class distributions of the training set kuhn and johnson 2016 cohen s kappa measures the agreement between two raters on a classification problem taking into account the accuracy that would result from chance alone cohen 1960 the kappa statistic can be computed as 13 κ o e 1 e where o is the observed accuracy and e is the expected accuracy by chance according to chicco et al 2021 the observed accuracy o and the expected accuracy e can be computed for binary classification problems as follows 14 o tp t n tp f p t n f n 15 e tp f p n tp f n n tn f p n tn f n n where n is the number of samples in the dataset and the expected accuracy is the value of accuracy under statistical independence of the observers chicco et al 2021 cohen s kappa can take values between 1 and 1 while 1 means perfect agreement and 0 means no agreement between the observations and predictions if the kappa statistic is less than 0 than the classifier is worse than agreement by chance landis and koch 1977 proposed the following classification of the kappa statistic 0 no agreement 0 0 20 slight 0 21 0 40 fair 0 41 0 60 moderate 0 61 0 80 substantial and 0 81 1 almost perfect agreement 4 10 model specific and model agnostic interpretation methods in this paper we applied model specific and model agnostic methods to interpret the trained classification model as model specific methods we used feature importance and pairwise feature importance also called feature interaction the feature importance describes the strength of the relationship between the predictor and the outcome in most tree based models feature importance is an intrinsic measure that monitors the performance as each feature is added to the model kuhn and johnson 2016 the feature importance ranges between 0 and 1 and the sum of the importance of all features is 1 in addition to feature importance we computed feature interaction which is a two way interaction measure indicating the interaction strength for each pair of features the feature interaction is a dimensionless statistic based on friedman s h statistic and describes the proportion of the variance explained by the interaction friedman and popescu 2008 as a model agnostic method we applied shapley additive explanations shap by lundberg and lee 2017 shap is a unified framework for interpreting individual predictions by assigning an importance value to each feature lundberg and lee 2017 shap computes shapley values shapley 1953 from coalitional game theory which describe how much each player feature contributed to the game prediction molnar 2019 the shapley value represents the average marginal contribution of a feature value to a prediction instance in all possible coalitions for each possible coalition the prediction instance is computed with and without the feature value with the difference representing the marginal contribution for a detailed description of the calculation and theory of shapley values and shap please see shapley 1953 lundberg and lee 2017 sundararajan and najmi 2020 and molnar 2019 to understand how much the classification model relies on each influencing factor for making predictions we used the training data to calculate feature importance feature interaction and shap 5 results 5 1 model validation and comparison we validated the three ensemble models using the 373 flood and non flood locations of the test set we did not apply spatial tolerance to the agreement between the observations and the classification results because the application of spatial tolerance would not affect the outcome this is because in the immediate vicinity of the sample points the values of most influencing factors change little or not at all for example the six catchment related factors and the zip code based heavy rain hours do not change in addition the soil geology and landform describing factors tend to be related to larger areas since the elevation usually changes gradually the terrain dependent influencing factors hand and slope also change rather gradually around the sample points all three models performed equally well on the test set and achieved similar performance statistics table 7 the catboost model correctly detected the most flood and non flood locations of the three models 281 the gradient boosting decision tree model achieved the highest sensitivity with 76 9 followed by the catboost and random forest model with 75 3 each regarding predictive accuracy the catboost model performed best 75 3 followed by the random forest model 75 1 and the gradient boosting decision tree model 74 8 according to the kappa classification by landis and koch 1977 all three models achieved a moderate agreement with the catboost model having the highest kappa value 0 51 we evaluated the global performance of the ensemble methods using the au roc method fig 7 compares the three classifiers roc curves and indicates their auc values the catboost random forest and gradient boosting decision tree models all achieved high auc values for the test set however the catboost model has the highest predictive power auc 81 9 followed by the random forest auc 81 6 and the gradient boosting decision tree model auc 81 3 overall we find that all three models proved to be powerful in identifying the general pattern of pluvial and flash flood susceptibility for a large area the predictive power of the classifiers is comparably good and thus no classifier significantly outperforms the others regarding accuracy auc and kappa statistic the catboost model is the best performing since the catboost model has the highest predictive power we used it to derive bavaria s pluvial and flash flood susceptibility 5 2 model interpretation based on the feature importance we rank the 17 influencing factors fig 8 by far the highest predictive power is attributed to the sealing degree 16 8 and the built up area share of a catchment 11 7 the importance of the 15 remaining influencing factors ranges between 7 and 2 the melton number of a catchment 7 2 and the hours of heavy rain 7 1 are equally important followed by x y coordinate 6 4 hand 6 0 and the proportion of the agricultural area of a catchment 5 9 contrary to our expectations the slope is not among the most predictive explanatory factors on the contrary the slope ranges in the lower third with 4 1 according to the feature importance the catchment related influencing factors are as important for model prediction as the spatially distributed ones among the three most influential explanatory factors two are catchment related built up area share melton number however catchment related variables describing land use distribution and relief conditions in the catchment are more important than variables describing the catchment shape the distance to the river is the least contributing influencing factor with 2 4 for model interpretation we also investigated pairwise feature importance of the 136 possible pairs of influencing factors we plotted the top 15 interaction pairs in fig 9 these 15 pairs contain 11 different influencing factors four of which are catchment related the sealing degree and the built up area share which had the highest feature importance occur in 9 and 6 of the 15 interaction pairs respectively interestingly the factor agricultural area share appears in the second and third strongest pair with the sealing degree and the proportion of the built up area although its feature importance was not so high however the agricultural area share probably complements the sealing degree and the proportion of the built up area although the slope s single feature importance was only ranked 12th the slope forms strong pairs with the built up area share the sealing degree and the heavy rain hours similarly the saturated hydraulic conductivity which was only ranked 10th regarding feature importance forms a strong pair with the sealing degree ranked 5th to investigate the relationship between the influencing factors and the prediction we used a shap summary plot the summary plot in fig 10 orders the features on the y axis according to their importance each point represents a shap value for a feature value and a prediction instance the continuous color scale indicates low to high feature values in the case of overlapping points the points were jittered in the direction of the y axis sealing degree and built up area share are the only factors that reach absolute shap values above 1 and thus can strongly influence the prediction result in one direction or the other a high sealing degree significantly increases the risk of flooding while medium to low sealing degrees reduce flood risk a high percentage of built up area in a catchment generally increases flood risk in addition to the sealing degree and the built up area share six other influencing factors can significantly affect the prediction absolute shap values above 0 5 heavy rain hours x y coordinate melton number agricultural area share elevation and saturated hydraulic conductivity it is plausible that a high number of heavy rain hours increases the risk of flooding the less rugged a catchment is low melton number the lower the risk of being affected by flash flooding interestingly a high percentage of agricultural land in a catchment leads to a decrease in flood risk and conversely a low percentage of agricultural area to an increase this can probably be attributed to the fact that the proportion of agricultural land is related to the proportion of built up land and thus a low proportion of agricultural land could mean a high proportion of built up area the summary plot also reveals that low lying areas are more susceptible and high values of saturated hydraulic conductivity reduce flood hazard overall the classification model seems to confirm relationships between influencing factors and flash flood occurrence that are known or suspected among hydrologists according to the summary plot elongated catchments with little ruggedness low relief and a low proportion of built up areas should be less susceptible to flash flooding in addition highly permeable topsoils and upper aquifers can reduce the risk of flooding although we can now better understand the relationship between the influencing factors and the prediction result we should be cautious about deriving general rules from the summary plot since the influencing factors affect each other each prediction instance is the result of all feature values therefore a low proportion of built up area in a catchment does not necessarily reduce flood risk fig 10 as shown in fig 9 the built up area share interacts strongly with the sealing degree the agricultural area share the heavy rain hours the melton number the relief and the slope the same is true for slope for example there is a tendency for high slope values to increase the risk of flooding fig 10 however there are also cases where the flood risk was reduced despite high slope values probably due to the strong interaction of the slope with the built up area share the sealing degree and the heavy rain hours fig 9 5 3 susceptibility map using the catboost model we determined the susceptibility of all raster pixels of bavaria as an indication of susceptibility we used the predicted probability of each raster pixel to be assigned to class 1 affected returned by the catboost model the predicted probability is the result of weighting each tree in the ensemble which in turn calculate a probability of class 1 for each of their leaves the susceptibility values ranged from 0 20 to 0 99 the methods quantile or natural breaks are usually chosen for classifying flash flood susceptibility maps e g costache 2019a khosravi et al 2018 ngo et al 2018 youssef et al 2016 with the natural breaks classification method classes are based on natural groupings within the dataset class boundaries are set in such a way that similar values are summarized and differences between the classes are maximized since our susceptibility values have a skewed distribution the natural breaks classification is more appropriate than the quantile classification using the natural breaks method we divided the prediction dataset into four susceptibility classes labeled low 0 20 moderate 0 20 0 35 high 0 35 0 53 and very high 0 53 0 99 the susceptibility classes low and moderate cover 37 and 33 of the bavarian region respectively according to our model 21 of bavaria is considered highly susceptible the highest susceptibility class makes up 8 of the region overall 30 of the region of bavaria is at high or very high risk of pluvial and flash flooding fig 11 regions with high and very high risk occur throughout bavaria the most flood prone areas are identified in the alpine region in the south of the alpine foothills and in the border region of southeast bavaria furthermore we find highly susceptible areas in northern bavaria especially along the main river and toward the region of hesse in northwestern bavaria less susceptible areas are identified in central bavaria 5 4 endangered cities to compare the hazard situation among cities we propose an overall susceptibility score for cities that is based on the susceptibility map generated the susceptibility score of the cities may help decision makers on regional level to prioritize cities for detailed investigations we calculated the overall susceptibility of a city using the smallest circumscribing rectangles so called bounding boxes of the cities the dataset by bkg 2015 provides the bounding boxes for all german cities which we used as an approximation of the city area within each bounding box we determined the area weighted average of the four susceptibility classes which was used as the city s overall susceptibility when calculating the overall susceptibility of a city we assume that the proportions of the susceptibility classes within the bounding box approximate the city s hazard situation consequently a city with a large number of high to very high classified raster pixels is more at risk than a city with a large number of low to moderate classified pixels we determined the overall susceptibility of all bavarian cities fig 12 similar to the pluvial and flash flood susceptibility map we classified the city scores into four susceptibility classes low moderate high very high using the natural breaks method according to this classification 28 and 32 of the cities are considered to be at low and moderate risk respectively classified as highly susceptible are 24 of the cities in the highest susceptibility class are 16 of the bavarian cities the cities at particularly high risk are distributed across bavaria but cluster in specific regions fig 12 especially in the southern part of the natural region eastern low mountain range many cities are classified as very endangered the border region with neighboring austria in southeastern bavaria also has many cities colored red furthermore the southern part of the alpine foothills as well as munich and its surrounding area are endangered in the north of bavaria the nuremberg metropolitan region stands out with the major cities of nuremberg fürth and erlangen in addition würzburg and the region toward the region of hesse along the main river are classified as highly endangered however not all major cities are automatically classified as being at highest risk as the regions around augsburg and ingolstadt show in histograms we compare the distribution of the susceptibility scores of the bavarian cities in the training and test set fig 13 in both the training and the test set the distribution of the susceptibility scores is roughly balanced however when differentiating the cities according to their affectedness it becomes apparent that affected cities tend to be assigned a higher and unaffected cities a lower susceptibility score this tendency is evident in both the training and test set although it is more pronounced in the training set on which the model was trained the histograms suggest that the overall classification of the cities can be considered plausible 6 discussion due to the nature of the flood inventory and its underlying assumptions limitations must be considered when evaluating model performance and interpreting the susceptibility map in terms of spatial accuracy the affected cities can be accurately located however the occurred inundation areas within the cities are unknown because they are not collected by the available sources it is not possible to map the affected inundation areas for hundreds of events at regional level therefore map interpretation must take into account limitations in spatial accuracy there are also limitations to consider regarding the comprehensiveness of the event dataset on the one hand it can be assumed that the flood inventory contains only a part of the events that occurred in reality on the other hand it is not known with certainty which cities were not affected therefore we assumed that no observation means no event however this assumption likely affects model performance since affected cities or cities that are highly susceptible but have not yet had an event are probably also among the negative examples thus statistical measures that evaluate false negative or true positive results e g sensitivity should be used primarily in evaluating model performance regarding the nature of the flood inventory it should be noted that the event dataset focuses on populated areas this is due to the applied event definition which relates the flood event to the city it affected where the term city stands for any settlement size ranging from a village to a city that can still be uniquely identified consequently flash floods that occurred in an uninhabited catchment for example are not included in our flood inventory due to the inventory s urban focus it is possible that related influencing factors such as the sealing degree and the catchment s built up area share gain weight in the classification model to the best of our knowledge only one study exists to date in which a flash flood susceptibility map was derived for an area larger than bavaria 70 500 km2 ma et al 2019 generated a susceptibility map for yunnan province china that covers an area of 380 000 km2 in recent flash flood susceptibility studies cf table 1 investigation areas generally range from 200 to 4 000 km2 and focus on watersheds rather than political entities an exception is the study by hosseini et al 2020 which investigates the flash flood hazard in the gorganroud river basin iran covering an area of 11 300 km2 compared to the other studies listed in table 1 the density of our sample points was lower the reviewed studies used between 7 and 213 sample points per 100 km2 with a median of 42 points per 100 km2 in our study we had only 3 points per 100 km2 available consequently the investigation sites in comparable studies are usually not only smaller but there are also more data points available for training and testing the comparatively low point density of our sample data certainly reduces the performance of our model this is because our model has to learn the complex relationships within four different major landscapes covering low mountain ranges cultural landscapes and the alps with only a few data therefore compared to other flash flood susceptibility studies differences in performance are apparent some ml models achieved auc values above 0 85 and even above 0 95 cf janizadeh et al 2019 bui et al 2019a khosravi et al 2018 tien bui et al 2020 however our performance metrics are in similar ranges as in the study by ma et al 2019 whose winning model achieved a kappa value of 0 59 an auc value of 0 81 and an accuracy of 0 79 nevertheless due to the spatially homogeneous distribution of the sample data and attention to the representativeness of major landscapes our model achieved good performance even at low sample point density an increase in model performance would only be conceivable with more training and test data so that the heterogeneity of the study area could be better learned and reproduced compared to similar flash flood studies we included more influencing factors in our model the studies we examined used between 7 and 12 influencing factors table 1 while we used 17 influencing factors a further reduction in the number of influencing factors led to a performance decrease so that 17 influencing factors were necessary to achieve the best possible model performance we assume that the large heterogeneity in the study area and the small amount of learning data necessitated a higher number of different influencing factors in contrast to comparable studies we included catchment related influencing factors in addition to spatially distributed ones the studies listed in table 1 do not use catchment related factors with exception of the study by costache 2019b costache 2019b used the catchment s circularity ratio among other spatially distributed influencing factors yet there are also studies that derived flash flood susceptibility maps based only on catchment related influencing factors but these use gis techniques and not machine learning e g abdelkareem 2017 abdo 2020 adnan et al 2019 in our model the catchment related influencing factors are crucial for prediction performance according to the feature importance the built up area share and the melton number both catchment related factors are the second and third most contributing influencing factors due to the use of catchment specific influencing factors entire catchments are sometimes assigned to the same susceptibility class this is the case when important catchment specific influencing factors such as the built up area share or the melton number assume critical values furthermore the representation of the map in four susceptibility classes makes minor differences disappear since our susceptibility map is intended to provide initial indications and is not a substitute for a detailed site investigation we decided to retain the catchment related influencing factors in the end we prioritized a higher degree of accuracy over representation 7 conclusion in this study we developed a novel methodology to enable pluvial and flash flood susceptibility assessments for vast territories by using a tree based ensemble method that considers both spatially distributed and catchment related influencing factors the performance of the developed methodology was evaluated based on the region of bavaria germany and three state of the art machine learning models random forest gradient boosting decision tree and catboost we trained the models using 11 spatially distributed and six catchment related influencing factors all three models performed well with auc values above 0 8 comparing the performance measures however the catboost model achieved the best performance and was therefore used to derive the pluvial and flash flood susceptibility map our goal was to develop a methodology to identify pluvial and flash flood susceptible areas at regional scale for this we investigated how to generate a susceptibility map for a vast region which is covering various major landscapes using a machine learning model our findings can be summarized as follows when modeling vast areas it is critical to account for low sample point density by ensuring i a homogeneous spatial coverage of the study area and ii the representation of the major landscapes in the training and test set to capture the natural heterogeneity of vast areas a larger number of spatially distributed and catchment related influencing factors is required than otherwise applied in previous studies to achieve high model performance the selected influencing factors must comprehensively describe the different characteristics of the various major landscapes in the study area there are area characteristics that can reduce susceptibility to flash flooding elongated catchments with low ruggedness and relief a low proportion of built up areas and highly permeable topsoils and upper aquifers are likely to be less susceptible to flash flooding by averaging the susceptibility classes within a city area on an area weighted basis an overall susceptibility classification for a city can be provided this overall susceptibility score allows for comparison between cities e g for prioritization purposes nevertheless the validity of our susceptibility assessment is affected by the small amount of training data that is focused on urban areas and the non consideration of time variable influences in our model we neglected time variant influencing factors such as triggering precipitation antecedent soil moisture or phenology of crops which can severely impact event magnitude in addition the susceptibility map only indicates the actual state since influencing factors such as the sealing degree and the proportion of built up area in the catchment can change over the years with the help of the method presented pluvial and flash flood susceptibility maps can be generated at regional level providing important initial indications for those responsible for spatial planning and flood risk management furthermore in depth investigations can be prioritized and initiated based on the proposed overall susceptibility score for cities in addition we have proven that machine learning models can handle naturally heterogeneous large study areas even with a small amount of training data further work should elaborate whether these new machine learning derived maps are understood and applied by the public and those in charge since distrust of machine learning is still high among the public researchers should also improve on the interpretability of the underlying models and the traceability of the derived flash flood susceptibility maps to increase trust it is necessary to communicate the strengths and weaknesses of the machine learning model and explain the model decisions with examples author contribution this work was conducted by m kaiser under the supervision and guidance of s günnemann and m disse m kaiser prepared the article all authors contributed to the final writing of the paper declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the research presented in this paper has been carried out as part of the hios project hinweiskarte oberflächenabfluss und sturzflut funded by the bavarian state ministry of the environment and consumer protection stmuv and supervised by the bavarian environment agency lfu furthermore this work was supported by the deutsche forschungsgemeinschaft dfg through the tum international graduate school of science and engineering igsse 
3224,flood events triggered by heavy rain such as pluvial and flash floods are a common threat worldwide however it is usually not known which areas and cities are particularly vulnerable to flooding caused by heavy rain to enable regional scale susceptibility assessment we developed a novel methodology based on tree based classifiers that considers both spatially distributed and catchment related influencing factors the performance of the developed methodology was evaluated using the region of bavaria germany for the case study area 70 500 km2 we trained a random forest rf a gradient boosting decision tree gbdt and a catboost model cb using 1 864 flood and non flood locations and 11 spatially distributed and six catchment related influencing factors regarding performance metrics all three models performed equally well cb auc 0 819 rf auc 0 816 gbdt auc 0 813 with the catboost model performing best when modeling large areas it proved critical to account for low sample density by ensuring i a homogeneous spatial coverage of the study area and ii the representation of the major landscapes in the training and test set in addition we propose an overall susceptibility score for cities based on the susceptibility map generated which can be used to prioritize cities for detailed investigations those responsible for spatial planning and flood risk management can apply the proposed methodology to obtain a pluvial and flash flood susceptibility assessment for large territories keywords flash flood machine learning catboost ensemble method shapley additive explanations data availability the authors do not have permission to share data 1 introduction since flash floods pose a deadly threat to humans researchers are looking for ways to improve flash flood forecasting and warning yet protecting against the natural hazard is challenging as flash floods are difficult to predict and can occur anywhere nonetheless researchers have succeeded in establishing early warning systems using weather radar hydrologic and hydrodynamic models and drainage network monitoring e g smith et al 2007 javelle et al 2010 looper and vieux 2012 bartos et al 2018 hofmann and schüttrumpf 2020 for warning purposes researchers identify flash flood susceptible areas using various modeling techniques such as gis hydrodynamic models or data driven models e g iosub et al 2020 nguyen et al 2020 li et al 2019 for several years researchers have increasingly used machine learning ml models instead of hydrodynamic or physical models in flood modeling according to mosavi et al 2018 the increasing popularity is because ml models can describe the flood events nonlinearity based on historical data alone without having to consider physical processes in addition ml models are less costly and faster to set up than hydrodynamic models wagenaar et al 2020 meanwhile researchers have successfully derived flood and flash flood susceptibility maps using ml models e g bui et al 2019a chen et al 2019 tien bui et al 2020 the ml algorithms applied in flash flood susceptibility modeling are diverse logistic regression e g costache 2019a decision trees e g costache and tien bui 2019 support vector machines e g tehrany et al 2014 naïve bayes e g khosravi et al 2019 ensemble methods e g bui et al 2019b artificial neural networks e g ngo et al 2018 to further improve prediction performance researchers have hybridized ml algorithms with bivariate models e g frequency ratio weights of evidence shannon s entropy multi criteria decision methods or gis techniques e g wang et al 2019 khosravi et al 2016 costache et al 2020c to identify pluvial and flash flood susceptible areas at regional level we need a modeling technique that is affordable quick and can process a large study area since the setup of hydrodynamic models is time consuming and the modeling is computationally intensive the application of hydrodynamic models to large regions is not practical however the application of data driven models in the literature is usually limited to individual catchments with sizes between 200 and 4 000 km2 e g youssef et al 2016 costache et al 2020b khosravi et al 2018 with the exception of ma et al 2019 ml algorithms have not yet been applied to vast regions with natural heterogeneity to take full advantage of data driven models for assessing flash floods we need to explore the challenges of modeling heterogeneous areas of several 10 000 km2 in this paper we illustrate how to generate a pluvial and flash flood susceptibility map for the region of bavaria germany using tree based ensemble models and appropriate explanatory factors first we review the influencing factors applied in flash flood susceptibility studies section 2 this is followed by a description of the study area section 3 we then introduce the datasets and methods used section 4 and present the study results section 5 finally we discuss our findings from susceptibility map generation at regional scale using tree based algorithms section 6 and provide concluding remarks section 7 2 influencing factors on flash flood occurrence to set up a good predictive model it is crucial to choose robust explanatory features several studies have proven that it is possible to predict whether a location was affected by a flash flood in the past based on area characteristics bui et al 2019a nguyen et al 2020 pham et al 2020a to find out the most frequently applied explanatory features we compared 23 recent studies on flash flood modeling that applied machine learning algorithms for direct comparison we listed the explanatory features used in each of these studies in table 1 we summarized the influencing factors in the categories of topography soil and geology land cover river network precipitation characteristics and anthropogenic factors in the studies examined the number and type of influencing factors applied varied we identified a total of 37 different influencing factors 12 of which were topography related the explanatory factors applied cover a broad spectrum and range from elevation and soil type through the fraction of vegetation cover and river density to the sealing degree the studies reviewed used between 6 and 15 influencing factors for flash flood modeling with 10 factors representing the median topographic parameters were the most frequently used explanatory factors in these studies table 1 the studies examined applied between 2 and 7 topographic parameters 5 parameters on average the slope as the only explanatory factor was used by each of the 23 studies reviewed other frequently considered topographic parameters were aspect elevation and the topographic wetness index twi which were applied in 16 15 and 14 studies respectively the most frequently used influencing factor after slope was lithology which was considered in 19 of the 23 studies in addition to the lithology soil properties were described by parameters such as soil type hydrological soil group and soil depth usually one or two parameters of the category soil and geology were used in four studies however no soil properties were considered in addition to lithology we attributed five explanatory factors to the category land cover namely normalized difference vegetation index ndvi fraction of vegetation cover land use curve number a combination of soil type and land use and surface roughness usually one of these five land cover parameters was used to explain the flash flood occurrence in 61 of the studies the explanatory factor land use was chosen which makes it the fifth most frequently used parameter besides twi in the studies investigated six parameters were used to describe the characteristics of the river network most often the parameter river density was applied followed by the distance to the river and the convergence index the flow direction in contrast was only considered in the study by pham et al 2020b in three studies a maximum of three river network parameters was considered simultaneously while the study by tang et al 2019 completely disregarded parameters of the river network in its modeling precipitation characteristics were considered in 14 studies anthropogenic factors in only three studies we counted population sealing degree gross domestic product and flash flood prevention investments among the anthropogenic factors the studies in which precipitation was considered generally used information on the mean of annual maximum precipitation amount for different durations e g 3h 24h and the precipitation amount of a specific event that triggered flash floods according to our literature review it is at least necessary to describe an area s topography lithology land use and river network to explain the occurrence of flash flooding with a ml model since topography has a significant influence on the occurrence of a flash flood the topography of an area must be comprehensively described by several parameters e g slope aspect elevation twi in the ml model the comparison of the studies also suggests that lithology is an essential parameter in explaining flash flooding often used in combination with soil type or hydrological soil group the characteristics of the river network should also be considered in the ml model e g via river density distance to the river convergence index although no river network parameter such as slope or lithology has prevailed in previous studies in addition to the predictive power of the influencing factor the availability of the underlying dataset certainly plays a role in the choice of influencing factors how well the chosen parameters can explain flash flood occurrence depends not least on the combination of parameters the quality of the underlying datasets and the characteristics of the study area the investigated studies were carried out in six different countries vietnam bui et al 2019b ngo et al 2018 nguyen et al 2020 pham et al 2020b tien bui et al 2019 tien bui et al 2020 romania costache and tien bui 2020 costache 2019b 2019a costache and zaharia 2017 costache et al 2019 costache et al 2020a costache et al 2020b costache et al 2020c iran hosseini et al 2020 janizadeh et al 2019 khosravi et al 2016 chapi et al 2017 pham et al 2020a khosravi et al 2018 china tang et al 2019 ma et al 2019 chen et al 2019 saudi arabia youssef et al 2016 and greece diakakis et al 2016 of the 23 studies 9 studies were performed in europe and 14 at study sites in asia although the studies cover different climate zones there is no discernible difference in the choice of the influencing factors based on the site location however the influencing factors may have had different feature importances in the models of the different regions which we have not examined 3 description of the study area the region of bavaria is located in southern germany and has an area of 70 542 km2 fig 1 bavaria can be divided into four major landscapes the alps the alpine foothills the eastern and south western low mountain range fig 2 the alps in southern bavaria are the smallest major landscape regarding area the bavarian alps are characterized by high mountains with heights between 1 500 and almost 3 000 m the alpine foothills elevation range 300 800 m describe the region north of the alps and south of the danube which is characterized by glacial deposits south of the danube there is fertile hilly country to which an area with numerous lakes connects further in the south the eastern low mountain range elevation range 300 1 500 m is located in the northeast of bavaria and the south western low mountain range elevation range 300 700 m is found in north west bavaria the low mountain ranges are mountainous areas with rounded wooded ridges and elongated valleys bavaria is characterized by a warm moderate climate as it is located in the transition zone between the maritime climate of western europe and the continental climate of eastern europe bavaria s climate is influenced by the different altitudes and the structure of the low mountain ranges the alpine foothills and the bavarian alps stmuv 2015 the altitudes range from 100 m in the northwestern part of bavaria to nearly 3 000 m in the alps differences between the seasons are clear but not extreme in bavaria in the period 1971 2000 the annual average temperature in bavaria was 7 8 c the average summer temperature in bavaria was 16 2 c the average winter temperature 0 5 c in the reference period 1971 2000 july was the warmest month and january the coldest due to the altitude dependence however the annual average temperatures vary greatly over bavaria stmuv 2015 the average annual precipitation amount in bavaria is 945 mm but regional differences are large with an average of 600 to 700 mm per year it rains the least in central and northwest bavaria the highest average annual precipitation amount with 1 800 mm is recorded in the alpine region with 111 mm on average july is the month with the most precipitation whereas february is the month with the fewest precipitation 56 mm in bavaria stmuv 2015 overall the warmer dry northwest contrasts the precipitation rich regions of the south and east of bavaria the pluvial and flash flood season in germany is summer with july being the month with the most events the region of bavaria also experiences heavy rain induced flood events in spring with 59 recorded events until 2017 munich was the second most affected city after berlin 65 kaiser et al 2021 bavaria is a water rich region with about 100 000 km of flowing waters large waters of national importance account for 4 200 km another 4 800 km are waters of regional importance lfu 2017c about 91 000 km are small waters that considerably influence the formation of flash floods a main european water divide runs through bavaria separating the danube rhine and elbe river basins fig 1 the danube is bavaria s largest river which is characterized by its water rich tributaries originating from the alps and the bavarian forest in the eastern low mountain range stmuv 2012 the danube drains most of bavaria from the south while the main drains the north west of bavaria agriculture and forestry are an important economic sector in bavaria more than half of the region 53 4 is used for agriculture 65 of which is arable land and 35 is grassland bbv 2020 forests and near natural areas account for 37 9 and are mainly found in the alpine region and the low mountain ranges in east and south western bavaria wetland and water bodies sum up to 1 2 of the area the built up area makes up 7 5 of the region in 2015 the degree of soil sealing in bavarian municipalities was between 27 and 75 in relation to bavaria s total settlement and traffic area the proportion of sealed surfaces amounts to 50 9 üreyen and thiel 2017 based on their runoff potential the bavarian soils can be divided into the four hydrological soil groups a b c and d the hydrological soil group a which corresponds to sand loamy sand or sandy loam types of soils occurs on 13 of the bavarian area the hydrological soil groups b silt loam or loam and c sandy clay loam have an area share of 14 and 49 respectively the hydrological soil group d which are soils of clay loam silty clay loam sandy clay silty clay or clay covers 24 of bavaria the soils of group d are characterized by very low infiltration rates and thus favor the generation of surface runoff in addition because soils of group c and d are characterized by high water retention capacity they can limit subsurface flow these soils with high runoff potential are mainly found in the alpine region and the south western low mountain ranges overall the soils with low and very low infiltration rates c and d account for 73 of the total area to sum up the study area has a large extent and includes a variety of different natural landscapes bavaria combines a great diversity regarding land use soil vegetation morphometry and climatic conditions therefore low mountain ranges and cultural landscapes stand for bavaria as much as large forests a multitude of rivers and lakes and the alpine region however the large natural heterogeneity of bavaria poses a challenge for the ml model which is typical for modeling vast areas and can therefore be considered exemplary to still achieve high model performance the selected explanatory factors must comprehensively describe the different characteristics of the major landscapes in the study area in addition a sufficient amount of training data in the different major landscapes is necessary to learn the relationships in a sound way 4 data and methods 4 1 methodical approach we recommend performing the steps shown in fig 3 to generate a pluvial and flash flood susceptibility map for a very large area the overall workflow consists of the following steps i feature selection based on literature review multicollinearity analysis and feature importance ii feature preparation including handling of missing data standardization and weight of evidence encoding iii construction of the training and test datasets iv model training and validation using performance statistics v final model selection and vi generation of the pluvial and flash flood susceptibility map in the following sections we describe each step in detail using bavaria as an example 4 2 flood inventory training and test dataset the derivation of the flash flood susceptibility for a given area using machine learning models requires the knowledge of formerly affected and unaffected locations based on the affected and unaffected locations the model learns which area characteristics and conditions favor the occurrence of flash floods in this study we used the pluvial and flash flood dataset generated within the framework of the hios project kaiser et al 2020 created a dataset of past german pluvial and flash flood events triggered by heavy rain using a variety of sources ranging from agencies over mission archives and insurance companies to media reports for a flash flood event to be documented the triggering heavy rain event must have lasted between 30 min and 24 h and must have exceeded a given minimum precipitation amount according to essl 2014 1 p 2 5 d where p is the precipitation amount in mm and d is the duration in min in addition the event must have occurred in a catchment smaller than 500 km2 and in the period between april and october if the heavy precipitation amount is unknown kaiser et al 2020 apply a short flooding rise time and or little to no warning time and a small watershed size as proxy indicators a pluvial flood event must meet the same thresholds to be documented however in the absence of a rainfall measurement the heavy rain event must have triggered flooding causing extreme impacts e g flooded cellars interruption of public traffic closed roads etc within the flood inventory no distinction is made between pluvial and flash floods because both flood types can occur simultaneously and the available event information does not allow a clear differentiation kaiser et al 2021 in the flood inventory an event can be uniquely identified by the affected city and the occurrence date kaiser et al 2021 the term city in this context refers to any size of settlement from village to city that can still be uniquely identified there are more than 10 000 uniquely identifiable villages towns and cities in bavaria due to the events spatial reference to settlements the flood inventory focuses on pluvial and flash flood events that occurred in populated areas because damaging flood events are more likely to be reported than non damaging events a bias toward damaging events in the flood inventory is likely according to the flood inventory by kaiser et al 2020 932 bavarian settlements and cities were affected by one or more pluvial and flash flood events by 2017 since the location of the flooding within the cities was generally unknown the center of the city was used as a uniform approximation the affected and unaffected cities are represented in the classification model by their city center point location which was converted to a raster pixel with a resolution of 25 m to prevent introducing bias into the model we chose the same number of unaffected cities as affected cities we randomly selected 932 cities from the remaining bavarian cities assuming that we know all affected cities and that the remaining cities have therefore not been affected to ensure the representativeness of the major landscapes we selected as many unaffected cities in each major landscape as affected cities were known subsequently we split the generated pluvial and flash flood dataset of 1 864 samples into a training and test dataset for the training dataset we randomly selected 80 of the dataset the remaining 20 of the dataset was used as a test dataset fig 2 since this is a binary classification problem the values 1 and 0 were assigned to the flood and non flood samples respectively at each sample point we also extracted the values of the influencing factors which were initially computed in a raster format with a 25 m resolution we calculated the catchment related factors for 12 964 catchments whose average size was 5 km2 and their median size was 2 km2 since flash floods mainly occur in small catchments we chose a catchment scale that can finely represent the catchment characteristics of bavaria in the case of catchment related influencing factors each sample point is assigned the value of the catchment in which it is located 4 3 feature selection based on the literature study and data availability we preselected possible influencing factors table 2 we investigated 42 influencing factors with respect to their suitability to explain the occurrence of heavy rain induced floods in bavaria the 42 selected influencing factors describe topography 11 soil and geology 6 land cover 3 river network 6 catchment properties 11 precipitation characteristics 2 anthropogenic factors 2 and spatial location 1 among the preselected features some are similar in that they describe the same area property but use different datasets or are derived differently this applies for example to the preselected features form factor and elongation ratio both of which describe the shape of a catchment we used multicollinearity analysis and the classifiers feature importance to reduce the 42 explanatory factors to the most influential ones an iterative trial and error approach was applied to determine the best combination and selection of influencing factors the lower limit for the importance of a feature was arbitrarily set at 2 and features contributing less were removed from the model we applied the indicators variance inflation factor vif and tolerance tol to evaluate collinearity among the predictors in the literature vif 10 and tol 0 1 are often considered as thresholds for the presence of multicollinearity dormann et al 2013 features that were very similar were characterized by high collinearity such as the features sediment transport index and stream power index topographic wetness index and slope elongation ratio and form factor of these collinear features the one that had the higher feature importance was selected using multicollinearity analysis and feature importance we gradually reduced the number of influencing factors to 17 while reducing the input features iteratively we took care to retain explanatory variables from all domains that can explain the occurrence of pluvial and flash floods thus we retained features describing topography land use soil and geology river network heavy precipitation anthropogenic factors and catchment properties overall we selected the following influencing factors elevation slope landforms land use sealing degree distance to river height above the nearest drainage the permeability of the upper aquifer saturated hydraulic conductivity the product of x and y coordinates representing the spatial location heavy rain hours and the catchment related variables relief melton number elongation ratio length area relation built up area share and agricultural area share table 3 lists the sources resolution and original format of the datasets used to derive the influencing factors for the study we converted the influencing factors into raster of 25 m resolution although tree based models are insensitive to collinear predictors we reduced the number of influencing factors as much as possible we aimed at reducing the model complexity to make them easier to understand to ensure the acceptance of the flash flood susceptibility map it is important to use models that are as simple and comprehensible as possible table 4 shows that all 17 selected influencing factors except the product of the x and y coordinate meet the vif and tol thresholds defined by dormann et al 2013 however we do not consider this to be a problem since on the one hand tree based classifiers are tolerant of multicollinearity and on the other hand the high vif was caused by the inclusion of products of two variables furthermore the predictor x y coordinate is a combined predictor that implicitly considers two predictors simultaneously and thus adding value to the model 4 4 description of the chosen influencing factors topography controls the hydrological processes of an area to a large extent researchers consider the slope to be the most important factor influencing the occurrence of flash floods tehrany et al 2013 vaezi et al 2017 diakakis et al 2016 the higher the slope the less the soil infiltration and the higher the surface runoff while high slopes promote runoff concentration flat areas favor flooding in addition to slope elevation is a good explanatory factor for flash flooding since water follows the gradient the lower lying areas of a catchment tend to be more often affected by flooding than the higher lying areas tehrany et al 2015 chapi et al 2017 for our study we derived the slope and elevation using the 25 m digital elevation model dem using arcgis pro fig 4 to assess inundation areas evaluating the overall topographic setting is crucial since the surface shape influences the flow accumulation macmillan and shary 2009 the division into landforms offers the possibility of a quantitative classification into different landscapes landform types can be determined by the characteristic terrain pattern which manifests through variation of the geomorphic features in shape size and scale macmillan and shary 2009 we applied the approach by weiss 2001 in which landforms are differentiated based on discrete slope position classes using the standard deviation of the topographic position index tpi to determine the tpi the elevation of each cell of a dem is compared to the mean elevations in the predefined vicinity of the cell weiss 2001 weiss 2001 distinguishes 10 landform types canyons midslope drainages upland drainages u shape valleys plains open slopes upper slopes local ridges midslope ridges and mountain tops we derived the landforms using the arcgis topography toolbox by dilts 2015 fig 5 land use influences the formation of runoff depending on the type of land use the nature of the topsoil changes and with it the infiltration capacity and surface roughness e g chandler et al 2018 sofia et al 2019 studies have shown that forests reduce the runoff volume and lower the peak discharge e g hundecha and bárdossy 2004 hümann et al 2011 while urbanization leads to an increase in peak flows e g miller and hess 2017 pumo et al 2017 niehoff et al 2002 have further proven that the impact of land use on the runoff generation depends on the precipitation characteristics accordingly the influence of land use on storm runoff generation is greater for convective storm events than for advective ones niehoff et al 2002 bronstert et al 2002 for the influencing factor land use the corine land cover dataset served as the data basis which was aggregated to the following classes built up area agricultural area forests and near natural areas wetlands water surfaces fig 4 as a supplement to land use we also considered the sealing degree in our models because the higher the degree of sealing the less water can infiltrate and the more surface runoff forms diakakis et al 2016 found that the sealing degree and the slope were the most important factors in explaining flash flooding in athens greece for our models we used the dataset of imperviousness density from copernicus 2018 which gives the percentage of soil sealing for each pixel fig 4 the porosity and permeability of the soils and rocks are decisive for the infiltration performance since the soil texture influences the hydraulic conductivity to account for the influence of the soil we incorporated the saturated hydraulic conductivity of the topsoil in the models in bavaria the saturated hydraulic conductivity varies between 0 and 156 cm d and averages 20 cm d fig 4 in this study we used the soil information contained in the soil map übk25 1 25 000 by lfu 2017b to derive the saturated hydraulic conductivity of the topsoil the übk25 uses among other things data from a geological map 1 25 000 forestry site maps and soil estimates each of the 614 map sheets of the übk25 was checked and supplemented by 300 to 800 boreholes and soil profiles in the field lfu 2017b for each soil profile detailed information about the soil layers and their properties is available in the übk25 profile database since the übk25 database does not contain information on saturated hydraulic conductivity we derived it as a function of soil texture and dry bulk density class each class combination of soil texture and dry bulk density was assigned a saturated hydraulic conductivity in cm d according to eckelmann 2005 we further averaged the saturated hydraulic conductivities according to their area fractions on each reference profile however due to the extrapolation of the soil profiles to the area the saturated hydraulic conductivity is subject to uncertainties bouvier et al 2021 have shown for a mediterranean catchment that subsurface flows can be predominant during flash floods in this context bedrock fractures and fracture orientation can influence the occurrence and direction of subsurface flow in catchments scaini et al 2018 bouvier et al 2021 therefore to reflect the runoff contribution of subsurface flows during flash floods we considered the permeability of the upper aquifer in our models the hydrogeological map hük 200 1 200 000 by bgr and sgd 2016 served as the data basis the hydrogeological map divides the upper aquifers of bavaria into 10 permeability classes ranging from high 10 2 10 3 m s to extremely low 10 9 m s fig 4 the hük 200 is derived from a geological map 1 200 000 and contains hydrogeological information on the upper large scale contiguous aquifer the permeability of the upper aquifer is based primarily on the evaluation of hydraulic tests performed on thousands of boreholes wells and gauges in areas where these hydraulic tests were not available hydrogeological units were assigned permeabilities by analogy bgr and sgd 2016 data uncertainties are particularly present in areas where complex geological hydrogeological conditions occur for example when the bedding conditions vary over small areas this was taken into account by assigning overarching permeability classes see table 5 the distance to a river is a good proxy of the probability of being affected by a flash flood and is thus a frequently chosen explanatory factor janizadeh et al 2019 hosseini et al 2020 khosravi et al 2018 because regions near streams are more likely affected by flooding than regions far away from any river however regions remote from streams can also be flooded by concentrated surface runoff we calculated the distance to the nearest stream using the euclidean distance tool of arcgis pro and the official river network of bavaria fig 5 in addition to the distance to a river we considered the metric height above the nearest drainage hand the hand value returns the hydrologic height by calculating the height of each catchment cell above the nearest river into which it flows nobre et al 2011 in summary the hand value normalizes the terrain heights with regard to the river network the hand value is often used to map susceptibility to flooding and gully erosion arabameri et al 2020 carvalho et al 2020 olorunfemi et al 2020 garousi nejad et al 2019 all in all the hand value is a valuable predictor for susceptibility to flash flooding that supplements the feature distance to a river we derived the hand metric using the arcgis toolbox by dilts 2015 the dem and the official river network of bavaria fig 5 heavy rain is the trigger of a flash flood and is therefore an important influencing factor hapuarachchi et al 2011 generally short and high intensity rains are associated with the occurrence of flash floods gaume et al 2009 borga et al 2011 lengfeld et al 2019 have shown based on radar measurements of 16 years that hourly precipitation events in germany are significantly less influenced by the orography than daily precipitation events to incorporate the rainfall in our models we used the dataset heavy rain hours from gdv and dwd 2018 fig 5 this dataset sums up all hours of heavy rain that occurred in germany since 2001 per zip code area to qualify as heavy rain the rain event had to exceed 25 l m2 in 1 h or 35 l m2 in 6 h gdv and dwd 2018 since there is a relationship between the catchment geomorphology and its hydrologic response costa 1987 harlin 1984 patnaik et al 2015 it is crucial to consider catchment properties as influencing factors we thus incorporated the catchment characteristics regarding shape topography and land use in our models it is known that the catchment shape affects the time of runoff concentration costache 2019b a rounded catchment will have a shorter concentration time than an elongated catchment schumm 1956 due to the shorter concentration time and the simultaneous drainage to the catchment center round catchments represent the greater risk for high peak discharges for each catchment we determined the elongation ratio which is a dimensionless parameter that quantifies the basin shape fig 6 according to schumm 1956 the elongation ratio e of a catchment is defined as follows 2 e 2 a π l b where a is the catchment area in km2 and l b is the basin length in km the elongation ratio of a catchment can be classified as follows more elongated 0 5 elongated 0 5 0 7 less elongated 0 7 0 8 oval 0 8 0 9 circular 0 9 schumm 1956 for further description of the catchment properties we delineated the length area relation using hack s law which is an empirical scaling law for river networks sassolas serrayet et al 2018 fig 6 based on fractal mathematics hack 1957 described the relationship between catchment size and mainstream length as a power function assuming that the mainstream length scales with the catchment size hack s law has been intensively studied and applied since its publication e g maritan et al 1996 reis 2006 sassolas serrayet et al 2018 hack s law is defined as follows 3 l 1 4 a 0 6 where a is the catchment area in km2 and l is the mainstream length in km using the given catchment size and hack s law we calculated the mainstream length in addition to the shape the topography of the catchment also plays a role in the runoff formation the basin relief allows conclusions about the geomorphological and hydrological characteristics of a catchment we used the relief to describe the terrain of the catchments fig 6 the catchment relief r is computed as the difference of the maximum catchment height h max and the minimum catchment height h min schumm 1956 4 r h max h min as another topography indicator we applied the melton ruggedness number fig 6 the melton ruggedness number is a slope index describing the relief conditions of a catchment in geomorphology the melton ruggedness number is mostly used to assess sediment transport processes within basins e g de scally and owens 2004 marchi and dalla fontana 2005 to quantify the ruggedness of a basin melton 1965 proposed the following dimensionless ratio 5 m r a where a is the catchment area in km2 and r is the basin relief in km the melton number can vary from zero to some large number although it rarely exceeds one melton numbers between 2 and 3 indicate a very rugged area for a first order catchment melton 1965 to further describe the catchment response to heavy rain we considered the land use distribution within the catchments to do so we included the percentage of built up area and agricultural area of a catchment derived from the corine land cover dataset fig 6 the spatial relationship between the sample points plays an important role because as we know from tobler s first law of geography everything is related to everything else but near things are more related than distant things tobler 1970 to account for the spatial dimension in the model we included the x and y coordinate as explanatory features however to reduce the number of features and minimize collinearity as well as to increase the informative value of the predictor we used the product of the x and y coordinates 4 5 data uncertainties and assumptions the hios dataset consists mainly of quality controlled heavy rain induced flood events two thirds of the events are fully verified meaning that an expert has not only confirmed the event but also investigated the nature and impact of the flood event more than a quarter of the events were verified by a trusted source such as the german weather service only 6 of the events have a low quality level as these events are considered plausible only due to the general meteorological situation in the affected region and period kaiser et al 2020 the data basis for hydrologic or hydrodynamic modeling and ml based modeling differs in some aspects to reconstruct a flash flood event for example by means of hydrological modeling highly accurate spatiotemporal information on precipitation and runoff is required this event information is often collected in post event surveys in the form of radar images station measurements discharge estimations videos photos and witness statements this information is then used to simulate the event runoff using a hydrological or hydrodynamic model based on the event precipitation since ml modeling is performed event independent precise knowledge of the spatiotemporal distribution of the event precipitation or runoff is not required for modeling nonetheless an event must meet the criteria explained in section 4 2 to be considered in the flood inventory including exceeding a specified precipitation amount during a specified time therefore although it is important for an ml driven approach to know the exact location of the event and the area characteristics the spatial and temporal resolution of the event information can be lower than required for a hydrologic hydrodynamic simulation the ml based derivation of the pluvial and flash flood susceptibility is based solely on static and event independent influencing factors thus the machine learning approach presented here differs fundamentally from hydrological or hydrodynamic modeling which is performed applying an event or scenario specific precipitation input in contrast the susceptibility derived using machine learning methods results solely from the properties of an area consequently it is possible that areas classified as having low susceptibility to heavy rain induced flooding could experience a flash flood under unfavorable conditions such as high antecedent soil moisture and extreme rainfall amounts furthermore we ensured that the spatial and temporal resolution of the underlying datasets of the influencing factors was as uniform as possible most of the datasets used are at a resolution of 1 25 000 and are from 2014 to 2020 table 3 since most of the 17 influencing factors are static e g slope elevation landforms we estimate the influence of temporal changes in the datasets on the modeling result to be minor only the influencing factors land use as well as the land use distribution in the catchments and the sealing degree are subject to temporal changes because most flood events cover a similar period as the datasets used namely 2006 to 2017 kaiser et al 2021 we assume that inaccuracies in the datasets due to temporal changes are small 4 6 feature preparation we prepared the influencing factors with the gis software arcgis pro 2 6 for further feature processing we used the python library scikit learn pedregosa et al 2011 to avoid losing valuable samples we replaced missing values in the dataset for categorical features we used the function simpleimputer to replace missing values with the most frequent feature value missing numerical values were replaced using the mean value from the five nearest neighbors in the dataset that have a value for the feature knnimputer we transformed the three categorical features i e landforms land use permeability of the upper aquifer into numbers as required by machine learning algorithms using weight of evidence woe encoding table 4 the woe method is a bivariate bayesian statistical approach that has been widely used in flood susceptibility studies e g tehrany et al 2014 hong et al 2018 costache 2019a 2019b using the woe method we can measure the predictive power of an independent variable an influencing factor regarding the dependent variable pluvial and flash flood occurrence in the case of flood modeling the woe coefficient is computed based on the relationship between the non occurrence and occurrence of pluvial and flash floods within each class of the categorical feature the woe coefficient for each categorical class is calculated as follows 6 woe ln b a b a where b indicates the number of flood pixels in the given class and a the total number of flood pixels of all classes accordingly the number of non flood pixels within the investigated class is given by b and the number of non flood pixels of all classes is represented by a we scaled all features using standardization standardscaler which is less affected by outliers than min max scaling in some flood susceptibility studies the numerical features are discretized using the natural breaks or quantile method e g costache 2019a 2019b tien bui et al 2019 tang et al 2020 however we did not group the continuous features as tree based classifiers are good at determining complex relationships between the independent and dependent variables 4 7 classifiers applied in this study we compare the ensemble methods random forest gradient boosting decision tree and catboost before choosing these three classifiers we tried several different machine learning algorithms e g naïve bayes support vector machines neural networks adaboost decision tree without optimizing their hyperparameters based on the first classification results we selected the three most promising of all classifiers in the following subsections we briefly describe the three classifiers used in this study a comprehensive description of the underlying principles can be found in the literature e g kuhn and johnson 2016 bentéjac et al 2020 bonaccorso 2020 4 7 1 random forest in 2001 leo breiman first introduced the random forest rf model breiman 2001 a random forest model consists of a large number of decision trees that are as uncorrelated as possible to generate an uncorrelated forest of trees the random forest uses bagging and feature randomness in the formation of the decision trees this means that each tree receives a random sample that is drawn from the training set with replacement bootstrapping each tree also only receives a random subset of the features due to this implemented randomness the prediction of the random forest model is often more accurate than the predictions of any individual decision tree breiman 2001 the random forest classifier is one of the most frequently used classification algorithms this is probably because random forest models are powerful ensemble methods that are not sensitive to multicollinearity and can handle missing and unbalanced data random forests have been applied in many studies of flood susceptibility mapping and achieved high predictive accuracies chen et al 2020 costache et al 2020a hosseini et al 2020 hong et al 2018 tang et al 2020 in this study we used the randomforestclassifier from the machine learning library scikit learn pedregosa et al 2011 for the python programming language unlike the original publication by breiman 2001 the scikit learn implementation averages the probabilistic prediction of the decision trees soft voting rather than using the majority decision of the classifier votes hard voting pedregosa et al 2011 compared to hard voting soft voting often achieves higher performance because it accounts for the uncertainty of each classifier in the final decision géron 2017 4 7 2 gradient boosting decision tree boosting is a method to create ensemble models under boosting we understand the training of several weak learners in a sequential and adaptive manner to obtain a stronger learner kuhn and johnson 2016 at first one initial model is fitted to the data then a second model is trained concentrating on improving the shortcomings of the previous model and so on the underlying assumption is that the combination of the models is better than one model alone since each subsequent model tries to improve the shortcomings of the combined ensemble model many boosting algorithms exist however adaboost and gradient boosting are among the most popular géron 2017 the gradient boosting algorithm was first presented by breiman 1997 and further developed by friedman 2001 gradient boosting fits the new learner to the residual errors of its predecessor the predecessor s shortcomings are identified by the gradient e g residual of an exponential loss function after the current model is added to the previous model a new model is fit to the residuals to minimize the loss function and so on kuhn and johnson 2016 in our study we applied the class gradientboostingclassifier of scikit learn pedregosa et al 2011 that uses decision trees as base estimators by default since each decision tree of the ensemble model uses a different feature subset to select the node s best split the individual decision trees differ from each other and thus can capture different signals from the data the gradient boosting algorithm has not yet been used in a flood susceptibility study however bui et al 2019b already applied the boosting algorithms adaboost and logitboost for flash flood susceptibility modeling 4 7 3 catboost the third classifier applied in this study also uses gradient boosting on decision trees and is called catboost for categorical boosting prokhorenkova et al 2018 as with the gradientboostingclassifier the decision trees of the catboost model differ from each other because they use different feature subsets the catboost classifier is available as an open source library for python provided by the technology and internet services company yandex the catboost algorithm differs from classical gradient boosting in two ways first catboost applies ordered boosting which is a permutation driven alternative to the standard gradient boosting algorithm prokhorenkova et al 2018 at each training step catboost uses the independent permuted historical samples and thus achieves unbiased boosting second catboost uses ordered target statistics to encode categorical features hancock and khoshgoftaar 2020 both techniques were implemented to combat target leakage a specific type of overfitting that occurs in all classical gradient boosting algorithms dorogush et al 2018 prokhorenkova et al 2018 due to the two algorithmic advances catboost has advantages over other implementations of gradient boosted decision trees dorogush et al 2018 proved that catboost outperforms other gradient boosting algorithms such as xgboost lightgbm or h2o on popular datasets besides catboost handles heterogeneous datasets with categorical features well and is easy to use hancock and khoshgoftaar 2020 in geosciences the catboost algorithm has recently been used to classify formation lithology dev and eden 2019 and to estimate reference evapotranspiration huang et al 2019 zhang et al 2020 kang et al 2020 also applied catboost to calculate an hourly wildfire risk index in a comparative study of four gradient boosting algorithms for landslide susceptibility mapping catboost achieved the highest predictive ability sahin 2020 recently hancock and khoshgoftaar 2020 reviewed studies from various disciplines that employed the catboost algorithm to analyze its effectiveness and shortcomings 4 8 model tuning to avoid overfitting we used 5 fold cross validation to tune the hyperparameters of the three ensemble models for this purpose the training set is divided into five distinct subsets folds one subset is used as the validation set and the remaining four subsets are used as the training set then the model is fitted on the training set and validated against the validation set this procedure is repeated for each of the five subsets using 5 fold cross validation the best combination of hyperparameter values for the three models was determined and applied to the training set depending on the model type different regularization parameters were optimized the models were constrained by limiting the size of their decision trees by the maximum depth of the tree max depth the number of features to consider when looking for the best split max features the minimum number of samples required to be at a leaf node min samples leaf the minimum number of samples required to split an internal node min samples split in addition the number of decision trees was limited n estimators for the gradient boosted algorithms the contribution of each tree was regulated by the learning rate the values of the regularization parameters are shown in table 6 the performance evaluation of the three classifiers was performed on the unseen test set 4 9 performance measures we assessed the predictive ability of the selected classifiers using various performance metrics the receiver operating characteristic roc curve is used to assess binary classifiers altman and bland 1994 brown and davis 2006 fawcett 2006 the roc curve plots the false positive rate fpr on the x axis against the true positive rate tpr on the y axis the true positive rate also called recall or sensitivity is the ratio of positive samples correctly identified by the classifier 7 tpr tp tp f n where tp is the number of true positive samples and fn is the number of false negative samples conversely the false positive rate describes the ratio of negative samples that are incorrectly identified as positive 8 fpr fp fp t n 1 t n r where tn is the number of true negative samples and fp is the number of false positive samples the fpr is equal to one minus the true negative rate tnr which is also called specificity the tnr is the proportion of negative samples correctly classified as negative 9 tnr tn tn f p the positive predictive rate ppr also called precision returns the accuracy of the classifiers positive predictions and is defined as follows 10 ppr tp tp f p conversely the negative predictive rate npr describes the accuracy of the classifiers negative predictions 11 npr tn tn f n the roc curve displays the classifier s ability to discriminate between a flood and a non flood event the diagonal of the roc curve represents a random classifier the aim is to obtain a classifier that is as far away from the diagonal as possible however there is a trade off as higher true positive rates come along with more false positive predictions for visual comparison the classifiers roc curves are superimposed graphically to compare the classifiers roc curves quantitatively we used the area under the roc curve auc see fig 7 the auc value measures the two dimensional area underneath the roc curve and thus aggregates the classifier s performance for all possible candidate thresholds possible auc values range from 0 to 1 the higher the auc value the better the predictive power of the classifier while an auc value of 1 represents the perfect classifier when comparing the models the most effective model is the one with the largest area under the roc curve we further evaluated the classifiers performances using the metrics accuracy and cohen s kappa also called kappa statistic the accuracy measures the correctly identified samples and is defined as follows 12 accuracy tp t n tp f p t n f n however the disadvantage of the accuracy metric is that it does not differentiate between the types of error being made cohen s kappa in contrast considers the class distributions of the training set kuhn and johnson 2016 cohen s kappa measures the agreement between two raters on a classification problem taking into account the accuracy that would result from chance alone cohen 1960 the kappa statistic can be computed as 13 κ o e 1 e where o is the observed accuracy and e is the expected accuracy by chance according to chicco et al 2021 the observed accuracy o and the expected accuracy e can be computed for binary classification problems as follows 14 o tp t n tp f p t n f n 15 e tp f p n tp f n n tn f p n tn f n n where n is the number of samples in the dataset and the expected accuracy is the value of accuracy under statistical independence of the observers chicco et al 2021 cohen s kappa can take values between 1 and 1 while 1 means perfect agreement and 0 means no agreement between the observations and predictions if the kappa statistic is less than 0 than the classifier is worse than agreement by chance landis and koch 1977 proposed the following classification of the kappa statistic 0 no agreement 0 0 20 slight 0 21 0 40 fair 0 41 0 60 moderate 0 61 0 80 substantial and 0 81 1 almost perfect agreement 4 10 model specific and model agnostic interpretation methods in this paper we applied model specific and model agnostic methods to interpret the trained classification model as model specific methods we used feature importance and pairwise feature importance also called feature interaction the feature importance describes the strength of the relationship between the predictor and the outcome in most tree based models feature importance is an intrinsic measure that monitors the performance as each feature is added to the model kuhn and johnson 2016 the feature importance ranges between 0 and 1 and the sum of the importance of all features is 1 in addition to feature importance we computed feature interaction which is a two way interaction measure indicating the interaction strength for each pair of features the feature interaction is a dimensionless statistic based on friedman s h statistic and describes the proportion of the variance explained by the interaction friedman and popescu 2008 as a model agnostic method we applied shapley additive explanations shap by lundberg and lee 2017 shap is a unified framework for interpreting individual predictions by assigning an importance value to each feature lundberg and lee 2017 shap computes shapley values shapley 1953 from coalitional game theory which describe how much each player feature contributed to the game prediction molnar 2019 the shapley value represents the average marginal contribution of a feature value to a prediction instance in all possible coalitions for each possible coalition the prediction instance is computed with and without the feature value with the difference representing the marginal contribution for a detailed description of the calculation and theory of shapley values and shap please see shapley 1953 lundberg and lee 2017 sundararajan and najmi 2020 and molnar 2019 to understand how much the classification model relies on each influencing factor for making predictions we used the training data to calculate feature importance feature interaction and shap 5 results 5 1 model validation and comparison we validated the three ensemble models using the 373 flood and non flood locations of the test set we did not apply spatial tolerance to the agreement between the observations and the classification results because the application of spatial tolerance would not affect the outcome this is because in the immediate vicinity of the sample points the values of most influencing factors change little or not at all for example the six catchment related factors and the zip code based heavy rain hours do not change in addition the soil geology and landform describing factors tend to be related to larger areas since the elevation usually changes gradually the terrain dependent influencing factors hand and slope also change rather gradually around the sample points all three models performed equally well on the test set and achieved similar performance statistics table 7 the catboost model correctly detected the most flood and non flood locations of the three models 281 the gradient boosting decision tree model achieved the highest sensitivity with 76 9 followed by the catboost and random forest model with 75 3 each regarding predictive accuracy the catboost model performed best 75 3 followed by the random forest model 75 1 and the gradient boosting decision tree model 74 8 according to the kappa classification by landis and koch 1977 all three models achieved a moderate agreement with the catboost model having the highest kappa value 0 51 we evaluated the global performance of the ensemble methods using the au roc method fig 7 compares the three classifiers roc curves and indicates their auc values the catboost random forest and gradient boosting decision tree models all achieved high auc values for the test set however the catboost model has the highest predictive power auc 81 9 followed by the random forest auc 81 6 and the gradient boosting decision tree model auc 81 3 overall we find that all three models proved to be powerful in identifying the general pattern of pluvial and flash flood susceptibility for a large area the predictive power of the classifiers is comparably good and thus no classifier significantly outperforms the others regarding accuracy auc and kappa statistic the catboost model is the best performing since the catboost model has the highest predictive power we used it to derive bavaria s pluvial and flash flood susceptibility 5 2 model interpretation based on the feature importance we rank the 17 influencing factors fig 8 by far the highest predictive power is attributed to the sealing degree 16 8 and the built up area share of a catchment 11 7 the importance of the 15 remaining influencing factors ranges between 7 and 2 the melton number of a catchment 7 2 and the hours of heavy rain 7 1 are equally important followed by x y coordinate 6 4 hand 6 0 and the proportion of the agricultural area of a catchment 5 9 contrary to our expectations the slope is not among the most predictive explanatory factors on the contrary the slope ranges in the lower third with 4 1 according to the feature importance the catchment related influencing factors are as important for model prediction as the spatially distributed ones among the three most influential explanatory factors two are catchment related built up area share melton number however catchment related variables describing land use distribution and relief conditions in the catchment are more important than variables describing the catchment shape the distance to the river is the least contributing influencing factor with 2 4 for model interpretation we also investigated pairwise feature importance of the 136 possible pairs of influencing factors we plotted the top 15 interaction pairs in fig 9 these 15 pairs contain 11 different influencing factors four of which are catchment related the sealing degree and the built up area share which had the highest feature importance occur in 9 and 6 of the 15 interaction pairs respectively interestingly the factor agricultural area share appears in the second and third strongest pair with the sealing degree and the proportion of the built up area although its feature importance was not so high however the agricultural area share probably complements the sealing degree and the proportion of the built up area although the slope s single feature importance was only ranked 12th the slope forms strong pairs with the built up area share the sealing degree and the heavy rain hours similarly the saturated hydraulic conductivity which was only ranked 10th regarding feature importance forms a strong pair with the sealing degree ranked 5th to investigate the relationship between the influencing factors and the prediction we used a shap summary plot the summary plot in fig 10 orders the features on the y axis according to their importance each point represents a shap value for a feature value and a prediction instance the continuous color scale indicates low to high feature values in the case of overlapping points the points were jittered in the direction of the y axis sealing degree and built up area share are the only factors that reach absolute shap values above 1 and thus can strongly influence the prediction result in one direction or the other a high sealing degree significantly increases the risk of flooding while medium to low sealing degrees reduce flood risk a high percentage of built up area in a catchment generally increases flood risk in addition to the sealing degree and the built up area share six other influencing factors can significantly affect the prediction absolute shap values above 0 5 heavy rain hours x y coordinate melton number agricultural area share elevation and saturated hydraulic conductivity it is plausible that a high number of heavy rain hours increases the risk of flooding the less rugged a catchment is low melton number the lower the risk of being affected by flash flooding interestingly a high percentage of agricultural land in a catchment leads to a decrease in flood risk and conversely a low percentage of agricultural area to an increase this can probably be attributed to the fact that the proportion of agricultural land is related to the proportion of built up land and thus a low proportion of agricultural land could mean a high proportion of built up area the summary plot also reveals that low lying areas are more susceptible and high values of saturated hydraulic conductivity reduce flood hazard overall the classification model seems to confirm relationships between influencing factors and flash flood occurrence that are known or suspected among hydrologists according to the summary plot elongated catchments with little ruggedness low relief and a low proportion of built up areas should be less susceptible to flash flooding in addition highly permeable topsoils and upper aquifers can reduce the risk of flooding although we can now better understand the relationship between the influencing factors and the prediction result we should be cautious about deriving general rules from the summary plot since the influencing factors affect each other each prediction instance is the result of all feature values therefore a low proportion of built up area in a catchment does not necessarily reduce flood risk fig 10 as shown in fig 9 the built up area share interacts strongly with the sealing degree the agricultural area share the heavy rain hours the melton number the relief and the slope the same is true for slope for example there is a tendency for high slope values to increase the risk of flooding fig 10 however there are also cases where the flood risk was reduced despite high slope values probably due to the strong interaction of the slope with the built up area share the sealing degree and the heavy rain hours fig 9 5 3 susceptibility map using the catboost model we determined the susceptibility of all raster pixels of bavaria as an indication of susceptibility we used the predicted probability of each raster pixel to be assigned to class 1 affected returned by the catboost model the predicted probability is the result of weighting each tree in the ensemble which in turn calculate a probability of class 1 for each of their leaves the susceptibility values ranged from 0 20 to 0 99 the methods quantile or natural breaks are usually chosen for classifying flash flood susceptibility maps e g costache 2019a khosravi et al 2018 ngo et al 2018 youssef et al 2016 with the natural breaks classification method classes are based on natural groupings within the dataset class boundaries are set in such a way that similar values are summarized and differences between the classes are maximized since our susceptibility values have a skewed distribution the natural breaks classification is more appropriate than the quantile classification using the natural breaks method we divided the prediction dataset into four susceptibility classes labeled low 0 20 moderate 0 20 0 35 high 0 35 0 53 and very high 0 53 0 99 the susceptibility classes low and moderate cover 37 and 33 of the bavarian region respectively according to our model 21 of bavaria is considered highly susceptible the highest susceptibility class makes up 8 of the region overall 30 of the region of bavaria is at high or very high risk of pluvial and flash flooding fig 11 regions with high and very high risk occur throughout bavaria the most flood prone areas are identified in the alpine region in the south of the alpine foothills and in the border region of southeast bavaria furthermore we find highly susceptible areas in northern bavaria especially along the main river and toward the region of hesse in northwestern bavaria less susceptible areas are identified in central bavaria 5 4 endangered cities to compare the hazard situation among cities we propose an overall susceptibility score for cities that is based on the susceptibility map generated the susceptibility score of the cities may help decision makers on regional level to prioritize cities for detailed investigations we calculated the overall susceptibility of a city using the smallest circumscribing rectangles so called bounding boxes of the cities the dataset by bkg 2015 provides the bounding boxes for all german cities which we used as an approximation of the city area within each bounding box we determined the area weighted average of the four susceptibility classes which was used as the city s overall susceptibility when calculating the overall susceptibility of a city we assume that the proportions of the susceptibility classes within the bounding box approximate the city s hazard situation consequently a city with a large number of high to very high classified raster pixels is more at risk than a city with a large number of low to moderate classified pixels we determined the overall susceptibility of all bavarian cities fig 12 similar to the pluvial and flash flood susceptibility map we classified the city scores into four susceptibility classes low moderate high very high using the natural breaks method according to this classification 28 and 32 of the cities are considered to be at low and moderate risk respectively classified as highly susceptible are 24 of the cities in the highest susceptibility class are 16 of the bavarian cities the cities at particularly high risk are distributed across bavaria but cluster in specific regions fig 12 especially in the southern part of the natural region eastern low mountain range many cities are classified as very endangered the border region with neighboring austria in southeastern bavaria also has many cities colored red furthermore the southern part of the alpine foothills as well as munich and its surrounding area are endangered in the north of bavaria the nuremberg metropolitan region stands out with the major cities of nuremberg fürth and erlangen in addition würzburg and the region toward the region of hesse along the main river are classified as highly endangered however not all major cities are automatically classified as being at highest risk as the regions around augsburg and ingolstadt show in histograms we compare the distribution of the susceptibility scores of the bavarian cities in the training and test set fig 13 in both the training and the test set the distribution of the susceptibility scores is roughly balanced however when differentiating the cities according to their affectedness it becomes apparent that affected cities tend to be assigned a higher and unaffected cities a lower susceptibility score this tendency is evident in both the training and test set although it is more pronounced in the training set on which the model was trained the histograms suggest that the overall classification of the cities can be considered plausible 6 discussion due to the nature of the flood inventory and its underlying assumptions limitations must be considered when evaluating model performance and interpreting the susceptibility map in terms of spatial accuracy the affected cities can be accurately located however the occurred inundation areas within the cities are unknown because they are not collected by the available sources it is not possible to map the affected inundation areas for hundreds of events at regional level therefore map interpretation must take into account limitations in spatial accuracy there are also limitations to consider regarding the comprehensiveness of the event dataset on the one hand it can be assumed that the flood inventory contains only a part of the events that occurred in reality on the other hand it is not known with certainty which cities were not affected therefore we assumed that no observation means no event however this assumption likely affects model performance since affected cities or cities that are highly susceptible but have not yet had an event are probably also among the negative examples thus statistical measures that evaluate false negative or true positive results e g sensitivity should be used primarily in evaluating model performance regarding the nature of the flood inventory it should be noted that the event dataset focuses on populated areas this is due to the applied event definition which relates the flood event to the city it affected where the term city stands for any settlement size ranging from a village to a city that can still be uniquely identified consequently flash floods that occurred in an uninhabited catchment for example are not included in our flood inventory due to the inventory s urban focus it is possible that related influencing factors such as the sealing degree and the catchment s built up area share gain weight in the classification model to the best of our knowledge only one study exists to date in which a flash flood susceptibility map was derived for an area larger than bavaria 70 500 km2 ma et al 2019 generated a susceptibility map for yunnan province china that covers an area of 380 000 km2 in recent flash flood susceptibility studies cf table 1 investigation areas generally range from 200 to 4 000 km2 and focus on watersheds rather than political entities an exception is the study by hosseini et al 2020 which investigates the flash flood hazard in the gorganroud river basin iran covering an area of 11 300 km2 compared to the other studies listed in table 1 the density of our sample points was lower the reviewed studies used between 7 and 213 sample points per 100 km2 with a median of 42 points per 100 km2 in our study we had only 3 points per 100 km2 available consequently the investigation sites in comparable studies are usually not only smaller but there are also more data points available for training and testing the comparatively low point density of our sample data certainly reduces the performance of our model this is because our model has to learn the complex relationships within four different major landscapes covering low mountain ranges cultural landscapes and the alps with only a few data therefore compared to other flash flood susceptibility studies differences in performance are apparent some ml models achieved auc values above 0 85 and even above 0 95 cf janizadeh et al 2019 bui et al 2019a khosravi et al 2018 tien bui et al 2020 however our performance metrics are in similar ranges as in the study by ma et al 2019 whose winning model achieved a kappa value of 0 59 an auc value of 0 81 and an accuracy of 0 79 nevertheless due to the spatially homogeneous distribution of the sample data and attention to the representativeness of major landscapes our model achieved good performance even at low sample point density an increase in model performance would only be conceivable with more training and test data so that the heterogeneity of the study area could be better learned and reproduced compared to similar flash flood studies we included more influencing factors in our model the studies we examined used between 7 and 12 influencing factors table 1 while we used 17 influencing factors a further reduction in the number of influencing factors led to a performance decrease so that 17 influencing factors were necessary to achieve the best possible model performance we assume that the large heterogeneity in the study area and the small amount of learning data necessitated a higher number of different influencing factors in contrast to comparable studies we included catchment related influencing factors in addition to spatially distributed ones the studies listed in table 1 do not use catchment related factors with exception of the study by costache 2019b costache 2019b used the catchment s circularity ratio among other spatially distributed influencing factors yet there are also studies that derived flash flood susceptibility maps based only on catchment related influencing factors but these use gis techniques and not machine learning e g abdelkareem 2017 abdo 2020 adnan et al 2019 in our model the catchment related influencing factors are crucial for prediction performance according to the feature importance the built up area share and the melton number both catchment related factors are the second and third most contributing influencing factors due to the use of catchment specific influencing factors entire catchments are sometimes assigned to the same susceptibility class this is the case when important catchment specific influencing factors such as the built up area share or the melton number assume critical values furthermore the representation of the map in four susceptibility classes makes minor differences disappear since our susceptibility map is intended to provide initial indications and is not a substitute for a detailed site investigation we decided to retain the catchment related influencing factors in the end we prioritized a higher degree of accuracy over representation 7 conclusion in this study we developed a novel methodology to enable pluvial and flash flood susceptibility assessments for vast territories by using a tree based ensemble method that considers both spatially distributed and catchment related influencing factors the performance of the developed methodology was evaluated based on the region of bavaria germany and three state of the art machine learning models random forest gradient boosting decision tree and catboost we trained the models using 11 spatially distributed and six catchment related influencing factors all three models performed well with auc values above 0 8 comparing the performance measures however the catboost model achieved the best performance and was therefore used to derive the pluvial and flash flood susceptibility map our goal was to develop a methodology to identify pluvial and flash flood susceptible areas at regional scale for this we investigated how to generate a susceptibility map for a vast region which is covering various major landscapes using a machine learning model our findings can be summarized as follows when modeling vast areas it is critical to account for low sample point density by ensuring i a homogeneous spatial coverage of the study area and ii the representation of the major landscapes in the training and test set to capture the natural heterogeneity of vast areas a larger number of spatially distributed and catchment related influencing factors is required than otherwise applied in previous studies to achieve high model performance the selected influencing factors must comprehensively describe the different characteristics of the various major landscapes in the study area there are area characteristics that can reduce susceptibility to flash flooding elongated catchments with low ruggedness and relief a low proportion of built up areas and highly permeable topsoils and upper aquifers are likely to be less susceptible to flash flooding by averaging the susceptibility classes within a city area on an area weighted basis an overall susceptibility classification for a city can be provided this overall susceptibility score allows for comparison between cities e g for prioritization purposes nevertheless the validity of our susceptibility assessment is affected by the small amount of training data that is focused on urban areas and the non consideration of time variable influences in our model we neglected time variant influencing factors such as triggering precipitation antecedent soil moisture or phenology of crops which can severely impact event magnitude in addition the susceptibility map only indicates the actual state since influencing factors such as the sealing degree and the proportion of built up area in the catchment can change over the years with the help of the method presented pluvial and flash flood susceptibility maps can be generated at regional level providing important initial indications for those responsible for spatial planning and flood risk management furthermore in depth investigations can be prioritized and initiated based on the proposed overall susceptibility score for cities in addition we have proven that machine learning models can handle naturally heterogeneous large study areas even with a small amount of training data further work should elaborate whether these new machine learning derived maps are understood and applied by the public and those in charge since distrust of machine learning is still high among the public researchers should also improve on the interpretability of the underlying models and the traceability of the derived flash flood susceptibility maps to increase trust it is necessary to communicate the strengths and weaknesses of the machine learning model and explain the model decisions with examples author contribution this work was conducted by m kaiser under the supervision and guidance of s günnemann and m disse m kaiser prepared the article all authors contributed to the final writing of the paper declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the research presented in this paper has been carried out as part of the hios project hinweiskarte oberflächenabfluss und sturzflut funded by the bavarian state ministry of the environment and consumer protection stmuv and supervised by the bavarian environment agency lfu furthermore this work was supported by the deutsche forschungsgemeinschaft dfg through the tum international graduate school of science and engineering igsse 
