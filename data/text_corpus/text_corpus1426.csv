index,text
7130,rainfall frequency analysis for ungauged regions using remotely sensed precipitation information mohammad faridzad a tiantian yang a b kuolin hsu a soroosh sorooshian a chan xiao c a department of civil and environmental engineering center for hydrometeorology and remote sensing chrs university of california irvine irvine ca usa department of civil and environmental engineering center for hydrometeorology and remote sensing chrs university of california irvine irvine ca usa b deltares usa inc silver spring md usa deltares usa inc silver spring md usa c national climate center china meteorological administration beijing china national climate center china meteorological administration beijing china corresponding author at department of civil and environmental engineering center for hydrometeorology and remote sensing chrs university of california irvine irvine ca usa department of civil and environmental engineering center for hydrometeorology and remote sensing chrs university of california irvine irvine ca usa this manuscript was handled by emmanouil anagnostou editor in chief with the assistance of francesco marra associate editor rainfall frequency analysis which is an important tool in hydrologic engineering has been traditionally performed using information from gauge observations this approach has proven to be a useful tool in planning and design for the regions where sufficient observational data are available however in many parts of the world where ground based observations are sparse and limited in length the effectiveness of statistical methods for such applications is highly limited the sparse gauge networks over those regions especially over remote areas and high elevation regions cannot represent the spatiotemporal variability of extreme rainfall events and hence preclude developing depth duration frequency curves ddf for rainfall frequency analysis in this study the persiann cdr dataset is used to propose a mechanism by which satellite precipitation information could be used for rainfall frequency analysis and development of ddf curves in the proposed framework we first adjust the extreme precipitation time series estimated by persiann cdr using an elevation based correction function then use the adjusted dataset to develop ddf curves as a proof of concept we have implemented our proposed approach in 20 river basins in the united states with different climatic conditions and elevations bias adjustment results indicate that the correction model can significantly reduce the biases in persiann cdr estimates of annual maximum series especially for high elevation regions comparison of the extracted ddf curves from both the original and adjusted persiann cdr data with the reported ddf curves from noaa atlas 14 shows that the extreme percentiles from the corrected persiann cdr are consistently closer to the gauge based estimates at the tested basins the median relative errors of the frequency estimates at the studied basins were less than 20 in most cases our proposed framework has the potential for constructing ddf curves for regions with limited or sparse gauge based observations using remotely sensed precipitation information and the spatiotemporal resolution of the adjusted persiann cdr data provides valuable information for various applications in remote and high elevation areas keywords rainfall frequency analysis extreme precipitation persiann cdr high elevation depth duration frequency curves 1 introduction rainfall frequency analysis rfa is an important tool in hydrologic engineering bonnin et al 2006 hosking and wallis 2005 stedinger 1993 depth duration frequency ddf curves which link extreme rainfall depths to their probability of occurrence are based on time series of extreme rainfall with different durations fitted with probability distribution functions rfa has been traditionally performed using information from rain gauges this approach has proven to be a useful tool in planning and design for regions where observational data is relatively abundant such as the united states or europe however many parts of the world particularly the developing countries do not have that advantage in many developing countries gauge observation networks over remote and mountainous regions are still sparse and limited in terms of duration with advances in tools and techniques for precipitation measurement using remotely sensed information investigation of rainfall characteristics over remote and mountainous regions with limited gauge observations has become possible in an effort to produce long and consistent climate records based on satellite observations national oceanic and atmospheric association noaa under the climate data record cdr program in cooperation with the university of california irvine developed a satellite precipitation product named the precipitation estimation from remotely sensed information and artificial neural networks climate data record persiann cdr ashouri et al 2015 persiann cdr provides near global 60on to 60os latitude and 0 to 360 longitude precipitation information with 0 25 spatial and daily temporal resolution from 1983 to the present given its relatively high spatial resolution and long record persiann cdr is a unique dataset for studying extreme precipitations and performing rainfall frequency analysis the length of the persiann cdr dataset 34 years is particularly valuable for parts of the world that lack the gauge information for rainfall frequency analysis in recent years several efforts have been made to develop ddf curves by employing remotely sensed precipitation information from weather radars and earth observing satellites eldardiry et al 2015 marra and morin 2015 overeem et al 2008 overeem et al 2009 wright et al 2013 for instance overeem et al 2009 used an 11 year gauge adjusted radar rainfall dataset and performed a regional frequency analysis to extract ddf curves for the netherlands they found that radar data despite being useful for real time rainfall analysis still suffer from serious limitations such as significant errors in extreme rainfall estimates and shortness of data that limit their usefulness for rfa thus the application of radar data for rainfall frequency analysis is hampered by 1 its relatively short length of record which leads to sampling issues during distribution fitting process and results in larger uncertainties of the frequency estimates especially for longer durations and 2 estimation uncertainties and heterogeneities due to the continuous development of radar quantitative precipitation estimation qpe instruments and methods allen and degaetano 2005 lombardo et al 2006 eldardiry et al 2015 quantified the effects of each of these sources of uncertainty and attributed much of the quantile estimation uncertainty to the length of the dataset however the conditional bias intrinsic to the radar dataset was the main reason for the observed systematic underestimations in the rainfall frequency estimates as compared to rain gauges and radar network satellite qpe is able to provide global coverage and has been employed in a number of studies for rainfall frequency analysis awadallah et al 2011 endreny and imbeah 2009 marra et al 2017 zhou et al 2015 yet similar to radars the application satellite qpes for rfa is undermined by the data length issues and estimation uncertainties associated with each of the precipitation estimation products among different remotely sensed precipitation datasets persiann cdr is a viable candidate for extreme precipitation analysis given 1 its high spatial and temporal resolution when compared with the long term global precipitation climatology project gpcp huffman et al 1997 product which is monthly and 2 5 by 2 5 persiann cdr has a higher temporal daily and spatial resolution 0 25 by 0 25 the 2 5 spatial and monthly temporal resolution is not capable of capturing the spatial and temporal variability of the extreme precipitations especially over regions with complex topographic conditions and 2 its long record persiann cdr has relatively longer data record 34 years and continually expanding in comparison to trmm 3b42 v7 huffman et al 2007 with 20 years of data or cmorph joyce et al 2004 with 16 years of record based on these strengths gado et al 2017 employed the persiann cdr dataset to estimate extreme rainfall quantiles at two homogenous regions in the western united states they combined information from the persiann cdr pixels and nearby gauges in a homogenous region and used an innovative regional frequency analysis method to derive quantile estimates at ungauged locations the primary goal of this research is to evaluate the feasibility of using the persiann cdr dataset for rainfall frequency analysis by constructing the required ddf curves over regions with limited gauge information or mountainous areas as a proof of concept this study has been conducted over the united states where longer gauge observations with sufficient spatial coverage exist as some studies have reported there are biases in the persiann cdr estimates which necessitate the application of bias adjustment techniques to improve the accuracy of the persiann cdr estimates of extreme precipitations miao et al 2015 duan et al 2016 shah and mishra 2016 yang et al 2016 liu et al 2017 this study was designed with the following objectives 1 to propose an elevation based bias correction model applicable to the persiann cdr dataset and to test it over a large number of river basins in the continental united states and 2 to demonstrate the usefulness of satellite based precipitation data in rainfall frequency analysis and to use the derived frequency estimates to further verify the effectiveness of the proposed bias correction model in the proposed frequency analysis framework only the persiann cdr information is used to estimate extreme precipitation quantiles and no information from nearby gauges is incorporated in the development of ddf curves gado et al 2017 the rest of this paper is organized as follows in section 2 a detailed description of gauge and persiann cdr datasets used in the study is presented followed by the specifications of the studied basins the bias adjustment approach cross validation techniques and the frequency analysis procedures pursued in the study are introduced in section 3 section 4 presents the results and discussion the main findings and conclusions are summarized in section 5 2 data 2 1 gauge data global historical climatology network ghcn daily is a quality controlled dataset that is used in this study this dataset contains comprehensive information of daily summaries of more than 40 meteorological variables including precipitation temperature snow depth wind information evaporation etc recorded by 100 000 land surface stations operated by 20 agencies around the world in this study we select 20 basins located in the eastern and western united states fig 1 the daily rainfall data from rain gauges with 34 years of observation 1 1 1983 12 31 2015 were downloaded from national oceanic and atmospheric association national climatic data center noaa ncdc database https www ncdc noaa gov ghcnd data access a brief description of the selected basins with their hydrologic unit codes huc and the number of gauges with 34 years of data selected for this study are presented in table 1 the selected basins incorporate a wide range of elevations from 0 to 3700 m mean sea level and diverse climatic conditions based on köppen geiger climate classification system kottek et al 2006 2 2 persiann cdr data persiann cdr is a retrospective multi satellite precipitation dataset that provides near global precipitation information 60 n 60 s latitude and 0 360 longitude at 0 25 spatial resolution around 25 km and daily temporal resolution from 1 january 1983 to near present ashouri et al 2015 the persiann cdr dataset was developed by the following steps in the first step the persiann algorithm hsu et al 1997 is implemented on the archive of gridded satellite gridsat b1 infrared data knapp et al 2011 from geostationary earth orbiting satellites geos the model is pre trained using the national center for environmental prediction ncep stage iv hourly precipitation data then the parameters of the model are kept fixed and the model is run on the entire historical records of gridsat b1 to estimate the historical precipitation at 3 hourly resolution in the next step the estimated rain rates are resampled to 2 5 spatial resolution and bias adjusted with gpcp product v2 2 adler et al 2003 to keep it consistent with the gpcp monthly product finally the persiann cdr dataset is obtained by accumulating the 3 hourly bias adjusted data in this research daily persiann cdr data for the selected basins for the time period of 1 1 1983 to 12 31 2015 was used 3 methodology 3 1 model description in our proposed bias correction model we first correct the persiann cdr estimates of annual maximum series with gauge data at pixels with available gauge records for the study period the time series of annual maximum precipitation from both gauge network and persiann cdr for the corresponding pixels are extracted and sorted in an ascending order for simplicity we denote the gauge based annual maximum series as gm and the persiann cdr annual maximum series as pm hereafter a zero intercept regression line is fitted to the scatterplot of gm and pm time series with the corresponding pm values in the y axis and gm values in the x axis fig 2 a the slope of this regression line called correction factor or cf hereafter shows the deviation of pm with respect to ground truth gm and it indicates the level of correction required for correcting pm to gm a cf value larger than one indicates an overestimation of the extreme precipitation by persiann cdr and a cf smaller than one implies the underestimation the larger the deviation of a cf value from the one to one case the greater the correction required for the pm fig 2a to investigate the orographic characteristics of bias at each basin the cf values at individual gauges are plotted against the corresponding gauge elevations fig 2b the basin scale plots are further merged to provide a more comprehensive view of the cf elevation relationship fig 2c following the approach mentioned above an exponential function is fitted to the derived cf elevation relationship at both individual basin and multi basins scale as shown in fig 2c we construct a correction function based on the cf elevation relationship derived from 4 western us basins and test its performance with different cross validation and validation methods on other basins the selected basins are san joaquin river basin california the willamette river basin oregon the upper columbia river basin washington and the colorado headwaters colorado these basins are selected since they provided bias elevation information at different elevations and encompassed different climatic conditions which are representative for building a robust and effective bias correction model applicable to other river bases in the united states finally the correction model based on these four selected river basins is tested on the other 16 basins with different elevation ranges and climatic conditions in the western and eastern u s 3 2 hold out cross validation hold out cross validation is implemented to examine how the performance of the correction model is influenced by the number of basins incorporated in the model calibration and to investigate whether incorporating information from fewer basins could improve the persiann cdr estimates of ams the four basins used for training the correction function are divided into two groups the basins are grouped in a way that information from different elevations and climates are included for each case a correction function based on the gauge and the persiann cdr information from the basins in the first group is used to adjust the pm for the basins of the other group and vice versa in other words an exponential regression function is fitted to the cf elevation relationship from the two basins in the first group and is then used to adjust the ams from the persiann cdr dataset for the basins in the second the effectiveness of the bias correction functions is assessed using the root mean squared error rmse of the sorted ams from the adjusted persiann cdr and that of gauge observations at each of the gauge locations and basins 3 3 comparison with gauge interpolation besides comparing the original and corrected persiann cdr data using the approaches mentioned above we also include a commonly used basin scale interpolation method for analyzing extreme precipitation over remote and mountainous areas where the gauge network is insufficient or even non existent chen et al 2008 doumounia et al 2014 3 3 1 leave one out cross validation precipitation intensity at an ungauged location is commonly estimated by interpolating observations from nearby gauges performance of the bias adjusted persiann cdr dataset in estimating the annual maximum time series at an ungauged location is compared with the estimates from the interpolation method and the original persiann cdr dataset at each of the calibration basins we leave one gauge out of the training phase and the entire time series of precipitation at this particular gauge location is constructed with the linear interpolation of observations from the remaining gauges then the annual maximum series at the location of the held out gauge is extracted from the interpolated time series the cf elevation relationship for the selected calibration basins is derived and the cf corresponding to the elevation of the removed gauge is used to correct the pm time series at the persiann cdr pixel over the left out gauge location finally the interpolation based annual maximum time series and the corrected pm are compared with the original gm rmse is used as the measure of the difference between the calculated time series and the gm it is worth mentioning that we repeat this procedure for all the gauges at each calibration basin to investigate the robustness of our proposed correction method 3 3 2 k fold cross validation the leave one out cross validation approach described in section 3 3 evaluates the performance of the suggested bias correction approach at a single gauge level when there is a dense gauge network in a basin interpolation of available gauge observations may result in better estimates of the ams at an ungauged site however the gauge interpolated estimates could be less reliable when the region has limited or sparse gauge observations therefore to find the breaking point where the corrected persian cdr dataset starts to outperform the interpolation based results we carry out the k fold cross validation at each of the four basins used in the calibration process different percentages i e 10 20 30 40 50 60 70 and 80 of gauges are randomly selected and left out then the entire time series of precipitation for the locations of the removed gauges are constructed using the linear interpolation of the daily observations from the remaining gauges the annual maximum series for the locations of the removed gauges are then extracted from the interpolated time series finally we compare the corrected pm and the interpolation based annual maximum time series at each gauge location with the gm for that location since various combinations of gauges could be selected as test samples results depend on the distribution of the remaining gauges and the distances between the held out and nearby gauges to reduce the sensitivity of the results to the selection of gauges we carry out 30 random selections of the hold out gauges and consider each selection as an independent test rmse of the interpolation based ams is then compared with rmse of the corrected pm for the selected gauges in each independent run the average rmse of the 30 independent runs is also calculated to have the overall error estimate for different hold out scenarios i e 10 20 30 40 50 60 70 and 80 of gauges being held out 3 4 satellite based rainfall frequency analysis national oceanic and atmospheric administration noaa atlas 14 is a source of rainfall frequency estimates for the united states and its territories noaa atlas 14 provides intensity duration frequency idf and depth duration frequency ddf curves for different regions based on the regional frequency analysis approach bonnin et al 2006 noaa atlas 14 idf and ddf curves were developed using the best fit among different probability distributions including the 3 parameter generalized extreme value gev the generalized normal the generalized pareto the generalized logistic the pearson type iii distributions the 4 parameter kappa distribution and the 5 parameter wakeby distribution at 80 of gauges and for sub daily and daily durations the gev gave the best statistics among the 3 parameter distributions and its performance was comparable to that of 4 and 5 parameter distributions thus the gev was adopted across all gauges and durations bonnin et al 2006 the gev distribution was firstly introduced by jenkinson 1955 and it has been widely used for frequency analysis of extreme precipitation and was demonstrated superior over other probability distribution functions in terms of fitting the annual maxima time series ams ben zvi 2009 bougadis and adamowski 2006 fowler and kilsby 2003 gellens 2002 norbiato et al 2007 villarini et al 2011 the gev distribution is a 3 parameter probability distribution that combines three extreme value distributions the type of the distribution is characterized by the value of the shape parameter ξ negative zero and positive values of the shape parameter determines the tail behavior of the distribution as short tailed weibull light tailed gumbel and heavy tailed fréchet respectively the gev cumulative distribution function is given by 1 f x exp 1 ξ x μ σ 1 ξ for ξ 0 2 f x exp exp x μ σ for ξ 0 where ξ μ and σ are the shape location and scale parameters respectively to fit the gev distribution with the persiann cdr daily precipitation we first adjust the data samples in which the annual maximum series of persiann cdr for 2 day 3 day 4 day 7 day 10 day 20 day 30 day 45 day and 60 day durations are corrected with gauge data using the same approach used for daily precipitation the gev distribution is fitted to the annual maxima series of the corrected persiann cdr data for different durations using gevfit function from the matlab statistics and machine learning toolbox https www mathworks com help stats gevfit html maximum likelihood estimation is used to estimate the parameters of the gev distribution and the corresponding confidence intervals embrechts et al 2013 kotz and nadarajah 2000 the return level for each return period and duration is estimated using the inverse gev function as in eqs 3 and 4 3 x t μ σ ξ 1 l n 1 1 t ξ for ξ 0 4 x t μ σ l n l n 1 1 t for ξ 0 where x t is the return level i e the rainfall depth that on average is exceeded once in t years and t 1 1 f is the return period using the return levels at different return periods and annual exceedance probabilities the ddf curves are generated noaa atlas 14 bonnin et al 2006 provides ddf curves with sub daily daily and multi day durations the ddf curves can be downloaded from the noaa precipitation frequency data server https hdsc nws noaa gov hdsc pfds since the persiann cdr dataset gives precipitation estimates at daily time scale the daily and multi day durations were considered in generating the ddf curves to remain consistent with noaa frequency estimates precipitation durations considered in this study are 1 day 2 day 3 day 4 day 7 day 10 day 20 day 30 day 45 day and 60 day it should be noted that the durations considered here do not mean precipitation occurred during the entire period but the sliding window gives the highest value of precipitation accumulation over the selected period lastly we compare the return levels based on the corrected persiann cdr estimates with that of noaa atlas 14 at each duration and return period 3 5 uncertainty assessment the confidence intervals of the return levels from the original and the adjusted persiann cdr datasets are estimated using a bootstrapping technique we generate 1000 random samples with replacements from the original and adjust ams at the target gauge locations then the maximum likelihood estimation is used to calculate the parameters of the gev distributions fitted to each of these random samples return levels for different durations are calculated using the inverse gev function evaluated at different return periods finally the 5th and 95th percentiles of the bootstrapped return levels at each duration and return period are taken as the 90 percent confidence intervals 4 results and discussion 4 1 training basins fig 3 shows the scatterplots of the persiann cdr and gauge ams and the regression line equation at a number of gauges in the willamette river basin in the state of oregon in some of the gauge locations fig 3a d and g i the original persiann cdr has a certain degree of underestimation or overestimation while in some other gauge locations the persiann cdr estimates are in good agreement with gauge observations fig 3e and f an important note here is that a persiann cdr pixel has an area about 625 km2 which is much larger than the sampling area of a rain gauge the value of a persiann cdr pixel represents the average precipitation within that pixel s spatial domain in fact even if the persiann cdr estimate at a pixel is completely accurate its value tends to be smaller than the subpixel point measurements in other words by comparing a persiann cdr pixel with a point measurement we are carrying out a point area comparison therefore by adjusting the persiann cdr pixels with point measurements we are downscaling persiann cdr to point resolution this implies that the adjusted dataset should be regarded as a point estimate rather than an area estimate furthermore there would be time discrepancies between the persiann cdr s daily interval and the gauges 24 hour intervals thus the correction factor accounts for the influence of both point area and time discrepancy issues at each of the four basins selected to build the correction model i e the san joaquin river basin the willamette river basin the upper columbia river basin and the colorado headwaters the cf at the basin scale is computed by fitting a zero intercept regression line to the sorted ams of all the available gauges and that of collocated persiann cdr pixels fig 4 in general the persiann cdr estimates of ams tend to be lower than the ams from gauge observation with different levels of underestimation in the different basins the ams estimates from the original persiann cdr dataset show considerable underestimation at colorado headwaters and upper columbia river basins with cfs equal to 0 46 and 0 57 respectively the scatterplots of cf and gauge elevation for different basins are shown in fig 5 a d in general an exponential relationship exists between cfs and elevations at each of the four basins furthermore when merging all the available gauge information from the selected river basins together a comprehensive view of this relationship is demonstrated fig 5e as we can see from fig 5e the cfs become smaller with increasing elevation of the gauges this reduction in the cfs implies the underestimation of ams at higher elevations both ir based such as persiann family and passive microwave based such as tmpa huffman et al 2007 precipitation products have been reported to underestimate precipitation in high elevations hashemi et al 2017 this underestimation has been related to several factors satellite based precipitation products have difficulties in retrieving the solid form of precipitation snow which is the prevailing type of precipitation at high elevation regions and in the winter season hashemi et al 2017 moreover since ir based precipitation algorithms rely on the cloud top temperatures they cannot fully detect the orographic enhancements in the liquid phase of precipitation in regions characterized by complex topographic conditions shige et al 2013 in addition to the technical and methodological issues inherent to the satellite precipitation estimation methods the spatial and temporal inconsistencies between the satellite precipitation estimates and gauge observations at high elevation regions can be related to the poor sampling of gauges gebregiorgis and hossain 2014 for instance libertino et al 2016 observed the lowest agreement in the timing of extreme events recorded by trmm and gauge observations in sparsely gauged regions miao et al 2015 also reported low spatial and temporal agreement in terms of extreme precipitation statistics between the persiann cdr estimates and gauge observations in regions with low density of gauges as shown in fig 5 we fit an exponential function to the scatterplots of cf for each gauge and its corresponding elevation this function is used to correct the persiann cdr estimates of ams at different basins in the eastern and western united states however since this correction model is based on only a few selected basins it is necessary to be validated using different cross validation techniques and then be tested on different basins over the continental united states 4 2 hold out cross validation results we first carry out hold out cross validation on the four selected river basins in which a correction function based on cf elevation relationship is built using the information from two of the river basins willamette and upper columbia river basins the model is tested on the other two river basins the san joaquin and the colorado headwater river basins and vice versa the goal is to examine the effects of limited gauge information and basin selection on the overall performance of the bias correction approach the effect of bias correction on the empirical cdf of the persiann cdr estimates at each of the calibration basins is shown in fig 6 at basin scale the correction method shifts the empirical cdf of the ams from the original persiann cdr towards the gauge based empirical cdf in the willamette river basin and the colorado headwaters river basin the corrected cdf is close to that of the observation in the san joaquin river basin the extreme quantiles from the corrected data are closer to the observation in the upper columbia river basin the corrected persiann cdr gives better estimates of the largest extreme values compared to the original persiann cdr estimates however it results in an overestimation of the lower quantiles this is consistent with the results shown in fig 4 where the regression based estimates gave some overestimation for values lower than 55 mm the statistics of the hold out cross validation results at gauge scale are presented in table 2 in most of the gauge locations 111 out of 127 gauges in different basins the rmse of the corrected persiann cdr is lower than that of the original persiann cdr this implies the effectiveness of the proposed bias adjustment approach in correcting the pm at pixel level even in basins with dense gauge networks at 16 gauges however the correction method tends to deteriorate the original persiann cdr estimates among these gauges 12 are located in low elevation regions 550 m from mean sea level and more than half of them are associated with elevations less than 200 m from mean sea level the upper columbia and the willamette river basins have a larger portion of these gauges with 7 and 5 unsuccessful corrections respectively the poor performance of the corrected persiann cdr at those gauge locations could be partly attributed to the complex topographic conditions of those basins which pose some challenges for the persiann algorithm to estimate precipitation accurately however as compared to the original persiann cdr the proposed correction approach works well for the majority of the gauges in the hold out cross validation as shown with the lower rmse values in table 2 4 3 leave one out cross validation in each basin one gauge is left out at a time and the time series of precipitation at that gauge location is constructed using linear interpolation the annual maximum time series from the gauge interpolation and the corrected persiann cdr are compared with the gauge observations at the corresponding location table 3 as shown in table 3 the rmse values from the corrected persiann cdr are consistently lower than those of the original persiann cdr for all basins when compared to the interpolation method the corrected persiann cdr gives lower rmse values at the san joaquin the willamette and the colorado headwaters river basins at the upper columbia river basin however the leave one out cross validation results suggest that the gauge interpolation performs better than the corrected persiann cdr data it is inferable from the gauge scale results that the correction model outperforms the interpolation method in most cases even if only one of the gauges at a densely gauged basin is removed from the sample the interpolation method also produces substantial errors at some gauge locations particularly those locations where the interpolated gauge is relatively far from its surrounding gauges it is possible that complex topography leads to different precipitation characteristics between nearby gauges and results in uncertainties in the interpolated precipitation estimates 4 4 k fold cross validation the leave one out cross validation results in the previous section demonstrate that the interpolation based estimates of ams achieved by removing one of the gauges may outperform the persiann cdr estimates at some gauges in a densely gauged region e g the upper columbia river basin in order to find the breakpoint where the corrected persiann cdr will outperform interpolation based estimates at a basin scale the k fold cross validation is implemented we randomly separate different fractions of all available gauges 0 1 0 2 0 8 of the gauges in a basin and remove the selected gauges from the model training phase this random selection and removal process is repeated 30 times for each fraction level then the entire precipitation time series at those locations are constructed by the linear interpolation of observations from the remaining gauges in that basin then rmse of ams estimates from both corrected persiann cdr and gauge interpolation are computed at the removed gauges fig 7 shows the average rmse values of ams estimates from the interpolation method blue line and the corrected persiann cdr red line for different exclusion ratios in each river basin the horizontal axis defines the number of the iteration and the vertical axis presents the average rmse value of the ams estimates on the excluded gauge locations using the corrected persiann cdr and gauge interpolation as the fraction of gauges being removed from the entire samples increases the errors associated with the interpolation method become larger in contrast the errors produced with our proposed correction method remain consistently low for different basins over most of the test scenarios i e different percentages of the gauge being removed moreover the interpolation based estimates result in large errors in some test scenarios and basins for example in the san joaquin river basin fig 7a substantial errors are observed in different scenarios and over several independent runs there are several reasons why errors from the interpolation method have large values in some of the iterations extreme precipitation events vary substantially in space and time the annual maximum precipitation at different points of a basin could be results of various extreme events occurring in different times of the year when interpolating the daily gauge observations in a region for constructing precipitation time series at an ungauged site heavy precipitation observed at one or more gauge locations could be falsely extended to the locations that were less impacted by the storm similarly by removing some of the gauges from the population the extreme events impacting those locations may not be represented in the interpolated time series from the remaining sample and as a result the extreme event at that location would be missed both of these cases may result in considerable errors in the annual maximum series estimated from the interpolation method other factors that contribute to the significant interpolation errors include long distance of sample gauges from the target locations substantial elevation differences between the target locations and sample gauges and the inability of the sample gauges to demonstrate the spatiotemporal variability of rainfall at target locations the overall errors from the corrected persiann cdr and the interpolation method at different exclusion ratios and basins are shown in fig 8 as the portion of gauges being left out increases the rmse produced by the interpolation method increases for all the basins while the proposed correction method for persiann cdr shows stable errors over different ratios and basins with respect to the ams results in the san joaquin river basin fig 8a the willamette river basin fig 8b and the colorado headwaters river basin fig 8d the corrected persiann cdr yields lower rmse values than the gauge interpolation method throughout different ratios suggesting the effectiveness of the proposed correction approach in the upper columbia river basin the gauge interpolation method results in better estimates of the ams at the ratios up to 30 however beyond the 30 threshold the corrected persiann cdr produces more accurate estimates of the ams therefore 30 of total gauges is the breakpoint for the upper columbia river basin in the context of interpolating point gauge information to spatial estimates by comparing the statistics of corrected persiann cdr and the traditional interpolation method it is observed that the proposed correction model generates more accurate estimates of the ams than does the linear interpolation method the superiority of the proposed bias correction method becomes increasingly evident as the gauges become sparser 4 5 validation on the continental u s in the previous sections we demonstrated the effectiveness and robustness of the proposed correction model on the four representative river basins in the western u s in this section we extensively validate the correction model on 16 additional river basins with different climates and topographic conditions across the continental united states table 1 the selected basins for validation cover all the climate classes available in the united states based on the köppen geiger climate classification system in addition these basins cover a broad range of elevations from low lying regions in the state of florida to high elevation regions in the state of utah these basins are also associated with various dominant precipitation mechanisms such as convective orographic and cyclonic which could influence the performance of the satellite based precipitation products hong et al 2007 liu and zipser 2009 table 4 presents the errors in ams estimates from the original and the corrected persiann cdr data on the tested river basins according to table 4 in 15 out of the 16 basins the correction model results in lower rmse values compared to the original persiann cdr data significant improvements are observed at high elevation regions such as dirty devil rio grande and upper yellowstone river basins with 78 7 72 3 71 6 reduction in the rmse of ams respectively also the correction model considerably decreases the errors in the ams estimates at mid elevation regions such as mississippi headwaters upper mississippi iowa and upper tennessee river basins table 4 among the low elevation regions nueces southwestern texas coastal and trinity river basins were quite successful with respect to the error reduction by the correction model however pascagoula river basin is less successful 7 3 decrease in rmse and south florida river basin fails to improve 29 7 increase in rmse both of these basins are located in the south atlantic gulf region which is characterized by warm convective precipitation mechanisms as a result of these convective systems satellite precipitation products often fail to provide accurate estimates at these regions as we see from the performance of raw data shown in table 4 both the pascagoula and the south florida river basins have high initial errors compared to the other river basins since the model is trained using the information from four river basins in the western united states with different hydroclimatic conditions it is reasonable for it to not perform as well under temperate and tropical climatic conditions and for extremes caused by warm convective systems generally satellite precipitation estimation algorithms perform poorly in estimating precipitation from shallow and warm convective clouds hong et al 2007 kubota et al 2009 liu and zipser 2009 sorooshian et al 2002 one reason behind this poor performance is that these algorithms relate heavy precipitations to deep convective clouds and subsequently underestimate heavy precipitations associated with shallow warm clouds hong et al 2007 liu and zipser 2009 moreover ir based methods such as persiann are based on cloud top temperature thresholds that are sometimes too cold for warm orographic clouds adler et al 2003 dinku et al 2008 finally due to the contamination by the cold anvil cirrus clouds ir based precipitation estimates typically display 1 3hr phase shift compared to the maximum diurnal precipitation these phase shifts influence the performance of ir based methods in regions dominated by warm convective clouds sorooshian et al 2002 4 6 multiday annual maximum series in fig 9 we present the scatterplots of the cf elevation for multi day duration ams at colorado headwaters fig 9a j as an illustrative example fig 9 a j suggest there is a similar cf elevation behavior in multi day ams analysis for different durations fig 9 k presents the exponential regression functions fitted to each of the n day maximum scatterplots according to fig 9 k as durations increase from 1 day to 60 days the original persiann cdr estimates of the ams become more accurate i e closer to the cf 1 line this is because the persiann cdr dataset is bias adjusted with gpcp dataset huffman et al 1997 at a monthly scale and the values from the two datasets become closer to each other at longer durations therefore at 30 days or 60 days analysis the ams estimates should be close to gauge observation although persiann cdr and gauge information are adjusted at a monthly scale the monthly coefficients are applied to daily estimates ashouri et al 2015 therefore the sub monthly or daily estimates may not be compatible with gauge observations at the corresponding scale furthermore the gpcp is a gauge interpolated dataset which its pixel values are essentially the average values of gauge observations within the large grid boundary however here we compare the persiann cdr estimates in pixel scale with the collocated gauge values which could differ substantially from the corresponding gpcp pixel values 4 7 depth duration frequency curves fig 10 shows the ddf curves derived from the adjusted persiann cdr data the frequency estimates from noaa atlas 14 and the 90 confidence intervals for a gauge location in dirty devil basin in the state of utah usc00420849 we present return levels for daily and multi day durations given the daily resolution of the persiann cdr dataset as shown in fig 10 the frequency estimates from the original persiann cdr data are outside the 90 confidence intervals of the noaa atlas 14 which suggests the necessity of bias adjustment prior to employing the data for frequency analysis on the other hand ddf curves from the adjusted persiann cdr data are well within the 90 confidence intervals of the noaa atlas 14 ddf curves in most cases the frequency estimates from the adjusted persiann cdr data are very close to the noaa atlas 14 estimates which are calculated by incorporating a large number of gauges and longer records of data for frequency estimation larger deviations from the gauge based estimates are observed at longer return periods and there is no clear trend in terms of overestimation or underestimation with respect to duration as shown in fig 10 the confidence intervals from the gauge based and satellite based ddf etimates become larger as the return periods increase this higher uncertainty is because of the lower sample size at the tails of the distributions furthermore the confidence intervals from the original and the adjusted persiann cdr datasets are relatively comparable given the similar lengths of the two datasets however the uncertainty bounds from the satellite based ddf estimates are larger than those from the noaa atlas 14 one reason behind these larger confidence intervals is the shortness of the persiann cdr dataset when compared to the gauge information used for the development of noaa atlas 14 ddf curves another reason is the difference between the frequency analysis method implemented here and the method employed in the development of noaa atlas 14 noaa uses the regional frequency analysis based on l moments to estimate the frequency and intensity of extremes hosking and wallis 2005 the regional frequency analysis method is used in atlas 14 in order to relieve the uncertainties arising from a low sample size limited years of observations during the gev parameter estimation process although the regional frequency analysis method gives frequency estimates with lower uncertainties it comes with the assumption of regional homogeneity in extreme rainfall characteristics which is not always a valid assumption here the frequency analysis methods and uncertainties of the frequency estimates are outside of the scope of this study and the ddf curves and their error analyses are the proof of concept as seen in fig 10 ddf curves from the original persiann cdr suggest underestimation of the extreme precipitation quantiles for different durations this is expected given the spatial resolution of this dataset in fact when considering the remotely sensed precipitation information we should be aware that the pixel value represents a spatial average of precipitation within the extent of a pixel in other words the pixel value disregards the subpixel variability and even if the persiann cdr estimate at a pixel is completely accurate its value tends to be smaller than the collocated point measurements as a result the extracted ddf curves from a satellite pixel tend to demonstrate lower return levels peleg et al 2018b as observed in fig 10 it is worth noting that the estimated ddf curves from the persiann cdr data are not necessarily based on the liquid phase precipitations and the extracted ams may comprise snowfalls as well this is because persiann cdr and many other satellite based precipitation estimation algorithms do not distinguish between precipitation phases in other words the annual maximum time series extracted and used here may contain solid phase precipitation extremes due to snowfalls although this study does not differentiate between solid and liquid phases of precipitation in order to obtain purely rain based ddf curves the current framework can be further modified to incorporate additional observations on solid precipitations there are two approaches to achieve this goal one approach for this would be to limit the analysis to warm seasons but the definitions of warm season vary among various geographic locations another approach would be to distinguish snowfall from rainfall but this would require snowfall and air temperature data that are not available everywhere future independent research may improve upon the current study by including such additional information fig 11 displays the box plots of rmse of the return level estimates from the original and corrected cdr for different durations and return periods at collocated persiann cdr pixels and gauges for different basins in the continental us the corrected persiann cdr data was used to obtain frequency estimates at different gauge locations in the selected basins and the results were compared with those from noaa atlas 14 note that three out of the 16 basins basins 5 6 and 15 were located in the pacific northwest region and two basins basins 8 and 14 were located in the state of texas all of which were not covered by or were being updated in the recent volumes of noaa atlas 14 thus the frequency estimates were only validated at the remaining 11 basins as shown in fig 11 according to fig 11 the rmse values for return level estimates corresponding to longer return periods are generally higher for both datasets this is expected as the persiann cdr dataset is relatively shorter than the gauge information used for the development of noaa atlas 14 the shorter record will result in smaller samples higher uncertainties and larger deviations at the tails of the distribution over the tested basins the frequency estimates from the corrected persiann cdr data have consistently lower median rmse values than those from the original persiann cdr at different return periods the rmse values at the basins with higher elevations such as central nevada or dirty devil basins were relatively lower than these at basins with lower elevations such as mississippi headwaters or upper mississippi iowa basins which implies the suitability of the correction approach for high elevation regions furthermore in most of the basins and at different return periods the corrected dataset shows lower variability in rmse of the frequency estimates corrected persiann cdr data also demonstrate superior performance in terms of median rmse and variability of rmse values at the gauges within the basins the only case for which the corrected persiann cdr results in higher rmse values at different return periods and durations is the south florida basin where it was previously shown that the correction model does not improve the ams estimates due to the climate and the precipitation mechanism the relative errors of the frequency estimates are calculated to show the relative magnitude of the return level errors compared to the return levels from noaa atlas 14 the relative error here is the difference between the frequency estimates from persiann cdr original and bias adjusted and noaa atlas 14 divided by the value from noaa atlas 14 fig 12 demonstrates the absolute value of the relative error for the frequency estimates at different durations and return periods from the corrected and original persiann cdr data as shown the relative errors from the corrected persiann cdr data have consistently lower median values as well as lower variability at different return periods in the tested basins the median relative errors from the corrected data are less than 20 different from the return levels estimated by noaa atlas 14 similar performance is observed when the relative errors of frequency estimates from the two datasets are compared with respect to the extreme precipitation duration it is also noted that the corrected persiann cdr dataset does not show a systematic increase or decrease in the relative errors of the frequency estimates with respect to the duration the relative errors of the return level estimates from the original persiann cdr data tend to decrease with increasing duration this finding is consistent with our observations in fig 9 that revealed lower errors of the original persiann cdr data for longer duration extreme events as with fig 11 the only case in which the corrected data resulted in higher rmse values was the south florida basin where the correction model did not improve the ams estimates section 4 5 5 summary and conclusions in this study the application of the persiann cdr dataset for rainfall frequency analysis was investigated a bias correction model was developed to further correct the persiann cdr estimates of annual maximum time series at the pixel scale the proposed correction approach was implemented in two steps 1 bias correction factors at limited gauge locations were estimated using linear regression analysis between annual maximum series ams of gauges and collocated pixels and 2 the correction factors from the limited gauge locations were extended to other regions where gauge data were not available the correction model was validated at 16 basins in the continental united states covering various climates and elevations finally depth duration frequency ddf curves were constructed by fitting the generalized extreme value distribution to the ams from the corrected data and estimating the quantiles of extreme precipitations below is a summary of our main findings 1 the proposed bias correction approach has been demonstrated effective and robust in improving the accuracy of a remote sensing precipitation estimation product i e persiann cdr especially in high elevation river basins where gauge or radar networks are either limited or non existent 2 the hold out cross validation results indicated that the proposed bias correction model is capable of improving the ams estimated by the persiann cdr dataset even in the case that limited gauge information was provided for the model calibration and the approach is generalizable to other locations with similar climates and elevations 3 as shown by the leave one out cross validation the bias adjusted persiann cdr gave better estimates of the ams for the ungauged sites at a majority of the basins even though these basins had dense gauge networks 4 results from the k fold cross validation method suggested that the persiann cdr data bias corrected with the proposed correction approach performs consistently better than the gauge interpolation method in estimating the ams at a majority of regions with limited gauge observations it was observed that the gauge interpolation may sometimes result in significant errors in ams estimates especially in regions with complex topography 5 the validation results over 16 basins across different climates and elevations indicated that the proposed correction method improves the persiann cdr estimates of ams especially in high elevation regions 6 the bias adjusted persiann cdr is further applied to derive the return levels for different return periods and durations the frequency estimates from the corrected persiann cdr data are compared with those from the original persiann cdr and noaa atlas 14 results revealed that the frequency estimates from the corrected dataset are consistently closer to the estimates from noaa atlas 14 they also lie within the uncertainty bounds of noaa atlas 14 thus the persiann cdr dataset has the potential for being used in rainfall frequency analysis for the regions with limited ground based observations however despite the promising results there are still some limitations in this dataset and the proposed correction method for the application of frequency analysis one of these limitations is the temporal resolution of the persiann cdr dataset the daily temporal resolution limits the investigation of extreme events with shorter durations e g 3 hourly or hourly another limitation is that the frequency analysis here is conducted at the pixel scale using relatively limited samples a sample of 33 annual maximum values is relatively limited for fitting a 3 parameter distribution this would result in high uncertainties in estimating the parameters of the distribution and the return levels one remedy to the sample size problem could be the application of regional frequency analysis methods to increase the sample size by incorporating information from the nearby locations with the same climatic conditions it is also important to note that given the rising global temperatures rainfall intensities especially at shorter durations are expected to increase therefore the increase in the global temperature could be used as an added factor to adjust historical design rainfall intensities for the warmer temperatures that lie ahead peleg et al 2018a this work is part of an ongoing research and the presented approaches and results are intended as a proof of concept future research in this area may involve bringing non stationarities into the bias adjustment framework tao et al 2018 including covariates into the bias adjustment framework which requires advanced optimization techniques yang et al 2017 investigating the hydrological modeling applications of the corrected persiann cdr data and developing ddf curves for ungauged regions or areas not included in the current noaa atlas 14 acknowledgments the financial support of this research is from u s department of energy doe prime award de ia0000018 california energy commission cec award 300 15 005 maseeh fellowship nsf cybersees project award ccf 1331915 noaa nesdis ncdc prime award na09nes4400006 and ncsu cics and subaward 2009 1380 01 the u s army research office award w911nf 11 1 0422 and the national key r d program of china grant no 2016yfe0102400 finally the authors sincerely thank the editor the associate editor and the anonymous reviewers for their valuable comments and suggestions 
7130,rainfall frequency analysis for ungauged regions using remotely sensed precipitation information mohammad faridzad a tiantian yang a b kuolin hsu a soroosh sorooshian a chan xiao c a department of civil and environmental engineering center for hydrometeorology and remote sensing chrs university of california irvine irvine ca usa department of civil and environmental engineering center for hydrometeorology and remote sensing chrs university of california irvine irvine ca usa b deltares usa inc silver spring md usa deltares usa inc silver spring md usa c national climate center china meteorological administration beijing china national climate center china meteorological administration beijing china corresponding author at department of civil and environmental engineering center for hydrometeorology and remote sensing chrs university of california irvine irvine ca usa department of civil and environmental engineering center for hydrometeorology and remote sensing chrs university of california irvine irvine ca usa this manuscript was handled by emmanouil anagnostou editor in chief with the assistance of francesco marra associate editor rainfall frequency analysis which is an important tool in hydrologic engineering has been traditionally performed using information from gauge observations this approach has proven to be a useful tool in planning and design for the regions where sufficient observational data are available however in many parts of the world where ground based observations are sparse and limited in length the effectiveness of statistical methods for such applications is highly limited the sparse gauge networks over those regions especially over remote areas and high elevation regions cannot represent the spatiotemporal variability of extreme rainfall events and hence preclude developing depth duration frequency curves ddf for rainfall frequency analysis in this study the persiann cdr dataset is used to propose a mechanism by which satellite precipitation information could be used for rainfall frequency analysis and development of ddf curves in the proposed framework we first adjust the extreme precipitation time series estimated by persiann cdr using an elevation based correction function then use the adjusted dataset to develop ddf curves as a proof of concept we have implemented our proposed approach in 20 river basins in the united states with different climatic conditions and elevations bias adjustment results indicate that the correction model can significantly reduce the biases in persiann cdr estimates of annual maximum series especially for high elevation regions comparison of the extracted ddf curves from both the original and adjusted persiann cdr data with the reported ddf curves from noaa atlas 14 shows that the extreme percentiles from the corrected persiann cdr are consistently closer to the gauge based estimates at the tested basins the median relative errors of the frequency estimates at the studied basins were less than 20 in most cases our proposed framework has the potential for constructing ddf curves for regions with limited or sparse gauge based observations using remotely sensed precipitation information and the spatiotemporal resolution of the adjusted persiann cdr data provides valuable information for various applications in remote and high elevation areas keywords rainfall frequency analysis extreme precipitation persiann cdr high elevation depth duration frequency curves 1 introduction rainfall frequency analysis rfa is an important tool in hydrologic engineering bonnin et al 2006 hosking and wallis 2005 stedinger 1993 depth duration frequency ddf curves which link extreme rainfall depths to their probability of occurrence are based on time series of extreme rainfall with different durations fitted with probability distribution functions rfa has been traditionally performed using information from rain gauges this approach has proven to be a useful tool in planning and design for regions where observational data is relatively abundant such as the united states or europe however many parts of the world particularly the developing countries do not have that advantage in many developing countries gauge observation networks over remote and mountainous regions are still sparse and limited in terms of duration with advances in tools and techniques for precipitation measurement using remotely sensed information investigation of rainfall characteristics over remote and mountainous regions with limited gauge observations has become possible in an effort to produce long and consistent climate records based on satellite observations national oceanic and atmospheric association noaa under the climate data record cdr program in cooperation with the university of california irvine developed a satellite precipitation product named the precipitation estimation from remotely sensed information and artificial neural networks climate data record persiann cdr ashouri et al 2015 persiann cdr provides near global 60on to 60os latitude and 0 to 360 longitude precipitation information with 0 25 spatial and daily temporal resolution from 1983 to the present given its relatively high spatial resolution and long record persiann cdr is a unique dataset for studying extreme precipitations and performing rainfall frequency analysis the length of the persiann cdr dataset 34 years is particularly valuable for parts of the world that lack the gauge information for rainfall frequency analysis in recent years several efforts have been made to develop ddf curves by employing remotely sensed precipitation information from weather radars and earth observing satellites eldardiry et al 2015 marra and morin 2015 overeem et al 2008 overeem et al 2009 wright et al 2013 for instance overeem et al 2009 used an 11 year gauge adjusted radar rainfall dataset and performed a regional frequency analysis to extract ddf curves for the netherlands they found that radar data despite being useful for real time rainfall analysis still suffer from serious limitations such as significant errors in extreme rainfall estimates and shortness of data that limit their usefulness for rfa thus the application of radar data for rainfall frequency analysis is hampered by 1 its relatively short length of record which leads to sampling issues during distribution fitting process and results in larger uncertainties of the frequency estimates especially for longer durations and 2 estimation uncertainties and heterogeneities due to the continuous development of radar quantitative precipitation estimation qpe instruments and methods allen and degaetano 2005 lombardo et al 2006 eldardiry et al 2015 quantified the effects of each of these sources of uncertainty and attributed much of the quantile estimation uncertainty to the length of the dataset however the conditional bias intrinsic to the radar dataset was the main reason for the observed systematic underestimations in the rainfall frequency estimates as compared to rain gauges and radar network satellite qpe is able to provide global coverage and has been employed in a number of studies for rainfall frequency analysis awadallah et al 2011 endreny and imbeah 2009 marra et al 2017 zhou et al 2015 yet similar to radars the application satellite qpes for rfa is undermined by the data length issues and estimation uncertainties associated with each of the precipitation estimation products among different remotely sensed precipitation datasets persiann cdr is a viable candidate for extreme precipitation analysis given 1 its high spatial and temporal resolution when compared with the long term global precipitation climatology project gpcp huffman et al 1997 product which is monthly and 2 5 by 2 5 persiann cdr has a higher temporal daily and spatial resolution 0 25 by 0 25 the 2 5 spatial and monthly temporal resolution is not capable of capturing the spatial and temporal variability of the extreme precipitations especially over regions with complex topographic conditions and 2 its long record persiann cdr has relatively longer data record 34 years and continually expanding in comparison to trmm 3b42 v7 huffman et al 2007 with 20 years of data or cmorph joyce et al 2004 with 16 years of record based on these strengths gado et al 2017 employed the persiann cdr dataset to estimate extreme rainfall quantiles at two homogenous regions in the western united states they combined information from the persiann cdr pixels and nearby gauges in a homogenous region and used an innovative regional frequency analysis method to derive quantile estimates at ungauged locations the primary goal of this research is to evaluate the feasibility of using the persiann cdr dataset for rainfall frequency analysis by constructing the required ddf curves over regions with limited gauge information or mountainous areas as a proof of concept this study has been conducted over the united states where longer gauge observations with sufficient spatial coverage exist as some studies have reported there are biases in the persiann cdr estimates which necessitate the application of bias adjustment techniques to improve the accuracy of the persiann cdr estimates of extreme precipitations miao et al 2015 duan et al 2016 shah and mishra 2016 yang et al 2016 liu et al 2017 this study was designed with the following objectives 1 to propose an elevation based bias correction model applicable to the persiann cdr dataset and to test it over a large number of river basins in the continental united states and 2 to demonstrate the usefulness of satellite based precipitation data in rainfall frequency analysis and to use the derived frequency estimates to further verify the effectiveness of the proposed bias correction model in the proposed frequency analysis framework only the persiann cdr information is used to estimate extreme precipitation quantiles and no information from nearby gauges is incorporated in the development of ddf curves gado et al 2017 the rest of this paper is organized as follows in section 2 a detailed description of gauge and persiann cdr datasets used in the study is presented followed by the specifications of the studied basins the bias adjustment approach cross validation techniques and the frequency analysis procedures pursued in the study are introduced in section 3 section 4 presents the results and discussion the main findings and conclusions are summarized in section 5 2 data 2 1 gauge data global historical climatology network ghcn daily is a quality controlled dataset that is used in this study this dataset contains comprehensive information of daily summaries of more than 40 meteorological variables including precipitation temperature snow depth wind information evaporation etc recorded by 100 000 land surface stations operated by 20 agencies around the world in this study we select 20 basins located in the eastern and western united states fig 1 the daily rainfall data from rain gauges with 34 years of observation 1 1 1983 12 31 2015 were downloaded from national oceanic and atmospheric association national climatic data center noaa ncdc database https www ncdc noaa gov ghcnd data access a brief description of the selected basins with their hydrologic unit codes huc and the number of gauges with 34 years of data selected for this study are presented in table 1 the selected basins incorporate a wide range of elevations from 0 to 3700 m mean sea level and diverse climatic conditions based on köppen geiger climate classification system kottek et al 2006 2 2 persiann cdr data persiann cdr is a retrospective multi satellite precipitation dataset that provides near global precipitation information 60 n 60 s latitude and 0 360 longitude at 0 25 spatial resolution around 25 km and daily temporal resolution from 1 january 1983 to near present ashouri et al 2015 the persiann cdr dataset was developed by the following steps in the first step the persiann algorithm hsu et al 1997 is implemented on the archive of gridded satellite gridsat b1 infrared data knapp et al 2011 from geostationary earth orbiting satellites geos the model is pre trained using the national center for environmental prediction ncep stage iv hourly precipitation data then the parameters of the model are kept fixed and the model is run on the entire historical records of gridsat b1 to estimate the historical precipitation at 3 hourly resolution in the next step the estimated rain rates are resampled to 2 5 spatial resolution and bias adjusted with gpcp product v2 2 adler et al 2003 to keep it consistent with the gpcp monthly product finally the persiann cdr dataset is obtained by accumulating the 3 hourly bias adjusted data in this research daily persiann cdr data for the selected basins for the time period of 1 1 1983 to 12 31 2015 was used 3 methodology 3 1 model description in our proposed bias correction model we first correct the persiann cdr estimates of annual maximum series with gauge data at pixels with available gauge records for the study period the time series of annual maximum precipitation from both gauge network and persiann cdr for the corresponding pixels are extracted and sorted in an ascending order for simplicity we denote the gauge based annual maximum series as gm and the persiann cdr annual maximum series as pm hereafter a zero intercept regression line is fitted to the scatterplot of gm and pm time series with the corresponding pm values in the y axis and gm values in the x axis fig 2 a the slope of this regression line called correction factor or cf hereafter shows the deviation of pm with respect to ground truth gm and it indicates the level of correction required for correcting pm to gm a cf value larger than one indicates an overestimation of the extreme precipitation by persiann cdr and a cf smaller than one implies the underestimation the larger the deviation of a cf value from the one to one case the greater the correction required for the pm fig 2a to investigate the orographic characteristics of bias at each basin the cf values at individual gauges are plotted against the corresponding gauge elevations fig 2b the basin scale plots are further merged to provide a more comprehensive view of the cf elevation relationship fig 2c following the approach mentioned above an exponential function is fitted to the derived cf elevation relationship at both individual basin and multi basins scale as shown in fig 2c we construct a correction function based on the cf elevation relationship derived from 4 western us basins and test its performance with different cross validation and validation methods on other basins the selected basins are san joaquin river basin california the willamette river basin oregon the upper columbia river basin washington and the colorado headwaters colorado these basins are selected since they provided bias elevation information at different elevations and encompassed different climatic conditions which are representative for building a robust and effective bias correction model applicable to other river bases in the united states finally the correction model based on these four selected river basins is tested on the other 16 basins with different elevation ranges and climatic conditions in the western and eastern u s 3 2 hold out cross validation hold out cross validation is implemented to examine how the performance of the correction model is influenced by the number of basins incorporated in the model calibration and to investigate whether incorporating information from fewer basins could improve the persiann cdr estimates of ams the four basins used for training the correction function are divided into two groups the basins are grouped in a way that information from different elevations and climates are included for each case a correction function based on the gauge and the persiann cdr information from the basins in the first group is used to adjust the pm for the basins of the other group and vice versa in other words an exponential regression function is fitted to the cf elevation relationship from the two basins in the first group and is then used to adjust the ams from the persiann cdr dataset for the basins in the second the effectiveness of the bias correction functions is assessed using the root mean squared error rmse of the sorted ams from the adjusted persiann cdr and that of gauge observations at each of the gauge locations and basins 3 3 comparison with gauge interpolation besides comparing the original and corrected persiann cdr data using the approaches mentioned above we also include a commonly used basin scale interpolation method for analyzing extreme precipitation over remote and mountainous areas where the gauge network is insufficient or even non existent chen et al 2008 doumounia et al 2014 3 3 1 leave one out cross validation precipitation intensity at an ungauged location is commonly estimated by interpolating observations from nearby gauges performance of the bias adjusted persiann cdr dataset in estimating the annual maximum time series at an ungauged location is compared with the estimates from the interpolation method and the original persiann cdr dataset at each of the calibration basins we leave one gauge out of the training phase and the entire time series of precipitation at this particular gauge location is constructed with the linear interpolation of observations from the remaining gauges then the annual maximum series at the location of the held out gauge is extracted from the interpolated time series the cf elevation relationship for the selected calibration basins is derived and the cf corresponding to the elevation of the removed gauge is used to correct the pm time series at the persiann cdr pixel over the left out gauge location finally the interpolation based annual maximum time series and the corrected pm are compared with the original gm rmse is used as the measure of the difference between the calculated time series and the gm it is worth mentioning that we repeat this procedure for all the gauges at each calibration basin to investigate the robustness of our proposed correction method 3 3 2 k fold cross validation the leave one out cross validation approach described in section 3 3 evaluates the performance of the suggested bias correction approach at a single gauge level when there is a dense gauge network in a basin interpolation of available gauge observations may result in better estimates of the ams at an ungauged site however the gauge interpolated estimates could be less reliable when the region has limited or sparse gauge observations therefore to find the breaking point where the corrected persian cdr dataset starts to outperform the interpolation based results we carry out the k fold cross validation at each of the four basins used in the calibration process different percentages i e 10 20 30 40 50 60 70 and 80 of gauges are randomly selected and left out then the entire time series of precipitation for the locations of the removed gauges are constructed using the linear interpolation of the daily observations from the remaining gauges the annual maximum series for the locations of the removed gauges are then extracted from the interpolated time series finally we compare the corrected pm and the interpolation based annual maximum time series at each gauge location with the gm for that location since various combinations of gauges could be selected as test samples results depend on the distribution of the remaining gauges and the distances between the held out and nearby gauges to reduce the sensitivity of the results to the selection of gauges we carry out 30 random selections of the hold out gauges and consider each selection as an independent test rmse of the interpolation based ams is then compared with rmse of the corrected pm for the selected gauges in each independent run the average rmse of the 30 independent runs is also calculated to have the overall error estimate for different hold out scenarios i e 10 20 30 40 50 60 70 and 80 of gauges being held out 3 4 satellite based rainfall frequency analysis national oceanic and atmospheric administration noaa atlas 14 is a source of rainfall frequency estimates for the united states and its territories noaa atlas 14 provides intensity duration frequency idf and depth duration frequency ddf curves for different regions based on the regional frequency analysis approach bonnin et al 2006 noaa atlas 14 idf and ddf curves were developed using the best fit among different probability distributions including the 3 parameter generalized extreme value gev the generalized normal the generalized pareto the generalized logistic the pearson type iii distributions the 4 parameter kappa distribution and the 5 parameter wakeby distribution at 80 of gauges and for sub daily and daily durations the gev gave the best statistics among the 3 parameter distributions and its performance was comparable to that of 4 and 5 parameter distributions thus the gev was adopted across all gauges and durations bonnin et al 2006 the gev distribution was firstly introduced by jenkinson 1955 and it has been widely used for frequency analysis of extreme precipitation and was demonstrated superior over other probability distribution functions in terms of fitting the annual maxima time series ams ben zvi 2009 bougadis and adamowski 2006 fowler and kilsby 2003 gellens 2002 norbiato et al 2007 villarini et al 2011 the gev distribution is a 3 parameter probability distribution that combines three extreme value distributions the type of the distribution is characterized by the value of the shape parameter ξ negative zero and positive values of the shape parameter determines the tail behavior of the distribution as short tailed weibull light tailed gumbel and heavy tailed fréchet respectively the gev cumulative distribution function is given by 1 f x exp 1 ξ x μ σ 1 ξ for ξ 0 2 f x exp exp x μ σ for ξ 0 where ξ μ and σ are the shape location and scale parameters respectively to fit the gev distribution with the persiann cdr daily precipitation we first adjust the data samples in which the annual maximum series of persiann cdr for 2 day 3 day 4 day 7 day 10 day 20 day 30 day 45 day and 60 day durations are corrected with gauge data using the same approach used for daily precipitation the gev distribution is fitted to the annual maxima series of the corrected persiann cdr data for different durations using gevfit function from the matlab statistics and machine learning toolbox https www mathworks com help stats gevfit html maximum likelihood estimation is used to estimate the parameters of the gev distribution and the corresponding confidence intervals embrechts et al 2013 kotz and nadarajah 2000 the return level for each return period and duration is estimated using the inverse gev function as in eqs 3 and 4 3 x t μ σ ξ 1 l n 1 1 t ξ for ξ 0 4 x t μ σ l n l n 1 1 t for ξ 0 where x t is the return level i e the rainfall depth that on average is exceeded once in t years and t 1 1 f is the return period using the return levels at different return periods and annual exceedance probabilities the ddf curves are generated noaa atlas 14 bonnin et al 2006 provides ddf curves with sub daily daily and multi day durations the ddf curves can be downloaded from the noaa precipitation frequency data server https hdsc nws noaa gov hdsc pfds since the persiann cdr dataset gives precipitation estimates at daily time scale the daily and multi day durations were considered in generating the ddf curves to remain consistent with noaa frequency estimates precipitation durations considered in this study are 1 day 2 day 3 day 4 day 7 day 10 day 20 day 30 day 45 day and 60 day it should be noted that the durations considered here do not mean precipitation occurred during the entire period but the sliding window gives the highest value of precipitation accumulation over the selected period lastly we compare the return levels based on the corrected persiann cdr estimates with that of noaa atlas 14 at each duration and return period 3 5 uncertainty assessment the confidence intervals of the return levels from the original and the adjusted persiann cdr datasets are estimated using a bootstrapping technique we generate 1000 random samples with replacements from the original and adjust ams at the target gauge locations then the maximum likelihood estimation is used to calculate the parameters of the gev distributions fitted to each of these random samples return levels for different durations are calculated using the inverse gev function evaluated at different return periods finally the 5th and 95th percentiles of the bootstrapped return levels at each duration and return period are taken as the 90 percent confidence intervals 4 results and discussion 4 1 training basins fig 3 shows the scatterplots of the persiann cdr and gauge ams and the regression line equation at a number of gauges in the willamette river basin in the state of oregon in some of the gauge locations fig 3a d and g i the original persiann cdr has a certain degree of underestimation or overestimation while in some other gauge locations the persiann cdr estimates are in good agreement with gauge observations fig 3e and f an important note here is that a persiann cdr pixel has an area about 625 km2 which is much larger than the sampling area of a rain gauge the value of a persiann cdr pixel represents the average precipitation within that pixel s spatial domain in fact even if the persiann cdr estimate at a pixel is completely accurate its value tends to be smaller than the subpixel point measurements in other words by comparing a persiann cdr pixel with a point measurement we are carrying out a point area comparison therefore by adjusting the persiann cdr pixels with point measurements we are downscaling persiann cdr to point resolution this implies that the adjusted dataset should be regarded as a point estimate rather than an area estimate furthermore there would be time discrepancies between the persiann cdr s daily interval and the gauges 24 hour intervals thus the correction factor accounts for the influence of both point area and time discrepancy issues at each of the four basins selected to build the correction model i e the san joaquin river basin the willamette river basin the upper columbia river basin and the colorado headwaters the cf at the basin scale is computed by fitting a zero intercept regression line to the sorted ams of all the available gauges and that of collocated persiann cdr pixels fig 4 in general the persiann cdr estimates of ams tend to be lower than the ams from gauge observation with different levels of underestimation in the different basins the ams estimates from the original persiann cdr dataset show considerable underestimation at colorado headwaters and upper columbia river basins with cfs equal to 0 46 and 0 57 respectively the scatterplots of cf and gauge elevation for different basins are shown in fig 5 a d in general an exponential relationship exists between cfs and elevations at each of the four basins furthermore when merging all the available gauge information from the selected river basins together a comprehensive view of this relationship is demonstrated fig 5e as we can see from fig 5e the cfs become smaller with increasing elevation of the gauges this reduction in the cfs implies the underestimation of ams at higher elevations both ir based such as persiann family and passive microwave based such as tmpa huffman et al 2007 precipitation products have been reported to underestimate precipitation in high elevations hashemi et al 2017 this underestimation has been related to several factors satellite based precipitation products have difficulties in retrieving the solid form of precipitation snow which is the prevailing type of precipitation at high elevation regions and in the winter season hashemi et al 2017 moreover since ir based precipitation algorithms rely on the cloud top temperatures they cannot fully detect the orographic enhancements in the liquid phase of precipitation in regions characterized by complex topographic conditions shige et al 2013 in addition to the technical and methodological issues inherent to the satellite precipitation estimation methods the spatial and temporal inconsistencies between the satellite precipitation estimates and gauge observations at high elevation regions can be related to the poor sampling of gauges gebregiorgis and hossain 2014 for instance libertino et al 2016 observed the lowest agreement in the timing of extreme events recorded by trmm and gauge observations in sparsely gauged regions miao et al 2015 also reported low spatial and temporal agreement in terms of extreme precipitation statistics between the persiann cdr estimates and gauge observations in regions with low density of gauges as shown in fig 5 we fit an exponential function to the scatterplots of cf for each gauge and its corresponding elevation this function is used to correct the persiann cdr estimates of ams at different basins in the eastern and western united states however since this correction model is based on only a few selected basins it is necessary to be validated using different cross validation techniques and then be tested on different basins over the continental united states 4 2 hold out cross validation results we first carry out hold out cross validation on the four selected river basins in which a correction function based on cf elevation relationship is built using the information from two of the river basins willamette and upper columbia river basins the model is tested on the other two river basins the san joaquin and the colorado headwater river basins and vice versa the goal is to examine the effects of limited gauge information and basin selection on the overall performance of the bias correction approach the effect of bias correction on the empirical cdf of the persiann cdr estimates at each of the calibration basins is shown in fig 6 at basin scale the correction method shifts the empirical cdf of the ams from the original persiann cdr towards the gauge based empirical cdf in the willamette river basin and the colorado headwaters river basin the corrected cdf is close to that of the observation in the san joaquin river basin the extreme quantiles from the corrected data are closer to the observation in the upper columbia river basin the corrected persiann cdr gives better estimates of the largest extreme values compared to the original persiann cdr estimates however it results in an overestimation of the lower quantiles this is consistent with the results shown in fig 4 where the regression based estimates gave some overestimation for values lower than 55 mm the statistics of the hold out cross validation results at gauge scale are presented in table 2 in most of the gauge locations 111 out of 127 gauges in different basins the rmse of the corrected persiann cdr is lower than that of the original persiann cdr this implies the effectiveness of the proposed bias adjustment approach in correcting the pm at pixel level even in basins with dense gauge networks at 16 gauges however the correction method tends to deteriorate the original persiann cdr estimates among these gauges 12 are located in low elevation regions 550 m from mean sea level and more than half of them are associated with elevations less than 200 m from mean sea level the upper columbia and the willamette river basins have a larger portion of these gauges with 7 and 5 unsuccessful corrections respectively the poor performance of the corrected persiann cdr at those gauge locations could be partly attributed to the complex topographic conditions of those basins which pose some challenges for the persiann algorithm to estimate precipitation accurately however as compared to the original persiann cdr the proposed correction approach works well for the majority of the gauges in the hold out cross validation as shown with the lower rmse values in table 2 4 3 leave one out cross validation in each basin one gauge is left out at a time and the time series of precipitation at that gauge location is constructed using linear interpolation the annual maximum time series from the gauge interpolation and the corrected persiann cdr are compared with the gauge observations at the corresponding location table 3 as shown in table 3 the rmse values from the corrected persiann cdr are consistently lower than those of the original persiann cdr for all basins when compared to the interpolation method the corrected persiann cdr gives lower rmse values at the san joaquin the willamette and the colorado headwaters river basins at the upper columbia river basin however the leave one out cross validation results suggest that the gauge interpolation performs better than the corrected persiann cdr data it is inferable from the gauge scale results that the correction model outperforms the interpolation method in most cases even if only one of the gauges at a densely gauged basin is removed from the sample the interpolation method also produces substantial errors at some gauge locations particularly those locations where the interpolated gauge is relatively far from its surrounding gauges it is possible that complex topography leads to different precipitation characteristics between nearby gauges and results in uncertainties in the interpolated precipitation estimates 4 4 k fold cross validation the leave one out cross validation results in the previous section demonstrate that the interpolation based estimates of ams achieved by removing one of the gauges may outperform the persiann cdr estimates at some gauges in a densely gauged region e g the upper columbia river basin in order to find the breakpoint where the corrected persiann cdr will outperform interpolation based estimates at a basin scale the k fold cross validation is implemented we randomly separate different fractions of all available gauges 0 1 0 2 0 8 of the gauges in a basin and remove the selected gauges from the model training phase this random selection and removal process is repeated 30 times for each fraction level then the entire precipitation time series at those locations are constructed by the linear interpolation of observations from the remaining gauges in that basin then rmse of ams estimates from both corrected persiann cdr and gauge interpolation are computed at the removed gauges fig 7 shows the average rmse values of ams estimates from the interpolation method blue line and the corrected persiann cdr red line for different exclusion ratios in each river basin the horizontal axis defines the number of the iteration and the vertical axis presents the average rmse value of the ams estimates on the excluded gauge locations using the corrected persiann cdr and gauge interpolation as the fraction of gauges being removed from the entire samples increases the errors associated with the interpolation method become larger in contrast the errors produced with our proposed correction method remain consistently low for different basins over most of the test scenarios i e different percentages of the gauge being removed moreover the interpolation based estimates result in large errors in some test scenarios and basins for example in the san joaquin river basin fig 7a substantial errors are observed in different scenarios and over several independent runs there are several reasons why errors from the interpolation method have large values in some of the iterations extreme precipitation events vary substantially in space and time the annual maximum precipitation at different points of a basin could be results of various extreme events occurring in different times of the year when interpolating the daily gauge observations in a region for constructing precipitation time series at an ungauged site heavy precipitation observed at one or more gauge locations could be falsely extended to the locations that were less impacted by the storm similarly by removing some of the gauges from the population the extreme events impacting those locations may not be represented in the interpolated time series from the remaining sample and as a result the extreme event at that location would be missed both of these cases may result in considerable errors in the annual maximum series estimated from the interpolation method other factors that contribute to the significant interpolation errors include long distance of sample gauges from the target locations substantial elevation differences between the target locations and sample gauges and the inability of the sample gauges to demonstrate the spatiotemporal variability of rainfall at target locations the overall errors from the corrected persiann cdr and the interpolation method at different exclusion ratios and basins are shown in fig 8 as the portion of gauges being left out increases the rmse produced by the interpolation method increases for all the basins while the proposed correction method for persiann cdr shows stable errors over different ratios and basins with respect to the ams results in the san joaquin river basin fig 8a the willamette river basin fig 8b and the colorado headwaters river basin fig 8d the corrected persiann cdr yields lower rmse values than the gauge interpolation method throughout different ratios suggesting the effectiveness of the proposed correction approach in the upper columbia river basin the gauge interpolation method results in better estimates of the ams at the ratios up to 30 however beyond the 30 threshold the corrected persiann cdr produces more accurate estimates of the ams therefore 30 of total gauges is the breakpoint for the upper columbia river basin in the context of interpolating point gauge information to spatial estimates by comparing the statistics of corrected persiann cdr and the traditional interpolation method it is observed that the proposed correction model generates more accurate estimates of the ams than does the linear interpolation method the superiority of the proposed bias correction method becomes increasingly evident as the gauges become sparser 4 5 validation on the continental u s in the previous sections we demonstrated the effectiveness and robustness of the proposed correction model on the four representative river basins in the western u s in this section we extensively validate the correction model on 16 additional river basins with different climates and topographic conditions across the continental united states table 1 the selected basins for validation cover all the climate classes available in the united states based on the köppen geiger climate classification system in addition these basins cover a broad range of elevations from low lying regions in the state of florida to high elevation regions in the state of utah these basins are also associated with various dominant precipitation mechanisms such as convective orographic and cyclonic which could influence the performance of the satellite based precipitation products hong et al 2007 liu and zipser 2009 table 4 presents the errors in ams estimates from the original and the corrected persiann cdr data on the tested river basins according to table 4 in 15 out of the 16 basins the correction model results in lower rmse values compared to the original persiann cdr data significant improvements are observed at high elevation regions such as dirty devil rio grande and upper yellowstone river basins with 78 7 72 3 71 6 reduction in the rmse of ams respectively also the correction model considerably decreases the errors in the ams estimates at mid elevation regions such as mississippi headwaters upper mississippi iowa and upper tennessee river basins table 4 among the low elevation regions nueces southwestern texas coastal and trinity river basins were quite successful with respect to the error reduction by the correction model however pascagoula river basin is less successful 7 3 decrease in rmse and south florida river basin fails to improve 29 7 increase in rmse both of these basins are located in the south atlantic gulf region which is characterized by warm convective precipitation mechanisms as a result of these convective systems satellite precipitation products often fail to provide accurate estimates at these regions as we see from the performance of raw data shown in table 4 both the pascagoula and the south florida river basins have high initial errors compared to the other river basins since the model is trained using the information from four river basins in the western united states with different hydroclimatic conditions it is reasonable for it to not perform as well under temperate and tropical climatic conditions and for extremes caused by warm convective systems generally satellite precipitation estimation algorithms perform poorly in estimating precipitation from shallow and warm convective clouds hong et al 2007 kubota et al 2009 liu and zipser 2009 sorooshian et al 2002 one reason behind this poor performance is that these algorithms relate heavy precipitations to deep convective clouds and subsequently underestimate heavy precipitations associated with shallow warm clouds hong et al 2007 liu and zipser 2009 moreover ir based methods such as persiann are based on cloud top temperature thresholds that are sometimes too cold for warm orographic clouds adler et al 2003 dinku et al 2008 finally due to the contamination by the cold anvil cirrus clouds ir based precipitation estimates typically display 1 3hr phase shift compared to the maximum diurnal precipitation these phase shifts influence the performance of ir based methods in regions dominated by warm convective clouds sorooshian et al 2002 4 6 multiday annual maximum series in fig 9 we present the scatterplots of the cf elevation for multi day duration ams at colorado headwaters fig 9a j as an illustrative example fig 9 a j suggest there is a similar cf elevation behavior in multi day ams analysis for different durations fig 9 k presents the exponential regression functions fitted to each of the n day maximum scatterplots according to fig 9 k as durations increase from 1 day to 60 days the original persiann cdr estimates of the ams become more accurate i e closer to the cf 1 line this is because the persiann cdr dataset is bias adjusted with gpcp dataset huffman et al 1997 at a monthly scale and the values from the two datasets become closer to each other at longer durations therefore at 30 days or 60 days analysis the ams estimates should be close to gauge observation although persiann cdr and gauge information are adjusted at a monthly scale the monthly coefficients are applied to daily estimates ashouri et al 2015 therefore the sub monthly or daily estimates may not be compatible with gauge observations at the corresponding scale furthermore the gpcp is a gauge interpolated dataset which its pixel values are essentially the average values of gauge observations within the large grid boundary however here we compare the persiann cdr estimates in pixel scale with the collocated gauge values which could differ substantially from the corresponding gpcp pixel values 4 7 depth duration frequency curves fig 10 shows the ddf curves derived from the adjusted persiann cdr data the frequency estimates from noaa atlas 14 and the 90 confidence intervals for a gauge location in dirty devil basin in the state of utah usc00420849 we present return levels for daily and multi day durations given the daily resolution of the persiann cdr dataset as shown in fig 10 the frequency estimates from the original persiann cdr data are outside the 90 confidence intervals of the noaa atlas 14 which suggests the necessity of bias adjustment prior to employing the data for frequency analysis on the other hand ddf curves from the adjusted persiann cdr data are well within the 90 confidence intervals of the noaa atlas 14 ddf curves in most cases the frequency estimates from the adjusted persiann cdr data are very close to the noaa atlas 14 estimates which are calculated by incorporating a large number of gauges and longer records of data for frequency estimation larger deviations from the gauge based estimates are observed at longer return periods and there is no clear trend in terms of overestimation or underestimation with respect to duration as shown in fig 10 the confidence intervals from the gauge based and satellite based ddf etimates become larger as the return periods increase this higher uncertainty is because of the lower sample size at the tails of the distributions furthermore the confidence intervals from the original and the adjusted persiann cdr datasets are relatively comparable given the similar lengths of the two datasets however the uncertainty bounds from the satellite based ddf estimates are larger than those from the noaa atlas 14 one reason behind these larger confidence intervals is the shortness of the persiann cdr dataset when compared to the gauge information used for the development of noaa atlas 14 ddf curves another reason is the difference between the frequency analysis method implemented here and the method employed in the development of noaa atlas 14 noaa uses the regional frequency analysis based on l moments to estimate the frequency and intensity of extremes hosking and wallis 2005 the regional frequency analysis method is used in atlas 14 in order to relieve the uncertainties arising from a low sample size limited years of observations during the gev parameter estimation process although the regional frequency analysis method gives frequency estimates with lower uncertainties it comes with the assumption of regional homogeneity in extreme rainfall characteristics which is not always a valid assumption here the frequency analysis methods and uncertainties of the frequency estimates are outside of the scope of this study and the ddf curves and their error analyses are the proof of concept as seen in fig 10 ddf curves from the original persiann cdr suggest underestimation of the extreme precipitation quantiles for different durations this is expected given the spatial resolution of this dataset in fact when considering the remotely sensed precipitation information we should be aware that the pixel value represents a spatial average of precipitation within the extent of a pixel in other words the pixel value disregards the subpixel variability and even if the persiann cdr estimate at a pixel is completely accurate its value tends to be smaller than the collocated point measurements as a result the extracted ddf curves from a satellite pixel tend to demonstrate lower return levels peleg et al 2018b as observed in fig 10 it is worth noting that the estimated ddf curves from the persiann cdr data are not necessarily based on the liquid phase precipitations and the extracted ams may comprise snowfalls as well this is because persiann cdr and many other satellite based precipitation estimation algorithms do not distinguish between precipitation phases in other words the annual maximum time series extracted and used here may contain solid phase precipitation extremes due to snowfalls although this study does not differentiate between solid and liquid phases of precipitation in order to obtain purely rain based ddf curves the current framework can be further modified to incorporate additional observations on solid precipitations there are two approaches to achieve this goal one approach for this would be to limit the analysis to warm seasons but the definitions of warm season vary among various geographic locations another approach would be to distinguish snowfall from rainfall but this would require snowfall and air temperature data that are not available everywhere future independent research may improve upon the current study by including such additional information fig 11 displays the box plots of rmse of the return level estimates from the original and corrected cdr for different durations and return periods at collocated persiann cdr pixels and gauges for different basins in the continental us the corrected persiann cdr data was used to obtain frequency estimates at different gauge locations in the selected basins and the results were compared with those from noaa atlas 14 note that three out of the 16 basins basins 5 6 and 15 were located in the pacific northwest region and two basins basins 8 and 14 were located in the state of texas all of which were not covered by or were being updated in the recent volumes of noaa atlas 14 thus the frequency estimates were only validated at the remaining 11 basins as shown in fig 11 according to fig 11 the rmse values for return level estimates corresponding to longer return periods are generally higher for both datasets this is expected as the persiann cdr dataset is relatively shorter than the gauge information used for the development of noaa atlas 14 the shorter record will result in smaller samples higher uncertainties and larger deviations at the tails of the distribution over the tested basins the frequency estimates from the corrected persiann cdr data have consistently lower median rmse values than those from the original persiann cdr at different return periods the rmse values at the basins with higher elevations such as central nevada or dirty devil basins were relatively lower than these at basins with lower elevations such as mississippi headwaters or upper mississippi iowa basins which implies the suitability of the correction approach for high elevation regions furthermore in most of the basins and at different return periods the corrected dataset shows lower variability in rmse of the frequency estimates corrected persiann cdr data also demonstrate superior performance in terms of median rmse and variability of rmse values at the gauges within the basins the only case for which the corrected persiann cdr results in higher rmse values at different return periods and durations is the south florida basin where it was previously shown that the correction model does not improve the ams estimates due to the climate and the precipitation mechanism the relative errors of the frequency estimates are calculated to show the relative magnitude of the return level errors compared to the return levels from noaa atlas 14 the relative error here is the difference between the frequency estimates from persiann cdr original and bias adjusted and noaa atlas 14 divided by the value from noaa atlas 14 fig 12 demonstrates the absolute value of the relative error for the frequency estimates at different durations and return periods from the corrected and original persiann cdr data as shown the relative errors from the corrected persiann cdr data have consistently lower median values as well as lower variability at different return periods in the tested basins the median relative errors from the corrected data are less than 20 different from the return levels estimated by noaa atlas 14 similar performance is observed when the relative errors of frequency estimates from the two datasets are compared with respect to the extreme precipitation duration it is also noted that the corrected persiann cdr dataset does not show a systematic increase or decrease in the relative errors of the frequency estimates with respect to the duration the relative errors of the return level estimates from the original persiann cdr data tend to decrease with increasing duration this finding is consistent with our observations in fig 9 that revealed lower errors of the original persiann cdr data for longer duration extreme events as with fig 11 the only case in which the corrected data resulted in higher rmse values was the south florida basin where the correction model did not improve the ams estimates section 4 5 5 summary and conclusions in this study the application of the persiann cdr dataset for rainfall frequency analysis was investigated a bias correction model was developed to further correct the persiann cdr estimates of annual maximum time series at the pixel scale the proposed correction approach was implemented in two steps 1 bias correction factors at limited gauge locations were estimated using linear regression analysis between annual maximum series ams of gauges and collocated pixels and 2 the correction factors from the limited gauge locations were extended to other regions where gauge data were not available the correction model was validated at 16 basins in the continental united states covering various climates and elevations finally depth duration frequency ddf curves were constructed by fitting the generalized extreme value distribution to the ams from the corrected data and estimating the quantiles of extreme precipitations below is a summary of our main findings 1 the proposed bias correction approach has been demonstrated effective and robust in improving the accuracy of a remote sensing precipitation estimation product i e persiann cdr especially in high elevation river basins where gauge or radar networks are either limited or non existent 2 the hold out cross validation results indicated that the proposed bias correction model is capable of improving the ams estimated by the persiann cdr dataset even in the case that limited gauge information was provided for the model calibration and the approach is generalizable to other locations with similar climates and elevations 3 as shown by the leave one out cross validation the bias adjusted persiann cdr gave better estimates of the ams for the ungauged sites at a majority of the basins even though these basins had dense gauge networks 4 results from the k fold cross validation method suggested that the persiann cdr data bias corrected with the proposed correction approach performs consistently better than the gauge interpolation method in estimating the ams at a majority of regions with limited gauge observations it was observed that the gauge interpolation may sometimes result in significant errors in ams estimates especially in regions with complex topography 5 the validation results over 16 basins across different climates and elevations indicated that the proposed correction method improves the persiann cdr estimates of ams especially in high elevation regions 6 the bias adjusted persiann cdr is further applied to derive the return levels for different return periods and durations the frequency estimates from the corrected persiann cdr data are compared with those from the original persiann cdr and noaa atlas 14 results revealed that the frequency estimates from the corrected dataset are consistently closer to the estimates from noaa atlas 14 they also lie within the uncertainty bounds of noaa atlas 14 thus the persiann cdr dataset has the potential for being used in rainfall frequency analysis for the regions with limited ground based observations however despite the promising results there are still some limitations in this dataset and the proposed correction method for the application of frequency analysis one of these limitations is the temporal resolution of the persiann cdr dataset the daily temporal resolution limits the investigation of extreme events with shorter durations e g 3 hourly or hourly another limitation is that the frequency analysis here is conducted at the pixel scale using relatively limited samples a sample of 33 annual maximum values is relatively limited for fitting a 3 parameter distribution this would result in high uncertainties in estimating the parameters of the distribution and the return levels one remedy to the sample size problem could be the application of regional frequency analysis methods to increase the sample size by incorporating information from the nearby locations with the same climatic conditions it is also important to note that given the rising global temperatures rainfall intensities especially at shorter durations are expected to increase therefore the increase in the global temperature could be used as an added factor to adjust historical design rainfall intensities for the warmer temperatures that lie ahead peleg et al 2018a this work is part of an ongoing research and the presented approaches and results are intended as a proof of concept future research in this area may involve bringing non stationarities into the bias adjustment framework tao et al 2018 including covariates into the bias adjustment framework which requires advanced optimization techniques yang et al 2017 investigating the hydrological modeling applications of the corrected persiann cdr data and developing ddf curves for ungauged regions or areas not included in the current noaa atlas 14 acknowledgments the financial support of this research is from u s department of energy doe prime award de ia0000018 california energy commission cec award 300 15 005 maseeh fellowship nsf cybersees project award ccf 1331915 noaa nesdis ncdc prime award na09nes4400006 and ncsu cics and subaward 2009 1380 01 the u s army research office award w911nf 11 1 0422 and the national key r d program of china grant no 2016yfe0102400 finally the authors sincerely thank the editor the associate editor and the anonymous reviewers for their valuable comments and suggestions 
7131,different aspects of management policies for shallow geothermal systems are currently under development although this technology has been used for a long time doubts and concerns have been raised in the last years due to the massive implementation of new systems to assess possible environmental impacts and manage subsurface energy resources collecting data from operating shallow geothermal systems is becoming mandatory in europe this study presents novel advances in the upscaling of operation datasets obtained from open loop geothermal energy systems for an optimal integration in hydrogeological models the proposed procedure allows efficient numerical simulations to be performed at an urban scale specifically this work proposes a novel methodology to optimize the data treatment of highly transient real exploitation regimes by integrating energy transfer in the environment to reduce more than 90 registered raw datasets the proposed methodology is then applied to and validated on five different real optimization scenarios in which upscaling transformation of the injection temperature series of 15 min sampling frequency has been considered the error derived from each approach was evaluated and compared for validation purposes the results obtained from the upscaling procedures have proven the usefulness and transferability of the proposed method for achieving daily time functions to efficiently reproduce the exploitation regimes of these systems with an acceptable error in a sustainable resource management framework keywords shallow geothermal energy gwhp urban hydrogeology thermal management groundwater thermal impact 1 introduction shallow geothermal systems are based on obtaining the heat energy from materials of the most superficial layers 250 m of the earth s crust and the water that flows through them the heat transfer from the earth s core to the outer areas of the crust and the capacity of the ground to dampen thermal oscillations occurring on the surface make thermal stability possible starting at a depth of approximately 15 m after the damping of thermal oscillations with depth the ground temperature is similar to the annual average temperature of the region plus 1 c or 2 c parsons 1970 this terrain feature justifies the development of these important systems as an adaptative measure to climate change for renewable energy development bayer et al 2012 the potential natural state of the aquifer is defined as a state without anthropogenic influences epting and huggenberger 2013 heat exchange with the ground can be performed by different types of ground source energy gse systems including closed or open systems by using heat pumps coupled with heat exchangers open systems also called groundwater heat pumps gwhps take direct advantage of the heat or cold of pumped groundwater and subsequently reinject pumped water into the aquifer garcía gil et al 2014a gwhps and gse in general are widely used worldwide and their demand is expected to increase in the next years epting et al 2017 jaudin 2013 lund and boyd 2016 the increasing trend of gwhps has resulted in an additional heat load in urban aquifers caused both by thermal interference between systems and interference between well doublets that is the thermal autointerference effect galgaro and cultrera 2013 garrido et al 2010b because the geothermal exploitation of the aquifer is not consumptive thermal pollution derived from these systems is the main impact on the aquifer lo russo et al 2014 the problems associated to the implementation of gwhps originate both from the lack of energy sustainability of the facilities due to thermal interference events garrido et al 2016 and from an insufficient legal framework the required regulatory frameworks to ensure sustainable exploitation of aquifer heat energy resources particularly beneath cities where there may be overlapping and conflicting demands on the resource are still being developed therefore generating great uncertainty for users garcía gil et al 2015 one of the possible ways to enhance the management of geothermal systems is using numerical models to simulate the thermal regime in the urban aquifer and to establish effective management strategies banks 2009 this approach allows considering the high complexity of these facilities the heterogeneity of the medium and the temporal variability of their operations in an integrated way on the other hand these methods have two limitations that is the high volume of data and the time requirements garcía gil et al 2014a the modelling process of gwhp systems was carried out using data from specific discrete measurements in previous studies epting et al 2013 gropius 2010 herbert et al 2013 this simplification was applied as an appropriate approximation considering the inherent difficulties in the monitoring of such complex installations and the consequent lack of exploitation regime datasets currently regulators are increasingly requiring monitoring data on the operation of gwhp systems these systems typically operate following a design power and thus as building demand varies the schemes may operate continuously or rather intermittently particularly the system may have a rather short operational cycling period at locations where a scheme delivers a high proportion of the total heating or cooling demand this leads to the need for very high frequency monitoring data to characterise the system s operation in contrast numerical modelling becomes computationally expensive and time consuming if this short cycling detail is to be represented explicitly the main purpose of this study is to develop and validate a methodology to obtain as optimally and efficiently as possible a subset of maximal representative data which can be easily implemented in numerical heat transport models that is to obtain data subsets for numerical models resulting in minimum deviations when compared with original data validation of the proposed methodology under standard hydrogeological parameters is carried out to ensure its transferability to other aquifers operated by gwhps this objective achievement will constitute an improvement in attaining a scientific based management tool which allows the reproduction of real exploitation regimes and therefore the aquifer response to intensive shallow geothermal exploitation a requirement in obtaining a global vision necessary for aquifer management accomplishing this objective will help to understand the hydrodynamics and existing heat transport processes beneath urban environments thus contributing to the improvement of sustainable management of shallow geothermal resources epting et al 2013 spitler 2005 to reach this goal different upscaling techniques were considered in this work to transform high resolution datasets obtained from high frequency data logging into lower frequency data subsets bierkens et al 2000 finke et al 2002 the upscaling procedure has been widely used and developed in applied research typical in environmental science where specific questions raised by society s decision makers policy scale and observation scale are not met the scale transfer procedure or upscaling has been classified by bierkens et al 2000 depending on the involvement or non involvement of a model in the research cycle the possible linear relationship between the model and input variables and parameters the applicability of the model to different locations time steps the form of the model at different scales or the possibility of deriving analytically a different scale model the major classes of upscaling methods consist of averaging the observations or output variables brus and de gruijter 1997 viscarra rossel et al 2016 finding representative parameters or input variables dagan 1981 wu et al 2006 averaging the model equations bedrikovetsky 2008 whitaker 1986 and performing a model simplification de vries et al 1998 vogler et al 2018 the upscaling procedure applied in this paper corresponds to the methods finding representative parameters since the input variables of the model involved are non linear so it cannot be applied at many time steps and the model has the same form at the two scales involved bierkens et al 2000 furthermore since it is not possible to obtain output variables at the larger scale and input data at the source scale is exhaustive deterministic methods have been applied in accordance with previous studies bierkens and van der gaast 1998 johannes dolman and blyth 1997 yu et al 2016 finally a sensitivity analysis was performed by numerically modelling different upscale scenarios to optimize the procedure and to perform and conduct an error assessment 2 study site the present work was carried out within the framework of the urban alluvial aquifer of zaragoza fig 1 located in the central sector of the ebro river basin spain this basin constitutes the last stage in the evolution of the southern foreland basin of the pyrenees barnolas and robador 1991 pardo et al 2004 the portion of the aquifer covering the urban area of the city is known as urban alluvial aquifer of zaragoza garrido et al 2006 2010a b due to the singular hydrogeological properties of environment and groundwater use among which geothermal exploitation is the most important the ebro river crosses the city in nw se direction where two of its tributaries the huerva and gállego rivers converge the urban alluvial aquifer consists of early pleistocene fluvial alluvial and aeolian sediments generated by wide alluvial plains over an evaporite dominated miocene substratum quirantes 1978 simon et al 2014 the surface of the alluvial aquifer under the city is 632 km2 and its thickness ranges from 5 to 60 m with a considerable spatial variability due to tectonic gravitational diapiric and dissolution processes affecting this detrital cover sánchez navarro et al 2004 soriano et al 2012 the transmissivities were derived from previous flow model calibrations garrido et al 2006 this groundwater numerical model developed for the alluvial aquifer of zaragoza recreated the transient regime of groundwater flow considering a cauchy boundary condition for the reproduction of the ebro river hydraulic connection this boundary condition is imposed according to a time function providing the river head corresponding to every time step hence the flood events are considered in the model the western and eastern boundary conditions were chosen as prescribed head or dirichlet corresponding to isohypses according to head data measured in the field of the respective zones the rest of the unassigned boundaries of the model were defined as neumann boundary conditions with zero flux which represent the lateral contact between the aquifer and the tertiary bedrock the results of the hydraulic test are in the range of 3 102 4 103 m2 day 1 garcía gil et al 2014b moreno 2008 the depth of the phreatic surface ranges from 7 to 34 m and the main groundwater flow pattern is nw se in the southeastern part of the urban aquifer and w e in the northwestern part or at the left margin of the ebro river garcía gil et al 2015 a total of 188 wells exploit the urban alluvial aquifer for geothermal energy production 112 extraction wells and 76 injection wells a registry of 73 geothermic installations 65 in use is currently gathered in an exploitation catalogue the pumped groundwater in 2010 was estimated to be approximately 24 106 m3 with only 0 93 106 m3 being consumptive the remaining withdrawal is associated with geothermal exploitation by means of gwhps garrido et al 2010b the total heat power installed has been estimated to be 110 mwt of energy used for cooling purposes with 21 installations being equipped with reverse cycle heat pumps with 34 mwt of the total heat power supplied garrido et al 2012a b the thermal groundwater regime of the city and the extension of the heat plumes generated by gwhps were reproduced by numerical modelling of the groundwater flow and heat transport garcía gil et al 2014a 2015 a total of five gwhp geothermal exploitations were selected as study objects in the present paper corresponding to three hotels a museum and a hospital in zaragoza the selection was carried out based on different aspects attempting to obtain the largest level of representativeness the oldest exploitation which started in 1999 is sgs 4 and the most recent ones are sgs 1 and sgs 2 which started in 2010 all geothermal exploitations have two extraction wells and one injection well except for sgs 5 which has four extraction and three injection wells the average depth below ground of these wells ranges between 24 and 44 m the average flow rate circulation of the installations sgs 1 to sgs 5 is 17 62 11 4 16 6 19 16 and 50 l s 1 respectively all studied installations have a climatization system that is used both for cooling and heating with an average heat power of 0 9 1 0 1 9 1 8 and 2 6 mw for sgs 1 to sgs 5 respectively 3 methodology 3 1 origin and data treatment by filtering synchronization and homogenization the monitoring of the regime of geothermal system exploitations is carried out by the owners by request of the local water management administrator che spanish acronym later datasets are transferred to the geological survey of spain igme spanish acronym for evaluation the data logging consists of flow l s 1 and temperature c measurements at both extraction and injection wells with a 15 min sampling frequency the data registry shows a high variability between the gwhp installations due to their different exploitation regimes data filtering and synchronization was performed based on a 15 min sampling frequency reference guide to obtain continuous and homogeneous data time series in addition temperatures registered while the operation wells were inactive were discarded because the recorded values are not representative 3 2 application of different upscaling approximations the upscaling problem posed in the introduction consists in finding representative input variables at a given scale e g daily scale for groundwater flow and heat transport modelling obtained from an exhaustive dataset at a smaller scale e g 15 min scale to reproduce adequately the operation of gwhp systems it has to be stressed that the term scale denoted on maps means exactly the opposite to the scale definition used in upscaling and downscaling methods for environmental research the operation of gwhps is commonly implemented in a groundwater flow and heat transport model by assigning a prescribed water input output rate and a prescribed temperature for each time step to the nodes of the model representing operation wells pumping injection rate non consumptive is an additive also called extensive variable that is proportional to the size of the sample this feature makes the upscaling of pumping injection rates trivial in contrast defining a representative injection temperature becomes non trivial temperature is an intensive variable therefore it is not proportional to the sample size to derive representative input injection temperatures theoretical expressions or experience have to be considered in this study different upscaling formulas have been used including daily quartiles q1 q2 and q3 geometric mean and daily maximum and minimum values of 15 minutal injection temperature datasets measured from injection wells also a daily random registered injection temperature value was considered to evaluate the effect of a random approach on upscaling these upscaling approaches allowed covering the high variance of injection temperature values providing a basic simple statistic approach and laying the base framework for comparison of more complex approaches on the other hand in this work we propose the integration of transferred energy by gwhp systems calculated at sampling frequency scale to obtain a representative equivalent temperature teq at a larger scale adequate for efficient numerical modelling the proposed calculation of this representative temperature is described by the following steps described hereafter first the energy transferred h m into the aquifer in the m time period of the sampling frequency is calculated by the following expression 1 h m q m c w ρ w t im t cm where q m is the pumping rate m3 s 1 measured every m time period c w is the specific heat capacity of water j kg 1 k 1 ρ w is the water density kg m 3 t i m t c m is the increment of the temperature between the injection temperature t i m and the extraction temperature t c m during two measurements of m sampling period interval in this research min g i v i n g t h e e n e r g y t r a n s f e r r e d w i t h t h e a q u i f e r w i t h a min giving the energy transferred with the aquifer with a 15 min resolution the energy transferred h n into the aquifer in n period required in a numerical model time step is given by 2 h n 1 n h m 1 n q m c w ρ w t eq t cn where 1 n q m q m 1 q m 2 q m 3 q mn and 1 n h m h m 1 h m 2 h m 3 h mn in this paper n 96 which is the number of measurements to obtain daily n 1 day integrated input values for the model the equivalent temperature t eq represents the temperature value of injection in the model in order to transfer the energy transferred in to the aquifer throughout the n period 1 day in this case the term t cn stands for average captation temperature defined as 3 t cn 1 n 1 n t cm by rearranging eq 2 the following upscaling formula for equivalent temperature teq is obtained 4 t eq 1 n h m 1 n q m c w ρ w t cn all the aforementioned upscaling approaches were used to calculate daily time series for each of five gwhps selected section 2 and were subsequently implemented in a numerical groundwater flow and heat transport model for validation 3 3 sensitivity analysis of the upscaling process as a function of the time resolution to assess the upscaling procedure using the calculated equivalent temperature different temporal resolutions were considered δ t three time series considering half daily two daily and five daily sampling frequency were performed to evaluate the deviation in the evolution of each scenario against 15 min measurement time series adopted as realistic control time series furthermore this analysis intended to explore the possibilities of working with lower sampling frequencies to correctly reproduce the exploitation regimes of the installations and quantitatively check the different options available 3 4 numerical groundwater flow and heat transport models the feflow code diersch 2013 was used to reproduce 3d groundwater flow and heat transport processes induced by five gwhp systems sgs 1 sgs 2 sgs 3 sgs 4 and sgs 5 fig 2 in the alluvial aquifer studied the installations were selected based on different criteria attempting to cover all existent ranges of hydraulic gradient permeability and exploitation regimes therefore they are considered representative of the general operation of shallow geothermal systems located in the study area a total of five local hydrogeological models corresponding to the gwhp installations were implemented based on a previous calibrated and validated 3d regional groundwater flow and heat transport model covering the urban area of zaragoza epting et al 2017 garcia gil et al 2014 the modelled domain for each hydrogeological model analysed considered a 600 m groundwater flux tube to prevent border effects with its geometry being obtained from a city scale 3d regional model epting et al 2017 a 40 m thick miocene basement and unsaturated zone nsz were considered for downward and upward heat conduction respectively the domain characteristics of each installation model reflect different dimensions and mesh refinements depending on the number and spatial distribution of the gwhp operation wells each domain was discretized into an unstructured finite element mesh with triangular elements table 1 all of which were distributed in 12 14 layers depending on the model domain fig 3 a fixed head boundary condition was imposed to the incoming and outlet groundwater flow nodes of the flux tube considered to reproduce the groundwater flow with hydraulic gradients of 1 35 10 4 8 60 10 4 4 78 10 3 7 27 10 4 and 1 04 10 2 for installations sgs 1 to sgs 5 respectively a fixed ground temperature of 17 c was assigned to the left groundwater inlet boundary the thermal and hydraulic parameterizations of the used models are listed in table 2 the boundary conditions defined for geothermal operation wells consisted of a transient prescribed flux and a transient prescribed temperature boundary condition according to the time series measured and calculated by upscaling procedures an initial temperature of 17 c was chosen approximately matching the annual mean of the region a set of observation points at different distances from the injection wells 1 3 5 10 20 40 100 and 200 m was defined in all models used the observation points allowed to register the temperature evolution as a heat transport state variable for each upscaling optimization scenario and to identify which calculated time series generated the closest thermal response to the real scenario i e 15 min sampling frequency the simulations were carried out under a transitory regime with a 15 min and one day time step for the sensitivity analysis of the proposed upscaling procedure a time step of 0 5 2 and 5 days was adopted in two gwhp system models sgs 1 and sgs 2 3 5 calculation of the upscaling procedure error the assessment of the deviation due to the proposed upscaling procedures was carried out by comparing the reference scenario 15 min measurements with the calculated upscaling scenarios the comparison was conducted using thermographs obtained from all defined observation points the temperature difference was obtained by subtracting the reference scenario from each optimization scenario to calculate the deviation for all time steps the deviation was calculated as root mean square error rmse using the following equation 5 rmse i 1 n d 1 2 n where d is the temperature difference between the optimization and reference scenarios and n is the total number of existing values in each considered set the rmse values approaching 0 were considered to reflect a high optimization degree of the upscaling procedure during the error analysis performed special attention was focused on the possible effects of the hydrogeological parameters on the rmse distribution 4 results 4 1 results for the exploitation regime and upscaling procedures the time series including the upscaling procedures proposed for each installation were calculated with the same approximation methods but with different temporal resolutions fig 4 a shows the analysis carried out for installation sgs 1 where the 15 min sampling frequency series of the injection temperature was considered to be the real exploitation regime the registry period includes 273 days with a mean flow of 661 m3 day the registry is continuously operating except for minor interruption intervals the graphic shows that 55 of the time was utilized for cooling thus causing a high temperature injection 25 c and the rest of the time was used for heating leading to cold water injection 15 c the calculated upscaling series of the injection temperature for the gwhp system sgs 1 geometric mean q1 q2 q3 maximum minimum random value and equivalent temperature for daily sampling frequency is shown in fig 4b c regarding optimization minimum data reduction of 98 9 was achieved through upscaling transformation of daily scenarios table 3 this generated a reduction in the computational time of 10 6 h on average table 4 4 2 results for the simulation of upscaling scenarios the temperature distribution obtained from the reference time series at the end of the simulations is shown in fig 5 the influence of thermal dispersion and its interaction with established observation points can be deduced based on the observation of heat plumes for the last calculated time the heat plume generated by installation sgs 1 fig 5a extends to a distance of 40 m with a temperature increase of 1 75 c with respect to the initial situation 18 75 c isotherm fig 5b shows the thermal impact of installation sgs 2 on the aquifer generating a larger heat plume than that of installation sgs 1 23 79 c isotherm 40 m from the observation point with a temperature increment of 3 84 c at a distance of 100 m from the observation point 20 84 c isotherm installation sgs 3 generated the largest plume fig 5c with the groundwater temperature at the 40 m observation point increasing to an isotherm of 24 31 c and at the furthest observation point of 200 m increasing by 3 65 c 20 65 c isotherm the thermal impact caused by installation sgs 4 is shown in fig 5d installation sgs 4 generated a heat plume with a temperature increment of 3 55 c at a distance of 40 m 20 55 c isotherm the temperature increased by only 0 82 c 17 82 c isotherm at a distance of 100 m the temperature effect is almost negligible at this observation point the smallest heat plume was generated by installation sgs 5 fig 5e with an observed temperature increment of only 0 43 c 17 43 c isotherm at a 40 m distance from the injection point in addition the thermographs obtained from reference scenario simulations are shown in fig 6 for all gwhp systems the temperature evolution of the reference scenarios and each upscaling procedure for all time steps simulated at 20 m from the injection point for each gwhp installation are shown in fig 7 all other distances are shown for sgs 3 as supplementary material fig s1 the temperature evolution for each of the upscaling procedures and their difference with respect to the measured reference values can be observed in these thermographs the upscaling approaches that cause the largest error are associated with scenarios in which maximum and minimum values were incorporated deviating up to 5 c from the real measured temperature scenario in some exploitations average deviation of 3 c the same phenomenon occurs when using q1 and q3 although the deviation is smaller average deviation of 1 8 c it does not follow the real regime of the studied installations regarding the data series of the geometric mean q2 and random value the deviation decreases to an average value of 1 c the equivalent calculated temperature scenario causes the least deviation 0 4 c with respect to the reference scenarios for all studied installations 4 3 upscaling procedure error calculation the distribution of upscaling errors rmse for each of the gwhp installations at all distances evaluated can be observed in fig 8 the largest upscaling error occurs when using the minimum value upscaling approach resulting in a 1 c to 5 5 c error the deviation of the maximum value approach ranges from 1 2 c to 3 5 c and that of q1 and q3 approaches results in an error of approximately 2 c the q2 geometric mean and random value approaches show a similar deviation of 1 5 c the equivalent calculated temperature is the only upscaling procedure with a deviation below 0 8 c for all gwhp installations in addition the average rmse values of all installations for all upscaling procedures are presented in fig 8 the results show that the minimum value upscaling approach produces the largest error ranging from 0 8 c to 4 6 c in contrast scenarios using the equivalent calculated temperature approach present the lowest error range 0 3 0 5 c 4 4 error calculation for the upscaling procedures as a function of the time resolution the error rmse associated with the use of the equivalent temperature series carried out in all studied installations with daily temporal frequency was calculated along different observation points fig 9 in addition the rmse values derived from the equivalent temperature upscaling approach for all observation points established in the numerical models sgs 1 and sgs 2 with different temporal frequency half daily two daily and five daily are shown in fig 10 a high deviation was observed for the five daily sampling frequency scenario for both installations with a deviation of 2 1 c and 2 5 c respectively at the nearest observation point 1 m from the injection well the two daily sampling frequency scenario caused a deviation similar to that of the daily sampling frequency scenario with rmse values of 1 6 c and 1 9 c respectively the daily and half daily sampling frequency scenarios presented a similar average deviation of 1 5 c on the other hand if observation points located near the injection well 1 3 and 5 m are ignored the deviation for both installations and each of the upscaling scenarios decreases to approximately 1 c for half daily and daily resolution scenarios a maximum deviation of 0 25 c was obtained at the first 10 m observation point 5 discussion the equivalent temperature upscaling approach calculated by the integration of the daily dissipated heat power by a gwhp installation is considered to provide the highest reliability because it is derived from the real operation regime of the installation using the equivalent temperature approach scenarios the time series half daily two daily and five daily sampling frequency were obtained for two of the studied installations where the simulation time was almost the same as that of the previous case daily sampling frequency approach because the exploitation regime of geothermal installations does not follow a common pattern each one was analysed individually in general sgs 1 sgs 3 and sgs 5 had a continuous operation regime with the highest average flows 661 865 and 892 m3 day respectively except for time intervals during which they did not inject water on the contrary installations sgs 2 and sgs 4 developed a variable regime with lower flows 457 and 265 m3 day and prolonged intervals in which they were off the climatization demand is only for cooling purposes in sgs 3 and sgs 4 a fact that differentiates them from the rest of the installations which have a combined use for both heating and cooling purposes the plumes that reached the largest thermic influence area 100 and 200 m were generated by installations sgs 2 and sgs 3 recording the highest injection temperatures 30 c during summertime installation sgs 4 generated a smaller plume due to the injection of water at a lower temperature 30 c and its variable regime remaining off during large periods of time the smallest measured plumes were generated by installations sgs 1 and sgs 5 derived from a combination of cooling and heating also they presented lower injection temperatures compared to those of the other exploitations 30 c based on the numerical model simulations of the different scenarios the error derived from the upscaling approximations of daily sampling frequency against high real sampling frequency data was quantified the temperatures generated along observation points allowed the deduction of the deviation evolution daily approximations calculated based on the minimum and maximum caused the widest average deviation range varying from 0 8 c to 4 6 c and from 1 2 c to 3 2 c respectively the rest of the statistic parameters had a global average standard deviation of 0 73 c the smallest deviation corresponded to the equivalent temperature scenario and ranged between 0 3 c and 0 5 c this means that the calculated temperature is the best and most useful upscaling approximation because it is the only parameter that generated a deviation range below 0 8 c among the analysed installations on the other hand the approximations carried out with different temporal resolutions using the equivalent temperature upscaling approach in two of the analysed installations proved that both half daily and daily scenarios considering all observation points generated the lowest deviation values thus reducing the established temporal intervals is not particularly useful furthermore ignoring observation points near the injection wells 1 3 and 5 m led to a 1 c reduction in the error range the maximum deviation was observed at the observation point at 10 m distance 0 25 c this is due to the fact that the points near the gwhp operation wells are affected by hourly thermal exploitation regimes thus indicating that a daily or even higher sampling frequency simulation is necessary to reproduce a thermal regime with a reasonable error at points close to the injection well considering that the evaluation of thermal impacts close to the injection wells of gwhp systems is not the aim of regional scale models and that the mean minimum distance of gwhp wells to the monitoring points for zaragoza is for all cases greater than 20 m the errors derived from the upscaling procedure proposed at the first 20 m from the injection well could be neglected based on the research carried out in this paper the obtained results are of interest to obtain time functions of daily sampling frequency even when their operation regime is highly variable thus enabling the implementation of these exploitation regimes in regional models at the city scale furthermore the validation of the proposed methodology based on standard hydrogeological parameters ensures its transferability to other equivalent hydrogeological environments that is shallow alluvial aquifers which are prevailing in urban environments the error analysis performed on the upscaling procedures has not highlighted any relationship between the hydrogeological parameters and the errors observed this indicates that the upscaling procedure proposed is significantly independent from the hydrogeological parameters in the parameter ranges evaluated in this study furthermore when applying the results found to other hydrogeologically different city aquifers it has to be noted that the optimal upscaling approach might be influenced by the relevance of advection versus conduction heat transport mechanisms given by the peclet number domenico and schwartz 1998 which relates the energy transported by fluid motion to the energy transported by conduction in the cases evaluated heat advection is greater than heat conduction which is a situation likely found in alluvial aquifers situations where transmissivities and hydraulic gradients are lower than those considered in this work are not suitable for the application of the conclusions found in this research conversely although the equivalent temperature upscaling procedure has been shown as the best approach to reproduce the thermal pollution signal of a gwhp other approaches might be of interest for authorities when considering the maximum impact produced by a gwhp in terms of regulatory requirements or derogation of other users from the resource regulators might consider using daily maximum temperature as a conservative approach to integrate the high frequency sampling data this approach would tend to overestimate thermal plumes generated but would be safer in case of future peak conflicts gwhp systems are likely to present similar temporal demand profiles but their interpolation to longer times might get to losing peak demands this perspective indicates that the equivalent temperature proposed does not have to be necessarily used in numerical models for the assessment of thermal impacts required for thermal discharge authorizations 6 conclusions the exploitation regime of geothermal systems presents a highly variable pattern that increases the difficulty of processing data from such installations prior to their implementation in regional scale groundwater flow and heat transport models monitoring of shallow geothermal systems is necessary and should be mandatory to guarantee the long term exploitation of the renewable energy resources and the aquifer sustainability the research performed in this paper revealed the difficulties associated with the necessary treatment to study the exploitation regime of gwhp installations based on daily approximations calculated by upscaling of the injection temperature a data reduction of 98 9 was obtained this makes an adequate implementation of gwhp systems in hydrogeological models possible thus making them more efficient because a large amount of simulation time is saved accounting for a decrease from an average duration of 10 h to 4 min the upscaling scenarios calculated by descriptive statistic parameters geometric mean q1 q2 q3 maximum and minimum together with a random approach and the equivalent injection temperature proposed in this paper allowed to obtain a global approximation to the variability range of the injection temperatures and to calculate the deviation for each scenario by making use of the proposed novel upscaling procedure proposed here to calculate a daily equivalent temperature from the heat power transferred by real groundwater heat pump installations with both daily and half daily sampling frequencies it is possible to reduce significantly the amount of data required to optimize its implementation in complex numerical models at an urban scale the methodology has been validated on five different real scenarios covering the most standard hydrogeological conditions in shallow alluvial aquifers to ensure its transferability the most relevant conclusions of this study with respect to numerical models of the heat regime in urban aquifers with shallow geothermal installations are the following recording temperature and pumping rates with a 15 min sampling frequency or higher is convenient to estimate the thermal power dissipated by shallow geothermal exploitations due to the potentially high variation of the exploitation regime over time the upscaling procedure that best fits real operations of geothermal systems is the calculation of the equivalent injection temperature by integrating the daily dissipated energy regional scale numerical modelling of all studied exploitation regimes can be efficiently implemented throughout daily resolution time series data the error analysis indicates that the error values of the calculated thermographs are lower than 0 4 c 0 1 c using the proposed upscaling procedure presented in this work these results add additional uncertainty to the calibration process of numerical models and therefore should be considered when estimating thermal parameters in conclusion the present paper depicts an advance in the implementation process of real shallow geothermal systems in numerical heat flow models and provides an enhancement of the optimal management of renewable energy resources in urban environments acknowledgments the authors would like to thank the anonymous reviewers for their valuable comments and suggestions to improve the quality of the paper this work was performed by the geological survey of spain igme under the cooperation agreement framework between igme and the ebro hydrographic confederation che for the specific arrangement entitled the application of a groundwater flow and heat transport numerical model for the simulation of management strategies of geothermal installations in the city of zaragoza alejandro garcía gil gratefully acknowledges mike by dhi for the sponsored feflow license appendix a supplementary data supplementary data associated with this article can be found in the online version at https doi org 10 1016 j jhydrol 2018 05 057 appendix a supplementary data the following are the supplementary data to this article supplementary fig s1 
7131,different aspects of management policies for shallow geothermal systems are currently under development although this technology has been used for a long time doubts and concerns have been raised in the last years due to the massive implementation of new systems to assess possible environmental impacts and manage subsurface energy resources collecting data from operating shallow geothermal systems is becoming mandatory in europe this study presents novel advances in the upscaling of operation datasets obtained from open loop geothermal energy systems for an optimal integration in hydrogeological models the proposed procedure allows efficient numerical simulations to be performed at an urban scale specifically this work proposes a novel methodology to optimize the data treatment of highly transient real exploitation regimes by integrating energy transfer in the environment to reduce more than 90 registered raw datasets the proposed methodology is then applied to and validated on five different real optimization scenarios in which upscaling transformation of the injection temperature series of 15 min sampling frequency has been considered the error derived from each approach was evaluated and compared for validation purposes the results obtained from the upscaling procedures have proven the usefulness and transferability of the proposed method for achieving daily time functions to efficiently reproduce the exploitation regimes of these systems with an acceptable error in a sustainable resource management framework keywords shallow geothermal energy gwhp urban hydrogeology thermal management groundwater thermal impact 1 introduction shallow geothermal systems are based on obtaining the heat energy from materials of the most superficial layers 250 m of the earth s crust and the water that flows through them the heat transfer from the earth s core to the outer areas of the crust and the capacity of the ground to dampen thermal oscillations occurring on the surface make thermal stability possible starting at a depth of approximately 15 m after the damping of thermal oscillations with depth the ground temperature is similar to the annual average temperature of the region plus 1 c or 2 c parsons 1970 this terrain feature justifies the development of these important systems as an adaptative measure to climate change for renewable energy development bayer et al 2012 the potential natural state of the aquifer is defined as a state without anthropogenic influences epting and huggenberger 2013 heat exchange with the ground can be performed by different types of ground source energy gse systems including closed or open systems by using heat pumps coupled with heat exchangers open systems also called groundwater heat pumps gwhps take direct advantage of the heat or cold of pumped groundwater and subsequently reinject pumped water into the aquifer garcía gil et al 2014a gwhps and gse in general are widely used worldwide and their demand is expected to increase in the next years epting et al 2017 jaudin 2013 lund and boyd 2016 the increasing trend of gwhps has resulted in an additional heat load in urban aquifers caused both by thermal interference between systems and interference between well doublets that is the thermal autointerference effect galgaro and cultrera 2013 garrido et al 2010b because the geothermal exploitation of the aquifer is not consumptive thermal pollution derived from these systems is the main impact on the aquifer lo russo et al 2014 the problems associated to the implementation of gwhps originate both from the lack of energy sustainability of the facilities due to thermal interference events garrido et al 2016 and from an insufficient legal framework the required regulatory frameworks to ensure sustainable exploitation of aquifer heat energy resources particularly beneath cities where there may be overlapping and conflicting demands on the resource are still being developed therefore generating great uncertainty for users garcía gil et al 2015 one of the possible ways to enhance the management of geothermal systems is using numerical models to simulate the thermal regime in the urban aquifer and to establish effective management strategies banks 2009 this approach allows considering the high complexity of these facilities the heterogeneity of the medium and the temporal variability of their operations in an integrated way on the other hand these methods have two limitations that is the high volume of data and the time requirements garcía gil et al 2014a the modelling process of gwhp systems was carried out using data from specific discrete measurements in previous studies epting et al 2013 gropius 2010 herbert et al 2013 this simplification was applied as an appropriate approximation considering the inherent difficulties in the monitoring of such complex installations and the consequent lack of exploitation regime datasets currently regulators are increasingly requiring monitoring data on the operation of gwhp systems these systems typically operate following a design power and thus as building demand varies the schemes may operate continuously or rather intermittently particularly the system may have a rather short operational cycling period at locations where a scheme delivers a high proportion of the total heating or cooling demand this leads to the need for very high frequency monitoring data to characterise the system s operation in contrast numerical modelling becomes computationally expensive and time consuming if this short cycling detail is to be represented explicitly the main purpose of this study is to develop and validate a methodology to obtain as optimally and efficiently as possible a subset of maximal representative data which can be easily implemented in numerical heat transport models that is to obtain data subsets for numerical models resulting in minimum deviations when compared with original data validation of the proposed methodology under standard hydrogeological parameters is carried out to ensure its transferability to other aquifers operated by gwhps this objective achievement will constitute an improvement in attaining a scientific based management tool which allows the reproduction of real exploitation regimes and therefore the aquifer response to intensive shallow geothermal exploitation a requirement in obtaining a global vision necessary for aquifer management accomplishing this objective will help to understand the hydrodynamics and existing heat transport processes beneath urban environments thus contributing to the improvement of sustainable management of shallow geothermal resources epting et al 2013 spitler 2005 to reach this goal different upscaling techniques were considered in this work to transform high resolution datasets obtained from high frequency data logging into lower frequency data subsets bierkens et al 2000 finke et al 2002 the upscaling procedure has been widely used and developed in applied research typical in environmental science where specific questions raised by society s decision makers policy scale and observation scale are not met the scale transfer procedure or upscaling has been classified by bierkens et al 2000 depending on the involvement or non involvement of a model in the research cycle the possible linear relationship between the model and input variables and parameters the applicability of the model to different locations time steps the form of the model at different scales or the possibility of deriving analytically a different scale model the major classes of upscaling methods consist of averaging the observations or output variables brus and de gruijter 1997 viscarra rossel et al 2016 finding representative parameters or input variables dagan 1981 wu et al 2006 averaging the model equations bedrikovetsky 2008 whitaker 1986 and performing a model simplification de vries et al 1998 vogler et al 2018 the upscaling procedure applied in this paper corresponds to the methods finding representative parameters since the input variables of the model involved are non linear so it cannot be applied at many time steps and the model has the same form at the two scales involved bierkens et al 2000 furthermore since it is not possible to obtain output variables at the larger scale and input data at the source scale is exhaustive deterministic methods have been applied in accordance with previous studies bierkens and van der gaast 1998 johannes dolman and blyth 1997 yu et al 2016 finally a sensitivity analysis was performed by numerically modelling different upscale scenarios to optimize the procedure and to perform and conduct an error assessment 2 study site the present work was carried out within the framework of the urban alluvial aquifer of zaragoza fig 1 located in the central sector of the ebro river basin spain this basin constitutes the last stage in the evolution of the southern foreland basin of the pyrenees barnolas and robador 1991 pardo et al 2004 the portion of the aquifer covering the urban area of the city is known as urban alluvial aquifer of zaragoza garrido et al 2006 2010a b due to the singular hydrogeological properties of environment and groundwater use among which geothermal exploitation is the most important the ebro river crosses the city in nw se direction where two of its tributaries the huerva and gállego rivers converge the urban alluvial aquifer consists of early pleistocene fluvial alluvial and aeolian sediments generated by wide alluvial plains over an evaporite dominated miocene substratum quirantes 1978 simon et al 2014 the surface of the alluvial aquifer under the city is 632 km2 and its thickness ranges from 5 to 60 m with a considerable spatial variability due to tectonic gravitational diapiric and dissolution processes affecting this detrital cover sánchez navarro et al 2004 soriano et al 2012 the transmissivities were derived from previous flow model calibrations garrido et al 2006 this groundwater numerical model developed for the alluvial aquifer of zaragoza recreated the transient regime of groundwater flow considering a cauchy boundary condition for the reproduction of the ebro river hydraulic connection this boundary condition is imposed according to a time function providing the river head corresponding to every time step hence the flood events are considered in the model the western and eastern boundary conditions were chosen as prescribed head or dirichlet corresponding to isohypses according to head data measured in the field of the respective zones the rest of the unassigned boundaries of the model were defined as neumann boundary conditions with zero flux which represent the lateral contact between the aquifer and the tertiary bedrock the results of the hydraulic test are in the range of 3 102 4 103 m2 day 1 garcía gil et al 2014b moreno 2008 the depth of the phreatic surface ranges from 7 to 34 m and the main groundwater flow pattern is nw se in the southeastern part of the urban aquifer and w e in the northwestern part or at the left margin of the ebro river garcía gil et al 2015 a total of 188 wells exploit the urban alluvial aquifer for geothermal energy production 112 extraction wells and 76 injection wells a registry of 73 geothermic installations 65 in use is currently gathered in an exploitation catalogue the pumped groundwater in 2010 was estimated to be approximately 24 106 m3 with only 0 93 106 m3 being consumptive the remaining withdrawal is associated with geothermal exploitation by means of gwhps garrido et al 2010b the total heat power installed has been estimated to be 110 mwt of energy used for cooling purposes with 21 installations being equipped with reverse cycle heat pumps with 34 mwt of the total heat power supplied garrido et al 2012a b the thermal groundwater regime of the city and the extension of the heat plumes generated by gwhps were reproduced by numerical modelling of the groundwater flow and heat transport garcía gil et al 2014a 2015 a total of five gwhp geothermal exploitations were selected as study objects in the present paper corresponding to three hotels a museum and a hospital in zaragoza the selection was carried out based on different aspects attempting to obtain the largest level of representativeness the oldest exploitation which started in 1999 is sgs 4 and the most recent ones are sgs 1 and sgs 2 which started in 2010 all geothermal exploitations have two extraction wells and one injection well except for sgs 5 which has four extraction and three injection wells the average depth below ground of these wells ranges between 24 and 44 m the average flow rate circulation of the installations sgs 1 to sgs 5 is 17 62 11 4 16 6 19 16 and 50 l s 1 respectively all studied installations have a climatization system that is used both for cooling and heating with an average heat power of 0 9 1 0 1 9 1 8 and 2 6 mw for sgs 1 to sgs 5 respectively 3 methodology 3 1 origin and data treatment by filtering synchronization and homogenization the monitoring of the regime of geothermal system exploitations is carried out by the owners by request of the local water management administrator che spanish acronym later datasets are transferred to the geological survey of spain igme spanish acronym for evaluation the data logging consists of flow l s 1 and temperature c measurements at both extraction and injection wells with a 15 min sampling frequency the data registry shows a high variability between the gwhp installations due to their different exploitation regimes data filtering and synchronization was performed based on a 15 min sampling frequency reference guide to obtain continuous and homogeneous data time series in addition temperatures registered while the operation wells were inactive were discarded because the recorded values are not representative 3 2 application of different upscaling approximations the upscaling problem posed in the introduction consists in finding representative input variables at a given scale e g daily scale for groundwater flow and heat transport modelling obtained from an exhaustive dataset at a smaller scale e g 15 min scale to reproduce adequately the operation of gwhp systems it has to be stressed that the term scale denoted on maps means exactly the opposite to the scale definition used in upscaling and downscaling methods for environmental research the operation of gwhps is commonly implemented in a groundwater flow and heat transport model by assigning a prescribed water input output rate and a prescribed temperature for each time step to the nodes of the model representing operation wells pumping injection rate non consumptive is an additive also called extensive variable that is proportional to the size of the sample this feature makes the upscaling of pumping injection rates trivial in contrast defining a representative injection temperature becomes non trivial temperature is an intensive variable therefore it is not proportional to the sample size to derive representative input injection temperatures theoretical expressions or experience have to be considered in this study different upscaling formulas have been used including daily quartiles q1 q2 and q3 geometric mean and daily maximum and minimum values of 15 minutal injection temperature datasets measured from injection wells also a daily random registered injection temperature value was considered to evaluate the effect of a random approach on upscaling these upscaling approaches allowed covering the high variance of injection temperature values providing a basic simple statistic approach and laying the base framework for comparison of more complex approaches on the other hand in this work we propose the integration of transferred energy by gwhp systems calculated at sampling frequency scale to obtain a representative equivalent temperature teq at a larger scale adequate for efficient numerical modelling the proposed calculation of this representative temperature is described by the following steps described hereafter first the energy transferred h m into the aquifer in the m time period of the sampling frequency is calculated by the following expression 1 h m q m c w ρ w t im t cm where q m is the pumping rate m3 s 1 measured every m time period c w is the specific heat capacity of water j kg 1 k 1 ρ w is the water density kg m 3 t i m t c m is the increment of the temperature between the injection temperature t i m and the extraction temperature t c m during two measurements of m sampling period interval in this research min g i v i n g t h e e n e r g y t r a n s f e r r e d w i t h t h e a q u i f e r w i t h a min giving the energy transferred with the aquifer with a 15 min resolution the energy transferred h n into the aquifer in n period required in a numerical model time step is given by 2 h n 1 n h m 1 n q m c w ρ w t eq t cn where 1 n q m q m 1 q m 2 q m 3 q mn and 1 n h m h m 1 h m 2 h m 3 h mn in this paper n 96 which is the number of measurements to obtain daily n 1 day integrated input values for the model the equivalent temperature t eq represents the temperature value of injection in the model in order to transfer the energy transferred in to the aquifer throughout the n period 1 day in this case the term t cn stands for average captation temperature defined as 3 t cn 1 n 1 n t cm by rearranging eq 2 the following upscaling formula for equivalent temperature teq is obtained 4 t eq 1 n h m 1 n q m c w ρ w t cn all the aforementioned upscaling approaches were used to calculate daily time series for each of five gwhps selected section 2 and were subsequently implemented in a numerical groundwater flow and heat transport model for validation 3 3 sensitivity analysis of the upscaling process as a function of the time resolution to assess the upscaling procedure using the calculated equivalent temperature different temporal resolutions were considered δ t three time series considering half daily two daily and five daily sampling frequency were performed to evaluate the deviation in the evolution of each scenario against 15 min measurement time series adopted as realistic control time series furthermore this analysis intended to explore the possibilities of working with lower sampling frequencies to correctly reproduce the exploitation regimes of the installations and quantitatively check the different options available 3 4 numerical groundwater flow and heat transport models the feflow code diersch 2013 was used to reproduce 3d groundwater flow and heat transport processes induced by five gwhp systems sgs 1 sgs 2 sgs 3 sgs 4 and sgs 5 fig 2 in the alluvial aquifer studied the installations were selected based on different criteria attempting to cover all existent ranges of hydraulic gradient permeability and exploitation regimes therefore they are considered representative of the general operation of shallow geothermal systems located in the study area a total of five local hydrogeological models corresponding to the gwhp installations were implemented based on a previous calibrated and validated 3d regional groundwater flow and heat transport model covering the urban area of zaragoza epting et al 2017 garcia gil et al 2014 the modelled domain for each hydrogeological model analysed considered a 600 m groundwater flux tube to prevent border effects with its geometry being obtained from a city scale 3d regional model epting et al 2017 a 40 m thick miocene basement and unsaturated zone nsz were considered for downward and upward heat conduction respectively the domain characteristics of each installation model reflect different dimensions and mesh refinements depending on the number and spatial distribution of the gwhp operation wells each domain was discretized into an unstructured finite element mesh with triangular elements table 1 all of which were distributed in 12 14 layers depending on the model domain fig 3 a fixed head boundary condition was imposed to the incoming and outlet groundwater flow nodes of the flux tube considered to reproduce the groundwater flow with hydraulic gradients of 1 35 10 4 8 60 10 4 4 78 10 3 7 27 10 4 and 1 04 10 2 for installations sgs 1 to sgs 5 respectively a fixed ground temperature of 17 c was assigned to the left groundwater inlet boundary the thermal and hydraulic parameterizations of the used models are listed in table 2 the boundary conditions defined for geothermal operation wells consisted of a transient prescribed flux and a transient prescribed temperature boundary condition according to the time series measured and calculated by upscaling procedures an initial temperature of 17 c was chosen approximately matching the annual mean of the region a set of observation points at different distances from the injection wells 1 3 5 10 20 40 100 and 200 m was defined in all models used the observation points allowed to register the temperature evolution as a heat transport state variable for each upscaling optimization scenario and to identify which calculated time series generated the closest thermal response to the real scenario i e 15 min sampling frequency the simulations were carried out under a transitory regime with a 15 min and one day time step for the sensitivity analysis of the proposed upscaling procedure a time step of 0 5 2 and 5 days was adopted in two gwhp system models sgs 1 and sgs 2 3 5 calculation of the upscaling procedure error the assessment of the deviation due to the proposed upscaling procedures was carried out by comparing the reference scenario 15 min measurements with the calculated upscaling scenarios the comparison was conducted using thermographs obtained from all defined observation points the temperature difference was obtained by subtracting the reference scenario from each optimization scenario to calculate the deviation for all time steps the deviation was calculated as root mean square error rmse using the following equation 5 rmse i 1 n d 1 2 n where d is the temperature difference between the optimization and reference scenarios and n is the total number of existing values in each considered set the rmse values approaching 0 were considered to reflect a high optimization degree of the upscaling procedure during the error analysis performed special attention was focused on the possible effects of the hydrogeological parameters on the rmse distribution 4 results 4 1 results for the exploitation regime and upscaling procedures the time series including the upscaling procedures proposed for each installation were calculated with the same approximation methods but with different temporal resolutions fig 4 a shows the analysis carried out for installation sgs 1 where the 15 min sampling frequency series of the injection temperature was considered to be the real exploitation regime the registry period includes 273 days with a mean flow of 661 m3 day the registry is continuously operating except for minor interruption intervals the graphic shows that 55 of the time was utilized for cooling thus causing a high temperature injection 25 c and the rest of the time was used for heating leading to cold water injection 15 c the calculated upscaling series of the injection temperature for the gwhp system sgs 1 geometric mean q1 q2 q3 maximum minimum random value and equivalent temperature for daily sampling frequency is shown in fig 4b c regarding optimization minimum data reduction of 98 9 was achieved through upscaling transformation of daily scenarios table 3 this generated a reduction in the computational time of 10 6 h on average table 4 4 2 results for the simulation of upscaling scenarios the temperature distribution obtained from the reference time series at the end of the simulations is shown in fig 5 the influence of thermal dispersion and its interaction with established observation points can be deduced based on the observation of heat plumes for the last calculated time the heat plume generated by installation sgs 1 fig 5a extends to a distance of 40 m with a temperature increase of 1 75 c with respect to the initial situation 18 75 c isotherm fig 5b shows the thermal impact of installation sgs 2 on the aquifer generating a larger heat plume than that of installation sgs 1 23 79 c isotherm 40 m from the observation point with a temperature increment of 3 84 c at a distance of 100 m from the observation point 20 84 c isotherm installation sgs 3 generated the largest plume fig 5c with the groundwater temperature at the 40 m observation point increasing to an isotherm of 24 31 c and at the furthest observation point of 200 m increasing by 3 65 c 20 65 c isotherm the thermal impact caused by installation sgs 4 is shown in fig 5d installation sgs 4 generated a heat plume with a temperature increment of 3 55 c at a distance of 40 m 20 55 c isotherm the temperature increased by only 0 82 c 17 82 c isotherm at a distance of 100 m the temperature effect is almost negligible at this observation point the smallest heat plume was generated by installation sgs 5 fig 5e with an observed temperature increment of only 0 43 c 17 43 c isotherm at a 40 m distance from the injection point in addition the thermographs obtained from reference scenario simulations are shown in fig 6 for all gwhp systems the temperature evolution of the reference scenarios and each upscaling procedure for all time steps simulated at 20 m from the injection point for each gwhp installation are shown in fig 7 all other distances are shown for sgs 3 as supplementary material fig s1 the temperature evolution for each of the upscaling procedures and their difference with respect to the measured reference values can be observed in these thermographs the upscaling approaches that cause the largest error are associated with scenarios in which maximum and minimum values were incorporated deviating up to 5 c from the real measured temperature scenario in some exploitations average deviation of 3 c the same phenomenon occurs when using q1 and q3 although the deviation is smaller average deviation of 1 8 c it does not follow the real regime of the studied installations regarding the data series of the geometric mean q2 and random value the deviation decreases to an average value of 1 c the equivalent calculated temperature scenario causes the least deviation 0 4 c with respect to the reference scenarios for all studied installations 4 3 upscaling procedure error calculation the distribution of upscaling errors rmse for each of the gwhp installations at all distances evaluated can be observed in fig 8 the largest upscaling error occurs when using the minimum value upscaling approach resulting in a 1 c to 5 5 c error the deviation of the maximum value approach ranges from 1 2 c to 3 5 c and that of q1 and q3 approaches results in an error of approximately 2 c the q2 geometric mean and random value approaches show a similar deviation of 1 5 c the equivalent calculated temperature is the only upscaling procedure with a deviation below 0 8 c for all gwhp installations in addition the average rmse values of all installations for all upscaling procedures are presented in fig 8 the results show that the minimum value upscaling approach produces the largest error ranging from 0 8 c to 4 6 c in contrast scenarios using the equivalent calculated temperature approach present the lowest error range 0 3 0 5 c 4 4 error calculation for the upscaling procedures as a function of the time resolution the error rmse associated with the use of the equivalent temperature series carried out in all studied installations with daily temporal frequency was calculated along different observation points fig 9 in addition the rmse values derived from the equivalent temperature upscaling approach for all observation points established in the numerical models sgs 1 and sgs 2 with different temporal frequency half daily two daily and five daily are shown in fig 10 a high deviation was observed for the five daily sampling frequency scenario for both installations with a deviation of 2 1 c and 2 5 c respectively at the nearest observation point 1 m from the injection well the two daily sampling frequency scenario caused a deviation similar to that of the daily sampling frequency scenario with rmse values of 1 6 c and 1 9 c respectively the daily and half daily sampling frequency scenarios presented a similar average deviation of 1 5 c on the other hand if observation points located near the injection well 1 3 and 5 m are ignored the deviation for both installations and each of the upscaling scenarios decreases to approximately 1 c for half daily and daily resolution scenarios a maximum deviation of 0 25 c was obtained at the first 10 m observation point 5 discussion the equivalent temperature upscaling approach calculated by the integration of the daily dissipated heat power by a gwhp installation is considered to provide the highest reliability because it is derived from the real operation regime of the installation using the equivalent temperature approach scenarios the time series half daily two daily and five daily sampling frequency were obtained for two of the studied installations where the simulation time was almost the same as that of the previous case daily sampling frequency approach because the exploitation regime of geothermal installations does not follow a common pattern each one was analysed individually in general sgs 1 sgs 3 and sgs 5 had a continuous operation regime with the highest average flows 661 865 and 892 m3 day respectively except for time intervals during which they did not inject water on the contrary installations sgs 2 and sgs 4 developed a variable regime with lower flows 457 and 265 m3 day and prolonged intervals in which they were off the climatization demand is only for cooling purposes in sgs 3 and sgs 4 a fact that differentiates them from the rest of the installations which have a combined use for both heating and cooling purposes the plumes that reached the largest thermic influence area 100 and 200 m were generated by installations sgs 2 and sgs 3 recording the highest injection temperatures 30 c during summertime installation sgs 4 generated a smaller plume due to the injection of water at a lower temperature 30 c and its variable regime remaining off during large periods of time the smallest measured plumes were generated by installations sgs 1 and sgs 5 derived from a combination of cooling and heating also they presented lower injection temperatures compared to those of the other exploitations 30 c based on the numerical model simulations of the different scenarios the error derived from the upscaling approximations of daily sampling frequency against high real sampling frequency data was quantified the temperatures generated along observation points allowed the deduction of the deviation evolution daily approximations calculated based on the minimum and maximum caused the widest average deviation range varying from 0 8 c to 4 6 c and from 1 2 c to 3 2 c respectively the rest of the statistic parameters had a global average standard deviation of 0 73 c the smallest deviation corresponded to the equivalent temperature scenario and ranged between 0 3 c and 0 5 c this means that the calculated temperature is the best and most useful upscaling approximation because it is the only parameter that generated a deviation range below 0 8 c among the analysed installations on the other hand the approximations carried out with different temporal resolutions using the equivalent temperature upscaling approach in two of the analysed installations proved that both half daily and daily scenarios considering all observation points generated the lowest deviation values thus reducing the established temporal intervals is not particularly useful furthermore ignoring observation points near the injection wells 1 3 and 5 m led to a 1 c reduction in the error range the maximum deviation was observed at the observation point at 10 m distance 0 25 c this is due to the fact that the points near the gwhp operation wells are affected by hourly thermal exploitation regimes thus indicating that a daily or even higher sampling frequency simulation is necessary to reproduce a thermal regime with a reasonable error at points close to the injection well considering that the evaluation of thermal impacts close to the injection wells of gwhp systems is not the aim of regional scale models and that the mean minimum distance of gwhp wells to the monitoring points for zaragoza is for all cases greater than 20 m the errors derived from the upscaling procedure proposed at the first 20 m from the injection well could be neglected based on the research carried out in this paper the obtained results are of interest to obtain time functions of daily sampling frequency even when their operation regime is highly variable thus enabling the implementation of these exploitation regimes in regional models at the city scale furthermore the validation of the proposed methodology based on standard hydrogeological parameters ensures its transferability to other equivalent hydrogeological environments that is shallow alluvial aquifers which are prevailing in urban environments the error analysis performed on the upscaling procedures has not highlighted any relationship between the hydrogeological parameters and the errors observed this indicates that the upscaling procedure proposed is significantly independent from the hydrogeological parameters in the parameter ranges evaluated in this study furthermore when applying the results found to other hydrogeologically different city aquifers it has to be noted that the optimal upscaling approach might be influenced by the relevance of advection versus conduction heat transport mechanisms given by the peclet number domenico and schwartz 1998 which relates the energy transported by fluid motion to the energy transported by conduction in the cases evaluated heat advection is greater than heat conduction which is a situation likely found in alluvial aquifers situations where transmissivities and hydraulic gradients are lower than those considered in this work are not suitable for the application of the conclusions found in this research conversely although the equivalent temperature upscaling procedure has been shown as the best approach to reproduce the thermal pollution signal of a gwhp other approaches might be of interest for authorities when considering the maximum impact produced by a gwhp in terms of regulatory requirements or derogation of other users from the resource regulators might consider using daily maximum temperature as a conservative approach to integrate the high frequency sampling data this approach would tend to overestimate thermal plumes generated but would be safer in case of future peak conflicts gwhp systems are likely to present similar temporal demand profiles but their interpolation to longer times might get to losing peak demands this perspective indicates that the equivalent temperature proposed does not have to be necessarily used in numerical models for the assessment of thermal impacts required for thermal discharge authorizations 6 conclusions the exploitation regime of geothermal systems presents a highly variable pattern that increases the difficulty of processing data from such installations prior to their implementation in regional scale groundwater flow and heat transport models monitoring of shallow geothermal systems is necessary and should be mandatory to guarantee the long term exploitation of the renewable energy resources and the aquifer sustainability the research performed in this paper revealed the difficulties associated with the necessary treatment to study the exploitation regime of gwhp installations based on daily approximations calculated by upscaling of the injection temperature a data reduction of 98 9 was obtained this makes an adequate implementation of gwhp systems in hydrogeological models possible thus making them more efficient because a large amount of simulation time is saved accounting for a decrease from an average duration of 10 h to 4 min the upscaling scenarios calculated by descriptive statistic parameters geometric mean q1 q2 q3 maximum and minimum together with a random approach and the equivalent injection temperature proposed in this paper allowed to obtain a global approximation to the variability range of the injection temperatures and to calculate the deviation for each scenario by making use of the proposed novel upscaling procedure proposed here to calculate a daily equivalent temperature from the heat power transferred by real groundwater heat pump installations with both daily and half daily sampling frequencies it is possible to reduce significantly the amount of data required to optimize its implementation in complex numerical models at an urban scale the methodology has been validated on five different real scenarios covering the most standard hydrogeological conditions in shallow alluvial aquifers to ensure its transferability the most relevant conclusions of this study with respect to numerical models of the heat regime in urban aquifers with shallow geothermal installations are the following recording temperature and pumping rates with a 15 min sampling frequency or higher is convenient to estimate the thermal power dissipated by shallow geothermal exploitations due to the potentially high variation of the exploitation regime over time the upscaling procedure that best fits real operations of geothermal systems is the calculation of the equivalent injection temperature by integrating the daily dissipated energy regional scale numerical modelling of all studied exploitation regimes can be efficiently implemented throughout daily resolution time series data the error analysis indicates that the error values of the calculated thermographs are lower than 0 4 c 0 1 c using the proposed upscaling procedure presented in this work these results add additional uncertainty to the calibration process of numerical models and therefore should be considered when estimating thermal parameters in conclusion the present paper depicts an advance in the implementation process of real shallow geothermal systems in numerical heat flow models and provides an enhancement of the optimal management of renewable energy resources in urban environments acknowledgments the authors would like to thank the anonymous reviewers for their valuable comments and suggestions to improve the quality of the paper this work was performed by the geological survey of spain igme under the cooperation agreement framework between igme and the ebro hydrographic confederation che for the specific arrangement entitled the application of a groundwater flow and heat transport numerical model for the simulation of management strategies of geothermal installations in the city of zaragoza alejandro garcía gil gratefully acknowledges mike by dhi for the sponsored feflow license appendix a supplementary data supplementary data associated with this article can be found in the online version at https doi org 10 1016 j jhydrol 2018 05 057 appendix a supplementary data the following are the supplementary data to this article supplementary fig s1 
7132,a wide range of real world flood routing problems can be approached using the parabolic approximation widely recognized as a convenient simplification of the saint venant equations from the parabolic approximation head and discharge based linear and non linear advection diffusion models were also derived by merging mass and momentum conservation equations into a single second order equation to be used for one dimensional flood routing both head and discharge based advection diffusion models were originally derived in their linear forms and extensively studied in the literature because they admit analytical solutions the discharge based non linear models have been used as the basis for the derivation of the variable parameter muskingum routing method by setting the truncation error at par with the diffusion parameter whereas the linear models are mass conservative but inadequately reproduce routing in a variety of cases the non linear ones have been recently declared to be non conservative in the literature it has also been stated that the variable parameter muskingum fails to approximate the discharge based non linear discharge based advection diffusion model on mild slopes this paper aims at providing a systematic overview of all these models showing that whereas the non linear head based advection diffusion model an original version of which is derived here is in fact conservative also the discharge based model which in principle is not remains de facto conservative if integrated over a space time grid with sufficiently small spatial cells to complete the overview we also demonstrate that in real world applications the variable parameter muskingum gives satisfactory results on mild slopes because lowland rivers in nature are essentially prone to slowly rising hydrographs the results of all models are intercompared by routing different flood waves through prismatic reference channels with slope 10 4 and smaller keywords 1d channel routing non linear advection diffusion equation parabolic equation mild slope variable parameter muskingum trapezoidal section mass and momentum conservation nomenclature abbreviations l ad linear advection diffusion equation nl adq non lnear discharge based advection diffusion equation nl ady non linear head based advection diffusion equation cr courant number da diffusion analogy pa parabolic approximation pe péclet number sbk sobek sv saint venant equation vpm variable parameter muskingum latin symbols a cross section area of the channel l 2 b channel top width l c convective speed coefficient in adey l t c k kinematic wave celerity l t d diffusion coefficient in adey l 2 t f generic source sink term of ψ term f generic flux term g gravitational acceleration l t 2 i non convective i e diffusive flux k hydraulic conveyance l 3 t j slope of the energy gradient n manning roughness l 1 3 t p pressure m t 2 l p w wetted perimeter l r h hydraulic radius l q discharge l 3 t s o channel bed slope t time t t period of input wave t t peak time to peak of input wave t v velocity vector at the point scale l t v reach volume l 3 x longitudinal channel coordinate l y channel water depth l z absolute free surface head l z f channel bed elevation l greek and mixed symbols α slope angle of prismatic channel side rad deg γ shape parameter for synthetic hydrograph event based scaling parameter 1 s o dy dx from price 1985 η implicit explicit weighting of the spatial grid discretization θ implicit explicit weighting of the temporal grid discretization μ fluid viscosity m lt δ t time increment t τ viscous stress tensor δ x reach segment length l ψ generic hydrodynamic property at the point scale ρ mass density m l 3 ω linearization parameter in the picard method 1 introduction hydraulic flow routing in river channels is a central topic for studying the propagation of flood waves a reliable prediction of the flood peak in terms of flow rates and heads forms the basis for decision making in flood warning and control the advent of large computing power has opened the possibilities for flood propagation in large basins at national regional or even continental scale in europe de roo et al 2003 sampson et al 2015 and the us maidment 2016 continental scale flood warning systems have been implemented which serve as real time forecasting and pre warning platforms in these systems grid based hydrological models are combined with flood routing to seamlessly predict stage and discharge at arbitrary river cross sections while the subject of flow routing has been amply studied in the past open issues still persist one relevant topic is accurate and computationally cheap flow routing for generating ensemble real time streamflow predictions pappenberger et al 2008 while the saint venant sv equations de saint venant 1871 also called dynamic wave model or long wave equations fenton 2010 is the most comprehensive description of 1d channel flow their solution remains computationally expensive especially when large river networks are concerned or multiple simulations are required to asses uncertainty krzysztofowicz 1999 raftery et al 2005 todini 2008 on stage and discharge simpler and computationally leaner solutions such as the kinematic wave model or non linear reservoir routing reggiani et al 2014 cannot be applied to large lowland river systems because the flow behavior that such models describe is essentially translatory while the typical flow dynamics of mildly sloping river systems with bed gradients in the order of 10 4 10 5 is strongly diffusive such behaviour is characterized by the relevance of pressure gradients that are related to the divergence of surface gradient and bed slope in steeper river systems with slopes in the order of 10 3 the surface slope can be considered parallel to the channel bed and thus pressure gradients become largely irrelevant the pressure gradient is also responsible for the hysteretic relationship between stage and discharge during flood propagation in lowland rivers and must be fully accounted for in flood propagation as an alternative to using the full sv equations we could resort to the variable parameter muskingum method vpm cunge 1969 ponce and yevjevich 1978 todini 2007 which is computationally cheap and able to reproduce the stage discharge hysteresis the vpm has been developed by approximating the second order parabolic non linear advection diffusion nl ad equation in terms of discharge nl adq by finite differences and setting the truncation error at par with the diffusion parameter cunge 1969 price 2009 while the vpm model has been shown to perform well for natural channels cappelaere 1997 reggiani et al 2016 its limit behaviour on very mild slope in the presence of steep waves has not been explicitly addressed on this topic fenton 2013 published a note in which he maintains that the vpm cannot reliably reproduce flood waves on slopes in the order of 10 4 or less in the presence of very steep flood waves he attributed this to the second order derivative of the advection diffusion equation approximated by the vpm which causes excessive diffusion following up on gasiorowski and szymkiewicz 2007 who showed that the l ad equation is always conservative while nl adq is inherently non conservative we extended their analysis to study the conservation properties of the nl adq equation from which the vpm has been derived first we solve the simplified sv equations which form the basis for the parabolic approximation pa model and compare the results to the full sv solution for prismatic channels second we derive a conservative head based formulation of the ad equation nl ady from the pa model which yields results that are very close to the full sv and the pa solutions also on very mild slopes third we investigate under which circumstances the solution of the nl adq equation which underlies the vpm can become de facto conservative and if it still allows for adequate wave propagation on mild slopes and fast rising waves we also study under which conditions the vpm can be used for flood propagation in such flow regimes then we report on the ways in which the ad equation in its various forms linear non linear discharge based or stage based formulation has been historically derived from the sv equations fourth we route four flood waves with different semiperiods across mildly sloping prismatic channels demonstrating that the nl adq indeed provides an accurate description and is conservative under conditions of convection and diffusion parameters that remain spatially but not necessarily time constant over a reach element a condition that is satisfied when the equation is integrated by finite differences over a fine spatial grid fifth and finally we compare the vpm against all previous results and show that it continues to provide an accurate solution for slowly rising flood waves a condition met in most natural lowland rivers the method becomes excessively diffusive with increasingly steep flood waves due to spurious numerical diffusion nevertheless the vpm continues to hold up as a computationally inexpensive method for accurate stage discharge flood propagation analysis in natural rivers the paper is structured as follows section 2 provides an overview of the advection diffusion equations and their derivation in section 3 we discuss the models and the numerical approaches used for numerical integration in section 4 we describe the setup for numerical experimentation section 5 describes the results while section 6 is devoted to the discussion conclusions are drawn in section 7 2 parabolic models for discharge routing 2 1 parabolic approximation pa the most complete but not exact flow description in a channel is provided by the sv equations de saint venant 1871 or mass and momentum conservation 1 a t q x 0 q t q 2 a x ga y x ga s o j this system of hyperbolic differential equations is also known in the literature as the dynamic wave model kundewicz 1985 or as long wave equations model fenton 2010 ponce et al 1978 showed that for a value of t s o g y 2 15 with t the period of a sinusoidal input hydrograph the momentum equation can be simplified by dropping the local and convective acceleration terms this hypothesis has been further analyzed and elaborated by morris and woolhiser 1980 daluz vieira 1983 moussa and bocquillon 1996 as a result we obtain a simplified equation system called also parabolic approximation pa or diffusion wave henderson 1966 weinmann and laurenson 1979 whereby we prefer to use the former denomination as it mathematically correctly classifies the equation system 2 a t q x 0 y x s o j 0 the system of coupled differential eq 2 with a highly non linear momentum equation can be solved directly in terms of water depth y and discharge q it will be shown that the pa model closely reproduces the sv results within a wide spectrum of very mild bed slopes and steep inflow waves 2 2 the advection diffusion ad model here we discuss the derivation of the advection diffusion model which is often also referred to as diffusion analogy model several authors have in fact proposed to merge the two equations of the pa into a single parabolic differential equation formulated in linear or non linear form in either one of the two variables flow depth y or discharge q 2 2 1 linear advection diffusion l ad hayami 1951 was among the first to observe that wave propagation in a channel was analogous to a advection diffusion process in which a perturbation moves with a characteristic wave speed and is simultaneously dispersed due to turbulence and shear effects taylor 1953 he amended the continuity equation for a channel of unit width formulated in terms of water depth y with the ad hoc introduction of a dispersive flux obtaining a non linear parabolic equation in y hayami then linearised the equation as he looked for an analytical solution by means of a series expansion a similar approach was also proposed by lighthill and whitham 1955 who started from the linearization of the sv equations around a constant reference discharge and depth q o and y o demye 1938 masse 1938 for a channel of unit width 3 gy o u o 2 2 q x 2 2 u o 2 q x t 2 q t 2 3 gs o q x 2 gs o u o q t where u o q o y o is a reference velocity this is a linear hyperbolic differential equation also known as telegraph equation valid for small perturbations q and y in which the second and third term on the left hand side can be neglected to retain a parabolic equation a condition rarely met during a flood event koussis 1976 dooge 1967a and dooge 1967b also obtained a very similar linear parabolic wave models by introducing kinematic wave approximations into 3 after neglecting all three left hand side terms all of these parabolic equations are often considered linear diffusion analogies da applied to flood routing kundewicz 1985 with only minor mutual differences in terms of parameter expressions 2 2 2 non linear head based ad equation nl ady leaning on hayami s work on linear flood routing we also derive a conservative non linear advection diffusion model nl ady formulated in terms of water depth y using the pa model 2 as a starting point it is possible to obtain an nl ady equation by expressing the discharge q as a function of the energy line slope j and the hydraulic conveyance k and then substituting for j from the second of eq 2 4 q kj 1 2 k z x 1 2 k y x s o 1 2 by substituting for q in the first of eq 2 one obtains 5 y t 1 b k y x s o 1 2 x 0 which is a non linear da equation written in non divergent form see section 2 3 below for reaches where the surface top width b can be considered constant after a number of algebraic manipulations described in appendix a one obtains 6 y t c y x d 2 y x 2 e where c q kb k y d q 2 jb and e q kb k x y const with the r h s term e equal to zero for prismatic channels the derivative k y for generic cross sections is given by eq a 12 this second order differential equation must be supplemented with suitable initial condition and two boundary conditions 7 y x 0 y o x y 0 t y t up dy x ds t dx s o j t ds whereby the downstream condition is a direct consequence of the pa model stated in eq 2 respectively a 1 2 2 3 non linear discharge based ad equation nl adq some years after hayami s derivation the non linear advection diffusion was studied as a direct derivation from the sv equations cunge 1969 koussis 1976 price 1985 to develop the variable parameter muskingum method cunge approximated the discharge based nl ad equation nl adq which he obtained by differentiating the momentum equation of the pa system 2 with respect to x and the continuity equation with respect to t and subtracting them a technique adopted earlier by dooge 1967b as an alternative way in getting a linear ad equation directly from the linearized eq 2 8 q t c q y y x q x d q y y x 2 q x 2 0 in this expression c is a convective wave speed and d the diffusion coefficient responsible for wave attenuation both parameters are non linear functions of q and the exogenous variables y and surface gradient y x later cappelaere 1997 showed that these quantities can be approximated as functions of q and q x thus ridding 8 of the dependence on y the details for the derivation of 8 including the coefficients c and d for arbitrary channel cross sections are given in appendix b eq 8 must be supplemented with an initial condition and two boundary conditions 9 q x 0 q o x q 0 t q t up dq x ds t dx da x ds t dt whereby the downstream boundary condition follows from the continuity equation price 1985 performed a different analysis on the full sv equation system 1 after introducing the event based dimensionless scaling parameter defined as the ratio of head gradient dy dx over bed slope s o by assuming fr 2 1 with fr the froude number he analyzed the momentum equation and ranked terms by various order in he performed various substitutions by exploiting the continuity equation with the goal to eliminate y as a dependent variable in the end he obtained a non linear hyperbolic equation in only discharge containing a mixed space time derivative for negligible lateral inflow his flow routing equation is of the form 10 q t c o q x c o t a o c o 2 q x 0 where c o is the wave celerity and a o is the wave diffusivity this equation can be solved with only the initial and a single boundary condition later price 2009 showed that his advection diffusion eq 10 is to order o equivalent to eq 8 whereby the ratio a o c o 2 can be taken out of the derivative and the mixed derivative is replaced ad hoc by a second order spatial differential operator 2 3 conservation the conservation equations for a generic hydrodynamic property ρ ψ i e mass momentum or energy of a fluid at the microscale can be written in tensor notation as follows eringen 1980 11 ρ ψ t ρ ψ v i ρ f where i is a diffusive flux and f is an external supply or sink of ψ to obtain point scale mass and momentum equations ψ is set equal to respectively 1 and v i to respectively the zero or the stress tensor t p i τ with p the pressure i the identity matrix and τ μ v the viscous shear stress tensor while f is set equal to respectively zero or to the gravity vector g eq 11 constitutes a global balance law which in the one dimensional case reduces to 12 ρ ψ t x f a f d ρ f with f a and f d local advective and diffusive fluxes of ψ in x direction both water depth y and discharge q based l ad equations as well as the nl ady equation can be cast in non divergent form whereas the nl adq 8 with variable parameters cannot as demonstrated by gasiorowski and szymkiewicz 2007 the same applies to the hyperbolic eq 10 price 1985 proving that both cases are formally non conservative nevertheless it is possible to notice that the non divergent form of the nl adq 13 q t x c q d q x 0 collapses into 14 q t c q x d 2 q x 2 0 for spatially constant parameters c and d which corresponds to a locally linearized nl adq therefore the nl adq model can be made conservative when integrated using a finite difference approach by locally linearizing the equation in space keeping c and d constant over each grid cell similarly to what is done when integrating the sv equations and their parabolic approximation 2 2 4 fenton s 2013 analysis price 2009 demonstrated some equivalence between 8 and 10 on the basis of perturbation analysis more recently fenton 2013 concluded that for a trapezoidal channel on a mild slope of 10 4 respective analytical solutions of exponential type for the two equations diverge due to excessive diffusion induced by the mixed derivative term in 10 he concluded that similar models based on the discretization of 10 fail in correctly representing wave propagation on nearly flat channels this concerns the finite difference discretization of 10 proposed by price 2009 and the vpm method cunge 1969 ponce and yevjevich 1978 todini 2007 which he mistakenly considers an approximation of 10 instead of 8 it is also important to note that in his analysis he started from the correct assumption that the nl adq 8 or eq 1 in his original paper is a correct description of wave propagation while the hyperbolic eq 10 containing the mixed space time derivative is its approximation in this paper we investigate the numerical solution of the nl adq 8 on mild slopes using fenton s prismatic trapezoidal channel and prove that it constitutes indeed a valid flow representation and thus has been correctly used as basis for the vpm approximation by cunge we will also show that the vpm becomes inaccurate in cases where peclet and courant numbers differ considerably 3 numerical solutions to solve the equations for wave propagation introduced so far we resort to a existing sv solvers and b implicit finite difference solvers developed by ourselves we also use the vpm solver based on the mct approach described in todini 2007 the models and their implementation are presented hereunder 3 1 solution of the full sv equation the solution of the full sv eq 1 is performed with the finite difference staggered grid solver sobek stelling and duinmeijer 2003 similarly to fassoni andrade et al 2018 we performed the same computations for verification purposes with the packages hec ras usace 1990 and mike11 dhi 2009 obtaining matching results thus we proceed using sobek and hec ras only the solution obtained with sobek is considered the reference case against which we benchmark the pa model 2 nl ady 6 respectively a 9 nl adq 8 and the vpm 3 2 solution of the pa model equations the solution of system 2 with the continuity equation and the simplified momentum conservation equation in which the local and convective acceleration have been dropped is performed in two ways 1 with the aid of the hec ras model in which the two terms are suppressed at run time 2 with the help of a finite difference solver that we have especially developed the system 2 consists of a linear and a highly non linear hyperbolic equation which are mutually coupled through the dependence on y and q to this end we approximate the equations with a 4 point implicit finite difference scheme yielding a system in 2 n 1 equations in the same amount of unknowns where n is the total number of reach segments the discretization is shown in more detail in appendix c and leads to a linear system with four sub matrices the matrices a 11 and a 22 are diagonal while a 12 and a 21 are non symmetrical incidence matrices with constant non zero elements equal to 1 and 1 because a 11 and a 22 are non linear functions of the vectors y and q some linearization must be introduced the standard picard iteration method celia and bouloutas 1990 applied here involves sequential estimation of the unknowns y t k 1 and q t k 1 at time t as weighted averages of the latest estimates at iterations k and k 1 15 q t k 1 q t k ω q t k 1 ω 1 y t k 1 y t k ω y t k 1 ω 1 the value of ω is chosen between 0 5 ω 1 and has an effect on the convergence rate for practical purposes we chose a value of ω 0 75 weighting the solution at step k higher as the one at k 1 for the estimate at k 1 from eq c 11 it can be seen that the solution of the coupled equation system reduces to matrix products involves simple inversion of diagonal matrices and the solution of a tri diagonal linear system these operations can be computationally optimized making this model appealing for fast flow routing we note that a sufficiently accurate solution can be obtained for time steps δ t 1800 s and reach lengths δ x 1000 m 3 3 solution of the nl ady equation to tackle the solution of 6 we propose an implicit 6 point finite difference solver the differential equation is second order in y and contains variable parameters c and d which have been derived for generic cross sections in appendix a and depend non linearly on y and q the continuity equation needs to be solved simultaneously through integration from the downstream boundary upwards the continuity equation requires the imposition of q t ds as a boundary condition which can be calculated from the downstream water level given by the parabolic equation and converted to discharge via the q y relationship the parabolic equation requires two boundary conditions which are given by eq 7 the guessed solution vector y t k 1 at iteration k 1 is estimated using the picard iteration method we note that solving the nl ady for the proposed application is computationally demanding as acceptable solution accuracy requires using small δ t 300 s and reach segments with δ x 10 m δ x δ t 0 03 which considerably increases the calculation time with respect to solving the pa model equations moreover the equation needs to be made fully implicit by weighting the finite difference scheme entirely on the solution at time step n 1 3 4 solution of the nl adq equation solving 8 is similar to the solution of the nl ady equation as the problem is formulated in only q we need to integrate the backwater profile using the standard step method henderson 1966 to obtain corresponding y values because of the mutual interdependence with the profile integration the solution needs to be approximated iteratively at each time step by alternatingly solving the parabolic equation and the profile integration for the estimation of the boundary water level y t ds required for the profile integration the y q relationship is used the solution of the parabolic equation requires the boundary conditions indicated in eq 9 also in this case successive approximation of the solution at each time step is required whereby the guessed solution is approximated through linearisation as done previously concerning the conditions on step size δ t needs to be reduced to 300 s and δ x to 10 m as otherwise accuracy of the solution deteriorates 4 simulations 4 1 experimental setup next we carry out numerical simulations in which we solve the full sv eq 1 and the pa model 2 and compare the solutions with those of the non linear adq and ady the solutions are also compared against the mass and momentum conservative vpm solution todini 2007 reggiani et al 2016 the equations are forced at the upper boundary with synthetic hydrographs described in section 4 2 below the entry hydrographs have different wave semi periods with time to peak t peak ensuring that slow elongated events are compared against faster ones with increasingly steeper input wave the acceleration terms become progressively larger nevertheless the criterion t s o g y 2 15 ponce et al 1978 is satisfied in all four cases justifying the use of the pa eq 2 to enable an unbiased inter comparison the simulations were performed in an uniform trapezoidal reference channel of 250 km length 40 m base width side walls with width to depth ratio 0 5 uniform bed slope s o 10 4 a manning roughness of 0 035 s m 1 3 and a rectangular channel of width 40 m the trapezoidal section is the same as the geometry used by fenton 2013 in his note the rectangular section has been chosen to represent a situation for which it can be ensured that b x y 0 and b y q 0 and therefore additional terms appearing in the convection coefficient vanish see appendices a resp b 4 2 boundary and initial conditions the upper inflow boundary condition is given by four synthetic hydrographs described by the following exponential function price 1985 16 q t q base q peak q base t t peak e 1 t t peak γ where the curvature parameter γ 5 the base flow q base 100 m 3 s and the peak discharged q peak 400 m 3 s we develop four hydrographs with different wave semi periods to be able to reproduce increasingly steep raising flood waves this is achieved by selecting t peak 12 24 48 and 96 h respectively the downstream boundary condition is given by a stage discharge relationship in which the steady state discharge q o is corrected to dynamic q with the aid of chow s formula chow 1964 17 q y q o y 1 1 s o dy dx 1 2 for a given flow depth y the initial condition is given as 100 m 3 s uniform flow rate at which the correspondig head is calculated using manning s formula the prismatic channel is discretized into 250 reaches of length δ x 1000 m down to 10 m depending on the situation the overall simulation time is 500 h involving 1000 time steps of δ t 1800 s each the ratio δ t δ x 1 8 constitutes a base configuration which is adapted in accordance with the requirements for the different numerical schemes we also note that for our particular case c 1 m s d 11 000 m 2 s and 10 m δ x 10 3 m thus the péclet number o 10 4 pe c δ x d o 10 2 while the courant number o 10 0 cr c δ t δ x o 10 with o order of magnitude indicating that diffusion clearly dominates convection 5 results the simulation results for the four input waves and the trapezoidal channel are presented in figs 1 4 which show the wave attenuation for the different solutions discussed in section 4 in each figure we present the input wave imposed at the upstream end 0 km and the respective model outputs at distance 100 km including a the sobek model with the full sv solution sbk b the hec ras solutions with and without suppressed local and advective terms ras pa ras c the finite difference solution of the parabolic approximation pa 2 d the head based nl ady 6 and e the nl adq 8 the figures also show the vpm solution using the mct approach with cappelaere pressure term correction mctc todini 2007 f fig 1 shows the case with t peak at 12 h while figs 2 4 show the solutions with t peak at respectively 24 48 and 96 h we performed the same simulations for the rectangular cross section given that the results look very similar they are not shown in fig 1 we have also plotted the result reproduced by fenton fenton 2013 solving his long wave equations purple line 1 for interpretation of the references to colour in fig 1 the reader is referred to the web version of this article 1 from the figures it is clearly visible that there is no difference between the full sv solution and the parabolic approximations pa corroborating the theoretical analysis by ponce et al 1978 that the pa model is valid for the selected bed slope and wave semi period of the input hydrograph moreover the inter comparison of sbk and ras ras pa with the finite difference solution of the pa validates the accuracy of our finite difference approximation and serves as the correct benchmark case for all remaining simulations next the finite difference solutions of the nl ady 6 shows that it is i mass conservative as to be expected table 1 and ii correctly estimates the diffusion and wave celerity for all t p values from 96 to 12 h the small underestimation of q around the peak is due to high sensitivity of the solution to the downstream flow boundary condition which is propagated upstream ideally the channel should be of infinite length to ensure that the surface slope is parallel to the bed slope or dy dx 0 at the downstream end a condition which is only approached asymptotically for channels far larger longer than 250 km the solution of the nl adq 8 also accurately reproduces the heads and discharges with overall mass error slightly larger than for nl ady finally the mct solution is very close to the full sv solution for t peak 96 h and 48 h and starts to deteriorate for t peak 24 h and less still fully mass conservative for t peak 12 h the mct clearly overestimates diffusion as pointed out by fenton 2013 in his note 6 discussion first of all we note that in figs 1 4 the finite difference solution of the nl adq 8 matches the sv and pa solutions for all input hydrographs and results are mass conservative in effect we have reduced the problem to an equation stated in non divergent form 14 because the parameters c and d are effectively constant in x over the 10 m reach segment and vary only in time fig 5 compares the effects of grid spacing on the solution by fixing δ t 300 s and increasing δ x from 10 m cr 30 to 100 m cr 3 and finally 250 m cr 1 2 the solution progressively deteriorates despite the courant numbers decreasing toward the numerically favourable value of 1 patankar 1980 discharge and head level lead both to a non conservative solution which is attributable to the fact that c x d x 0 does not hold over reach segments δ x 10 m the results also show that under specific conditions the vpm which is an approximation of the nl adq leads to numerical solutions that are very close to those of the sv model the results are near coincident with the sv solution for waves with a slow rate of raise of the input wave and deviate from it with increasingly steeper rising inflow hydrographs the divergence is manifested as excessive diffusion while wave celerity is underestimated the reason for the poor diffusion and celerity approximation lies in the fact that for rapid rate of rise of the hydrograph the second order approximation of the continuity equation and numerical diffusion set at par with physical diffusion becomes inaccurate for strongly diffusive situations t p 12 24 h while the approximation is much better for slow waves t p 48 h and a near perfect match for t p 96 h where the wave peak at distance 100 km undergoes mainly translation and is dampened over downstream distances much larger than 100 km see figs 3 and 4 in this context we note that in natural river systems slow waves with time to peak of multiple days are very common while rapidly rising waves with times to peak of 24 h or less constitute artificial situations that occur during weir operations and are far from natural flow behaviour to provide a more quantitative explanation for the vpm behaviour for fast rising waves we refer to the work of szél and gáspár 2000 they formulate the vpm model in terms of péclet and courant numbers with the aim to analyze the numerical scheme in terms of accuracy absence of oscillations and numerical stability from their analysis it emerges that the differential operator in the nl adq equation the basis for the vpm can only be approximated correctly by finite difference approximation if d and c δ x are of the same order of magnitude a condition not met in our particular case given δ x 1000 m δ t 1800 s and c 1 m s in our and the fenton 2013 case d o 10 4 while δ x c o 10 3 a difference of one order of magnitude based on their analysis our situation does not meet the condition of no oscillation either i e absence of the third order error term in the finite difference approximation of the differential operator as pe o 10 1 and thus always smaller than 3 nevertheless given the type of wave propagation phenomenon studied here oscillations practically do not affect the solution it is also straightforward to show that both conditions for strong stability i e cr 1 pe 1 and cr 1 pe 1 are always met in our case as far as numerical diffusion is concerned we cannot meet the condition 0 5 η cr 0 5 θ 0 5 pe of no diffusion for θ 0 5 as in the original paper by cunge 1969 θ and η being the time and space finite difference grid weighting coefficients the solution therefore becomes over diffusive as visible in figs 1 and 2 we also note that zero numerical diffusion can only be approached with negative or zero values of η or θ finally we observe that the use of the vpm on mildly sloping lowland rivers is justified and delivers accurate results as natural hydrographs always correspond to very slow waves with semi periods of multiple days in table 2 we list typical flood waves observed on major natural rivers that are all characterized by flows taking several days or months to peak the last column shows the values of the t s o g y 2 m criterion by ponce et al 1978 a necessary and sufficient condition for the application of the pa model in all cases the minimum threshold value of m 15 is exceeded in this context the vpm approach constitutes a suitable and computationally cheap simulation tool for flood propagation fig 6 depicts the simulation results of a synthetic flood wave propagation along a river of 1000 m width 5 0 10 5 bed slope manning roughness 0 020 s m 1 3 and time to peak 5 days a situation akin to the lower po river in table 2 which is probably one of the most severe conditions with t s o g y 2 20 not far from the threshold value of 15 on the po river a real time flood forecasting system based on the pab parabolic approximation and backwater model todini and bossi 1986 has been successfully used for operational flood forecasting since the 1990 s which corroborates the correctness of the parabolic assumption a typical input wave commences at 2000 m 3 s and reaches a peak of 9000 m 3 s after 5 days which occurred during the po flood of the year 2000 the inter comparison between the pa solution and mct at 0 100 250 and 500 km shows a near perfect match proving the validity of the vpm approach for this type of system 7 summary and conclusions in this paper we have performed wave propagation analysis for four types of hydrographs across a trapezoidal channel with very mild slope of 10 4 effectively reproducing the geometry used by fenton 2013 the same simulations were performed for a rectangular cross section to ensure absence of effects of channel enlargements aim of the analysis was to explore the conservation properties and range of validity of the nl ad model for various flood waves and to investigate the limits of the vpm method an approximation of the nl adq all cases are benchmarked against the solution of the full sv equations all simulated cases with slope 10 4 or less can be modelled with the parabolic approximation pa of the sv equations as acceleration effects are irrelevant the results effectively match the full sv solution the head based nl ady formulation 6 is conservative for reaches where the surface width can be considered approximately constant as it can be stated in non divergent form an accurate solution of the mass conservation equation in terms of q is nevertheless numerically challenging this equation constitutes the basis for a head based vpm formulation the nl adq model perfectly represents the propagation of all four waves however the solution converges to the sv solution and is conservative only if c and d are assumed spatially constant over a reach element i e c x d x 0 as we are integrating the equation with small sized grid spacing of 10 m the two parameters are effectively spatially constant and we are solving a conservative equation of type 14 excessive diffusion for rapidly rising waves pointed out by fenton 2013 arises in situations in which the vpm approximation of 8 breaks down as the relation between diffusion and advection expressed by the courant and péclet numbers becomes unfavourable szél and gáspár 2000 nonetheless the good vpm approximation continues to hold up for real world large river floods where the very mild slopes of the lowlands are accompanied by very slow rising waves for this reason the vpm remains an appealing computationally inexpensive method for representing the flood propagation in an extremely wide variety of real world rivers acknowledgements we acknowledge dipl ing dennis meißner of the german federal office of hydrology bafg who has performed the sobek simulations appendix a the non linear ady equation using the pa model 2 as a starting point we begin by expressing the spatial gradient of y as a gradient of the absolute head z by noting that z y z f with z f the bed elevation and adhere to the convention that the x axis points in the direction of flow a 1 z t y t z x y x s o j we also note that the energy line slope j is given by manning s formula a 2 j n 2 q 2 p w 4 3 a 10 3 with a the cross section area and p w the wetted perimeter and subsequently introduce the hydraulic conveyance k a 3 k 1 n a 5 3 p w 2 3 the conveyance allow us to express q in terms of k and the spatial gradient of z thanks to the parabolic approximation assumption a 4 q kj 1 2 k z x 1 2 k y x s o 1 2 next we express the gradient of the discharge q in term of k a 5 q x k z x 1 2 x k x y x s o 1 2 k 2 y x s o 1 2 2 y x 2 and substitute into the continuity equation to obtain a 6 a t k x y x s o 1 2 k 2 y x s o 1 2 2 y x 2 0 given that a t b y t with b the channel top width we substitute via a 4 in a 6 to get a 7 y t q kb k x q 2 jb 2 y x 2 0 next we expand the derivative k x a 8 k x k y y x k x y cost and substitute in eq a 6 to obtain a non linear parabolic equation a 9 y t q kb k y y x q 2 jb 2 y x 2 q kb k x y cost calculation of dk dy to get dk dy in a 9 we differentiate a 3 with respect to y a 10 dk y dy 1 n 5 3 a 2 3 p w 2 3 da y dy 2 3 a 5 3 p w 5 3 dp y dy for a generic cross section the following relations hold a 11 da y dy b dp y dy 2 sin α 1 which we substitute into eq a 10 to yield a 12 dk y dy k 3 5 b a 4 p w sin α 1 appendix b the non linear adq equation to derive the adq eq 8 we follow the classical procedure used in the literature dooge 1967a dooge 1967b cunge 1969 cappelaere 1997 starting out from the sv equations which have been simplified to the parabolic approximation pa as in eq 2 we first take the derivative of the mass balance equation with respect to x and of the momentum balance equation with respect to t while acknowledging that the infinitesimal cross section area can be approximated as da bdy b 1 2 y x t 1 b y t b x 1 b y t b y y x 1 b 2 q x 2 0 2 y t x j t 0 then we subtract the two equations from each other and collect b 2 y t b x y b y q y x 2 q x 2 b j t the momentum eq 2 states that y x s o j thus we substitute and obtain b 3 y t b x y b y q s o j 2 q 2 x b j t from the continuity equation we know that y t 1 b q x and therefore b 4 1 b q x b x y b y q s o j 2 q 2 x b j t by once more using the continuity equation we can expand and transform the right hand side term in b 4 b 5 b j t b j q y q t j y q y t b j q y q t j y q q x and substitute back to get b 6 b j q y q t 1 b q x b x y b y q s o j 2 q 2 x j y q q x 0 through a further manipulation step and by collecting terms we finally obtain b 7 b j q y q t 1 b b x y j s o b y q q x 2 q 2 x 0 this equation can now be cast into the classical form of the advection diffusion equation by division trough b j q b 8 q t c q x y q x d q x y 2 q x 2 0 where the convection and diffusion coefficients are respectively given by the following expressions b 9 c q x y 1 b 2 j q y b x y j s o b y q d q x y 1 b j q y as the friction slope can be expressed in terms of manning s formula b 10 j n 2 q 2 r h 4 3 a 2 we obtain an expression for the derivative dj dq b 11 j q y 2 n 2 q r h 4 3 a 2 2 j q and therefore obtain the classical expression for the diffusion coefficient b 12 d q 2 bj similarly we obtain an expression for the parameter c b 13 c d 1 b b x y j y q s o j b b y q as a next step we expand the derivative j y q b 14 j y y n 2 q 2 p w 4 3 a 10 3 n 2 q 2 p w 4 3 a 10 3 4 3 1 p w p w y 10 3 1 a a y j 4 3 1 p w p w y 10 3 b a 2 jb q c k 1 d c k with c k the kinematic wave celerity given by the following expression cappelaere 1997 todini and bossi 1986 b 15 c k q a 5 3 1 2 5 a bp w p w y substitution into b 13 gives the following analytical expression for c b 16 c d b b x y c k d s o j b b y q which can be restated as a product between the coefficient d and the expression between square brackets b 17 c d 1 b b x 10 3 jb a 2 5 j p w p w y s o j b b y by considering that the infinitesimal increments of the channel top width b and of the wetted perimeter p w can be expressed via trigonometric relationships by assuming the infinitesimal cross section increments of trapezoidal shape the following relationships hold b 18 b 2 cot α y p w 2 sin α y which can be substituted into b 17 to get c for generic sections b 19 c 5 3 q a 1 4 5 a bp w sin α d b b x 2 s o j tan α rectangular section in the rectangular section case b x b y 0 and therefore b 20 c 1 b j q j y d j y if the section is wide y b r h y and via eq b 10 one gets b 21 j y 10 3 n 2 q 2 b 2 y 13 3 which finally yields the wave propagation speed expression of eq 13 in lighthill and whitham 1955 b 22 c 5 3 q b y appendix c finite difference discretization of eq 2 the governing equations for the parabolic approximation model are provided by 2 the system of two coupled hyperbolic differential equations includes a linear differential equation the mass conservation and a highly non linear equation the momentum conservation the non linearity is introduced by the energy slope j which is a non linear function of q and y we discretize the equations with an implicit four point finite difference scheme whereby we use a weight of 0 5 in space and a weighting parameter 0 θ 1 in time we denote the point x t with the subscript 0 0 while x δ x t δ t is indicated with 1 1 the mixed combinations of indices 0 1 and 1 0 are self explanatory the discretized equations are as follows c 1 y 1 1 y 0 1 δ x s 0 j 1 θ δ x q 1 0 q 0 0 θ δ x q 1 1 q 0 1 b δ t y 1 1 y 1 0 0 with c 2 j n 2 q 1 1 q 1 1 p w 4 3 y 1 1 a 10 3 y 1 1 b 1 θ b y 1 0 θ b y 1 1 we now separate variables with the aim to arrange the equations into a linear system of banded matrices in the four unknowns y 1 1 y 0 1 q 1 1 q 0 1 c 3 δ x n 2 q 1 1 q 1 1 p w 4 3 y 1 1 a 10 3 y 1 1 y 1 1 y 0 1 s 0 δ x θ δ x q 1 1 q 0 1 b y 1 1 δ t 1 θ δ x q 1 0 q 0 0 b y 1 0 δ t we rearrange c 4 δ x n 2 q 1 1 p w 4 3 y 1 1 a 10 3 y 1 1 q 1 1 y 1 1 y 0 1 s 0 δ x q 1 1 q 0 1 b δ x θ δ t y 1 1 1 θ θ q 1 0 q 0 0 b δ x θ δ t y 1 0 and write the system in terms of four submatrices c 5 a 11 a 12 a 21 a 22 q t δ t y t δ t b 1 b 2 where a 11 is a diagonal matrix c 6 a 11 δ x n 2 q 1 1 p w 4 3 y 1 1 a 10 3 y 1 1 0 0 0 δ x n 2 q 1 1 p w 4 3 y 1 1 a 10 3 y 1 1 0 0 0 0 δ x n 2 q 1 1 p w 4 3 y 1 1 a 10 3 y 1 1 while a 12 and a 21 are non symmetric band matrices c 7 a 12 1 1 0 0 0 1 1 0 1 1 0 0 1 a 21 1 0 0 1 1 0 1 1 0 0 0 1 1 and c 8 a 22 diag b δ x θ δ t the boundary conditions are imposed as known q 0 t q t up at the upper channel end and as known y n t y t ds at the lower channel end the value y t ds is calculated from the corresponding discharge value at the previous time step via a stage discharge relationship q y and application of chow s formula 17 to convert the steady state level given by the q y relationship to dynamic after incorporating the boundary conditions the right hand side vectors are given by the following expressions c 9 b 1 δ xs o δ xs o y t δ t ds c 10 b 2 1 θ θ a 21 q t a 22 y t q t δ t up 1 θ θ q t up 0 0 the solution of the system in 2 n 1 equations in as many unknowns can be calculated by performing matrix products trivial inversions of diagonal matrices and by solving a tri diagonal system using the thomas algorithm c 11 y t δ t a 21 a 11 1 a 12 a 22 1 a 21 a 11 1 b 1 v 2 q t δ t a 11 1 b 1 a 11 1 a 12 y t δ t because the diagonal matrix a 11 and right hand side vector depend on both independent variables q t and y t it is necessary to approximate the solution iteratively at every timestep t using the standard picard method celia and bouloutas 1990 the guessed solution q t k 1 and y t k 1 at every new iteration k 1 is linearized in terms of solutions at iterations k and k 1 as in eq 15 
7132,a wide range of real world flood routing problems can be approached using the parabolic approximation widely recognized as a convenient simplification of the saint venant equations from the parabolic approximation head and discharge based linear and non linear advection diffusion models were also derived by merging mass and momentum conservation equations into a single second order equation to be used for one dimensional flood routing both head and discharge based advection diffusion models were originally derived in their linear forms and extensively studied in the literature because they admit analytical solutions the discharge based non linear models have been used as the basis for the derivation of the variable parameter muskingum routing method by setting the truncation error at par with the diffusion parameter whereas the linear models are mass conservative but inadequately reproduce routing in a variety of cases the non linear ones have been recently declared to be non conservative in the literature it has also been stated that the variable parameter muskingum fails to approximate the discharge based non linear discharge based advection diffusion model on mild slopes this paper aims at providing a systematic overview of all these models showing that whereas the non linear head based advection diffusion model an original version of which is derived here is in fact conservative also the discharge based model which in principle is not remains de facto conservative if integrated over a space time grid with sufficiently small spatial cells to complete the overview we also demonstrate that in real world applications the variable parameter muskingum gives satisfactory results on mild slopes because lowland rivers in nature are essentially prone to slowly rising hydrographs the results of all models are intercompared by routing different flood waves through prismatic reference channels with slope 10 4 and smaller keywords 1d channel routing non linear advection diffusion equation parabolic equation mild slope variable parameter muskingum trapezoidal section mass and momentum conservation nomenclature abbreviations l ad linear advection diffusion equation nl adq non lnear discharge based advection diffusion equation nl ady non linear head based advection diffusion equation cr courant number da diffusion analogy pa parabolic approximation pe péclet number sbk sobek sv saint venant equation vpm variable parameter muskingum latin symbols a cross section area of the channel l 2 b channel top width l c convective speed coefficient in adey l t c k kinematic wave celerity l t d diffusion coefficient in adey l 2 t f generic source sink term of ψ term f generic flux term g gravitational acceleration l t 2 i non convective i e diffusive flux k hydraulic conveyance l 3 t j slope of the energy gradient n manning roughness l 1 3 t p pressure m t 2 l p w wetted perimeter l r h hydraulic radius l q discharge l 3 t s o channel bed slope t time t t period of input wave t t peak time to peak of input wave t v velocity vector at the point scale l t v reach volume l 3 x longitudinal channel coordinate l y channel water depth l z absolute free surface head l z f channel bed elevation l greek and mixed symbols α slope angle of prismatic channel side rad deg γ shape parameter for synthetic hydrograph event based scaling parameter 1 s o dy dx from price 1985 η implicit explicit weighting of the spatial grid discretization θ implicit explicit weighting of the temporal grid discretization μ fluid viscosity m lt δ t time increment t τ viscous stress tensor δ x reach segment length l ψ generic hydrodynamic property at the point scale ρ mass density m l 3 ω linearization parameter in the picard method 1 introduction hydraulic flow routing in river channels is a central topic for studying the propagation of flood waves a reliable prediction of the flood peak in terms of flow rates and heads forms the basis for decision making in flood warning and control the advent of large computing power has opened the possibilities for flood propagation in large basins at national regional or even continental scale in europe de roo et al 2003 sampson et al 2015 and the us maidment 2016 continental scale flood warning systems have been implemented which serve as real time forecasting and pre warning platforms in these systems grid based hydrological models are combined with flood routing to seamlessly predict stage and discharge at arbitrary river cross sections while the subject of flow routing has been amply studied in the past open issues still persist one relevant topic is accurate and computationally cheap flow routing for generating ensemble real time streamflow predictions pappenberger et al 2008 while the saint venant sv equations de saint venant 1871 also called dynamic wave model or long wave equations fenton 2010 is the most comprehensive description of 1d channel flow their solution remains computationally expensive especially when large river networks are concerned or multiple simulations are required to asses uncertainty krzysztofowicz 1999 raftery et al 2005 todini 2008 on stage and discharge simpler and computationally leaner solutions such as the kinematic wave model or non linear reservoir routing reggiani et al 2014 cannot be applied to large lowland river systems because the flow behavior that such models describe is essentially translatory while the typical flow dynamics of mildly sloping river systems with bed gradients in the order of 10 4 10 5 is strongly diffusive such behaviour is characterized by the relevance of pressure gradients that are related to the divergence of surface gradient and bed slope in steeper river systems with slopes in the order of 10 3 the surface slope can be considered parallel to the channel bed and thus pressure gradients become largely irrelevant the pressure gradient is also responsible for the hysteretic relationship between stage and discharge during flood propagation in lowland rivers and must be fully accounted for in flood propagation as an alternative to using the full sv equations we could resort to the variable parameter muskingum method vpm cunge 1969 ponce and yevjevich 1978 todini 2007 which is computationally cheap and able to reproduce the stage discharge hysteresis the vpm has been developed by approximating the second order parabolic non linear advection diffusion nl ad equation in terms of discharge nl adq by finite differences and setting the truncation error at par with the diffusion parameter cunge 1969 price 2009 while the vpm model has been shown to perform well for natural channels cappelaere 1997 reggiani et al 2016 its limit behaviour on very mild slope in the presence of steep waves has not been explicitly addressed on this topic fenton 2013 published a note in which he maintains that the vpm cannot reliably reproduce flood waves on slopes in the order of 10 4 or less in the presence of very steep flood waves he attributed this to the second order derivative of the advection diffusion equation approximated by the vpm which causes excessive diffusion following up on gasiorowski and szymkiewicz 2007 who showed that the l ad equation is always conservative while nl adq is inherently non conservative we extended their analysis to study the conservation properties of the nl adq equation from which the vpm has been derived first we solve the simplified sv equations which form the basis for the parabolic approximation pa model and compare the results to the full sv solution for prismatic channels second we derive a conservative head based formulation of the ad equation nl ady from the pa model which yields results that are very close to the full sv and the pa solutions also on very mild slopes third we investigate under which circumstances the solution of the nl adq equation which underlies the vpm can become de facto conservative and if it still allows for adequate wave propagation on mild slopes and fast rising waves we also study under which conditions the vpm can be used for flood propagation in such flow regimes then we report on the ways in which the ad equation in its various forms linear non linear discharge based or stage based formulation has been historically derived from the sv equations fourth we route four flood waves with different semiperiods across mildly sloping prismatic channels demonstrating that the nl adq indeed provides an accurate description and is conservative under conditions of convection and diffusion parameters that remain spatially but not necessarily time constant over a reach element a condition that is satisfied when the equation is integrated by finite differences over a fine spatial grid fifth and finally we compare the vpm against all previous results and show that it continues to provide an accurate solution for slowly rising flood waves a condition met in most natural lowland rivers the method becomes excessively diffusive with increasingly steep flood waves due to spurious numerical diffusion nevertheless the vpm continues to hold up as a computationally inexpensive method for accurate stage discharge flood propagation analysis in natural rivers the paper is structured as follows section 2 provides an overview of the advection diffusion equations and their derivation in section 3 we discuss the models and the numerical approaches used for numerical integration in section 4 we describe the setup for numerical experimentation section 5 describes the results while section 6 is devoted to the discussion conclusions are drawn in section 7 2 parabolic models for discharge routing 2 1 parabolic approximation pa the most complete but not exact flow description in a channel is provided by the sv equations de saint venant 1871 or mass and momentum conservation 1 a t q x 0 q t q 2 a x ga y x ga s o j this system of hyperbolic differential equations is also known in the literature as the dynamic wave model kundewicz 1985 or as long wave equations model fenton 2010 ponce et al 1978 showed that for a value of t s o g y 2 15 with t the period of a sinusoidal input hydrograph the momentum equation can be simplified by dropping the local and convective acceleration terms this hypothesis has been further analyzed and elaborated by morris and woolhiser 1980 daluz vieira 1983 moussa and bocquillon 1996 as a result we obtain a simplified equation system called also parabolic approximation pa or diffusion wave henderson 1966 weinmann and laurenson 1979 whereby we prefer to use the former denomination as it mathematically correctly classifies the equation system 2 a t q x 0 y x s o j 0 the system of coupled differential eq 2 with a highly non linear momentum equation can be solved directly in terms of water depth y and discharge q it will be shown that the pa model closely reproduces the sv results within a wide spectrum of very mild bed slopes and steep inflow waves 2 2 the advection diffusion ad model here we discuss the derivation of the advection diffusion model which is often also referred to as diffusion analogy model several authors have in fact proposed to merge the two equations of the pa into a single parabolic differential equation formulated in linear or non linear form in either one of the two variables flow depth y or discharge q 2 2 1 linear advection diffusion l ad hayami 1951 was among the first to observe that wave propagation in a channel was analogous to a advection diffusion process in which a perturbation moves with a characteristic wave speed and is simultaneously dispersed due to turbulence and shear effects taylor 1953 he amended the continuity equation for a channel of unit width formulated in terms of water depth y with the ad hoc introduction of a dispersive flux obtaining a non linear parabolic equation in y hayami then linearised the equation as he looked for an analytical solution by means of a series expansion a similar approach was also proposed by lighthill and whitham 1955 who started from the linearization of the sv equations around a constant reference discharge and depth q o and y o demye 1938 masse 1938 for a channel of unit width 3 gy o u o 2 2 q x 2 2 u o 2 q x t 2 q t 2 3 gs o q x 2 gs o u o q t where u o q o y o is a reference velocity this is a linear hyperbolic differential equation also known as telegraph equation valid for small perturbations q and y in which the second and third term on the left hand side can be neglected to retain a parabolic equation a condition rarely met during a flood event koussis 1976 dooge 1967a and dooge 1967b also obtained a very similar linear parabolic wave models by introducing kinematic wave approximations into 3 after neglecting all three left hand side terms all of these parabolic equations are often considered linear diffusion analogies da applied to flood routing kundewicz 1985 with only minor mutual differences in terms of parameter expressions 2 2 2 non linear head based ad equation nl ady leaning on hayami s work on linear flood routing we also derive a conservative non linear advection diffusion model nl ady formulated in terms of water depth y using the pa model 2 as a starting point it is possible to obtain an nl ady equation by expressing the discharge q as a function of the energy line slope j and the hydraulic conveyance k and then substituting for j from the second of eq 2 4 q kj 1 2 k z x 1 2 k y x s o 1 2 by substituting for q in the first of eq 2 one obtains 5 y t 1 b k y x s o 1 2 x 0 which is a non linear da equation written in non divergent form see section 2 3 below for reaches where the surface top width b can be considered constant after a number of algebraic manipulations described in appendix a one obtains 6 y t c y x d 2 y x 2 e where c q kb k y d q 2 jb and e q kb k x y const with the r h s term e equal to zero for prismatic channels the derivative k y for generic cross sections is given by eq a 12 this second order differential equation must be supplemented with suitable initial condition and two boundary conditions 7 y x 0 y o x y 0 t y t up dy x ds t dx s o j t ds whereby the downstream condition is a direct consequence of the pa model stated in eq 2 respectively a 1 2 2 3 non linear discharge based ad equation nl adq some years after hayami s derivation the non linear advection diffusion was studied as a direct derivation from the sv equations cunge 1969 koussis 1976 price 1985 to develop the variable parameter muskingum method cunge approximated the discharge based nl ad equation nl adq which he obtained by differentiating the momentum equation of the pa system 2 with respect to x and the continuity equation with respect to t and subtracting them a technique adopted earlier by dooge 1967b as an alternative way in getting a linear ad equation directly from the linearized eq 2 8 q t c q y y x q x d q y y x 2 q x 2 0 in this expression c is a convective wave speed and d the diffusion coefficient responsible for wave attenuation both parameters are non linear functions of q and the exogenous variables y and surface gradient y x later cappelaere 1997 showed that these quantities can be approximated as functions of q and q x thus ridding 8 of the dependence on y the details for the derivation of 8 including the coefficients c and d for arbitrary channel cross sections are given in appendix b eq 8 must be supplemented with an initial condition and two boundary conditions 9 q x 0 q o x q 0 t q t up dq x ds t dx da x ds t dt whereby the downstream boundary condition follows from the continuity equation price 1985 performed a different analysis on the full sv equation system 1 after introducing the event based dimensionless scaling parameter defined as the ratio of head gradient dy dx over bed slope s o by assuming fr 2 1 with fr the froude number he analyzed the momentum equation and ranked terms by various order in he performed various substitutions by exploiting the continuity equation with the goal to eliminate y as a dependent variable in the end he obtained a non linear hyperbolic equation in only discharge containing a mixed space time derivative for negligible lateral inflow his flow routing equation is of the form 10 q t c o q x c o t a o c o 2 q x 0 where c o is the wave celerity and a o is the wave diffusivity this equation can be solved with only the initial and a single boundary condition later price 2009 showed that his advection diffusion eq 10 is to order o equivalent to eq 8 whereby the ratio a o c o 2 can be taken out of the derivative and the mixed derivative is replaced ad hoc by a second order spatial differential operator 2 3 conservation the conservation equations for a generic hydrodynamic property ρ ψ i e mass momentum or energy of a fluid at the microscale can be written in tensor notation as follows eringen 1980 11 ρ ψ t ρ ψ v i ρ f where i is a diffusive flux and f is an external supply or sink of ψ to obtain point scale mass and momentum equations ψ is set equal to respectively 1 and v i to respectively the zero or the stress tensor t p i τ with p the pressure i the identity matrix and τ μ v the viscous shear stress tensor while f is set equal to respectively zero or to the gravity vector g eq 11 constitutes a global balance law which in the one dimensional case reduces to 12 ρ ψ t x f a f d ρ f with f a and f d local advective and diffusive fluxes of ψ in x direction both water depth y and discharge q based l ad equations as well as the nl ady equation can be cast in non divergent form whereas the nl adq 8 with variable parameters cannot as demonstrated by gasiorowski and szymkiewicz 2007 the same applies to the hyperbolic eq 10 price 1985 proving that both cases are formally non conservative nevertheless it is possible to notice that the non divergent form of the nl adq 13 q t x c q d q x 0 collapses into 14 q t c q x d 2 q x 2 0 for spatially constant parameters c and d which corresponds to a locally linearized nl adq therefore the nl adq model can be made conservative when integrated using a finite difference approach by locally linearizing the equation in space keeping c and d constant over each grid cell similarly to what is done when integrating the sv equations and their parabolic approximation 2 2 4 fenton s 2013 analysis price 2009 demonstrated some equivalence between 8 and 10 on the basis of perturbation analysis more recently fenton 2013 concluded that for a trapezoidal channel on a mild slope of 10 4 respective analytical solutions of exponential type for the two equations diverge due to excessive diffusion induced by the mixed derivative term in 10 he concluded that similar models based on the discretization of 10 fail in correctly representing wave propagation on nearly flat channels this concerns the finite difference discretization of 10 proposed by price 2009 and the vpm method cunge 1969 ponce and yevjevich 1978 todini 2007 which he mistakenly considers an approximation of 10 instead of 8 it is also important to note that in his analysis he started from the correct assumption that the nl adq 8 or eq 1 in his original paper is a correct description of wave propagation while the hyperbolic eq 10 containing the mixed space time derivative is its approximation in this paper we investigate the numerical solution of the nl adq 8 on mild slopes using fenton s prismatic trapezoidal channel and prove that it constitutes indeed a valid flow representation and thus has been correctly used as basis for the vpm approximation by cunge we will also show that the vpm becomes inaccurate in cases where peclet and courant numbers differ considerably 3 numerical solutions to solve the equations for wave propagation introduced so far we resort to a existing sv solvers and b implicit finite difference solvers developed by ourselves we also use the vpm solver based on the mct approach described in todini 2007 the models and their implementation are presented hereunder 3 1 solution of the full sv equation the solution of the full sv eq 1 is performed with the finite difference staggered grid solver sobek stelling and duinmeijer 2003 similarly to fassoni andrade et al 2018 we performed the same computations for verification purposes with the packages hec ras usace 1990 and mike11 dhi 2009 obtaining matching results thus we proceed using sobek and hec ras only the solution obtained with sobek is considered the reference case against which we benchmark the pa model 2 nl ady 6 respectively a 9 nl adq 8 and the vpm 3 2 solution of the pa model equations the solution of system 2 with the continuity equation and the simplified momentum conservation equation in which the local and convective acceleration have been dropped is performed in two ways 1 with the aid of the hec ras model in which the two terms are suppressed at run time 2 with the help of a finite difference solver that we have especially developed the system 2 consists of a linear and a highly non linear hyperbolic equation which are mutually coupled through the dependence on y and q to this end we approximate the equations with a 4 point implicit finite difference scheme yielding a system in 2 n 1 equations in the same amount of unknowns where n is the total number of reach segments the discretization is shown in more detail in appendix c and leads to a linear system with four sub matrices the matrices a 11 and a 22 are diagonal while a 12 and a 21 are non symmetrical incidence matrices with constant non zero elements equal to 1 and 1 because a 11 and a 22 are non linear functions of the vectors y and q some linearization must be introduced the standard picard iteration method celia and bouloutas 1990 applied here involves sequential estimation of the unknowns y t k 1 and q t k 1 at time t as weighted averages of the latest estimates at iterations k and k 1 15 q t k 1 q t k ω q t k 1 ω 1 y t k 1 y t k ω y t k 1 ω 1 the value of ω is chosen between 0 5 ω 1 and has an effect on the convergence rate for practical purposes we chose a value of ω 0 75 weighting the solution at step k higher as the one at k 1 for the estimate at k 1 from eq c 11 it can be seen that the solution of the coupled equation system reduces to matrix products involves simple inversion of diagonal matrices and the solution of a tri diagonal linear system these operations can be computationally optimized making this model appealing for fast flow routing we note that a sufficiently accurate solution can be obtained for time steps δ t 1800 s and reach lengths δ x 1000 m 3 3 solution of the nl ady equation to tackle the solution of 6 we propose an implicit 6 point finite difference solver the differential equation is second order in y and contains variable parameters c and d which have been derived for generic cross sections in appendix a and depend non linearly on y and q the continuity equation needs to be solved simultaneously through integration from the downstream boundary upwards the continuity equation requires the imposition of q t ds as a boundary condition which can be calculated from the downstream water level given by the parabolic equation and converted to discharge via the q y relationship the parabolic equation requires two boundary conditions which are given by eq 7 the guessed solution vector y t k 1 at iteration k 1 is estimated using the picard iteration method we note that solving the nl ady for the proposed application is computationally demanding as acceptable solution accuracy requires using small δ t 300 s and reach segments with δ x 10 m δ x δ t 0 03 which considerably increases the calculation time with respect to solving the pa model equations moreover the equation needs to be made fully implicit by weighting the finite difference scheme entirely on the solution at time step n 1 3 4 solution of the nl adq equation solving 8 is similar to the solution of the nl ady equation as the problem is formulated in only q we need to integrate the backwater profile using the standard step method henderson 1966 to obtain corresponding y values because of the mutual interdependence with the profile integration the solution needs to be approximated iteratively at each time step by alternatingly solving the parabolic equation and the profile integration for the estimation of the boundary water level y t ds required for the profile integration the y q relationship is used the solution of the parabolic equation requires the boundary conditions indicated in eq 9 also in this case successive approximation of the solution at each time step is required whereby the guessed solution is approximated through linearisation as done previously concerning the conditions on step size δ t needs to be reduced to 300 s and δ x to 10 m as otherwise accuracy of the solution deteriorates 4 simulations 4 1 experimental setup next we carry out numerical simulations in which we solve the full sv eq 1 and the pa model 2 and compare the solutions with those of the non linear adq and ady the solutions are also compared against the mass and momentum conservative vpm solution todini 2007 reggiani et al 2016 the equations are forced at the upper boundary with synthetic hydrographs described in section 4 2 below the entry hydrographs have different wave semi periods with time to peak t peak ensuring that slow elongated events are compared against faster ones with increasingly steeper input wave the acceleration terms become progressively larger nevertheless the criterion t s o g y 2 15 ponce et al 1978 is satisfied in all four cases justifying the use of the pa eq 2 to enable an unbiased inter comparison the simulations were performed in an uniform trapezoidal reference channel of 250 km length 40 m base width side walls with width to depth ratio 0 5 uniform bed slope s o 10 4 a manning roughness of 0 035 s m 1 3 and a rectangular channel of width 40 m the trapezoidal section is the same as the geometry used by fenton 2013 in his note the rectangular section has been chosen to represent a situation for which it can be ensured that b x y 0 and b y q 0 and therefore additional terms appearing in the convection coefficient vanish see appendices a resp b 4 2 boundary and initial conditions the upper inflow boundary condition is given by four synthetic hydrographs described by the following exponential function price 1985 16 q t q base q peak q base t t peak e 1 t t peak γ where the curvature parameter γ 5 the base flow q base 100 m 3 s and the peak discharged q peak 400 m 3 s we develop four hydrographs with different wave semi periods to be able to reproduce increasingly steep raising flood waves this is achieved by selecting t peak 12 24 48 and 96 h respectively the downstream boundary condition is given by a stage discharge relationship in which the steady state discharge q o is corrected to dynamic q with the aid of chow s formula chow 1964 17 q y q o y 1 1 s o dy dx 1 2 for a given flow depth y the initial condition is given as 100 m 3 s uniform flow rate at which the correspondig head is calculated using manning s formula the prismatic channel is discretized into 250 reaches of length δ x 1000 m down to 10 m depending on the situation the overall simulation time is 500 h involving 1000 time steps of δ t 1800 s each the ratio δ t δ x 1 8 constitutes a base configuration which is adapted in accordance with the requirements for the different numerical schemes we also note that for our particular case c 1 m s d 11 000 m 2 s and 10 m δ x 10 3 m thus the péclet number o 10 4 pe c δ x d o 10 2 while the courant number o 10 0 cr c δ t δ x o 10 with o order of magnitude indicating that diffusion clearly dominates convection 5 results the simulation results for the four input waves and the trapezoidal channel are presented in figs 1 4 which show the wave attenuation for the different solutions discussed in section 4 in each figure we present the input wave imposed at the upstream end 0 km and the respective model outputs at distance 100 km including a the sobek model with the full sv solution sbk b the hec ras solutions with and without suppressed local and advective terms ras pa ras c the finite difference solution of the parabolic approximation pa 2 d the head based nl ady 6 and e the nl adq 8 the figures also show the vpm solution using the mct approach with cappelaere pressure term correction mctc todini 2007 f fig 1 shows the case with t peak at 12 h while figs 2 4 show the solutions with t peak at respectively 24 48 and 96 h we performed the same simulations for the rectangular cross section given that the results look very similar they are not shown in fig 1 we have also plotted the result reproduced by fenton fenton 2013 solving his long wave equations purple line 1 for interpretation of the references to colour in fig 1 the reader is referred to the web version of this article 1 from the figures it is clearly visible that there is no difference between the full sv solution and the parabolic approximations pa corroborating the theoretical analysis by ponce et al 1978 that the pa model is valid for the selected bed slope and wave semi period of the input hydrograph moreover the inter comparison of sbk and ras ras pa with the finite difference solution of the pa validates the accuracy of our finite difference approximation and serves as the correct benchmark case for all remaining simulations next the finite difference solutions of the nl ady 6 shows that it is i mass conservative as to be expected table 1 and ii correctly estimates the diffusion and wave celerity for all t p values from 96 to 12 h the small underestimation of q around the peak is due to high sensitivity of the solution to the downstream flow boundary condition which is propagated upstream ideally the channel should be of infinite length to ensure that the surface slope is parallel to the bed slope or dy dx 0 at the downstream end a condition which is only approached asymptotically for channels far larger longer than 250 km the solution of the nl adq 8 also accurately reproduces the heads and discharges with overall mass error slightly larger than for nl ady finally the mct solution is very close to the full sv solution for t peak 96 h and 48 h and starts to deteriorate for t peak 24 h and less still fully mass conservative for t peak 12 h the mct clearly overestimates diffusion as pointed out by fenton 2013 in his note 6 discussion first of all we note that in figs 1 4 the finite difference solution of the nl adq 8 matches the sv and pa solutions for all input hydrographs and results are mass conservative in effect we have reduced the problem to an equation stated in non divergent form 14 because the parameters c and d are effectively constant in x over the 10 m reach segment and vary only in time fig 5 compares the effects of grid spacing on the solution by fixing δ t 300 s and increasing δ x from 10 m cr 30 to 100 m cr 3 and finally 250 m cr 1 2 the solution progressively deteriorates despite the courant numbers decreasing toward the numerically favourable value of 1 patankar 1980 discharge and head level lead both to a non conservative solution which is attributable to the fact that c x d x 0 does not hold over reach segments δ x 10 m the results also show that under specific conditions the vpm which is an approximation of the nl adq leads to numerical solutions that are very close to those of the sv model the results are near coincident with the sv solution for waves with a slow rate of raise of the input wave and deviate from it with increasingly steeper rising inflow hydrographs the divergence is manifested as excessive diffusion while wave celerity is underestimated the reason for the poor diffusion and celerity approximation lies in the fact that for rapid rate of rise of the hydrograph the second order approximation of the continuity equation and numerical diffusion set at par with physical diffusion becomes inaccurate for strongly diffusive situations t p 12 24 h while the approximation is much better for slow waves t p 48 h and a near perfect match for t p 96 h where the wave peak at distance 100 km undergoes mainly translation and is dampened over downstream distances much larger than 100 km see figs 3 and 4 in this context we note that in natural river systems slow waves with time to peak of multiple days are very common while rapidly rising waves with times to peak of 24 h or less constitute artificial situations that occur during weir operations and are far from natural flow behaviour to provide a more quantitative explanation for the vpm behaviour for fast rising waves we refer to the work of szél and gáspár 2000 they formulate the vpm model in terms of péclet and courant numbers with the aim to analyze the numerical scheme in terms of accuracy absence of oscillations and numerical stability from their analysis it emerges that the differential operator in the nl adq equation the basis for the vpm can only be approximated correctly by finite difference approximation if d and c δ x are of the same order of magnitude a condition not met in our particular case given δ x 1000 m δ t 1800 s and c 1 m s in our and the fenton 2013 case d o 10 4 while δ x c o 10 3 a difference of one order of magnitude based on their analysis our situation does not meet the condition of no oscillation either i e absence of the third order error term in the finite difference approximation of the differential operator as pe o 10 1 and thus always smaller than 3 nevertheless given the type of wave propagation phenomenon studied here oscillations practically do not affect the solution it is also straightforward to show that both conditions for strong stability i e cr 1 pe 1 and cr 1 pe 1 are always met in our case as far as numerical diffusion is concerned we cannot meet the condition 0 5 η cr 0 5 θ 0 5 pe of no diffusion for θ 0 5 as in the original paper by cunge 1969 θ and η being the time and space finite difference grid weighting coefficients the solution therefore becomes over diffusive as visible in figs 1 and 2 we also note that zero numerical diffusion can only be approached with negative or zero values of η or θ finally we observe that the use of the vpm on mildly sloping lowland rivers is justified and delivers accurate results as natural hydrographs always correspond to very slow waves with semi periods of multiple days in table 2 we list typical flood waves observed on major natural rivers that are all characterized by flows taking several days or months to peak the last column shows the values of the t s o g y 2 m criterion by ponce et al 1978 a necessary and sufficient condition for the application of the pa model in all cases the minimum threshold value of m 15 is exceeded in this context the vpm approach constitutes a suitable and computationally cheap simulation tool for flood propagation fig 6 depicts the simulation results of a synthetic flood wave propagation along a river of 1000 m width 5 0 10 5 bed slope manning roughness 0 020 s m 1 3 and time to peak 5 days a situation akin to the lower po river in table 2 which is probably one of the most severe conditions with t s o g y 2 20 not far from the threshold value of 15 on the po river a real time flood forecasting system based on the pab parabolic approximation and backwater model todini and bossi 1986 has been successfully used for operational flood forecasting since the 1990 s which corroborates the correctness of the parabolic assumption a typical input wave commences at 2000 m 3 s and reaches a peak of 9000 m 3 s after 5 days which occurred during the po flood of the year 2000 the inter comparison between the pa solution and mct at 0 100 250 and 500 km shows a near perfect match proving the validity of the vpm approach for this type of system 7 summary and conclusions in this paper we have performed wave propagation analysis for four types of hydrographs across a trapezoidal channel with very mild slope of 10 4 effectively reproducing the geometry used by fenton 2013 the same simulations were performed for a rectangular cross section to ensure absence of effects of channel enlargements aim of the analysis was to explore the conservation properties and range of validity of the nl ad model for various flood waves and to investigate the limits of the vpm method an approximation of the nl adq all cases are benchmarked against the solution of the full sv equations all simulated cases with slope 10 4 or less can be modelled with the parabolic approximation pa of the sv equations as acceleration effects are irrelevant the results effectively match the full sv solution the head based nl ady formulation 6 is conservative for reaches where the surface width can be considered approximately constant as it can be stated in non divergent form an accurate solution of the mass conservation equation in terms of q is nevertheless numerically challenging this equation constitutes the basis for a head based vpm formulation the nl adq model perfectly represents the propagation of all four waves however the solution converges to the sv solution and is conservative only if c and d are assumed spatially constant over a reach element i e c x d x 0 as we are integrating the equation with small sized grid spacing of 10 m the two parameters are effectively spatially constant and we are solving a conservative equation of type 14 excessive diffusion for rapidly rising waves pointed out by fenton 2013 arises in situations in which the vpm approximation of 8 breaks down as the relation between diffusion and advection expressed by the courant and péclet numbers becomes unfavourable szél and gáspár 2000 nonetheless the good vpm approximation continues to hold up for real world large river floods where the very mild slopes of the lowlands are accompanied by very slow rising waves for this reason the vpm remains an appealing computationally inexpensive method for representing the flood propagation in an extremely wide variety of real world rivers acknowledgements we acknowledge dipl ing dennis meißner of the german federal office of hydrology bafg who has performed the sobek simulations appendix a the non linear ady equation using the pa model 2 as a starting point we begin by expressing the spatial gradient of y as a gradient of the absolute head z by noting that z y z f with z f the bed elevation and adhere to the convention that the x axis points in the direction of flow a 1 z t y t z x y x s o j we also note that the energy line slope j is given by manning s formula a 2 j n 2 q 2 p w 4 3 a 10 3 with a the cross section area and p w the wetted perimeter and subsequently introduce the hydraulic conveyance k a 3 k 1 n a 5 3 p w 2 3 the conveyance allow us to express q in terms of k and the spatial gradient of z thanks to the parabolic approximation assumption a 4 q kj 1 2 k z x 1 2 k y x s o 1 2 next we express the gradient of the discharge q in term of k a 5 q x k z x 1 2 x k x y x s o 1 2 k 2 y x s o 1 2 2 y x 2 and substitute into the continuity equation to obtain a 6 a t k x y x s o 1 2 k 2 y x s o 1 2 2 y x 2 0 given that a t b y t with b the channel top width we substitute via a 4 in a 6 to get a 7 y t q kb k x q 2 jb 2 y x 2 0 next we expand the derivative k x a 8 k x k y y x k x y cost and substitute in eq a 6 to obtain a non linear parabolic equation a 9 y t q kb k y y x q 2 jb 2 y x 2 q kb k x y cost calculation of dk dy to get dk dy in a 9 we differentiate a 3 with respect to y a 10 dk y dy 1 n 5 3 a 2 3 p w 2 3 da y dy 2 3 a 5 3 p w 5 3 dp y dy for a generic cross section the following relations hold a 11 da y dy b dp y dy 2 sin α 1 which we substitute into eq a 10 to yield a 12 dk y dy k 3 5 b a 4 p w sin α 1 appendix b the non linear adq equation to derive the adq eq 8 we follow the classical procedure used in the literature dooge 1967a dooge 1967b cunge 1969 cappelaere 1997 starting out from the sv equations which have been simplified to the parabolic approximation pa as in eq 2 we first take the derivative of the mass balance equation with respect to x and of the momentum balance equation with respect to t while acknowledging that the infinitesimal cross section area can be approximated as da bdy b 1 2 y x t 1 b y t b x 1 b y t b y y x 1 b 2 q x 2 0 2 y t x j t 0 then we subtract the two equations from each other and collect b 2 y t b x y b y q y x 2 q x 2 b j t the momentum eq 2 states that y x s o j thus we substitute and obtain b 3 y t b x y b y q s o j 2 q 2 x b j t from the continuity equation we know that y t 1 b q x and therefore b 4 1 b q x b x y b y q s o j 2 q 2 x b j t by once more using the continuity equation we can expand and transform the right hand side term in b 4 b 5 b j t b j q y q t j y q y t b j q y q t j y q q x and substitute back to get b 6 b j q y q t 1 b q x b x y b y q s o j 2 q 2 x j y q q x 0 through a further manipulation step and by collecting terms we finally obtain b 7 b j q y q t 1 b b x y j s o b y q q x 2 q 2 x 0 this equation can now be cast into the classical form of the advection diffusion equation by division trough b j q b 8 q t c q x y q x d q x y 2 q x 2 0 where the convection and diffusion coefficients are respectively given by the following expressions b 9 c q x y 1 b 2 j q y b x y j s o b y q d q x y 1 b j q y as the friction slope can be expressed in terms of manning s formula b 10 j n 2 q 2 r h 4 3 a 2 we obtain an expression for the derivative dj dq b 11 j q y 2 n 2 q r h 4 3 a 2 2 j q and therefore obtain the classical expression for the diffusion coefficient b 12 d q 2 bj similarly we obtain an expression for the parameter c b 13 c d 1 b b x y j y q s o j b b y q as a next step we expand the derivative j y q b 14 j y y n 2 q 2 p w 4 3 a 10 3 n 2 q 2 p w 4 3 a 10 3 4 3 1 p w p w y 10 3 1 a a y j 4 3 1 p w p w y 10 3 b a 2 jb q c k 1 d c k with c k the kinematic wave celerity given by the following expression cappelaere 1997 todini and bossi 1986 b 15 c k q a 5 3 1 2 5 a bp w p w y substitution into b 13 gives the following analytical expression for c b 16 c d b b x y c k d s o j b b y q which can be restated as a product between the coefficient d and the expression between square brackets b 17 c d 1 b b x 10 3 jb a 2 5 j p w p w y s o j b b y by considering that the infinitesimal increments of the channel top width b and of the wetted perimeter p w can be expressed via trigonometric relationships by assuming the infinitesimal cross section increments of trapezoidal shape the following relationships hold b 18 b 2 cot α y p w 2 sin α y which can be substituted into b 17 to get c for generic sections b 19 c 5 3 q a 1 4 5 a bp w sin α d b b x 2 s o j tan α rectangular section in the rectangular section case b x b y 0 and therefore b 20 c 1 b j q j y d j y if the section is wide y b r h y and via eq b 10 one gets b 21 j y 10 3 n 2 q 2 b 2 y 13 3 which finally yields the wave propagation speed expression of eq 13 in lighthill and whitham 1955 b 22 c 5 3 q b y appendix c finite difference discretization of eq 2 the governing equations for the parabolic approximation model are provided by 2 the system of two coupled hyperbolic differential equations includes a linear differential equation the mass conservation and a highly non linear equation the momentum conservation the non linearity is introduced by the energy slope j which is a non linear function of q and y we discretize the equations with an implicit four point finite difference scheme whereby we use a weight of 0 5 in space and a weighting parameter 0 θ 1 in time we denote the point x t with the subscript 0 0 while x δ x t δ t is indicated with 1 1 the mixed combinations of indices 0 1 and 1 0 are self explanatory the discretized equations are as follows c 1 y 1 1 y 0 1 δ x s 0 j 1 θ δ x q 1 0 q 0 0 θ δ x q 1 1 q 0 1 b δ t y 1 1 y 1 0 0 with c 2 j n 2 q 1 1 q 1 1 p w 4 3 y 1 1 a 10 3 y 1 1 b 1 θ b y 1 0 θ b y 1 1 we now separate variables with the aim to arrange the equations into a linear system of banded matrices in the four unknowns y 1 1 y 0 1 q 1 1 q 0 1 c 3 δ x n 2 q 1 1 q 1 1 p w 4 3 y 1 1 a 10 3 y 1 1 y 1 1 y 0 1 s 0 δ x θ δ x q 1 1 q 0 1 b y 1 1 δ t 1 θ δ x q 1 0 q 0 0 b y 1 0 δ t we rearrange c 4 δ x n 2 q 1 1 p w 4 3 y 1 1 a 10 3 y 1 1 q 1 1 y 1 1 y 0 1 s 0 δ x q 1 1 q 0 1 b δ x θ δ t y 1 1 1 θ θ q 1 0 q 0 0 b δ x θ δ t y 1 0 and write the system in terms of four submatrices c 5 a 11 a 12 a 21 a 22 q t δ t y t δ t b 1 b 2 where a 11 is a diagonal matrix c 6 a 11 δ x n 2 q 1 1 p w 4 3 y 1 1 a 10 3 y 1 1 0 0 0 δ x n 2 q 1 1 p w 4 3 y 1 1 a 10 3 y 1 1 0 0 0 0 δ x n 2 q 1 1 p w 4 3 y 1 1 a 10 3 y 1 1 while a 12 and a 21 are non symmetric band matrices c 7 a 12 1 1 0 0 0 1 1 0 1 1 0 0 1 a 21 1 0 0 1 1 0 1 1 0 0 0 1 1 and c 8 a 22 diag b δ x θ δ t the boundary conditions are imposed as known q 0 t q t up at the upper channel end and as known y n t y t ds at the lower channel end the value y t ds is calculated from the corresponding discharge value at the previous time step via a stage discharge relationship q y and application of chow s formula 17 to convert the steady state level given by the q y relationship to dynamic after incorporating the boundary conditions the right hand side vectors are given by the following expressions c 9 b 1 δ xs o δ xs o y t δ t ds c 10 b 2 1 θ θ a 21 q t a 22 y t q t δ t up 1 θ θ q t up 0 0 the solution of the system in 2 n 1 equations in as many unknowns can be calculated by performing matrix products trivial inversions of diagonal matrices and by solving a tri diagonal system using the thomas algorithm c 11 y t δ t a 21 a 11 1 a 12 a 22 1 a 21 a 11 1 b 1 v 2 q t δ t a 11 1 b 1 a 11 1 a 12 y t δ t because the diagonal matrix a 11 and right hand side vector depend on both independent variables q t and y t it is necessary to approximate the solution iteratively at every timestep t using the standard picard method celia and bouloutas 1990 the guessed solution q t k 1 and y t k 1 at every new iteration k 1 is linearized in terms of solutions at iterations k and k 1 as in eq 15 
7133,flood forecasting is a pre emptive non structural measure used to mitigate inundation most current flood forecasting techniques incorporate complex processes such as training and optimization before the technique can be applied conventional flood forecasting techniques based on flood volume provide alerts even if there is no significant risk of flood damage in this study a new flood forecasting technique has been developed based on likely flood damage using the multi dimensional flood damage analysis method this new flood forecasting technique overcomes the drawbacks of current flood forecasting techniques because it can be easily applied using rainfall data the studied drainage area was divided into subareas and the damage functions were obtained for each subarea using the flood volumes and damage information using these damage functions the rainfall intensity when the flood damage initially occurred was calculated for each duration and subarea the damage graph produced for flood forecasting in each subarea identified the rainfall intensities and durations that resulted from the initial occurrence of flood damage this new flood forecasting technique could be used to save lives valuable assets and manage drainage areas keywords flood forecasting flood volume flood damage damage graph 1 introduction climate change has caused extreme rainfall events to occur and the frequency of these events has increased the proportion of impermeable areas as well as the frequency of extreme rainfall events has increased sharply the number of floods has increased and various measures such as structural and nonstructural measures for preventing floods have been prepared to prepare for or prevent flood damage in drainage areas one of the nonstructural measures that has been implemented is the application of various types of flood forecasting models for example a flood forecasting model was suggested and applied in three uk catchments and real time flood forecasting was proposed beven et al 1984 a distributed model using digital elevation models garrote and bras 1995 and short term rainfall prediction models toth et al 2000 have both been developed for real time flood forecasting jasper et al 2002 coupled meteorological observations and a distributed hydrological model for advanced flood forecasting recursive state space estimation young 2002 and soil moisture updated by ensemble kalman filtering komma et al 2008 have been suggested for real time flood forecasting it has been difficult to apply these methods to small watersheds such as single drainage areas because the time intervals for recording the rainfall data used in previous studies were not small enough to capture the detail required for accurate predictions at smaller scales time consuming detailed analysis using 2d hydrodynamic models is appropriate as part of the preliminary work for flood forecasting but the time required for real time forecasting should be minimal new approaches such as applying neural networks distributed hydrological modeling an ingredients based methodology and machine learning have also been introduced neural networks were developed as computer models based on the human brain and nervous system with mathematics and algorithms called threshold logic mcculloch and pitts 1943 robert hecht niesen defined one as a computing system made up of a number of simple highly interconnected processing elements which process information by their dynamic state response to external inputs caudill 1989 a neural network has been applied to river flood and flash flood forecasting campolo et al 1999 and quantitative flood forecasting using multisensory data and neural networks was proposed by kim and barros 2001 research using neural networks combined with other techniques has been suggested for flood forecasting machine learning and neural network techniques were combined for flood forecasting by solomatine and xue 2004 chau et al 2005 compared two hybrid models the genetic algorithm based artificial neural network and the adaptive network based fuzzy inference system additionally the limitations of flash flood forecasting have also been considered collier 2007 flood forecasting techniques using neural networks require time consuming training and the application process is complex many flood forecasting studies have been conducted since 2010 in scotland a new surface water flood forecasting model using 24 hour ensemble rainfall predictions was conducted with static flood risk maps using the grid to grid hydrological model speight et al 2016 the process of flood forecasting using 24 hour ensemble rainfall prediction is complicated to apply but has wide scope for application initial state variable correction and particle swarm optimization were used for flood forecasting in southern china li et al 2017 the application process is so complex because a significant amount of optimization is required additionally the application area and the prediction intervals are large and thus are unsuitable for single drainage areas recently several techniques have been suggested for flood forecasting flash flood modeling and forecasting using a multi layer third generation conceptual model were applied to 50 basins in russia and the results were compared with the sacramento soil moisture accounting model sokolova et al 2018 as it was applied to the river basins the size of the target watershed was significantly larger than a single drainage basin the application process was complicated and required a significant amount of time due to the large application area and the complexity of the techniques a flood nomograph using the regression curve of the first flooding nodes was suggested for flood forecasting applications in single drainage basins in urban areas lee et al 2018 it is difficult to estimate flood damage in the target watershed because flood forecasting was conducted using only the flood volume ensemble flood forecasting using the numerical weather prediction model was applied to the futatsuno and nanairo dam catchments in the shingu river basin yu et al 2018 the model had a two kilometer horizontal resolution and made 30 hour forecasts of quantitative precipitation the urban coastal flood severity from crowd sourced flood reports street flooding record using poisson regression and random forest techniques was suggested and applied in norfolk virginia usa sadler et al 2018 the national water model nwm with 1 h time steps was developed by national oceanic and atmospheric administration noaa in the department of commerce united states the nwm consists of three forecast ranges such as short range 18 hour deterministic forecast medium range 10 day deterministic forecast and long range 30 day ensemble forecast these methods are not appropriate for single drainage areas because the application areas and prediction intervals are too large in the aforementioned studies the flood forecasting methods used long term rainfall data that are unsuitable for single drainage areas that require rainfall data on a per minute scale additionally the applied flood forecasting processes are complex because training and optimization techniques are required campolo et al 1999 kim and barros 2001 solomatine and xue 2004 chau et al 2005 li et al 2017 furthermore the occurrence and extent of flood damage varies across different areas because properties across each area such as land use vary there is a need for a simple real time flood forecasting technique that uses rainfall data per minute to be developed and applied to drainage areas to improve the applicability of the method the rainfall data per minute used in this study are provided by korea meteorological administration and they are used for flood forecasting and the operation of drainage facilities in korea in this study a new flood forecasting method based on damage graphs is introduced the method only requires rainfall data and predicts flood damage without the need for long training and optimization processes prior to application the flood damage in each subarea of the target watershed was calculated using multi dimensional flood damage analysis md fda md fda was suggested by choi et al 2006 and has been applied in previous studies choi et al 2016 lee and kim 2017 each subarea was categorized by land use and the damage functions in each subarea were generated using the md fda all rainfall runoff simulations in the target watersheds were conducted using the storm water management model swmm united states environmental protection agency 2010 damage graphs were generated for each subarea by recording the rainfall intensity when the flood damage initially occurs after relating historical rainfall events to the damage graph it was possible to prepare predictions of flood damage to the target watershed 2 methodologies 2 1 overview this study consists of seven parts first synthetic rainfall which is not recorded but artificial generated using selected rainfall distributions were generated for the rainfall runoff simulations next damage functions between flood volumes and damage were obtained for each subarea then the results of flood volume were obtained using rainfall runoff simulations with synthetic rainfall data for selected rainfall durations subsequently the results of flood volume were converted to flood damage using the damage functions then the rainfall intensities that correspond to the total rainfall that caused the initial flood damage for selected rainfall durations initial flood damage intensities were recorded next damage graphs for each subarea were generated using the initial flood damage intensities of selected rainfall durations finally a historical rainfall event instead of real time rainfall data was applied to the new flood forecasting technique the flood forecasting process workflow for the method proposed in this study is shown in fig 1 the process shown in fig 1 was applied to each subarea resulting in a flood forecasting graph a flood damage alert occurs for each subarea of the target watershed if the rainfall intensity of a historical rainfall event is higher than the safety threshold in the damage graph swmm was developed by the united states environmental protection agency us epa in 1971 it can be used for rainfall runoff simulations of combined sanitary sewer conduits and or drainage facilities in urban as well as rural areas flow routing models such as steady flow routing kinematic wave routing and dynamic wave routing models are available to be selected in swmm the steady flow routing model is based on manning s equation the kinematic wave routing model allows flow and area to vary both spatially and temporally in a sewer conduit the essential elements in flood simulation such as backwater effects pressurized flow entrance and exit losses and runoff analysis in a loop network are not available in both flow routing models the dynamic wave routing model is based on the complete one dimensional saint venant flow equations it was used for all rainfall runoff simulations in this study because it is possible to apply all essential elements in the dynamic wave routing model 2 2 generation of synthetic rainfall data for damage graphs the real time rainfall data based on the actual observations by korea meteorological administration is used for the real application of the new flood forecasting technique however the damage graph required for the forecast is based on rainfall runoff simulations therefore synthetic rainfall data are required as input data for the simulations the generation of synthetic rainfall data consists of five steps 1 selection of the appropriate regression equation for the target watershed 2 selection of rainfall quantities and durations 3 generation of the cumulative distribution for the selected regression equation 4 conversion from cumulative distribution to dispersed distribution 5 application of the rainfall quantities to obtain the synthetic rainfall event the representative distribution for generating synthetic rainfall data in korea is the huff distribution upon which all drainage facilities in korea are designed huff 1967 the huff distribution consists of four quartiles based on the locations of the peak values in the huff distribution the peak values of rainfall in the first second third and fourth quartiles of the time range are significant the third quartile in the huff distribution has been considered appropriate for the design and operation of urban drainage facilities in korea yoon et al 2013 the new flood forecasting technique in this study is based on the rainfall distribution used in the design of the sewer networks in the target watershed to maintain alignment between the design and forecasting since the rainfall distribution used in the new flood forecasting technique is same as that used for the design of the sewer networks in the target watershed a customized flood forecast was generated in the target watershed equation 1 shows the cumulative regression equation using the third quartile of the huff distribution in jeongup korea precipitation frequency data server 1 y 36 029 x 6 98 986 x 5 95 279 x 4 38 622 x 3 7 4086 x 2 0 1088 x 0 0002 where y is the cumulative proportion of the rainfall and x represents the cumulative proportion of time cumulative regression equations of the huff distribution vary at each region fig 2 shows the process for generating synthetic rainfall data from the huff distribution the cumulative regression equation is used for generating the cumulative distribution in the cumulative distribution shown in fig 2 the rainfall quantities are 3 5 and 8 5 when the rainfall durations are 10 and 20 respectively in the dispersed distribution of fig 2 the rainfall quantity is 5 0 which is the difference between 8 5 and 3 5 when the rainfall duration is 20 the rainfall quantity and duration are applied to the dispersed distribution for the generation of the synthetic rainfall distribution the process is repeated for each duration because a range of synthetic rainfall data is required to generate the threshold of the new flood forecasting using rainfall runoff simulations 2 3 conversion from flood volume to flood damage 2 3 1 concept of flood damage for flood forecasting flood volumes are region dependent and vary because floods are more likely to occur in low elevation areas for populated areas at low elevation flood damage will correlate with flood volumes conversely in the same area no flood damage will occur in unpopulated areas without houses or buildings fig 3 is a schematic diagram comparing areas where flood volumes result in flood damage in fig 3 a the flood volume enters an undeveloped unpopulated area therefore no flood damage occurs in this case a flood alert is generated if the flood forecasting is based solely on the flood volume conversely if the flood forecasting is based on the likelihood of flood damage a flood alert is not generated in fig 3 b flood alerts based on both flood volumes and the likelihood of flood damage are generated when the flood occurs in the planted area hence the new flood forecasting technique differs from the current techniques in this regard however current flood forecasting methods are still required because the people can avoid flooded areas and lives can be saved by these techniques the suggested technique can be used to supplement the current flood forecasting approaches and segment based flood forecasting can be introduced in several stages 2 3 2 calculating flood damage the multi dimensional flood damage analysis md fda method is used for converting flood volumes to flood damage choi et al 2006 md fda has several components including area and damage classifications the three area classifications in the analysis are residential agricultural and industrial damage classifications include damage caused to humans buildings farmland crops inventory public facilities and the contents of buildings the md fda requires three steps determination of property value calculation of the inundated inclusion ratio and flood damage evaluation the property value for each asset is based on statistical data and the most important element of md fda is the precise survey of the property value for each asset the superposition data is generated by administrative district data land use data and flood depth data a 2d drainage model is required to obtain the results of flood depth the inundated property value flood damage in each subarea is based on the superposition data the results of flood volume and flood damage using rainfall data are used for the generation of damage functions in each subarea table 1 shows the property value estimations for residential industrial and agricultural areas the construction industry deflator and consumer price index in table 1 were based on data from statistics korea 2017 the second required factor is the calculation of the inundated inclusion ratio for calculating the flood damage the ratio is estimated using the flood depth at each property as well as the flood area the ratio of the damage to the flood depth for residential industrial and agricultural areas is shown in table 2 ministry of construction and transportation 2004 the final factor required for calculating flood damage is the flood area the flood depth and area can be obtained from historical records or rainfall runoff simulation models however the flood area in this study was conducted by rainfall runoff simulation using a 2d hydrodynamic model because the flood area is difficult to estimate accurately from historical records in korea therefore the calculation of flood damage using the md fda is based on the property value ratio of the flood area and the ratio of damage to flood depth as shown in eq 2 2 f d p v r a r d where fd is the flood damage pv is the property value in the area exposed to the flood ra represents the ratio of the flood area and rd is the ratio of the damage to the flood depth in vulnerable areas the conversion from flood volume to flood damage is based on the damage functions the damage functions can be derived from an estimate of the damage threshold using a rainfall runoff model because the resulting simulations provide the flood volume area and depth the values of flood volume per minute obtained from the rainfall runoff simulations are converted to a value of flood damage per minute for each subarea the conversion process workflow for using the damage functions is shown in fig 4 in fig 4 the unit of flood volume is m3 and the unit of flood damage is won korean currency the conversion process workflow for using damage functions consists of three steps in the first step the study area in the target watershed is divided into subareas according to land use in the second step the distributions of flood volume over time in each subarea are obtained using rainfall runoff simulations in the third step the results of flood volume over time are converted to flood damage using damage functions in each area in cases where there are many flood events based on actual observations and the status of the target watershed does not change during this period it is possible to create damage thresholds in damage functions based on actual observations however flood events do not occur frequently and the status of the target watershed changes during that time damage graphs should be revised if the status of the target watershed changes for this reason it is challenging to use the flood records based on actual observations to overcome this shortcoming synthetic rainfall data used in the design of the sewer network in the target watershed was applied to generate the damage graphs 2 4 development of the new flood forecasting technique flood forecasting used in this study was based on flood damage information obtained from rainfall runoff simulations using synthetic rainfall data the rainfall runoff was simulated for various durations it was initiated when the total rainfall quantity was 1 mm and continued at 1 mm increments until flood damage occurred the quantity of rainfall that caused the initial flood damage was identified and this rainfall quantity was converted to the rainfall intensity that caused the initial flood damage for each period of rainfall in this process the rainfall intensity that caused the initial flood damage for each period of rainfall was the quantity of rainfall divided by the duration of the rainfall a damage graph for each subarea consists of the rainfall intensities that caused the initial flood damage for each duration the damage graph was generated from the synthetic rainfall data and the historical rainfall data were applied for the flood forecasting a flood damage alert occurred if the historical rainfall intensities were higher than the threshold in the damage graph and vice versa therefore the damage alert is a mechanism for determining safe or at risk areas the entire process of conversion from flood volume to flood damage consists of four parts 1 classification of nodes in each subarea 2 estimation of the flood volume in each subarea 3 conversion of the flood volume to damage functions in each subarea and 4 estimation of flood damage in each subarea fig 5 shows the process workflow for generating a damage graph the safety threshold in the damage graph was generated from the points when the rainfall intensities generated the initial flood damage for each duration fig 6 shows the flood forecasting concept using the damage graph damage graphs are generated in each subarea and they are the thresholds for the new flood forecasting technique in fig 6 a subarea a1 is safe because all the rainfall intensities are located in the safe zone i e below the solid black line in fig 6 b subarea a2 is dangerous because all rainfall intensities except the first one are located in the danger zone in fig 6 c subarea a3 is safe because all rainfall intensities are below the threshold of the damage graph in a3 all rainfall intensities are less than the minimum value on the y axis in a3 subarea a4 has a similar pattern to subarea a2 because all rainfall intensities except the first one are located in the danger zone of fig 6 d thus subarea a4 is unsafe in fig 6 e only the third rainfall intensity is located in the dangerous zone the flood forecasting can be conducted using damage graphs in each area damage graphs can be generated according to each quartile in the huff distribution and all types of rainfall data both real and synthetic can be applied to a damage graph for flood forecasting the time interval of the applied rainfall data has a direct correlation with the flood forecasting precision the damage graph can allow more detailed flood forecasting if the time interval of the applied rainfall data is smaller therefore if the time intervals of the predicted rainfall data are too long the flood forecasting is less accurate furthermore the predicted rainfall data should be replaced by measured data as it becomes available fig 7 shows the process workflow for applying the real time rainfall data to the target subarea fig 7 is an example for showing the real application with real time rainfall data over time in fig 7 all rainfall data regardless of the time interval can be applied to the damage graph however a new method for truncating the long term rainfall data is required for application to the damage graph the intensity of the applied rainfall data over time gradually decreases and becomes too low if long term rainfall data is applied to the damage graph when hourly rainfall data is applied to a large watershed rainfall data can be divided considering the inter event time definition ietd of the target watershed when rainfall data per minute is applied to a small watershed the rainfall data can be divided considering the time of concentration in the target watershed the appropriate truncation of long term rainfall data in small watersheds can be determined when the rainless time is longer than the time of concentration because the rainfall is discharged outside of the watershed after the time of concentration 3 application and results 3 1 study area korea consists of nine provinces gyeonggi do gangwon do chungcheongbuk do chungcheongnam do jeollabuk do jeollanam do gyeongsangbuk do gyeongsangnam do and jeju island in 2011 there was inundation and flood damage in jeollabuk do province korea floods also occurred in the jeongup area the sintaein basin is a representative urbanized area in jeongup and was selected as a study area to validate our technique fig 8 shows the subarea divisions and sewer networks in the target watershed the sintaein basin was divided into five subareas a1 a2 a3 a4 and a5 according to land use a1 is a residential area with other facilities such as schools and apartments a2 consists of residential and commercial areas a3 is mostly composed of undeveloped land with a few facilities a4 is an area vulnerable to inundation because it receives flows from a1 a2 and a3 public facilities such as offices post offices and schools are located in a5 the drainage area in the sintaein basin is 67 9 ha a1 a2 a3 a4 and a5 are 17 6 21 5 0 7 15 1 and 13 0 ha in area respectively the average impermeability of each subarea is 69 78 55 83 and 72 respectively the subarea divisions and sewer networks in the target watershed were modelled with 175 sub catchments 175 conduits and 175 nodes 3 2 generation of damage graphs for each subarea 3 2 1 standard of calculation for flood damage the calculation of human related flood damage was not typically considered because of the complex factors and uncertainties such as the number and ages of transient populations in this study human related flood damage was excluded from the damage calculation so that only factors that can be calculated explicitly would be included the price per area is an important factor for evaluating flood damage and can be calculated by considering the current status in the study area the price per area in the study area was obtained from the korean appraisal board website korea appraisal board 2017 additionally construction industry deflators and residence property values should be considered and were obtained from the korean development institute website korea development institute 2017 in korea residences are categorized as detached houses apartments and buildings the value of contents within a residential property was estimated at 12 182 399 won per household korea development institute in the study area there are no agricultural areas therefore agricultural area property values were excluded the price per area values used were 1 385 000 won m2 for detached houses 1 690 000 won m2 for apartments and 853 000 won m2 for buildings the study area was divided into five subareas and the status of each subarea should be considered the number of types of residence and number of works in each subarea are required in order to calculate the property value of each area because there are no agricultural areas in the study area the property values including only residential and industrial areas were calculated table 3 shows the number of residence types number of works and property values in each subarea of the study area calculating flood damage requires rainfall runoff simulations from the swmm model to obtain the flood volume flood depth and flood area for each subarea the 1d drainage model is not appropriate to simulate overland flooding because the results are very approximate which is problematic for a flood forecasting scheme a 2d drainage model such as xp swmm is required for the simulation of overland flooding the surface overland flooding by md fda can be obtained using xp swmm xp solutions 2013 the 2d simulation of xp swmm was developed by the combination between xp swmm 1d and the tuflow 2d module phillips et al 2005 damage functions include the property value and flood volume for each subarea fig 9 shows the process workflow for generating the damage functions 3 2 2 damage functions for generating damage graphs the damage functions for a1 a2 a3 a4 and a5 in the study area were provided from previous studies and are shown in eqs 3 7 respectively choi et al 2016 3 d 1 107 10 6 v 63 9 0 46012 4 d 5 927 10 5 v 0 44332 5 d 1 331 10 7 v 291 5 0 25969 6 d 4 074 10 5 v 0 5774 7 d 3 549 10 4 v 0 9496 where d is the flood damage per minute measured in south korean won and v is the flood volume per minute for the subareas m3 rainfall runoff simulations were conducted using the swmm model to obtain the flood volumes which were then converted to flood damage per minute using the damage functions for each subarea in eqs 4 6 and 7 the flood volumes were directly linked to the flood damage in eqs 3 and 5 the flood volume was not directly linked to the flood damage suggesting that some of the subareas can resist flood damage while other subareas cannot the difference is due to the varying components within each subarea the synthetic rainfall data was required to provide a preliminary damage graph in the target watershed as mentioned previously the third quartile of the huff distribution is used to apply the rainfall runoff simulations and identify the quantities of rainfall and intensities that caused the initial flood damage in the study area the initial flood damage amounts were verified when the initial flood damage occurred for each duration and then was converted to rainfall intensities for the damage graph the damage graph is used as a threshold for the flood forecasting the calculated quantities of rainfall and average intensities that caused the initial flood damage in each subarea are shown in table 4 the results in table 4 were used to generate the damage graphs for the target watershed as expected a1 and a3 could withstand flood damage even at high rainfall intensities a1 was safe from flood damage for rainfall events as high as 100 mm hour and a3 was safe from flood damage for rainfall events as high as 200 mm hour for durations as long as 60 min in contrast a4 was easily flooded even at low rainfall intensities of less than 70 mm hour on the damage graph for each subarea is a threshold that categorizes rainfall events as safe or dangerous fig 10 shows the damage graphs for each subarea the results in fig 10 show that rainfall intensities causing flood damage in each subarea are different and highlight the necessity of dividing a drainage area into subareas according to land use the damage graphs for a1 and a3 show that high rainfall intensities for a short duration are not directly linked to flood damage because a1 is at a higher elevation and few structures are located in a3 therefore rainfall intensities as high as 200 mm hour for 10 minute durations do not cause flood damage in a1 and a3 the other damage graph results illustrate that rainfall intensities of less than 100 mm hour do not cause flood damage in a2 a4 and a5 the damage graph provides a threshold for generating flood alerts by applying synthetic or historical rainfall data 3 3 flood forecasting by damage graphs historical rainfall data from 2011 was selected for flood forecasting using the damage graphs in the target watershed the historical rainfall volume per minute data were converted to rainfall intensity for flood forecasting a flood alert based on the risk of flood damage was generated if the historical rainfall intensity data was located in the dangerous section of the damage graph in this case the predicted rainfall data was unnecessary because the historical rainfall data from 2011 was observed historical rainfall data gauged at 10 min were used for the application of damage graphs because the first damage intensities at each location were calculated at 10 minute intervals fig 11 shows the application of the damage graph in each subarea fig 11 shows the results of flood forecasting for each subarea as shown in fig 11 a a flood damage alert would not have been generated in 2011 for subarea a1 based on the historical rainfall durations however fig 11 b illustrates that flood damage started at 30 min and continued until 60 min had passed in a2 flood damage continued to occur even though the rainfall intensity decreased after 60 min fig 11 c indicates that no flood damage occurred in a3 and therefore no alert would be generated the threshold from the damage graph in a3 was high and the rainfall intensity never went above the 200 mm hour value that would have triggered flood damage fig 11 d shows that the rainfall intensity in a4 produced a similar result to that of a2 flood damage also started at 30 min and continued until 60 min had passed the results of a5 in fig 11 e indicate that flood damage occurred between 50 and 60 min the initial flood damage time for a5 was later than for a2 and a4 because a5 is located downstream of the target watershed and receives inflow from a1 a2 and a4 therefore flood damage in a5 occurred after receiving the inflows from a1 a2 and a4 in 2011 the historical flood damage 1 person died 1449 people lost houses total damage cost 124 397 000 occurred due to the historical rainfall event total quantity of rain 420 5 mm fig 11 f shows the historical flood area in the target watershed ministry of public safety and security 2011 table 5 provides a comparison of the rainfall intensities between the damage graph and the historical rainfall data for each subarea in table 5 the historical rainfall data was applied equally to all subareas different results would be obtained for varying rainfall intensities when applied to each subarea the damage graphs were generated from 10 minute interval simulations and rainfall intensities however more detailed applications are possible if the damage graphs were generated using 1 minute intervals as shown in this study new flood forecasting techniques can be conducted by the application of rainfall data and it is based on the thresholds of damage graphs for each subarea in the target watershed 4 discussion the practical application of newly developed methods is an important step because the development itself does not necessarily provide a solution to engineering problems the process of actually applying the new flood forecasting technique based on flood damage to drainage areas should be explained fig 12 shows a schematic diagram of how to apply the new flood forecasting technique in our case study the real time rainfall data were provided by the korea meteorological administration to the rainfall analysis server in the integrated computing system for disaster management the real time rainfall data were converted to potential flood damage estimates using the damage functions that were already available for each subarea the data were applied to the damage graphs and the flood damage occurrences were validated the results of the flood forecasting were sent to the disaster damage database db server additionally real time data including the number of transient populations cars and other properties were provided to the disaster damage db server using web logic and extensible markup language xml parsing software the data of the disaster damage db server were linked to the computer center for management via a proxy server db server and web server the web server provided information regarding the flood damage forecasts to users via the internet which makes it particularly accessible to those with smart phones 5 conclusions various structural and non structural measures are required to prepare for inundation conventional measures focus on reducing flood volumes and preparing pre emptive management procedures such as forecasting the purpose of current flood forecasting is to predict flooding and prevent flood damage the proposed flood forecasting technique in this study investigated flood damage in the study area an additional feature of the proposed flood forecasting technique is that flood forecasting was conducted in smaller subareas this study consisted of seven parts 1 generation of synthetic rainfall data for the rainfall runoff simulations 2 calculation of damage functions between flood volumes and damage 3 acquisition of flood volumes using rainfall runoff simulations 4 conversion from flood volume to flood damage 5 record the rainfall intensities occurring at the time of initial flood damage 6 generation of damage graphs for each subarea using the initial flood damage intensities 7 application of a historical rainfall event instead of real time rainfall data to the new flood forecasting technique downscaling is not conducted in this study since predicted and real time rainfall data provided by korea meteorological administration were used for the suggested technique the synthetic rainfall data in this study was generated using the third quartile of the huff distribution which is appropriate for korea yoon et al 2013 damage functions between flood volume and damage for each of the five subareas in the study area were generated using the md fda approach swmm was used to generate the rainfall runoff simulations and the flood volume per minute was obtained at 1 mm increments until the initial flood damage occurred in each subarea the rainfall intensities of the initial flood damage were selected every 10 min individual damage graphs were generated using the rainfall intensities when the initial flood damage occurred in each of the five subareas these values were then applied as the thresholds for flood forecasting historical rainfall data from 2011 were used for the flood forecasting the results indicate that during flooding flood damage only occurred in some subareas a2 a4 and a5 while other areas a1 and a3 remained undamaged this method of flood forecasting using damage graphs is a new flood forecasting technique which may be useful for creating management policies and preparing for flood disasters flood forecasting using damage graphs can reduce potential damage to humans and property when applied to urban and rural areas damage graphs can be used to determine dangerous or at risk subareas in target drainage areas and may be useful for taking structural and non structural measures in dangerous zones the new flood forecasting technique requires the recalculation of damage functions when the status in the target watershed dramatically changes for factors as diverse as property values and sewer networks damage graphs should be revised to consider the new conditions in each subarea if the status of the target watershed changes because damage graphs in the new flood forecasting technique are based on the current conditions of target watershed in future studies this new flood forecasting technique will incorporate resilience into the flood damage prediction additionally studies that rank regional flood risk based on variable rainfall events across a region will be an important next step various data about flooding including flood flow velocity are required in order to estimate flood damage with more detail and more accuracy a study of the accurate prediction of rainfall data which is a fundamental input data will also be conducted acknowledgements this work was supported by a grant from national research foundation nrf of korea funded by the korean government msip no 2016r1a2a1a05005306 
7133,flood forecasting is a pre emptive non structural measure used to mitigate inundation most current flood forecasting techniques incorporate complex processes such as training and optimization before the technique can be applied conventional flood forecasting techniques based on flood volume provide alerts even if there is no significant risk of flood damage in this study a new flood forecasting technique has been developed based on likely flood damage using the multi dimensional flood damage analysis method this new flood forecasting technique overcomes the drawbacks of current flood forecasting techniques because it can be easily applied using rainfall data the studied drainage area was divided into subareas and the damage functions were obtained for each subarea using the flood volumes and damage information using these damage functions the rainfall intensity when the flood damage initially occurred was calculated for each duration and subarea the damage graph produced for flood forecasting in each subarea identified the rainfall intensities and durations that resulted from the initial occurrence of flood damage this new flood forecasting technique could be used to save lives valuable assets and manage drainage areas keywords flood forecasting flood volume flood damage damage graph 1 introduction climate change has caused extreme rainfall events to occur and the frequency of these events has increased the proportion of impermeable areas as well as the frequency of extreme rainfall events has increased sharply the number of floods has increased and various measures such as structural and nonstructural measures for preventing floods have been prepared to prepare for or prevent flood damage in drainage areas one of the nonstructural measures that has been implemented is the application of various types of flood forecasting models for example a flood forecasting model was suggested and applied in three uk catchments and real time flood forecasting was proposed beven et al 1984 a distributed model using digital elevation models garrote and bras 1995 and short term rainfall prediction models toth et al 2000 have both been developed for real time flood forecasting jasper et al 2002 coupled meteorological observations and a distributed hydrological model for advanced flood forecasting recursive state space estimation young 2002 and soil moisture updated by ensemble kalman filtering komma et al 2008 have been suggested for real time flood forecasting it has been difficult to apply these methods to small watersheds such as single drainage areas because the time intervals for recording the rainfall data used in previous studies were not small enough to capture the detail required for accurate predictions at smaller scales time consuming detailed analysis using 2d hydrodynamic models is appropriate as part of the preliminary work for flood forecasting but the time required for real time forecasting should be minimal new approaches such as applying neural networks distributed hydrological modeling an ingredients based methodology and machine learning have also been introduced neural networks were developed as computer models based on the human brain and nervous system with mathematics and algorithms called threshold logic mcculloch and pitts 1943 robert hecht niesen defined one as a computing system made up of a number of simple highly interconnected processing elements which process information by their dynamic state response to external inputs caudill 1989 a neural network has been applied to river flood and flash flood forecasting campolo et al 1999 and quantitative flood forecasting using multisensory data and neural networks was proposed by kim and barros 2001 research using neural networks combined with other techniques has been suggested for flood forecasting machine learning and neural network techniques were combined for flood forecasting by solomatine and xue 2004 chau et al 2005 compared two hybrid models the genetic algorithm based artificial neural network and the adaptive network based fuzzy inference system additionally the limitations of flash flood forecasting have also been considered collier 2007 flood forecasting techniques using neural networks require time consuming training and the application process is complex many flood forecasting studies have been conducted since 2010 in scotland a new surface water flood forecasting model using 24 hour ensemble rainfall predictions was conducted with static flood risk maps using the grid to grid hydrological model speight et al 2016 the process of flood forecasting using 24 hour ensemble rainfall prediction is complicated to apply but has wide scope for application initial state variable correction and particle swarm optimization were used for flood forecasting in southern china li et al 2017 the application process is so complex because a significant amount of optimization is required additionally the application area and the prediction intervals are large and thus are unsuitable for single drainage areas recently several techniques have been suggested for flood forecasting flash flood modeling and forecasting using a multi layer third generation conceptual model were applied to 50 basins in russia and the results were compared with the sacramento soil moisture accounting model sokolova et al 2018 as it was applied to the river basins the size of the target watershed was significantly larger than a single drainage basin the application process was complicated and required a significant amount of time due to the large application area and the complexity of the techniques a flood nomograph using the regression curve of the first flooding nodes was suggested for flood forecasting applications in single drainage basins in urban areas lee et al 2018 it is difficult to estimate flood damage in the target watershed because flood forecasting was conducted using only the flood volume ensemble flood forecasting using the numerical weather prediction model was applied to the futatsuno and nanairo dam catchments in the shingu river basin yu et al 2018 the model had a two kilometer horizontal resolution and made 30 hour forecasts of quantitative precipitation the urban coastal flood severity from crowd sourced flood reports street flooding record using poisson regression and random forest techniques was suggested and applied in norfolk virginia usa sadler et al 2018 the national water model nwm with 1 h time steps was developed by national oceanic and atmospheric administration noaa in the department of commerce united states the nwm consists of three forecast ranges such as short range 18 hour deterministic forecast medium range 10 day deterministic forecast and long range 30 day ensemble forecast these methods are not appropriate for single drainage areas because the application areas and prediction intervals are too large in the aforementioned studies the flood forecasting methods used long term rainfall data that are unsuitable for single drainage areas that require rainfall data on a per minute scale additionally the applied flood forecasting processes are complex because training and optimization techniques are required campolo et al 1999 kim and barros 2001 solomatine and xue 2004 chau et al 2005 li et al 2017 furthermore the occurrence and extent of flood damage varies across different areas because properties across each area such as land use vary there is a need for a simple real time flood forecasting technique that uses rainfall data per minute to be developed and applied to drainage areas to improve the applicability of the method the rainfall data per minute used in this study are provided by korea meteorological administration and they are used for flood forecasting and the operation of drainage facilities in korea in this study a new flood forecasting method based on damage graphs is introduced the method only requires rainfall data and predicts flood damage without the need for long training and optimization processes prior to application the flood damage in each subarea of the target watershed was calculated using multi dimensional flood damage analysis md fda md fda was suggested by choi et al 2006 and has been applied in previous studies choi et al 2016 lee and kim 2017 each subarea was categorized by land use and the damage functions in each subarea were generated using the md fda all rainfall runoff simulations in the target watersheds were conducted using the storm water management model swmm united states environmental protection agency 2010 damage graphs were generated for each subarea by recording the rainfall intensity when the flood damage initially occurs after relating historical rainfall events to the damage graph it was possible to prepare predictions of flood damage to the target watershed 2 methodologies 2 1 overview this study consists of seven parts first synthetic rainfall which is not recorded but artificial generated using selected rainfall distributions were generated for the rainfall runoff simulations next damage functions between flood volumes and damage were obtained for each subarea then the results of flood volume were obtained using rainfall runoff simulations with synthetic rainfall data for selected rainfall durations subsequently the results of flood volume were converted to flood damage using the damage functions then the rainfall intensities that correspond to the total rainfall that caused the initial flood damage for selected rainfall durations initial flood damage intensities were recorded next damage graphs for each subarea were generated using the initial flood damage intensities of selected rainfall durations finally a historical rainfall event instead of real time rainfall data was applied to the new flood forecasting technique the flood forecasting process workflow for the method proposed in this study is shown in fig 1 the process shown in fig 1 was applied to each subarea resulting in a flood forecasting graph a flood damage alert occurs for each subarea of the target watershed if the rainfall intensity of a historical rainfall event is higher than the safety threshold in the damage graph swmm was developed by the united states environmental protection agency us epa in 1971 it can be used for rainfall runoff simulations of combined sanitary sewer conduits and or drainage facilities in urban as well as rural areas flow routing models such as steady flow routing kinematic wave routing and dynamic wave routing models are available to be selected in swmm the steady flow routing model is based on manning s equation the kinematic wave routing model allows flow and area to vary both spatially and temporally in a sewer conduit the essential elements in flood simulation such as backwater effects pressurized flow entrance and exit losses and runoff analysis in a loop network are not available in both flow routing models the dynamic wave routing model is based on the complete one dimensional saint venant flow equations it was used for all rainfall runoff simulations in this study because it is possible to apply all essential elements in the dynamic wave routing model 2 2 generation of synthetic rainfall data for damage graphs the real time rainfall data based on the actual observations by korea meteorological administration is used for the real application of the new flood forecasting technique however the damage graph required for the forecast is based on rainfall runoff simulations therefore synthetic rainfall data are required as input data for the simulations the generation of synthetic rainfall data consists of five steps 1 selection of the appropriate regression equation for the target watershed 2 selection of rainfall quantities and durations 3 generation of the cumulative distribution for the selected regression equation 4 conversion from cumulative distribution to dispersed distribution 5 application of the rainfall quantities to obtain the synthetic rainfall event the representative distribution for generating synthetic rainfall data in korea is the huff distribution upon which all drainage facilities in korea are designed huff 1967 the huff distribution consists of four quartiles based on the locations of the peak values in the huff distribution the peak values of rainfall in the first second third and fourth quartiles of the time range are significant the third quartile in the huff distribution has been considered appropriate for the design and operation of urban drainage facilities in korea yoon et al 2013 the new flood forecasting technique in this study is based on the rainfall distribution used in the design of the sewer networks in the target watershed to maintain alignment between the design and forecasting since the rainfall distribution used in the new flood forecasting technique is same as that used for the design of the sewer networks in the target watershed a customized flood forecast was generated in the target watershed equation 1 shows the cumulative regression equation using the third quartile of the huff distribution in jeongup korea precipitation frequency data server 1 y 36 029 x 6 98 986 x 5 95 279 x 4 38 622 x 3 7 4086 x 2 0 1088 x 0 0002 where y is the cumulative proportion of the rainfall and x represents the cumulative proportion of time cumulative regression equations of the huff distribution vary at each region fig 2 shows the process for generating synthetic rainfall data from the huff distribution the cumulative regression equation is used for generating the cumulative distribution in the cumulative distribution shown in fig 2 the rainfall quantities are 3 5 and 8 5 when the rainfall durations are 10 and 20 respectively in the dispersed distribution of fig 2 the rainfall quantity is 5 0 which is the difference between 8 5 and 3 5 when the rainfall duration is 20 the rainfall quantity and duration are applied to the dispersed distribution for the generation of the synthetic rainfall distribution the process is repeated for each duration because a range of synthetic rainfall data is required to generate the threshold of the new flood forecasting using rainfall runoff simulations 2 3 conversion from flood volume to flood damage 2 3 1 concept of flood damage for flood forecasting flood volumes are region dependent and vary because floods are more likely to occur in low elevation areas for populated areas at low elevation flood damage will correlate with flood volumes conversely in the same area no flood damage will occur in unpopulated areas without houses or buildings fig 3 is a schematic diagram comparing areas where flood volumes result in flood damage in fig 3 a the flood volume enters an undeveloped unpopulated area therefore no flood damage occurs in this case a flood alert is generated if the flood forecasting is based solely on the flood volume conversely if the flood forecasting is based on the likelihood of flood damage a flood alert is not generated in fig 3 b flood alerts based on both flood volumes and the likelihood of flood damage are generated when the flood occurs in the planted area hence the new flood forecasting technique differs from the current techniques in this regard however current flood forecasting methods are still required because the people can avoid flooded areas and lives can be saved by these techniques the suggested technique can be used to supplement the current flood forecasting approaches and segment based flood forecasting can be introduced in several stages 2 3 2 calculating flood damage the multi dimensional flood damage analysis md fda method is used for converting flood volumes to flood damage choi et al 2006 md fda has several components including area and damage classifications the three area classifications in the analysis are residential agricultural and industrial damage classifications include damage caused to humans buildings farmland crops inventory public facilities and the contents of buildings the md fda requires three steps determination of property value calculation of the inundated inclusion ratio and flood damage evaluation the property value for each asset is based on statistical data and the most important element of md fda is the precise survey of the property value for each asset the superposition data is generated by administrative district data land use data and flood depth data a 2d drainage model is required to obtain the results of flood depth the inundated property value flood damage in each subarea is based on the superposition data the results of flood volume and flood damage using rainfall data are used for the generation of damage functions in each subarea table 1 shows the property value estimations for residential industrial and agricultural areas the construction industry deflator and consumer price index in table 1 were based on data from statistics korea 2017 the second required factor is the calculation of the inundated inclusion ratio for calculating the flood damage the ratio is estimated using the flood depth at each property as well as the flood area the ratio of the damage to the flood depth for residential industrial and agricultural areas is shown in table 2 ministry of construction and transportation 2004 the final factor required for calculating flood damage is the flood area the flood depth and area can be obtained from historical records or rainfall runoff simulation models however the flood area in this study was conducted by rainfall runoff simulation using a 2d hydrodynamic model because the flood area is difficult to estimate accurately from historical records in korea therefore the calculation of flood damage using the md fda is based on the property value ratio of the flood area and the ratio of damage to flood depth as shown in eq 2 2 f d p v r a r d where fd is the flood damage pv is the property value in the area exposed to the flood ra represents the ratio of the flood area and rd is the ratio of the damage to the flood depth in vulnerable areas the conversion from flood volume to flood damage is based on the damage functions the damage functions can be derived from an estimate of the damage threshold using a rainfall runoff model because the resulting simulations provide the flood volume area and depth the values of flood volume per minute obtained from the rainfall runoff simulations are converted to a value of flood damage per minute for each subarea the conversion process workflow for using the damage functions is shown in fig 4 in fig 4 the unit of flood volume is m3 and the unit of flood damage is won korean currency the conversion process workflow for using damage functions consists of three steps in the first step the study area in the target watershed is divided into subareas according to land use in the second step the distributions of flood volume over time in each subarea are obtained using rainfall runoff simulations in the third step the results of flood volume over time are converted to flood damage using damage functions in each area in cases where there are many flood events based on actual observations and the status of the target watershed does not change during this period it is possible to create damage thresholds in damage functions based on actual observations however flood events do not occur frequently and the status of the target watershed changes during that time damage graphs should be revised if the status of the target watershed changes for this reason it is challenging to use the flood records based on actual observations to overcome this shortcoming synthetic rainfall data used in the design of the sewer network in the target watershed was applied to generate the damage graphs 2 4 development of the new flood forecasting technique flood forecasting used in this study was based on flood damage information obtained from rainfall runoff simulations using synthetic rainfall data the rainfall runoff was simulated for various durations it was initiated when the total rainfall quantity was 1 mm and continued at 1 mm increments until flood damage occurred the quantity of rainfall that caused the initial flood damage was identified and this rainfall quantity was converted to the rainfall intensity that caused the initial flood damage for each period of rainfall in this process the rainfall intensity that caused the initial flood damage for each period of rainfall was the quantity of rainfall divided by the duration of the rainfall a damage graph for each subarea consists of the rainfall intensities that caused the initial flood damage for each duration the damage graph was generated from the synthetic rainfall data and the historical rainfall data were applied for the flood forecasting a flood damage alert occurred if the historical rainfall intensities were higher than the threshold in the damage graph and vice versa therefore the damage alert is a mechanism for determining safe or at risk areas the entire process of conversion from flood volume to flood damage consists of four parts 1 classification of nodes in each subarea 2 estimation of the flood volume in each subarea 3 conversion of the flood volume to damage functions in each subarea and 4 estimation of flood damage in each subarea fig 5 shows the process workflow for generating a damage graph the safety threshold in the damage graph was generated from the points when the rainfall intensities generated the initial flood damage for each duration fig 6 shows the flood forecasting concept using the damage graph damage graphs are generated in each subarea and they are the thresholds for the new flood forecasting technique in fig 6 a subarea a1 is safe because all the rainfall intensities are located in the safe zone i e below the solid black line in fig 6 b subarea a2 is dangerous because all rainfall intensities except the first one are located in the danger zone in fig 6 c subarea a3 is safe because all rainfall intensities are below the threshold of the damage graph in a3 all rainfall intensities are less than the minimum value on the y axis in a3 subarea a4 has a similar pattern to subarea a2 because all rainfall intensities except the first one are located in the danger zone of fig 6 d thus subarea a4 is unsafe in fig 6 e only the third rainfall intensity is located in the dangerous zone the flood forecasting can be conducted using damage graphs in each area damage graphs can be generated according to each quartile in the huff distribution and all types of rainfall data both real and synthetic can be applied to a damage graph for flood forecasting the time interval of the applied rainfall data has a direct correlation with the flood forecasting precision the damage graph can allow more detailed flood forecasting if the time interval of the applied rainfall data is smaller therefore if the time intervals of the predicted rainfall data are too long the flood forecasting is less accurate furthermore the predicted rainfall data should be replaced by measured data as it becomes available fig 7 shows the process workflow for applying the real time rainfall data to the target subarea fig 7 is an example for showing the real application with real time rainfall data over time in fig 7 all rainfall data regardless of the time interval can be applied to the damage graph however a new method for truncating the long term rainfall data is required for application to the damage graph the intensity of the applied rainfall data over time gradually decreases and becomes too low if long term rainfall data is applied to the damage graph when hourly rainfall data is applied to a large watershed rainfall data can be divided considering the inter event time definition ietd of the target watershed when rainfall data per minute is applied to a small watershed the rainfall data can be divided considering the time of concentration in the target watershed the appropriate truncation of long term rainfall data in small watersheds can be determined when the rainless time is longer than the time of concentration because the rainfall is discharged outside of the watershed after the time of concentration 3 application and results 3 1 study area korea consists of nine provinces gyeonggi do gangwon do chungcheongbuk do chungcheongnam do jeollabuk do jeollanam do gyeongsangbuk do gyeongsangnam do and jeju island in 2011 there was inundation and flood damage in jeollabuk do province korea floods also occurred in the jeongup area the sintaein basin is a representative urbanized area in jeongup and was selected as a study area to validate our technique fig 8 shows the subarea divisions and sewer networks in the target watershed the sintaein basin was divided into five subareas a1 a2 a3 a4 and a5 according to land use a1 is a residential area with other facilities such as schools and apartments a2 consists of residential and commercial areas a3 is mostly composed of undeveloped land with a few facilities a4 is an area vulnerable to inundation because it receives flows from a1 a2 and a3 public facilities such as offices post offices and schools are located in a5 the drainage area in the sintaein basin is 67 9 ha a1 a2 a3 a4 and a5 are 17 6 21 5 0 7 15 1 and 13 0 ha in area respectively the average impermeability of each subarea is 69 78 55 83 and 72 respectively the subarea divisions and sewer networks in the target watershed were modelled with 175 sub catchments 175 conduits and 175 nodes 3 2 generation of damage graphs for each subarea 3 2 1 standard of calculation for flood damage the calculation of human related flood damage was not typically considered because of the complex factors and uncertainties such as the number and ages of transient populations in this study human related flood damage was excluded from the damage calculation so that only factors that can be calculated explicitly would be included the price per area is an important factor for evaluating flood damage and can be calculated by considering the current status in the study area the price per area in the study area was obtained from the korean appraisal board website korea appraisal board 2017 additionally construction industry deflators and residence property values should be considered and were obtained from the korean development institute website korea development institute 2017 in korea residences are categorized as detached houses apartments and buildings the value of contents within a residential property was estimated at 12 182 399 won per household korea development institute in the study area there are no agricultural areas therefore agricultural area property values were excluded the price per area values used were 1 385 000 won m2 for detached houses 1 690 000 won m2 for apartments and 853 000 won m2 for buildings the study area was divided into five subareas and the status of each subarea should be considered the number of types of residence and number of works in each subarea are required in order to calculate the property value of each area because there are no agricultural areas in the study area the property values including only residential and industrial areas were calculated table 3 shows the number of residence types number of works and property values in each subarea of the study area calculating flood damage requires rainfall runoff simulations from the swmm model to obtain the flood volume flood depth and flood area for each subarea the 1d drainage model is not appropriate to simulate overland flooding because the results are very approximate which is problematic for a flood forecasting scheme a 2d drainage model such as xp swmm is required for the simulation of overland flooding the surface overland flooding by md fda can be obtained using xp swmm xp solutions 2013 the 2d simulation of xp swmm was developed by the combination between xp swmm 1d and the tuflow 2d module phillips et al 2005 damage functions include the property value and flood volume for each subarea fig 9 shows the process workflow for generating the damage functions 3 2 2 damage functions for generating damage graphs the damage functions for a1 a2 a3 a4 and a5 in the study area were provided from previous studies and are shown in eqs 3 7 respectively choi et al 2016 3 d 1 107 10 6 v 63 9 0 46012 4 d 5 927 10 5 v 0 44332 5 d 1 331 10 7 v 291 5 0 25969 6 d 4 074 10 5 v 0 5774 7 d 3 549 10 4 v 0 9496 where d is the flood damage per minute measured in south korean won and v is the flood volume per minute for the subareas m3 rainfall runoff simulations were conducted using the swmm model to obtain the flood volumes which were then converted to flood damage per minute using the damage functions for each subarea in eqs 4 6 and 7 the flood volumes were directly linked to the flood damage in eqs 3 and 5 the flood volume was not directly linked to the flood damage suggesting that some of the subareas can resist flood damage while other subareas cannot the difference is due to the varying components within each subarea the synthetic rainfall data was required to provide a preliminary damage graph in the target watershed as mentioned previously the third quartile of the huff distribution is used to apply the rainfall runoff simulations and identify the quantities of rainfall and intensities that caused the initial flood damage in the study area the initial flood damage amounts were verified when the initial flood damage occurred for each duration and then was converted to rainfall intensities for the damage graph the damage graph is used as a threshold for the flood forecasting the calculated quantities of rainfall and average intensities that caused the initial flood damage in each subarea are shown in table 4 the results in table 4 were used to generate the damage graphs for the target watershed as expected a1 and a3 could withstand flood damage even at high rainfall intensities a1 was safe from flood damage for rainfall events as high as 100 mm hour and a3 was safe from flood damage for rainfall events as high as 200 mm hour for durations as long as 60 min in contrast a4 was easily flooded even at low rainfall intensities of less than 70 mm hour on the damage graph for each subarea is a threshold that categorizes rainfall events as safe or dangerous fig 10 shows the damage graphs for each subarea the results in fig 10 show that rainfall intensities causing flood damage in each subarea are different and highlight the necessity of dividing a drainage area into subareas according to land use the damage graphs for a1 and a3 show that high rainfall intensities for a short duration are not directly linked to flood damage because a1 is at a higher elevation and few structures are located in a3 therefore rainfall intensities as high as 200 mm hour for 10 minute durations do not cause flood damage in a1 and a3 the other damage graph results illustrate that rainfall intensities of less than 100 mm hour do not cause flood damage in a2 a4 and a5 the damage graph provides a threshold for generating flood alerts by applying synthetic or historical rainfall data 3 3 flood forecasting by damage graphs historical rainfall data from 2011 was selected for flood forecasting using the damage graphs in the target watershed the historical rainfall volume per minute data were converted to rainfall intensity for flood forecasting a flood alert based on the risk of flood damage was generated if the historical rainfall intensity data was located in the dangerous section of the damage graph in this case the predicted rainfall data was unnecessary because the historical rainfall data from 2011 was observed historical rainfall data gauged at 10 min were used for the application of damage graphs because the first damage intensities at each location were calculated at 10 minute intervals fig 11 shows the application of the damage graph in each subarea fig 11 shows the results of flood forecasting for each subarea as shown in fig 11 a a flood damage alert would not have been generated in 2011 for subarea a1 based on the historical rainfall durations however fig 11 b illustrates that flood damage started at 30 min and continued until 60 min had passed in a2 flood damage continued to occur even though the rainfall intensity decreased after 60 min fig 11 c indicates that no flood damage occurred in a3 and therefore no alert would be generated the threshold from the damage graph in a3 was high and the rainfall intensity never went above the 200 mm hour value that would have triggered flood damage fig 11 d shows that the rainfall intensity in a4 produced a similar result to that of a2 flood damage also started at 30 min and continued until 60 min had passed the results of a5 in fig 11 e indicate that flood damage occurred between 50 and 60 min the initial flood damage time for a5 was later than for a2 and a4 because a5 is located downstream of the target watershed and receives inflow from a1 a2 and a4 therefore flood damage in a5 occurred after receiving the inflows from a1 a2 and a4 in 2011 the historical flood damage 1 person died 1449 people lost houses total damage cost 124 397 000 occurred due to the historical rainfall event total quantity of rain 420 5 mm fig 11 f shows the historical flood area in the target watershed ministry of public safety and security 2011 table 5 provides a comparison of the rainfall intensities between the damage graph and the historical rainfall data for each subarea in table 5 the historical rainfall data was applied equally to all subareas different results would be obtained for varying rainfall intensities when applied to each subarea the damage graphs were generated from 10 minute interval simulations and rainfall intensities however more detailed applications are possible if the damage graphs were generated using 1 minute intervals as shown in this study new flood forecasting techniques can be conducted by the application of rainfall data and it is based on the thresholds of damage graphs for each subarea in the target watershed 4 discussion the practical application of newly developed methods is an important step because the development itself does not necessarily provide a solution to engineering problems the process of actually applying the new flood forecasting technique based on flood damage to drainage areas should be explained fig 12 shows a schematic diagram of how to apply the new flood forecasting technique in our case study the real time rainfall data were provided by the korea meteorological administration to the rainfall analysis server in the integrated computing system for disaster management the real time rainfall data were converted to potential flood damage estimates using the damage functions that were already available for each subarea the data were applied to the damage graphs and the flood damage occurrences were validated the results of the flood forecasting were sent to the disaster damage database db server additionally real time data including the number of transient populations cars and other properties were provided to the disaster damage db server using web logic and extensible markup language xml parsing software the data of the disaster damage db server were linked to the computer center for management via a proxy server db server and web server the web server provided information regarding the flood damage forecasts to users via the internet which makes it particularly accessible to those with smart phones 5 conclusions various structural and non structural measures are required to prepare for inundation conventional measures focus on reducing flood volumes and preparing pre emptive management procedures such as forecasting the purpose of current flood forecasting is to predict flooding and prevent flood damage the proposed flood forecasting technique in this study investigated flood damage in the study area an additional feature of the proposed flood forecasting technique is that flood forecasting was conducted in smaller subareas this study consisted of seven parts 1 generation of synthetic rainfall data for the rainfall runoff simulations 2 calculation of damage functions between flood volumes and damage 3 acquisition of flood volumes using rainfall runoff simulations 4 conversion from flood volume to flood damage 5 record the rainfall intensities occurring at the time of initial flood damage 6 generation of damage graphs for each subarea using the initial flood damage intensities 7 application of a historical rainfall event instead of real time rainfall data to the new flood forecasting technique downscaling is not conducted in this study since predicted and real time rainfall data provided by korea meteorological administration were used for the suggested technique the synthetic rainfall data in this study was generated using the third quartile of the huff distribution which is appropriate for korea yoon et al 2013 damage functions between flood volume and damage for each of the five subareas in the study area were generated using the md fda approach swmm was used to generate the rainfall runoff simulations and the flood volume per minute was obtained at 1 mm increments until the initial flood damage occurred in each subarea the rainfall intensities of the initial flood damage were selected every 10 min individual damage graphs were generated using the rainfall intensities when the initial flood damage occurred in each of the five subareas these values were then applied as the thresholds for flood forecasting historical rainfall data from 2011 were used for the flood forecasting the results indicate that during flooding flood damage only occurred in some subareas a2 a4 and a5 while other areas a1 and a3 remained undamaged this method of flood forecasting using damage graphs is a new flood forecasting technique which may be useful for creating management policies and preparing for flood disasters flood forecasting using damage graphs can reduce potential damage to humans and property when applied to urban and rural areas damage graphs can be used to determine dangerous or at risk subareas in target drainage areas and may be useful for taking structural and non structural measures in dangerous zones the new flood forecasting technique requires the recalculation of damage functions when the status in the target watershed dramatically changes for factors as diverse as property values and sewer networks damage graphs should be revised to consider the new conditions in each subarea if the status of the target watershed changes because damage graphs in the new flood forecasting technique are based on the current conditions of target watershed in future studies this new flood forecasting technique will incorporate resilience into the flood damage prediction additionally studies that rank regional flood risk based on variable rainfall events across a region will be an important next step various data about flooding including flood flow velocity are required in order to estimate flood damage with more detail and more accuracy a study of the accurate prediction of rainfall data which is a fundamental input data will also be conducted acknowledgements this work was supported by a grant from national research foundation nrf of korea funded by the korean government msip no 2016r1a2a1a05005306 
7134,over the millennia peatlands have sequestered and stored carbon in the form of peat that today accounts for around 30 of the global soil organic carbon the peat has formed and accumulated due to the slow decomposition rate of organic matter in waterlogged anaerobic conditions the particularly close interdependence of the carbon and water cycles in peatland ecosystems signal the importance of understanding the water cycle to the functioning of peatlands with this aim the water and energy cycle of an alpine catchment in italy which includes a peatland was studied using the process based hydrological model geotop and a set of in situ measurements over 4 years 2012 2015 this is a challenging modelling exercise that has not been tried before with geotop the catchment is heterogenous with land covers of peatland grassland scree and bare rock in a mountainous area the geotop model was able to replicate the energy and water fluxes measured by an eddy covariance tower located in the peatland as well as the volumetric water content and soil temperature of the peatland accurately over the four years this study shows that a process based hydrological model can be used to study the water and energy dynamics of a peatland in a mountainous area keywords peatland hydrology geotop energy balance water balance alps 1 introduction over millennia global peatlands have stored up to 547 473 621 pg of soil organic carbon in the form of peat gorham 1991 yu 2012 which is around 30 of the global soil organic carbon gorham 1991 peatland formation is dependent on water as the decomposition of organic matter is very slow in waterlogged and anaerobic conditions resulting in organic matter accumulation in peat layers up to several metres pristine northern latitude peatlands are today mainly acting as carbon sinks lund et al 2010 mcveigh et al 2014 while disturbed peatland e g harvested and drained predominantly act as carbon sources hendriks et al 2007 petrescu et al 2015 aslan sungur et al 2016 the accumulated carbon in peatlands can be lost via three fluxes as co2 through respiration as methane ch4 emissions and or as dissolved organic carbon doc in stream runoff these carbon fluxes are influenced by environmental and meteorological conditions lafleur et al 2003 camill 2005 frolking et al 2011 mcveigh et al 2014 aslan sungur et al 2016 apart from the climate change many factors can influence the fate of the stored carbon such as ground water level roulet et al 2007 murphy et al 2009 sottocornola and kiely 2010 koehler et al 2011 lewis et al 2013 miller et al 2015 pullens et al 2016 and different management strategies hendriks et al 2007 wilson et al 2007 van der molen et al 2007 baldocchi et al 2012 the source of water ombrotrophic or minerotrophic and its dissolved minerals is important for peatland ecosystems differences in water sources and the dissolved nutrients lead to different plant communities koerselman 1989 the water cycle and in particular the variation in water table level plays a crucial role in the functioning of peatland ecosystems gorham 1957 the level of the water table impacts the carbon balance of peatlands since a raise in water table depth can result in higher methane emissions wagner et al 2003 jungkunst et al 2008 and or in more runoff and thus a removal of organic matter through doc in the stream water a rising water table will also reduce the respiration of the soil due to the occurrence of anaerobic soil conditions sonnentag et al 2010 mcveigh et al 2014 strachan et al 2016 on the contrary a decrease in precipitation will lead to the drying of peatlands these less waterlogged soils have a higher decomposition rates of the peat the stored carbon in the soil is vulnerable to environmental management and climate change lafleur et al 2003 camill 2005 frolking et al 2011 mcveigh et al 2014 aslan sungur et al 2016 the predicted rising temperatures from climate change will likely result in higher evaporation and decomposition rates which may change the peatlands from natural carbon sinks to carbon sources to resolve the uncertainties around the future functioning of these ecosystems it is important to understand the dynamics of the water table and thus the water cycle of the peatland catchment therefore improving the understanding of the water cycle and the energy budget is the aim of this work and is possible through the use of a detailed resolution process based model fatichi et al 2016 many studied peatlands are located in high latitude regions where the vulnerability of their carbon storage the effects on climate change and permafrost degradation has been investigated frolking et al 2001 camill 2005 dorrepaal et al 2009 beside the occurrence of peatlands at high latitudes peatlands also occur at high elevation although less research has been done on the latter because they are not the dominant ecosystems in those areas parish et al 2008 even though there are numerous peatlands in the alps which are being threatened by rising temperatures and changes in the precipitation regime beniston et al 1997 ipcc 2007 2013 im et al 2010 eccel et al 2012 steger et al 2013 pepin et al 2015 tudoroiu et al 2016 their carbon energy and water cycle has been little studied to the authors knowledge the carbon fluxes has been studied continuously over multiple years only in the monte bondone alpine peatland pullens et al 2016 which is the focus of this paper to analyse the energy and water cycles in complex ecosystems hydrological models are an essential tool for water and environment resource management devia et al 2015 fatichi et al 2016 there are various hydrological models available todini 2007 ranging from models based on the unit hydrograph to complex process based catchment models beven and kirkby 1979 gregory et al 1996 rinaldo and rodriguez iturbe 1996 ewen et al 2000 vivoni et al 2004 rigon et al 2006 2016 endrizzi et al 2014 the requirements and research questions of the user of the model determine the choice of which model to be used devia et al 2015 because of the multiplicity of interactions responsible for peatlands dynamics a process based model of the water and energy budget in conjunction with a series of in situ measurements is desirable fatichi et al 2016 in this paper the process based hydrological model geotop rigon et al 2006 was implemented on a catchment in the italian alps geotop v 2 0 models the energy and water budgets at and below the soil surface endrizzi et al 2014 and provides the soil vegetation atmospheric interactions svat for complex terrain the model has previously been implemented by lewis et al 2013 to study the effect of afforestation on the hydrology of a blanket peatland in ireland where it was shown that geotop is a suitable tool for modelling the hydrology in a peatland the model has also reproduced the spatial patterns of land surface temperature soil moisture and the energy balance components in alpine and pre alpine environments bertoldi et al 2006 2010 2014 della chiesa et al 2014 hingerl et al 2016 to date geotop has not been used to simulate the water and energy balance over multiple years of an alpine catchment in which a fen type of peatland is located specifically the paper aims at investigating 1 the possibility to simulate the water and energy budgets of an alpine peatland with a process based model and 2 the capability of the model to reproduce the observed interannual variability 2012 2015 of the energy fluxes in a mountain area the two objectives are addressed using the geotop v 2 0 model and in situ measurements the modelled water cycle was combined with doc concentration measurements to estimate the doc loss from the peatlands and complete the quantification of the main components of the ecosystem carbon budget 2 materials and methods 2 1 the site the study site is a 10 ha minerotrophic fen located at 1563 m asl on the monte bondone plateau near trento in the eastern italian alps fig 1 a 46 01 n 11 02 e the peatland is located in a 93 ha catchment and includes part of the palon mountain and the surrounding slopes to the mountain tops cima verde dos d abramo and cornetto the catchment area was calculated by means of the qgis software version 2 6 brighton with the dem resolution 1 by 1 m provided by the autonomous province of trento the catchment consists of bare rock limestone scree managed grassland cut once a year coniferous forest norway spruce picea abies and european larch larix decidua and a minerotrophic fen type peatland the resolution of the modelled catchment was rescaled to 25 by 25 m to decrease the model s computational demand the peatland is a relic of an ancient lake that was formed by a glacier during the last ice age cescatti et al 1999 the thickness of the peat layer ranges from 80 cm at the border to 4 4 m in the centre dalla fior 1969 in 1914 the top peat layer of 0 35 ha of the peatland was harvested for fuel cescatti et al 1999 the peatland has different microforms from high to low hummocks lawns and hollows where the lawns are the flat areas in the peatland the peatland at monte bondone has a diverse vegetation perucco et al 2013 the lawns are dominated by either grasses molinia caerulea or sedges carex nigra and eriophorum vaginatum while the main vegetation in the hollows consists of carex rostrata and the mosses scorpidium cossonii sphagnum subsecundum and s compactum the few short hummocks are covered by s compactum and s sect acutifolia and calluna vulgaris in the centre of the peatland there is a small open pool which can act as a discharge buffer for the discharge after a precipitation event and has a storage capacity of 15 30 m3 close to this pond the common vegetation are birches betula pendula and cotton deergrass trichophorum alpinum the vegetation at plot 1 2 4 5 and 6 fig 1b are mainly dominated by m caerulea at plot 4 t alpinum has a high cover and plot 4 is the only plot where sphagnum species are highly present s subsecundum apart from m caerulea the other abundant occurring plant species are at plot 1 c vulgaris t alpinum at plot 2 c nigra at plot 5 aulacomnium palustre equisetum palustre tomentypnum nitens and at plot 6 campylium stellatum c rostrata s cossonii at the other plots no m caerulea is present plot 3 is dominated by c nigra plot 7 is dominated by c rostrata and valeriana dioica plot 8 by s cossonii and plot 9 by c vulgaris the water table of the complete catchment flows on deep impermeable morainic strata which seeps into the fen cescatti et al 1999 seepage reaches the surface in the peatland at two ephemeral springs defined as inflows fig 1b this water flows through superficial streams to the lowest point of the peatland where the water drains out of the peatland into a perennial stream defined as outflow fig 1b the modelling exercise covered the period from 2012 to 2015 the average annual air temperature of 2012 2014 and 2015 were close to the long term average of 5 4 c while the temperature in 2013 was lower table 1 in the first three years the precipitation was higher than the long term average of 1290 mm with 2014 being an exceptional wet year while in 2015 the precipitation was well below the long term average table 1 2 2 experimental setup an eddy covariance instrumented tower was mounted in the centre of the peatland in 2011 to measure water carbon and energy fluxes and meteorological conditions a shielded probe rotronic m103a bassersdorf germany measured air temperature and relative humidity at 2 m height incoming photosynthetically active radiation par was measured by a licor 190sz sensor licor lincoln ne usa while the incoming and outgoing short and longwave infrared radiations were measured by a cnr1 kipp zonen delft the netherlands both of the latter two sensors were installed on a horizontal bar at 3 5 m above the soil surface the volumetric soil water content was measured between tussocks of m caerulea and under c vulgaris at 5 cm depth with a cs616 water content reflectometer campbell scientific logan ut usa the distance between these two sensors is approximately 10 m at these locations also the soil heat flux was measured with a hfp01 soil heat flux plates hukseflux thermal sensors b v delft the netherlands the soil temperature was measured along a profile at 2 5 10 20 and 50 cm depth with a stp01 sensor hukseflux thermal sensors b v delft the netherlands from the 1st july 2012 all data were collected once a minute on a cr3000 data logger campbell scientific logan ut usa with an am16 32 multiplexer campbell scientific logan ut usa and averaged to half hourly values wind speed wind direction and sensible heat fluxes were measured at 20 hz with a r3 100 3d sonic anemometer gill instruments lymington hampshire uk the latent heat fluxes were measured with the eddy covariance system consisting of two infrared gas analysers li7500 and li7200 licor lincoln ne usa the data of the gas analysers and the anemometer were stored on an industrial computer fx5507 fabiatech taipei taiwan located in the field pullens et al 2016 on 16th december 2013 a heated tipping bucket rain gauge model 52202 young traverse city mi usa was installed before then the precipitation data from a meteorological station located 400 m away from the ec tower at a similar altitude 1490 m asl were used meteo trentino station name giardino in addition the total snow height and the snow density of the freshly fallen snow were daily manually measured at this location by the provincial meteorological service meteo trentino missing meteorological data approximately 20 of the half hourly data points due to malfunctioning of the sensors or power shortage were replaced by data from a nearby 500 m horizontal distance eddy covariance tower located in a grassland fig 1a marcolla et al 2011 sakowska et al 2014 for soil temperature a dissimilarity was expected between the grassland and the peatland but a highly significant relationship between the two sites was found for all depths r2 0 95 tsoil peatland 5cm 0 46 0 94 tsoil grassland 5cm the remaining gaps in the meteorological data 0 9 caused by malfunction of both towers were replaced by data from two nearby meteorological stations of meteo trentino fig 1a giardino botanico and viote the latter 1 km away from the peatland tower soil temperature was not measured at the meteo trentino weather stations therefore when missing at both ec towers soil temperatures were replaced by linear interpolations a more detailed overview of the site and the instruments is described in pullens et al 2016 the measured soil water content data were not reliable when the soil was covered by snow under these conditions the temperature of the soil at the depth of the soil water content probes 5 cm was around zero degrees celsius which resulted in soil water content measurements dropping to unrealistic values 20 these low soil water content measurements could suggest a frozen soil and are therefore not included in the manuscript 2 3 soil properties soil samples were collected on 3 4 july 2014 for physical and chemical analyses under all nine previously described main vegetation types occurring in the fen fig 1b at each point three soil samples replicates located at 1 m distance were collected at different depths 0 5 50 55 and 100 105 cm the samples at 0 5 cm were collected with eijkelkamp soil core sampler rings diameter 5 cm height 5 cm volume 98 cm3 while at the depths of 50 55 and 100 105 cm the samples were collected with an eijkelkamp split tube sampler both instruments eijkelkamp giesbeek the netherlands same dimensions as the soil core sampler rings overall 81 samples were collected from the 9 plots samples that were too compacted due to compression by the sampling spongy tissue were discarded as well as samples which were too wet i e slurry in total 50 soil samples were transported in closed plastic bags to the laboratory for analyses the samples were composed of 27 samples from 0 to 5 cm 15 samples from 50 to 55 cm and 8 samples from 100 to 105 cm depths each sample was weighed on the day of sampling before it was oven dried at 55 c until a stable mass was reached after 24 h to calculate the bulk density the bulk density was sampled to explore the soil variability and to identify the plots for hydraulic conductivity measurements assouline 2006 an increase in soil bulk density associated with compaction is known to influence soil hydraulic properties laliberte et al 1966 after this the soil was grinded with a mortar and pestle to break clumps of peat before it was sieved 2 mm sieve to remove leaves and roots a subset of the sample was placed in a metal container with a metal ball and inserted in a mixer mill mm 200 retsch haan germany which was run at the speed of 25 hz for 5 min the rest of the sample was placed in a 105 c oven to measure the remaining water content a homogenous subsample 5 mg from the ball milled samples was inserted in tin caps and the amount of soil carbon and nitrogen was measured in a perkinelmer 2400 series ii chns o elemental analyser perkinelmer waltham ma usa before statistical analysis the bulk density data were tested for normality in this study all values were normally distributed shapiro wilk test w 0 99 p value 0 9 for the statistical analysis of the bulk density the tukey s hsd honest significant difference test was used using the r software version 3 1 2 r development core team r foundation for statistical computing 2017 2 4 hydraulic conductivity the saturated hydraulic conductivity of the peatland soil was measured using the adjusted modified cube method presented in lewis et al 2012 in literature different methods are used to measure the hydraulic conductivity such as the standpipe technique hvorslev 1951 and the modified cube method beckwith et al 2003 an advantage of the mcm over the standpipe technique is that with the mcm both the horizontal and the vertical hydraulic conductivity can be measured in august 2014 two soil samples from 5 of the 9 bulk density plots were sampled at 0 10 and 40 50 cm depths these plots were chosen because they showed the biggest range of bulk density see results section and therefore the extent of hydraulic conductivity in the peatland could be analysed assouline 2006 a metal tube with a square section 12 12 100 cm and a cutting edge was driven into the peat to a depth of 70 cm after the vegetation was removed from the top soil to prevent damage to the peat core the tube was extracted and due to suction the sample stayed in the sampling tube during the retrieval after retrieval of the column samples at 0 10 and 40 50 cm depths were cut with a blunt knife the samples were stored in a plastic bag and a container before being transported to the laboratory immediately after returning to the laboratory the samples were cut into cubes of sides 10 cm and sealed with molten paraffin to assure a good seal the cubes were dipped into paraffin until the thickness of the paraffin around the block was about 1 cm after the blocks were completely sealed the paraffin was removed from two opposite sides on the top of the block a wooden collar was added and the seams were blocked with paraffin since the measurement of the saturated hydraulic conductivity needs a constant pressure head a large reservoir of water was connected with a tube to the peat samples to create a syphon which maintained a constant pressure head of 7 cm the water used during the experiment was collected in the outflow stream to eliminate any influences of different chemical signatures of water from a different source lewis et al 2012 the peat blocks were positioned on a fine mesh in a plastic bowl where a tube was inserted 1 cm above the bottom the saturated hydraulic conductivity was calculated by using darcy s law darcy 1856 1 k sat ql a δ h in which ksat is the saturated hydraulic conductivity m s 1 q is the discharge m3 s 1 l is the length of the sample m a is the area of one open side of the cube m2 and δh is the difference in head between the top and bottom of the sample m where the constant head method was used for each peat block both the horizontal and vertical hydraulic conductivities were measured for the horizontal hydraulic conductivity the procedure was identical only the blocks were rotated 90 the hydraulic conductivity was only measured on soil samples from the peatland for mineral soils in the catchment literature values were used 2 5 water table streamflow and doc the stream heights at the two inflows and the outflow of the peatland were measured with pressure transducers dipper pt seba hydrometrie gmbh co germany the pressure transducers were installed in perforated pipes on 14th may 2014 the water level was measured every half hour and the data were stored on internal flash memory and collected at regular intervals the streamflow was measured and water samples for dissolved organic carbon doc were collected at the same locations as the stream height fig 1b the streamflow was measured 10 times during 2014 and 2015 with a fp111 global flow probe college station texas usa to convert the stream height measurement time series in discharge time series the uncertainty of the device used for the flow measurements is 0 1 m s no weir was allowed to be installed at the in and outflows as the site is a natura 2000 protected area stream flow measurements were complicated due to the dynamic cross sections of the stream channels and the occurrence of dense vegetation in the stream during the streamflow measurements stream water samples around 500 ml were collected for later doc analysis the samples were filtered in the laboratory with a millipore system using isopore membrane filters with 0 4 µm pore size merck millipore billerica ma usa after the filtration the samples were frozen and stored at 20 c before shipment to the department of agriculture of the university of bologna at the university of bologna the samples were melted and well mixed from each sample a 2 ml aliquot was taken where 30 µl of 2 m hcl was added to eliminate inorganic carbon the samples were sparged for 3 min after this 50 µl of each sample was inserted in a toc v rmn1 shimadzu analyser and combusted at a temperature of 720 c shimadzu kyoto japan at least three measurements of organic c and total n mineral and organic dissolved n were performed with the same sample a minimum of three to a maximum of five in order to have the lowest coefficient of variation among three measurements 2 6 geotop geotop is a process based distributed model of the water and energy budget rigon et al 2006 that requires a digital elevation model dem soil type vegetation and river networks in distributed maps for the catchment also meteorological data are needed to calculate the water and energy balance of the given catchment precipitation air temperature wind speed wind direction incoming short and longwave radiation which were all measured from 2011 onwards at an eddy covariance tower in the peatland pullens et al 2016 during this study the geotop model was run with an hourly time step to calculate the water and energy balance over the complete catchment therefore all input data were aggregated to hourly values geotop incorporates precipitation to calculate evapotranspiration runoff and seepage to model the horizontal and vertical movement of the water in the soil a fully three dimensional description of the richards equation was used endrizzi et al 2014 while it soles the heat conduction equation for the ground heat and water budgets in one dimension for this modelling exercise the authors used the geotop version 2 0 which also simulates freezing soils dall amico et al 2011 and a multi layered snow cover endrizzi et al 2014 which is necessary since the monte bondone site receives snow every winter the modelled period was october 1 2011 until january 1 2016 where the initial three months were used as a spin up the temperature and precipitation at higher altitudes than the tower were calculated using lapse rates for temperature 6 5 c km 1 and a precipitation lapse rate of 0 2 mm km 1 gubler et al 2013 which are the average lapse rate values calculated for the complete autonomous province of trento eccel et al 2012 the air temperature threshold when the precipitation was considered as snow instead of rain was set to 0 c the soil and plant parameters are needed as input variables in geotop and are crucial for the water and energy dynamics all plant parameters were set constant over the complete modelling period as no detailed in situ information was available over the complete modelling period on the dynamics of the vegetation in the complete catchment table 2 the catchment was partitioned into four different soil types bare rock scree peat a mineral soil and a mineral soil with high hydraulic conductivity appendix 3 a mineral soil with a high hydraulic conductivity an artificial pathway was needed to be able to model water flowing into the peatland an average of measured values of hydraulic conductivity was used for the peatland more details of the field measurements are presented in the results section for the rocks and grassland the hydraulic conductivity values were initially taken from literature holden and burt 2003 xie et al 2012 lewis et al 2012 gubler et al 2013 cunliffe et al 2013 endrizzi et al 2014 della chiesa et al 2014 and were manually varied minimal 10 around those values but within the measured ranges which are presented in the mentioned references table 3 the depth of the bedrock impermeable soil layer was set to the measured maximum depth of the peat of 4 m dalla fior 1969 the soils in the catchment were divided in 6 layers with increasing depth from 0 1 to 4 m 0 0 1 0 1 0 4 0 4 0 5 0 5 1 5 1 5 2 5 2 5 4 m the soil depth of the bare rock and rock with a very shallow layer of soil was set to 0 5 m while for the other sites a constant soil depth of 4 m was used geotop calculates the water and energy balance of each 25 m size grid cell composing the catchment to validate the model the output of the grid cell where the eddy covariance tower is located was compared with the measured 5 cm depth soil temperature and water content with snow depth outgoing short and longwave radiation the latent and sensible heat fluxes and the stream discharge of the complete catchment 3 results 3 1 bulk density and soil carbon the bulk density was sampled to explore the soil variability and to identify the plots for hydraulic conductivity measurements laliberte et al 1966 assouline 2006 the bulk density of all samples ranged from 0 04 to 0 22 g cm3 while its averages were 0 10 0 03 0 14 0 02 and 0 16 0 05 g cm3 at 0 5 50 55 and 100 105 cm depth respectively no significant difference between the three replicates at each of the nine plots was found data not shown tukey test α 0 05 the bulk density of all the plots was significantly higher at 100 105 cm depth compared to 0 5 cm depth p 0 002 while there was no statistical difference between 50 and 55 cm depth and 100 105 cm depth and 0 5 cm depth and 50 55 cm depth at the 0 5 cm depth layer the bulk density of two plots 3 and 7 fig 2 a both located close to the edge of the peatland and with plant species common to grasslands e g deschampsia cespitosa and primula farinosa was statistically higher than all the other samples at the same depth fig 2 tukey test α 0 05 no statistical difference between plots was found at deeper layers the mean soil carbon to nitrogen c n ratios were 23 61 8 98 20 34 4 68 and 20 13 5 21 at 0 5 50 55 and 100 105 cm depth respectively the c n ratio of the peat was only significantly higher at 0 5 cm depth under a c vulgaris dominated plot plot 9 fig 2b there was no significant difference among the depths 3 2 hydraulic conductivity since the highest variation in bulk density was observed between sample plots 1 3 4 7 and 9 the horizontal and vertical saturated hydraulic conductivity was measured at these plots in general the hydraulic conductivity of the peatland was very low but the variation between the plots was very high table 4 the horizontal saturated hydraulic conductivity khsat at the surface 0 5 cm ranged from 2 11 10 6 m s to 2 74 10 4 m s a difference of two orders of magnitude while at deeper depth 40 50 cm the khsat ranged from 9 0 10 8 m s to 1 51 10 5 m s a difference of three orders of magnitude the vertical saturated hydraulic conductivity showed the same variability kvsat at the surface 0 5 cm ranged from 2 35 10 6 m s to 2 24 10 5 m s a difference of one order of magnitude and at deeper depth 40 50 cm kvsat ranged from 4 0 10 7 m s to 1 55 10 5 m s a difference of two orders of magnitude at plot 3 dominated by carex nigra it was not possible to collect the sample at 40 50 cm depth because the soil was too wet it is interesting to note that at plot 9 where there is mainly calluna vulgaris the water drains very fast while at plot 4 where more sphagnum spp is located the water has a tendency to drain slowly 3 3 dissolved organic carbon the doc concentrations at both inflows were low while the outflow showed higher values fig 3 the average amount of doc over the 9 month period is 0 72 0 26 mg l 1 34 1 07 mg l and 3 02 0 76 mg l for inflow 1 2 and the outflow respectively the average difference between the outflow and the sum of the inflows is 0 96 1 34 mg l indicating a carbon loss a peak in doc concentrations was observed in inflow 3 in the autumn when the vegetation was senescing after snowmelt there was a peak in inflow 2 fig 3 indicating a source of doc from vegetation located higher in the catchment 3 4 geotop over the four years 1st january 2012 until 1st january 2016 geotop was able to simulate the soil temperature at 5 cm depth very accurately fig 4 and table 6 with a small overestimation in the years 2012 and 2013 during the period of snow cover the model did not perform as well as the snow free period table 6 over the complete catchment the peatland had the smallest amplitude in the soil temperature of all soil types appendix 1 due to the higher soil water content appendix 2 the outgoing shortwave radiation in winter is high due to the reflection of the light by the snow and the values rapidly decrease during the course of snowmelt fig 5 the model slightly underestimated the measured outgoing shortwave radiation table 6 fig 5 replicating measurements better during snow cover than in the snow free period table 6 for outgoing longwave radiation the model performs overall better than for shortwave radiation table 6 fig 6 the model tracks the measured seasonality and its outputs are in the same range as the measured values particularly in the snow free period table 6 the modelled soil water content of the catchment has a large spatial and temporal variability appendix 2 in the peatland the measured soil water content during the snow free period ranged between 25 and 90 the model was able to follow the dynamics of the soil water content swc over the years fig 7 the swc measured under molinia caerulea and c vulgaris correlated well when no snow cover was present r2 0 75 rmse 11 8 swcmolinia 11 5 0 76 swccalluna the output of the model replicated the data from the sensor under c vulgaris slightly better than the data from the sensor under m caerulea fig 7 and table 6 overall the model was able to follow the dynamics measured in the field even the big drop in soil water content at the beginning of 2015 was captured by the model in 2014 which was the wettest year of the simulation the model overestimated the soil water content during the summer months after the big rain events occurred the model represented the soil water content accurately in 2012 and 2013 with an overestimation of the swc from mid 2015 onwards the modelled heat fluxes were compared with the measured non gap filled latent table 6 fig 8 and sensible table 6 fig 9 heat fluxes measured with the eddy covariance system over the peatland pullens et al 2016 the modelled latent heat fluxes were overestimated fig 8 while the sensible heat fluxes were underestimated fig 9 the positive sensible heat fluxes occur when the air temperature is colder than the soil temperature and indicate a heat flux from the soil surface the simulated monthly energy balance of the peatland is illustrated in fig 10 most of the net radiation is converted by the system into latent heat flux annual 51 with a maximum of 65 in august the model is able to track the interannual variability of the snow height correctly for the 4 years table 6 fig 11 the measured snow height ranged from 43 to 204 cm in 2012 and 2014 at the beginning of the 2014 2015 winter the measured air temperature was below 0 c therefore the model simulated snow while in the field no snow was present yet the modelled discharge is spiky as expected given the small catchment size fig 12 a and b these spikes are not in the episodic measurements of the stream flow since the flow was not measured immediately after a precipitation event 2014 was the wettest year and the modelled discharge is higher than the episodic measurements in 2015 the driest year the model is much closer to the episodic measurements the model simulated a mean annual flow of 29 36 50 and 20 l s for 2012 2013 2014 and 2015 respectively during 2012 2015 the average annual measured precipitation at the peatland was 1318 mm year the evapotranspiration was 214 mm year 16 of precipitation and the discharge 1148 was mm year 84 of precipitation the modelled monthly water balance is depicted in fig 13 in winter there was very low evapotranspiration during spring april and may the snow melts which results in a high discharge indicating that precipitation was stored as snowpack while in november and december the precipitation is higher than the discharge indicating the arrival of snow the estimation of discharges was required to determine the doc flux the annual doc export was calculated with the mean monthly concentration of doc 0 96 1 34 mg l and the modelled monthly stream flow table 5 the carbon balance of the peatland over the years 2012 2015 including co2 ch4 and doc fluxes was 168 3 82 0 g c m 2 yr 1 table 5 indicating a loss of carbon 4 discussion the vegetation in the peatland is heterogeneous which results in differences in bulk densities and c n ratios the occurrence of some grassland plant species in two plots could suggest that the peatland is slowly being transformed into a grassland via the encroachment of grassland and woody plant species surrounding the peatland gerdol et al 2008 stine et al 2011 holmgren et al 2015 at the plot where c vulgaris was located plot 9 the bulk density did not differ significantly from the other plots while the c n ratio of the soil top layer was higher than any other plots due to the differences in vegetation and bulk densities the hydraulic conductivity in the peatland was highly variable with differences of two three orders of magnitude plot 1 3 4 7 and 9 table 4 differences of these orders of magnitude have been observed in other peatlands lewis et al 2012 cunliffe et al 2013 and can arise from different humification and fibrosity of the peat hoag and price 1995 cunliffe et al 2013 the high spatial variability of the hydraulic conductivity introduces a high uncertainty in the simulation of soil water content for this model study the average of the hydraulic conductivity of the plots with the most abundant plant species was used table 4 the hydraulic conductivity of the rock and mineral soils in the catchment is based on a literature values and on model evaluations the error introduced into the model by using literature values cannot be quantified because no hydraulic conductivity has been measured in the other soil types due to the low moisture content of these soils there was no suction of the soil to remain in the sampler and therefore no samples were taken during the first trial simulations the water did not flow into the peatland the water stagnated around the peatland which indicates that the water in the field had quick preferential pathways this was possibly due the existence of macropores that make the soil drain faster through seepage and underground water flows which are not possible to simulate and quantify in the field nousiainen et al 2015 differences in water retention between different soil types may also play a role in addition the coarse model grid may have cancelled out the small scale sub grid variability with the addition of a mineral soil with high hydraulic conductivity along water pathways the model was able to simulate a superficial underground flow overall the model performs better during the snow free period when the model simulated the soil water content and soil temperature well and was able to follow the interannual variability of the soil temperature for all the four years for almost all outputs a lower coefficient of determination was found for the period of snow cover apart from sensible heat and outgoing shortwave radiation both these results could potentially be caused by the use of a static non dynamic vegetation nevertheless the low values of rmse indicate a reasonable fit for all outputs under snow cover the comparison between the data from the eddy covariance tower and the geotop simulation also indicates that the model reasonably represents the energy balance even with the vegetation parameters set as static non dynamic in the winter of 2014 2015 snow was modelled earlier than it was measured in the field in geotop the snowfall is simulated based on the measured air temperature at 2 m the threshold for precipitation to fall as snow was set to 0 c the mismatch between model outputs and measurements could indicate that the air temperature at 2 m is not a good indicator of predicting snow since the soil temperature could be too high resulting in immediate snowmelt the relatively high latent heat flux compared to the net radiation was comparable with the geotop simulation of the rott catchment hingerl et al 2016 supporting the suitability of geotop to model complex catchments containing peatland ecosystems the rott catchment is located in a pre alpine region of southern germany ranging from 550 to 850 m asl and the predominant land use types are pasture 44 coniferous 37 and mixed forest 18 overall the energy balance and its components simulated by the model follow the expected pattern and are similar to the measured values of the heat fluxes figs 8 and 9 the model simulates a low sensible heat and a high latent heat flux compared to the measurements this is most likely due to the fact that in the model the vegetation is set as a constant without any seasonal trends this results in mismatches between the model outputs and the measurements which have been seen in other simulations with geotop in complex mountainous terrains hingerl et al 2016 a sensitivity analysis indicated that for this system the latent heat is most sensitive to changes in plant height where an increase in plant height up to 1 m increased the latent heat while a decrease in plant height decreased the latent heat flux the sensible heat flux was very sensitive to changes in canopy fraction and lai when both lai and canopy fraction were reduced separately to 0 5 and 50 respectively the sensible heat flux increased notably while an increase of lai to 4 would reduce the sensible heat flux the model seems to overestimate the stream discharge and simulates high discharge peaks which were not measured in the field but may have occurred due to the lack of continuous monitoring of discharge possible explanations for the differences between the measurements and the model output can be the lack of measurements just after heavy rain events the high spatial variability of the hydraulic conductivity which can give high percentage errors in the water budget koerselman 1989 and or the fact that after a precipitation event the runoff flows over the banks additionally to the occurrence of flow overbanks no stage discharge relation could be established because of the dynamic shape of the stream during the field campaign the measured streamflow velocity was between 0 and 0 1 m s this is within the accuracy of the instrument in 1914 the top layer of some sections of the peatland was harvested cescatti et al 1999 which could have resulted in pathways where the water drains faster subsequently there are now some flat areas where water stagnates the occurrence of these pools in the catchment is not incorporated in geotop while in the peatland there are some areas with standing water almost year round the size of these pools vary significantly over the year which indicates they could act as a buffer for the discharge resulting in the modelled discharge being higher than the measured discharge since the complete catchment is located on a deep impermeable morainic strata there is seepage into the fen cescatti et al 1999 the seep reaches the surface in the peatland in two places where it forms two streams inflows due to the complex topography of the catchment and the variation in soil depth and soil composition in the catchment the flow in the unconfined aquifer is not quantifiable a possible existence of a confined aquifer cannot be identified in the field this could lead to an additional sink of water not integrated in the model holden and burt 2002 lowry et al 2009 lewis et al 2012 cunliffe et al 2013 the depth of the soils in the complete catchment is variable in the peatland the depth ranges from 0 8 m at the border to 4 4 m in the centre dalla fior 1969 these irregularities result in the variability in the amount of water that can be stored in this soil column in the model a constant soil thickness for the peatland has been assumed and due to the coarse grid size the variability in the soil was cancelled out the average annual water balance shows that the highest amounts of discharge are at the end of the snow period in these periods the snow is melting which results in high runoff the model is able to simulate the evapotranspiration clear seasonal trend with its very low values 5 mm month during the period of snow cover which is also found in the measured data 4 1 dissolved organic carbon just before snowfall the concentration of doc has a small spike in all streams which could be coming from the vegetated area higher up the mountain where snow arrived earlier in the year during the first days after snowfall the snow melts during daytime and the water drains into the soil during this period the water could have collected doc in the forested area located above the peatland only after snowmelt the doc concentration in one of the two inflows inflow 2 was higher than the outflow for all the other samples the doc in the outflow was higher than the sum of the two inflows indicating a carbon loss of the peatland via doc the monthly measurement of doc used in this study was an adequate interval of sampling to calculate the annual flux of doc since the changes in flow are a bigger contribution to annual doc export than changes in doc concentration grünheid et al 2005 worrall et al 2006 nilsson et al 2008 koehler et al 2009 the carbon lost via doc can have a big influence on the carbon balance of the ecosystem gielen et al 2011 in pristine peatlands the doc can account for 20 40 of the net ecosystem co2 exchange roulet et al 2007 nilsson et al 2008 koehler et al 2011 also in drained peatlands a big carbon loss through doc has been measured even up to 3 3 1 5 kg doc over several days after draining a 900 m2 poor fen strack et al 2008 the concentration of doc at the monte bondone peatland site is at the lower range of doc measured at peatlands freeman et al 2004 munir et al 2015 due to the aerobic conditions in the soil the dead organic matter was decomposed and mineralized to co2 the most abundant plant species is m caerulea which decomposes fast van vuuren et al 1993 and results in high ecosystem respiration pullens et al 2016 also with a deep water table the doc values were expected to be high freeman et al 2004 worrall et al 2006 dieleman et al 2016 the fact that the doc values were low could indicate that most of the carbon is emitted via ecosystem respiration and that the main source of doc were the partially decomposed recalcitrant carbon compounds in the soil such as root exudates fenner and freeman 2011 hribljan et al 2014 dieleman et al 2016 the modelled annual water balance indicates that 87 of the precipitation leaves the catchment as discharge in accord with abera et al 2017 that studies a catchment sharing the same climatic conditions based on the average of the measured doc concentrations and the monthly average modelled discharge values an initial estimate of the total doc export can be calculated it has to be taken into account that this method introduces high uncertainties in the total carbon export since it is a combination of the uncertainty of the measured water flow the uncertainty in the model outcomes and the uncertainty of the doc measurements in this study the modelled discharge is likely to be overestimated which results in an overestimation of the doc export the small measured amount of doc in combination with the modelled discharge results in a small export of doc from the peatland over the four years the average of the loss of doc is 10 2 3 8 g c m 2 yr 1 table 5 this is comparable to a blanket bog peatland in ireland 14 1 1 5 g c m 2 yr 1 for 2007 koehler et al 2009 an upland peat catchment in england 9 4 g c m 2 yr 1 worrall et al 2003 a subarctic atlantic blanket bog in norway 7 2 0 7 g c m 2 yr 1 de wit et al 2016 a fen in minnesota united states 10 0 3 0 g c m 2 yr 1 pastor et al 2003 at a restored peatland on former agricultural land in the netherlands the doc export is higher 20 6 4 3 g c m 2 yr 1 hendriks et al 2007 also a mixed acid mire system in sweden 17 8 0 7 g c m 2 yr 1 nilsson et al 2008 a rich fen in minnesota united states 21 2 6 9 g c m 2 yr 1 urban et al 1989 were found to have higher losses the combination of doc with the other two carbon fluxes co2 and ch4 pullens et al 2016 indicate that the peatland is acting as a carbon source for all four subsequent years with a carbon loss ranging from 95 3 to 273 8 g c m 2 yr 1 5 conclusion this study presents a study of a peatland on the monte bondone plateau near trento in the eastern italian alps conducted with the model geotop v2 0 and an ad hoc field campaign the peatland has a heterogeneous vegetation which results in diverse soil bulk density and hydraulic conductivity there was therefore the necessity to make some hypotheses in the setup of simulations to fill the gap of knowledge with estimates regarding some controls especially hydraulic conductivity on the water dynamics overall geotop was able to simulate the water and energy dynamics of the peatland in a complex catchment over four years and obtain satisfying outputs which were in good agreement with the available measured values the bulk density and hydraulic conductivity of the peatland were measured from in situ samples to investigate soil variability the modelled streamflow has higher discharge peaks compared to the infrequent measured discharge nevertheless the model was able to simulate the interannual variability of the volumetric water content and soil temperature accurately the comparison of the modelled energy fluxes and the measured values from the eddy covariance tower indicate that geotop is able to follow the seasonal patterns and is within the same range of the measured values the model has a smaller difference of the energy balance compared to the measurements made by the on site eddy covariance tower based on the modelled discharge and the measured doc values an initial estimation of doc export of the peatland through extrapolation was made the doc concentration in the stream water was low but the high stream flows lead to similar doc losses compared to other peatlands the study shows that a process based hydrological model can be used to study the water and energy dynamics of a peatland in a mountainous area for the peatland at monte bondone this resulted in the estimate of four years of carbon balance co2 ch4 fluxes and doc losses combined which indicated that the peatland was a carbon source for all four subsequent years the carbon balance was 111 2 275 2 189 5 and 97 3 g c m 2 yr 1 for 2012 2015 respectively 6 replicable research geotop model is freely available as indicated on the site http www geotop org the orthophoto and dem data of the site are available from autonomous province of trento p a t from http www urbanistica provincia tn it sez siat banchedati repertoriocartografico pagina3 html data from the campaign and hydrometeorological data used in this paper are available from the authors a simplified test case of this study is online available at https github com jeroenpullens geotop monte bondone sample acknowledgements the authors would like to thank the two anonymous reviewers for their useful suggestions and comments the authors would also like to thank pablo torralba rubio marilu solas obra lorenzo frizzera roberto zampedri and mauro cavagna for field assistance the authors would also like to thank paola gioacchini of dipsa bologna for the analysis of the doc samples the authors would also like to thank the geotop community for their help and support the authors also like to thank cristina bruno and bruno maiolini for lending the fp111 global flow probe one of the authors work r rigon was partially financed by the trento university project climaware appendix a supplementary data supplementary data associated with this article can be found in the online version at https doi org 10 1016 j jhydrol 2018 05 041 appendix a supplementary data supplementary video 1 supplementary video 2 supplementary data 3 
7134,over the millennia peatlands have sequestered and stored carbon in the form of peat that today accounts for around 30 of the global soil organic carbon the peat has formed and accumulated due to the slow decomposition rate of organic matter in waterlogged anaerobic conditions the particularly close interdependence of the carbon and water cycles in peatland ecosystems signal the importance of understanding the water cycle to the functioning of peatlands with this aim the water and energy cycle of an alpine catchment in italy which includes a peatland was studied using the process based hydrological model geotop and a set of in situ measurements over 4 years 2012 2015 this is a challenging modelling exercise that has not been tried before with geotop the catchment is heterogenous with land covers of peatland grassland scree and bare rock in a mountainous area the geotop model was able to replicate the energy and water fluxes measured by an eddy covariance tower located in the peatland as well as the volumetric water content and soil temperature of the peatland accurately over the four years this study shows that a process based hydrological model can be used to study the water and energy dynamics of a peatland in a mountainous area keywords peatland hydrology geotop energy balance water balance alps 1 introduction over millennia global peatlands have stored up to 547 473 621 pg of soil organic carbon in the form of peat gorham 1991 yu 2012 which is around 30 of the global soil organic carbon gorham 1991 peatland formation is dependent on water as the decomposition of organic matter is very slow in waterlogged and anaerobic conditions resulting in organic matter accumulation in peat layers up to several metres pristine northern latitude peatlands are today mainly acting as carbon sinks lund et al 2010 mcveigh et al 2014 while disturbed peatland e g harvested and drained predominantly act as carbon sources hendriks et al 2007 petrescu et al 2015 aslan sungur et al 2016 the accumulated carbon in peatlands can be lost via three fluxes as co2 through respiration as methane ch4 emissions and or as dissolved organic carbon doc in stream runoff these carbon fluxes are influenced by environmental and meteorological conditions lafleur et al 2003 camill 2005 frolking et al 2011 mcveigh et al 2014 aslan sungur et al 2016 apart from the climate change many factors can influence the fate of the stored carbon such as ground water level roulet et al 2007 murphy et al 2009 sottocornola and kiely 2010 koehler et al 2011 lewis et al 2013 miller et al 2015 pullens et al 2016 and different management strategies hendriks et al 2007 wilson et al 2007 van der molen et al 2007 baldocchi et al 2012 the source of water ombrotrophic or minerotrophic and its dissolved minerals is important for peatland ecosystems differences in water sources and the dissolved nutrients lead to different plant communities koerselman 1989 the water cycle and in particular the variation in water table level plays a crucial role in the functioning of peatland ecosystems gorham 1957 the level of the water table impacts the carbon balance of peatlands since a raise in water table depth can result in higher methane emissions wagner et al 2003 jungkunst et al 2008 and or in more runoff and thus a removal of organic matter through doc in the stream water a rising water table will also reduce the respiration of the soil due to the occurrence of anaerobic soil conditions sonnentag et al 2010 mcveigh et al 2014 strachan et al 2016 on the contrary a decrease in precipitation will lead to the drying of peatlands these less waterlogged soils have a higher decomposition rates of the peat the stored carbon in the soil is vulnerable to environmental management and climate change lafleur et al 2003 camill 2005 frolking et al 2011 mcveigh et al 2014 aslan sungur et al 2016 the predicted rising temperatures from climate change will likely result in higher evaporation and decomposition rates which may change the peatlands from natural carbon sinks to carbon sources to resolve the uncertainties around the future functioning of these ecosystems it is important to understand the dynamics of the water table and thus the water cycle of the peatland catchment therefore improving the understanding of the water cycle and the energy budget is the aim of this work and is possible through the use of a detailed resolution process based model fatichi et al 2016 many studied peatlands are located in high latitude regions where the vulnerability of their carbon storage the effects on climate change and permafrost degradation has been investigated frolking et al 2001 camill 2005 dorrepaal et al 2009 beside the occurrence of peatlands at high latitudes peatlands also occur at high elevation although less research has been done on the latter because they are not the dominant ecosystems in those areas parish et al 2008 even though there are numerous peatlands in the alps which are being threatened by rising temperatures and changes in the precipitation regime beniston et al 1997 ipcc 2007 2013 im et al 2010 eccel et al 2012 steger et al 2013 pepin et al 2015 tudoroiu et al 2016 their carbon energy and water cycle has been little studied to the authors knowledge the carbon fluxes has been studied continuously over multiple years only in the monte bondone alpine peatland pullens et al 2016 which is the focus of this paper to analyse the energy and water cycles in complex ecosystems hydrological models are an essential tool for water and environment resource management devia et al 2015 fatichi et al 2016 there are various hydrological models available todini 2007 ranging from models based on the unit hydrograph to complex process based catchment models beven and kirkby 1979 gregory et al 1996 rinaldo and rodriguez iturbe 1996 ewen et al 2000 vivoni et al 2004 rigon et al 2006 2016 endrizzi et al 2014 the requirements and research questions of the user of the model determine the choice of which model to be used devia et al 2015 because of the multiplicity of interactions responsible for peatlands dynamics a process based model of the water and energy budget in conjunction with a series of in situ measurements is desirable fatichi et al 2016 in this paper the process based hydrological model geotop rigon et al 2006 was implemented on a catchment in the italian alps geotop v 2 0 models the energy and water budgets at and below the soil surface endrizzi et al 2014 and provides the soil vegetation atmospheric interactions svat for complex terrain the model has previously been implemented by lewis et al 2013 to study the effect of afforestation on the hydrology of a blanket peatland in ireland where it was shown that geotop is a suitable tool for modelling the hydrology in a peatland the model has also reproduced the spatial patterns of land surface temperature soil moisture and the energy balance components in alpine and pre alpine environments bertoldi et al 2006 2010 2014 della chiesa et al 2014 hingerl et al 2016 to date geotop has not been used to simulate the water and energy balance over multiple years of an alpine catchment in which a fen type of peatland is located specifically the paper aims at investigating 1 the possibility to simulate the water and energy budgets of an alpine peatland with a process based model and 2 the capability of the model to reproduce the observed interannual variability 2012 2015 of the energy fluxes in a mountain area the two objectives are addressed using the geotop v 2 0 model and in situ measurements the modelled water cycle was combined with doc concentration measurements to estimate the doc loss from the peatlands and complete the quantification of the main components of the ecosystem carbon budget 2 materials and methods 2 1 the site the study site is a 10 ha minerotrophic fen located at 1563 m asl on the monte bondone plateau near trento in the eastern italian alps fig 1 a 46 01 n 11 02 e the peatland is located in a 93 ha catchment and includes part of the palon mountain and the surrounding slopes to the mountain tops cima verde dos d abramo and cornetto the catchment area was calculated by means of the qgis software version 2 6 brighton with the dem resolution 1 by 1 m provided by the autonomous province of trento the catchment consists of bare rock limestone scree managed grassland cut once a year coniferous forest norway spruce picea abies and european larch larix decidua and a minerotrophic fen type peatland the resolution of the modelled catchment was rescaled to 25 by 25 m to decrease the model s computational demand the peatland is a relic of an ancient lake that was formed by a glacier during the last ice age cescatti et al 1999 the thickness of the peat layer ranges from 80 cm at the border to 4 4 m in the centre dalla fior 1969 in 1914 the top peat layer of 0 35 ha of the peatland was harvested for fuel cescatti et al 1999 the peatland has different microforms from high to low hummocks lawns and hollows where the lawns are the flat areas in the peatland the peatland at monte bondone has a diverse vegetation perucco et al 2013 the lawns are dominated by either grasses molinia caerulea or sedges carex nigra and eriophorum vaginatum while the main vegetation in the hollows consists of carex rostrata and the mosses scorpidium cossonii sphagnum subsecundum and s compactum the few short hummocks are covered by s compactum and s sect acutifolia and calluna vulgaris in the centre of the peatland there is a small open pool which can act as a discharge buffer for the discharge after a precipitation event and has a storage capacity of 15 30 m3 close to this pond the common vegetation are birches betula pendula and cotton deergrass trichophorum alpinum the vegetation at plot 1 2 4 5 and 6 fig 1b are mainly dominated by m caerulea at plot 4 t alpinum has a high cover and plot 4 is the only plot where sphagnum species are highly present s subsecundum apart from m caerulea the other abundant occurring plant species are at plot 1 c vulgaris t alpinum at plot 2 c nigra at plot 5 aulacomnium palustre equisetum palustre tomentypnum nitens and at plot 6 campylium stellatum c rostrata s cossonii at the other plots no m caerulea is present plot 3 is dominated by c nigra plot 7 is dominated by c rostrata and valeriana dioica plot 8 by s cossonii and plot 9 by c vulgaris the water table of the complete catchment flows on deep impermeable morainic strata which seeps into the fen cescatti et al 1999 seepage reaches the surface in the peatland at two ephemeral springs defined as inflows fig 1b this water flows through superficial streams to the lowest point of the peatland where the water drains out of the peatland into a perennial stream defined as outflow fig 1b the modelling exercise covered the period from 2012 to 2015 the average annual air temperature of 2012 2014 and 2015 were close to the long term average of 5 4 c while the temperature in 2013 was lower table 1 in the first three years the precipitation was higher than the long term average of 1290 mm with 2014 being an exceptional wet year while in 2015 the precipitation was well below the long term average table 1 2 2 experimental setup an eddy covariance instrumented tower was mounted in the centre of the peatland in 2011 to measure water carbon and energy fluxes and meteorological conditions a shielded probe rotronic m103a bassersdorf germany measured air temperature and relative humidity at 2 m height incoming photosynthetically active radiation par was measured by a licor 190sz sensor licor lincoln ne usa while the incoming and outgoing short and longwave infrared radiations were measured by a cnr1 kipp zonen delft the netherlands both of the latter two sensors were installed on a horizontal bar at 3 5 m above the soil surface the volumetric soil water content was measured between tussocks of m caerulea and under c vulgaris at 5 cm depth with a cs616 water content reflectometer campbell scientific logan ut usa the distance between these two sensors is approximately 10 m at these locations also the soil heat flux was measured with a hfp01 soil heat flux plates hukseflux thermal sensors b v delft the netherlands the soil temperature was measured along a profile at 2 5 10 20 and 50 cm depth with a stp01 sensor hukseflux thermal sensors b v delft the netherlands from the 1st july 2012 all data were collected once a minute on a cr3000 data logger campbell scientific logan ut usa with an am16 32 multiplexer campbell scientific logan ut usa and averaged to half hourly values wind speed wind direction and sensible heat fluxes were measured at 20 hz with a r3 100 3d sonic anemometer gill instruments lymington hampshire uk the latent heat fluxes were measured with the eddy covariance system consisting of two infrared gas analysers li7500 and li7200 licor lincoln ne usa the data of the gas analysers and the anemometer were stored on an industrial computer fx5507 fabiatech taipei taiwan located in the field pullens et al 2016 on 16th december 2013 a heated tipping bucket rain gauge model 52202 young traverse city mi usa was installed before then the precipitation data from a meteorological station located 400 m away from the ec tower at a similar altitude 1490 m asl were used meteo trentino station name giardino in addition the total snow height and the snow density of the freshly fallen snow were daily manually measured at this location by the provincial meteorological service meteo trentino missing meteorological data approximately 20 of the half hourly data points due to malfunctioning of the sensors or power shortage were replaced by data from a nearby 500 m horizontal distance eddy covariance tower located in a grassland fig 1a marcolla et al 2011 sakowska et al 2014 for soil temperature a dissimilarity was expected between the grassland and the peatland but a highly significant relationship between the two sites was found for all depths r2 0 95 tsoil peatland 5cm 0 46 0 94 tsoil grassland 5cm the remaining gaps in the meteorological data 0 9 caused by malfunction of both towers were replaced by data from two nearby meteorological stations of meteo trentino fig 1a giardino botanico and viote the latter 1 km away from the peatland tower soil temperature was not measured at the meteo trentino weather stations therefore when missing at both ec towers soil temperatures were replaced by linear interpolations a more detailed overview of the site and the instruments is described in pullens et al 2016 the measured soil water content data were not reliable when the soil was covered by snow under these conditions the temperature of the soil at the depth of the soil water content probes 5 cm was around zero degrees celsius which resulted in soil water content measurements dropping to unrealistic values 20 these low soil water content measurements could suggest a frozen soil and are therefore not included in the manuscript 2 3 soil properties soil samples were collected on 3 4 july 2014 for physical and chemical analyses under all nine previously described main vegetation types occurring in the fen fig 1b at each point three soil samples replicates located at 1 m distance were collected at different depths 0 5 50 55 and 100 105 cm the samples at 0 5 cm were collected with eijkelkamp soil core sampler rings diameter 5 cm height 5 cm volume 98 cm3 while at the depths of 50 55 and 100 105 cm the samples were collected with an eijkelkamp split tube sampler both instruments eijkelkamp giesbeek the netherlands same dimensions as the soil core sampler rings overall 81 samples were collected from the 9 plots samples that were too compacted due to compression by the sampling spongy tissue were discarded as well as samples which were too wet i e slurry in total 50 soil samples were transported in closed plastic bags to the laboratory for analyses the samples were composed of 27 samples from 0 to 5 cm 15 samples from 50 to 55 cm and 8 samples from 100 to 105 cm depths each sample was weighed on the day of sampling before it was oven dried at 55 c until a stable mass was reached after 24 h to calculate the bulk density the bulk density was sampled to explore the soil variability and to identify the plots for hydraulic conductivity measurements assouline 2006 an increase in soil bulk density associated with compaction is known to influence soil hydraulic properties laliberte et al 1966 after this the soil was grinded with a mortar and pestle to break clumps of peat before it was sieved 2 mm sieve to remove leaves and roots a subset of the sample was placed in a metal container with a metal ball and inserted in a mixer mill mm 200 retsch haan germany which was run at the speed of 25 hz for 5 min the rest of the sample was placed in a 105 c oven to measure the remaining water content a homogenous subsample 5 mg from the ball milled samples was inserted in tin caps and the amount of soil carbon and nitrogen was measured in a perkinelmer 2400 series ii chns o elemental analyser perkinelmer waltham ma usa before statistical analysis the bulk density data were tested for normality in this study all values were normally distributed shapiro wilk test w 0 99 p value 0 9 for the statistical analysis of the bulk density the tukey s hsd honest significant difference test was used using the r software version 3 1 2 r development core team r foundation for statistical computing 2017 2 4 hydraulic conductivity the saturated hydraulic conductivity of the peatland soil was measured using the adjusted modified cube method presented in lewis et al 2012 in literature different methods are used to measure the hydraulic conductivity such as the standpipe technique hvorslev 1951 and the modified cube method beckwith et al 2003 an advantage of the mcm over the standpipe technique is that with the mcm both the horizontal and the vertical hydraulic conductivity can be measured in august 2014 two soil samples from 5 of the 9 bulk density plots were sampled at 0 10 and 40 50 cm depths these plots were chosen because they showed the biggest range of bulk density see results section and therefore the extent of hydraulic conductivity in the peatland could be analysed assouline 2006 a metal tube with a square section 12 12 100 cm and a cutting edge was driven into the peat to a depth of 70 cm after the vegetation was removed from the top soil to prevent damage to the peat core the tube was extracted and due to suction the sample stayed in the sampling tube during the retrieval after retrieval of the column samples at 0 10 and 40 50 cm depths were cut with a blunt knife the samples were stored in a plastic bag and a container before being transported to the laboratory immediately after returning to the laboratory the samples were cut into cubes of sides 10 cm and sealed with molten paraffin to assure a good seal the cubes were dipped into paraffin until the thickness of the paraffin around the block was about 1 cm after the blocks were completely sealed the paraffin was removed from two opposite sides on the top of the block a wooden collar was added and the seams were blocked with paraffin since the measurement of the saturated hydraulic conductivity needs a constant pressure head a large reservoir of water was connected with a tube to the peat samples to create a syphon which maintained a constant pressure head of 7 cm the water used during the experiment was collected in the outflow stream to eliminate any influences of different chemical signatures of water from a different source lewis et al 2012 the peat blocks were positioned on a fine mesh in a plastic bowl where a tube was inserted 1 cm above the bottom the saturated hydraulic conductivity was calculated by using darcy s law darcy 1856 1 k sat ql a δ h in which ksat is the saturated hydraulic conductivity m s 1 q is the discharge m3 s 1 l is the length of the sample m a is the area of one open side of the cube m2 and δh is the difference in head between the top and bottom of the sample m where the constant head method was used for each peat block both the horizontal and vertical hydraulic conductivities were measured for the horizontal hydraulic conductivity the procedure was identical only the blocks were rotated 90 the hydraulic conductivity was only measured on soil samples from the peatland for mineral soils in the catchment literature values were used 2 5 water table streamflow and doc the stream heights at the two inflows and the outflow of the peatland were measured with pressure transducers dipper pt seba hydrometrie gmbh co germany the pressure transducers were installed in perforated pipes on 14th may 2014 the water level was measured every half hour and the data were stored on internal flash memory and collected at regular intervals the streamflow was measured and water samples for dissolved organic carbon doc were collected at the same locations as the stream height fig 1b the streamflow was measured 10 times during 2014 and 2015 with a fp111 global flow probe college station texas usa to convert the stream height measurement time series in discharge time series the uncertainty of the device used for the flow measurements is 0 1 m s no weir was allowed to be installed at the in and outflows as the site is a natura 2000 protected area stream flow measurements were complicated due to the dynamic cross sections of the stream channels and the occurrence of dense vegetation in the stream during the streamflow measurements stream water samples around 500 ml were collected for later doc analysis the samples were filtered in the laboratory with a millipore system using isopore membrane filters with 0 4 µm pore size merck millipore billerica ma usa after the filtration the samples were frozen and stored at 20 c before shipment to the department of agriculture of the university of bologna at the university of bologna the samples were melted and well mixed from each sample a 2 ml aliquot was taken where 30 µl of 2 m hcl was added to eliminate inorganic carbon the samples were sparged for 3 min after this 50 µl of each sample was inserted in a toc v rmn1 shimadzu analyser and combusted at a temperature of 720 c shimadzu kyoto japan at least three measurements of organic c and total n mineral and organic dissolved n were performed with the same sample a minimum of three to a maximum of five in order to have the lowest coefficient of variation among three measurements 2 6 geotop geotop is a process based distributed model of the water and energy budget rigon et al 2006 that requires a digital elevation model dem soil type vegetation and river networks in distributed maps for the catchment also meteorological data are needed to calculate the water and energy balance of the given catchment precipitation air temperature wind speed wind direction incoming short and longwave radiation which were all measured from 2011 onwards at an eddy covariance tower in the peatland pullens et al 2016 during this study the geotop model was run with an hourly time step to calculate the water and energy balance over the complete catchment therefore all input data were aggregated to hourly values geotop incorporates precipitation to calculate evapotranspiration runoff and seepage to model the horizontal and vertical movement of the water in the soil a fully three dimensional description of the richards equation was used endrizzi et al 2014 while it soles the heat conduction equation for the ground heat and water budgets in one dimension for this modelling exercise the authors used the geotop version 2 0 which also simulates freezing soils dall amico et al 2011 and a multi layered snow cover endrizzi et al 2014 which is necessary since the monte bondone site receives snow every winter the modelled period was october 1 2011 until january 1 2016 where the initial three months were used as a spin up the temperature and precipitation at higher altitudes than the tower were calculated using lapse rates for temperature 6 5 c km 1 and a precipitation lapse rate of 0 2 mm km 1 gubler et al 2013 which are the average lapse rate values calculated for the complete autonomous province of trento eccel et al 2012 the air temperature threshold when the precipitation was considered as snow instead of rain was set to 0 c the soil and plant parameters are needed as input variables in geotop and are crucial for the water and energy dynamics all plant parameters were set constant over the complete modelling period as no detailed in situ information was available over the complete modelling period on the dynamics of the vegetation in the complete catchment table 2 the catchment was partitioned into four different soil types bare rock scree peat a mineral soil and a mineral soil with high hydraulic conductivity appendix 3 a mineral soil with a high hydraulic conductivity an artificial pathway was needed to be able to model water flowing into the peatland an average of measured values of hydraulic conductivity was used for the peatland more details of the field measurements are presented in the results section for the rocks and grassland the hydraulic conductivity values were initially taken from literature holden and burt 2003 xie et al 2012 lewis et al 2012 gubler et al 2013 cunliffe et al 2013 endrizzi et al 2014 della chiesa et al 2014 and were manually varied minimal 10 around those values but within the measured ranges which are presented in the mentioned references table 3 the depth of the bedrock impermeable soil layer was set to the measured maximum depth of the peat of 4 m dalla fior 1969 the soils in the catchment were divided in 6 layers with increasing depth from 0 1 to 4 m 0 0 1 0 1 0 4 0 4 0 5 0 5 1 5 1 5 2 5 2 5 4 m the soil depth of the bare rock and rock with a very shallow layer of soil was set to 0 5 m while for the other sites a constant soil depth of 4 m was used geotop calculates the water and energy balance of each 25 m size grid cell composing the catchment to validate the model the output of the grid cell where the eddy covariance tower is located was compared with the measured 5 cm depth soil temperature and water content with snow depth outgoing short and longwave radiation the latent and sensible heat fluxes and the stream discharge of the complete catchment 3 results 3 1 bulk density and soil carbon the bulk density was sampled to explore the soil variability and to identify the plots for hydraulic conductivity measurements laliberte et al 1966 assouline 2006 the bulk density of all samples ranged from 0 04 to 0 22 g cm3 while its averages were 0 10 0 03 0 14 0 02 and 0 16 0 05 g cm3 at 0 5 50 55 and 100 105 cm depth respectively no significant difference between the three replicates at each of the nine plots was found data not shown tukey test α 0 05 the bulk density of all the plots was significantly higher at 100 105 cm depth compared to 0 5 cm depth p 0 002 while there was no statistical difference between 50 and 55 cm depth and 100 105 cm depth and 0 5 cm depth and 50 55 cm depth at the 0 5 cm depth layer the bulk density of two plots 3 and 7 fig 2 a both located close to the edge of the peatland and with plant species common to grasslands e g deschampsia cespitosa and primula farinosa was statistically higher than all the other samples at the same depth fig 2 tukey test α 0 05 no statistical difference between plots was found at deeper layers the mean soil carbon to nitrogen c n ratios were 23 61 8 98 20 34 4 68 and 20 13 5 21 at 0 5 50 55 and 100 105 cm depth respectively the c n ratio of the peat was only significantly higher at 0 5 cm depth under a c vulgaris dominated plot plot 9 fig 2b there was no significant difference among the depths 3 2 hydraulic conductivity since the highest variation in bulk density was observed between sample plots 1 3 4 7 and 9 the horizontal and vertical saturated hydraulic conductivity was measured at these plots in general the hydraulic conductivity of the peatland was very low but the variation between the plots was very high table 4 the horizontal saturated hydraulic conductivity khsat at the surface 0 5 cm ranged from 2 11 10 6 m s to 2 74 10 4 m s a difference of two orders of magnitude while at deeper depth 40 50 cm the khsat ranged from 9 0 10 8 m s to 1 51 10 5 m s a difference of three orders of magnitude the vertical saturated hydraulic conductivity showed the same variability kvsat at the surface 0 5 cm ranged from 2 35 10 6 m s to 2 24 10 5 m s a difference of one order of magnitude and at deeper depth 40 50 cm kvsat ranged from 4 0 10 7 m s to 1 55 10 5 m s a difference of two orders of magnitude at plot 3 dominated by carex nigra it was not possible to collect the sample at 40 50 cm depth because the soil was too wet it is interesting to note that at plot 9 where there is mainly calluna vulgaris the water drains very fast while at plot 4 where more sphagnum spp is located the water has a tendency to drain slowly 3 3 dissolved organic carbon the doc concentrations at both inflows were low while the outflow showed higher values fig 3 the average amount of doc over the 9 month period is 0 72 0 26 mg l 1 34 1 07 mg l and 3 02 0 76 mg l for inflow 1 2 and the outflow respectively the average difference between the outflow and the sum of the inflows is 0 96 1 34 mg l indicating a carbon loss a peak in doc concentrations was observed in inflow 3 in the autumn when the vegetation was senescing after snowmelt there was a peak in inflow 2 fig 3 indicating a source of doc from vegetation located higher in the catchment 3 4 geotop over the four years 1st january 2012 until 1st january 2016 geotop was able to simulate the soil temperature at 5 cm depth very accurately fig 4 and table 6 with a small overestimation in the years 2012 and 2013 during the period of snow cover the model did not perform as well as the snow free period table 6 over the complete catchment the peatland had the smallest amplitude in the soil temperature of all soil types appendix 1 due to the higher soil water content appendix 2 the outgoing shortwave radiation in winter is high due to the reflection of the light by the snow and the values rapidly decrease during the course of snowmelt fig 5 the model slightly underestimated the measured outgoing shortwave radiation table 6 fig 5 replicating measurements better during snow cover than in the snow free period table 6 for outgoing longwave radiation the model performs overall better than for shortwave radiation table 6 fig 6 the model tracks the measured seasonality and its outputs are in the same range as the measured values particularly in the snow free period table 6 the modelled soil water content of the catchment has a large spatial and temporal variability appendix 2 in the peatland the measured soil water content during the snow free period ranged between 25 and 90 the model was able to follow the dynamics of the soil water content swc over the years fig 7 the swc measured under molinia caerulea and c vulgaris correlated well when no snow cover was present r2 0 75 rmse 11 8 swcmolinia 11 5 0 76 swccalluna the output of the model replicated the data from the sensor under c vulgaris slightly better than the data from the sensor under m caerulea fig 7 and table 6 overall the model was able to follow the dynamics measured in the field even the big drop in soil water content at the beginning of 2015 was captured by the model in 2014 which was the wettest year of the simulation the model overestimated the soil water content during the summer months after the big rain events occurred the model represented the soil water content accurately in 2012 and 2013 with an overestimation of the swc from mid 2015 onwards the modelled heat fluxes were compared with the measured non gap filled latent table 6 fig 8 and sensible table 6 fig 9 heat fluxes measured with the eddy covariance system over the peatland pullens et al 2016 the modelled latent heat fluxes were overestimated fig 8 while the sensible heat fluxes were underestimated fig 9 the positive sensible heat fluxes occur when the air temperature is colder than the soil temperature and indicate a heat flux from the soil surface the simulated monthly energy balance of the peatland is illustrated in fig 10 most of the net radiation is converted by the system into latent heat flux annual 51 with a maximum of 65 in august the model is able to track the interannual variability of the snow height correctly for the 4 years table 6 fig 11 the measured snow height ranged from 43 to 204 cm in 2012 and 2014 at the beginning of the 2014 2015 winter the measured air temperature was below 0 c therefore the model simulated snow while in the field no snow was present yet the modelled discharge is spiky as expected given the small catchment size fig 12 a and b these spikes are not in the episodic measurements of the stream flow since the flow was not measured immediately after a precipitation event 2014 was the wettest year and the modelled discharge is higher than the episodic measurements in 2015 the driest year the model is much closer to the episodic measurements the model simulated a mean annual flow of 29 36 50 and 20 l s for 2012 2013 2014 and 2015 respectively during 2012 2015 the average annual measured precipitation at the peatland was 1318 mm year the evapotranspiration was 214 mm year 16 of precipitation and the discharge 1148 was mm year 84 of precipitation the modelled monthly water balance is depicted in fig 13 in winter there was very low evapotranspiration during spring april and may the snow melts which results in a high discharge indicating that precipitation was stored as snowpack while in november and december the precipitation is higher than the discharge indicating the arrival of snow the estimation of discharges was required to determine the doc flux the annual doc export was calculated with the mean monthly concentration of doc 0 96 1 34 mg l and the modelled monthly stream flow table 5 the carbon balance of the peatland over the years 2012 2015 including co2 ch4 and doc fluxes was 168 3 82 0 g c m 2 yr 1 table 5 indicating a loss of carbon 4 discussion the vegetation in the peatland is heterogeneous which results in differences in bulk densities and c n ratios the occurrence of some grassland plant species in two plots could suggest that the peatland is slowly being transformed into a grassland via the encroachment of grassland and woody plant species surrounding the peatland gerdol et al 2008 stine et al 2011 holmgren et al 2015 at the plot where c vulgaris was located plot 9 the bulk density did not differ significantly from the other plots while the c n ratio of the soil top layer was higher than any other plots due to the differences in vegetation and bulk densities the hydraulic conductivity in the peatland was highly variable with differences of two three orders of magnitude plot 1 3 4 7 and 9 table 4 differences of these orders of magnitude have been observed in other peatlands lewis et al 2012 cunliffe et al 2013 and can arise from different humification and fibrosity of the peat hoag and price 1995 cunliffe et al 2013 the high spatial variability of the hydraulic conductivity introduces a high uncertainty in the simulation of soil water content for this model study the average of the hydraulic conductivity of the plots with the most abundant plant species was used table 4 the hydraulic conductivity of the rock and mineral soils in the catchment is based on a literature values and on model evaluations the error introduced into the model by using literature values cannot be quantified because no hydraulic conductivity has been measured in the other soil types due to the low moisture content of these soils there was no suction of the soil to remain in the sampler and therefore no samples were taken during the first trial simulations the water did not flow into the peatland the water stagnated around the peatland which indicates that the water in the field had quick preferential pathways this was possibly due the existence of macropores that make the soil drain faster through seepage and underground water flows which are not possible to simulate and quantify in the field nousiainen et al 2015 differences in water retention between different soil types may also play a role in addition the coarse model grid may have cancelled out the small scale sub grid variability with the addition of a mineral soil with high hydraulic conductivity along water pathways the model was able to simulate a superficial underground flow overall the model performs better during the snow free period when the model simulated the soil water content and soil temperature well and was able to follow the interannual variability of the soil temperature for all the four years for almost all outputs a lower coefficient of determination was found for the period of snow cover apart from sensible heat and outgoing shortwave radiation both these results could potentially be caused by the use of a static non dynamic vegetation nevertheless the low values of rmse indicate a reasonable fit for all outputs under snow cover the comparison between the data from the eddy covariance tower and the geotop simulation also indicates that the model reasonably represents the energy balance even with the vegetation parameters set as static non dynamic in the winter of 2014 2015 snow was modelled earlier than it was measured in the field in geotop the snowfall is simulated based on the measured air temperature at 2 m the threshold for precipitation to fall as snow was set to 0 c the mismatch between model outputs and measurements could indicate that the air temperature at 2 m is not a good indicator of predicting snow since the soil temperature could be too high resulting in immediate snowmelt the relatively high latent heat flux compared to the net radiation was comparable with the geotop simulation of the rott catchment hingerl et al 2016 supporting the suitability of geotop to model complex catchments containing peatland ecosystems the rott catchment is located in a pre alpine region of southern germany ranging from 550 to 850 m asl and the predominant land use types are pasture 44 coniferous 37 and mixed forest 18 overall the energy balance and its components simulated by the model follow the expected pattern and are similar to the measured values of the heat fluxes figs 8 and 9 the model simulates a low sensible heat and a high latent heat flux compared to the measurements this is most likely due to the fact that in the model the vegetation is set as a constant without any seasonal trends this results in mismatches between the model outputs and the measurements which have been seen in other simulations with geotop in complex mountainous terrains hingerl et al 2016 a sensitivity analysis indicated that for this system the latent heat is most sensitive to changes in plant height where an increase in plant height up to 1 m increased the latent heat while a decrease in plant height decreased the latent heat flux the sensible heat flux was very sensitive to changes in canopy fraction and lai when both lai and canopy fraction were reduced separately to 0 5 and 50 respectively the sensible heat flux increased notably while an increase of lai to 4 would reduce the sensible heat flux the model seems to overestimate the stream discharge and simulates high discharge peaks which were not measured in the field but may have occurred due to the lack of continuous monitoring of discharge possible explanations for the differences between the measurements and the model output can be the lack of measurements just after heavy rain events the high spatial variability of the hydraulic conductivity which can give high percentage errors in the water budget koerselman 1989 and or the fact that after a precipitation event the runoff flows over the banks additionally to the occurrence of flow overbanks no stage discharge relation could be established because of the dynamic shape of the stream during the field campaign the measured streamflow velocity was between 0 and 0 1 m s this is within the accuracy of the instrument in 1914 the top layer of some sections of the peatland was harvested cescatti et al 1999 which could have resulted in pathways where the water drains faster subsequently there are now some flat areas where water stagnates the occurrence of these pools in the catchment is not incorporated in geotop while in the peatland there are some areas with standing water almost year round the size of these pools vary significantly over the year which indicates they could act as a buffer for the discharge resulting in the modelled discharge being higher than the measured discharge since the complete catchment is located on a deep impermeable morainic strata there is seepage into the fen cescatti et al 1999 the seep reaches the surface in the peatland in two places where it forms two streams inflows due to the complex topography of the catchment and the variation in soil depth and soil composition in the catchment the flow in the unconfined aquifer is not quantifiable a possible existence of a confined aquifer cannot be identified in the field this could lead to an additional sink of water not integrated in the model holden and burt 2002 lowry et al 2009 lewis et al 2012 cunliffe et al 2013 the depth of the soils in the complete catchment is variable in the peatland the depth ranges from 0 8 m at the border to 4 4 m in the centre dalla fior 1969 these irregularities result in the variability in the amount of water that can be stored in this soil column in the model a constant soil thickness for the peatland has been assumed and due to the coarse grid size the variability in the soil was cancelled out the average annual water balance shows that the highest amounts of discharge are at the end of the snow period in these periods the snow is melting which results in high runoff the model is able to simulate the evapotranspiration clear seasonal trend with its very low values 5 mm month during the period of snow cover which is also found in the measured data 4 1 dissolved organic carbon just before snowfall the concentration of doc has a small spike in all streams which could be coming from the vegetated area higher up the mountain where snow arrived earlier in the year during the first days after snowfall the snow melts during daytime and the water drains into the soil during this period the water could have collected doc in the forested area located above the peatland only after snowmelt the doc concentration in one of the two inflows inflow 2 was higher than the outflow for all the other samples the doc in the outflow was higher than the sum of the two inflows indicating a carbon loss of the peatland via doc the monthly measurement of doc used in this study was an adequate interval of sampling to calculate the annual flux of doc since the changes in flow are a bigger contribution to annual doc export than changes in doc concentration grünheid et al 2005 worrall et al 2006 nilsson et al 2008 koehler et al 2009 the carbon lost via doc can have a big influence on the carbon balance of the ecosystem gielen et al 2011 in pristine peatlands the doc can account for 20 40 of the net ecosystem co2 exchange roulet et al 2007 nilsson et al 2008 koehler et al 2011 also in drained peatlands a big carbon loss through doc has been measured even up to 3 3 1 5 kg doc over several days after draining a 900 m2 poor fen strack et al 2008 the concentration of doc at the monte bondone peatland site is at the lower range of doc measured at peatlands freeman et al 2004 munir et al 2015 due to the aerobic conditions in the soil the dead organic matter was decomposed and mineralized to co2 the most abundant plant species is m caerulea which decomposes fast van vuuren et al 1993 and results in high ecosystem respiration pullens et al 2016 also with a deep water table the doc values were expected to be high freeman et al 2004 worrall et al 2006 dieleman et al 2016 the fact that the doc values were low could indicate that most of the carbon is emitted via ecosystem respiration and that the main source of doc were the partially decomposed recalcitrant carbon compounds in the soil such as root exudates fenner and freeman 2011 hribljan et al 2014 dieleman et al 2016 the modelled annual water balance indicates that 87 of the precipitation leaves the catchment as discharge in accord with abera et al 2017 that studies a catchment sharing the same climatic conditions based on the average of the measured doc concentrations and the monthly average modelled discharge values an initial estimate of the total doc export can be calculated it has to be taken into account that this method introduces high uncertainties in the total carbon export since it is a combination of the uncertainty of the measured water flow the uncertainty in the model outcomes and the uncertainty of the doc measurements in this study the modelled discharge is likely to be overestimated which results in an overestimation of the doc export the small measured amount of doc in combination with the modelled discharge results in a small export of doc from the peatland over the four years the average of the loss of doc is 10 2 3 8 g c m 2 yr 1 table 5 this is comparable to a blanket bog peatland in ireland 14 1 1 5 g c m 2 yr 1 for 2007 koehler et al 2009 an upland peat catchment in england 9 4 g c m 2 yr 1 worrall et al 2003 a subarctic atlantic blanket bog in norway 7 2 0 7 g c m 2 yr 1 de wit et al 2016 a fen in minnesota united states 10 0 3 0 g c m 2 yr 1 pastor et al 2003 at a restored peatland on former agricultural land in the netherlands the doc export is higher 20 6 4 3 g c m 2 yr 1 hendriks et al 2007 also a mixed acid mire system in sweden 17 8 0 7 g c m 2 yr 1 nilsson et al 2008 a rich fen in minnesota united states 21 2 6 9 g c m 2 yr 1 urban et al 1989 were found to have higher losses the combination of doc with the other two carbon fluxes co2 and ch4 pullens et al 2016 indicate that the peatland is acting as a carbon source for all four subsequent years with a carbon loss ranging from 95 3 to 273 8 g c m 2 yr 1 5 conclusion this study presents a study of a peatland on the monte bondone plateau near trento in the eastern italian alps conducted with the model geotop v2 0 and an ad hoc field campaign the peatland has a heterogeneous vegetation which results in diverse soil bulk density and hydraulic conductivity there was therefore the necessity to make some hypotheses in the setup of simulations to fill the gap of knowledge with estimates regarding some controls especially hydraulic conductivity on the water dynamics overall geotop was able to simulate the water and energy dynamics of the peatland in a complex catchment over four years and obtain satisfying outputs which were in good agreement with the available measured values the bulk density and hydraulic conductivity of the peatland were measured from in situ samples to investigate soil variability the modelled streamflow has higher discharge peaks compared to the infrequent measured discharge nevertheless the model was able to simulate the interannual variability of the volumetric water content and soil temperature accurately the comparison of the modelled energy fluxes and the measured values from the eddy covariance tower indicate that geotop is able to follow the seasonal patterns and is within the same range of the measured values the model has a smaller difference of the energy balance compared to the measurements made by the on site eddy covariance tower based on the modelled discharge and the measured doc values an initial estimation of doc export of the peatland through extrapolation was made the doc concentration in the stream water was low but the high stream flows lead to similar doc losses compared to other peatlands the study shows that a process based hydrological model can be used to study the water and energy dynamics of a peatland in a mountainous area for the peatland at monte bondone this resulted in the estimate of four years of carbon balance co2 ch4 fluxes and doc losses combined which indicated that the peatland was a carbon source for all four subsequent years the carbon balance was 111 2 275 2 189 5 and 97 3 g c m 2 yr 1 for 2012 2015 respectively 6 replicable research geotop model is freely available as indicated on the site http www geotop org the orthophoto and dem data of the site are available from autonomous province of trento p a t from http www urbanistica provincia tn it sez siat banchedati repertoriocartografico pagina3 html data from the campaign and hydrometeorological data used in this paper are available from the authors a simplified test case of this study is online available at https github com jeroenpullens geotop monte bondone sample acknowledgements the authors would like to thank the two anonymous reviewers for their useful suggestions and comments the authors would also like to thank pablo torralba rubio marilu solas obra lorenzo frizzera roberto zampedri and mauro cavagna for field assistance the authors would also like to thank paola gioacchini of dipsa bologna for the analysis of the doc samples the authors would also like to thank the geotop community for their help and support the authors also like to thank cristina bruno and bruno maiolini for lending the fp111 global flow probe one of the authors work r rigon was partially financed by the trento university project climaware appendix a supplementary data supplementary data associated with this article can be found in the online version at https doi org 10 1016 j jhydrol 2018 05 041 appendix a supplementary data supplementary video 1 supplementary video 2 supplementary data 3 
