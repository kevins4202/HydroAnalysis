index,text
0,although the fluid invasion process impacts the transport and dispersion of chemicals and contaminants in the resident phase of natural manufactured and biological unsaturated porous media these effects remain poorly understood in this study we investigate the role of hydrodynamic dispersion in different invasion patterns within the resident phase while nonidealities in pore structures contribute to mechanical dispersion our findings demonstrate that the interface front s morphology and the invading phase s distribution significantly influence mechanical dispersion within the resident phase our results show that the complex distribution of the invading phase under unstable displacement greatly affects mechanical dispersion measures such as velocity distribution variance and mean square displacement obtained through particle tracking the interface also alters hydrodynamic dispersion measures such as the local peclet number and generates extensive diffusion dominated regions across the domain this ultimately leads to solute trapping between pores that cannot reach the outlet however under stable invasion mechanical and hydrodynamic dispersion do not deviate significantly from the reference saturated condition in this scenario dispersion anomalies occur within a short distance ahead of the moving interface beyond which variations become similar to the reference media keywords transport in porous media unsaturated porous media hydrodynamic dispersion invasion process interface induced dispersion pore scale modeling data availability no data was used for the research described in the article 1 introduction hydrodynamic dispersion is a primary transport mechanisms occurring in diverse forms of porous media including natural e g aquifers and hydrocarbon reservoirs manufactured e g membranes and paper and biological e g bones and woods porous media anna et al 2021 essandoh et al 2013 hunt and sahimi 2017 puyguiraud et al 2021 sookhak lari et al 2019 wang et al 2014 the study of dispersion involving contaminants dissolved minerals pesticides fertilizers dissolved gasses and organic substances in the field of hydrogeology has gained significant attention due to the increased environmental concerns bear 2018 liu et al 2021 yan et al 2020 additionally advancements in numerical methods and experimental facilities have greatly contributed to the development of this knowledge blunt et al 2013 jahanbakhsh et al 2020 at its core hydrodynamic dispersion occurs in a porous medium due to two processes molecular diffusion which is the mixing caused by random molecular brownian motion and mechanical dispersion which results from variations in velocity causing mass to disperse as velocity changes dentz et al 2016 domenico and scbwartz 1982 fetter et al 1992 these two physical processes become more complex in a porous media due to intricate pore structures and pore space tortuosity leading to smaller values for molecular diffusion compared to non porous media moreover mechanical dispersion in porous media can be intensified by alterations in transport direction and flow rates caused by nonidealities in the porous medium fetter 1992 patrick a domenico 1998 less studies have explored dispersion under unsaturated conditions gong and piri 2020 jiménez martínez et al 2020 mohammadmoradi et al 2018 relative to saturated conditions dentz et al 2018 2016 puyguiraud et al 2021 2019 significant advances in mathematical models computational methods and experimental equipment have resulted in more accurate ways to understanding the underlying physics of this phenomena blunt et al blunt et al 2013 and wildenschild et al wildenschild et al 2002 have reviewed possible imaging and numerical techniques for simulating single and multiphase flow and transport simultaneously with studying dispersion in saturated media in recent years dentz et al 2011 kang et al 2014 souzy et al 2020 there has been a surge of interest in research on dispersion and its dependence on the saturation of the media in unsaturated porous media in an early study examining dispersion under unsaturated conditions sahimi et al 1986 explored both longitudinal and transverse dispersion across phases by analyzing the statistical distribution of two phase flow in a simple cube network of random radius pores their main finding was that the dispersivities were dependent on the phase distributions saturation and saturation history bekri 2002 used the immiscible lattice boltzmann and random walk methods to investigate the effect of peclet number and water saturation on the dispersion in two phase flow in a reconstructed porous media by utilizing peclet number and water saturation they established general correlations and identified asymptotic values for the longitudinal dispersion coefficient over extended timeframes later studies hasan et al 2019 jimenez martinez 2017 jimenez martinez et al 2015 karadimitriou et al 2017 have been conducted to demonstrate an increase of solute mixing in invading phase with a decrease of water content which was caused by an increase of flow velocity variation for various pathways however raoof and hassanizadeh 2013 revealed that the dispersity reaches a maximum value and then decreases with further decrease in saturation using pore scale modeling additionally many attempts have been made using direct numerical modeling to explore solute transport through various steady state fluid flow configurations aziz et al 2018 hasan et al 2020 jimenez martinez 2017 jiménez martínez et al 2020 almost all studies that explore two phase conditions have focused on the dispersion of solute within the invading phase furthermore these studies have not addressed dispersion under fully transient two phase flow missing the dynamic behavior of the system aziz et al 2018 gong and piri 2020 invasion of the fluid phases and associated movement of interfaces cause significant changes in the flow velocity field flow paths and tortuosity of the streamlines which controls transport of solute and colloids ahead of the interfaces the impact of interface dynamics in different invasion regimes on resident phase dispersion has not yet been investigated prior studies bekri 2002 sahimi et al 1986 do not clearly explain how interface movement alters mechanical and hydrodynamic dispersion and the associated length scales the absence of dynamic behavior in the interface front across various invasion regimes leaves the areas of the resident phase affected by the invading phase front and the depth of this impact uncertain gaining insight into the pore scale specifics of a real sand pack sample such as the effects of grain heterogeneity will aid in determining the essential parameters for continuum modeling as a result a pore scale examination of the invasion process is necessary for a comprehensive investigation of resident phase dispersion in this study we simulate the dynamics of different invasion regimes to examine the resulting changes in the flow field and solute transport within the resident phase the primary objective of this study is to assess the influence of the invading phase on the hydrodynamic dispersion of the resident phase by comparing the region ahead of the interface during the invasion process to saturated cases as a reference we explore the underlying physics of pore scale variations not previously addressed in the literature several simulations were conducted to perform a systematic study and quantitatively measure the impact of two phase regime effect the outcomes were then compared with the results obtained under saturated conditions for the same flow velocity serving as a reference case to examine the influence of various invasion patterns the advection effects and advection diffusion effects were explored separately the study is divided into three main sections section 2 presents an overview of the methods utilized to obtain the flow field and solute spreading section 3 discusses the results and their implications while section 4 provides the conclusions drawn from this study 2 methodology to investigate how the invasion process affects dispersion in the resident phase it is necessary to examine fluid flow and solute transport in both fully saturated and unsaturated media at the pore scale there are two commonly used methods for simulating pore scale flow network modeling and direct numerical modeling pore network modeling simplifies the complexity of pore structures into interconnected simple pores which can result in a loss of detailed information blunt et al 2013 shams et al 2018 to obtain a detailed understanding of the flow field this study uses direct numerical modeling in the following section we describe the governing equations computational procedure and properties of the computational domain used in this work 2 1 computational domain a two dimensional representation of the computed tomography image of a natural sample of granular sand pack from the previous published work vries et al 2022 is used to perform the direct numerical simulation as shown in fig 1 the computational domain and the generated numerical grid are kept identical throughout this study presence of a large number of grains in each principal direction provide a large enough domain for our simulations table 1 provides properties of the computational domain numerical discretization of the domain is performed using the openfoam utility snappyhexmesh which is capable of producing high quality grids within narrower pore throats and non smooth edges from complex porous media geometry to study grid independency of results we have performed simulation for different grid resolutions in a sub section of the whole domain resulting in the final number of grids in the table 1 which provides a minimum of ten numerical cells in the narrowest pore throats 2 2 governing equations and numerical methods of saturated simulation conservation of mass and momentum for isothermal and incompressible flow of newtonian fluid is governed by the navier stokes equations as 1 u 0 2 ρ u t ρ u u p 2 μ e f b where u denotes the fluid velocity p the static pressure ρ fluid density and μ fluid dynamic viscosity f b accounts for all body forces and e 1 2 u u t is the rate of strain tensor white 1991 in order to solve the advection diffusion of a passive tracer within the carrier flow eq 3 has been used 3 c t u c d m c 0 where c denotes concentration of solute and dm is molecular diffusion coefficient the numerical solution of saturated governing equations was performed using the openfoam open source toolbox jasak 1996 the steady state form of navier stokes eqs 1 and 2 was used to obtain the velocity field at the saturated medium using simplefoam solver next eq 3 is solved to obtain the transient evolution of solute transport within the steady state flow domain using the scalartransportfoam solver the advective term was interpolated using the upwind approach which is a first order and explicit method the diffusive term was discretized using a second order scheme known as gaussian linear corrected scheme and the time derivative was discretized using the euler scheme because of the sharp gradient of concentration between stagnant and flowing regions the transport matrix was solved using smoothsolver with a small relative tolerance of 10 16 aziz et al 2018 2 3 governing equations and numerical methods for invasion process fundamentally calculating the velocity field in immiscible two phase flow involves solving two separate navier stokes equations one for each primary phase a common boundary condition is employed to monitor the interface and depict the development of the two phase system however this approach necessitates solving the moving boundary problems for each phase which is challenging and extremely time intensive particularly for porous media problems involving intricate pore structures a more effective alternative for addressing the immiscible two phase flow is the whole domain formulation hirt and nichols 1981 this method treats the two phase system as a single phase system with location dependent properties and replaces the jump condition with an extra force that only operates at the interface the phase function f x characterizes the spatial distribution of both phases and is defined as follows 4 f x 1 with in the wett ing phase 0 with in the non wett ing phase the interface is represented by the surface of discontinuity of the phase function and it is indicated by the delta function concentrated on the two phase interface and space dependent properties are described by the following equations 5 ρ x f x ρ w 1 f x ρ n w 6 μ x f x μ w 1 f x μ n w ρ x and μ x are the density and the dynamic viscosity respectively considering the bulk fluid properties are assumed constant consequently the immiscible two phase flow is governed by only one set of navier stokes equations which is written as 7 ρ u t ρ u u p 2 μ e f b f s with an extra term f s which represents surface tension forces which is nonzero only at the phase interfaces and describes the effect of the laplace pressure in whole domain approach as numerical diffusion would smear the interface due to space truncation error a standard finite difference discretization cannot be used deshpande et al 2012 the numerical solution of governing equations of immiscible two phase flow was performed using the openfoam open source toolbox contrary to the conventional volume of fluid vof method hirt and nichols 1981 an efficient approach for keeping a sharp interface is applied in interfoam solver in openfoam deshpande et al 2012 henrik rusche 2002 as a nonlinear convective term to overcome the diffusion of the interface which is accomplished by computing the compression velocity with the multidimensional universal limiter with explicit solution mules deshpande et al 2012 to obtain solute transport within the resident phase an advection diffusion eq 3 equation is solved using phasescalartransport as a function object in openfoam toolbox a flowchart for the two main stages of solute transport in the invasion process is shown in fig 2 the distribution of the immiscible phases is solved first followed by the solute transport process within the resident phase in each time step 2 4 numerical conditions both the top and bottom boundaries of the domain are treated as no flow boundaries the right boundary maintains a constant pressure and for solute transport simulations a zero solute concentration gradient in the normal direction is assumed the left or inlet boundary has a set injection velocity and a consistent dimensionless solute concentration for solute transport initially the domain is completely saturated with the wetting phase and a contact angle of 45 is presumed the non wetting phase is introduced at a stable flow rate on the domain s inlet face various simulation scenarios with differing capillary numbers and viscosity ratios were executed the findings indicate that the new capillary and viscosity ratios did not have a significant impact on the analysis 2 5 validation of the numerical method to ensure reliable simulation results and analysis we evaluated the employed numerical schemes for immiscible two phase flow in invasion processes against experimental data to verify our model we utilized micromodel experiments conducted by yin et al 2018 which involved exploring two phase flow displacement in a porous medium by comparing our simulation results with the experimental observations we were able to validate our model and ensure its accuracy table 3 provides a summary of our model validation approach which involved applying similar geometries and conditions specifically we investigated the displacement of a resident phase water by a non wetting phase a flourinert solution over time in a porous domain we obtained the inlet boundary condition for our numerical simulation by analyzing the variable inlet flowrate in the yin et al experiment from the original images in the numerical simulation we assumed a relative zero pressure for the outlet boundary condition due to the experimental conditions the comparison of phase distributions during the invasion process as shown in fig 3 exhibits excellent agreement with the experimental observations it is worth noting that the validation of openfoam formulations has been the subject of various studies involving diverse porous media for simulating immiscible two phase flow deshpande et al 2012 ferrari et al 2015 ferrari and lunati 2013 3 results we aim to investigate how the invasion process affects the flow field and solute spreading within the resident phase previous studies by lenormand et al 1988 have highlighted the distinct invasion patterns that emerge based on the capillary number and viscosity ratio m of the two phases to conduct a thorough analysis under various flow regimes we conducted three simulations a saturated medium as the reference case along with stable and unstable invasion simulations table 2 presents the conditions and properties of these simulations because the simulation of different capillary numbers and viscosity ratios used in this study had no effect on the analysis the findings of a single set of capillary numbers and viscosity ratios are presented here during a stable invasion process which occurs under high capillary numbers and high viscosity ratios the front of the invading phase advances uniformly with minor irregularities typically only a few grains in size as depicted in fig 4 however a decrease in capillary number or viscosity ratio results in an unstable invasion which is characterized by the development of growing fingers within the medium these fingers can move either in the direction of the main flow or in the normal direction to it referred to as viscous fingering and capillary fingering respectively an et al 2020 tsuji et al 2016 in this study we consider both types of fingerings as unstable invasions and simulate viscous fingering as an example of an unstable invasion to ensure a fair comparison of different invasions against the reference case properties must be utilized that maintain the same velocity despite variations in capillary numbers the study findings are divided into two sections the first section focuses on the effect of invasion on mechanical dispersion while the second section examines the impact of invasion on hydrodynamic dispersion in the resident phase through this approach both aspects of the invasion s impact on dispersion are explored and discussed 3 1 mechanical dispersion the presence of nonidealities in pore structures such as variations in grain size distribution morphology and topology pore and grain arrangements can lead to velocity fluctuations and increase mechanical dispersion although the pore system parameters remain consistent across all models in this study variations in flow regime result in diverse flowlines local velocity distributions and ultimately discrepancies in mechanical dispersion the next section examines and compares the degree of mechanical dispersion in the resident phase with the reference case 3 1 1 velocity variations fig 5 a displays the velocity distribution within the resident phase during stable invasion stb over normalized time values of 0 08 0 20 0 32 and 0 44 compared to the reference case of flow under saturated conditions sat the corresponding invading phase distribution profiles during stable invasion are also shown although the fraction of velocities with negative values increases over time due to small irregularities there is no significant difference between the four stable displacements and the reference case for velocity distributions a rather uniform interface characterized by a stable invasion serves as a relatively uniform boundary condition driving the flow results in a minor disruption specifically less than 2 5 percent in the velocity field of the resident phase in fig 5b the velocity distribution under unstable invasion event unstb is shown indicating an increase in the generation of velocities with near zero and even negative values due to the generation of fingers and their impact on the velocity field within the resident phase the velocity distribution tends towards lower and even negative values resulting in transverse movement of fluid streamline in the medium and larger residence times for fluid particles in the domain contrary to the stable invasion scenario fig 5b demonstrates a noticeable shift in velocity distribution for the unstable invasion instance exhibiting a change of approximately 25 when compared to the reference case to assess the impact of the invasion process we calculated the variance of velocity values in the resident phase as illustrated in fig 6 under stable invasion the magnitude of velocity variance fluctuates around that of the reference case with less than a 1 difference between the two cases conversely during unstable invasion we observed a rapid decline in velocity variance due to the emergence of finger structures within the resident phase in an unstable invasion the variance values approach zero at the last time step as depicted in fig 6 with nearly 98 percent of the velocity values falling into bins close to zero the interquartile range which serves as a statistical indicator of local velocity distribution demonstrates a comparable pattern of reduced mechanical dispersion throughout the unstable invasion process as illustrated in fig 6 3 1 2 associated length scales of the invasion in the preceding section we investigated the effects of stable and unstable invasions on velocity variations in the resident phase over time as a measure of mechanical dispersion however that analysis did not account for the positions and spatial distribution effects within the resident phase and instead examined the deviation of the velocity distribution from the reference case across the entire domain in this section we aim to explore the length scale of interface induced velocity variations ahead of the interface during stable invasion we calculated the velocity variations within the resident phase as a function of longitudinal distance from the interface fig 7 shows the velocity variations over distance for different times we found that the majority of velocity variations occur within a short distance equivalent to a few grain sizes from the interface as a result there exists a clear characteristic length for interface induced velocity variations and over larger distances the velocity field converges to that of a saturated media indicating that the impact of interface induced variations diminishes in fig 7 the orange dashed line illustrates the position of the interface tip where the left side primarily signifies the invading phase and the right side mainly denotes the resident phase the green solid circles in fig 7 represent the point by point velocity deviation values from the reference case across the entire domain this figure reveals that significant velocity deviations are mostly focused near the interface front rather than being dispersed throughout the entire resident phase in other words there is a notable disparity between the orange and blue dashed lines as demonstrated in fig 7 the blue dashed line is consistently around 2 mm beyond the interface tip for all time steps the corresponding length scale or the gap between the orange and blue dashed lines which is roughly 2 mm is on the order of a few grain diameters based on the average grain diameter value found in table 2 due to the significant irregularity in the interface it is challenging to determine a distinct length scale for the effect of the invading phase on the resident phase in simpler terms the highly irregular shape of the interface front during unstable invasion makes it difficult to establish a well defined parameter such as the maximum velocity variation within a related length as was done in the stable invasion analysis consequently this examination is only applicable to the stable invasion process 3 1 3 lagrangian particle tracking the quantification of mechanical dispersion in porous media can be accomplished through lagrangian particle tracking to this end we have initialized 25 thousand non diffusive point particles close to the entrance of the resident phase at the onset of the invasion process by tracking the transient location of each particle we have calculated the longitudinal and transverse mean square displacement msd as a measure of mechanical dispersion while disregarding the effect of diffusion bijeljic et al 2004 bijeljic and blunt 2007 fig 8 a displays the trajectories of the particles and fig 8b presents the longitudinal and transverse msd for all simulated scenarios our findings indicate that the spreading of particles within the resident phase is similar under stable invasion and reference conditions but deviates in unstable invasion scenarios while high velocity regions are scarce in unstable invasion low velocity regions are abundant throughout the pore space leading to a reduction in the msd magnitude over time and hence a decrease in mechanical dispersion the zones with dashed lines in fig 8a highlight the areas where the particle velocity is comparatively high in stable invasion 3 2 solute transport in section 3 1 we investigated and discussed the impact of the invasion process on the advection component of hydrodynamic dispersion specifically mechanical dispersion in the subsequent section we will examine the impact of the diffusion process to accomplish this we present and analyze the results of a local peclet distribution analysis and solute transport simulation 3 2 1 local peclet number distribution the porous medium is composed of a large number of interconnected pores each of which can be analyzed separately to determine its local peclet number defined as p e l o c a l u d d m where u is the mean cross sectional velocity within the pore d is the diameter of the pore and dm is the molecular diffusion coefficient low peclet number values indicate diffusion dominated transport while high values indicate advection dominated transport transitional values result in a mixture of the two dentz et al 2018 as shown in fig 9 the invasion process can influence the value of individual peclet numbers and their distribution within the resident phase depending on the type of invasion regime the distribution of local peclet numbers changes as the interface advances across the medium producing variations in hydrodynamic dispersion despite the fact that the macroscopic peclet number p e v d d m remains constant throughout all invasion cases where v is the average velocity d is the average grain diameter and dm is the molecular diffusion coefficient the local peclet number distributions in fig 9 show similarities at early stages of invasion but deviate from each other at later stages as time progresses unstable invasions tend to result in smaller local peclet numbers due to the evolution of complex fingers and the formation of stagnant regions resulting in a diffusion dominated transport mechanism conversely stable invasions maintain a relatively constant distribution over time in order to quantitatively evaluate the differences in local peclet distributions we present skewness and kurtosis values as indicators of distribution symmetry and tail behavior for each time step in fig 9 skewness values can fluctuate by as much as 70 during unsaturated invasion while they only change by around 20 in stable invasion scenarios the deviation of kurtosis values from the mean in stable invasion is approximately 37 whereas in other cases kurtosis values can deviate from the average by as much as 150 the local peclet number distributions exhibit greater similarities during the early stages of invasion however as time progresses the distributions deviate from each other unstable invasions lead to smaller local peclet numbers due to the formation of stagnant regions and the evolution of complex fingers as a result diffusion dominates the majority of pore spaces in unstable invasions conversely stable invasions maintain a relatively constant distribution over time the 2d profiles in each plot illustrate the invasion progress under stable conditions at each time step the value of skewness and kurtosis are presented in each time step to compare the symmetry and the tail of distribution respectively the terms stb and unstb stand for stable invasion and unstable invasion respectively 3 2 2 solute transport simulation to examine how the invasion process affects the spreading of solute in the resident phase we injected a pulse of solute that was initially distributed over a vertical strip within the pore spaces we then conducted simulations to observe the transient solute spreading under different invasion regimes the simulation results for the reference case stable invasion and unstable invasion are presented in fig 10 notably solute spreading in the resident phase was significantly different under unstable invasion compared to the reference case whereas in the case of stable invasion the concentration distribution was relatively similar to the reference case the reason for this is that the invading phase did not significantly impact the resident phase under stable invasion leading to minimal changes in the flow field and comparable advection and diffusion processes as illustrated in fig 10 fig 11 compares the longitudinal msd calculated with particle tracking to the longitudinal msd calculated with solute transport simulation which show the same trend particularly at the end times the early difference in fig 11 is caused by different solute and particle configurations at the beginning of the msd calculation in addition to examining the spatial distribution of concentrations see fig 10 comparisons between simulations can also be made by analyzing the outflow from different domains such as concentration breakthrough curves btcs fig 12 shows a comparison of the btcs of concentrations measured at the outlet face of the sample under stable and unstable invasions as well as the reference case our results indicate that during unstable invasion a significant portion of the solute is confined within the resident phase resulting in only a small amount of solute reaching the outlet this confinement causes the concentration values to be significantly smaller than those obtained under stable invasion and the reference case although the distribution of solute under stable invasion is similar to the reference case there are considerable differences between the two these differences are mainly due to solute confinement within trapped pores in the resident phase as the invading phase advances a percentage of the resident phase becomes trapped between the grains resulting in the confined solute not reaching the outlet to further analyze our findings we calculated the total area of the domain covered by concentration values ranging from 0 1 to 1 the evolution of this area in different regimes reveals that solute spreading is smaller in stable invasion than in the reference case but considerably larger values are observed for unstable invasion due to solute confinement within trapped regions in the resident phase in all scenarios the total area increases due to hydrodynamic dispersion although the slope varies during unstable invasion as a large amount of solute remains stationary and confined within the pore spaces the plot of the total area reached a relatively horizontal slope over time 4 conclusion in this study we employed a validated openfoam code to investigate the influence of two phase flow invasion processes on hydrodynamic dispersion within the resident phase a systematic investigation was conducted by performing multiple simulations and quantitatively assessing the invasion effects results were compared to those obtained under saturated conditions at the same flow velocity which served as a reference case although a single pore structure was utilized for simulations variations in flow regime led to different invasion patterns resulting in diverse flow lines local velocity distributions and ultimately mechanical dispersion a relatively uniform invasion front and interface during stable invasion induced minor variations in the resident phase velocity field compared to the reference case in contrast unstable invasion led to significant velocity fluctuations within the resident phase with values often falling within lower or even negative ranges causing transverse fluid particle movement within the medium a sharp decline in velocity variations as determined by interquartile range and variance parameters was supported by results from lagrangian tracking of fluid particles examination of spatial distributions revealed the associated length scale for interface induced velocity variations with major deviations occurring a few grain sizes ahead of the interface beyond this point deviations diminished and local velocities resembled those of the saturated reference case the pore space was segmented into pore elements and the distribution of local péclet numbers within these pores served as a metric for hydrodynamic dispersion throughout the entire resident phase our findings indicated that the local péclet number distribution tended towards lower or even zero value ranges resulting in an increased prevalence of diffusion dominated pores during unstable invasion solute transport simulations and concentration breakthrough curves validated this observation under unstable invasion the concentration breakthrough curve revealed the arrival of only a small solute fraction as the majority of mass remained behind the invading front in the case of stable invasion solute distribution resembled that of the reference case but concentration values were reduced due to solute confinement within trapped pores in the resident phase these results demonstrated that unstable invasion strongly impacts dispersion mechanisms whereas the influence of stable invasion is confined to a small length scale with deviations gradually diminishing beyond this range declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper author statement all persons who meet authorship criteria are listed as authors and all authors certify that they have participated sufficiently in the work to take public responsibility for the content including participation in the concept design analysis writing or revision of the manuscript 
0,although the fluid invasion process impacts the transport and dispersion of chemicals and contaminants in the resident phase of natural manufactured and biological unsaturated porous media these effects remain poorly understood in this study we investigate the role of hydrodynamic dispersion in different invasion patterns within the resident phase while nonidealities in pore structures contribute to mechanical dispersion our findings demonstrate that the interface front s morphology and the invading phase s distribution significantly influence mechanical dispersion within the resident phase our results show that the complex distribution of the invading phase under unstable displacement greatly affects mechanical dispersion measures such as velocity distribution variance and mean square displacement obtained through particle tracking the interface also alters hydrodynamic dispersion measures such as the local peclet number and generates extensive diffusion dominated regions across the domain this ultimately leads to solute trapping between pores that cannot reach the outlet however under stable invasion mechanical and hydrodynamic dispersion do not deviate significantly from the reference saturated condition in this scenario dispersion anomalies occur within a short distance ahead of the moving interface beyond which variations become similar to the reference media keywords transport in porous media unsaturated porous media hydrodynamic dispersion invasion process interface induced dispersion pore scale modeling data availability no data was used for the research described in the article 1 introduction hydrodynamic dispersion is a primary transport mechanisms occurring in diverse forms of porous media including natural e g aquifers and hydrocarbon reservoirs manufactured e g membranes and paper and biological e g bones and woods porous media anna et al 2021 essandoh et al 2013 hunt and sahimi 2017 puyguiraud et al 2021 sookhak lari et al 2019 wang et al 2014 the study of dispersion involving contaminants dissolved minerals pesticides fertilizers dissolved gasses and organic substances in the field of hydrogeology has gained significant attention due to the increased environmental concerns bear 2018 liu et al 2021 yan et al 2020 additionally advancements in numerical methods and experimental facilities have greatly contributed to the development of this knowledge blunt et al 2013 jahanbakhsh et al 2020 at its core hydrodynamic dispersion occurs in a porous medium due to two processes molecular diffusion which is the mixing caused by random molecular brownian motion and mechanical dispersion which results from variations in velocity causing mass to disperse as velocity changes dentz et al 2016 domenico and scbwartz 1982 fetter et al 1992 these two physical processes become more complex in a porous media due to intricate pore structures and pore space tortuosity leading to smaller values for molecular diffusion compared to non porous media moreover mechanical dispersion in porous media can be intensified by alterations in transport direction and flow rates caused by nonidealities in the porous medium fetter 1992 patrick a domenico 1998 less studies have explored dispersion under unsaturated conditions gong and piri 2020 jiménez martínez et al 2020 mohammadmoradi et al 2018 relative to saturated conditions dentz et al 2018 2016 puyguiraud et al 2021 2019 significant advances in mathematical models computational methods and experimental equipment have resulted in more accurate ways to understanding the underlying physics of this phenomena blunt et al blunt et al 2013 and wildenschild et al wildenschild et al 2002 have reviewed possible imaging and numerical techniques for simulating single and multiphase flow and transport simultaneously with studying dispersion in saturated media in recent years dentz et al 2011 kang et al 2014 souzy et al 2020 there has been a surge of interest in research on dispersion and its dependence on the saturation of the media in unsaturated porous media in an early study examining dispersion under unsaturated conditions sahimi et al 1986 explored both longitudinal and transverse dispersion across phases by analyzing the statistical distribution of two phase flow in a simple cube network of random radius pores their main finding was that the dispersivities were dependent on the phase distributions saturation and saturation history bekri 2002 used the immiscible lattice boltzmann and random walk methods to investigate the effect of peclet number and water saturation on the dispersion in two phase flow in a reconstructed porous media by utilizing peclet number and water saturation they established general correlations and identified asymptotic values for the longitudinal dispersion coefficient over extended timeframes later studies hasan et al 2019 jimenez martinez 2017 jimenez martinez et al 2015 karadimitriou et al 2017 have been conducted to demonstrate an increase of solute mixing in invading phase with a decrease of water content which was caused by an increase of flow velocity variation for various pathways however raoof and hassanizadeh 2013 revealed that the dispersity reaches a maximum value and then decreases with further decrease in saturation using pore scale modeling additionally many attempts have been made using direct numerical modeling to explore solute transport through various steady state fluid flow configurations aziz et al 2018 hasan et al 2020 jimenez martinez 2017 jiménez martínez et al 2020 almost all studies that explore two phase conditions have focused on the dispersion of solute within the invading phase furthermore these studies have not addressed dispersion under fully transient two phase flow missing the dynamic behavior of the system aziz et al 2018 gong and piri 2020 invasion of the fluid phases and associated movement of interfaces cause significant changes in the flow velocity field flow paths and tortuosity of the streamlines which controls transport of solute and colloids ahead of the interfaces the impact of interface dynamics in different invasion regimes on resident phase dispersion has not yet been investigated prior studies bekri 2002 sahimi et al 1986 do not clearly explain how interface movement alters mechanical and hydrodynamic dispersion and the associated length scales the absence of dynamic behavior in the interface front across various invasion regimes leaves the areas of the resident phase affected by the invading phase front and the depth of this impact uncertain gaining insight into the pore scale specifics of a real sand pack sample such as the effects of grain heterogeneity will aid in determining the essential parameters for continuum modeling as a result a pore scale examination of the invasion process is necessary for a comprehensive investigation of resident phase dispersion in this study we simulate the dynamics of different invasion regimes to examine the resulting changes in the flow field and solute transport within the resident phase the primary objective of this study is to assess the influence of the invading phase on the hydrodynamic dispersion of the resident phase by comparing the region ahead of the interface during the invasion process to saturated cases as a reference we explore the underlying physics of pore scale variations not previously addressed in the literature several simulations were conducted to perform a systematic study and quantitatively measure the impact of two phase regime effect the outcomes were then compared with the results obtained under saturated conditions for the same flow velocity serving as a reference case to examine the influence of various invasion patterns the advection effects and advection diffusion effects were explored separately the study is divided into three main sections section 2 presents an overview of the methods utilized to obtain the flow field and solute spreading section 3 discusses the results and their implications while section 4 provides the conclusions drawn from this study 2 methodology to investigate how the invasion process affects dispersion in the resident phase it is necessary to examine fluid flow and solute transport in both fully saturated and unsaturated media at the pore scale there are two commonly used methods for simulating pore scale flow network modeling and direct numerical modeling pore network modeling simplifies the complexity of pore structures into interconnected simple pores which can result in a loss of detailed information blunt et al 2013 shams et al 2018 to obtain a detailed understanding of the flow field this study uses direct numerical modeling in the following section we describe the governing equations computational procedure and properties of the computational domain used in this work 2 1 computational domain a two dimensional representation of the computed tomography image of a natural sample of granular sand pack from the previous published work vries et al 2022 is used to perform the direct numerical simulation as shown in fig 1 the computational domain and the generated numerical grid are kept identical throughout this study presence of a large number of grains in each principal direction provide a large enough domain for our simulations table 1 provides properties of the computational domain numerical discretization of the domain is performed using the openfoam utility snappyhexmesh which is capable of producing high quality grids within narrower pore throats and non smooth edges from complex porous media geometry to study grid independency of results we have performed simulation for different grid resolutions in a sub section of the whole domain resulting in the final number of grids in the table 1 which provides a minimum of ten numerical cells in the narrowest pore throats 2 2 governing equations and numerical methods of saturated simulation conservation of mass and momentum for isothermal and incompressible flow of newtonian fluid is governed by the navier stokes equations as 1 u 0 2 ρ u t ρ u u p 2 μ e f b where u denotes the fluid velocity p the static pressure ρ fluid density and μ fluid dynamic viscosity f b accounts for all body forces and e 1 2 u u t is the rate of strain tensor white 1991 in order to solve the advection diffusion of a passive tracer within the carrier flow eq 3 has been used 3 c t u c d m c 0 where c denotes concentration of solute and dm is molecular diffusion coefficient the numerical solution of saturated governing equations was performed using the openfoam open source toolbox jasak 1996 the steady state form of navier stokes eqs 1 and 2 was used to obtain the velocity field at the saturated medium using simplefoam solver next eq 3 is solved to obtain the transient evolution of solute transport within the steady state flow domain using the scalartransportfoam solver the advective term was interpolated using the upwind approach which is a first order and explicit method the diffusive term was discretized using a second order scheme known as gaussian linear corrected scheme and the time derivative was discretized using the euler scheme because of the sharp gradient of concentration between stagnant and flowing regions the transport matrix was solved using smoothsolver with a small relative tolerance of 10 16 aziz et al 2018 2 3 governing equations and numerical methods for invasion process fundamentally calculating the velocity field in immiscible two phase flow involves solving two separate navier stokes equations one for each primary phase a common boundary condition is employed to monitor the interface and depict the development of the two phase system however this approach necessitates solving the moving boundary problems for each phase which is challenging and extremely time intensive particularly for porous media problems involving intricate pore structures a more effective alternative for addressing the immiscible two phase flow is the whole domain formulation hirt and nichols 1981 this method treats the two phase system as a single phase system with location dependent properties and replaces the jump condition with an extra force that only operates at the interface the phase function f x characterizes the spatial distribution of both phases and is defined as follows 4 f x 1 with in the wett ing phase 0 with in the non wett ing phase the interface is represented by the surface of discontinuity of the phase function and it is indicated by the delta function concentrated on the two phase interface and space dependent properties are described by the following equations 5 ρ x f x ρ w 1 f x ρ n w 6 μ x f x μ w 1 f x μ n w ρ x and μ x are the density and the dynamic viscosity respectively considering the bulk fluid properties are assumed constant consequently the immiscible two phase flow is governed by only one set of navier stokes equations which is written as 7 ρ u t ρ u u p 2 μ e f b f s with an extra term f s which represents surface tension forces which is nonzero only at the phase interfaces and describes the effect of the laplace pressure in whole domain approach as numerical diffusion would smear the interface due to space truncation error a standard finite difference discretization cannot be used deshpande et al 2012 the numerical solution of governing equations of immiscible two phase flow was performed using the openfoam open source toolbox contrary to the conventional volume of fluid vof method hirt and nichols 1981 an efficient approach for keeping a sharp interface is applied in interfoam solver in openfoam deshpande et al 2012 henrik rusche 2002 as a nonlinear convective term to overcome the diffusion of the interface which is accomplished by computing the compression velocity with the multidimensional universal limiter with explicit solution mules deshpande et al 2012 to obtain solute transport within the resident phase an advection diffusion eq 3 equation is solved using phasescalartransport as a function object in openfoam toolbox a flowchart for the two main stages of solute transport in the invasion process is shown in fig 2 the distribution of the immiscible phases is solved first followed by the solute transport process within the resident phase in each time step 2 4 numerical conditions both the top and bottom boundaries of the domain are treated as no flow boundaries the right boundary maintains a constant pressure and for solute transport simulations a zero solute concentration gradient in the normal direction is assumed the left or inlet boundary has a set injection velocity and a consistent dimensionless solute concentration for solute transport initially the domain is completely saturated with the wetting phase and a contact angle of 45 is presumed the non wetting phase is introduced at a stable flow rate on the domain s inlet face various simulation scenarios with differing capillary numbers and viscosity ratios were executed the findings indicate that the new capillary and viscosity ratios did not have a significant impact on the analysis 2 5 validation of the numerical method to ensure reliable simulation results and analysis we evaluated the employed numerical schemes for immiscible two phase flow in invasion processes against experimental data to verify our model we utilized micromodel experiments conducted by yin et al 2018 which involved exploring two phase flow displacement in a porous medium by comparing our simulation results with the experimental observations we were able to validate our model and ensure its accuracy table 3 provides a summary of our model validation approach which involved applying similar geometries and conditions specifically we investigated the displacement of a resident phase water by a non wetting phase a flourinert solution over time in a porous domain we obtained the inlet boundary condition for our numerical simulation by analyzing the variable inlet flowrate in the yin et al experiment from the original images in the numerical simulation we assumed a relative zero pressure for the outlet boundary condition due to the experimental conditions the comparison of phase distributions during the invasion process as shown in fig 3 exhibits excellent agreement with the experimental observations it is worth noting that the validation of openfoam formulations has been the subject of various studies involving diverse porous media for simulating immiscible two phase flow deshpande et al 2012 ferrari et al 2015 ferrari and lunati 2013 3 results we aim to investigate how the invasion process affects the flow field and solute spreading within the resident phase previous studies by lenormand et al 1988 have highlighted the distinct invasion patterns that emerge based on the capillary number and viscosity ratio m of the two phases to conduct a thorough analysis under various flow regimes we conducted three simulations a saturated medium as the reference case along with stable and unstable invasion simulations table 2 presents the conditions and properties of these simulations because the simulation of different capillary numbers and viscosity ratios used in this study had no effect on the analysis the findings of a single set of capillary numbers and viscosity ratios are presented here during a stable invasion process which occurs under high capillary numbers and high viscosity ratios the front of the invading phase advances uniformly with minor irregularities typically only a few grains in size as depicted in fig 4 however a decrease in capillary number or viscosity ratio results in an unstable invasion which is characterized by the development of growing fingers within the medium these fingers can move either in the direction of the main flow or in the normal direction to it referred to as viscous fingering and capillary fingering respectively an et al 2020 tsuji et al 2016 in this study we consider both types of fingerings as unstable invasions and simulate viscous fingering as an example of an unstable invasion to ensure a fair comparison of different invasions against the reference case properties must be utilized that maintain the same velocity despite variations in capillary numbers the study findings are divided into two sections the first section focuses on the effect of invasion on mechanical dispersion while the second section examines the impact of invasion on hydrodynamic dispersion in the resident phase through this approach both aspects of the invasion s impact on dispersion are explored and discussed 3 1 mechanical dispersion the presence of nonidealities in pore structures such as variations in grain size distribution morphology and topology pore and grain arrangements can lead to velocity fluctuations and increase mechanical dispersion although the pore system parameters remain consistent across all models in this study variations in flow regime result in diverse flowlines local velocity distributions and ultimately discrepancies in mechanical dispersion the next section examines and compares the degree of mechanical dispersion in the resident phase with the reference case 3 1 1 velocity variations fig 5 a displays the velocity distribution within the resident phase during stable invasion stb over normalized time values of 0 08 0 20 0 32 and 0 44 compared to the reference case of flow under saturated conditions sat the corresponding invading phase distribution profiles during stable invasion are also shown although the fraction of velocities with negative values increases over time due to small irregularities there is no significant difference between the four stable displacements and the reference case for velocity distributions a rather uniform interface characterized by a stable invasion serves as a relatively uniform boundary condition driving the flow results in a minor disruption specifically less than 2 5 percent in the velocity field of the resident phase in fig 5b the velocity distribution under unstable invasion event unstb is shown indicating an increase in the generation of velocities with near zero and even negative values due to the generation of fingers and their impact on the velocity field within the resident phase the velocity distribution tends towards lower and even negative values resulting in transverse movement of fluid streamline in the medium and larger residence times for fluid particles in the domain contrary to the stable invasion scenario fig 5b demonstrates a noticeable shift in velocity distribution for the unstable invasion instance exhibiting a change of approximately 25 when compared to the reference case to assess the impact of the invasion process we calculated the variance of velocity values in the resident phase as illustrated in fig 6 under stable invasion the magnitude of velocity variance fluctuates around that of the reference case with less than a 1 difference between the two cases conversely during unstable invasion we observed a rapid decline in velocity variance due to the emergence of finger structures within the resident phase in an unstable invasion the variance values approach zero at the last time step as depicted in fig 6 with nearly 98 percent of the velocity values falling into bins close to zero the interquartile range which serves as a statistical indicator of local velocity distribution demonstrates a comparable pattern of reduced mechanical dispersion throughout the unstable invasion process as illustrated in fig 6 3 1 2 associated length scales of the invasion in the preceding section we investigated the effects of stable and unstable invasions on velocity variations in the resident phase over time as a measure of mechanical dispersion however that analysis did not account for the positions and spatial distribution effects within the resident phase and instead examined the deviation of the velocity distribution from the reference case across the entire domain in this section we aim to explore the length scale of interface induced velocity variations ahead of the interface during stable invasion we calculated the velocity variations within the resident phase as a function of longitudinal distance from the interface fig 7 shows the velocity variations over distance for different times we found that the majority of velocity variations occur within a short distance equivalent to a few grain sizes from the interface as a result there exists a clear characteristic length for interface induced velocity variations and over larger distances the velocity field converges to that of a saturated media indicating that the impact of interface induced variations diminishes in fig 7 the orange dashed line illustrates the position of the interface tip where the left side primarily signifies the invading phase and the right side mainly denotes the resident phase the green solid circles in fig 7 represent the point by point velocity deviation values from the reference case across the entire domain this figure reveals that significant velocity deviations are mostly focused near the interface front rather than being dispersed throughout the entire resident phase in other words there is a notable disparity between the orange and blue dashed lines as demonstrated in fig 7 the blue dashed line is consistently around 2 mm beyond the interface tip for all time steps the corresponding length scale or the gap between the orange and blue dashed lines which is roughly 2 mm is on the order of a few grain diameters based on the average grain diameter value found in table 2 due to the significant irregularity in the interface it is challenging to determine a distinct length scale for the effect of the invading phase on the resident phase in simpler terms the highly irregular shape of the interface front during unstable invasion makes it difficult to establish a well defined parameter such as the maximum velocity variation within a related length as was done in the stable invasion analysis consequently this examination is only applicable to the stable invasion process 3 1 3 lagrangian particle tracking the quantification of mechanical dispersion in porous media can be accomplished through lagrangian particle tracking to this end we have initialized 25 thousand non diffusive point particles close to the entrance of the resident phase at the onset of the invasion process by tracking the transient location of each particle we have calculated the longitudinal and transverse mean square displacement msd as a measure of mechanical dispersion while disregarding the effect of diffusion bijeljic et al 2004 bijeljic and blunt 2007 fig 8 a displays the trajectories of the particles and fig 8b presents the longitudinal and transverse msd for all simulated scenarios our findings indicate that the spreading of particles within the resident phase is similar under stable invasion and reference conditions but deviates in unstable invasion scenarios while high velocity regions are scarce in unstable invasion low velocity regions are abundant throughout the pore space leading to a reduction in the msd magnitude over time and hence a decrease in mechanical dispersion the zones with dashed lines in fig 8a highlight the areas where the particle velocity is comparatively high in stable invasion 3 2 solute transport in section 3 1 we investigated and discussed the impact of the invasion process on the advection component of hydrodynamic dispersion specifically mechanical dispersion in the subsequent section we will examine the impact of the diffusion process to accomplish this we present and analyze the results of a local peclet distribution analysis and solute transport simulation 3 2 1 local peclet number distribution the porous medium is composed of a large number of interconnected pores each of which can be analyzed separately to determine its local peclet number defined as p e l o c a l u d d m where u is the mean cross sectional velocity within the pore d is the diameter of the pore and dm is the molecular diffusion coefficient low peclet number values indicate diffusion dominated transport while high values indicate advection dominated transport transitional values result in a mixture of the two dentz et al 2018 as shown in fig 9 the invasion process can influence the value of individual peclet numbers and their distribution within the resident phase depending on the type of invasion regime the distribution of local peclet numbers changes as the interface advances across the medium producing variations in hydrodynamic dispersion despite the fact that the macroscopic peclet number p e v d d m remains constant throughout all invasion cases where v is the average velocity d is the average grain diameter and dm is the molecular diffusion coefficient the local peclet number distributions in fig 9 show similarities at early stages of invasion but deviate from each other at later stages as time progresses unstable invasions tend to result in smaller local peclet numbers due to the evolution of complex fingers and the formation of stagnant regions resulting in a diffusion dominated transport mechanism conversely stable invasions maintain a relatively constant distribution over time in order to quantitatively evaluate the differences in local peclet distributions we present skewness and kurtosis values as indicators of distribution symmetry and tail behavior for each time step in fig 9 skewness values can fluctuate by as much as 70 during unsaturated invasion while they only change by around 20 in stable invasion scenarios the deviation of kurtosis values from the mean in stable invasion is approximately 37 whereas in other cases kurtosis values can deviate from the average by as much as 150 the local peclet number distributions exhibit greater similarities during the early stages of invasion however as time progresses the distributions deviate from each other unstable invasions lead to smaller local peclet numbers due to the formation of stagnant regions and the evolution of complex fingers as a result diffusion dominates the majority of pore spaces in unstable invasions conversely stable invasions maintain a relatively constant distribution over time the 2d profiles in each plot illustrate the invasion progress under stable conditions at each time step the value of skewness and kurtosis are presented in each time step to compare the symmetry and the tail of distribution respectively the terms stb and unstb stand for stable invasion and unstable invasion respectively 3 2 2 solute transport simulation to examine how the invasion process affects the spreading of solute in the resident phase we injected a pulse of solute that was initially distributed over a vertical strip within the pore spaces we then conducted simulations to observe the transient solute spreading under different invasion regimes the simulation results for the reference case stable invasion and unstable invasion are presented in fig 10 notably solute spreading in the resident phase was significantly different under unstable invasion compared to the reference case whereas in the case of stable invasion the concentration distribution was relatively similar to the reference case the reason for this is that the invading phase did not significantly impact the resident phase under stable invasion leading to minimal changes in the flow field and comparable advection and diffusion processes as illustrated in fig 10 fig 11 compares the longitudinal msd calculated with particle tracking to the longitudinal msd calculated with solute transport simulation which show the same trend particularly at the end times the early difference in fig 11 is caused by different solute and particle configurations at the beginning of the msd calculation in addition to examining the spatial distribution of concentrations see fig 10 comparisons between simulations can also be made by analyzing the outflow from different domains such as concentration breakthrough curves btcs fig 12 shows a comparison of the btcs of concentrations measured at the outlet face of the sample under stable and unstable invasions as well as the reference case our results indicate that during unstable invasion a significant portion of the solute is confined within the resident phase resulting in only a small amount of solute reaching the outlet this confinement causes the concentration values to be significantly smaller than those obtained under stable invasion and the reference case although the distribution of solute under stable invasion is similar to the reference case there are considerable differences between the two these differences are mainly due to solute confinement within trapped pores in the resident phase as the invading phase advances a percentage of the resident phase becomes trapped between the grains resulting in the confined solute not reaching the outlet to further analyze our findings we calculated the total area of the domain covered by concentration values ranging from 0 1 to 1 the evolution of this area in different regimes reveals that solute spreading is smaller in stable invasion than in the reference case but considerably larger values are observed for unstable invasion due to solute confinement within trapped regions in the resident phase in all scenarios the total area increases due to hydrodynamic dispersion although the slope varies during unstable invasion as a large amount of solute remains stationary and confined within the pore spaces the plot of the total area reached a relatively horizontal slope over time 4 conclusion in this study we employed a validated openfoam code to investigate the influence of two phase flow invasion processes on hydrodynamic dispersion within the resident phase a systematic investigation was conducted by performing multiple simulations and quantitatively assessing the invasion effects results were compared to those obtained under saturated conditions at the same flow velocity which served as a reference case although a single pore structure was utilized for simulations variations in flow regime led to different invasion patterns resulting in diverse flow lines local velocity distributions and ultimately mechanical dispersion a relatively uniform invasion front and interface during stable invasion induced minor variations in the resident phase velocity field compared to the reference case in contrast unstable invasion led to significant velocity fluctuations within the resident phase with values often falling within lower or even negative ranges causing transverse fluid particle movement within the medium a sharp decline in velocity variations as determined by interquartile range and variance parameters was supported by results from lagrangian tracking of fluid particles examination of spatial distributions revealed the associated length scale for interface induced velocity variations with major deviations occurring a few grain sizes ahead of the interface beyond this point deviations diminished and local velocities resembled those of the saturated reference case the pore space was segmented into pore elements and the distribution of local péclet numbers within these pores served as a metric for hydrodynamic dispersion throughout the entire resident phase our findings indicated that the local péclet number distribution tended towards lower or even zero value ranges resulting in an increased prevalence of diffusion dominated pores during unstable invasion solute transport simulations and concentration breakthrough curves validated this observation under unstable invasion the concentration breakthrough curve revealed the arrival of only a small solute fraction as the majority of mass remained behind the invading front in the case of stable invasion solute distribution resembled that of the reference case but concentration values were reduced due to solute confinement within trapped pores in the resident phase these results demonstrated that unstable invasion strongly impacts dispersion mechanisms whereas the influence of stable invasion is confined to a small length scale with deviations gradually diminishing beyond this range declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper author statement all persons who meet authorship criteria are listed as authors and all authors certify that they have participated sufficiently in the work to take public responsibility for the content including participation in the concept design analysis writing or revision of the manuscript 
1,in order to investigate the migration of nanoparticles in porous media the model developed by katzourakis and chrysikopoulos 2021 is applied to simulate the transport of aggregating nanoparticles under various initial conditions in the aforementioned model nanoparticles may collide with each other and form larger particle structures with different mobility and reactivity characteristics individual particles as well as aggregates can be found suspended in aqueous phase or attached reversibly and or irreversibly on the solid matrix the aggregation process modelled after the smoluchowski population balanced equation pbe is coupled with the conventional advection dispersion attachment ada equation to form a system of coupled equations that govern the transport of aggregating nanoparticles particle collisions are expected to increase exponentially with increasing initial number of injected particles n0 therefore substantially pronounced aggregation is expected when n0 is increased similarly the initial particle diameter distribution of the injected particles is expected to affect the average size of aggregates and in turn influence their mobility in a porous medium several model simulations were performed with different n0 and particle diameter distributions the results indicated the strong importance of taking into account the initial particle concentration and realistic particle diameter population distribution into consideration graphical abstract image graphical abstract keywords nanoparticles transport aggregation fitting porous media mathematical modeling data availability data will be made available on request 1 introduction in recent years the development of nanoparticles has attracted the attention of the scientific community because nanoparticles are used in a wide variety of applications in sectors such as energy health electronic pharmaceuticals cosmetics nutrition biomedicine oil and gas hubbard et al 2020 malandrakis et al 2022 mohajerani et al 2019 simonsen et al 2018 wang et al 2016 the unique structure of nanoparticles makes them special purohit et al 2019 simonsen et al 2018 nanoparticles have one dimension less than 100 nm and large surface to volume ratio nowack and bucheli 2007 due to their large surface areas nanoparticles can act as carriers for other substances while at the same time they are very reactive which is very useful for a number of catalytic applications purohit et al 2019 nanoparticles occur naturally from geological processes wind erosion and glaciations incomplete combustion volcano activity and forest fires and biochemical cycling performed by bacteria plants and fungi malakar and snow 2020 nanoparticles can also enter the environment from inappropriate disposal of industrial substances agricultural activities pesticides and fertilizers automobiles and remediation efforts water and soil decontamination buzea and pacheco 2017 mohajerani et al 2019 furthermore there is a constant flux of natural nanoparticles from the earth s surface to the atmosphere which is calculated to be 342 mt year while for artificially generated nanoparticles this flux is estimated to be 10 3 mt year hochella et al 2019 nanoparticles suspended in the atmosphere subsequently precipitate with rain deposit onto surface waters hochella et al 2019 and eventually reach the groundwater nanoparticles can be very reactive and may cause toxic or carcinogenic effects on human beings and other living organisms ganguly et al 2018 for example titanium dioxide nanoparticles and gold nanoparticles can cause genotoxicity damage the dna and create inflammation chen et al 2014 thusly understanding the mechanics of nanoparticle transport in environmental systems is of great importance as it can help to better predict and restrict water contamination a basic and extremely significant characteristic of nanoparticle transport in environmental systems is aggregation nanoparticles tend to contact each other often stick together and form larger structures aggregates with different mobility and reactive behaviour arosio et al 2012 babakhani et al 2019 solovitch et al 2010 typical mathematical models developed for nanoparticle transport in porous media often ignore the aggregation process these models use the filtration theory ft or blocking equations which fail to capture the physicochemical processes that nanoparticles undergo during their migration goldberg et al 2014 recently developed models may include an expression for aggregation coupled with the transport equation but they do not account for appropriate particle dispersion repulsive interactions between aggregates and realistic attachment or they do not provide an explicit mathematical formulation for the resulting governing transport babakhani 2019 babakhani et al 2018 li and prigiobbe 2021 2020 quik et al 2015 taghavy et al 2015 in typical bench scale column experiments nanoparticle aggregation erroneously may appear to be insignificant because the experimental conditions may promote strong repulsive forces between particles the initial particle concentration is low and the initial nanoparticle size distribution is ignored in view of these reasons particle aggregation is discouraged in this study the nanoparticle transport model developed by katzourakis and chrysikopoulos 2021 will be employed to highlight the importance of the initial nanoparticle concentration and size distribution on nanoparticle transport in porous media saturated with pure water to the knowledge of the authors of this study such investigation has not been explored previously in literature 2 mathematical modelling 2 1 general transport equations the current study employs the mathematical model developed by katzourakis and chrysikopoulos 2021 which briefly assumes that nanoparticles can be found suspended in the aqueous phase with number concentration nk npk l3 or attached onto the matrix of the porous medium with number concentration n k npk ms where ms is the mass of the solid matrix k is the class number that indicates how many single nanoparticles each aggregate has and npk is the number of aggregates of class k classifying nanoparticles into k classes is necessary because during transport aggregates are formed with different transport characteristics the governing equation that describes the migration of nanoparticles in homogeneous water saturated porous media accounting for both aggregation and non equilibrium attachment onto the solid matrix can be written as katzourakis and chrysikopoulos 2021 2014 lee et al 2000 sabelfeld and kolodko 2002 1 n k t x t ρ b θ n k t x t d x k 2 n k t x x 2 u n k t x x f n k t x a n k t x where u l t is the average interstitial velocity dx k l2 t is the longitudinal hydrodynamic dispersion coefficient of the suspended nanoparticles that belong to class k ρb ms l3 is the bulk density of the solid matrix θ is the porosity of the porous medium fn k t x npk l3t is a general source configuration form of nanoparticles which belong to class k t t is time x l is the spatial coordinate in the longitudinal direction and an k t x npk l3t is the number concentration of nanoparticles attached onto the solid matrix n k npk ms is the sum of irreversibly n k i npk ms and of the reversibly n k r npk ms attached particles the corresponding accumulation term in eq 1 can be expressed by a two site attachment model as 2 n k t n k r t n k i t the reversible attachment rate component can be expressed by a nonequilibrium equation as sim and chrysikopoulos 1998 1999 3 ρ b θ n k r t r n k n k r n k r n k r n k ρ b θ n k r where r n k n k r 1 t is the rate coefficient of reversible nanoparticle attachment onto the solid matrix and r n k r n k 1 t is the rate coefficient of reversible nanoparticle detachment from the solid matrix similarly the irreversible attachment rate component in eq 2 can be written as compère et al 2001 katzourakis and chrysikopoulos 2014 4 ρ b θ n k i t r n k n k i n k where r n k n k i 1 t is the rate coefficient of irreversible nanoparticle attachment onto the solid matrix furthermore the nanoparticle aggregation term found on the right hand side of eq 1 which might act either as source or sink depending the class k can be modelled after the smoluchowski population balance equation pbe smoluchowski 1916 5 a n k d n k dt 1 2 i 1 k 1 b i k i n i n k i n k i 1 b k i n i where the term bi k represents the collision rate between particles that belong to classes i and k assuming laminar flow one of the common collision kernels bi k for diffusion limited aggregation dla processes which accounts for collisions resulting from brownian diffusion while ignoring negligible contributions from fluid shear and sedimentation li and prigiobbe 2020 petosa et al 2010 taghavy et al 2015 is axford 1997 von smoluchowski 1917 6 b i k dla 2 k b t 3 μ w r i r k 2 r i r k where t k is temperature k b m l2 t2 t is the boltzmann constant μ w m t l is the dynamic viscosity of water and rk l is the radius of a nanoparticle that belongs to class k the nanoparticles source term found on the right hand side of eq 1 can be written as sim and chrysikopoulos 1999 7 f n k t x g k t w x where w x 1 l refers to a point source geometry 8 w x δ x x 0 where δ x x0 1 l is the dirac delta function x0 l is the cartesian x coordinate of the source center the gk t npk l2t term found in eq 7 is the particle release function which for an instantaneous source is given by 9 g k t n i n j k a c θ δ t where ninj k npk is the injected number of particles which belong to class k and ac l2 is the cross sectional area of the porous medium 2 2 filtration theory the effect of particle size change onto the forward attachment rate can be approached with the use of the well established filtration theory ft the attachment rate term found on the right hand side of eq 3 can be expressed as sim and chrysikopoulos 1995 10 r n k n k r u φ f n k where f n k is the dynamic blocking function which accounts for porosity variations when particle attachment increases however for submicron particles such as nanoparticles it can be assumed that the porous medium is clean and thusly f n k 1 the φ 1 l parameter is the filter coefficient which can be expressed as rajagopalan and tien 1976 11 φ 3 1 θ 2 d c η where dc l is the collector average diameter and η is the single collector removal efficiency yao et al 1971 12 η α η o where α is the collision efficiency and ηo is the single collector contact efficiency which can be estimated with the correlation developed by tufenkji and elimelech 2004 2 3 aggregate structure the coalesced sphere assumption dictates that when two spherical particles collide they form a new spherical aggregate with mass equal to the sum of the masses of the two initial particles while the same is true for their volumes therefore the density of the aggregate is maintained constant however in reality the newly formed aggregates contain void spaces the relationship between the initial monomer dp 1 l and the diameter of the final aggregate that belongs to class k dp k l can be written as feder 1988 lee et al 2000 13 n p k ζ d p k d p 1 d f where np k npk is the number of particles present in an aggregate that belong to class k df is the fractal dimension of an aggregate and depends on the type of aggregation ζ is the packing factor which accounts for the void pore space within the spherical aggregate and depends on the shape of both monomers and aggregates 2 4 initial and boundary equations to solve the system of differential eqs 1 12 it is essential to present the appropriate initial and the boundary conditions for a one dimensional confined aquifer 14 n k 0 x 0 15 n k t 0 n k 0 t t p 0 t t p 16 n k t 0 0 17 n k 2 t l x d x 2 0 where lx l is the length of the porous medium and tp t is the source release period over which nanoparticles enter the porous medium it is noted that eqs 14 17 refer to two possible source configurations for the first one nanoparticles are introduced into the aquifer through a broad pulse source located at the inlet of the aquifer with source concentration n k 0 npk and pulse duration tp t see eq 15 while for the second one nanoparticles are instantaneously injected at a specified location within the aquifer and only clean water enters the aquifer see eqs 9 16 the initial condition eq 14 clarifies that initially there are no nanoparticles inside the porous medium the boundary condition eq 17 establishes that at the downstream end of the finite aquifer a concentration slope continuity is preserved shamir and harleman 1967 finally eqs 14 17 are applied k times once for each class 3 methods solving the nanoparticle transport model eqs 1 17 is not an easy task because several physical processes are involved advection dispersion aggregation etc forming a family of coupled partial differential equations applying direct conventional numerical approaches would require enormous amount of memory by introducing large matrices directly correlated to the total number of classes kmax instead the physical processes were decoupled through symmetrically weighted sequential splitting operator sws methods barry et al 2000 kanney et al 2003 steefel and macquarrie 1996 wood and baptista 1993 and subsequently each process was solved individually furthermore to restrict the total relative error an adaptive time step scheme was adopted the selection of the correct number of classes k is of great importance setting kmax too low fails to properly account for the formation of larger nanoparticles however setting it too high exponentially increases the computation costs here kmax was selected so that the maximum relative error on the non negligible concentrations between the different models runs is lower than 2 additional details of the solution procedure can be found in the supplementary information si section in this study a one dimensional confined porous medium with length lx 0 6 m was considered nanoparticles can enter the aquifer either through the inlet at x0 0 m over the duration of a broad pulse tp t or instantaneously at a preselected point x0 0 1 m in the first case while t tp the nanoparticle concentration n k 0 is constant and after t tp only clean water enters the aquifer in the second case there is an instantaneous mass injection while clean water continuously enters the aquifer from the inlet in order to highlight the impact of initial concentration on the nanoparticle transport two different simulations were performed with different initial concentrations for both source configurations furthermore to emphasize the impact that the initial particle diameter distribution has on the nanoparticle transport two simulations were performed in which equal amount of mass was instantaneously injected note that in the first case the nanoparticles injected have different sizes and in the second case the nanoparticles injected are of the same size six distinct simulations were conducted four to investigate the effect of the initial source concentration and two to explore whether the source distribution affects nanoparticle transport the mathematical model was incorporated in a fortran code which was used for all the numerical simulations conducted in this study the fortran code was compiled by intel fortran compiler classic on a conventional pc system build upon an amd ryzen 5 3600 cpu and paired with fast 16 gb of ddr4 3200 mhz memory due to the extensive parallel programming that was implemented through the use of the multithreading openmp protocol the cpu utilization during simulations averaged at 90 simulation times were rather lengthy and varied depending on the extend of aggregation and the number of classes used kmax higher rates of aggregation required finer time steps resulting in increased simulation times the total run time for cases of lower aggregation rates e g lower initial concentration was trun 256 minutes and for higher rates was trun 1361 minutes as previously stated the forward attachment rate of nanoparticles can be modeled after the ft this means that the r n k n k r parameter can be calculated for any class k as a function of the particle diameter with eqs 10 12 the relationship between r n k n k r rate and dp k diameter is illustrated in fig 1 note that the r n k n k r decreases monotically till dp 1 800 nm and then monotically increases thusly it is expected that particles of size dp 1 800 nm experience reduction in the attachment rate with increasing particle diameter while particles of size dp 1 850 nm display an increasing attachment rate with increasing particle diameter in this study two particle diameters were selected for simulations dp 1 39 9 and 850 nm to highlight each one of the two cases a further increase in particle size due to aggregation causes decrease in attachment rate and b further increase in particle size increases attachment rate for the calculation of r n k n k r parameter in fig 1 the following parameters were used collision efficiency α 0 009 syngouna and chrysikopoulos 2012 collector grain diameter dc 6 10 4 m and interstitial velocity u 0 3 m hr in order to highlight the effects of initial concentration on the nanoparticle transport two simulations were performed using an instantaneous source for two different initial injection concentrations ninj 1 1 109 np1 and ninj 1 1 1010 np1 the nanoparticle model described by eqs 1 14 16 17 and the model developed by katzourakis and chrysikopoulos 2015 subsequently this biocolloid transport model will be referred to as kc model were applied to particles with dp 1 39 9 nm which are injected at x0 0 10 m of the 1 d aquifer it should be noted here that the kc model can simulate the transport of colloids in a water saturated homogeneous porous media without considering particle aggregation both models kc and current model take into account the same physical process but the kc model ignores completely particle aggregation for the nanoparticle transport model the forward reversible attachment rate for k 1 was set to r n 1 n 1 r 0 331 1 hr see fig 1 while for the kc model the forward reversible attachment rate was also set to r n n r 0 331 1 hr the rest of the required model parameters are listed in table 1 the total number of suspended nanoparticles of class k 1 divided by ninj 1 injection number of nanoparticles introduced instantaneously n 1 t n i n j 1 is shown in figs 2 a b at three different locations within the 1 d aquifer x 0 25 0 4 and 0 6 m it should be noted that n 1 t np1 l3 is the total number concentration of suspended nanoparticles or equivalently the sum of nanoparticles initially present in class k 1 which at subsequent times contribute to formation of aggregates in various classes similarly and for the same locations the respective dimensionless average size of suspended aggregates d p d p 1 are shown in figs 2c d the nanoparticle model eqs 1 6 10 15 17 which accounts for a source with duration of tp 28 hr was applied to the 1 d aquifer for two different source concentrations n 1 0 3 1014 np1 m3 and n 1 0 8 1014 np1 m3 assuming that nanoparticles with diameter dp 1 850 nm enter the aquifer at x 0 m the forward reversible attachment rate for k 1 was set to r n 1 n 1 r 0 042 1 hr all other required model parameter values are listed in table 1 the kc model was also applied for the same source configuration with attachment rate independent of aggregate size r n n r 0 042 1 hr 4 simulation results and discussion 4 1 instantaneous source the results of fig 2 indicate that the concentrations n 1 t n i n j 1 decrease with increasing time and distance from the source as expected furthermore concentrations produced by the present model were higher than those produced by the kc model see figs 2a b this discrepancy is attributed to the fact that as aggregates are formed the average particle size increases and thusly the average attachment rate decreases see fig 1 for particle size dp 1 39 9 nm the kc model employs a constant attachment rate the differences between the current model and kc model are pronounced even further with the increase of the initial injection concentration see shaded areas in fig 2a b this difference can be attributed to the nature of the pbe eq 5 which establises that the rate of collisions between particles increases exponentially with increasing class concentration to summarize increasing the initial injected concentration increases the aggregation process which in turn decreases the average particle attachment rate and creates higher n 1 t n i n j 1 peaks additionally this observation is also supported by the values of d p d p 1 ratios seen in figs 2c d the average particle size which reflects the extend of the aggregation process becomes greater as time passes and as the distance from the source increases following the shape of the n 1 t n i n j 1 curve however it becomes much greater when the initial particle injection concentration increases see figs 2c d indicating that the aggregation process is sensitive to the source configuration figs 2c d suggest that after the d p d p 1 curves reach a peak they start to descend till they obtain an almost constant value for the rest of the simulation time as nanoparticles migrate downstream they attach onto the solid matrix of the porous medium with varying rates the smaller nanoparticles attach at higher rates compared to the larger ones see fig 1 for dp 39 9 nm after the main plume of nanoparticles exits the aquifer detaching nanoparticles have on the average smaller diameter than the respective suspended ones which keep aggregating as they migrate downstream consequently the average size of nanoparticles decreases forcing the d p d p 1 curves to acquire a negative slope of course at large times the suspended concentration becomes very small the aggregation process effectively stops and the ratio d p d p 1 is controlled by the diameter of the detaching aggregates which is mostly constant and reflects the time average of all particles that migrated through the aquifer in order to investigate the effects that a possible existing initial particle diameter distribution might have on nanoparticle transport two simulations were performed for the case of an instantaneous source for two different initial source configurations in the first simulation the diameters of nanoparticles injected followed a normal distribution with average diameter d p i n j 39 9 nm and standard deviation σ d p i n j 10 nm while in the second simulation the injected nanoparticles were all of the same size dp 1 39 9 nm for both source configurations the same nanoparticle model described by eqs 1 14 16 17 was used and the same amount of nanoparticle mass was injected into the aquifer the exact particle number ninj k for each class k injected in the aquifer for the first simulation following the normal distribution can be seen in fig 3 while for the second simulation ninj 1 3 13 108 np1 the mass based concentration of nanoparticles cm mn l3 with mn referring to the total mass of all suspended particles divided by the minj mass of nanoparticles introduced instantaneously is shown in fig 4 a at three different locations within the 1 d aquifer x 0 25 0 4 and 0 6 m the respective dimensionless average size of suspended aggregates d p d p 1 are shown at the same locations in fig 4b the cm minj ratios for the first source loading the one with the diameter distribution exhibit higher peaks than the ones with fixed initial diameter the reason for this is that despite both source configurations having the same amount of mass injected and the same average diameter dp 1 39 9 nm the total number of all distinct particles of all classes for the first source is n inj k t 1 6 109 npk which is larger than the total number of all distinct particles for the second source which is ninj 1 3 13 108 np1 the increased injected number of nanoparticles causes increased aggregation rate which in turn decreases the average particle attachment rate see fig 1 and creates higher cm minj peaks furthermore the difference in particle numbers between the two sources is caused by the presence of voids in the aggregate structures the first source consists of a size distribution with particles of various sizes monomers dimmers trimmers etc which may contain voids in their structure however the second source consists only of particles of a single size monomers which are dense the existence of voids means that for the same volume less nanoparticle mass is required and results to an increased distinct particle number the d p d p 1 ratios as shown in in fig 4b exhibit larger values in the presence of an initial nanoparticle size distribution than when injected particles are all of the same size this observation clearly supports the already established argument that the aggregation process is more pronounced when injected particles possess a size distribution finally it is noted that both source configurations considered here refer to monodisperse particles therefore the injected nanoparticles even if they have already formed aggregates at the time of injection can be decomposed to particles of the same size called monomers 4 2 broadpulse source the total number of suspended nanoparticles of class k 1 n 1 t n 1 0 is shown in figs 5 a b c at three different locations within the 1 d aquifer x 0 2 0 35 and 0 6 m and in figs 6 a b c at three different times t 3 28 and 32 hr the results from fig 5 indicate that the simulated breakthrough curves reach peak concentrations slower and exhibit more pronounced tailing than the kc model see figs 5a b c similarly it is evident from fig 6 that suspended nanoparticle concentrations simulated by the kc model expand faster downstream and exhibit higher concentration levels than the respective nanoparticle models see figs 6a b while after the end of the broad pulse tp 28 hr the breakthrough curves simulated by the kc model exit the aquifer faster see fig c these discrepancies between the two models are caused by the increased attachment rate the current nanoparticle model accounts for due to aggregation while the kc model ignores aggregation as seen in fig 1 for dp 850 nm further increase in particle size increases the attachment rate furthermore the differences between the current model and kc model are even more pronounced when the source concentration n 1 0 is increased the dimensionless average size of the suspended aggregates ratios d p d p 1 are presend in figs 5d e f at three different locations within the 1 d aquifer x 0 2 0 35 and 0 6 m and in figs 6d e f at three different times t 3 28 and 32 hr as expected the d p d p 1 ratio in fig 5 follows the respective concentration curves n 1 t see fig 5a b c and increases markedly up to a 8 fold however at the end of tp 28 hr the d p d p 1 ratios instead of decreasing as the respective concentration curves do they increase this behavior is explained by recognizing that during the simulation larger particles attach with higher rates onto the aquifer see fig 1 for dp 850 nm and that at the end of broad pulse injection at tp 28 hr they detach and produce steep positive d p d p 1 slopes this process is more pronounced exactly after t tp causing the formation of a step in d p d p 1 slopes at which point maximum detachment rate is achieved as indicated by the kinetic eq 3 max detachment rate is attained when the suspended concentration is zero and the attached concentration has reached its max value both conditions are satisfied at the end of broad pulse injection tp 28 hr and subsequently only clean water enters the aquifer furthermore the increase in d p d p 1 with distance along the 1 d aquifer seen in figs 5d e f as well as in figs 6d e f is attributed to the fact that as the nanoparticles move downstream they aggregate and consequently increase in size finally the increase in source concentration n 1 0 increases the collision rate see eq 5 promotes aggregation and increases all d p d p 1 ratios as seen in figs 5d e f and 6d e f in order to investigate the effect that kmax has on nanoparticle transport several simulations were performed for various k values which are presented in fig 7 the total number of suspended nanoparticles of class k 1 n 1 t n 1 0 is shown in figs 7a b as a function of time for two different initial number concentrations n 1 0 8 1014 and 3 1014 np1 m3 at x 0 6 m the absolute relative average error rae for the respective initial number concentrations is shown in fig 7c the results suggest that as the kmax parameter increases the breakthrough curves become smaller and exhibit increased tailing which is a consequence of increased attachment therefore as the k increases the extend of aggregation that can be accounted for also increases see fig 1 for dp 850 nm the rae can be calculated as follows 18 rae i 1 m x i k max x i 200 x i 200 m where m is the number of observations that participate in the calculation of the error and x i k max np1 l3 is the concentration of observation i calculated with the use of kmax classes it should be noted here that a value of kmax 200 classes was used as point of reference for calculating the rae it is clearly shown in fig 7c that increasing kmax causes the rae to decrease fast at early times but slowly at late times note that the y axis is on a log scale the slopes of the two rae lines suggest that if the initial concentration n 1 0 is increased the same error can be achieved by increasing the number of classes kmax 4 3 general observations from simulation results the discrepancies between models accounting for aggregation and those that do not varied spatially and temporally and were more pronounced as the initial injected concentration was increased furthermore all the graphs of the ratio d p d p 1 see figs 2c d 4b 5d e f and 6d e f which reflect the extend of aggregation process indicated that aggregation progressed became greater with increasing time and distance from the source additionally d p d p 1 ratios in conjunction with the filtration theory can be used to characterize the average attachment rate of nanoparticles onto the solid matrix of the porous medium it is evident that the attachment rate is not constant but varies spatially and temporally depending on the initial particle size the attachment rate increases or decreases in a practically linear fashion with increasing distance from the source see figs 6d e f certainly the impact of aggregation on nanoparticle transport cannot be discarded as negligible consequently proper modeling of aggregation is recommended 4 4 comparison with similar transport models babakhani 2019 developed a nanoparticle transport model that considers particle aggregation and evaluates size exclusion and concluded that accounting for particle aggregation the predictive ability of the model is improved similarly li and prigiobbe 2021 2020 developed a mechanistic model to describe the transport of nanaoparticles in the presence of foam the model takes into account nanoparticle aggeregation and particle attachment detachment onto the solid matrix it was shown that foam can be used to enchance nanoparticle delivery even in a low permeability medium within a shallow subsurface however both models mentioned above do not clearly describe how the transport and aggregation equations are coupled a governing mass transport equation that incorporates aggregation was not explicitly provided futhermore the first model overlooked the importance of particle attachment onto the solid matrix and the latter one considered particle hydrodynamic dispersion as negligible compared to particle advection particle dispersion might be negligible in cases where foam 90 vol gas is employed but certainly it can not be ignored in cases of low velocity laminar flow commonly occurring in water satured aquifers in this study the two site attachment model was adopted compère et al 2001 katzourakis and chrysikopoulos 2014 which has been proven to work very well with colloid biocolloid nanoparticle transport and cotransport experiments chrysikopoulos and katzourakis 2015 katzourakis and chrysikopoulos 2014 stefanarou et al 2023 the two site approach allows particles to be attached reveresibly or irreversibly onto the solid matrix of the porous medium if particles are attached reversibly then there is a chance depending on the suspended concentration to detach and transfer again into the aqueous phase however if particles are attached irreversibly then they remain attached till the end of the experiment contributing to the total column retention particles of the first type usually attach on the secondary minimum reversible attachment and of the second type on the primary minimum irreversible attachment tufenkji and elimelech 2005 finally in some cases katzourakis and chrysikopoulos 2014 the presence of irreversible particle attachment was absolutely necessary for the model to reduce extended tailing 5 summary and conclusions the conceptual model of katzourakis and chrysikopoulos 2021 was applied to simulate the transport of aggregating nanoparticles under various initial conditions in order to investigate the effects of initial particle concentration and particle diameter distribution on nanoparticle transport the results from several simulations suggested that nanoparticle aggregation can affect significantly the nanoparticle migration in porous media either hindering it or enhancing it depending on the initial particle diameter if further increase of the initial particle size causes decreasing attachment rates then aggregation enhances nanoparticle transport whereas if further increase leads to increasing attachment rates then aggregation hinders nanoparticle transport furthermore the increase of the source concentration significantly affects the aggregation rate additionally the existence of an initial particle diameter distribution can increase the aggregation rate and alter the transport characteristics of nanoparticles both observations are of great importance because they highlight the effects that the source configuration might have on particle transport and are often overlooked it should be noted that by changing the source configuration the shape of the resulting breakthrough curves might change considerably therefore ignoring nanoparticle aggregation or neglecting to take into account the actual source configuration might lead to erroneous results credit authorship contribution statement vasileios e katzourakis writing original draft methodology software constantinos v chrysikopoulos conceptualization supervision writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this research has received funding from khalifa university grant award number fsu 2023 12 supplementary materials supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2023 104475 appendix supplementary materials image application 1 
1,in order to investigate the migration of nanoparticles in porous media the model developed by katzourakis and chrysikopoulos 2021 is applied to simulate the transport of aggregating nanoparticles under various initial conditions in the aforementioned model nanoparticles may collide with each other and form larger particle structures with different mobility and reactivity characteristics individual particles as well as aggregates can be found suspended in aqueous phase or attached reversibly and or irreversibly on the solid matrix the aggregation process modelled after the smoluchowski population balanced equation pbe is coupled with the conventional advection dispersion attachment ada equation to form a system of coupled equations that govern the transport of aggregating nanoparticles particle collisions are expected to increase exponentially with increasing initial number of injected particles n0 therefore substantially pronounced aggregation is expected when n0 is increased similarly the initial particle diameter distribution of the injected particles is expected to affect the average size of aggregates and in turn influence their mobility in a porous medium several model simulations were performed with different n0 and particle diameter distributions the results indicated the strong importance of taking into account the initial particle concentration and realistic particle diameter population distribution into consideration graphical abstract image graphical abstract keywords nanoparticles transport aggregation fitting porous media mathematical modeling data availability data will be made available on request 1 introduction in recent years the development of nanoparticles has attracted the attention of the scientific community because nanoparticles are used in a wide variety of applications in sectors such as energy health electronic pharmaceuticals cosmetics nutrition biomedicine oil and gas hubbard et al 2020 malandrakis et al 2022 mohajerani et al 2019 simonsen et al 2018 wang et al 2016 the unique structure of nanoparticles makes them special purohit et al 2019 simonsen et al 2018 nanoparticles have one dimension less than 100 nm and large surface to volume ratio nowack and bucheli 2007 due to their large surface areas nanoparticles can act as carriers for other substances while at the same time they are very reactive which is very useful for a number of catalytic applications purohit et al 2019 nanoparticles occur naturally from geological processes wind erosion and glaciations incomplete combustion volcano activity and forest fires and biochemical cycling performed by bacteria plants and fungi malakar and snow 2020 nanoparticles can also enter the environment from inappropriate disposal of industrial substances agricultural activities pesticides and fertilizers automobiles and remediation efforts water and soil decontamination buzea and pacheco 2017 mohajerani et al 2019 furthermore there is a constant flux of natural nanoparticles from the earth s surface to the atmosphere which is calculated to be 342 mt year while for artificially generated nanoparticles this flux is estimated to be 10 3 mt year hochella et al 2019 nanoparticles suspended in the atmosphere subsequently precipitate with rain deposit onto surface waters hochella et al 2019 and eventually reach the groundwater nanoparticles can be very reactive and may cause toxic or carcinogenic effects on human beings and other living organisms ganguly et al 2018 for example titanium dioxide nanoparticles and gold nanoparticles can cause genotoxicity damage the dna and create inflammation chen et al 2014 thusly understanding the mechanics of nanoparticle transport in environmental systems is of great importance as it can help to better predict and restrict water contamination a basic and extremely significant characteristic of nanoparticle transport in environmental systems is aggregation nanoparticles tend to contact each other often stick together and form larger structures aggregates with different mobility and reactive behaviour arosio et al 2012 babakhani et al 2019 solovitch et al 2010 typical mathematical models developed for nanoparticle transport in porous media often ignore the aggregation process these models use the filtration theory ft or blocking equations which fail to capture the physicochemical processes that nanoparticles undergo during their migration goldberg et al 2014 recently developed models may include an expression for aggregation coupled with the transport equation but they do not account for appropriate particle dispersion repulsive interactions between aggregates and realistic attachment or they do not provide an explicit mathematical formulation for the resulting governing transport babakhani 2019 babakhani et al 2018 li and prigiobbe 2021 2020 quik et al 2015 taghavy et al 2015 in typical bench scale column experiments nanoparticle aggregation erroneously may appear to be insignificant because the experimental conditions may promote strong repulsive forces between particles the initial particle concentration is low and the initial nanoparticle size distribution is ignored in view of these reasons particle aggregation is discouraged in this study the nanoparticle transport model developed by katzourakis and chrysikopoulos 2021 will be employed to highlight the importance of the initial nanoparticle concentration and size distribution on nanoparticle transport in porous media saturated with pure water to the knowledge of the authors of this study such investigation has not been explored previously in literature 2 mathematical modelling 2 1 general transport equations the current study employs the mathematical model developed by katzourakis and chrysikopoulos 2021 which briefly assumes that nanoparticles can be found suspended in the aqueous phase with number concentration nk npk l3 or attached onto the matrix of the porous medium with number concentration n k npk ms where ms is the mass of the solid matrix k is the class number that indicates how many single nanoparticles each aggregate has and npk is the number of aggregates of class k classifying nanoparticles into k classes is necessary because during transport aggregates are formed with different transport characteristics the governing equation that describes the migration of nanoparticles in homogeneous water saturated porous media accounting for both aggregation and non equilibrium attachment onto the solid matrix can be written as katzourakis and chrysikopoulos 2021 2014 lee et al 2000 sabelfeld and kolodko 2002 1 n k t x t ρ b θ n k t x t d x k 2 n k t x x 2 u n k t x x f n k t x a n k t x where u l t is the average interstitial velocity dx k l2 t is the longitudinal hydrodynamic dispersion coefficient of the suspended nanoparticles that belong to class k ρb ms l3 is the bulk density of the solid matrix θ is the porosity of the porous medium fn k t x npk l3t is a general source configuration form of nanoparticles which belong to class k t t is time x l is the spatial coordinate in the longitudinal direction and an k t x npk l3t is the number concentration of nanoparticles attached onto the solid matrix n k npk ms is the sum of irreversibly n k i npk ms and of the reversibly n k r npk ms attached particles the corresponding accumulation term in eq 1 can be expressed by a two site attachment model as 2 n k t n k r t n k i t the reversible attachment rate component can be expressed by a nonequilibrium equation as sim and chrysikopoulos 1998 1999 3 ρ b θ n k r t r n k n k r n k r n k r n k ρ b θ n k r where r n k n k r 1 t is the rate coefficient of reversible nanoparticle attachment onto the solid matrix and r n k r n k 1 t is the rate coefficient of reversible nanoparticle detachment from the solid matrix similarly the irreversible attachment rate component in eq 2 can be written as compère et al 2001 katzourakis and chrysikopoulos 2014 4 ρ b θ n k i t r n k n k i n k where r n k n k i 1 t is the rate coefficient of irreversible nanoparticle attachment onto the solid matrix furthermore the nanoparticle aggregation term found on the right hand side of eq 1 which might act either as source or sink depending the class k can be modelled after the smoluchowski population balance equation pbe smoluchowski 1916 5 a n k d n k dt 1 2 i 1 k 1 b i k i n i n k i n k i 1 b k i n i where the term bi k represents the collision rate between particles that belong to classes i and k assuming laminar flow one of the common collision kernels bi k for diffusion limited aggregation dla processes which accounts for collisions resulting from brownian diffusion while ignoring negligible contributions from fluid shear and sedimentation li and prigiobbe 2020 petosa et al 2010 taghavy et al 2015 is axford 1997 von smoluchowski 1917 6 b i k dla 2 k b t 3 μ w r i r k 2 r i r k where t k is temperature k b m l2 t2 t is the boltzmann constant μ w m t l is the dynamic viscosity of water and rk l is the radius of a nanoparticle that belongs to class k the nanoparticles source term found on the right hand side of eq 1 can be written as sim and chrysikopoulos 1999 7 f n k t x g k t w x where w x 1 l refers to a point source geometry 8 w x δ x x 0 where δ x x0 1 l is the dirac delta function x0 l is the cartesian x coordinate of the source center the gk t npk l2t term found in eq 7 is the particle release function which for an instantaneous source is given by 9 g k t n i n j k a c θ δ t where ninj k npk is the injected number of particles which belong to class k and ac l2 is the cross sectional area of the porous medium 2 2 filtration theory the effect of particle size change onto the forward attachment rate can be approached with the use of the well established filtration theory ft the attachment rate term found on the right hand side of eq 3 can be expressed as sim and chrysikopoulos 1995 10 r n k n k r u φ f n k where f n k is the dynamic blocking function which accounts for porosity variations when particle attachment increases however for submicron particles such as nanoparticles it can be assumed that the porous medium is clean and thusly f n k 1 the φ 1 l parameter is the filter coefficient which can be expressed as rajagopalan and tien 1976 11 φ 3 1 θ 2 d c η where dc l is the collector average diameter and η is the single collector removal efficiency yao et al 1971 12 η α η o where α is the collision efficiency and ηo is the single collector contact efficiency which can be estimated with the correlation developed by tufenkji and elimelech 2004 2 3 aggregate structure the coalesced sphere assumption dictates that when two spherical particles collide they form a new spherical aggregate with mass equal to the sum of the masses of the two initial particles while the same is true for their volumes therefore the density of the aggregate is maintained constant however in reality the newly formed aggregates contain void spaces the relationship between the initial monomer dp 1 l and the diameter of the final aggregate that belongs to class k dp k l can be written as feder 1988 lee et al 2000 13 n p k ζ d p k d p 1 d f where np k npk is the number of particles present in an aggregate that belong to class k df is the fractal dimension of an aggregate and depends on the type of aggregation ζ is the packing factor which accounts for the void pore space within the spherical aggregate and depends on the shape of both monomers and aggregates 2 4 initial and boundary equations to solve the system of differential eqs 1 12 it is essential to present the appropriate initial and the boundary conditions for a one dimensional confined aquifer 14 n k 0 x 0 15 n k t 0 n k 0 t t p 0 t t p 16 n k t 0 0 17 n k 2 t l x d x 2 0 where lx l is the length of the porous medium and tp t is the source release period over which nanoparticles enter the porous medium it is noted that eqs 14 17 refer to two possible source configurations for the first one nanoparticles are introduced into the aquifer through a broad pulse source located at the inlet of the aquifer with source concentration n k 0 npk and pulse duration tp t see eq 15 while for the second one nanoparticles are instantaneously injected at a specified location within the aquifer and only clean water enters the aquifer see eqs 9 16 the initial condition eq 14 clarifies that initially there are no nanoparticles inside the porous medium the boundary condition eq 17 establishes that at the downstream end of the finite aquifer a concentration slope continuity is preserved shamir and harleman 1967 finally eqs 14 17 are applied k times once for each class 3 methods solving the nanoparticle transport model eqs 1 17 is not an easy task because several physical processes are involved advection dispersion aggregation etc forming a family of coupled partial differential equations applying direct conventional numerical approaches would require enormous amount of memory by introducing large matrices directly correlated to the total number of classes kmax instead the physical processes were decoupled through symmetrically weighted sequential splitting operator sws methods barry et al 2000 kanney et al 2003 steefel and macquarrie 1996 wood and baptista 1993 and subsequently each process was solved individually furthermore to restrict the total relative error an adaptive time step scheme was adopted the selection of the correct number of classes k is of great importance setting kmax too low fails to properly account for the formation of larger nanoparticles however setting it too high exponentially increases the computation costs here kmax was selected so that the maximum relative error on the non negligible concentrations between the different models runs is lower than 2 additional details of the solution procedure can be found in the supplementary information si section in this study a one dimensional confined porous medium with length lx 0 6 m was considered nanoparticles can enter the aquifer either through the inlet at x0 0 m over the duration of a broad pulse tp t or instantaneously at a preselected point x0 0 1 m in the first case while t tp the nanoparticle concentration n k 0 is constant and after t tp only clean water enters the aquifer in the second case there is an instantaneous mass injection while clean water continuously enters the aquifer from the inlet in order to highlight the impact of initial concentration on the nanoparticle transport two different simulations were performed with different initial concentrations for both source configurations furthermore to emphasize the impact that the initial particle diameter distribution has on the nanoparticle transport two simulations were performed in which equal amount of mass was instantaneously injected note that in the first case the nanoparticles injected have different sizes and in the second case the nanoparticles injected are of the same size six distinct simulations were conducted four to investigate the effect of the initial source concentration and two to explore whether the source distribution affects nanoparticle transport the mathematical model was incorporated in a fortran code which was used for all the numerical simulations conducted in this study the fortran code was compiled by intel fortran compiler classic on a conventional pc system build upon an amd ryzen 5 3600 cpu and paired with fast 16 gb of ddr4 3200 mhz memory due to the extensive parallel programming that was implemented through the use of the multithreading openmp protocol the cpu utilization during simulations averaged at 90 simulation times were rather lengthy and varied depending on the extend of aggregation and the number of classes used kmax higher rates of aggregation required finer time steps resulting in increased simulation times the total run time for cases of lower aggregation rates e g lower initial concentration was trun 256 minutes and for higher rates was trun 1361 minutes as previously stated the forward attachment rate of nanoparticles can be modeled after the ft this means that the r n k n k r parameter can be calculated for any class k as a function of the particle diameter with eqs 10 12 the relationship between r n k n k r rate and dp k diameter is illustrated in fig 1 note that the r n k n k r decreases monotically till dp 1 800 nm and then monotically increases thusly it is expected that particles of size dp 1 800 nm experience reduction in the attachment rate with increasing particle diameter while particles of size dp 1 850 nm display an increasing attachment rate with increasing particle diameter in this study two particle diameters were selected for simulations dp 1 39 9 and 850 nm to highlight each one of the two cases a further increase in particle size due to aggregation causes decrease in attachment rate and b further increase in particle size increases attachment rate for the calculation of r n k n k r parameter in fig 1 the following parameters were used collision efficiency α 0 009 syngouna and chrysikopoulos 2012 collector grain diameter dc 6 10 4 m and interstitial velocity u 0 3 m hr in order to highlight the effects of initial concentration on the nanoparticle transport two simulations were performed using an instantaneous source for two different initial injection concentrations ninj 1 1 109 np1 and ninj 1 1 1010 np1 the nanoparticle model described by eqs 1 14 16 17 and the model developed by katzourakis and chrysikopoulos 2015 subsequently this biocolloid transport model will be referred to as kc model were applied to particles with dp 1 39 9 nm which are injected at x0 0 10 m of the 1 d aquifer it should be noted here that the kc model can simulate the transport of colloids in a water saturated homogeneous porous media without considering particle aggregation both models kc and current model take into account the same physical process but the kc model ignores completely particle aggregation for the nanoparticle transport model the forward reversible attachment rate for k 1 was set to r n 1 n 1 r 0 331 1 hr see fig 1 while for the kc model the forward reversible attachment rate was also set to r n n r 0 331 1 hr the rest of the required model parameters are listed in table 1 the total number of suspended nanoparticles of class k 1 divided by ninj 1 injection number of nanoparticles introduced instantaneously n 1 t n i n j 1 is shown in figs 2 a b at three different locations within the 1 d aquifer x 0 25 0 4 and 0 6 m it should be noted that n 1 t np1 l3 is the total number concentration of suspended nanoparticles or equivalently the sum of nanoparticles initially present in class k 1 which at subsequent times contribute to formation of aggregates in various classes similarly and for the same locations the respective dimensionless average size of suspended aggregates d p d p 1 are shown in figs 2c d the nanoparticle model eqs 1 6 10 15 17 which accounts for a source with duration of tp 28 hr was applied to the 1 d aquifer for two different source concentrations n 1 0 3 1014 np1 m3 and n 1 0 8 1014 np1 m3 assuming that nanoparticles with diameter dp 1 850 nm enter the aquifer at x 0 m the forward reversible attachment rate for k 1 was set to r n 1 n 1 r 0 042 1 hr all other required model parameter values are listed in table 1 the kc model was also applied for the same source configuration with attachment rate independent of aggregate size r n n r 0 042 1 hr 4 simulation results and discussion 4 1 instantaneous source the results of fig 2 indicate that the concentrations n 1 t n i n j 1 decrease with increasing time and distance from the source as expected furthermore concentrations produced by the present model were higher than those produced by the kc model see figs 2a b this discrepancy is attributed to the fact that as aggregates are formed the average particle size increases and thusly the average attachment rate decreases see fig 1 for particle size dp 1 39 9 nm the kc model employs a constant attachment rate the differences between the current model and kc model are pronounced even further with the increase of the initial injection concentration see shaded areas in fig 2a b this difference can be attributed to the nature of the pbe eq 5 which establises that the rate of collisions between particles increases exponentially with increasing class concentration to summarize increasing the initial injected concentration increases the aggregation process which in turn decreases the average particle attachment rate and creates higher n 1 t n i n j 1 peaks additionally this observation is also supported by the values of d p d p 1 ratios seen in figs 2c d the average particle size which reflects the extend of the aggregation process becomes greater as time passes and as the distance from the source increases following the shape of the n 1 t n i n j 1 curve however it becomes much greater when the initial particle injection concentration increases see figs 2c d indicating that the aggregation process is sensitive to the source configuration figs 2c d suggest that after the d p d p 1 curves reach a peak they start to descend till they obtain an almost constant value for the rest of the simulation time as nanoparticles migrate downstream they attach onto the solid matrix of the porous medium with varying rates the smaller nanoparticles attach at higher rates compared to the larger ones see fig 1 for dp 39 9 nm after the main plume of nanoparticles exits the aquifer detaching nanoparticles have on the average smaller diameter than the respective suspended ones which keep aggregating as they migrate downstream consequently the average size of nanoparticles decreases forcing the d p d p 1 curves to acquire a negative slope of course at large times the suspended concentration becomes very small the aggregation process effectively stops and the ratio d p d p 1 is controlled by the diameter of the detaching aggregates which is mostly constant and reflects the time average of all particles that migrated through the aquifer in order to investigate the effects that a possible existing initial particle diameter distribution might have on nanoparticle transport two simulations were performed for the case of an instantaneous source for two different initial source configurations in the first simulation the diameters of nanoparticles injected followed a normal distribution with average diameter d p i n j 39 9 nm and standard deviation σ d p i n j 10 nm while in the second simulation the injected nanoparticles were all of the same size dp 1 39 9 nm for both source configurations the same nanoparticle model described by eqs 1 14 16 17 was used and the same amount of nanoparticle mass was injected into the aquifer the exact particle number ninj k for each class k injected in the aquifer for the first simulation following the normal distribution can be seen in fig 3 while for the second simulation ninj 1 3 13 108 np1 the mass based concentration of nanoparticles cm mn l3 with mn referring to the total mass of all suspended particles divided by the minj mass of nanoparticles introduced instantaneously is shown in fig 4 a at three different locations within the 1 d aquifer x 0 25 0 4 and 0 6 m the respective dimensionless average size of suspended aggregates d p d p 1 are shown at the same locations in fig 4b the cm minj ratios for the first source loading the one with the diameter distribution exhibit higher peaks than the ones with fixed initial diameter the reason for this is that despite both source configurations having the same amount of mass injected and the same average diameter dp 1 39 9 nm the total number of all distinct particles of all classes for the first source is n inj k t 1 6 109 npk which is larger than the total number of all distinct particles for the second source which is ninj 1 3 13 108 np1 the increased injected number of nanoparticles causes increased aggregation rate which in turn decreases the average particle attachment rate see fig 1 and creates higher cm minj peaks furthermore the difference in particle numbers between the two sources is caused by the presence of voids in the aggregate structures the first source consists of a size distribution with particles of various sizes monomers dimmers trimmers etc which may contain voids in their structure however the second source consists only of particles of a single size monomers which are dense the existence of voids means that for the same volume less nanoparticle mass is required and results to an increased distinct particle number the d p d p 1 ratios as shown in in fig 4b exhibit larger values in the presence of an initial nanoparticle size distribution than when injected particles are all of the same size this observation clearly supports the already established argument that the aggregation process is more pronounced when injected particles possess a size distribution finally it is noted that both source configurations considered here refer to monodisperse particles therefore the injected nanoparticles even if they have already formed aggregates at the time of injection can be decomposed to particles of the same size called monomers 4 2 broadpulse source the total number of suspended nanoparticles of class k 1 n 1 t n 1 0 is shown in figs 5 a b c at three different locations within the 1 d aquifer x 0 2 0 35 and 0 6 m and in figs 6 a b c at three different times t 3 28 and 32 hr the results from fig 5 indicate that the simulated breakthrough curves reach peak concentrations slower and exhibit more pronounced tailing than the kc model see figs 5a b c similarly it is evident from fig 6 that suspended nanoparticle concentrations simulated by the kc model expand faster downstream and exhibit higher concentration levels than the respective nanoparticle models see figs 6a b while after the end of the broad pulse tp 28 hr the breakthrough curves simulated by the kc model exit the aquifer faster see fig c these discrepancies between the two models are caused by the increased attachment rate the current nanoparticle model accounts for due to aggregation while the kc model ignores aggregation as seen in fig 1 for dp 850 nm further increase in particle size increases the attachment rate furthermore the differences between the current model and kc model are even more pronounced when the source concentration n 1 0 is increased the dimensionless average size of the suspended aggregates ratios d p d p 1 are presend in figs 5d e f at three different locations within the 1 d aquifer x 0 2 0 35 and 0 6 m and in figs 6d e f at three different times t 3 28 and 32 hr as expected the d p d p 1 ratio in fig 5 follows the respective concentration curves n 1 t see fig 5a b c and increases markedly up to a 8 fold however at the end of tp 28 hr the d p d p 1 ratios instead of decreasing as the respective concentration curves do they increase this behavior is explained by recognizing that during the simulation larger particles attach with higher rates onto the aquifer see fig 1 for dp 850 nm and that at the end of broad pulse injection at tp 28 hr they detach and produce steep positive d p d p 1 slopes this process is more pronounced exactly after t tp causing the formation of a step in d p d p 1 slopes at which point maximum detachment rate is achieved as indicated by the kinetic eq 3 max detachment rate is attained when the suspended concentration is zero and the attached concentration has reached its max value both conditions are satisfied at the end of broad pulse injection tp 28 hr and subsequently only clean water enters the aquifer furthermore the increase in d p d p 1 with distance along the 1 d aquifer seen in figs 5d e f as well as in figs 6d e f is attributed to the fact that as the nanoparticles move downstream they aggregate and consequently increase in size finally the increase in source concentration n 1 0 increases the collision rate see eq 5 promotes aggregation and increases all d p d p 1 ratios as seen in figs 5d e f and 6d e f in order to investigate the effect that kmax has on nanoparticle transport several simulations were performed for various k values which are presented in fig 7 the total number of suspended nanoparticles of class k 1 n 1 t n 1 0 is shown in figs 7a b as a function of time for two different initial number concentrations n 1 0 8 1014 and 3 1014 np1 m3 at x 0 6 m the absolute relative average error rae for the respective initial number concentrations is shown in fig 7c the results suggest that as the kmax parameter increases the breakthrough curves become smaller and exhibit increased tailing which is a consequence of increased attachment therefore as the k increases the extend of aggregation that can be accounted for also increases see fig 1 for dp 850 nm the rae can be calculated as follows 18 rae i 1 m x i k max x i 200 x i 200 m where m is the number of observations that participate in the calculation of the error and x i k max np1 l3 is the concentration of observation i calculated with the use of kmax classes it should be noted here that a value of kmax 200 classes was used as point of reference for calculating the rae it is clearly shown in fig 7c that increasing kmax causes the rae to decrease fast at early times but slowly at late times note that the y axis is on a log scale the slopes of the two rae lines suggest that if the initial concentration n 1 0 is increased the same error can be achieved by increasing the number of classes kmax 4 3 general observations from simulation results the discrepancies between models accounting for aggregation and those that do not varied spatially and temporally and were more pronounced as the initial injected concentration was increased furthermore all the graphs of the ratio d p d p 1 see figs 2c d 4b 5d e f and 6d e f which reflect the extend of aggregation process indicated that aggregation progressed became greater with increasing time and distance from the source additionally d p d p 1 ratios in conjunction with the filtration theory can be used to characterize the average attachment rate of nanoparticles onto the solid matrix of the porous medium it is evident that the attachment rate is not constant but varies spatially and temporally depending on the initial particle size the attachment rate increases or decreases in a practically linear fashion with increasing distance from the source see figs 6d e f certainly the impact of aggregation on nanoparticle transport cannot be discarded as negligible consequently proper modeling of aggregation is recommended 4 4 comparison with similar transport models babakhani 2019 developed a nanoparticle transport model that considers particle aggregation and evaluates size exclusion and concluded that accounting for particle aggregation the predictive ability of the model is improved similarly li and prigiobbe 2021 2020 developed a mechanistic model to describe the transport of nanaoparticles in the presence of foam the model takes into account nanoparticle aggeregation and particle attachment detachment onto the solid matrix it was shown that foam can be used to enchance nanoparticle delivery even in a low permeability medium within a shallow subsurface however both models mentioned above do not clearly describe how the transport and aggregation equations are coupled a governing mass transport equation that incorporates aggregation was not explicitly provided futhermore the first model overlooked the importance of particle attachment onto the solid matrix and the latter one considered particle hydrodynamic dispersion as negligible compared to particle advection particle dispersion might be negligible in cases where foam 90 vol gas is employed but certainly it can not be ignored in cases of low velocity laminar flow commonly occurring in water satured aquifers in this study the two site attachment model was adopted compère et al 2001 katzourakis and chrysikopoulos 2014 which has been proven to work very well with colloid biocolloid nanoparticle transport and cotransport experiments chrysikopoulos and katzourakis 2015 katzourakis and chrysikopoulos 2014 stefanarou et al 2023 the two site approach allows particles to be attached reveresibly or irreversibly onto the solid matrix of the porous medium if particles are attached reversibly then there is a chance depending on the suspended concentration to detach and transfer again into the aqueous phase however if particles are attached irreversibly then they remain attached till the end of the experiment contributing to the total column retention particles of the first type usually attach on the secondary minimum reversible attachment and of the second type on the primary minimum irreversible attachment tufenkji and elimelech 2005 finally in some cases katzourakis and chrysikopoulos 2014 the presence of irreversible particle attachment was absolutely necessary for the model to reduce extended tailing 5 summary and conclusions the conceptual model of katzourakis and chrysikopoulos 2021 was applied to simulate the transport of aggregating nanoparticles under various initial conditions in order to investigate the effects of initial particle concentration and particle diameter distribution on nanoparticle transport the results from several simulations suggested that nanoparticle aggregation can affect significantly the nanoparticle migration in porous media either hindering it or enhancing it depending on the initial particle diameter if further increase of the initial particle size causes decreasing attachment rates then aggregation enhances nanoparticle transport whereas if further increase leads to increasing attachment rates then aggregation hinders nanoparticle transport furthermore the increase of the source concentration significantly affects the aggregation rate additionally the existence of an initial particle diameter distribution can increase the aggregation rate and alter the transport characteristics of nanoparticles both observations are of great importance because they highlight the effects that the source configuration might have on particle transport and are often overlooked it should be noted that by changing the source configuration the shape of the resulting breakthrough curves might change considerably therefore ignoring nanoparticle aggregation or neglecting to take into account the actual source configuration might lead to erroneous results credit authorship contribution statement vasileios e katzourakis writing original draft methodology software constantinos v chrysikopoulos conceptualization supervision writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this research has received funding from khalifa university grant award number fsu 2023 12 supplementary materials supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2023 104475 appendix supplementary materials image application 1 
2,in august 2019 this journal published the paper water sorptivity of unsaturated fractured sandstone fractal modeling and neutron radiography experiment which reports on the experimental monitoring and analytical calculation of capillary moisture absorption in fractured sandstone this discussion considered a post publication review raises a number of concerns in relation to this paper these concern the observed deviation between the visual and numeric results of the moisture front penetrations the suboptimal character of the power law equation for the temporal evolutions of the moisture penetrations and the doubtful physical foundation for the pore fractal dimension and pore size ratio applied these concerns threaten the validity of the disputed paper s findings and conclusions and this discussion therefore invites its authors to refute these in their reply keywords critique bundle of tubes model capillary moisture absorption moisture front penetration fractal tortuosity data availability data will be made available on request 1 introduction in august 2019 this journal published the paper water sorptivity of unsaturated fractured sandstone fractal modeling and neutron radiography experiment zhao et al 2019 which reports on the experimental monitoring and analytical calculation of capillary moisture absorption in fractured sandstone for the experimental monitoring a neutron radiography set up is used for the analytical calculation a fractal imbibition model is developed the study visualises the capillary moisture absorption in the matrix via the bottom and fracture surface as well as in the fracture with neutron radiography the resulting temporal evolution of the moisture penetrations is then fitted with a base version of the analytical equation to attain the time exponents involved once these are found the sorptivities of matrix and fracture are predicted with the full analytical equation and the good agreement with their experimental counterparts is confirmed this discussion which is considered a post publication review however brings about a number of concerns with respect to zhao et al 2019 in section 2 it is firstly revealed that the values quantifying the moisture absorptions in the matrix reported in the paper s table 3 do mostly not correspond with the visual data illustrated in the paper s figures 7 8 section 3 secondly establishes that more proper fits of the temporal evolutions of the moisture penetrations are possible and argues that the fit for the fracture should have omitted the final four data points in section 4 thirdly it is reasoned that the values adopted for the sorptivity predictions are physically tenuous downgrading the paper s sorptivity prediction model to a humble calibratable formula this review is finally concluded with a discussion which formulates a number of concrete questions on the disputed paper and invites its authors to respond to these in their reply 2 moisture penetrations in matrix the paper s table 3 collects the temporal evolutions of the penetration distances of the moisture fronts originating from the bottom and fracture surfaces respectively lm1 lm2 and hm1 hm2 see the paper s figure 8a this table should hence be the quantification of the neutron radiography images shown in the paper s figures 7 8 the validity of the visual data in the paper s figures 7 8 is confirmed by the agreement of figure 7 s red lines for the fracture water level heights figure 7 s red values for these fracture water level heights and figure 10 s blue triangles for these fracture water level heights table 1 repeats the paper s table 3 s values at 0 4 and 0 8 s which should directly connect to the paper s figures 8a and 7 bottom right respectively table 1 shows that the penetrations via the fracture surface hm1 hm2 are about double as large as the penetrations via the bottom surface lm1 lm2 that conclusion is however contradicted by the visual information in the paper s figures 7 8 which suggest that the penetrations via bottom and fracture surfaces are very similar in extent as sample height and width are 35 mm and 41 mm respectively a ruler based evaluation allows quantifying that visual information which is equally collected in table 1 these values on the one hand highly differ from these gathered in the paper s table 3 and are on the other hand much more similar for bottom and fracture surfaces it is unclear how the paper s table 3 s values have been obtained but it is clear that these do not agree with the original visual data in the paper s figures 7 8 given that the paper s table 3 forms the basis for the further fits of the time exponents and predictions of the matrix sorptivities it cannot but be concluded that the derived results can neither be reliable given this divergence between the paper s table 3 and figures 7 8 3 time exponents of penetrations to predict the matrix sorptivities the time exponents of the moisture penetrations are necessary and these are obtained by fitting the paper s equation 13 to the measured moisture penetrations see the paper s figures 9 10 and tables 4 5 it should be noted that this constitutes a circularity as one needs to first measure the sorptivity in order to then predict the sorptivity that being said the typical time exponent for moisture accumulation and moisture penetration in capillary absorption experiments is 0 5 feng and janssen 2018 ren et al 2019 this typical value is however not obtained in the disputed paper as the fits lead to values ranging from 0 33 to 0 46 see the paper s figures 9 10 and tables 4 5 according to the paper the deviating exponents can be explained by fractal tortuosity see the paper s equation 7 the literature however contradicts that finding and that reason since there is ample proof for the standard linear with square root of time progression of capillary absorption in sandstone thomachot schneider et al 2008 al naddaf 2011 fu et al 2021 fig 1 confirms that such is also the case for the sandstone considered here the top graph reproduces the paper s power law fits see its equation 13 based on the values reported in its table 3 the center graph applies these same values but now fits a square root of time relation via the square root of time horizontal axis x hence equals t0 5 the obtained r² values are typically better except for hm but the difference is minimal for the square root of time fits than for the power law fits indicating that the latter are suboptimal in this respect it can however be noted that the fits for hm1 and hm2 are not perfect as it appears that the measured data sets do not cross the origin given the findings in section 1 above though there is severe distrust in the validity of these values the bottom graph hence fits the square root of time law to the corrected data see table 1 confirming that the moisture penetrations in the sandstone considered most plausibly behave standardly hence linear with square root of time while it could be countered that the fits in the bottom graph are established on two data points only their near perfect alignment with a square root of time expression without intercept as expected from the state of the art substantiates their plausibility this implies that the α values reported for the matrix moisture penetrations in the disputed paper are doubtful since it is far more likely that they are simply equal to 0 5 this moreover infers that also the sorptivities reported in the disputed paper are flawed since fig 1 bottom reveals sorptivities between 4 and 5 mm s0 5 instead of the earlier values ranging from 2 to 5 mm sα additionally it should be noted here that also the fit for the fracture is far from optimal the paper s figure 7 reveals that the moisture in the fracture reaches the top at around 0 65 s suggesting that the data points from 0 6 s onwards are already affected by this end of sample effect which is not considered in the analytical calculation the fit should have hence been restricted to the data points up to 0 5 s instead of using the full data set based on the values depicted in the paper s figure 7 a power law fit would yield time exponent 0 59 and sorptivity 52 2 mm s0 59 with r² 0 9769 whereas a square root of time fit would lead to time exponent 0 5 and sorptivity 51 5 mm s0 5 with r² 0 9826 also in this case thus the square root of time fit gives a more optimal fit what is more the power law fit results in a time exponent above 0 5 which is physically impossible 4 analytical prediction of sorptivity 4 1 introduction the paper s equations 13 and 15 describe the sorptivity relation adopted in the disputed paper 1 l sm σ cos θ 4 μ α d λ max 1 α 1 d α β d β 1 α t α with lsm m the moisture penetration σ n m surface tension θ contact angle μ pa s dynamic viscosity α time exponent d pore fractal dimension λmax m maximum pore size β ratio of smallest and largest pore size t s time this expression is based on a bundle of tubes model which simplifies the pore structure of the porous material to a set of independent parallel tubes in the disputed paper the tubes radii and tortuosities are governed by fractal concepts see the paper s equations 13 and 9 these concepts require values for the tortuosity fractal dimension dt pore fractal dimension d pore size ratio β and maximum pore size λmax dt is hidden in α as dt equals 1 2α where the latter is obtained as the time exponent fitted to the moisture penetration d is valued via fractal box counting executed on the paper s figure 6b while β is simply quantified based on literature values λmax finally is calculated with the paper s equation 14 with the values assumed in the disputed paper it finds a nice agreement between experiment and calculation for the moisture absorption via the bottom surface while the agreement is less solid for the moisture absorption via the fracture surface see its table 7 the latter deviation is explained in the disputed paper by microcracking near the fracture surfaces it should be stated from the start that such success of the bundle of tubes method is surprising given the current insights on the reliability of such models for moisture transfer in porous media such approximation of porous media s pore structures with bundle of tubes surrogates and their subsequent translation to moisture storage and transport properties has a long history already the early papers in this field stem from the 1950 s purcell 1949 burdine et al 1950 1953 that bundle of tubes approach however received criticism from the start as fatt 1956 avowed that the two models used in the past the sphere pack and the bundle of tubes have been too simple and as a result the equations derived from them have failed to predict observed properties agreement between theory and observation has been achieved for these models by inserting parameters of doubtful physical significance that invalidity of bundle of tubes models for moisture transfer in porous media has since been confirmed many times over for multiple material categories construction materials reservoir rocks soil strata fischer and celia 1999 scheffler and plagge 2010 hunt et al 2013 in recent decades hence the application of these overly simplified pore structure representations has been abandoned in favour of models using a more complex and complete topological and geometrical model for the pore space islahuddin and janssen 2019 among very many others the good agreement between experiment and calculation as reported in the disputed paper is however already degraded by the findings in sections 2 and 3 above fig 1 bottom reveals measured sorptivities of 4 to 5 mm s0 5 and exponents of 0 5 predictions with equation 1 using these corrected α values lead to 2 8 mm s0 5 hence strongly different already from their measured counterparts these predictions however still apply d equal to 1 84 and β equal to 0 01 values which are both queried in the two subsections below 4 2 fractal pore dimension in the disputed paper the fractal pore dimension d is determined via fractal box counting of the ct slice shown in the paper s figure 6b based on the scale indicated the largest pore feature observable in that ct slice is below 100 µm based on the voxel count versus that scale indicated the voxel size is about 1 25 µm implying that the smallest pore feature detectible in that ct slice is above 3 µm that infers that the fractal box counting only considers a part of the pore structure given that the paper s figure 4 presenting the mercury intrusion porosimetry results reveals pore sizes running from below 0 003 µm to over 300 µm certainly at the lower end a lot of pore features are not represented in the ct slice in the paper s figure 6b moreover it is impossible to account for tortuosity from a single two dimensional ct slice while that tortuosity plays a large role in the disputed paper the resulting d value is hence not necessarily representative for the actual pore structure of the sandstone following wang et al 2019 the pore fractal dimension can be derived from the mercury intrusion results as well fractal theory imposes this correlation between the number of pores with diameter larger than λ n λ and pore diameter λ 2 n λ λ d assuming a bundle of tubes model with fractal tortuosity the number of pores with diameter λ n λ m² per unit cross section of the sample can be calculated as 3 n λ 4 v λ l s π λ 2 τ λ l s 4 v λ π λ 2 τ λ 4 v λ l s 1 d t π λ 2 λ 1 d t with v λ m³ m³ the volume of mercury associated with pores with diameter λ per unit volume of material τ λ tortuosity of pores with diameter λ and ls m sample height this computation assumes a sample with cross section 1 m² and height ls this sample height ls thus also forms the net length of the pores stretching tortuously from bottom to top of the sample plotting pore number n λ and pore diameter λ in a log log graph then permits determining the pore fractal dimension d from the slope of the curve s as d s wang et al 2019 see fig 2 that figure represents the application of eq 3 to the paper s mercury intrusion data shown in its figure 4 two variants are assessed for the first α equals 0 41 the average time exponent obtained for the moisture penetrations in the matrix in the disputed paper while for the second α equals 0 5 as argued in section 3 which respectively translate to dt s of 1 22 and 1 00 fig 2 reveals that the resulting values for the pore fractal dimension d are respectively 1 55 and 1 77 both different from the 1 84 given in the disputed paper fig 2 moreover exposes that there is some turmoil in the two curves in the 10 to 100 µm range which brings further doubts on the reliability of the box counting approach applied in the disputed paper since the considered ct slice primarily comprises pore features in that spectrum fortunately the impact of these corrected d values on the predicted sorptivities is not overly big for d equal to 1 77 and α equal to 0 5 on the one hand the predictions reduce to 2 6 mm s0 5 for d equal to 1 55 and α equal to 0 41 on the other hand the predictions lower to 1 7 mm s0 41 instead of the 2 8 mm s0 5 obtained in section 4 1 4 3 pore size ratio fig 2 however also sheds light on the pore size ratio β in the disputed paper this is presumed 0 01 based on literature and not on physical information concerning the sandstone considered actually this value is used as a calibration parameter as the disputed paper states that when β equals 10 4 or 10 3 the predicted values of the developed model significantly underestimate the sorptivity of matrix when β 10 2 the matrix sorptivity predicted by the developed model is close to the experimental nonlinear fitting value such calibratable parameter reduces the paper s prediction model to a humble calibratable formula as no argumentation is provided on how to dependably and independently determine β this incidentally introduces another circularity in the prediction model see also the time exponent in section 3 as one again needs to first measure the sorptivity in order to then predict the sorptivity fig 2 however invalidates β being equal to 0 01 as it exposes that the pore features come in sizes ranging 5 orders of magnitude given the smallest and largest pore diameters are respectively below 0 003 µm and above 300 µm and unfortunately β does have critical impacts on the sorptivity predictions with β equal to 10 5 and α equal to 0 5 d equal to 1 77 the predicted sorptivities go down to 0 08 mm s0 5 hence roughly 50 to 60 times below the measured sorptivities of 4 to 5 mm s0 5 this outcome of course utterly invalidates the paper s matching measured and predicted values and thus nullifies the reliability of the sorptivity prediction model in zhao et al 2019 this of course does not come as a surprise given the state of the art with respect to bundle of tubes models for the quantification of moisture transfer in porous media see section 4 1 in this case β is fatt s 1956 parameter of doubtful physical significance inserted to match measurements and simulations 5 discussion the observations discussed above in sections 2 3 and 4 lead to a number of serious concerns on zhao et al 20 109 and this discussion therefore invites its authors to address these in their reply the following concrete questions may serve as guideline for such reply 1 can zhao et al 2019 explain the deviations observed between the visual findings in its figures 7 8 and the quantitative information in their table 3 see section 2 2 can zhao et al 2019 support why their power law fits contradicting the state of the art with respect to moisture absorption in sandstone should be preferred over the more optimal square root of time fits and more in line with the state of the art see section 3 3 can zhao et al 2019 defend their choices with respect to d and β in light of the physical information available for the sandstone see sections 4 2 4 3 4 can zhao et al 2019 reflect over their defining their equation 15 as a sorptivity prediction model given that determination of the parameters α and β in their actually requires performing the capillary absorption experiment 6 conclusions in august 2019 this journal published the paper water sorptivity of unsaturated fractured sandstone fractal modeling and neutron radiography experiment zhao et al 2019 which reports on the experimental monitoring and analytical calculation of capillary moisture absorption in fractured sandstone this discussion considered a post publication review however brings about a number of concerns in relation to zhao et al 2019 concerning 1 the divergence between visual and numeric results of the moisture front penetrations 2 the optimality of standard square root of time fits over the applied power law fits 3 the deviation between physical data and preferred d and β values 4 the contradiction that the prediction of sorptivity requires measurement of sorptivity these concerns threaten the validity of zhao et al 2019 findings and conclusions and the authors are therefore invited to refute these in their reply declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper 
2,in august 2019 this journal published the paper water sorptivity of unsaturated fractured sandstone fractal modeling and neutron radiography experiment which reports on the experimental monitoring and analytical calculation of capillary moisture absorption in fractured sandstone this discussion considered a post publication review raises a number of concerns in relation to this paper these concern the observed deviation between the visual and numeric results of the moisture front penetrations the suboptimal character of the power law equation for the temporal evolutions of the moisture penetrations and the doubtful physical foundation for the pore fractal dimension and pore size ratio applied these concerns threaten the validity of the disputed paper s findings and conclusions and this discussion therefore invites its authors to refute these in their reply keywords critique bundle of tubes model capillary moisture absorption moisture front penetration fractal tortuosity data availability data will be made available on request 1 introduction in august 2019 this journal published the paper water sorptivity of unsaturated fractured sandstone fractal modeling and neutron radiography experiment zhao et al 2019 which reports on the experimental monitoring and analytical calculation of capillary moisture absorption in fractured sandstone for the experimental monitoring a neutron radiography set up is used for the analytical calculation a fractal imbibition model is developed the study visualises the capillary moisture absorption in the matrix via the bottom and fracture surface as well as in the fracture with neutron radiography the resulting temporal evolution of the moisture penetrations is then fitted with a base version of the analytical equation to attain the time exponents involved once these are found the sorptivities of matrix and fracture are predicted with the full analytical equation and the good agreement with their experimental counterparts is confirmed this discussion which is considered a post publication review however brings about a number of concerns with respect to zhao et al 2019 in section 2 it is firstly revealed that the values quantifying the moisture absorptions in the matrix reported in the paper s table 3 do mostly not correspond with the visual data illustrated in the paper s figures 7 8 section 3 secondly establishes that more proper fits of the temporal evolutions of the moisture penetrations are possible and argues that the fit for the fracture should have omitted the final four data points in section 4 thirdly it is reasoned that the values adopted for the sorptivity predictions are physically tenuous downgrading the paper s sorptivity prediction model to a humble calibratable formula this review is finally concluded with a discussion which formulates a number of concrete questions on the disputed paper and invites its authors to respond to these in their reply 2 moisture penetrations in matrix the paper s table 3 collects the temporal evolutions of the penetration distances of the moisture fronts originating from the bottom and fracture surfaces respectively lm1 lm2 and hm1 hm2 see the paper s figure 8a this table should hence be the quantification of the neutron radiography images shown in the paper s figures 7 8 the validity of the visual data in the paper s figures 7 8 is confirmed by the agreement of figure 7 s red lines for the fracture water level heights figure 7 s red values for these fracture water level heights and figure 10 s blue triangles for these fracture water level heights table 1 repeats the paper s table 3 s values at 0 4 and 0 8 s which should directly connect to the paper s figures 8a and 7 bottom right respectively table 1 shows that the penetrations via the fracture surface hm1 hm2 are about double as large as the penetrations via the bottom surface lm1 lm2 that conclusion is however contradicted by the visual information in the paper s figures 7 8 which suggest that the penetrations via bottom and fracture surfaces are very similar in extent as sample height and width are 35 mm and 41 mm respectively a ruler based evaluation allows quantifying that visual information which is equally collected in table 1 these values on the one hand highly differ from these gathered in the paper s table 3 and are on the other hand much more similar for bottom and fracture surfaces it is unclear how the paper s table 3 s values have been obtained but it is clear that these do not agree with the original visual data in the paper s figures 7 8 given that the paper s table 3 forms the basis for the further fits of the time exponents and predictions of the matrix sorptivities it cannot but be concluded that the derived results can neither be reliable given this divergence between the paper s table 3 and figures 7 8 3 time exponents of penetrations to predict the matrix sorptivities the time exponents of the moisture penetrations are necessary and these are obtained by fitting the paper s equation 13 to the measured moisture penetrations see the paper s figures 9 10 and tables 4 5 it should be noted that this constitutes a circularity as one needs to first measure the sorptivity in order to then predict the sorptivity that being said the typical time exponent for moisture accumulation and moisture penetration in capillary absorption experiments is 0 5 feng and janssen 2018 ren et al 2019 this typical value is however not obtained in the disputed paper as the fits lead to values ranging from 0 33 to 0 46 see the paper s figures 9 10 and tables 4 5 according to the paper the deviating exponents can be explained by fractal tortuosity see the paper s equation 7 the literature however contradicts that finding and that reason since there is ample proof for the standard linear with square root of time progression of capillary absorption in sandstone thomachot schneider et al 2008 al naddaf 2011 fu et al 2021 fig 1 confirms that such is also the case for the sandstone considered here the top graph reproduces the paper s power law fits see its equation 13 based on the values reported in its table 3 the center graph applies these same values but now fits a square root of time relation via the square root of time horizontal axis x hence equals t0 5 the obtained r² values are typically better except for hm but the difference is minimal for the square root of time fits than for the power law fits indicating that the latter are suboptimal in this respect it can however be noted that the fits for hm1 and hm2 are not perfect as it appears that the measured data sets do not cross the origin given the findings in section 1 above though there is severe distrust in the validity of these values the bottom graph hence fits the square root of time law to the corrected data see table 1 confirming that the moisture penetrations in the sandstone considered most plausibly behave standardly hence linear with square root of time while it could be countered that the fits in the bottom graph are established on two data points only their near perfect alignment with a square root of time expression without intercept as expected from the state of the art substantiates their plausibility this implies that the α values reported for the matrix moisture penetrations in the disputed paper are doubtful since it is far more likely that they are simply equal to 0 5 this moreover infers that also the sorptivities reported in the disputed paper are flawed since fig 1 bottom reveals sorptivities between 4 and 5 mm s0 5 instead of the earlier values ranging from 2 to 5 mm sα additionally it should be noted here that also the fit for the fracture is far from optimal the paper s figure 7 reveals that the moisture in the fracture reaches the top at around 0 65 s suggesting that the data points from 0 6 s onwards are already affected by this end of sample effect which is not considered in the analytical calculation the fit should have hence been restricted to the data points up to 0 5 s instead of using the full data set based on the values depicted in the paper s figure 7 a power law fit would yield time exponent 0 59 and sorptivity 52 2 mm s0 59 with r² 0 9769 whereas a square root of time fit would lead to time exponent 0 5 and sorptivity 51 5 mm s0 5 with r² 0 9826 also in this case thus the square root of time fit gives a more optimal fit what is more the power law fit results in a time exponent above 0 5 which is physically impossible 4 analytical prediction of sorptivity 4 1 introduction the paper s equations 13 and 15 describe the sorptivity relation adopted in the disputed paper 1 l sm σ cos θ 4 μ α d λ max 1 α 1 d α β d β 1 α t α with lsm m the moisture penetration σ n m surface tension θ contact angle μ pa s dynamic viscosity α time exponent d pore fractal dimension λmax m maximum pore size β ratio of smallest and largest pore size t s time this expression is based on a bundle of tubes model which simplifies the pore structure of the porous material to a set of independent parallel tubes in the disputed paper the tubes radii and tortuosities are governed by fractal concepts see the paper s equations 13 and 9 these concepts require values for the tortuosity fractal dimension dt pore fractal dimension d pore size ratio β and maximum pore size λmax dt is hidden in α as dt equals 1 2α where the latter is obtained as the time exponent fitted to the moisture penetration d is valued via fractal box counting executed on the paper s figure 6b while β is simply quantified based on literature values λmax finally is calculated with the paper s equation 14 with the values assumed in the disputed paper it finds a nice agreement between experiment and calculation for the moisture absorption via the bottom surface while the agreement is less solid for the moisture absorption via the fracture surface see its table 7 the latter deviation is explained in the disputed paper by microcracking near the fracture surfaces it should be stated from the start that such success of the bundle of tubes method is surprising given the current insights on the reliability of such models for moisture transfer in porous media such approximation of porous media s pore structures with bundle of tubes surrogates and their subsequent translation to moisture storage and transport properties has a long history already the early papers in this field stem from the 1950 s purcell 1949 burdine et al 1950 1953 that bundle of tubes approach however received criticism from the start as fatt 1956 avowed that the two models used in the past the sphere pack and the bundle of tubes have been too simple and as a result the equations derived from them have failed to predict observed properties agreement between theory and observation has been achieved for these models by inserting parameters of doubtful physical significance that invalidity of bundle of tubes models for moisture transfer in porous media has since been confirmed many times over for multiple material categories construction materials reservoir rocks soil strata fischer and celia 1999 scheffler and plagge 2010 hunt et al 2013 in recent decades hence the application of these overly simplified pore structure representations has been abandoned in favour of models using a more complex and complete topological and geometrical model for the pore space islahuddin and janssen 2019 among very many others the good agreement between experiment and calculation as reported in the disputed paper is however already degraded by the findings in sections 2 and 3 above fig 1 bottom reveals measured sorptivities of 4 to 5 mm s0 5 and exponents of 0 5 predictions with equation 1 using these corrected α values lead to 2 8 mm s0 5 hence strongly different already from their measured counterparts these predictions however still apply d equal to 1 84 and β equal to 0 01 values which are both queried in the two subsections below 4 2 fractal pore dimension in the disputed paper the fractal pore dimension d is determined via fractal box counting of the ct slice shown in the paper s figure 6b based on the scale indicated the largest pore feature observable in that ct slice is below 100 µm based on the voxel count versus that scale indicated the voxel size is about 1 25 µm implying that the smallest pore feature detectible in that ct slice is above 3 µm that infers that the fractal box counting only considers a part of the pore structure given that the paper s figure 4 presenting the mercury intrusion porosimetry results reveals pore sizes running from below 0 003 µm to over 300 µm certainly at the lower end a lot of pore features are not represented in the ct slice in the paper s figure 6b moreover it is impossible to account for tortuosity from a single two dimensional ct slice while that tortuosity plays a large role in the disputed paper the resulting d value is hence not necessarily representative for the actual pore structure of the sandstone following wang et al 2019 the pore fractal dimension can be derived from the mercury intrusion results as well fractal theory imposes this correlation between the number of pores with diameter larger than λ n λ and pore diameter λ 2 n λ λ d assuming a bundle of tubes model with fractal tortuosity the number of pores with diameter λ n λ m² per unit cross section of the sample can be calculated as 3 n λ 4 v λ l s π λ 2 τ λ l s 4 v λ π λ 2 τ λ 4 v λ l s 1 d t π λ 2 λ 1 d t with v λ m³ m³ the volume of mercury associated with pores with diameter λ per unit volume of material τ λ tortuosity of pores with diameter λ and ls m sample height this computation assumes a sample with cross section 1 m² and height ls this sample height ls thus also forms the net length of the pores stretching tortuously from bottom to top of the sample plotting pore number n λ and pore diameter λ in a log log graph then permits determining the pore fractal dimension d from the slope of the curve s as d s wang et al 2019 see fig 2 that figure represents the application of eq 3 to the paper s mercury intrusion data shown in its figure 4 two variants are assessed for the first α equals 0 41 the average time exponent obtained for the moisture penetrations in the matrix in the disputed paper while for the second α equals 0 5 as argued in section 3 which respectively translate to dt s of 1 22 and 1 00 fig 2 reveals that the resulting values for the pore fractal dimension d are respectively 1 55 and 1 77 both different from the 1 84 given in the disputed paper fig 2 moreover exposes that there is some turmoil in the two curves in the 10 to 100 µm range which brings further doubts on the reliability of the box counting approach applied in the disputed paper since the considered ct slice primarily comprises pore features in that spectrum fortunately the impact of these corrected d values on the predicted sorptivities is not overly big for d equal to 1 77 and α equal to 0 5 on the one hand the predictions reduce to 2 6 mm s0 5 for d equal to 1 55 and α equal to 0 41 on the other hand the predictions lower to 1 7 mm s0 41 instead of the 2 8 mm s0 5 obtained in section 4 1 4 3 pore size ratio fig 2 however also sheds light on the pore size ratio β in the disputed paper this is presumed 0 01 based on literature and not on physical information concerning the sandstone considered actually this value is used as a calibration parameter as the disputed paper states that when β equals 10 4 or 10 3 the predicted values of the developed model significantly underestimate the sorptivity of matrix when β 10 2 the matrix sorptivity predicted by the developed model is close to the experimental nonlinear fitting value such calibratable parameter reduces the paper s prediction model to a humble calibratable formula as no argumentation is provided on how to dependably and independently determine β this incidentally introduces another circularity in the prediction model see also the time exponent in section 3 as one again needs to first measure the sorptivity in order to then predict the sorptivity fig 2 however invalidates β being equal to 0 01 as it exposes that the pore features come in sizes ranging 5 orders of magnitude given the smallest and largest pore diameters are respectively below 0 003 µm and above 300 µm and unfortunately β does have critical impacts on the sorptivity predictions with β equal to 10 5 and α equal to 0 5 d equal to 1 77 the predicted sorptivities go down to 0 08 mm s0 5 hence roughly 50 to 60 times below the measured sorptivities of 4 to 5 mm s0 5 this outcome of course utterly invalidates the paper s matching measured and predicted values and thus nullifies the reliability of the sorptivity prediction model in zhao et al 2019 this of course does not come as a surprise given the state of the art with respect to bundle of tubes models for the quantification of moisture transfer in porous media see section 4 1 in this case β is fatt s 1956 parameter of doubtful physical significance inserted to match measurements and simulations 5 discussion the observations discussed above in sections 2 3 and 4 lead to a number of serious concerns on zhao et al 20 109 and this discussion therefore invites its authors to address these in their reply the following concrete questions may serve as guideline for such reply 1 can zhao et al 2019 explain the deviations observed between the visual findings in its figures 7 8 and the quantitative information in their table 3 see section 2 2 can zhao et al 2019 support why their power law fits contradicting the state of the art with respect to moisture absorption in sandstone should be preferred over the more optimal square root of time fits and more in line with the state of the art see section 3 3 can zhao et al 2019 defend their choices with respect to d and β in light of the physical information available for the sandstone see sections 4 2 4 3 4 can zhao et al 2019 reflect over their defining their equation 15 as a sorptivity prediction model given that determination of the parameters α and β in their actually requires performing the capillary absorption experiment 6 conclusions in august 2019 this journal published the paper water sorptivity of unsaturated fractured sandstone fractal modeling and neutron radiography experiment zhao et al 2019 which reports on the experimental monitoring and analytical calculation of capillary moisture absorption in fractured sandstone this discussion considered a post publication review however brings about a number of concerns in relation to zhao et al 2019 concerning 1 the divergence between visual and numeric results of the moisture front penetrations 2 the optimality of standard square root of time fits over the applied power law fits 3 the deviation between physical data and preferred d and β values 4 the contradiction that the prediction of sorptivity requires measurement of sorptivity these concerns threaten the validity of zhao et al 2019 findings and conclusions and the authors are therefore invited to refute these in their reply declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper 
3,reservoir management and contaminant transport simulations rely on accurate modeling of the subsurface this task becomes more challenging when the reservoir of interest also has a large amount of fractures inverse modeling of these fractured media involves first performing multiple field tests such as pumping tests tracer tests or seismic surveys inverse modeling methods then are used to find potential geologic models that produce simulated results that match field observations popular methods include the ensemble kalman filter markov chain monte carlo and simulated annealing a common challenge these methods face is that inverse modeling of fractured media is inherently a multiobjective optimization task the inverse modeling method must find a discrete fracture model that produces the same flow characteristics as observed in field pumping or tracer tests but it also must find a discrete fracture model with a fracture network that matches the fracture parameter distribution observed by the field measurements such as seismic surveys this challenge can be approached in two steps the first step is producing discrete fracture network models with parameter distributions that match field observations from seismic surveys this study focused on the second step involving the development of a method that can take a population of discrete fracture networks and generate new fracture networks in a way that preserves the fracture parameter distribution this was done using a genetic algorithm modified to apply to the domain of discrete fracture networks during genetic mixing the fractures of the child model are generated by randomly copying over fractures from the parent models this process ensures the child model adopts the fracture parameter distribution of the parent models this study tests the effectiveness of this genetic algorithm on a synthetic example the results of the experiment show the genetic algorithm is able to effectively produce a population of discrete fracture models with breakthrough curves that match the curves of the reference model keywords discrete fracture networks genetic algorithms stochastic inversion subsurface flow and transport data availability no data was used for the research described in the article 1 introduction fracture network models are important tools across multiple industries the most prominent use of fracture network models is in the petroleum industry where they are used for determining the optimal methods for fracturing and recovering petroleum from reservoirs another application for fracture network models is geothermal energy production to design effective methods for producing fractures that transfer thermal energy from the crust to the fluids flowing through the fracture network researchers rely on accurate fracture network models a wide variety of conceptual models exist for simulating flows through fractured rock concepts include modeling the rock is a discrete network of fractures another as nonuniform continuum of hydraulic parameters or even as a hybrid where the rock is modeled as a nonuniform continuum that contains a small number of dominant discrete features neuman 2005 such variety exists because of the different geologic contexts that produces a range of flow behavior this study focuses on the use discrete fracture network models to simulate flows through fractured rock this conceptual model would be suitable to simulating flows through impermeable crystalline rock unlike traditional groundwater flow simulations that discretize the domain as a grid of cells each with its hydraulic properties discrete fracture networks describe fractures as a set of finite planes with a defined shapes sizes locations and orientations the fracture parameters often are defined on the basis of a probability distribution that imitates distributions of fractures measured in the field during flow simulations the fluids strictly flow through these fracture planes with the flow behavior governed by the hydraulic properties of the fracture planes and how the fracture planes are connected note that for real fractured rock fluid exchange can exist between the fracture and the matrix for this work we assume that most of the fluid dynamics is governed by the geometry of fractures and how they are connected to each other the goal is to investigate how the complexities of geometry dominated flow affect the performance of a proposed method using genetic algorithms thanks to the flexibility of genetic algorithms the proposed method can be adjusted to handle scenarios where the fluid exchange between fractures and the rock becomes more significant even with a method for accurately simulating the flow of fluids through the fracture network researchers must still somehow convert field measurements into a set of discrete fracture network parameters such that the simulated flow model produces results that match observations from the field this task of inverse modeling is known as history matching and researchers have deployed a wide range of methods to accomplish it methods include ensemble kalman filters ping and zhang 2013 nejadi et al 2017 markov chain monte carlo optimization ma et al 2008 xu et al 2013 and simulated annealing tran 2007 mahmoodpour and masihi 2016 an important method for converting a set of well observations into a hydraulic model for fractured rock is hydraulic tomography illman et al 2009 illman 2014 zha et al 2015 klepikova et al 2020 ringel et al 2021 the process for hydraulic tomography begins by first acquiring data from pumping tests applied to a well field at the site of interest these tests must be designed in a way that properly extracts information about the hydraulic properties of the rock between the wells the tests usually involve selecting a well to pressurize then monitoring the pressure of adjacent wells through time to see how the pressure from the injection well propagates through the rock once data is collected a variety of inversion methods can be used to convert the collected data into hydraulic models for klepikova et al 2020 a misfit function was first defined to measure the differences between the simulated and observed the parameters of the model were then tuned using the nelder mead simplex algorithm mckinnon 1998 in order to find the optimal parameter values that minimizes the misfit function zha et al 2015 performed their inversion step using the simultaneous successive linear estimator algorithm xiang et al 2009 where a successive bayesian linear estimator derives the mean parameter fields using the results of field tests and assumes prior knowledge of the mean value and spatial structures of estimated parameters to perform inversion for a three dimensional discrete fracture network ringel et al 2021 used a reversible jump markov chain monte carlo method fan and sisson 2011 to calculate the likelihood a given change to a proposed model such as adding or removing a fracture yields a new model that is most likely to produce pressure transients that match with field tests note that many of these methods have the same underlying process of running multiple fluid flow simulations to find a group of model parameters that produce results that fit field observations often these simulations are large and computationally expensive so researchers use a variety of techniques and heuristics to efficiently find solutions while minimizing the required amount of computational resources another popular family of methods used by researchers includes evolutionary techniques which are optimization algorithms with heuristics that are inspired by biological processes in nature one of the most popular heuristics is the concept of natural selection possible solutions to an optimization problem are represented as individuals in a population the fitness of these individuals is determined by how well they solve the optimization problem individuals that perform poorly are assigned a low fitness value and removed from the population but individuals that perform well are assigned a high fitness value and are allowed to stay in the population after the weakest individuals have been removed from the population the remaining individuals then are allowed to reproduce and create a new generation of possible solutions during the reproduction process the child solutions adopt characteristics from each of the parent solutions the idea is that the children will adopt the beneficial characteristics of their parents ideally producing a new generation of solutions that will perform better than the previous generation evolutionary methods such as differential evolution das and suganthan 2010 pant et al 2020 and genetic algorithms mitchell 1998 sivanandam and deepa 2008 rely on this main heuristic of natural selection researchers then can use these techniques to find the optimal set of fracture network parameters that fits simulation results to field observations cadini et al 2013 liu and reynolds 2019 zhang et al 2019 maucec et al 2020 although it is important to be able to generate a set of possible fracture network parameters that produce simulation results that fit with field observations it is just as important that the distribution of the fracture parameters also match what is observed in the rock to effectively produce a set of fractures that imitate the parameter distributions in real life stochastic fracture network generation methods can be used j p 2005 bonneau et al 2013 these methods often deploy simple generating heuristics inspired by the rock mechanics involved with fracture formation and propagation these simple rules can generate fractures with parameters such as fracture shape size orientation or spatial density with the same distributions as observed in the field using techniques like well logging and micro seismics willis et al 2006 han et al 2021 kennedy et al 2022 because fracture networks involve complexity at multiple scales fractal dimensions are another popular method used to ensure that generated fracture models imitate distributions found in the field liu et al 2015 zhang et al 2019 overall researchers have many methods available for them to produce realistic and accurate distributions of fractures though seismic surveys can be used to generate a discrete fracture network sicking and malin 2019 this does not necessarily mean the generated model will produce simulated well test results that will match with all field observations such as well pressure tests instead it is best to create discrete fracture network models using results from both seismic surveys and well pressure tests mayerhofer et al 2006 for example seismic survey data can used to define the number of fractures and the density of fractures for a given study volume hydraulic tomography can then use well pressure tests to determine remaining model parameters such fracture connectivity orientations and size once discrete fracture networks can be generated to have some of the same parameter distributions measured from seismic surveys it can be challenging to also modify the generate models such that they also match the flow characteristics measured from pumping and tracer tests performed in the field when naively changing the parameters of the fracture network to better fit with measured flow characteristics this often can cause the distribution of fracture parameters to deviate from distributions measured in the field to deal with this need for a fracture model with both accurate fracture distributions and accurate flow characteristics researchers use optimization techniques that can handle multiobjective functions maucec et al 2020 instead of using techniques that iteratively generate a population of fracture models that converge toward both accurate distributions and flow characteristics at the same time this work proposes a different method that can meet this multiobjective goal in two distinct and straightforward steps the first step involves establishing a method for readily generating a set of fracture models with fracture parameter distributions that match field observations this step can be done by simply using any of the popular methods in the literature such as stochastic generation or using fractal dimensions the second critical step is to use the population of generated fracture networks to create new fracture models that are more likely to match flow characteristics but in a way that preserves the fracture distributions of the previously generated fracture network models this is done by creating a genetic algorithm optimization architecture that is designed to preserve the fracture parameter distribution of the population with each generation this work includes a synthetic experiment to investigate the effectiveness of this technique 2 methodology 2 1 discrete fracture network to model the discrete fracture networks the software package dfnworks by the los alamos national laboratory was used hyman et al 2015 dfnworks is composed of multiple software packages combined to form a seamless software suite for generating three dimensional models of discrete fracture networks as well as simulating flow and transport of particles through these fracture networks to generate a discrete fracture network dfnworks first uses a feature rejection algorithm for meshing fram method to generate the underlying geometric model for the discrete fracture network at this stage fractures are represented as flat polygons of various sizes and are distributed across a three dimensional volume each of the polygons is assigned additional attributes such as aperture size to generate a fracture network the distribution of the fracture network parameters are first defined once defined the distributions then are sampled and the fracture network is iteratively built one fracture at a time after each fracture is added the fram method checks if a series of constraints are met constraints such as proximity to other fractures or proximity to the model domain boundary are considered if the added fracture fails to meet a criterion then the fracture is rejected and a new one is created otherwise the fracture is kept within the fracture model and the process continues when the growing fracture model achieves a certain stopping criterion such as a set number of fractures the iteration loop halts and the geometric fracture model is presented as the final model ready for the next step of the process after the geometric fracture model is created the lagrit meshing tool converts the geometric model into a mesh model the meshing process produces a delaunay triangulation mesh that is ideal for parallel computations during the fracture generation phase the user has the freedom to define a probability distribution from which the software can sample to generate the fractures for fracture radius dfnworks provides options for a log normal distribution an exponential distribution a truncated power law distribution or simply a constant for this study the truncated power law distribution was used 1 p r α r 0 r r 0 1 α 1 r u r 0 α in this equation the probability density function p r is parameterized with the maximum fracture radius r u the minimum fracture radius r 0 and an exponent α that defines the shape of the power law distribution the truncated power law distribution was used because its additional parameters allow greater control on the shape of the distribution that controls the fracture radius di federico and neuman 1997 di federico et al 1999 neuman 2008 but the proposed method does not rely on a specific distribution to function so the approach will still work when an alternate fracture distribution function is used for the fracture orientation the orientation vector normal to the fracture plane must be sampled by essentially using a gaussian distribution that is embedded onto the surface of a three dimensional sphere this was done by using the three dimensional von mises fisher distribution 2 f x μ κ κ e x p κ μ t x 4 π sinh κ in this equation the probability distribution function f x is parameterized by μ and κ μ is the vector for the mean orientation with t indicating that the vector is transposed κ is the parameter that represents the variance of the distribution around the mean orientation dfnworks samples this distribution by using the algorithm outlined by wood 1994 2 2 flow and transport simulation the dfnworks suite hyman et al 2015 also includes software for running flow and transport simulations using the discrete fracture network called dfnflow the software takes the meshed fracture network model and uses pflotran to compute the pressure field across the network pflotran lichtner et al 2015 is an open source code base that was written by core developers from the u s department of national laboratories such as the los alamos national laboratory sandia national laboratory lawrence berkeley national laboratory and oak ridge national laboratory as well as contributions from universities and research labs around the world pflotran is capable of massively parallel operations that can operate at multiple scales and physics pflotran also can solve differential equations for non isothermal multiphase flow reactive transport and geomechanics in porous media this study uses pflotran to solve for the single phase flow across the fractures during steady state pflotran does this by solving the three dimensional richards equation richards 1931 the mixed form of the richards equation proposed by celia et al 1990 yields robust numerical solutions and maintains mass balance for unsaturated flow problems wu et al 2021 3 t ϕ s η η q q w in this equation t is time ϕ is the porosity of the soil matrix s is the water saturation η is the molar water density q is the darcy flux and q w is a positive source or a negative sink of water the darcy flux q is calculated using darcy s law 4 q k k r μ p w w η g z where k is the intrinsic permeability k r is the relative permeability μ is the water viscosity p is the pressure w w is the formula weight of water g is the acceleration of gravity and z is the vertical component of the position vector in this study the van genuchten soil water retention curve van genuchten 1980 and the mualem relative permeability function mualem 1976 was used to calculate the water saturation and the relative permeability for this study the boundary conditions for the model will be a no flow boundary condition for all boundaries except for the injection and extraction well the two wells will be set to a boundary condition of a constant pressure the permeability of the fractures are derived as a function of the aperture of the fracture this is done by first assuming the fluid flows between two smooth impermeable parallel plates this assumption allows the boussinesq equation boussinesq 1868 to yield the volumetric flow rate q per unit fracture width normal to the direction flow hyman et al 2016 5 q b 3 ρ g 12 ν h in this equation b is the fracture aperture ρ is the fluid density g is gravitational acceleration and h is hydraulic head this relationship between fracture aperture and flow rate can be used to derive a relationship between aperture and transmissivity hyman et al 2016 6 t b 3 ρ g 12 ν this equation is referred to as the cubic law witherspoon et al 1980 pflotran uses this relationship to convert fracture aperture into permeability note that these equations assume that the flow through the fracture is homogeneous and that there is no fluid exchange between the fracture and the matrix such an assumption can be justified when modeling flow through impermeable crystalline rock for this study focus is on investigating whether the genetic algorithm is capable of tuning a model with its flow dynamics dominated by the geometry and connectivity of the fractures however the methods developed in this work can be extended to study sites where there is significant fluid exchange between the fracture and the matrix after the flow and pressure field is calculated this information is sent to the software called dfntrans for particle transport calculations during this step dfntrans adopts the lagrangian approach for calculating the path that a cluster of non reactive indivisible particles take through the discrete fracture model to calculate the path the particles take the fluid velocity field first is calculated using the pressure field result from pflotran as well as other flow attributes derived from the fracture network to calculate the velocity field pflotran was set to use a constant porosity of 25 since the goal of this work was to test whether the proposed method can tune a model with flows dominated by fracture geometry and connectivity porosity was simply set to a constant the proposed method also does not rely on a constant porosity so it can be extended to handle distributions where porosity does become a function of aperture after the velocity field is solved the particle paths can be calculated by numerically integrating the trajectory equation for each particle at the intersection of fractures it is assumed that complete mixing occurs this means when a traveling particle meets an intersection a flux weighted stochastic method is used to determine whether or not the particle stays within the fracture plane or would flow into the intersecting fracture plane because of the stochastic nature of flow through fracture intersections the general flow structure of the entire fracture network can be studied by using many tracer particles or with multiple runs of the transport simulation 2 3 genetic algorithm for fracture networks genetic algorithms refer to a class of optimization algorithms in which a family of optimal solutions is found by iteratively applying the process of natural selection in this work we show that genetic algorithms can be used to tune a dfnm where the genetic algorithm is tasked with finding the correct distribution of fractures such that it produces simulated tracer test results that match with observations the process begins with generating an initial population of potential solutions to an optimization problem each member of the population is ranked with a fitness function a fitness function is a function designed to convert the performance of single candidate solution into a single number this fitness function is used to rank members of the population candidate solutions by their ability to perform the task solutions that do a superior job of optimizing the given problem will receive a high fitness score while solutions that do a poor job solving the optimization problem are given a low fitness score below a certain fitness threshold solutions with a lower fitness score are removed from the population the remaining solutions then are used to generate a new generation of potential solutions this is done by selecting high fitness parents and mixing their genetic information to produce new children solutions with a greater chance of yielding a high performance score after repeating this process for multiple generations the population evolves into a family of high fitness solutions that effectively solve the optimization problem one important factor in a successful genetic algorithm is having a well designed method for encoding the solutions as a genetic code if done correctly the genetic mixing process can properly explore the solution space and quickly find the optimal solution within the context of discrete fracture networks this study proposes to represent each fracture as a single genetic base within the complete genetic code of a discrete fracture network model fig 1 for example a discrete fracture network with 300 fractures will have a genetic code that is 300 bases long to create new discrete fracture networks the genetic code of the child discrete fracture network will be composed of the genetic bases fractures randomly sampled from the genetic code of high fitness parent discrete fracture networks the key idea is that this method of genetic mixing preserves the distributions of the fracture parameters that means if the genetic algorithm process begins with a fracture network model population that was all generated with the same parameter distribution then after multiple generations of applying the genetic algorithm the final population will have the same fracture parameter distribution the main difference between the initial and final generation is that the final population of discrete fracture networks will be able to produce simulated flow behaviors that best match with field observations note that the genetic mixing process does not create new fractures after many iterations of the genetic algorithm the population becomes filled with copies of the same genetic code although the genetic algorithm process will converge quickly it will also have a high risk of converging prematurely and stopping at a suboptimal solution to prevent the genetic algorithm from halting at a locally optimum solution new fractures must be added to the population the new fractures must be added in a way that does not change the parameter distribution of the fractures this is done by adding newly generated fracture networks midway through the process these new fracture networks are generated using the same parameter distributions as were used to generate the initial population if these new fracture network models were simply added to the population then these models would be removed quickly from the population because of a low fitness value that could not compete with models that have already evolved to force the genetic influence of the newly added models the genetic mixing process allows the combination of genetic code between high fitness models and newly generated models by controlling the size and frequency the freshly generated models that are added to the population the user can increase the likelihood that the genetic algorithm would converge to the global optimum a complete description of the discrete fracture genetic algorithm used in this study begins with generating a population of discrete fracture network models for this study a population of 20 models was maintained each of the fracture network models was generated with the same parameter distribution for the fracture radius and fracture orientation a fitness value then was calculated for each member of the population for this study fitness was determined by how well a given fracture network could recreate the time versus concentration curves of a tracer transport test measured on a reference fracture model the individuals of the population then were ranked by their fitness value and a certain fraction of the lowest performing individuals was removed from the population to maintain the size of the population newly generated discrete fracture models then were added to the population some of these new models were generated using the same parameter distributions as the initial generation other new models were generated by randomly selecting two parent models then randomly copying the genetic code from each of the parent models to produce the genetic code for the child model the length of the genetic code of the child model the number of fractures in the child model was set randomly as a number between the number of fractures for each of the parent models for example for parent models with 180 and 200 fractures then the generated child model could have anywhere from 180 to 200 fractures newly generated models were added until the number of models was equal to the set population number as an example for a current generation of 20 models the next generation could contain ten copies of the best models from the previous generation six new models generated using genetic mixing and four models generated from sampling the same parameter distribution as used to generate the initial population the fitness of the new generation then was calculated and the entire process can be repeated multiple times to create many generations the entire loop then could be halted when the fitness of subsequent generations no longer improved a complete summary of the genetic algorithm is presented in algorithm 1 a flowchart of the process is also presented on fig 4 2 4 synthetic fracture model experiment for this study the discrete fracture genetic algorithm was tested on a synthetic fracture model in which tracer particles were injected simulated to flow through the fracture network and extracted through two wells fig 2 the domain of the fracture model was in the form of a cube with a side length of 15 m all the sides of the domain were set to a no flow boundary condition there was also no water exchange between the fractures and the matrix water only flows through the fractures water was only allowed to enter and exit the model through two wells that intersect the model domain the two wells were vertically oriented centered along the diagonal of the model and spaced 14 meters apart to create flow the wells were set with a constant pressure difference of 4 0 1 0 6 pascals to generate the discrete fracture network within the model domain the dfngen function which uses fram and lagrit of dfnworks was used in the geometric model fractures were modeled as two dimensional octagons with their radius and orientation determined by sampling defined parameter distributions the fracture radius was sampled from a truncated power law distribution using a maximum fracture radius of r u 5 0 meters a minimum fracture radius of r 0 1 0 meters and an exponent of α 2 6 that defines the shape of the power law distribution the fracture orientation was sampled using the three dimensional von mises fisher distribution with the mean orientation vector μ set to a vertically oriented normal vector and the orientation variance set to κ 1 0 the aperture of the fractures was set to a constant width of 1 0 1 0 5 meters note that a constant fracture aperture is a valid simplification for this study this work aims to test how a genetic algorithm can handle discrete fracture network models where the flow is mainly governed by how the fractures are connected to each other to focus on this mechanism all fractures were given a constant width the proposed method can handle variations in fracture aperture but this simplification is valid for initial tests of the proposed method dfngen was instructed to randomly insert 2100 fractures into the model domain this number of fractures was determined by the computational time and resources available for this work for the given model dimension and constraints this was the minimum number of fractures needed to study how the genetic algorithm would perform when tuning a discrete fracture network model the proposed technique can be scaled to handle models with a larger number of fractures this number of fractures was also chosen to ensure that any randomly generated fracture model would be likely to hydraulically connect the two wells to run the particle transport simulation the flow simulation was run first until it reached steady state afterward tracer particles were added to the model through the injection well fig 3 ten tracer particles were added to every point where a fracture intersected the wells dfntrans then calculated the trajectory of the particles as the transport simulation progressed dfntrans recorded the time it took for each particle to reach the extraction well the transport simulation ended when all particles reached the extracting well the recorded arrival times then could be used to create the breakthrough curves for the given model note that the breakthrough curve derived using arrival times of simulated particles can be used as a proxy for the cumulative molar amount of tracer recovered at the extraction well recorded over time for example recovering 25 out of 50 simulated tracer particles collected over the span of one simulated week would be the same as recovering 0 5 moles out 1 0 moles of an injected tracer compound collected over a span of a week to select the reference model one of the randomly generated fracture models was chosen as the reference model the goal of the genetic algorithm is to look for a discrete fracture model that produces the same breakthrough curves as the reference model to do this the genetic algorithm needs a fitness function that can rank how well each of the breakthrough curves matches with the breakthrough curves of the reference model this is done by first calculating the quantiles of the arrival times of the tracer particles for this study 11 quantiles were calculated 0 10 20 and so on up to 100 this converts the breakthrough curve into an 11 dimensional vector to calculate the fitness function the l2 distance is calculated between the quantile vector of the tested model and the reference model this means taking the difference between the quantile vectors for the two models summing the squares of all the elements of the new difference vector and finally taking the square root of this final sum note that this metric is an error value models that yield a low value are the best fit with the reference model and so are most likely to be kept in the population conversely models with a high value poorly match the reference model so they are most likely to be removed from the population after the fitness of each model is evaluated half of the population s worst performing models are removed from the population newly generated models then are added until the number of models in the population is the same as the starting population for this study the population was initialized with 20 models with each new generation ten models are copies of the previous generation s best models nine are newly generated from genetic mixing of models from the previous generation and one model is generated using the same parameter distribution as was used to create the initial population during model generation heavy emphasis was placed on genetic mixing because this ensures the genetic algorithm can converge quickly toward optimal solutions the genetic evolution process was repeated until no further improvement was observed from subsequent generations in this study the process continued for 40 generations for this study it took 11 days to complete the entire experiment with dfnworks running single threaded on an amd ryzen 3900x processor the runtime for this method can be significantly reduced by running multiple parallel processes when calculating the breakthrough curves for each of the candidate solutions fig 4 2 since genetic algorithms are a method that can easily take advantage of parallel processing this method can computational scale as well as other methods 3 results during the genetic algorithm loop the performance of the models within each generation was recorded fig 5 plots the distribution of the model performances within each generation of 20 models the graph includes the 10 50 and 90 quantiles of the distribution the model error value is the value from the fitness functions recall that this fitness function is based on the differences in the tracer arrival time quantiles between the tested reference model the graph shows that starting with a median model error of 3 5 successive iterations of the genetic algorithm led to a population of models with a median model error of 0 75 the population reached this value by 15 generations beyond 15 generations the population performance did not improve but instead remained at this performance level fig 6 shows that the variance of a population s model performances also evolved throughout the iterations of genetic evolution at the start the initial generated models had a model error variance of 0 4 then during the genetic algorithm process each iteration yielded a very different variance value although the variance of the model error fluctuated widely from generation to generation overall the population s variance trended downward the downward trend stopped at generation 15 the same generation that the median model error reached its steady state value after 15 generations the variance no longer decreased but instead stayed at a value of 0 09 the volatility of the model error variance also decreased substantially beyond 15 generations during the experiment the breakthrough curves for each of the models for each generation were recorded fig 7 shows the breakthrough curves recorded for the first generation the final generation and the reference model the breakthrough curves are jagged in appearance because of the relatively small number of tracer particles used such simulated breakthrough curves are expected to be more smooth with the use of more tracer particles note that for the initial population of generated models the majority of the models had an early breakthrough curve with the median of the curves having their 50 point at 2 4 1 0 3 years to test the genetic algorithm s ability to generate models with behaviors outside of the initial generated distribution note that the selected reference model has a breakthrough curve with a 50 point at 8 4 1 0 3 years after 40 generations of applying the genetic algorithm the final population of models successfully shifted right toward the breakthrough curve of the reference model after 40 generations the median of the curves had their 50 point occur at 8 5 1 0 3 years which is essentially the same as the breakthrough curve for the reference model although the final curves fit well at the 50 point the slope of the final breakthrough curves did not match well with the curves of the reference model at the particle recovery amount of 10 the reference model s breakthrough curve reached this point at 2 8 1 0 3 years yet after 40 generations the median of breakthrough curves reached this point at 4 0 1 0 3 years for the tracer recovery percentage of 90 the reference model s breakthrough curve reached this percentage at 3 7 1 0 2 years yet after 40 generations the median of breakthrough curves reached this percentage at 2 4 1 0 2 years 4 discussion overall the results show that the genetic algorithm was able to successfully evolve the population to produce discrete fracture models with tracer breakthrough curves that matched with an observed breakthrough curve fig 5 shows that the algorithm was able to reach convergence to a set of optimal solutions within 15 generations by modifying a population of 20 models note that the population model error decreased until it reaches a limit of 0 75 recall that with hundreds of fractures and with each fracture having its own set of parameters the overall discrete fracture model was heavily parameterized this means that the genetic algorithm theoretically should have the ability to produce discrete fracture models with breakthrough curves that perfectly match the curve of the reference model and achieve a model error of zero yet the algorithm only achieved a minimum of 0 75 one reason why the genetic algorithm failed to achieve a lower model error is that the simulation of particle transport was not completely deterministic if the particle transport simulation were run twice on the same discrete fracture model it would yield slightly different results the reason is that at every fracture intersection the path the particle would take is a stochastic process with its likelihoods weighted by flux because the fitness function was calculated by using the results of the particle transport simulation the fitness function adopted its stochastic value so evaluating the fitness function on the same model multiple times produced different results the uncertainty of the function limited the genetic algorithm s ability to find the most optimal model so the genetic algorithm generated a population of models that had a high likelihood of producing a good fitness score the uncertainty associated with the transport simulation can be reduced by either increasing the number of particles added to the simulation or by running the transport simulation multiple times and recording the average result another reason why model error did not reach zero is because of forcing the genetic mixing between high fitness models and models that are newly generated recall that to prevent the genetic algorithm from prematurely stopping at a local optimum genetic diversity was introduced to the population by forcing the high fitness models to genetically mix with models that were newly generated when producing new child models as a population of optimal models evolves the genetic variation of the models gets reduced and at every step genetic diversity is injected into the population from newly generated discrete fracture models at a certain point the reduction in genetic diversity caused by removing the least fit models is equal to the genetic diversity added by the newly generated models and so the genetic diversity reaches a steady state the aforementioned reasons also explain why the variance of the model error did not reach zero fig 6 the stochastic nature of the fitness function and the consistent addition of genetic diversity to the population prevented the variance of the model error from reaching zero the breakthrough curves of fig 7 also show that the genetic algorithm was able to adjust the models successfully so that they produced curves that better followed what was produced by the reference model after 40 generations the breakthrough curves shifted toward the right meaning that the tracer particles arrived at the extraction well later than earlier generations of models the genetic algorithm achieved this by adjusting populations of fracture parameters until the bulk hydraulic conductivity of the fracture network was increased this led to a lower overall flow rate and so a later arrival time for the particles fig 5 also shows that the average slope of the breakthrough curves of the final generation did not match the slope of the breakthrough curve of the reference model for the first 10 of the arriving particles the particles in the final generation models arrived later than the particles for the reference models for the last 10 percent of the arriving particles the particles in the final generation models arrived earlier than the particles for the reference model this behavior can be attributed to the uncertainty of the fitness function during the start of the genetic algorithm the fitness functions could produce an error signal that was much greater than the uncertainty of the fitness function this allowed the genetic algorithm to quickly distinguish which members of the population were high performing at this stage errors such as the temporal shift of the breakthrough curve could be fitted quickly but as evolution progressed the fitness function produced smaller error signals until finally the error signal was smaller than the noise generated by the fitness function at that stage the slopes between breakthrough curves became difficult to distinguish from each other thereby inhibiting further improvement one limitation for the results of these experiments is that there is no guarantee that the calibrated models will generalize beyond the specific placement of the test wells during the calibration process this means that if a tracer test was performed on the calibrated model with the test wells in a new orientation then the resulting breakthrough curve may be different than the breakthrough curves from applying the tracer test on the ground truth model with the same new orientation of the test wells since information about the hydraulic structure of the fractures comes solely from hydraulic tests with the wells the tracer tests essentially become blind to any regions in the fracture network that are not hydraulically connected to the wells a similar limitation applies to fracture networks which are anisotropic if tracer tests were performed on wells that were installed in an anisotropic fracture matrix in only a single orientation rotating the wells by 90 degrees would yield a different breakthrough curve to reduce the risk of discrete fracture models overfitting to the biases introduced by well placement multiple wells can be installed in multiple orientations this allows for a more comprehensive measurement of the hydraulic structure of the fracture network and helps to mitigate the limitations of wells installed in a single orientation 5 conclusion this study introduces a method for generating a population of discrete fracture models that produce simulated results that match with field observations the method can achieve this model tuning capability without changing the distribution of the fracture parameters this allows the method to not only produce models with flow characteristics that match field pumping tests but to do so in a way that creates a population of fractures that can match fracture distribution parameters observed by field seismic surveys note that this study does not use any data from a seismic survey the key idea is that if there is prior knowledge about the distribution of a fracture parameter then the proposed method can generate possible solutions that preserve this distribution this prior knowledge of parameter distributions may come from analysis of seismic surveys but can also come from other methods the method used in this study is the genetic algorithm modified in a way that can handle discrete fracture networks this was done by encoding every model s fracture as a genetic base in the model s genome during genetic mixing the child model is generated by randomly copying genetic bases from each of the parent models this process allows the fracture parameter distribution of the child model to be the same as the distribution of the parent models to test how well a genetic algorithm modified for discrete fracture networks can perform the genetic algorithm was applied to a synthetic case in which the goal was to find a population of discrete fracture models that when run with particle transport simulations can produce breakthrough curves that match the observed breakthrough curve from a reference model the results show that the genetic algorithm was able to successfully produce a population of discrete fracture models with breakthrough curves that closely match the reference model s breakthrough curve within the span of 15 generations the genetic algorithm reduced the model error and variance to a minimum that was bounded by the uncertainty of the fitness function and by the algorithm injecting genetic diversity into the population the genetic algorithm was found to excel at adjusting the fracture network s bulk hydraulic conductivity to temporally shift the breakthrough curve until it closely matched the breakthrough curve of the reference model the genetic algorithm changed the bulk hydraulic conductivity of the model by adjusting the number of fractures the orientation of the fractures the orientation of the fractures and the location of the fractures by changing these parameters the connectivity of the fractures changes thereby changing the bulk hydraulic conductivity of the model the genetic algorithm also was found to struggle with adjusting the population s breakthrough curves to match the slope of the reference breakthrough curve many issues caused by the uncertainty of the fitness function can be resolved by increasing the number of particles used in the transport simulation or by re running the simulation and using the average simulation result future work for this study includes testing the discrete fracture genetic algorithm on models with a different fracture parameter distribution for example this study involved fractures that have a relatively even distribution of orientations but there are subsurface reservoirs with fractures that are heavily biased toward one or two orientations such fracture models with multiple families of fracture orientations might change the effectiveness of the genetic algorithm another path of study is the development and testing of new fitness functions that are based on different pumping or tracer tests because this study found that the performance can be bounded by the uncertainty associated with the fitness function future work could focus on developing better fitness functions for applications with discrete fracture networks related work also could involve development of better genetic mixing strategies the current strategy randomly selects fractures to be copied over to the child model this method ignores all the other fractures connected to the copied fracture and so that connectivity information can be destroyed during the genetic mixing process future work could help develop a better genetic mixing scheme that considers the hydraulic connections made by adjacent fractures any future improvement of the discrete fracture genetic algorithm could help researchers quickly and more efficiently solve the important problem of inverse modeling of fractured subsurface reservoirs credit authorship contribution statement fleford redoloza conceptulization methodology investigation writing original draft liangping li conceptulization supervision funding acquisition arden davis writing review editing declaration of competing interest liangping li fleford redoloza and arden davis declares that they have no conflict of interest acknowledgments this work has been supported through a grant from the national science foundation united states oia 1833069 the authors wish to thank the associate editor as well as two anonymous reviewers for their comments which substantially helped to improve the final version of the manuscript 
3,reservoir management and contaminant transport simulations rely on accurate modeling of the subsurface this task becomes more challenging when the reservoir of interest also has a large amount of fractures inverse modeling of these fractured media involves first performing multiple field tests such as pumping tests tracer tests or seismic surveys inverse modeling methods then are used to find potential geologic models that produce simulated results that match field observations popular methods include the ensemble kalman filter markov chain monte carlo and simulated annealing a common challenge these methods face is that inverse modeling of fractured media is inherently a multiobjective optimization task the inverse modeling method must find a discrete fracture model that produces the same flow characteristics as observed in field pumping or tracer tests but it also must find a discrete fracture model with a fracture network that matches the fracture parameter distribution observed by the field measurements such as seismic surveys this challenge can be approached in two steps the first step is producing discrete fracture network models with parameter distributions that match field observations from seismic surveys this study focused on the second step involving the development of a method that can take a population of discrete fracture networks and generate new fracture networks in a way that preserves the fracture parameter distribution this was done using a genetic algorithm modified to apply to the domain of discrete fracture networks during genetic mixing the fractures of the child model are generated by randomly copying over fractures from the parent models this process ensures the child model adopts the fracture parameter distribution of the parent models this study tests the effectiveness of this genetic algorithm on a synthetic example the results of the experiment show the genetic algorithm is able to effectively produce a population of discrete fracture models with breakthrough curves that match the curves of the reference model keywords discrete fracture networks genetic algorithms stochastic inversion subsurface flow and transport data availability no data was used for the research described in the article 1 introduction fracture network models are important tools across multiple industries the most prominent use of fracture network models is in the petroleum industry where they are used for determining the optimal methods for fracturing and recovering petroleum from reservoirs another application for fracture network models is geothermal energy production to design effective methods for producing fractures that transfer thermal energy from the crust to the fluids flowing through the fracture network researchers rely on accurate fracture network models a wide variety of conceptual models exist for simulating flows through fractured rock concepts include modeling the rock is a discrete network of fractures another as nonuniform continuum of hydraulic parameters or even as a hybrid where the rock is modeled as a nonuniform continuum that contains a small number of dominant discrete features neuman 2005 such variety exists because of the different geologic contexts that produces a range of flow behavior this study focuses on the use discrete fracture network models to simulate flows through fractured rock this conceptual model would be suitable to simulating flows through impermeable crystalline rock unlike traditional groundwater flow simulations that discretize the domain as a grid of cells each with its hydraulic properties discrete fracture networks describe fractures as a set of finite planes with a defined shapes sizes locations and orientations the fracture parameters often are defined on the basis of a probability distribution that imitates distributions of fractures measured in the field during flow simulations the fluids strictly flow through these fracture planes with the flow behavior governed by the hydraulic properties of the fracture planes and how the fracture planes are connected note that for real fractured rock fluid exchange can exist between the fracture and the matrix for this work we assume that most of the fluid dynamics is governed by the geometry of fractures and how they are connected to each other the goal is to investigate how the complexities of geometry dominated flow affect the performance of a proposed method using genetic algorithms thanks to the flexibility of genetic algorithms the proposed method can be adjusted to handle scenarios where the fluid exchange between fractures and the rock becomes more significant even with a method for accurately simulating the flow of fluids through the fracture network researchers must still somehow convert field measurements into a set of discrete fracture network parameters such that the simulated flow model produces results that match observations from the field this task of inverse modeling is known as history matching and researchers have deployed a wide range of methods to accomplish it methods include ensemble kalman filters ping and zhang 2013 nejadi et al 2017 markov chain monte carlo optimization ma et al 2008 xu et al 2013 and simulated annealing tran 2007 mahmoodpour and masihi 2016 an important method for converting a set of well observations into a hydraulic model for fractured rock is hydraulic tomography illman et al 2009 illman 2014 zha et al 2015 klepikova et al 2020 ringel et al 2021 the process for hydraulic tomography begins by first acquiring data from pumping tests applied to a well field at the site of interest these tests must be designed in a way that properly extracts information about the hydraulic properties of the rock between the wells the tests usually involve selecting a well to pressurize then monitoring the pressure of adjacent wells through time to see how the pressure from the injection well propagates through the rock once data is collected a variety of inversion methods can be used to convert the collected data into hydraulic models for klepikova et al 2020 a misfit function was first defined to measure the differences between the simulated and observed the parameters of the model were then tuned using the nelder mead simplex algorithm mckinnon 1998 in order to find the optimal parameter values that minimizes the misfit function zha et al 2015 performed their inversion step using the simultaneous successive linear estimator algorithm xiang et al 2009 where a successive bayesian linear estimator derives the mean parameter fields using the results of field tests and assumes prior knowledge of the mean value and spatial structures of estimated parameters to perform inversion for a three dimensional discrete fracture network ringel et al 2021 used a reversible jump markov chain monte carlo method fan and sisson 2011 to calculate the likelihood a given change to a proposed model such as adding or removing a fracture yields a new model that is most likely to produce pressure transients that match with field tests note that many of these methods have the same underlying process of running multiple fluid flow simulations to find a group of model parameters that produce results that fit field observations often these simulations are large and computationally expensive so researchers use a variety of techniques and heuristics to efficiently find solutions while minimizing the required amount of computational resources another popular family of methods used by researchers includes evolutionary techniques which are optimization algorithms with heuristics that are inspired by biological processes in nature one of the most popular heuristics is the concept of natural selection possible solutions to an optimization problem are represented as individuals in a population the fitness of these individuals is determined by how well they solve the optimization problem individuals that perform poorly are assigned a low fitness value and removed from the population but individuals that perform well are assigned a high fitness value and are allowed to stay in the population after the weakest individuals have been removed from the population the remaining individuals then are allowed to reproduce and create a new generation of possible solutions during the reproduction process the child solutions adopt characteristics from each of the parent solutions the idea is that the children will adopt the beneficial characteristics of their parents ideally producing a new generation of solutions that will perform better than the previous generation evolutionary methods such as differential evolution das and suganthan 2010 pant et al 2020 and genetic algorithms mitchell 1998 sivanandam and deepa 2008 rely on this main heuristic of natural selection researchers then can use these techniques to find the optimal set of fracture network parameters that fits simulation results to field observations cadini et al 2013 liu and reynolds 2019 zhang et al 2019 maucec et al 2020 although it is important to be able to generate a set of possible fracture network parameters that produce simulation results that fit with field observations it is just as important that the distribution of the fracture parameters also match what is observed in the rock to effectively produce a set of fractures that imitate the parameter distributions in real life stochastic fracture network generation methods can be used j p 2005 bonneau et al 2013 these methods often deploy simple generating heuristics inspired by the rock mechanics involved with fracture formation and propagation these simple rules can generate fractures with parameters such as fracture shape size orientation or spatial density with the same distributions as observed in the field using techniques like well logging and micro seismics willis et al 2006 han et al 2021 kennedy et al 2022 because fracture networks involve complexity at multiple scales fractal dimensions are another popular method used to ensure that generated fracture models imitate distributions found in the field liu et al 2015 zhang et al 2019 overall researchers have many methods available for them to produce realistic and accurate distributions of fractures though seismic surveys can be used to generate a discrete fracture network sicking and malin 2019 this does not necessarily mean the generated model will produce simulated well test results that will match with all field observations such as well pressure tests instead it is best to create discrete fracture network models using results from both seismic surveys and well pressure tests mayerhofer et al 2006 for example seismic survey data can used to define the number of fractures and the density of fractures for a given study volume hydraulic tomography can then use well pressure tests to determine remaining model parameters such fracture connectivity orientations and size once discrete fracture networks can be generated to have some of the same parameter distributions measured from seismic surveys it can be challenging to also modify the generate models such that they also match the flow characteristics measured from pumping and tracer tests performed in the field when naively changing the parameters of the fracture network to better fit with measured flow characteristics this often can cause the distribution of fracture parameters to deviate from distributions measured in the field to deal with this need for a fracture model with both accurate fracture distributions and accurate flow characteristics researchers use optimization techniques that can handle multiobjective functions maucec et al 2020 instead of using techniques that iteratively generate a population of fracture models that converge toward both accurate distributions and flow characteristics at the same time this work proposes a different method that can meet this multiobjective goal in two distinct and straightforward steps the first step involves establishing a method for readily generating a set of fracture models with fracture parameter distributions that match field observations this step can be done by simply using any of the popular methods in the literature such as stochastic generation or using fractal dimensions the second critical step is to use the population of generated fracture networks to create new fracture models that are more likely to match flow characteristics but in a way that preserves the fracture distributions of the previously generated fracture network models this is done by creating a genetic algorithm optimization architecture that is designed to preserve the fracture parameter distribution of the population with each generation this work includes a synthetic experiment to investigate the effectiveness of this technique 2 methodology 2 1 discrete fracture network to model the discrete fracture networks the software package dfnworks by the los alamos national laboratory was used hyman et al 2015 dfnworks is composed of multiple software packages combined to form a seamless software suite for generating three dimensional models of discrete fracture networks as well as simulating flow and transport of particles through these fracture networks to generate a discrete fracture network dfnworks first uses a feature rejection algorithm for meshing fram method to generate the underlying geometric model for the discrete fracture network at this stage fractures are represented as flat polygons of various sizes and are distributed across a three dimensional volume each of the polygons is assigned additional attributes such as aperture size to generate a fracture network the distribution of the fracture network parameters are first defined once defined the distributions then are sampled and the fracture network is iteratively built one fracture at a time after each fracture is added the fram method checks if a series of constraints are met constraints such as proximity to other fractures or proximity to the model domain boundary are considered if the added fracture fails to meet a criterion then the fracture is rejected and a new one is created otherwise the fracture is kept within the fracture model and the process continues when the growing fracture model achieves a certain stopping criterion such as a set number of fractures the iteration loop halts and the geometric fracture model is presented as the final model ready for the next step of the process after the geometric fracture model is created the lagrit meshing tool converts the geometric model into a mesh model the meshing process produces a delaunay triangulation mesh that is ideal for parallel computations during the fracture generation phase the user has the freedom to define a probability distribution from which the software can sample to generate the fractures for fracture radius dfnworks provides options for a log normal distribution an exponential distribution a truncated power law distribution or simply a constant for this study the truncated power law distribution was used 1 p r α r 0 r r 0 1 α 1 r u r 0 α in this equation the probability density function p r is parameterized with the maximum fracture radius r u the minimum fracture radius r 0 and an exponent α that defines the shape of the power law distribution the truncated power law distribution was used because its additional parameters allow greater control on the shape of the distribution that controls the fracture radius di federico and neuman 1997 di federico et al 1999 neuman 2008 but the proposed method does not rely on a specific distribution to function so the approach will still work when an alternate fracture distribution function is used for the fracture orientation the orientation vector normal to the fracture plane must be sampled by essentially using a gaussian distribution that is embedded onto the surface of a three dimensional sphere this was done by using the three dimensional von mises fisher distribution 2 f x μ κ κ e x p κ μ t x 4 π sinh κ in this equation the probability distribution function f x is parameterized by μ and κ μ is the vector for the mean orientation with t indicating that the vector is transposed κ is the parameter that represents the variance of the distribution around the mean orientation dfnworks samples this distribution by using the algorithm outlined by wood 1994 2 2 flow and transport simulation the dfnworks suite hyman et al 2015 also includes software for running flow and transport simulations using the discrete fracture network called dfnflow the software takes the meshed fracture network model and uses pflotran to compute the pressure field across the network pflotran lichtner et al 2015 is an open source code base that was written by core developers from the u s department of national laboratories such as the los alamos national laboratory sandia national laboratory lawrence berkeley national laboratory and oak ridge national laboratory as well as contributions from universities and research labs around the world pflotran is capable of massively parallel operations that can operate at multiple scales and physics pflotran also can solve differential equations for non isothermal multiphase flow reactive transport and geomechanics in porous media this study uses pflotran to solve for the single phase flow across the fractures during steady state pflotran does this by solving the three dimensional richards equation richards 1931 the mixed form of the richards equation proposed by celia et al 1990 yields robust numerical solutions and maintains mass balance for unsaturated flow problems wu et al 2021 3 t ϕ s η η q q w in this equation t is time ϕ is the porosity of the soil matrix s is the water saturation η is the molar water density q is the darcy flux and q w is a positive source or a negative sink of water the darcy flux q is calculated using darcy s law 4 q k k r μ p w w η g z where k is the intrinsic permeability k r is the relative permeability μ is the water viscosity p is the pressure w w is the formula weight of water g is the acceleration of gravity and z is the vertical component of the position vector in this study the van genuchten soil water retention curve van genuchten 1980 and the mualem relative permeability function mualem 1976 was used to calculate the water saturation and the relative permeability for this study the boundary conditions for the model will be a no flow boundary condition for all boundaries except for the injection and extraction well the two wells will be set to a boundary condition of a constant pressure the permeability of the fractures are derived as a function of the aperture of the fracture this is done by first assuming the fluid flows between two smooth impermeable parallel plates this assumption allows the boussinesq equation boussinesq 1868 to yield the volumetric flow rate q per unit fracture width normal to the direction flow hyman et al 2016 5 q b 3 ρ g 12 ν h in this equation b is the fracture aperture ρ is the fluid density g is gravitational acceleration and h is hydraulic head this relationship between fracture aperture and flow rate can be used to derive a relationship between aperture and transmissivity hyman et al 2016 6 t b 3 ρ g 12 ν this equation is referred to as the cubic law witherspoon et al 1980 pflotran uses this relationship to convert fracture aperture into permeability note that these equations assume that the flow through the fracture is homogeneous and that there is no fluid exchange between the fracture and the matrix such an assumption can be justified when modeling flow through impermeable crystalline rock for this study focus is on investigating whether the genetic algorithm is capable of tuning a model with its flow dynamics dominated by the geometry and connectivity of the fractures however the methods developed in this work can be extended to study sites where there is significant fluid exchange between the fracture and the matrix after the flow and pressure field is calculated this information is sent to the software called dfntrans for particle transport calculations during this step dfntrans adopts the lagrangian approach for calculating the path that a cluster of non reactive indivisible particles take through the discrete fracture model to calculate the path the particles take the fluid velocity field first is calculated using the pressure field result from pflotran as well as other flow attributes derived from the fracture network to calculate the velocity field pflotran was set to use a constant porosity of 25 since the goal of this work was to test whether the proposed method can tune a model with flows dominated by fracture geometry and connectivity porosity was simply set to a constant the proposed method also does not rely on a constant porosity so it can be extended to handle distributions where porosity does become a function of aperture after the velocity field is solved the particle paths can be calculated by numerically integrating the trajectory equation for each particle at the intersection of fractures it is assumed that complete mixing occurs this means when a traveling particle meets an intersection a flux weighted stochastic method is used to determine whether or not the particle stays within the fracture plane or would flow into the intersecting fracture plane because of the stochastic nature of flow through fracture intersections the general flow structure of the entire fracture network can be studied by using many tracer particles or with multiple runs of the transport simulation 2 3 genetic algorithm for fracture networks genetic algorithms refer to a class of optimization algorithms in which a family of optimal solutions is found by iteratively applying the process of natural selection in this work we show that genetic algorithms can be used to tune a dfnm where the genetic algorithm is tasked with finding the correct distribution of fractures such that it produces simulated tracer test results that match with observations the process begins with generating an initial population of potential solutions to an optimization problem each member of the population is ranked with a fitness function a fitness function is a function designed to convert the performance of single candidate solution into a single number this fitness function is used to rank members of the population candidate solutions by their ability to perform the task solutions that do a superior job of optimizing the given problem will receive a high fitness score while solutions that do a poor job solving the optimization problem are given a low fitness score below a certain fitness threshold solutions with a lower fitness score are removed from the population the remaining solutions then are used to generate a new generation of potential solutions this is done by selecting high fitness parents and mixing their genetic information to produce new children solutions with a greater chance of yielding a high performance score after repeating this process for multiple generations the population evolves into a family of high fitness solutions that effectively solve the optimization problem one important factor in a successful genetic algorithm is having a well designed method for encoding the solutions as a genetic code if done correctly the genetic mixing process can properly explore the solution space and quickly find the optimal solution within the context of discrete fracture networks this study proposes to represent each fracture as a single genetic base within the complete genetic code of a discrete fracture network model fig 1 for example a discrete fracture network with 300 fractures will have a genetic code that is 300 bases long to create new discrete fracture networks the genetic code of the child discrete fracture network will be composed of the genetic bases fractures randomly sampled from the genetic code of high fitness parent discrete fracture networks the key idea is that this method of genetic mixing preserves the distributions of the fracture parameters that means if the genetic algorithm process begins with a fracture network model population that was all generated with the same parameter distribution then after multiple generations of applying the genetic algorithm the final population will have the same fracture parameter distribution the main difference between the initial and final generation is that the final population of discrete fracture networks will be able to produce simulated flow behaviors that best match with field observations note that the genetic mixing process does not create new fractures after many iterations of the genetic algorithm the population becomes filled with copies of the same genetic code although the genetic algorithm process will converge quickly it will also have a high risk of converging prematurely and stopping at a suboptimal solution to prevent the genetic algorithm from halting at a locally optimum solution new fractures must be added to the population the new fractures must be added in a way that does not change the parameter distribution of the fractures this is done by adding newly generated fracture networks midway through the process these new fracture networks are generated using the same parameter distributions as were used to generate the initial population if these new fracture network models were simply added to the population then these models would be removed quickly from the population because of a low fitness value that could not compete with models that have already evolved to force the genetic influence of the newly added models the genetic mixing process allows the combination of genetic code between high fitness models and newly generated models by controlling the size and frequency the freshly generated models that are added to the population the user can increase the likelihood that the genetic algorithm would converge to the global optimum a complete description of the discrete fracture genetic algorithm used in this study begins with generating a population of discrete fracture network models for this study a population of 20 models was maintained each of the fracture network models was generated with the same parameter distribution for the fracture radius and fracture orientation a fitness value then was calculated for each member of the population for this study fitness was determined by how well a given fracture network could recreate the time versus concentration curves of a tracer transport test measured on a reference fracture model the individuals of the population then were ranked by their fitness value and a certain fraction of the lowest performing individuals was removed from the population to maintain the size of the population newly generated discrete fracture models then were added to the population some of these new models were generated using the same parameter distributions as the initial generation other new models were generated by randomly selecting two parent models then randomly copying the genetic code from each of the parent models to produce the genetic code for the child model the length of the genetic code of the child model the number of fractures in the child model was set randomly as a number between the number of fractures for each of the parent models for example for parent models with 180 and 200 fractures then the generated child model could have anywhere from 180 to 200 fractures newly generated models were added until the number of models was equal to the set population number as an example for a current generation of 20 models the next generation could contain ten copies of the best models from the previous generation six new models generated using genetic mixing and four models generated from sampling the same parameter distribution as used to generate the initial population the fitness of the new generation then was calculated and the entire process can be repeated multiple times to create many generations the entire loop then could be halted when the fitness of subsequent generations no longer improved a complete summary of the genetic algorithm is presented in algorithm 1 a flowchart of the process is also presented on fig 4 2 4 synthetic fracture model experiment for this study the discrete fracture genetic algorithm was tested on a synthetic fracture model in which tracer particles were injected simulated to flow through the fracture network and extracted through two wells fig 2 the domain of the fracture model was in the form of a cube with a side length of 15 m all the sides of the domain were set to a no flow boundary condition there was also no water exchange between the fractures and the matrix water only flows through the fractures water was only allowed to enter and exit the model through two wells that intersect the model domain the two wells were vertically oriented centered along the diagonal of the model and spaced 14 meters apart to create flow the wells were set with a constant pressure difference of 4 0 1 0 6 pascals to generate the discrete fracture network within the model domain the dfngen function which uses fram and lagrit of dfnworks was used in the geometric model fractures were modeled as two dimensional octagons with their radius and orientation determined by sampling defined parameter distributions the fracture radius was sampled from a truncated power law distribution using a maximum fracture radius of r u 5 0 meters a minimum fracture radius of r 0 1 0 meters and an exponent of α 2 6 that defines the shape of the power law distribution the fracture orientation was sampled using the three dimensional von mises fisher distribution with the mean orientation vector μ set to a vertically oriented normal vector and the orientation variance set to κ 1 0 the aperture of the fractures was set to a constant width of 1 0 1 0 5 meters note that a constant fracture aperture is a valid simplification for this study this work aims to test how a genetic algorithm can handle discrete fracture network models where the flow is mainly governed by how the fractures are connected to each other to focus on this mechanism all fractures were given a constant width the proposed method can handle variations in fracture aperture but this simplification is valid for initial tests of the proposed method dfngen was instructed to randomly insert 2100 fractures into the model domain this number of fractures was determined by the computational time and resources available for this work for the given model dimension and constraints this was the minimum number of fractures needed to study how the genetic algorithm would perform when tuning a discrete fracture network model the proposed technique can be scaled to handle models with a larger number of fractures this number of fractures was also chosen to ensure that any randomly generated fracture model would be likely to hydraulically connect the two wells to run the particle transport simulation the flow simulation was run first until it reached steady state afterward tracer particles were added to the model through the injection well fig 3 ten tracer particles were added to every point where a fracture intersected the wells dfntrans then calculated the trajectory of the particles as the transport simulation progressed dfntrans recorded the time it took for each particle to reach the extraction well the transport simulation ended when all particles reached the extracting well the recorded arrival times then could be used to create the breakthrough curves for the given model note that the breakthrough curve derived using arrival times of simulated particles can be used as a proxy for the cumulative molar amount of tracer recovered at the extraction well recorded over time for example recovering 25 out of 50 simulated tracer particles collected over the span of one simulated week would be the same as recovering 0 5 moles out 1 0 moles of an injected tracer compound collected over a span of a week to select the reference model one of the randomly generated fracture models was chosen as the reference model the goal of the genetic algorithm is to look for a discrete fracture model that produces the same breakthrough curves as the reference model to do this the genetic algorithm needs a fitness function that can rank how well each of the breakthrough curves matches with the breakthrough curves of the reference model this is done by first calculating the quantiles of the arrival times of the tracer particles for this study 11 quantiles were calculated 0 10 20 and so on up to 100 this converts the breakthrough curve into an 11 dimensional vector to calculate the fitness function the l2 distance is calculated between the quantile vector of the tested model and the reference model this means taking the difference between the quantile vectors for the two models summing the squares of all the elements of the new difference vector and finally taking the square root of this final sum note that this metric is an error value models that yield a low value are the best fit with the reference model and so are most likely to be kept in the population conversely models with a high value poorly match the reference model so they are most likely to be removed from the population after the fitness of each model is evaluated half of the population s worst performing models are removed from the population newly generated models then are added until the number of models in the population is the same as the starting population for this study the population was initialized with 20 models with each new generation ten models are copies of the previous generation s best models nine are newly generated from genetic mixing of models from the previous generation and one model is generated using the same parameter distribution as was used to create the initial population during model generation heavy emphasis was placed on genetic mixing because this ensures the genetic algorithm can converge quickly toward optimal solutions the genetic evolution process was repeated until no further improvement was observed from subsequent generations in this study the process continued for 40 generations for this study it took 11 days to complete the entire experiment with dfnworks running single threaded on an amd ryzen 3900x processor the runtime for this method can be significantly reduced by running multiple parallel processes when calculating the breakthrough curves for each of the candidate solutions fig 4 2 since genetic algorithms are a method that can easily take advantage of parallel processing this method can computational scale as well as other methods 3 results during the genetic algorithm loop the performance of the models within each generation was recorded fig 5 plots the distribution of the model performances within each generation of 20 models the graph includes the 10 50 and 90 quantiles of the distribution the model error value is the value from the fitness functions recall that this fitness function is based on the differences in the tracer arrival time quantiles between the tested reference model the graph shows that starting with a median model error of 3 5 successive iterations of the genetic algorithm led to a population of models with a median model error of 0 75 the population reached this value by 15 generations beyond 15 generations the population performance did not improve but instead remained at this performance level fig 6 shows that the variance of a population s model performances also evolved throughout the iterations of genetic evolution at the start the initial generated models had a model error variance of 0 4 then during the genetic algorithm process each iteration yielded a very different variance value although the variance of the model error fluctuated widely from generation to generation overall the population s variance trended downward the downward trend stopped at generation 15 the same generation that the median model error reached its steady state value after 15 generations the variance no longer decreased but instead stayed at a value of 0 09 the volatility of the model error variance also decreased substantially beyond 15 generations during the experiment the breakthrough curves for each of the models for each generation were recorded fig 7 shows the breakthrough curves recorded for the first generation the final generation and the reference model the breakthrough curves are jagged in appearance because of the relatively small number of tracer particles used such simulated breakthrough curves are expected to be more smooth with the use of more tracer particles note that for the initial population of generated models the majority of the models had an early breakthrough curve with the median of the curves having their 50 point at 2 4 1 0 3 years to test the genetic algorithm s ability to generate models with behaviors outside of the initial generated distribution note that the selected reference model has a breakthrough curve with a 50 point at 8 4 1 0 3 years after 40 generations of applying the genetic algorithm the final population of models successfully shifted right toward the breakthrough curve of the reference model after 40 generations the median of the curves had their 50 point occur at 8 5 1 0 3 years which is essentially the same as the breakthrough curve for the reference model although the final curves fit well at the 50 point the slope of the final breakthrough curves did not match well with the curves of the reference model at the particle recovery amount of 10 the reference model s breakthrough curve reached this point at 2 8 1 0 3 years yet after 40 generations the median of breakthrough curves reached this point at 4 0 1 0 3 years for the tracer recovery percentage of 90 the reference model s breakthrough curve reached this percentage at 3 7 1 0 2 years yet after 40 generations the median of breakthrough curves reached this percentage at 2 4 1 0 2 years 4 discussion overall the results show that the genetic algorithm was able to successfully evolve the population to produce discrete fracture models with tracer breakthrough curves that matched with an observed breakthrough curve fig 5 shows that the algorithm was able to reach convergence to a set of optimal solutions within 15 generations by modifying a population of 20 models note that the population model error decreased until it reaches a limit of 0 75 recall that with hundreds of fractures and with each fracture having its own set of parameters the overall discrete fracture model was heavily parameterized this means that the genetic algorithm theoretically should have the ability to produce discrete fracture models with breakthrough curves that perfectly match the curve of the reference model and achieve a model error of zero yet the algorithm only achieved a minimum of 0 75 one reason why the genetic algorithm failed to achieve a lower model error is that the simulation of particle transport was not completely deterministic if the particle transport simulation were run twice on the same discrete fracture model it would yield slightly different results the reason is that at every fracture intersection the path the particle would take is a stochastic process with its likelihoods weighted by flux because the fitness function was calculated by using the results of the particle transport simulation the fitness function adopted its stochastic value so evaluating the fitness function on the same model multiple times produced different results the uncertainty of the function limited the genetic algorithm s ability to find the most optimal model so the genetic algorithm generated a population of models that had a high likelihood of producing a good fitness score the uncertainty associated with the transport simulation can be reduced by either increasing the number of particles added to the simulation or by running the transport simulation multiple times and recording the average result another reason why model error did not reach zero is because of forcing the genetic mixing between high fitness models and models that are newly generated recall that to prevent the genetic algorithm from prematurely stopping at a local optimum genetic diversity was introduced to the population by forcing the high fitness models to genetically mix with models that were newly generated when producing new child models as a population of optimal models evolves the genetic variation of the models gets reduced and at every step genetic diversity is injected into the population from newly generated discrete fracture models at a certain point the reduction in genetic diversity caused by removing the least fit models is equal to the genetic diversity added by the newly generated models and so the genetic diversity reaches a steady state the aforementioned reasons also explain why the variance of the model error did not reach zero fig 6 the stochastic nature of the fitness function and the consistent addition of genetic diversity to the population prevented the variance of the model error from reaching zero the breakthrough curves of fig 7 also show that the genetic algorithm was able to adjust the models successfully so that they produced curves that better followed what was produced by the reference model after 40 generations the breakthrough curves shifted toward the right meaning that the tracer particles arrived at the extraction well later than earlier generations of models the genetic algorithm achieved this by adjusting populations of fracture parameters until the bulk hydraulic conductivity of the fracture network was increased this led to a lower overall flow rate and so a later arrival time for the particles fig 5 also shows that the average slope of the breakthrough curves of the final generation did not match the slope of the breakthrough curve of the reference model for the first 10 of the arriving particles the particles in the final generation models arrived later than the particles for the reference models for the last 10 percent of the arriving particles the particles in the final generation models arrived earlier than the particles for the reference model this behavior can be attributed to the uncertainty of the fitness function during the start of the genetic algorithm the fitness functions could produce an error signal that was much greater than the uncertainty of the fitness function this allowed the genetic algorithm to quickly distinguish which members of the population were high performing at this stage errors such as the temporal shift of the breakthrough curve could be fitted quickly but as evolution progressed the fitness function produced smaller error signals until finally the error signal was smaller than the noise generated by the fitness function at that stage the slopes between breakthrough curves became difficult to distinguish from each other thereby inhibiting further improvement one limitation for the results of these experiments is that there is no guarantee that the calibrated models will generalize beyond the specific placement of the test wells during the calibration process this means that if a tracer test was performed on the calibrated model with the test wells in a new orientation then the resulting breakthrough curve may be different than the breakthrough curves from applying the tracer test on the ground truth model with the same new orientation of the test wells since information about the hydraulic structure of the fractures comes solely from hydraulic tests with the wells the tracer tests essentially become blind to any regions in the fracture network that are not hydraulically connected to the wells a similar limitation applies to fracture networks which are anisotropic if tracer tests were performed on wells that were installed in an anisotropic fracture matrix in only a single orientation rotating the wells by 90 degrees would yield a different breakthrough curve to reduce the risk of discrete fracture models overfitting to the biases introduced by well placement multiple wells can be installed in multiple orientations this allows for a more comprehensive measurement of the hydraulic structure of the fracture network and helps to mitigate the limitations of wells installed in a single orientation 5 conclusion this study introduces a method for generating a population of discrete fracture models that produce simulated results that match with field observations the method can achieve this model tuning capability without changing the distribution of the fracture parameters this allows the method to not only produce models with flow characteristics that match field pumping tests but to do so in a way that creates a population of fractures that can match fracture distribution parameters observed by field seismic surveys note that this study does not use any data from a seismic survey the key idea is that if there is prior knowledge about the distribution of a fracture parameter then the proposed method can generate possible solutions that preserve this distribution this prior knowledge of parameter distributions may come from analysis of seismic surveys but can also come from other methods the method used in this study is the genetic algorithm modified in a way that can handle discrete fracture networks this was done by encoding every model s fracture as a genetic base in the model s genome during genetic mixing the child model is generated by randomly copying genetic bases from each of the parent models this process allows the fracture parameter distribution of the child model to be the same as the distribution of the parent models to test how well a genetic algorithm modified for discrete fracture networks can perform the genetic algorithm was applied to a synthetic case in which the goal was to find a population of discrete fracture models that when run with particle transport simulations can produce breakthrough curves that match the observed breakthrough curve from a reference model the results show that the genetic algorithm was able to successfully produce a population of discrete fracture models with breakthrough curves that closely match the reference model s breakthrough curve within the span of 15 generations the genetic algorithm reduced the model error and variance to a minimum that was bounded by the uncertainty of the fitness function and by the algorithm injecting genetic diversity into the population the genetic algorithm was found to excel at adjusting the fracture network s bulk hydraulic conductivity to temporally shift the breakthrough curve until it closely matched the breakthrough curve of the reference model the genetic algorithm changed the bulk hydraulic conductivity of the model by adjusting the number of fractures the orientation of the fractures the orientation of the fractures and the location of the fractures by changing these parameters the connectivity of the fractures changes thereby changing the bulk hydraulic conductivity of the model the genetic algorithm also was found to struggle with adjusting the population s breakthrough curves to match the slope of the reference breakthrough curve many issues caused by the uncertainty of the fitness function can be resolved by increasing the number of particles used in the transport simulation or by re running the simulation and using the average simulation result future work for this study includes testing the discrete fracture genetic algorithm on models with a different fracture parameter distribution for example this study involved fractures that have a relatively even distribution of orientations but there are subsurface reservoirs with fractures that are heavily biased toward one or two orientations such fracture models with multiple families of fracture orientations might change the effectiveness of the genetic algorithm another path of study is the development and testing of new fitness functions that are based on different pumping or tracer tests because this study found that the performance can be bounded by the uncertainty associated with the fitness function future work could focus on developing better fitness functions for applications with discrete fracture networks related work also could involve development of better genetic mixing strategies the current strategy randomly selects fractures to be copied over to the child model this method ignores all the other fractures connected to the copied fracture and so that connectivity information can be destroyed during the genetic mixing process future work could help develop a better genetic mixing scheme that considers the hydraulic connections made by adjacent fractures any future improvement of the discrete fracture genetic algorithm could help researchers quickly and more efficiently solve the important problem of inverse modeling of fractured subsurface reservoirs credit authorship contribution statement fleford redoloza conceptulization methodology investigation writing original draft liangping li conceptulization supervision funding acquisition arden davis writing review editing declaration of competing interest liangping li fleford redoloza and arden davis declares that they have no conflict of interest acknowledgments this work has been supported through a grant from the national science foundation united states oia 1833069 the authors wish to thank the associate editor as well as two anonymous reviewers for their comments which substantially helped to improve the final version of the manuscript 
4,this study introduces firm information gain for model discrimination based on shannon entropy and worst case scenario experimental design firm information gain is the minimal additional information gained by an experimental design with respect to existing information robust experimental design aims to maximize the firm information gain by searching for the least number of new pumping wells and observation wells robust experimental design includes a bayes factor threshold to ensure that new data provide strong evidence for model discrimination to maximize the firm information gain a framework is proposed that combines the parallel sequential genetic algorithm ga for parallel computing and the nested quadrature rule for efficiently solving multidimensional integrals the numerical experiment involves the true model for the purpose of verification the results show that using a full covariance matrix is imperative to avoid exaggerating firm information gain collecting new groundwater data is prioritized over exploring additional pumping wells maximizing firm information gain is able to identify the same and true model keywords robust experimental design entropy information theory model discrimination uncertainty data availability data will be made available on request 1 introduction groundwater is a crucial source of freshwater throughout the world for both hydrologic and human systems alley et al 2002 giordano 2009 siebert et al 2010 groundwater modeling has been widely used for decades as essential tools for the planning and management of groundwater resources gleeson et al 2012 wada et al 2010 however developing a groundwater model has never been an easy task as groundwater data is always sparse and uncertainty always exists multiple conceptualizations of a groundwater system are often investigated yet considering too many conceptual models indicates high model prediction uncertainty and may lose the purpose of model development bredehoeft 2005 højberg and refsgaard 2005 collecting and incorporating new data into groundwater models helps advance conceptual understanding and management of groundwater resources kikuchi 2017 and in turn reduces the number of models nevertheless collecting groundwater data is usually costly and optimal experimental design techniques are often conducted before data collection to gain the maximum amount of information given a pre defined monitoring objective according to sun 1994 experimental design in groundwater modeling generally falls into two parts the observation part e g state variables to be observed the number and locations of observation wells and observation frequency and the excitation part e g the number and locations of extraction and injection wells pumping and injection rates and periods of extraction and injection if the excitation part is predetermined and only the observation part is considered the experimental design is referred to as an observation network design observation network designs have been studied extensively in the literature a variety of methodologies have been introduced to design a groundwater observation network kollat et al 2011 loaiciga et al 1992 mogheir et al 2006 among these methods physically based simulation approaches cieniawski et al 1995 cleveland and yeh 1990 dhar and datta 2007 hudak and loaiciga 1992 mckinney and loucks 1992 meyer et al 1994 reed et al 2000 storck et al 1997 and information theory entropy based method alfonso et al 2010 mogheir et al 2006 mogheir and singh 2002 nowak and guthke 2016 poeter and anderson 2005 are commonly employed owing to their flexibility in examining design scenarios and design constraints the objectives of observation network designs are usually to 1 improve parameter estimation altmann dieses et al 2002 chang et al 2005 cleveland and yeh 1990 herrera and pinder 2005 hsu and yeh 1989 sciortino et al 2002 siade et al 2017 sun and yeh 2007 ushijima and yeh 2015 2 minimize prediction uncertainty chadalavada and datta 2008 janssen et al 2008 mckinney and loucks 1992 nowak et al 2010 wagner 1995 wöhling et al 2016 3 detect plumes bode et al 2019 dhar and datta 2007 dokou and pinder 2009 kim and lee 2007 leube et al 2012 meyer and brill 1988 storck et al 1997 and 4 to discriminate among candidate models and identify the most probable model kikuchi et al 2015 knopman and voss 1988 pham and tsai 2016 2015 usunoff et al 1992 yakirevich et al 2013 readers are referred to several in depth review articles hassan 2003 kollat et al 2011 loaiciga et al 1992 minsker 2003 to achieve the objective of model discrimination observation networks aim to provide the most useful information with respect to model discrimination several criteria have been developed for model discrimination in optimal observation network designs based on the maximum differences between model predictions knopman et al 1991 knopman and voss 1988 nordqvist and voss 1996 usunoff et al 1992 the maximum kullback leibler information kikuchi et al 2015 nowak and guthke 2016 yakirevich et al 2013 the maximum change in entropy box and hill 1967 alfonso et al 2010 and the maximation of posterior model probability pham and tsai 2016 2015 the basic concept underlying all these criteria is to sample the state variable s at spatiotemporal locations i e predicted data where the variance among the ensemble of proposed competing model s predictions is maximized the worth of new data has been analyzed in various water related problems such as prediction uncertainty reduction dausman et al 2010 feyen and gorelick 2005 freer et al 1996 gates and kisiel 1974 rojas et al 2010 sohn and small 2000 tiedeman et al 2004 2003 yokota and thompson 2004 model selection wöhling et al 2015 decision making ben zvi et al 1988 davis and dvoranchik 1971 james et al 1996 reichard and evans 1989 and cost effectiveness james and gorelick 1994 neuman et al 2012 norberg and rosén 2006 wagner 1999 though optimal observation network designs have been studied extensively in the past there is still a lack of clear understanding of the amount and the worth of new data required to justify a certain level of model discrimination and identify the most probable model besides none of these studies could guarantee identifying the same most probable model moreover all these studies only considered the observation part of the experimental design in this study we introduce a robust experimental design for model discrimination based on the shannon entropy shannon 1948 and the worst case scenario experimental design sun and yeh 2007 first we introduce a firm information gain concept and derive a new model discrimination criterion based on the shannon entropy and the bayes factor the firm information is defined to be the minimum information guaranteed from an experimental design the crux of experimental design based on firm information gain is that the design objective can be achieved with the least information as a result any other experimental designs under the same experimental conditions will result in higher information gain and therefore guarantee the same design outcome according to sun and yeh 2007 an experimental design is considered robust if it accounts for both the excitation part pumping activities and observation part observation activities to maximize the firm information gain this is achieved through a max min optimization problem to improve model discrimination while using the fewest possible pumping and observation wells in the context of robust experimental design robust means that the optimized pumping well network performs well across all possible observation well networks as it performs well even with the worst case observation well network we hypothesize that the same most probable model can be identified from a pool of competing models by maximizing the firm information gain for a system e g a set of conceptual groundwater models that differ in boundary conditions geological structures etc and satisfying a bayes factor threshold any other experimental design solutions having information negative shannon entropy higher than the firm information will result in the same most probable model second we introduce a parallel computing framework that combines the parallel sequential ga carroll 1996 and the nested quadrature rule genz and keister 1996 to efficiently solve the time consuming max min optimization problem finally we test the proposed framework and conduct the robust experimental design on a hypothetical numerical example where nine competing groundwater models were generated and a robust experimental design is needed to discriminate among the models and identify the same most probable model the robust experimental design in this study is different from that in box and hill 1967 and pham and tsai 2016 first this study considers measurement errors and data correlation in the experimental design second a new model discrimination criterion is introduced that maximizes firm information gain to obtain the robust experimental design instead of finding an upper bound of the expected information gain usually referred to the box hill discrimination function 2 methodology 2 1 shannon entropy and expected information gain shannon entropy shannon 1948 provides a measure of the information value of a system using probabilities of the occurrence of events in the system consider that a set of m candidate models m m1 m2 m m represents the events of the system e g candidate models are groundwater models that differ in model conceptualizations such as boundary conditions geological structures and parameter structures their posterior model probabilities are pr m i δ obs given existing observation data δ obs the shannon entropy of the system is 1 s m δ obs i 1 m pr m i δ obs lnpr m i δ obs where lnpr m i δ obs is the information of the model m i negative entropy s represents the average amount of information i provided by all candidate models 2 i m δ obs s m δ obs the least information corresponds to the maximum entropy when all models have an equal posterior model probability the maximum information from the system corresponds to the minimum entropy when one model has a 100 posterior model probability and other models have zero posterior model probability the main purpose of an experimental design d for model discrimination is to maximize information gain through acquiring new data such that the most probable model can be identified from a pool of candidate models of the system the information gain is defined as follows 3 i g i m δ d new i m δ obs where i g is the information gain after an experimental design δ d new r n is a vector of n new data and i m δ d new represents the combined information obtained from both the new and existing data new data are unknown and uncertain before sampling this study proposes an expected information gain of the new data for the experimental design 4 i g e i m δ d new i m δ obs where i g is the expected information gain and e is the expectation operator the expected new information under a probability distribution function of new data δ d new is 5 e i m δ d new i 1 m pr m i δ d new ln pr m i δ d new q δ d new d δ d new where pr m i δ d new are the posterior model probabilities given new data δ d new and q δ d new is the averaged probability density function of new data δ d new via bayesian model averaging 6 q δ d new j 1 m pr m j δ obs p δ d new m j where p δ d new m j is the probability density function of predicted new data δ d new using the model m j inserting eqs 2 and 5 into the eq 4 appendix a shows the expected information gain as follows 7 i g i 1 m pr m i δ obs p δ d new m i ln p δ d new m i q δ d new d δ d new the integral in the eq 7 is the kullback leibler kl divergence that measures the difference between the bma weighted probability distribution q δ d new and the probability distribution p δ d new m i maximizing i g enhances the diversity of probability distributions of each model s prediction compared to the bma weighted probability distribution the expected information gain is the averaged kl divergence weighted by pr m i δ obs because the kl divergence is always non negative the expected information gain is always non negative to solve eq 7 we need to know the probability density function p δ d new m j of predicted new data δ d new considering that new data are correlated and multivariate gaussian the probability density function p δ d new m i is 8 p δ d new m i 2 π n 2 σ i 1 2 e 1 2 δ d new δ i t σ i 1 δ d new δ i where δ i are the expected values of new data estimated by the model m i σ i σ σ ε is the total covariance matrix of new data involving the use of the model m i which is the sum of the covariance matrix of the estimated new data representing parameter and model structure uncertainties and the covariance matrix of measurement errors in new data this study considers correlated data which results in a full covariance matrix appendix b further expands the eq 7 with the multivariate gaussian distribution as follows 9 i g i 1 m pr m i δ obs ln 2 π n 2 σ i 1 2 n 2 e δ d new m i ln q δ d new e δ d new m i ln q δ d new in the eq 9 is the expectation of ln q δ d new under randomness of δ d new given model m i which is 10 e δ d new m i ln q δ d new p δ d new m i ln q δ d new d δ d new for one dimensional integral i e only one new observation is collected gaussian quadrature rules and monte carlo methods are powerful however when the new data are in high dimensions and correlated these approaches become impractical due to prohibitive computing costs rising exponentially with the number of dimensions and there is no analytical solution for the eq 10 as far as authors knowledge it is noted that the method presented in this study is not limited to the gaussian distribution of the predicted new data the general form of the expected information gain is in eq 7 as soon as one knows the probability density function p δ d new m j eq 7 can be solved assuming the gaussian distribution is for the convenience purpose that the general form of i g in eq 7 is reduced to a simplifier form in eqs 9 and 10 genz and keister 1996 presented a nested quadrature rule to efficiently calculate high dimensional integrals for the multivariate normal distribution with zero means and an identity matrix unfortunately the integral in eq 10 was for multivariate gaussian with a non zero mean and a full covariance matrix therefore this study adopted the cholesky decomposition to transform e δ d new m i ln q δ d new into a multivariate normal distribution with zero means and an identity matrix i e the covariance matrix is an identity matrix in which all the diagonal elements are ones and all off diagonal elements are zeros and used genz and keister 1996 approach to calculate e δ d new m i ln q δ d new numerically as shown in appendix c 2 2 max min information gain criterion for model discrimination consider an experimental design d that includes a pumping design and an observation design to collect new groundwater level data using the least number of pumping wells and observation wells the new head observation locations serve to obtain firm information gain while the new pumping test locations serve to maximize the firm information gain data from new head observation wells stimulated by new pumping test locations will serve to discriminate groundwater models such that the same most probable groundwater model can be identified this study adopts the bayes factor as a model discrimination function to achieve the design objective the max min optimization problem to maximize firm information gain is introduced for the robust experimental design as follows 11 max d q min d δ i g d where dq are the pumping design and d δ are the observation design eq 11 is subject to 12 min b f k i p δ d new m k p δ d new m i i 1 2 m and i k γ where m i n i g d is the firm information gain from experimental design bf ki is the bayes factor which is the likelihood ratio of the most probable model m k having the highest posterior model probability against other models m i and γ is a bayes factor threshold p δ d new m i is the likelihood that new data are predicted using the model m i p δ d new m k is the highest likelihood among m models given pumping locations in an experimental design d m i n i g d can be obtained by minimizing i g given new observation data as dependent variables the maximum of m i n i g d in eq 11 can be solved by solving the maximization optimization problem where the dependent variables are pumping locations eq 12 ensures that the same most probable model has sufficient evidence to be discriminated from all other models the classification of harold jeffreys jeffreys 1998 presents how strong the new data evidence supports one model over other models the higher the γ value the stronger the data evidence that supports one model over the other competing models for example when the bayes factor is between 5 and 10 the data evidence is classified as substantial when bayes factor is greater than 10 the data evidence is classified as strong jeffreys 1998 2 3 total covariance matrix σ i for new observation data the total covariance matrix of new data includes the covariance matrix of measurement errors in new data and the covariance matrix of the estimated new data random measurement errors are usually modeled by uncorrelated gaussian noise with zero means therefore the covariance matrix of measurement errors in new data can be σ ε σ ε 2 i where σ ε 2 is a constant error variance and i is an identity matrix monte carlo simulation on model parameters is adopted to calculate the covariance matrix of the expected values of new data estimated by model m i 13 i 1 q 1 q 1 q δ β i q δ i δ β i q δ i t where q is the number of realizations of model parameters β i q of the model m i these parameter realizations are sampled from the posterior distribution upon history matching for each model δ i is the mean of the new data simulated by model m i 14 δ i 1 q q 1 q δ β i q the bma method hoeting et al 1999 draper 1995 is used to calculate the covariance matrix σof the estimated new data as follows 15 bma i 1 m i δ β i δ d new δ β i δ d new t pr m i δ obs where δ β i is the predicted new data using the model m i and the estimated model parameters β i of the model m i δ d new is the bma mean of the predicted new data the total covariance matrix for δ d new is σ i σ σ ε σ bma σ ε 2 4 model calibration and posterior model probability the covariance matrix adaptation evolution strategy cma es hansen and ostermeier 2001 hansen et al 2003 is employed to estimate model parameters and to obtain a covariance matrix for the estimated model parameters model parameters are estimated by minimizing the root mean square error rmse between calculated and observed heads the cma es is a global local stochastic derivative free algorithm that was parallelized for time consuming groundwater model calibration and uncertainty analysis elshall et al 2013 once the estimated parameters and their covariance matrix are obtained by the cma es the marginal likelihood function is calculated as follows for existing observation data δ obs which is similar to the eq 8 16 p δ obs m i 2 π n 1 2 σ ε σ i 1 2 exp 1 2 δ obs δ i t σ ε σ i 1 δ obs δ i where n1 is the number of existing observation data σ ε is the covariance matrix of measurement errors δ i is the simulated observation data using the model m i with the estimated model parameters β i obtained by the cma es and σ i is the covariance matrix of simulated observation data which is calculated by the monte carlo simulation based on the estimated model parameters and their covariance matrix obtained by the cma es it is important to acknowledge that the covariance matrix generated by the cma es is merely an estimate it is crucial to confirm the accuracy of the covariance matrix obtained through this method this can be achieved by running simulations using realizations of model parameters and verifying that the resulting root mean square errors rmses are comparable to those obtained using the estimated model parameters the posterior model probability for each groundwater model is commonly calculated the same as the likelihood given the assumption that all models have the same prior model probability other than the cma es the null space monte carlo method siade et al 2017 and the iterative ensemble smoother method white 2018 can also quantify model output uncertainty 3 numerical example this study uses a steady state groundwater flow condition in a 5 layer synthetic anisotropic confined aquifer to illustrate the robust experimental design based on the firm information gain the size of the aquifer is 5 km by 5 km and is discretized into 5 layers 25 rows and 25 columns see fig 1 the cell size is 200 m by 200 m with variable thickness there are two pumping wells pws screened at layer 1 and layer 5 and one injection well iw screened at layer 3 fig 1 shows the well locations and pump rates the true constant head boundary condition of 50 m is assigned to the boundary cells of all layers at the south boundary no flow boundary condition is assigned to the north east and west boundaries to allow better variations higher sensitivities in simulated heads upon pumping fig 2 a shows the true aquifer structure table 1 lists the true model parameters the usgs modflow 2005 harbaugh 2005 is adopted to simulate true steady state groundwater levels at the 5 existing observation wells in the model domain see fig 1 gaussian noises of a zero mean and a standard deviation of 0 1 m are added to the groundwater level data to simulate measurement errors we pretend that we do not know the true aquifer structure i e the geometry of the aquifer system and lithology the true constant head boundary value and the true horizontal hydraulic conductivity three dimensional geometry views of three aquifer structures are given in fig 2 denoted as g1 g2 and g3 respectively these aquifer structures were extracted from the real world case study of the baton rouge aquifer system using three different geostatistical methods the generalized parameterization the indicator zonation methods and the indicator kriging respectively pham and tsai 2016 2015 g3 is a highly connected aquifer system following up with g1 and g2 the number of active model cells is 2018 1566 and 2021 for gp iz and ik respectively three head values 49 50 and 51 m for the south boundary are considered and denoted as b1 b2 and b3 respectively the number of boundary cells at the south boundary for g1 g2 and g3 are 30 25 and 29 respectively these boundary cells are only in layers from 3 to 5 for all three aquifer structures the connections between the south boundary cells and the aquifer are weaker in the iz structure in comparison to the gp and ik structures combinations of three aquifer structures and three head boundary values result in nine conceptual groundwater models 4 solving the max min optimization problem to obtain robust experimental design this study solved eqs 11 and 12 to identify the robust experimental design for model discrimination to identify the most probable groundwater model the bayes factor threshold was set to be 10 such that the most probable model will be at least strongly discriminated from the other eight competing models decision variables were the number of new pumping wells and the number of new observation wells a pumping rate of 200 m3 day was assigned for all new pumping wells experimental designs were conducted by gradually increasing the number of pumping wells and the number of observation wells of the system the robust experimental design was the one that optimizes eq 11 using the least number of new pumping wells and new observation wells until eq 12 is satisfied to maximize the firm information gain in eq 11 the max min optimization problem this study utilized a parallel sequential genetic algorithm ga optimization scheme given a number of pumping wells and observation wells a parallel ga was employed to optimize pumping locations the outer loop of the max min optimization problem and under the parallel ga a sequential ga the inner loop was employed to optimize observation locations the ga code of carroll 1996 was employed to solve the max min optimization problem and was parallelized to be run in supermic a supercomputer at louisiana state university using an embarrassingly parallel technique a population size of 80 i e used 80 cores was assigned to the parallel ga and a population size of five was assigned to the sequential ga micro ga the number of generations was 50 for the parallel ga and 500 for the sequential ga other default settings were set the same in the ga code 5 results 5 1 model calibration posterior model probability and entropy of the current system table 2 shows model calibration results the posterior model probability and bayes factor for each groundwater model the parallel cma es estimated horizontal hydraulic conductivity for all layers using the five noisy head observation data the top five models g1b1 g1b2 g1b3 g3b1 and g3b2 showed comparably small rmses the three conceptual models with g2 aquifer structure resulted in a much larger rmse it indicates that the aquifer structure significantly affected the model calibration results the models with the g1 and g3 aquifer structures better represented the aquifer than the g2 aquifer structure the top five models had posterior model probabilities greater than 17 g3b2 model had the highest posterior model probability but did not have the lowest rmse because of the impact of the covariance matrix σ i in eq 16 insufficient observation data used in the model calibration prevented the true model g1b2 from having the highest posterior model probability and outperforming the other models nevertheless the bayes factor suggested that the current data did not discriminate g3b2 model from the other top four models the entropy of the system was 1 748 nat calculated using eq 1 and the posterior model probabilities in table 2 the nat the natural unit of information is the natural unit for information entropy given a system of nine models the entropy of the system is between zero highest information and 2 197 nat lowest information therefore 1 748 nat 79 6 of the maximal entropy of the system was a high value this indicates that more data are needed to reduce the entropy increase information of the system and to identify the most probable model 5 2 information gain and data correlation evaluation using the current system in this section we intend to study the changes in expected information gain i g and firm information gain min i g d by systematically adding new observation data before conducting an exhaustive robust experimental design no new pumping and injection wells were added we only draw new head data out of active cells that are in common in three aquifer structures i e 1024 possible locations additionally we investigate the impacts of data correlation on min i g d fig 3 shows the spatial distributions of expected information gain i g from drawing one new head data in layers 2 to 5 i g was found varied between 0 723 nat and 1 472 nat high expected information gain occurred in the areas near the constant head boundary and near the injection well where heads predicted by the candidate models were quite different drawing one additional head data for either layer 3 4 or 5 gained higher i g than that from layer 2 data collected from different locations provided different i g robust experimental designs are needed to identify optimal locations fig 4 compares firm information gain min i g d calculated by using an experimental design d i e using the existing pumping and injection wells and adding one to five new head data for both cases of uncorrelated and correlated heads the result indicates that min i g d increased as the size of new head data increased experimental designs considering uncorrelated new data overestimated min i g d the degree of overestimation increased dramatically with the size of uncorrelated data data correlation significantly impacted on min i g d therefore this study will only focus on experimental designs utilizing correlated data in the later sections experimental designs using one to five new head data and the existing pumping and injection wells were unable to reach the highest possible information gain i g of 1 748 nat the bayes factor threshold eq 12 was also not satisfied the bayes factor values will be presented in section 5 5 the experimental designs using one to five new head data showed that new pumping wells are needed 5 3 information gain using one new well detailed maximum firm information gain was illustrated by the case of searching for one optimal pumping location and one optimal observation location although there are 1024 possible locations model cells available for installing new pumping and observation wells only 256 locations every other model cell were considered for potential pumping locations to ensure the experimental design remains tractable the potential pumping locations were indexed from 1 to 256 the potential head observation locations are indexed from 1 to 1024 fig 5 a shows the firm information gain min i g d given by each of the 256 potential pumping well locations for each new pumping location 1024 alternatives of new observation wells were investigated and the observation location that resulted in min i g d was recorded the result showed that min i g d were varied from 0 532 nat to 0 684 nat not much changes in min i g d were detected if placing a new pumping well in layer 1 layer 2 layer 4 or layer 5 however large changes in min i g d were found if placing a new pumping well in layer 3 the maximum change of min i g d was found to be 0 684 nat occurring at pumping location index 141 in layer 3 which is denoted as circle a in fig 5 a and c the horizontal coordinates of the optimal pumping location are x 4300 m y 3100 m given the optimal pumping location fig 5 b shows the expected information gain i g for each of the 1024 potential observation locations the result showed that i g were varied from 0 684 nat to 1 110 nat drawing a new head data from layer 3 low conductivity generally provided higher i g than other layers some observation locations in layers 4 and 5 also provided higher i g the firm information gain of 0 684 nat was obtained at the observation location index 548 in layer 3 and denoted at circle b in fig 5 b and d the horizontal coordinates of the optimal observation location are also x 500 m y 2100 m the result verified that all experimental designs using one new head observation and one new pumping well at pumping location index 141 in layer 3 resulted in higher i g than 0 684 nat similar to section 5 2 experimental designs using one new pumping well and one new head data were unable to reach the highest possible information gain i g of 1 748 nat and failed to meet the bayes factor threshold two or more pumping wells are needed to achieve the design objective 5 4 data worth of adding new pumping wells versus new observation wells adding more pumping wells or adding more observation wells showed different maximize firm information gain max min i g d as illustrated in fig 6 red circles show max min i g d by increasing the number of new pumping wells up to five while keeping the number of new observation wells to be one yellow squares show max min i g d by increasing the number of new observation wells up to five while keeping the number of new pumping wells to be one given the same number of new wells e g one new pumping well or one new observation well adding new observation wells always resulted in higher max min i g d than adding new pumping wells fig 6 suggested that experimental designs should emphasize new head data collection before exploring new pumping wells we acknowledge that this observation may vary depending on the specific case in this numerical example where the model domain is relatively small the addition of a single pumping well can potentially influence the entire model domain therefore incorporating additional observation wells would be a more effective strategy than adding more pumping wells 5 5 robust experimental designs fig 7 shows whether the first rank model can or cannot be discriminated from others considering only adding up to five new head data bf12 is the bayes factor of the first rank model to the second rank model bf13 is the bayes factor of the first rank model to the third rank model and so forth the model rank is determined by the likelihood after new data are acquired the rank may change for different scenarios fig 7 a presents the bayes factors for the status quo no new pumping well and no new head data the result showed that adding a new head data discriminated the first rank model from the last three models by adding two new head data the first rank model was discriminated from the last two models adding up to five new head data only discriminated the first rank model from the last five models experimental designs using up to 5 new observations and the current system were unable to discriminate the most probable model from the other eight competing models if one new pumping well was added fig 7 b showed that adding a new head data discriminated the first rank model from six other models adding two new head data discriminated the first rank model from five other models the results also indicate that increasing new head data increased the maximum firm information gain see fig 6b but might not increase the number of models to be discriminated against similar to fig 7 a experimental designs using one new pumping well and up to five new observations were unable to discriminate the most probable model from the other eight competing models if two new pumping wells were added the same most probable model can be identified by using two to five new head data as shown in fig 7 c where the first rank model dominated all other models the most probable model was the g1b2 model which was the true model the robust experimental design found that two new pumping wells and two new head observation wells sufficed with firm information gain of 1 707 nat and reduce the entropy of the system to 0 041 nat the minimum bayes factor of 152 98 exceeded the selected threshold of 10 given the optimal locations of the two pumping wells we verified all possible locations of two new head observation wells produced entropy of the system less than 0 041 nat all identified most probable models which met the bayes factor threshold were the g1b2 model the true model this verification indicates that the same most probable model can be consistently identified regardless of sample locations 6 discussion the presence of head data correlation attributable to several factors like spatiotemporal location model domain size boundary conditions and model parameterizations showed significant impacts on firm information gain min i g d and should be considered in experimental designs this is because the most probable model tends to receive overwhelming posterior model probability close to 100 when the data size is large and the data are assumed uncorrelated this finding is consistent with lu et al 2013 that suggests accounting for the correlation of model data errors in the covariance matrix to avoid deriving unrealistic posterior model probabilities for this study it poses a serious concern that exaggerated min i g d by assuming data uncorrelated may eventually fail the experimental designs due to low information gain in actual data collection to gain maximum firm information this study found that the best locations to draw new pumping wells are in low hydraulic conductivity zones i e layer 3 in this case study see fig 5a and 5c and the best locations to draw new observation wells are the areas that are far from the pumping wells see fig 5d this is because pumping in these areas tends to generate high variation in groundwater levels in these low conductivity zones and thereafter provides higher expected information gain in comparison with pumping in high conductivity zones therefore this numerical example suggests drawing new pumping wells in the low conductivity zone and observe at a far enough distance from the pumping wells to obtain firm information for model discrimination and identification it is noted that different aquifer settings e g boundary conditions will result in different design outcomes groundwater systems are highly heterogeneous and nonlinear different locations of pumping wells and observation wells yield different information given a design objective determining the best pumping and observation locations is an important step before any field data collection as pumping tests are costly and time consuming considering only the observation part e g adding new observation wells was not a good strategy for this case study potentially because many new observation locations might yield similar information i e did not help to increase firm information gain simultaneously accounting for both the observation part and the excitation part e g adding new pumping wells was found a more efficient way to obtain new additional information the robust experimental design was succeeded in determining the optimal locations to draw new pumping wells and measure groundwater levels to achieve the design objective using the least number of wells after the robust experimental design was succeeded all other designs using the same number of wells such as 2 pumping wells and two observation wells identified the same most probable groundwater model which differs from the author s previous model discrimination criterion based on posterior model probability pham and tsai 2016 2015 where the most probable model was varied by design alternatives it is important to recognize that drilling a new pumping well is generally more expensive than drilling a new observation well and groundwater managers are not typically interested in drilling a new pumping well solely for model discrimination purposes consequently in a real world application it is more feasible to apply the method to the existing pumping network and concentrate on drilling new observation wells only the nested quadrature rule genz and keister 1996 was found an efficient approach to calculate high dimensional integrals such as e δ d new m i ln q δ d new for deriving the expected information gain i g in this study using 5 nodes and searching for one new observation at a time the dimension of δ d new is one calculating e δ d new m i ln q δ d new required nine samples of δ d new calculating e δ d new m i ln q δ d new required sample sizes of 37 93 201 and 401 when the dimension δ d new increased from 2 to 5 the computation time dramatically increased with δ d new dimension yet the number of samples required was small in comparison with the traditional monte carlo simulation approaches that usually require thousands of samples solving the max min programming problem to maximize the firm information gain was extremely time consuming even with the hypothetical case study where a single model run was less than one minute for example using the parallel ga to search for two new pumping wells the computation time for solving the max min programming problem was 6 68 7 86 8 87 11 5 and 16 4 h for δ d new size to be 1 2 3 4 and 5 using 80 cores computing time grew substantially by just increasing a few new observation data the most time consuming part came from e δ d new m i ln q δ d new calculations combining the parallel sequential ga and the nested quadrature rule efficiently solved the time consuming max min optimization problem the presented methodology assumed that the probability distribution function of observable states i e groundwater level given a realization of model events follows a multivariate gaussian distribution this assumption may not hold for the nonlinear groundwater problem e g the reactive transport model shi et al 2014 and may have an impact on the results of the robust experimental design however assuming the multivariate gaussian allows transferring the complicated multiple integrals of eq 7 into an easier form of eqs 9 and 10 these equations can accurately and efficiently be solved by utilizing the cholesky decomposition and the nested quadrature rule this gaussian assumption can be resolved by using monte carlo approaches such as the dream package vrugt 2016 however this approach requires high computational cost e g requires thousands of sample sizes and is not suitable for solving the max min optimization program even with the hypothetical numerical example in this study the robust experimental design may not guarantee a global optimal solution when the search dimension increases e g greater than five because solving the nonlinear and non convex max min problem is challenging and the computation time increases exponentially with the increase in search dimension to increase the global search capability in finding the global optimal solution one can increase the population size in the ga however this will significantly increase the computation time therefore we limited our search dimensions to less than five i e less than five new wells to avoid potential numerical issues in the numerical calculation of e δ d new m i ln q δ d new and make our optimization problem trackable the computational burden of the robust experimental design may be reduced by using surrogate modeling approaches also known as reduced order model where a complex model is replaced with an approximate but computationally efficient model ushijima and yeh 2013 asher et al 2015 jefferson et al 2015 yin and tsai 2020 for the numerical example in this study the true model was added to a pool of 9 competing models for verification purposes when the true model was removed from the robust experimental design presented in section 5 5 the minimum bayes factor decreased to 44 73 which was almost 3 42 times less than when the true model was included consequently the most probable model became g1b3 it is worth noting that a true model is typically unknown and including it does not reflect a realistic scenario however whether or not the true model is included has no impact on the methodology but it could affect the optimal locations for pumping and observation wells and the number of wells used in the robust experimental design if a model that is close to the true model is included among the competing models there is a higher likelihood of obtaining a robust experimental design with lower costs i e using fewer pumping and observation wells while achieving the minimum bayes factor of 10 was possible in the numerical example expensive experimental designs may be resulted for real world applications it is analyst s discretion in setting the bayes factor to achieve a certain level of model discrimination jeffreys 1998 7 conclusions incorporating the concept of firm information gain nominally the minimum expected information gain in the robust experimental design reveals the minimum information required while acquiring new data to identify the most probable model this is a robust approach and places the experimental design in the context of information theory the bayes factor threshold of 10 in the robust experimental design ensures that new data provides strong evidence to discriminate the most probable model from other candidate models considering a full covariance matrix of data substantially affects the calculation of firm information gain the full covariance matrix in this study is comprehensive which accounts for measurement errors and errors from model conceptualization and model parameters the bayesian model averaging method and the monte carlo approach are suitable to quantify covariances due to conceptual uncertainty and parametric uncertainty respectively neglecting covariances between data tends to exaggerate true firm information gain and results in unrealistic bayes factor values maximizing the firm information gain in the robust experimental design is a unique choice and results in more direct solutions than those from maximizing the value of the box hill discrimination function an upper bound of the expected information gain however calculating the firm information gain is not straightforward this study found that the genz keister genz and keister 1996 method can efficiently calculate the multi dimensional integral in the expected information gain when data size is small this study also found that the parallel sequential genetic algorithm scheme is an efficient scheme to maximize the firm information gain which is posed as a max min programming problem through the numerical groundwater example this study found that 1 maximum firm information gain grows faster with the size of new head data than with the number of new pumping wells in other words this study suggests that experimental designs should emphasize new head data collection before exploring new pumping wells for this specific numerical example and 2 the same most probable groundwater model could be identified as long as solutions of experimental designs result in higher than firm information gain and satisfy a bayes factor threshold future research should focus on evaluating the impacts of gaussian assumption on the robust experimental design and comparing the informatics metric proposed in this study with available metrics presented in the introduction section credit authorship contribution statement hai v pham conceptualization methodology software validation formal analysis investigation visualization writing original draft frank t c tsai conceptualization methodology writing original draft resources supervision project administration funding acquisition declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this study was supported in part by the u s geological survey under grant cooperative agreement no g21ap10577 and the u s national science foundation award no 2019561 lsu high performance computing and lsu center for computation technology are acknowledged for providing a supercomputer for this study all of the numerical data are provided in the tables and figures produced by solving the equations in the paper appendix a derivations for expected information gain the expected information gain i g in the eq 4 can be further expanded as a1 i g i 1 m pr m i δ new lnpr m i δ new q δ d new d δ d new i 1 m pr m i δ obs lnpr m i δ obs where a2 q δ d new j 1 m pr m j δ obs p δ d new m j and a3 pr m i δ new p δ new m i pr m i δ obs q δ d new inserting eqs a3 and a2 into a1 we have a4 i g i 1 m p δ new m i pr m i δ obs lnpr m i δ obs d δ d new i 1 m p δ new m i pr m i δ obs ln p δ new m i q δ d new d δ d new i 1 m pr m i δ obs lnpr m i δ obs then a5 i g i 1 m pr m i δ obs p δ d new m i ln p δ d new m i q δ d new d δ d new appendix b expected information gain for correlated multivariate gaussian data the expected information gain is b1 i g i 1 m pr m i δ obs p δ d new m i ln p δ d new m i d δ d new i 1 m pr m i δ obs p δ d new m i ln q δ d new d δ d new the probability density function is b2 p δ d new m i 2 π n 2 σ i 1 2 e 1 2 δ d new δ i t σ i 1 δ d new δ i where σ i σ σ ε is the total covariance matrix for δ d new when model m i is used substituting p δ d new m i in b1 with b2 the first integral in b1 is b3 p δ d new m i ln p δ d new m i d δ d new e ln p δ d new m i e ln 2 π n 2 σ i 1 2 e 1 2 δ d new δ i t σ i 1 δ d new δ i ln 2 π n 2 σ i 1 2 1 2 e δ d new δ i t σ i 1 δ d new δ i ln 2 π n 2 σ i 1 2 n 2 where e is the expectation operator and b4 e δ d new δ i t σ i 1 δ d new δ i e tr δ d new δ i t σ i 1 δ d new δ i e tr σ i 1 δ d new δ i δ d new δ i t tr e σ i 1 δ d new δ i δ d new δ i t tr σ i 1 e δ d new δ i δ d new δ i t tr σ i 1 σ i tr i n where tr is the trace of a square matrix and i is the identity matrix the second integral in b1 is b5 e δ d new m i ln q δ d new p δ d new m i ln q δ d new d δ d new appendix c calculate e δ d new m i ln q δ d new the correlated new data δ d new are transformed into uncorrelated random variables x by the cholesky decomposition since the covariance matrices σ i are positive definite and symmetric let σ i l i l i t and δ d new l i x δ i where l i is a lower triangular matrix with real and positive diagonal entries the random variables x have zero means and an identity matrix for the covariance matrix δ d new obtained through x include measurement errors the probability density function p δ d new m i in terms of x is c1 p δ d new m i 2 π n 2 σ i 1 2 e 1 2 δ d new δ i t σ i 1 δ d new δ i 2 π n 2 l i l i t 1 2 e 1 2 l i x t l i l i t 1 l i x 2 π n 2 l i 1 e 1 2 x t x therefore q δ d new can be calculated in terms of x c2 q δ d new j 1 m pr m j δ obs p δ d new m j j 1 m pr m j δ obs 2 π n 2 σ j 1 2 e 1 2 δ d new δ j t σ j 1 δ d new δ j j 1 m pr m j δ obs 2 π n 2 σ j 1 2 e 1 2 l i x δ i δ j t σ j 1 l i x δ i δ j q x the transformation from d δ d new to d x needs the jacobian which is the determinant of l i d δ d new abs l i d x where abs l i is the absolute value of the determinant of l i since positive diagonal entries in l i it becomes δ d new l i d x therefore c3 e δ d new m i ln q δ d new p δ d new m i ln q δ d new d δ d new 2 π n 2 l i 1 e 1 2 x t x ln q x l i d x 2 π n 2 e 1 2 x t x ln q x d x the nested quadrature rule for n dimensional numerical integration genz and keister 1996 is c4 e δ d new m i ln q δ d new r 1 r ln q x r 1 x r n w r where n is the number of dimensions of δ d new r 1 r where r is the number of nodes after removing duplicates heiss and winschel 2008 and x r n is a set of nodes and wr is a set of weights sampling x r 1 x r n from a sparse grid we get δ d new 
4,this study introduces firm information gain for model discrimination based on shannon entropy and worst case scenario experimental design firm information gain is the minimal additional information gained by an experimental design with respect to existing information robust experimental design aims to maximize the firm information gain by searching for the least number of new pumping wells and observation wells robust experimental design includes a bayes factor threshold to ensure that new data provide strong evidence for model discrimination to maximize the firm information gain a framework is proposed that combines the parallel sequential genetic algorithm ga for parallel computing and the nested quadrature rule for efficiently solving multidimensional integrals the numerical experiment involves the true model for the purpose of verification the results show that using a full covariance matrix is imperative to avoid exaggerating firm information gain collecting new groundwater data is prioritized over exploring additional pumping wells maximizing firm information gain is able to identify the same and true model keywords robust experimental design entropy information theory model discrimination uncertainty data availability data will be made available on request 1 introduction groundwater is a crucial source of freshwater throughout the world for both hydrologic and human systems alley et al 2002 giordano 2009 siebert et al 2010 groundwater modeling has been widely used for decades as essential tools for the planning and management of groundwater resources gleeson et al 2012 wada et al 2010 however developing a groundwater model has never been an easy task as groundwater data is always sparse and uncertainty always exists multiple conceptualizations of a groundwater system are often investigated yet considering too many conceptual models indicates high model prediction uncertainty and may lose the purpose of model development bredehoeft 2005 højberg and refsgaard 2005 collecting and incorporating new data into groundwater models helps advance conceptual understanding and management of groundwater resources kikuchi 2017 and in turn reduces the number of models nevertheless collecting groundwater data is usually costly and optimal experimental design techniques are often conducted before data collection to gain the maximum amount of information given a pre defined monitoring objective according to sun 1994 experimental design in groundwater modeling generally falls into two parts the observation part e g state variables to be observed the number and locations of observation wells and observation frequency and the excitation part e g the number and locations of extraction and injection wells pumping and injection rates and periods of extraction and injection if the excitation part is predetermined and only the observation part is considered the experimental design is referred to as an observation network design observation network designs have been studied extensively in the literature a variety of methodologies have been introduced to design a groundwater observation network kollat et al 2011 loaiciga et al 1992 mogheir et al 2006 among these methods physically based simulation approaches cieniawski et al 1995 cleveland and yeh 1990 dhar and datta 2007 hudak and loaiciga 1992 mckinney and loucks 1992 meyer et al 1994 reed et al 2000 storck et al 1997 and information theory entropy based method alfonso et al 2010 mogheir et al 2006 mogheir and singh 2002 nowak and guthke 2016 poeter and anderson 2005 are commonly employed owing to their flexibility in examining design scenarios and design constraints the objectives of observation network designs are usually to 1 improve parameter estimation altmann dieses et al 2002 chang et al 2005 cleveland and yeh 1990 herrera and pinder 2005 hsu and yeh 1989 sciortino et al 2002 siade et al 2017 sun and yeh 2007 ushijima and yeh 2015 2 minimize prediction uncertainty chadalavada and datta 2008 janssen et al 2008 mckinney and loucks 1992 nowak et al 2010 wagner 1995 wöhling et al 2016 3 detect plumes bode et al 2019 dhar and datta 2007 dokou and pinder 2009 kim and lee 2007 leube et al 2012 meyer and brill 1988 storck et al 1997 and 4 to discriminate among candidate models and identify the most probable model kikuchi et al 2015 knopman and voss 1988 pham and tsai 2016 2015 usunoff et al 1992 yakirevich et al 2013 readers are referred to several in depth review articles hassan 2003 kollat et al 2011 loaiciga et al 1992 minsker 2003 to achieve the objective of model discrimination observation networks aim to provide the most useful information with respect to model discrimination several criteria have been developed for model discrimination in optimal observation network designs based on the maximum differences between model predictions knopman et al 1991 knopman and voss 1988 nordqvist and voss 1996 usunoff et al 1992 the maximum kullback leibler information kikuchi et al 2015 nowak and guthke 2016 yakirevich et al 2013 the maximum change in entropy box and hill 1967 alfonso et al 2010 and the maximation of posterior model probability pham and tsai 2016 2015 the basic concept underlying all these criteria is to sample the state variable s at spatiotemporal locations i e predicted data where the variance among the ensemble of proposed competing model s predictions is maximized the worth of new data has been analyzed in various water related problems such as prediction uncertainty reduction dausman et al 2010 feyen and gorelick 2005 freer et al 1996 gates and kisiel 1974 rojas et al 2010 sohn and small 2000 tiedeman et al 2004 2003 yokota and thompson 2004 model selection wöhling et al 2015 decision making ben zvi et al 1988 davis and dvoranchik 1971 james et al 1996 reichard and evans 1989 and cost effectiveness james and gorelick 1994 neuman et al 2012 norberg and rosén 2006 wagner 1999 though optimal observation network designs have been studied extensively in the past there is still a lack of clear understanding of the amount and the worth of new data required to justify a certain level of model discrimination and identify the most probable model besides none of these studies could guarantee identifying the same most probable model moreover all these studies only considered the observation part of the experimental design in this study we introduce a robust experimental design for model discrimination based on the shannon entropy shannon 1948 and the worst case scenario experimental design sun and yeh 2007 first we introduce a firm information gain concept and derive a new model discrimination criterion based on the shannon entropy and the bayes factor the firm information is defined to be the minimum information guaranteed from an experimental design the crux of experimental design based on firm information gain is that the design objective can be achieved with the least information as a result any other experimental designs under the same experimental conditions will result in higher information gain and therefore guarantee the same design outcome according to sun and yeh 2007 an experimental design is considered robust if it accounts for both the excitation part pumping activities and observation part observation activities to maximize the firm information gain this is achieved through a max min optimization problem to improve model discrimination while using the fewest possible pumping and observation wells in the context of robust experimental design robust means that the optimized pumping well network performs well across all possible observation well networks as it performs well even with the worst case observation well network we hypothesize that the same most probable model can be identified from a pool of competing models by maximizing the firm information gain for a system e g a set of conceptual groundwater models that differ in boundary conditions geological structures etc and satisfying a bayes factor threshold any other experimental design solutions having information negative shannon entropy higher than the firm information will result in the same most probable model second we introduce a parallel computing framework that combines the parallel sequential ga carroll 1996 and the nested quadrature rule genz and keister 1996 to efficiently solve the time consuming max min optimization problem finally we test the proposed framework and conduct the robust experimental design on a hypothetical numerical example where nine competing groundwater models were generated and a robust experimental design is needed to discriminate among the models and identify the same most probable model the robust experimental design in this study is different from that in box and hill 1967 and pham and tsai 2016 first this study considers measurement errors and data correlation in the experimental design second a new model discrimination criterion is introduced that maximizes firm information gain to obtain the robust experimental design instead of finding an upper bound of the expected information gain usually referred to the box hill discrimination function 2 methodology 2 1 shannon entropy and expected information gain shannon entropy shannon 1948 provides a measure of the information value of a system using probabilities of the occurrence of events in the system consider that a set of m candidate models m m1 m2 m m represents the events of the system e g candidate models are groundwater models that differ in model conceptualizations such as boundary conditions geological structures and parameter structures their posterior model probabilities are pr m i δ obs given existing observation data δ obs the shannon entropy of the system is 1 s m δ obs i 1 m pr m i δ obs lnpr m i δ obs where lnpr m i δ obs is the information of the model m i negative entropy s represents the average amount of information i provided by all candidate models 2 i m δ obs s m δ obs the least information corresponds to the maximum entropy when all models have an equal posterior model probability the maximum information from the system corresponds to the minimum entropy when one model has a 100 posterior model probability and other models have zero posterior model probability the main purpose of an experimental design d for model discrimination is to maximize information gain through acquiring new data such that the most probable model can be identified from a pool of candidate models of the system the information gain is defined as follows 3 i g i m δ d new i m δ obs where i g is the information gain after an experimental design δ d new r n is a vector of n new data and i m δ d new represents the combined information obtained from both the new and existing data new data are unknown and uncertain before sampling this study proposes an expected information gain of the new data for the experimental design 4 i g e i m δ d new i m δ obs where i g is the expected information gain and e is the expectation operator the expected new information under a probability distribution function of new data δ d new is 5 e i m δ d new i 1 m pr m i δ d new ln pr m i δ d new q δ d new d δ d new where pr m i δ d new are the posterior model probabilities given new data δ d new and q δ d new is the averaged probability density function of new data δ d new via bayesian model averaging 6 q δ d new j 1 m pr m j δ obs p δ d new m j where p δ d new m j is the probability density function of predicted new data δ d new using the model m j inserting eqs 2 and 5 into the eq 4 appendix a shows the expected information gain as follows 7 i g i 1 m pr m i δ obs p δ d new m i ln p δ d new m i q δ d new d δ d new the integral in the eq 7 is the kullback leibler kl divergence that measures the difference between the bma weighted probability distribution q δ d new and the probability distribution p δ d new m i maximizing i g enhances the diversity of probability distributions of each model s prediction compared to the bma weighted probability distribution the expected information gain is the averaged kl divergence weighted by pr m i δ obs because the kl divergence is always non negative the expected information gain is always non negative to solve eq 7 we need to know the probability density function p δ d new m j of predicted new data δ d new considering that new data are correlated and multivariate gaussian the probability density function p δ d new m i is 8 p δ d new m i 2 π n 2 σ i 1 2 e 1 2 δ d new δ i t σ i 1 δ d new δ i where δ i are the expected values of new data estimated by the model m i σ i σ σ ε is the total covariance matrix of new data involving the use of the model m i which is the sum of the covariance matrix of the estimated new data representing parameter and model structure uncertainties and the covariance matrix of measurement errors in new data this study considers correlated data which results in a full covariance matrix appendix b further expands the eq 7 with the multivariate gaussian distribution as follows 9 i g i 1 m pr m i δ obs ln 2 π n 2 σ i 1 2 n 2 e δ d new m i ln q δ d new e δ d new m i ln q δ d new in the eq 9 is the expectation of ln q δ d new under randomness of δ d new given model m i which is 10 e δ d new m i ln q δ d new p δ d new m i ln q δ d new d δ d new for one dimensional integral i e only one new observation is collected gaussian quadrature rules and monte carlo methods are powerful however when the new data are in high dimensions and correlated these approaches become impractical due to prohibitive computing costs rising exponentially with the number of dimensions and there is no analytical solution for the eq 10 as far as authors knowledge it is noted that the method presented in this study is not limited to the gaussian distribution of the predicted new data the general form of the expected information gain is in eq 7 as soon as one knows the probability density function p δ d new m j eq 7 can be solved assuming the gaussian distribution is for the convenience purpose that the general form of i g in eq 7 is reduced to a simplifier form in eqs 9 and 10 genz and keister 1996 presented a nested quadrature rule to efficiently calculate high dimensional integrals for the multivariate normal distribution with zero means and an identity matrix unfortunately the integral in eq 10 was for multivariate gaussian with a non zero mean and a full covariance matrix therefore this study adopted the cholesky decomposition to transform e δ d new m i ln q δ d new into a multivariate normal distribution with zero means and an identity matrix i e the covariance matrix is an identity matrix in which all the diagonal elements are ones and all off diagonal elements are zeros and used genz and keister 1996 approach to calculate e δ d new m i ln q δ d new numerically as shown in appendix c 2 2 max min information gain criterion for model discrimination consider an experimental design d that includes a pumping design and an observation design to collect new groundwater level data using the least number of pumping wells and observation wells the new head observation locations serve to obtain firm information gain while the new pumping test locations serve to maximize the firm information gain data from new head observation wells stimulated by new pumping test locations will serve to discriminate groundwater models such that the same most probable groundwater model can be identified this study adopts the bayes factor as a model discrimination function to achieve the design objective the max min optimization problem to maximize firm information gain is introduced for the robust experimental design as follows 11 max d q min d δ i g d where dq are the pumping design and d δ are the observation design eq 11 is subject to 12 min b f k i p δ d new m k p δ d new m i i 1 2 m and i k γ where m i n i g d is the firm information gain from experimental design bf ki is the bayes factor which is the likelihood ratio of the most probable model m k having the highest posterior model probability against other models m i and γ is a bayes factor threshold p δ d new m i is the likelihood that new data are predicted using the model m i p δ d new m k is the highest likelihood among m models given pumping locations in an experimental design d m i n i g d can be obtained by minimizing i g given new observation data as dependent variables the maximum of m i n i g d in eq 11 can be solved by solving the maximization optimization problem where the dependent variables are pumping locations eq 12 ensures that the same most probable model has sufficient evidence to be discriminated from all other models the classification of harold jeffreys jeffreys 1998 presents how strong the new data evidence supports one model over other models the higher the γ value the stronger the data evidence that supports one model over the other competing models for example when the bayes factor is between 5 and 10 the data evidence is classified as substantial when bayes factor is greater than 10 the data evidence is classified as strong jeffreys 1998 2 3 total covariance matrix σ i for new observation data the total covariance matrix of new data includes the covariance matrix of measurement errors in new data and the covariance matrix of the estimated new data random measurement errors are usually modeled by uncorrelated gaussian noise with zero means therefore the covariance matrix of measurement errors in new data can be σ ε σ ε 2 i where σ ε 2 is a constant error variance and i is an identity matrix monte carlo simulation on model parameters is adopted to calculate the covariance matrix of the expected values of new data estimated by model m i 13 i 1 q 1 q 1 q δ β i q δ i δ β i q δ i t where q is the number of realizations of model parameters β i q of the model m i these parameter realizations are sampled from the posterior distribution upon history matching for each model δ i is the mean of the new data simulated by model m i 14 δ i 1 q q 1 q δ β i q the bma method hoeting et al 1999 draper 1995 is used to calculate the covariance matrix σof the estimated new data as follows 15 bma i 1 m i δ β i δ d new δ β i δ d new t pr m i δ obs where δ β i is the predicted new data using the model m i and the estimated model parameters β i of the model m i δ d new is the bma mean of the predicted new data the total covariance matrix for δ d new is σ i σ σ ε σ bma σ ε 2 4 model calibration and posterior model probability the covariance matrix adaptation evolution strategy cma es hansen and ostermeier 2001 hansen et al 2003 is employed to estimate model parameters and to obtain a covariance matrix for the estimated model parameters model parameters are estimated by minimizing the root mean square error rmse between calculated and observed heads the cma es is a global local stochastic derivative free algorithm that was parallelized for time consuming groundwater model calibration and uncertainty analysis elshall et al 2013 once the estimated parameters and their covariance matrix are obtained by the cma es the marginal likelihood function is calculated as follows for existing observation data δ obs which is similar to the eq 8 16 p δ obs m i 2 π n 1 2 σ ε σ i 1 2 exp 1 2 δ obs δ i t σ ε σ i 1 δ obs δ i where n1 is the number of existing observation data σ ε is the covariance matrix of measurement errors δ i is the simulated observation data using the model m i with the estimated model parameters β i obtained by the cma es and σ i is the covariance matrix of simulated observation data which is calculated by the monte carlo simulation based on the estimated model parameters and their covariance matrix obtained by the cma es it is important to acknowledge that the covariance matrix generated by the cma es is merely an estimate it is crucial to confirm the accuracy of the covariance matrix obtained through this method this can be achieved by running simulations using realizations of model parameters and verifying that the resulting root mean square errors rmses are comparable to those obtained using the estimated model parameters the posterior model probability for each groundwater model is commonly calculated the same as the likelihood given the assumption that all models have the same prior model probability other than the cma es the null space monte carlo method siade et al 2017 and the iterative ensemble smoother method white 2018 can also quantify model output uncertainty 3 numerical example this study uses a steady state groundwater flow condition in a 5 layer synthetic anisotropic confined aquifer to illustrate the robust experimental design based on the firm information gain the size of the aquifer is 5 km by 5 km and is discretized into 5 layers 25 rows and 25 columns see fig 1 the cell size is 200 m by 200 m with variable thickness there are two pumping wells pws screened at layer 1 and layer 5 and one injection well iw screened at layer 3 fig 1 shows the well locations and pump rates the true constant head boundary condition of 50 m is assigned to the boundary cells of all layers at the south boundary no flow boundary condition is assigned to the north east and west boundaries to allow better variations higher sensitivities in simulated heads upon pumping fig 2 a shows the true aquifer structure table 1 lists the true model parameters the usgs modflow 2005 harbaugh 2005 is adopted to simulate true steady state groundwater levels at the 5 existing observation wells in the model domain see fig 1 gaussian noises of a zero mean and a standard deviation of 0 1 m are added to the groundwater level data to simulate measurement errors we pretend that we do not know the true aquifer structure i e the geometry of the aquifer system and lithology the true constant head boundary value and the true horizontal hydraulic conductivity three dimensional geometry views of three aquifer structures are given in fig 2 denoted as g1 g2 and g3 respectively these aquifer structures were extracted from the real world case study of the baton rouge aquifer system using three different geostatistical methods the generalized parameterization the indicator zonation methods and the indicator kriging respectively pham and tsai 2016 2015 g3 is a highly connected aquifer system following up with g1 and g2 the number of active model cells is 2018 1566 and 2021 for gp iz and ik respectively three head values 49 50 and 51 m for the south boundary are considered and denoted as b1 b2 and b3 respectively the number of boundary cells at the south boundary for g1 g2 and g3 are 30 25 and 29 respectively these boundary cells are only in layers from 3 to 5 for all three aquifer structures the connections between the south boundary cells and the aquifer are weaker in the iz structure in comparison to the gp and ik structures combinations of three aquifer structures and three head boundary values result in nine conceptual groundwater models 4 solving the max min optimization problem to obtain robust experimental design this study solved eqs 11 and 12 to identify the robust experimental design for model discrimination to identify the most probable groundwater model the bayes factor threshold was set to be 10 such that the most probable model will be at least strongly discriminated from the other eight competing models decision variables were the number of new pumping wells and the number of new observation wells a pumping rate of 200 m3 day was assigned for all new pumping wells experimental designs were conducted by gradually increasing the number of pumping wells and the number of observation wells of the system the robust experimental design was the one that optimizes eq 11 using the least number of new pumping wells and new observation wells until eq 12 is satisfied to maximize the firm information gain in eq 11 the max min optimization problem this study utilized a parallel sequential genetic algorithm ga optimization scheme given a number of pumping wells and observation wells a parallel ga was employed to optimize pumping locations the outer loop of the max min optimization problem and under the parallel ga a sequential ga the inner loop was employed to optimize observation locations the ga code of carroll 1996 was employed to solve the max min optimization problem and was parallelized to be run in supermic a supercomputer at louisiana state university using an embarrassingly parallel technique a population size of 80 i e used 80 cores was assigned to the parallel ga and a population size of five was assigned to the sequential ga micro ga the number of generations was 50 for the parallel ga and 500 for the sequential ga other default settings were set the same in the ga code 5 results 5 1 model calibration posterior model probability and entropy of the current system table 2 shows model calibration results the posterior model probability and bayes factor for each groundwater model the parallel cma es estimated horizontal hydraulic conductivity for all layers using the five noisy head observation data the top five models g1b1 g1b2 g1b3 g3b1 and g3b2 showed comparably small rmses the three conceptual models with g2 aquifer structure resulted in a much larger rmse it indicates that the aquifer structure significantly affected the model calibration results the models with the g1 and g3 aquifer structures better represented the aquifer than the g2 aquifer structure the top five models had posterior model probabilities greater than 17 g3b2 model had the highest posterior model probability but did not have the lowest rmse because of the impact of the covariance matrix σ i in eq 16 insufficient observation data used in the model calibration prevented the true model g1b2 from having the highest posterior model probability and outperforming the other models nevertheless the bayes factor suggested that the current data did not discriminate g3b2 model from the other top four models the entropy of the system was 1 748 nat calculated using eq 1 and the posterior model probabilities in table 2 the nat the natural unit of information is the natural unit for information entropy given a system of nine models the entropy of the system is between zero highest information and 2 197 nat lowest information therefore 1 748 nat 79 6 of the maximal entropy of the system was a high value this indicates that more data are needed to reduce the entropy increase information of the system and to identify the most probable model 5 2 information gain and data correlation evaluation using the current system in this section we intend to study the changes in expected information gain i g and firm information gain min i g d by systematically adding new observation data before conducting an exhaustive robust experimental design no new pumping and injection wells were added we only draw new head data out of active cells that are in common in three aquifer structures i e 1024 possible locations additionally we investigate the impacts of data correlation on min i g d fig 3 shows the spatial distributions of expected information gain i g from drawing one new head data in layers 2 to 5 i g was found varied between 0 723 nat and 1 472 nat high expected information gain occurred in the areas near the constant head boundary and near the injection well where heads predicted by the candidate models were quite different drawing one additional head data for either layer 3 4 or 5 gained higher i g than that from layer 2 data collected from different locations provided different i g robust experimental designs are needed to identify optimal locations fig 4 compares firm information gain min i g d calculated by using an experimental design d i e using the existing pumping and injection wells and adding one to five new head data for both cases of uncorrelated and correlated heads the result indicates that min i g d increased as the size of new head data increased experimental designs considering uncorrelated new data overestimated min i g d the degree of overestimation increased dramatically with the size of uncorrelated data data correlation significantly impacted on min i g d therefore this study will only focus on experimental designs utilizing correlated data in the later sections experimental designs using one to five new head data and the existing pumping and injection wells were unable to reach the highest possible information gain i g of 1 748 nat the bayes factor threshold eq 12 was also not satisfied the bayes factor values will be presented in section 5 5 the experimental designs using one to five new head data showed that new pumping wells are needed 5 3 information gain using one new well detailed maximum firm information gain was illustrated by the case of searching for one optimal pumping location and one optimal observation location although there are 1024 possible locations model cells available for installing new pumping and observation wells only 256 locations every other model cell were considered for potential pumping locations to ensure the experimental design remains tractable the potential pumping locations were indexed from 1 to 256 the potential head observation locations are indexed from 1 to 1024 fig 5 a shows the firm information gain min i g d given by each of the 256 potential pumping well locations for each new pumping location 1024 alternatives of new observation wells were investigated and the observation location that resulted in min i g d was recorded the result showed that min i g d were varied from 0 532 nat to 0 684 nat not much changes in min i g d were detected if placing a new pumping well in layer 1 layer 2 layer 4 or layer 5 however large changes in min i g d were found if placing a new pumping well in layer 3 the maximum change of min i g d was found to be 0 684 nat occurring at pumping location index 141 in layer 3 which is denoted as circle a in fig 5 a and c the horizontal coordinates of the optimal pumping location are x 4300 m y 3100 m given the optimal pumping location fig 5 b shows the expected information gain i g for each of the 1024 potential observation locations the result showed that i g were varied from 0 684 nat to 1 110 nat drawing a new head data from layer 3 low conductivity generally provided higher i g than other layers some observation locations in layers 4 and 5 also provided higher i g the firm information gain of 0 684 nat was obtained at the observation location index 548 in layer 3 and denoted at circle b in fig 5 b and d the horizontal coordinates of the optimal observation location are also x 500 m y 2100 m the result verified that all experimental designs using one new head observation and one new pumping well at pumping location index 141 in layer 3 resulted in higher i g than 0 684 nat similar to section 5 2 experimental designs using one new pumping well and one new head data were unable to reach the highest possible information gain i g of 1 748 nat and failed to meet the bayes factor threshold two or more pumping wells are needed to achieve the design objective 5 4 data worth of adding new pumping wells versus new observation wells adding more pumping wells or adding more observation wells showed different maximize firm information gain max min i g d as illustrated in fig 6 red circles show max min i g d by increasing the number of new pumping wells up to five while keeping the number of new observation wells to be one yellow squares show max min i g d by increasing the number of new observation wells up to five while keeping the number of new pumping wells to be one given the same number of new wells e g one new pumping well or one new observation well adding new observation wells always resulted in higher max min i g d than adding new pumping wells fig 6 suggested that experimental designs should emphasize new head data collection before exploring new pumping wells we acknowledge that this observation may vary depending on the specific case in this numerical example where the model domain is relatively small the addition of a single pumping well can potentially influence the entire model domain therefore incorporating additional observation wells would be a more effective strategy than adding more pumping wells 5 5 robust experimental designs fig 7 shows whether the first rank model can or cannot be discriminated from others considering only adding up to five new head data bf12 is the bayes factor of the first rank model to the second rank model bf13 is the bayes factor of the first rank model to the third rank model and so forth the model rank is determined by the likelihood after new data are acquired the rank may change for different scenarios fig 7 a presents the bayes factors for the status quo no new pumping well and no new head data the result showed that adding a new head data discriminated the first rank model from the last three models by adding two new head data the first rank model was discriminated from the last two models adding up to five new head data only discriminated the first rank model from the last five models experimental designs using up to 5 new observations and the current system were unable to discriminate the most probable model from the other eight competing models if one new pumping well was added fig 7 b showed that adding a new head data discriminated the first rank model from six other models adding two new head data discriminated the first rank model from five other models the results also indicate that increasing new head data increased the maximum firm information gain see fig 6b but might not increase the number of models to be discriminated against similar to fig 7 a experimental designs using one new pumping well and up to five new observations were unable to discriminate the most probable model from the other eight competing models if two new pumping wells were added the same most probable model can be identified by using two to five new head data as shown in fig 7 c where the first rank model dominated all other models the most probable model was the g1b2 model which was the true model the robust experimental design found that two new pumping wells and two new head observation wells sufficed with firm information gain of 1 707 nat and reduce the entropy of the system to 0 041 nat the minimum bayes factor of 152 98 exceeded the selected threshold of 10 given the optimal locations of the two pumping wells we verified all possible locations of two new head observation wells produced entropy of the system less than 0 041 nat all identified most probable models which met the bayes factor threshold were the g1b2 model the true model this verification indicates that the same most probable model can be consistently identified regardless of sample locations 6 discussion the presence of head data correlation attributable to several factors like spatiotemporal location model domain size boundary conditions and model parameterizations showed significant impacts on firm information gain min i g d and should be considered in experimental designs this is because the most probable model tends to receive overwhelming posterior model probability close to 100 when the data size is large and the data are assumed uncorrelated this finding is consistent with lu et al 2013 that suggests accounting for the correlation of model data errors in the covariance matrix to avoid deriving unrealistic posterior model probabilities for this study it poses a serious concern that exaggerated min i g d by assuming data uncorrelated may eventually fail the experimental designs due to low information gain in actual data collection to gain maximum firm information this study found that the best locations to draw new pumping wells are in low hydraulic conductivity zones i e layer 3 in this case study see fig 5a and 5c and the best locations to draw new observation wells are the areas that are far from the pumping wells see fig 5d this is because pumping in these areas tends to generate high variation in groundwater levels in these low conductivity zones and thereafter provides higher expected information gain in comparison with pumping in high conductivity zones therefore this numerical example suggests drawing new pumping wells in the low conductivity zone and observe at a far enough distance from the pumping wells to obtain firm information for model discrimination and identification it is noted that different aquifer settings e g boundary conditions will result in different design outcomes groundwater systems are highly heterogeneous and nonlinear different locations of pumping wells and observation wells yield different information given a design objective determining the best pumping and observation locations is an important step before any field data collection as pumping tests are costly and time consuming considering only the observation part e g adding new observation wells was not a good strategy for this case study potentially because many new observation locations might yield similar information i e did not help to increase firm information gain simultaneously accounting for both the observation part and the excitation part e g adding new pumping wells was found a more efficient way to obtain new additional information the robust experimental design was succeeded in determining the optimal locations to draw new pumping wells and measure groundwater levels to achieve the design objective using the least number of wells after the robust experimental design was succeeded all other designs using the same number of wells such as 2 pumping wells and two observation wells identified the same most probable groundwater model which differs from the author s previous model discrimination criterion based on posterior model probability pham and tsai 2016 2015 where the most probable model was varied by design alternatives it is important to recognize that drilling a new pumping well is generally more expensive than drilling a new observation well and groundwater managers are not typically interested in drilling a new pumping well solely for model discrimination purposes consequently in a real world application it is more feasible to apply the method to the existing pumping network and concentrate on drilling new observation wells only the nested quadrature rule genz and keister 1996 was found an efficient approach to calculate high dimensional integrals such as e δ d new m i ln q δ d new for deriving the expected information gain i g in this study using 5 nodes and searching for one new observation at a time the dimension of δ d new is one calculating e δ d new m i ln q δ d new required nine samples of δ d new calculating e δ d new m i ln q δ d new required sample sizes of 37 93 201 and 401 when the dimension δ d new increased from 2 to 5 the computation time dramatically increased with δ d new dimension yet the number of samples required was small in comparison with the traditional monte carlo simulation approaches that usually require thousands of samples solving the max min programming problem to maximize the firm information gain was extremely time consuming even with the hypothetical case study where a single model run was less than one minute for example using the parallel ga to search for two new pumping wells the computation time for solving the max min programming problem was 6 68 7 86 8 87 11 5 and 16 4 h for δ d new size to be 1 2 3 4 and 5 using 80 cores computing time grew substantially by just increasing a few new observation data the most time consuming part came from e δ d new m i ln q δ d new calculations combining the parallel sequential ga and the nested quadrature rule efficiently solved the time consuming max min optimization problem the presented methodology assumed that the probability distribution function of observable states i e groundwater level given a realization of model events follows a multivariate gaussian distribution this assumption may not hold for the nonlinear groundwater problem e g the reactive transport model shi et al 2014 and may have an impact on the results of the robust experimental design however assuming the multivariate gaussian allows transferring the complicated multiple integrals of eq 7 into an easier form of eqs 9 and 10 these equations can accurately and efficiently be solved by utilizing the cholesky decomposition and the nested quadrature rule this gaussian assumption can be resolved by using monte carlo approaches such as the dream package vrugt 2016 however this approach requires high computational cost e g requires thousands of sample sizes and is not suitable for solving the max min optimization program even with the hypothetical numerical example in this study the robust experimental design may not guarantee a global optimal solution when the search dimension increases e g greater than five because solving the nonlinear and non convex max min problem is challenging and the computation time increases exponentially with the increase in search dimension to increase the global search capability in finding the global optimal solution one can increase the population size in the ga however this will significantly increase the computation time therefore we limited our search dimensions to less than five i e less than five new wells to avoid potential numerical issues in the numerical calculation of e δ d new m i ln q δ d new and make our optimization problem trackable the computational burden of the robust experimental design may be reduced by using surrogate modeling approaches also known as reduced order model where a complex model is replaced with an approximate but computationally efficient model ushijima and yeh 2013 asher et al 2015 jefferson et al 2015 yin and tsai 2020 for the numerical example in this study the true model was added to a pool of 9 competing models for verification purposes when the true model was removed from the robust experimental design presented in section 5 5 the minimum bayes factor decreased to 44 73 which was almost 3 42 times less than when the true model was included consequently the most probable model became g1b3 it is worth noting that a true model is typically unknown and including it does not reflect a realistic scenario however whether or not the true model is included has no impact on the methodology but it could affect the optimal locations for pumping and observation wells and the number of wells used in the robust experimental design if a model that is close to the true model is included among the competing models there is a higher likelihood of obtaining a robust experimental design with lower costs i e using fewer pumping and observation wells while achieving the minimum bayes factor of 10 was possible in the numerical example expensive experimental designs may be resulted for real world applications it is analyst s discretion in setting the bayes factor to achieve a certain level of model discrimination jeffreys 1998 7 conclusions incorporating the concept of firm information gain nominally the minimum expected information gain in the robust experimental design reveals the minimum information required while acquiring new data to identify the most probable model this is a robust approach and places the experimental design in the context of information theory the bayes factor threshold of 10 in the robust experimental design ensures that new data provides strong evidence to discriminate the most probable model from other candidate models considering a full covariance matrix of data substantially affects the calculation of firm information gain the full covariance matrix in this study is comprehensive which accounts for measurement errors and errors from model conceptualization and model parameters the bayesian model averaging method and the monte carlo approach are suitable to quantify covariances due to conceptual uncertainty and parametric uncertainty respectively neglecting covariances between data tends to exaggerate true firm information gain and results in unrealistic bayes factor values maximizing the firm information gain in the robust experimental design is a unique choice and results in more direct solutions than those from maximizing the value of the box hill discrimination function an upper bound of the expected information gain however calculating the firm information gain is not straightforward this study found that the genz keister genz and keister 1996 method can efficiently calculate the multi dimensional integral in the expected information gain when data size is small this study also found that the parallel sequential genetic algorithm scheme is an efficient scheme to maximize the firm information gain which is posed as a max min programming problem through the numerical groundwater example this study found that 1 maximum firm information gain grows faster with the size of new head data than with the number of new pumping wells in other words this study suggests that experimental designs should emphasize new head data collection before exploring new pumping wells for this specific numerical example and 2 the same most probable groundwater model could be identified as long as solutions of experimental designs result in higher than firm information gain and satisfy a bayes factor threshold future research should focus on evaluating the impacts of gaussian assumption on the robust experimental design and comparing the informatics metric proposed in this study with available metrics presented in the introduction section credit authorship contribution statement hai v pham conceptualization methodology software validation formal analysis investigation visualization writing original draft frank t c tsai conceptualization methodology writing original draft resources supervision project administration funding acquisition declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this study was supported in part by the u s geological survey under grant cooperative agreement no g21ap10577 and the u s national science foundation award no 2019561 lsu high performance computing and lsu center for computation technology are acknowledged for providing a supercomputer for this study all of the numerical data are provided in the tables and figures produced by solving the equations in the paper appendix a derivations for expected information gain the expected information gain i g in the eq 4 can be further expanded as a1 i g i 1 m pr m i δ new lnpr m i δ new q δ d new d δ d new i 1 m pr m i δ obs lnpr m i δ obs where a2 q δ d new j 1 m pr m j δ obs p δ d new m j and a3 pr m i δ new p δ new m i pr m i δ obs q δ d new inserting eqs a3 and a2 into a1 we have a4 i g i 1 m p δ new m i pr m i δ obs lnpr m i δ obs d δ d new i 1 m p δ new m i pr m i δ obs ln p δ new m i q δ d new d δ d new i 1 m pr m i δ obs lnpr m i δ obs then a5 i g i 1 m pr m i δ obs p δ d new m i ln p δ d new m i q δ d new d δ d new appendix b expected information gain for correlated multivariate gaussian data the expected information gain is b1 i g i 1 m pr m i δ obs p δ d new m i ln p δ d new m i d δ d new i 1 m pr m i δ obs p δ d new m i ln q δ d new d δ d new the probability density function is b2 p δ d new m i 2 π n 2 σ i 1 2 e 1 2 δ d new δ i t σ i 1 δ d new δ i where σ i σ σ ε is the total covariance matrix for δ d new when model m i is used substituting p δ d new m i in b1 with b2 the first integral in b1 is b3 p δ d new m i ln p δ d new m i d δ d new e ln p δ d new m i e ln 2 π n 2 σ i 1 2 e 1 2 δ d new δ i t σ i 1 δ d new δ i ln 2 π n 2 σ i 1 2 1 2 e δ d new δ i t σ i 1 δ d new δ i ln 2 π n 2 σ i 1 2 n 2 where e is the expectation operator and b4 e δ d new δ i t σ i 1 δ d new δ i e tr δ d new δ i t σ i 1 δ d new δ i e tr σ i 1 δ d new δ i δ d new δ i t tr e σ i 1 δ d new δ i δ d new δ i t tr σ i 1 e δ d new δ i δ d new δ i t tr σ i 1 σ i tr i n where tr is the trace of a square matrix and i is the identity matrix the second integral in b1 is b5 e δ d new m i ln q δ d new p δ d new m i ln q δ d new d δ d new appendix c calculate e δ d new m i ln q δ d new the correlated new data δ d new are transformed into uncorrelated random variables x by the cholesky decomposition since the covariance matrices σ i are positive definite and symmetric let σ i l i l i t and δ d new l i x δ i where l i is a lower triangular matrix with real and positive diagonal entries the random variables x have zero means and an identity matrix for the covariance matrix δ d new obtained through x include measurement errors the probability density function p δ d new m i in terms of x is c1 p δ d new m i 2 π n 2 σ i 1 2 e 1 2 δ d new δ i t σ i 1 δ d new δ i 2 π n 2 l i l i t 1 2 e 1 2 l i x t l i l i t 1 l i x 2 π n 2 l i 1 e 1 2 x t x therefore q δ d new can be calculated in terms of x c2 q δ d new j 1 m pr m j δ obs p δ d new m j j 1 m pr m j δ obs 2 π n 2 σ j 1 2 e 1 2 δ d new δ j t σ j 1 δ d new δ j j 1 m pr m j δ obs 2 π n 2 σ j 1 2 e 1 2 l i x δ i δ j t σ j 1 l i x δ i δ j q x the transformation from d δ d new to d x needs the jacobian which is the determinant of l i d δ d new abs l i d x where abs l i is the absolute value of the determinant of l i since positive diagonal entries in l i it becomes δ d new l i d x therefore c3 e δ d new m i ln q δ d new p δ d new m i ln q δ d new d δ d new 2 π n 2 l i 1 e 1 2 x t x ln q x l i d x 2 π n 2 e 1 2 x t x ln q x d x the nested quadrature rule for n dimensional numerical integration genz and keister 1996 is c4 e δ d new m i ln q δ d new r 1 r ln q x r 1 x r n w r where n is the number of dimensions of δ d new r 1 r where r is the number of nodes after removing duplicates heiss and winschel 2008 and x r n is a set of nodes and wr is a set of weights sampling x r 1 x r n from a sparse grid we get δ d new 
