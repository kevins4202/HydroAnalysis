index,text
25795,scientific visualizations are the foundation for communicating results and findings to a variety of audiences as the creation of novel and large environmental datasets has grown this has necessitated new schemes and recommendations for creating effective visualizations in this overview we review the foundations of scientific visualization and considerations for visualization of large datasets within the context of the four vs of big data volume variety veracity and velocity using big datasets requires making decisions as to whether to aggregate or preserve details approaches for grouping to enable comparisons and considering how best to show complex data in many dimensional space to enable more effective visualizations we provide several considerations regarding common decisions faced during the visualization process these recommendations are accompanied by examples applied to existing large datasets while our recommendations are just that they encourage intentionality and awareness of the choices faced when visualizing scientific datasets keywords scientific visualization visual communication plots graphics multidimensional visual analytics 1 introduction visualization is one of the foundational mechanisms used to communicate science visuals help us make sense of complex problems and interact with information kirsh 2010 liu and stasko 2010 scaife and rogers 1996 more specifically visuals aid in decision making deitrick and edsall 2006 kinkeldey et al 2014 2017 learning gordin and pea 1995 höffler 2010 höffler and leutner 2007 yang et al 2003 and science communication desnoyers 2011 in the past several decades the creation of environmental datasets skyrocketed this trend emerged for several reasons in general large datasets are more widely available because of technological advances resulting in constantly improving computing abilities enabling analysis and modeling to be performed at higher spatial and temporal resolutions over broader spatio temporal domains these technological improvements contribute to growing volumes of data and shrinking costs of in situ alam et al 2020 murphy et al 2015 parra et al 2018 wickert 2014 wickert et al 2019 and remote sensing technologies zhang et al 2019 and new often open source analysis tools gorelick et al 2017 vos et al 2019 in addition to the generation of new data support for providing public access to datasets used in publications has also increased the scientific community continues to show broad interest and support for reproducibility and open science baker 2016 munafò et al 2017 sandve et al 2013 stagge et al 2019 journals and funding agencies are precipitating these efforts through the creation and maintenance of online repositories and requirements to store data of various types finally collaboration has spurred the generation of new large datasets through model intercomparison experiments baroni et al 2019 best 2019 krysanova et al 2017 maxwell et al 2014 smith et al 2004 open source coding packages decicco et al 2020 fuka dr et al 2018 slater et al 2019 new journals aimed at publishing large and unique datasets e g scientific data earth system science data community based data collection e g ameriflux phenocam and citizen science datasets e g crowdwater stream tracker all of this amounts to a diverse sometimes overwhelming and altogether impressive collection of data now at the fingertips of the earth ecological and environmental science and engineering communities the acceleration of data availability entails the growth of the spatial temporal and uncertainty dimensions of environmental data contained in publications and presentations to borrow a buzzword this means many publications are now making use of and visualizing big data while there are numerous definitions of big data the criteria for defining big data generally is associated with dataset size and complexity as well as the need for advanced tools or technologies to interact with such datasets chang and grady 2019 ward and barker 2013 while the line where data becomes big is unclear any dataset by virtue of its volume e g size variety e g different types of data or variables veracity e g uncertainty and velocity e g speed at which data is collected may fall under the heading of big data farley et al 2018 these different attributes termed the four v s of big data introduced by ibm in the 2000s can complicate visualizations and visualization goals yang and huang 2013 though many recommendations exist for how to best use scientific visualizations in publications and presentations few 2009 kelleher and wagener 2011 rougier et al 2014 tufte 1990 2001 weissgerber et al 2019 the growing volume and variety of data synthesized by researchers necessitates augmenting existing recommendations to consider the technical and aesthetic challenges associated with the visualization of large datasets as highlighted by liu et al 2017 there are numerous decisions to be made especially when visualizing high dimensional datasets large datasets are cumbersome and present technical challenges to data wrangling the transformation of raw values into a form that can be leveraged to address research objectives though many of general principles that were famously introduced by edward tufte in the 1970s and 1980s still apply to a visualization regardless of the amount of data contained within how best to meet those recommendations as well as how to approach decision making when creating visualizations with large datasets remains a common challenge colloquially visualizations produced over the last decade include more raw data data points data series and more variables visualizations that move beyond 2d into 3d and higher dimensional space are now common to date there has been much attention given to computational processing analysis and user interfacing and interaction liu et al 2017 however there has been much less attention given to how best to effectively convey information in visual form to address this need we deliver a set of approaches and recommendations paying heed to potential pitfalls for visualizing large environmental datasets existing recommendations regarding scientific visualization generated over the last few decades serve as a sound basis for evaluating the effectiveness of any visualization our recommendations serve to augment these sound principles in the context of big data visuals analysis and presentation of large datasets in many ways stretch the limits of traditional recommendations for visualization therefore our focus is specifically on visualizing large volumes and varieties of data to assist in the analysis synthesis and comparison of large datasets for presentation and publications 2 challenges posed by large environmental datasets large environmental datasets present major challenges when it comes to developing succinct easily interpretable and visually aesthetic plots these difficulties arise from two sources technical challenges introduced by computational constraints when visualizing a large dataset and the decision making that is involved in how to best display and convey large datasets these challenges are best expressed when considering the major characteristics of big data also known as the 4 vs large datasets often have large volume many values large variety many types of data and inherent but challenging to communicate veracity the fourth v velocity we describe in more detail in a later section here we interpret this fourth v to refer to the dynamic nature of many large datasets that may often be best conveyed using animated or interactive approaches however the majority of our overview focuses on static visualizations as these are still the major currency of visual communication below we outline the major challenges introduced by three of the four vs when it comes to approaching data visualization with large datasets that may fall into one or multiple of these categories 2 1 challenge 1 large datasets are unsurprisingly big the sheer volume of large environmental datasets introduces several considerations for visualizations beyond posing technical challenges while there are many examples of voluminous visualizations there is a tension between ensuring a visualization shows broad patterns and the distribution of the data while at the same time allowing a reader to identify all of the data or the most important data too often we synthesize and remove key pieces of information through aggregation yet this may also be a necessary step to pursue scientific questions that span spatio temporal scales 2 2 challenge 2 large datasets often contain variety variety in large datasets refers to the inclusion of different types of data categories of data or different variables or descriptors a common challenge in large datasets with exceptional variety is how to best display multi dimensional data to show broad relationships across many variables or descriptors likewise plots that highlight variety often deal with multiple categories and comparisons complexity should not be avoided when creating such visualizations though it can be challenging to walk the line between clean visuals and overcomplicated visuals when displaying datasets with large variety 2 3 challenge 3 large datasets are frequently used to communicate veracity veracity is often interpreted as data uncertainty here we broadly interpret this term to refer to all types of uncertainty variability and comparisons between values to determine veracity plots concerned with veracity may be used to show aggregated metrics such as objective functions jackson et al 2019 uncertainty error probabilities or confidence these approaches often rely on comparison to a baseline e g modeled uncertainty applied to a timeseries plot error bars applied to bar chart or dot plot or feature error as a derived value e g boxplot or violin plot of errors bar chart of difference from true or zero communication of veracity can be especially challenging spiegelhalter et al 2011 as emphasized by the misinterpretation of common graphics used to communicate uncertainty such as the hurricane cone of uncertainty boone et al 2018 3 decision making for visualizing large datasets the term visualization can be ambiguous it may refer to a tool being used to create or generate a visualization to the process of creating a visualization to the analysis of data or to a generated visual parsons and sedig 2014 in this article we use the term scientific visualization to refer to visual representations of datasets in the literature two common types of visualizations exist glyphs and plots glyphs e g multidimensional icons combine multiple encoding attributes into symbols or graphical representations e g chernoff faces chernoff 1973 or infographics in contrast plots display datasets using coordinate systems we focus specifically on the creation of scientific visualizations as plots though note that many of our recommendations also apply to glyphs generating a visualization from a large dataset introduces both technical challenges as well as several often somewhat subjective decisions that must be made to generate a visual display when considering how best to approach visualization of a large dataset there are four central questions that must be answered when creating a visualization fig 1 plot type or the decision to use multiple plots which visualization s will you use to display your data raw values or aggregation is aggregation needed or should viewers see raw values dimensionality how many dimensions do you need to display color are you using color and are you using color wisely in the sections that follow we present common challenges or pitfalls when using traditional visualization techniques and considerations and recommendations for how to re envision these plots in the context of these four key decisions we also envision these decisions in fig 1 as a series of steppingstones to arriving at a final plot amongst these recommendations we qualify that this overview is by no means represents an exhaustive list of all considerations when plotting datasets whether small or large but serves as a starting point for thinking about visualizations in the context of large datasets importantly these recommendations are not intended to be applied in isolation instead they are complimentary ideas that should be used to identify how visualizations of large datasets may be approached or improved using these decisions as a guide we include examples created from existing large environmental datasets these include the gagesii dataset falcone 2011 falcone et al 2010 land cover change data for alaska from the national land cover dataset homer et al 2015 national land cover dataset 2020 and the continuously updated digital elevation model dataset cooperative institute for research in environmental sciences cires at the university of colorado boulder 2014 all visualizations were created within rstudio v 3 6 3 and code is available on github 3 1 choosing a plot type encoding attributes and overall visualization approach at a basic level a visualization is composed of encoding attributes scales and coordinate systems wickham 2010 scientific visualizations rely on the selection of encoding attributes also known as visual encodings or visual marks these attributes are used to convey quantitative and qualitative information within the context of a visualization as summarized by few 2009 attributes include those associated with form e g length width orientation size shape curvature enclosure and blur color hue value saturation transparency spatial position 2 d position spatial grouping or density and motion direction path scales are used to encode information using attributes associated with form size and color they may be quantitative e g color size or categorical color shape coordinate systems provide a means of assessing spatial position coordinate systems may be cartesian logarithmic on one or multiple axes polar r θ or multidimensional these building blocks of scientific figures ultimately come together in a visualization while it is sometimes helpful to think about these individual pieces perhaps more important is to consider the overall plot type as this is one of the most crucial choices faced in the visualization of a large multi dimensional dataset this decision is an inherently subjective choice but can benefit from keeping in mind the overarching plot goal or message what is the main message you wish to convey this choice will ultimately determine how many dimensions you seek to encapsulate within your plot which then will help to identify what plot types are at your disposal regardless of the big picture selection of a plot type the details associated with the plot building blocks are equally important within the open source programming language r these components are often described and implemented as the grammar of graphics wilkinson et al 2005 wickham 2010 though not all programming languages or tools implement a graphical grammar the grammar introduced by wilkinson et al 2005 and refined by wickham 2010 is helpful for identifying the choices faced and the refinements that can be used in the process of generating a graphic such details are crucial for refining visualizations 3 1 1 consideration 1 match your plot type and encoding attributes to your key message visualizations are built upon the selection of encoding attributes and the choices made in the selection of components of a figure while we are all aware of the components that are used to build a visualization the selection of these components is a key development step in creating a visualization visualizations of any type should begin with identifying a key message aimed to be conveyed with the visualization from this message we can select a particular plot type scale and coordinate system built on the selection of encoding attributes to display quantitative information or qualitative groupings kelleher and wagener 2011 writing out a key message or the visualization take away can be a good place to begin especially when parsing components of a complex visualization for example does the reader need to compare groups or categories to determine the key message likewise revisiting these choices during revision of a given visualization can help to clarify the message conveyed by a particular plot though visualizations are unique to the dataset and creator there are several common key messages that visualizations seek to highlight these include connections between values i e flow the distribution of a dataset data density including spatial density geospatial position magnitude outliers part to whole i e hierarchical or layered datasets patterns rankings relationships i e correlation timeseries and uncertainty these common themes may represent a starting point for designing a plot to convey a key message in fig 2 we show several cartoon examples of multidimensional visualizations and highlight common key messages or themes that may be conveyed using each of these plots we do also highlight that many figures may be composed of multiple plots aimed at showing groupings relative to the larger dataset or other groups also called facets wickham 2010 or groupings relative to subsetted data groups we encourage visual creators to remember that there are a multitude of different approaches and types of plots that can be used to visualize data for inspiration we direct you to several impressive summaries including the visual vocabulary smith et al 2019 the data visualization catalogue ribecca 2020 and the graphic continuum schwabish and ribecca 2014 in particular the graphic continuum highlights six key plot groups distributions time comparing categories geospatial part to whole and relationships 3 1 2 consideration 2 pay heed to overall composition as you finetune your visualization as discussed above visualizations inherently consist of many different components that must work together to tell a story how best to arrange these components such that they most clearly articulate a key message can be thought of as composition the composition of a plot includes selection of a color palette the use of annotation through legends direct labeling and other words included on the visualization including the caption and the choice of plot and how the plot is designed visualizations often include annotations text or enclosures used to highlight or explain features of the visualization beyond the caption annotations are a way to use text or other visual cues to direct the eye of the reader and to aid interpretation annotations also encompass the figure legend that is used to describe a qualitative and or quantitative scale ensuring a strong composition requires attention to annotations which enhance a viewer s understanding of a given visualization composition also includes the creation of what we will call mega figures composed of many subplots or facets though a single visualization may feature one plot visualizations particularly those of complex datasets may also include a composite of many small multiples tufte 1990 also known as subplots matlab or facets r the combination of small multiples may be used to provide additional detail regarding a component of a dataset and can be especially useful for parsing and displaying subsets of a large dataset in the literature small multiples are commonly used to parse a single dataset often using a repeated coordinate system encoding attributes and scale but varying the data displayed enabling visualization of high dimensional data however these subplots or small multiples can also be superimposed on larger plots to display different types of data e g spatial versus temporal versus categorical or to visualize data at different scales in composing a plot we encourage readers to think beyond generating a single plot to producing an integrative visualization that may be composed of many plots and plotting elements 3 1 3 consideration 3 give thought to how you can simplify and clarify for your key message across large environmental datasets that may exhibit large volume or variety there are several common approaches to simplifying such visualizations that may clarify the overarching message of a particular plot aggregating large volumes into simple distributions or statistics multiple values into indices single points into footprints combining multiple types of data into multidimensional plots highlighting outliers certain groups of data trends a single observation when designing a plot it is important to consider these options for creating a clear and concise visualization finally ensuring that key message is clear and perceivable by others is one the most important considerations when creating a visualization generating a useful and effective visualization not only requires that you have in mind what the goal of your plot is and how you want to use encoding attributes color shape width size orientation few 2009 kelleher and wagener 2011 to convey key messages but also that this key message is perceivable by others 3 1 4 consideration 4 aesthetics are important think beyond just color visualizations are as much science as art often we associate color with aesthetics so much so that we have dedicated an entire section of this overview to the discussion of color however aesthetics of visualizations go far beyond color alone during the visualization process give thought and attention to the details annotations font size font type legend placement axis widths tick mark spacing for publication quality graphics many journals may have recommendations for particular font sizes or types to use and may specify the location inside the axis or outside the axis for tick marks helvetica and arial are often preferred fonts when creating visualizations in addition there are several details that can be used to improve the overall interpretation of your visualization fig 3 these include enclosure e g to highlight data points that meet a certain p value arrows e g to show directional connections annotation e g to explain or label an unusual or exceptional data value or sets of data values and transparency e g when elements overlap overall attention to these small details can be used to improve the overall aesthetics of your visualization 3 2 preserving individual values versus transformation or aggregation often the analysis of a large dataset begins with visualization of raw untransformed unaggregated data on the path to presentation and publications this data is often repackaged in different ways within visualizations this re packaging often includes the use of transformations and the use of aggregations transformations depending on the visualization tool may be applied to the data to the scale or to the coordinate system wickham 2010 when applying transformations to scales or coordinate systems clarity and communication is key this requires attention to and use of tick marks legends and even the figure caption visualizations may also rely on statistical transformations that aggregate or alter data in some way this includes data binning as is done when plotting distributions or density data jittering data smoothing or categorial or other groupings applied to datasets while transformations and aggregations are a necessary part of visualizing large datasets they can also alter the perception of the data and the visualization one existing tension in the visualization of large datasets is whether or not it is important to show all values in a given visualization or whether these values should be aggregated this decision depends on a few factors particularly the size of the dataset consideration 1 and the approach to aggregation consideration 2 but should also be viewed in the context of which approach produces a clear visualization that enables viewers to perceive the overarching plot message several plot types including scatter plots spatial scatter plots and parallel coordinate plots are used to enable readers to quantitatively perceive all values within a dataset humans have a remarkable ability to lump or categorize visual information so often preserving information while highlighting the main or macro pattern is key for effective visualization tufte 1990 as stated by tufte 1990 clutter and confusion are failures of design not attributes of information yet displaying all raw values may overwhelm or obscure trends variation or groups when it comes to large datasets showing all values may not be possible for high volume datasets e g a long timeseries or for many raw values for these situations aggregation is often necessary however it is important to keep in mind that aggregation can subsume extensive variability in raw values which can challenge interpretation of veracity in this section we highlight two considerations when making the decision regarding whether to aggregate or preserve raw data 3 2 1 consideration 1 can raw values be distinguished preserving the visualization of all points is particularly challenging for large datasets as the information contained in the plot may become obscured fig 4 fig 5 for instance plotting many sites or locations or plotting dense datasets can produce overlapping values that may be poorly visualized to combat this the most commonly used strategy is to plot the shape outline with an empty interior fig 4a fig 5a while this strategy may be effective for intermediately sized data the intended outcome of ensuring that all values can be visually interpreted can be difficult as the number of values to be visualized increases as an alternative there are several ways to preserve visibility of all data points in figures displaying large datasets plotters can vary the size of attributes transparency e g kelleher and wagener 2011 or create inset figures where individual points can be distinguished from one another however transparency may not be a solution for displaying density across large volumes of data fig 4b plotting that does not enable the viewer to distinguish all points or values should be avoided as this approach may obscure outliers density or the interpretation of overarching relationships within a dataset preserving raw values encoded as points or lines can be especially useful when the goal is to highlight outliers or a particular subset of observations within a particular dataset from a data science perspective outliers are often an important source of information using a strong color contrast or changing size or shape enables perception of this group or set of outliers as compared to the rest of the dataset fig 4d such an approach can also be used with subplots or facets to highlight multiple sub groups and to emphasize how they relate to the larger dataset as we show in fig 4c aggregation can be useful for conveying where values are concentrated such as the conclusion from fig 4c that most streamflow records occur in moderately sized rivers with record lengths of between 30 and 60 years however as shown in fig 4d when this information is aggregated the individual data points are lost instead our plotting of outliers shows how streamflow record length varies with watershed drainage area aiding in the conclusion that larger watersheds typically have longer record lengths it can be especially challenging to visualize raw values when all data points are plotted along a single axis e g boxplots or violin plots parallel coordinate plots jittering data values which creates slight offsets can be helpful when points are used as an encoding attribute when lines are used as an encoding attribute e g parallel coordinate plots de cluttering strategies may include use of transparency or bundling raseman et al 2019 3 2 2 consideration 2 aggregation to emphasize patterns enabling perception of all values may not be possible for visualization of large datasets in this case aggregation may be used to summarize values aggregation can enter the visualization process either after a plot type is selected prior to selecting a plot type or as part of the iteration when selecting a plot type to use approaches to data aggregation will depend on the type of data you are using and in what way you seek to aggregate when aggregating a dataset for visualizations you must first decide how you would like your output data to be organized this requires considering how you will group your values quantitatively or categorically second you must decide what statistic you will use to transform many values to one value within your groups aggregation may occur during plot creation such as with a density based plot fig 4c but often happens prior to plot generation with the goal of condensing data to be visualized in these contexts aggregation may be used to address technical challenges encountered when trying to plot a large volume dataset and or may be an approach to simplify the plot itself and the overarching message such as when summarizing spatio temporal datasets in these cases the choice of a statistic for aggregation will depend on the overall plot message for instance frequency is used to highlight density statistics available for aggregation include but are not limited to the frequency or count mean median maximum minimum and variance of a dataset during this process decisions regarding how to group data are especially important sometimes these groups may be evident within the dataset such as countries cities watersheds or species while others may require choices in these instances we encourage transparency to describe such choices and justification in the figure caption when working with spatially distributed data additional decisions are required during aggregation aggregation requires the selection of a window or footprint size and shape as we chose to do in fig 5b it may be easy to assume a certain footprint size e g municipalities counties or other geographic boundaries or more challenging in some cases e g geolocated reports of flooding area of hurricane cover we note that the subject of how best to represent and visualize a footprint is also an interesting and open ended question these selections can bias the interpretations gathered from a particular dataset and should be clearly indicated in the figure caption similar decisions are encountered when using non spatial bivariate plots aimed at highlighting density as a third dimension plots that aim to highlight density have commonly used transparency e g kelleher and wagener 2011 raseman et al 2019 but this approach falls short for very large and or very dense datasets fig 4c one option that can be used to visualize density in large datasets is the use of color to indicate density fig 4c fig 5c or to show density groups that highlight the fraction of a dataset across the figure space see example from harrison 2017 3 2 3 consideration 3 when possible show raw values and aggregated information one of the most common ways to visually contextualize or compare large datasets to use a plot that shows distributions these types of plots represent succinct ways to summarize large volume datasets while preserving the dataset statistical properties of the many plot types that exist for showing distributions two of the most common are bar plots and box plots fig 6 a krzywinski and altman 2014 mcgill et al 1978 tukey 1977 however there is growing evidence that shows both of these plot types can be misleading matejka and fitzmaurice 2017 weissgerber et al 2015 this confusion arises because different dataset distributions may contain similar or even equivalent summary statistics given bar plots and box plots primarily show summary statistics medians interquartile ranges and 95th and 5th percentiles for box plots and median or mean plus standard error or confidence intervals for bar plots two similar plots may incorrectly suggest that dataset distributions are equivalent this problem is even more pronounced with bar plots that use a bar to represent the mean or median of the data and lines to indicate standard error or confidence intervals weissgerber et al 2019 three alternative plots for large datasets that preserve distribution shape are density plots violin plots and a new combined approach termed raincloud plots allen et al 2019 density plots can be strong alternatives to boxplots when the goal of a visualization is to show volume but not variety e g multiple groups overlaid density plots can summarize density for a small number of groups however it may become hard to distinguish between groups for more than three to four categories wickham 2010 depending on the degree of difference between the distributions as the number of series comparisons grows subplots should be used to break out individual groups one alternative to density plots for comparing multiple groupings with large volumes are violin plots which are essentially mirrored density plots fig 6b the myriad of violin plot iterations also enables encoding summary statistics alongside the distribution to preserve both types of information however there is an argument to be made that violin plots may include redundant information through mirroring allen et al 2019 raincloud plots are a different type of approach that combine visualization of the distribution showing an aggregated distribution and individual data values allen et al 2019 while these three approaches represent endmembers in the visualization of distributions many other iterations of these types of plots exist for instance one iteration is to combine a barplot and violin plot fig 6c hintze and nelson 1998 enabling interpretation and comparison of summary statistics and overall distribution in addition to the variant shown in fig 6c other variants include beeswarm plots eklund 2015 2016 a re envisioning of the dot plot wilkinson 1999 and beanplots kampstra 2008 2014 one question that may arise when considering plotting distributions if the goal of a plot is to highlight the distribution of the data should we just be plotting the raw data the answer here is an emphatic no estimating distributions and statistics from raw data is notoriously challenging bobko and karren 1979 spence et al 2016 3 3 decision making in the context of dimensionality large datasets are often high dimensional either in terms of the variables they contain or in terms of how those variables are categorically or quantitatively grouped therefore selecting the number of dimensions to display within a given plot is often challenging with so many potential encoding attributes to add spatial location shape width size and color to name a few it is easy to overcomplicate at the same time as the volume and variety of data encapsulated within scientific visualizations grows plot complexity in terms of dimensionality volume of data encoded and composition is certainly growing though simplicity should still be the ultimate goal of any visualization this does not have to be in conflict with employing a visualization that exceeds three dimensions that shows an exceptional volume of data or that combines multiple subplots into a single visualization 3 3 1 consideration 1 balance the number of dimensions you show with overall plot simplicity decision making surrounding the choice of a plot the number of dimensions to display and a key plot message are inherently linked giving thought to how these pieces work together from an early stage is therefore important to creating an effective visualization when making decisions regarding the number of dimensions you seek to display it is important to remember that encoding attributes inherently limit us to just a few dimensions two continuous variables for positions on a bivariate plot one continuous or categorical dimension for color and or size and one categorical dimension for shape therefore many plot types support displaying anywhere between two and five dimensions though some plots such as the parallel coordinate plot and the rose plot can display many more dimensions however as the old adage goes just because you can doesn t mean you should additionally the goal with any plot should be to avoid redundancy as shown in fig 6c color is redundant with labeling on the x axis in fig 7 and fig 8 we explore the catchment attributes and meteorology for large sample studies camels addor et al 2017a addor et al 2017b dataset using a series of figures moving from two dimensional fig 7a c and three dimensional fig 7b visualizations to higher dimensional visualizations such as parallel coordinate plots fig 8 in figs 7 and 8 below adding higher dimensions and showing variety across hydrological signatures fig 8 presents a clearer picture of how watershed behavior is organized across the us fig 8a and to what extent behavior is similar within a larger basin fig 8c however as additional elements are added to the plot such as shown in fig 8b it becomes difficult to extract useful patterns and to compare across multiple dimensions 3 3 2 consideration 2 is the number of groups or series in a single visualization manageable and discernible while our plotting is often limited by dimensions it is not inherently limited by quantitative or categorical groupings these groupings are regularly used when visualizing large datasets to emphasize comparisons between quantitative or qualitative groups comparison is at the heart of understanding trends or differences in data visualization must make comparison between groups easy to interpret tufte et al 1990 grouping is an approach for reducing dimensionality that enables assessment of similarities and differences across dataset subsets when plotting large datasets we often use grouping with colors symbols or sometimes both to show organization within a complex multi dimensional and or large volume visualizations however when the focus is on comparison of these different groups it can be easy to overwhelm when the number of groups shown plotted concurrently not side by side begins to exceed three wickham 2010 in these types of plots aimed at showing many groups all values may be plotted within the same visualization or may be separated into subplots or insets the latter can be an effective way to highlight a subset of the data in the context of the broader dataset in fig 9 we show examples of hydrologic simulations produced from four different models and compared to observed streamflow for one watershed when plotted together the timeseries are hard to distinguish from one another fig 9a even on a logarithmically transformed axis fig 9b separating each of these comparisons into subplots more clearly illuminates the periods when simulated streamflow is in agreement with observed values fig 9c in line with ensuring the number of groups or series to be compared are interpretable is giving thought to how these groups are organized within a visualization when you must specify the order of such groups e g in a heatmap a parallel coordinate plot or a distribution based plot the choice of how to order components of your plot matters this ordering should be done intuitively such as from small drainage areas to large when comparing watersheds e g fig 6 by magnitudes in rank plots fig 7c or potentially by mean values in heat maps 3 3 3 consideration 3 make complexity work for you one of the best ways to simplify large volumes and varieties of data is by using synthesis plots here we define a synthesis plot as any type of plot that combines multiple graphical approaches and encoding attributes by this definition many of the best visualizations today combine multiples of plot types to convey key messages by nature synthesis plots are complex summarizing multidimensional datasets with multiple encoding attributes points lines color arrows they may incorporate symbols often make heavy use of color and include strong labeling and text throughout exceptional examples of such plots include circos diagrams fig 10 a sankey diagrams table based diagrams treemaps fig 10b and ordered bar charts while these visualizations are complicated what often makes them successful is that the creators give narrative to these plots tufte et al 1990 through captions labeling and either verbal e g during a presentation or described in an animation or written descriptions e g captions or manuscript or article text if all else fails your caption should be able to explain your figure if it can t your figure is probably too complicated and needs revision complexity does not necessarily detract from interpretability synthesis plots represent an exceptional opportunity for visual communication 3 4 challenges and opportunities in the use of color color is central to the creation of scientific visualizations zeller and rogers 2020 while color can mislead the reader when interpreting figures ware et al 2008 samsel et al 2018 removing color as an encoding attribute is not always feasible when using color there are several approaches that can be used to make visualizations clear and easy to interpret 3 4 1 consideration 1 use care with color based pattern plots and color based comparisons color is often used in data visualization to differentiate between groups outliers and across scales although color can be useful in the right context it is difficult to differentiate between colors of the same intensity or saturation samsel et al 2018 ware 2008 additionally assigning a scale to a color gradient or understanding how far apart two colors are on a scale are difficult for readers to interpret i e gradients force the readers to do visual math in this vein using color contrast can distort the reader s view of the data displayed when not used properly in visualization samsel et al 2018 for example in gradients of data ratios of hues red to green are about equal but hues orange to blue are about 1 2 for an equal gradient of data this color inequality is particularly important when areas are displayed because our brains will try to bring the complementary colors into balance and misjudge the aerial extent of a color samsel et al 2018 and are not appropriate for displaying categorical variables i e categories of vegetable should not be displayed on a white to red color gradient when it comes to visualizations that rely on color to support interpretation the widely used heat map is particularly difficult to assess because colors can look different depending on what colors are surrounding them albers 1975 in addition colors like red can dominate other colors leading to interpretation that there is more or extensive red in relation to other colors fig 6a often referred to as contrast of extension alternatively it is difficult to compare colors with increasing distance or blank space between the colors to top all of these issues off color blindness is a serious limitation in interpreting figures including but not limited to heatmaps because color can mislead readers we suggest that color is used carefully and after other alternatives in big data visualization there are many other encoding attributes that can be used for differentiation between groups or scale alternatives include shape line weight size and length 3 4 2 consideration 2 is it useful to add color to highlight an overall pattern or to a distinct group of variables using color to convey an additional dimension within visualizations of large volumes of data can be an effective way to convey additional information typically the intent of using color in these types of visualizations is to either 1 convey an overarching pattern e g figs 4c and 5b c fig 7b and c fig 8a and b or 2 to display outliers or a categorical group fig 4d fig 8c however a plot that tries to convey both patterns and outliers may easily overwhelm perception of key messages or interpretation of the visualization the major challenge to using color to visualize patterns for a large dataset is that there are many aspects of a visualization that can obscure our ability to discern a given pattern albers 1975 samsel et al 2018 care should be given to ensure that all values can be distinguished importantly the choice of a color bar should enable interpretation of the perceived pattern an alternative to using color may be to use another type of differentiator e g shape that is easier to interpret an alternative to visualizing patterns is to use color to highlight distinct groups of variables e g details this type of visualization may be employed to identify a statistical grouping such as outliers a categorical grouping such as different types of values e g watersheds within the same sub basin fig 8c or a research based outcome such as groupings from hierarchical clustering or the like as opposed to a typical pattern based plot this type of plot may simplify the number of colors being quantitatively perceived across a large volume of data 3 4 3 consideration 3 when using color choose color schemes or gradients that allow for contrast in hue value and saturation color theory represents an extensive study of how colors interact how best to mix colors and how best to use colors within visualizations of any kind albers 1975 the basis of color theory is the color wheel which is used to identify colors that may work together to make visuals appealing and to provide contrast rhyne 2012 as defined by color theory there are three aspects of color hue color wavelength e g blue or green saturation depth of the color and value grayness of the color these aspects of color may be used to generate a color scheme or gradient for a particular plot many different combinations of colors that incorporate color theory already exist a complementary color scheme corresponds to colors that exist on opposite sides of the color wheel an alternative to a complementary color scheme is a split complementary color scheme which uses a base color along with two colors on opposites sides of the first color complement i e the color directly across from the base color on the color wheel an analogous color scheme is when three colors are adjacent to each other and a triadic color scheme is when three colors are equally spaced around the color wheel color schemes employing complementary color schemes as end members of a gradient are often called divergent color schemes frequently with white or a neutral color in the middle schemes with colors of a similar hue or within an analogous color palette are commonly called sequential color schemes i e light green grass green dark green the best way to make or select a color scheme that is easier to differentiate and distinguish is to leverage the three different aspects of color hue saturation and value by employing at least two aspects of color such as saturation and value visualizations can appear more interpretable ware 2008 likewise combining colors of different saturation or value with changes in hue can make colors more distinguishable particularly for viewers with color blindness colors appropriate for visualization are frequently difficult and overwhelming to choose for those who may be new to the principles of color theory there are several web based resources to assist with selecting a gradient or categorical color scheme table 1 includes a list of these resources that can be used for developing a color palette 3 4 4 consideration 4 avoid rainbow color scales and limit the number of categories to enable interpretation while this recommendation is widely known it still bears repeating despite their broad proliferation rainbow color ramps use constant saturation and value only varying in hue borland and taylor 2007 therefore it is very difficult for colorblind individuals to distinguish between colors on a rainbow scale in addition the gradient includes all colors on the visible spectrum making it hard for viewers to perceive which colors correspond to a positive or negative value this aspect also makes it challenging to interpret which part of the scale the color on the gradient represents borland and taylor 2007 to emphasize the problems with this particular color scale we include a particularly horrific example in fig 11 a to avoid the rainbow color scale pick a type of contrast that best fits the visualization samsel et al 2018 cool to warm contrasts such as blue to red or yellow are often good for scales with gradients from low to high additionally using single color gradients is a strong approach for producing easy to discern gradients humans are biased to prefer balanced gradients with either cool to warm or complementary contrast in a gradient albers 1975 limiting colors to seven categories or less helps the viewer interpret gradient or categorical color scale e g fig 5c fig 11 3 4 5 consideration 5 think beyond just colorblindness colorblindness is not the only visual disability that affects the interpretation of visualizations low vision individuals may find it difficult to read or interpret details in a visualization often increasing the contrast of the colors used in the visualization or increasing font size can make the figure more accessible to those with low vision power and jürgensen 2010 in addition rasters are inaccessible to the visually impaired because the components of the figure are hard to separate choi et al 2019 often those with visual impairments will use computer reading software to get information from a manuscript to provide more information make sure the caption in the figure is accessible to reading programs and fully describes the graph and the results accessibility can be increased by considering creative choices to explain visualizations such as by adding supplemental movies or audio that describe the study or the visualizations power and jürgensen 2010 finally there are new frontiers opening up to make visualizations more accessible for instance machine learning approaches are being tested to identify individual elements of a visualization to pass to a reading software choi et al 2019 3 4 6 consideration 6 use color to tell a story go beyond best practices for a single visualization throughout a presentation or a text whether a manuscript interactive video blog post or other published article colors not only tell the story of one visualization but the larger narrative of the manuscript to keep the message and narrative consistent it is important to keep a consistent color scheme throughout particularly for visualizations that use the same components e g variables parameters groups as discussed above picking colors that are easy to distinguish that visually represent the figure s trends and that can be interpreted by all audiences is the most important aspect of choosing a color palette for a presentation or manuscript in particular it s important to select color palettes that do not violate or challenge cultural or archetypal expectations and that support and correspond to the data being shown e g fig 10b versus fig 10c for example do not use warm colors for water or snowfall or cool colors for fire frequency when using a dataset that is known to have an associated color palette employ this same color scheme e g fig 11 displaying land cover data with a previously created and widely used color palette if choosing a color palette fills you with dread the easiest direction is to pick an already existing color palette that is good for color blind persons and printing grey scale i e viridis there are also really fun ways to dream up color schemes such as using movie color palettes see twitter account cinemapalettes or movies in color https moviesincolor com 4 from static to interactive the fourth v of big data velocity refers to the speed and temporal resolution at which data is collected both of which are accelerating and are a challenge to visualize and represent we can work with this type of data through two methods summarizing such information in static formats or creative visualization techniques such as animation in this particular piece we focus less on velocity as a visualization need though note that the importance of visualizing velocity will likely continue to grow in the near future common approaches to visualize velocity in static formats include visualizing time using color or grouping values and using time as an organizing dimension for instance heat maps or barcode plots are commonly used to convey temporally resolute datasets at one or many locations other plotting options include the use of spatial snapshots e g showing temporal plots for a particular point slice or volume or temporal snapshots e g showing spatial plots for a particular time however it can be quite challenging to convey velocity in static visualizations at current more and more journals offer the option to upload animations as supplementary material journals are also beginning to develop visualization based submissions e g hpeye within the journal hydrologic processes for those interested in such options table 2 lists several packages in r and python that can be used for creating animations or interactive graphics while the scientific literature largely draws on static visualizations interactive visualizations are becoming increasingly common for science communication to a range of technical and non technical audiences though the focus of our article is primarily static visualizations we briefly summarize considerations for interactive visualizations interactive visualizations can be used for a variety of purposes they may be helpful in the data exploration phase or may accompany peer reviewed manuscripts as most journals do not support interactive formats it is important to remember that creating a static version of the figure is often necessary for a manuscript however such interactive visualizations are an option for supplemental or accompanying websites interactive graphs lead to more complexity and additional decisions while there are many types of interactive visualizations the most common examples involve 1 changing choosing a dataset or a unit of analysis in a visualization 2 filtering or querying a dataset in the visualization 3 toggling visualization features such as variables or colors 4 combining or separating visualizations 5 interactive annotations that provide more information and 6 zooming in and out or changing the level of detail walsh 2020 approaches to parse interactive graphic components are now being described through grammars of interaction e g vega https vega github io vega vega lite https vega github io vega lite which provide a framework for approaching the creation of interactive visualizations in addition many common scientific programming languages are facilitating the creation of interactive web apps e g r shiny https shiny rstudio com with these common types of interactive visualizations come pitfalls the following are a brief list of recommendations to consider when creating interactive visualizations 1 ensure that the static version is explanatory as most people will not interact with the figure 2 consider whether interactive features can be engaged and used on all types of devices e g individual points can be difficult to interact with on touch devices and 3 make the interactive components of the visualization well paced and manageable as opposed to too cumbersome or slow to maintain interest with your audience walsh 2020 it is likely that the tools and recommendations for best practices concerning interactive visualizations will continue to evolve especially given the growing interest in the creation and use of interactive visualizations across the scientific community 5 conclusions conveying large datasets within publications or presentations is challenging especially when it comes to visualizing large amounts of data the visualization of large complex datasets requires rethinking our common assumptions and approaches for creating plots in particular when visualizing large datasets researchers must make many decisions before arriving at a final plot these decisions include whether to show all values or to aggregate how to convey multiple categories or comparisons and how to display multiple dimensions all in succinct easily interpretable ways our introductory overview provides several recommendations such as choice of plot type encoding attributes and groupings to aid in the creation of clear visualizations thinking carefully about these choices can enhance visualization quality and messaging however whether or not our recommendations apply in any particular case will ultimately depend on the overarching message conveyed by each visualization and the size and character of the associated dataset though we often focus on encoding attributes and plot choice when approaching visualizations equally important is to give narrative to our plots using notations legends and a clear caption above all when creating plots of large datasets iteration is key be sure to ask for input along the way ensuring that the key takeaways intended for a particular visualization are clear to others scientific visualization is something that we all have very strong feelings about we therefore emphasize that our recommendations are just that not hard and fast rules that should be unilaterally applied in all scenarios be creative thoughtful and intentional with your designs and use your best judgment along the way declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors acknowledge funding from the cuse grant program at syracuse university to ck and a national science foundation award 1940006 to ab all data used in the creation of scientific visualizations are publicly available and cited in the text all code for visualizations are included in a github repository at https github com abraswell bigdataviz the article was improved by thoughtful comments from two anonymous reviewers appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105113 
25795,scientific visualizations are the foundation for communicating results and findings to a variety of audiences as the creation of novel and large environmental datasets has grown this has necessitated new schemes and recommendations for creating effective visualizations in this overview we review the foundations of scientific visualization and considerations for visualization of large datasets within the context of the four vs of big data volume variety veracity and velocity using big datasets requires making decisions as to whether to aggregate or preserve details approaches for grouping to enable comparisons and considering how best to show complex data in many dimensional space to enable more effective visualizations we provide several considerations regarding common decisions faced during the visualization process these recommendations are accompanied by examples applied to existing large datasets while our recommendations are just that they encourage intentionality and awareness of the choices faced when visualizing scientific datasets keywords scientific visualization visual communication plots graphics multidimensional visual analytics 1 introduction visualization is one of the foundational mechanisms used to communicate science visuals help us make sense of complex problems and interact with information kirsh 2010 liu and stasko 2010 scaife and rogers 1996 more specifically visuals aid in decision making deitrick and edsall 2006 kinkeldey et al 2014 2017 learning gordin and pea 1995 höffler 2010 höffler and leutner 2007 yang et al 2003 and science communication desnoyers 2011 in the past several decades the creation of environmental datasets skyrocketed this trend emerged for several reasons in general large datasets are more widely available because of technological advances resulting in constantly improving computing abilities enabling analysis and modeling to be performed at higher spatial and temporal resolutions over broader spatio temporal domains these technological improvements contribute to growing volumes of data and shrinking costs of in situ alam et al 2020 murphy et al 2015 parra et al 2018 wickert 2014 wickert et al 2019 and remote sensing technologies zhang et al 2019 and new often open source analysis tools gorelick et al 2017 vos et al 2019 in addition to the generation of new data support for providing public access to datasets used in publications has also increased the scientific community continues to show broad interest and support for reproducibility and open science baker 2016 munafò et al 2017 sandve et al 2013 stagge et al 2019 journals and funding agencies are precipitating these efforts through the creation and maintenance of online repositories and requirements to store data of various types finally collaboration has spurred the generation of new large datasets through model intercomparison experiments baroni et al 2019 best 2019 krysanova et al 2017 maxwell et al 2014 smith et al 2004 open source coding packages decicco et al 2020 fuka dr et al 2018 slater et al 2019 new journals aimed at publishing large and unique datasets e g scientific data earth system science data community based data collection e g ameriflux phenocam and citizen science datasets e g crowdwater stream tracker all of this amounts to a diverse sometimes overwhelming and altogether impressive collection of data now at the fingertips of the earth ecological and environmental science and engineering communities the acceleration of data availability entails the growth of the spatial temporal and uncertainty dimensions of environmental data contained in publications and presentations to borrow a buzzword this means many publications are now making use of and visualizing big data while there are numerous definitions of big data the criteria for defining big data generally is associated with dataset size and complexity as well as the need for advanced tools or technologies to interact with such datasets chang and grady 2019 ward and barker 2013 while the line where data becomes big is unclear any dataset by virtue of its volume e g size variety e g different types of data or variables veracity e g uncertainty and velocity e g speed at which data is collected may fall under the heading of big data farley et al 2018 these different attributes termed the four v s of big data introduced by ibm in the 2000s can complicate visualizations and visualization goals yang and huang 2013 though many recommendations exist for how to best use scientific visualizations in publications and presentations few 2009 kelleher and wagener 2011 rougier et al 2014 tufte 1990 2001 weissgerber et al 2019 the growing volume and variety of data synthesized by researchers necessitates augmenting existing recommendations to consider the technical and aesthetic challenges associated with the visualization of large datasets as highlighted by liu et al 2017 there are numerous decisions to be made especially when visualizing high dimensional datasets large datasets are cumbersome and present technical challenges to data wrangling the transformation of raw values into a form that can be leveraged to address research objectives though many of general principles that were famously introduced by edward tufte in the 1970s and 1980s still apply to a visualization regardless of the amount of data contained within how best to meet those recommendations as well as how to approach decision making when creating visualizations with large datasets remains a common challenge colloquially visualizations produced over the last decade include more raw data data points data series and more variables visualizations that move beyond 2d into 3d and higher dimensional space are now common to date there has been much attention given to computational processing analysis and user interfacing and interaction liu et al 2017 however there has been much less attention given to how best to effectively convey information in visual form to address this need we deliver a set of approaches and recommendations paying heed to potential pitfalls for visualizing large environmental datasets existing recommendations regarding scientific visualization generated over the last few decades serve as a sound basis for evaluating the effectiveness of any visualization our recommendations serve to augment these sound principles in the context of big data visuals analysis and presentation of large datasets in many ways stretch the limits of traditional recommendations for visualization therefore our focus is specifically on visualizing large volumes and varieties of data to assist in the analysis synthesis and comparison of large datasets for presentation and publications 2 challenges posed by large environmental datasets large environmental datasets present major challenges when it comes to developing succinct easily interpretable and visually aesthetic plots these difficulties arise from two sources technical challenges introduced by computational constraints when visualizing a large dataset and the decision making that is involved in how to best display and convey large datasets these challenges are best expressed when considering the major characteristics of big data also known as the 4 vs large datasets often have large volume many values large variety many types of data and inherent but challenging to communicate veracity the fourth v velocity we describe in more detail in a later section here we interpret this fourth v to refer to the dynamic nature of many large datasets that may often be best conveyed using animated or interactive approaches however the majority of our overview focuses on static visualizations as these are still the major currency of visual communication below we outline the major challenges introduced by three of the four vs when it comes to approaching data visualization with large datasets that may fall into one or multiple of these categories 2 1 challenge 1 large datasets are unsurprisingly big the sheer volume of large environmental datasets introduces several considerations for visualizations beyond posing technical challenges while there are many examples of voluminous visualizations there is a tension between ensuring a visualization shows broad patterns and the distribution of the data while at the same time allowing a reader to identify all of the data or the most important data too often we synthesize and remove key pieces of information through aggregation yet this may also be a necessary step to pursue scientific questions that span spatio temporal scales 2 2 challenge 2 large datasets often contain variety variety in large datasets refers to the inclusion of different types of data categories of data or different variables or descriptors a common challenge in large datasets with exceptional variety is how to best display multi dimensional data to show broad relationships across many variables or descriptors likewise plots that highlight variety often deal with multiple categories and comparisons complexity should not be avoided when creating such visualizations though it can be challenging to walk the line between clean visuals and overcomplicated visuals when displaying datasets with large variety 2 3 challenge 3 large datasets are frequently used to communicate veracity veracity is often interpreted as data uncertainty here we broadly interpret this term to refer to all types of uncertainty variability and comparisons between values to determine veracity plots concerned with veracity may be used to show aggregated metrics such as objective functions jackson et al 2019 uncertainty error probabilities or confidence these approaches often rely on comparison to a baseline e g modeled uncertainty applied to a timeseries plot error bars applied to bar chart or dot plot or feature error as a derived value e g boxplot or violin plot of errors bar chart of difference from true or zero communication of veracity can be especially challenging spiegelhalter et al 2011 as emphasized by the misinterpretation of common graphics used to communicate uncertainty such as the hurricane cone of uncertainty boone et al 2018 3 decision making for visualizing large datasets the term visualization can be ambiguous it may refer to a tool being used to create or generate a visualization to the process of creating a visualization to the analysis of data or to a generated visual parsons and sedig 2014 in this article we use the term scientific visualization to refer to visual representations of datasets in the literature two common types of visualizations exist glyphs and plots glyphs e g multidimensional icons combine multiple encoding attributes into symbols or graphical representations e g chernoff faces chernoff 1973 or infographics in contrast plots display datasets using coordinate systems we focus specifically on the creation of scientific visualizations as plots though note that many of our recommendations also apply to glyphs generating a visualization from a large dataset introduces both technical challenges as well as several often somewhat subjective decisions that must be made to generate a visual display when considering how best to approach visualization of a large dataset there are four central questions that must be answered when creating a visualization fig 1 plot type or the decision to use multiple plots which visualization s will you use to display your data raw values or aggregation is aggregation needed or should viewers see raw values dimensionality how many dimensions do you need to display color are you using color and are you using color wisely in the sections that follow we present common challenges or pitfalls when using traditional visualization techniques and considerations and recommendations for how to re envision these plots in the context of these four key decisions we also envision these decisions in fig 1 as a series of steppingstones to arriving at a final plot amongst these recommendations we qualify that this overview is by no means represents an exhaustive list of all considerations when plotting datasets whether small or large but serves as a starting point for thinking about visualizations in the context of large datasets importantly these recommendations are not intended to be applied in isolation instead they are complimentary ideas that should be used to identify how visualizations of large datasets may be approached or improved using these decisions as a guide we include examples created from existing large environmental datasets these include the gagesii dataset falcone 2011 falcone et al 2010 land cover change data for alaska from the national land cover dataset homer et al 2015 national land cover dataset 2020 and the continuously updated digital elevation model dataset cooperative institute for research in environmental sciences cires at the university of colorado boulder 2014 all visualizations were created within rstudio v 3 6 3 and code is available on github 3 1 choosing a plot type encoding attributes and overall visualization approach at a basic level a visualization is composed of encoding attributes scales and coordinate systems wickham 2010 scientific visualizations rely on the selection of encoding attributes also known as visual encodings or visual marks these attributes are used to convey quantitative and qualitative information within the context of a visualization as summarized by few 2009 attributes include those associated with form e g length width orientation size shape curvature enclosure and blur color hue value saturation transparency spatial position 2 d position spatial grouping or density and motion direction path scales are used to encode information using attributes associated with form size and color they may be quantitative e g color size or categorical color shape coordinate systems provide a means of assessing spatial position coordinate systems may be cartesian logarithmic on one or multiple axes polar r θ or multidimensional these building blocks of scientific figures ultimately come together in a visualization while it is sometimes helpful to think about these individual pieces perhaps more important is to consider the overall plot type as this is one of the most crucial choices faced in the visualization of a large multi dimensional dataset this decision is an inherently subjective choice but can benefit from keeping in mind the overarching plot goal or message what is the main message you wish to convey this choice will ultimately determine how many dimensions you seek to encapsulate within your plot which then will help to identify what plot types are at your disposal regardless of the big picture selection of a plot type the details associated with the plot building blocks are equally important within the open source programming language r these components are often described and implemented as the grammar of graphics wilkinson et al 2005 wickham 2010 though not all programming languages or tools implement a graphical grammar the grammar introduced by wilkinson et al 2005 and refined by wickham 2010 is helpful for identifying the choices faced and the refinements that can be used in the process of generating a graphic such details are crucial for refining visualizations 3 1 1 consideration 1 match your plot type and encoding attributes to your key message visualizations are built upon the selection of encoding attributes and the choices made in the selection of components of a figure while we are all aware of the components that are used to build a visualization the selection of these components is a key development step in creating a visualization visualizations of any type should begin with identifying a key message aimed to be conveyed with the visualization from this message we can select a particular plot type scale and coordinate system built on the selection of encoding attributes to display quantitative information or qualitative groupings kelleher and wagener 2011 writing out a key message or the visualization take away can be a good place to begin especially when parsing components of a complex visualization for example does the reader need to compare groups or categories to determine the key message likewise revisiting these choices during revision of a given visualization can help to clarify the message conveyed by a particular plot though visualizations are unique to the dataset and creator there are several common key messages that visualizations seek to highlight these include connections between values i e flow the distribution of a dataset data density including spatial density geospatial position magnitude outliers part to whole i e hierarchical or layered datasets patterns rankings relationships i e correlation timeseries and uncertainty these common themes may represent a starting point for designing a plot to convey a key message in fig 2 we show several cartoon examples of multidimensional visualizations and highlight common key messages or themes that may be conveyed using each of these plots we do also highlight that many figures may be composed of multiple plots aimed at showing groupings relative to the larger dataset or other groups also called facets wickham 2010 or groupings relative to subsetted data groups we encourage visual creators to remember that there are a multitude of different approaches and types of plots that can be used to visualize data for inspiration we direct you to several impressive summaries including the visual vocabulary smith et al 2019 the data visualization catalogue ribecca 2020 and the graphic continuum schwabish and ribecca 2014 in particular the graphic continuum highlights six key plot groups distributions time comparing categories geospatial part to whole and relationships 3 1 2 consideration 2 pay heed to overall composition as you finetune your visualization as discussed above visualizations inherently consist of many different components that must work together to tell a story how best to arrange these components such that they most clearly articulate a key message can be thought of as composition the composition of a plot includes selection of a color palette the use of annotation through legends direct labeling and other words included on the visualization including the caption and the choice of plot and how the plot is designed visualizations often include annotations text or enclosures used to highlight or explain features of the visualization beyond the caption annotations are a way to use text or other visual cues to direct the eye of the reader and to aid interpretation annotations also encompass the figure legend that is used to describe a qualitative and or quantitative scale ensuring a strong composition requires attention to annotations which enhance a viewer s understanding of a given visualization composition also includes the creation of what we will call mega figures composed of many subplots or facets though a single visualization may feature one plot visualizations particularly those of complex datasets may also include a composite of many small multiples tufte 1990 also known as subplots matlab or facets r the combination of small multiples may be used to provide additional detail regarding a component of a dataset and can be especially useful for parsing and displaying subsets of a large dataset in the literature small multiples are commonly used to parse a single dataset often using a repeated coordinate system encoding attributes and scale but varying the data displayed enabling visualization of high dimensional data however these subplots or small multiples can also be superimposed on larger plots to display different types of data e g spatial versus temporal versus categorical or to visualize data at different scales in composing a plot we encourage readers to think beyond generating a single plot to producing an integrative visualization that may be composed of many plots and plotting elements 3 1 3 consideration 3 give thought to how you can simplify and clarify for your key message across large environmental datasets that may exhibit large volume or variety there are several common approaches to simplifying such visualizations that may clarify the overarching message of a particular plot aggregating large volumes into simple distributions or statistics multiple values into indices single points into footprints combining multiple types of data into multidimensional plots highlighting outliers certain groups of data trends a single observation when designing a plot it is important to consider these options for creating a clear and concise visualization finally ensuring that key message is clear and perceivable by others is one the most important considerations when creating a visualization generating a useful and effective visualization not only requires that you have in mind what the goal of your plot is and how you want to use encoding attributes color shape width size orientation few 2009 kelleher and wagener 2011 to convey key messages but also that this key message is perceivable by others 3 1 4 consideration 4 aesthetics are important think beyond just color visualizations are as much science as art often we associate color with aesthetics so much so that we have dedicated an entire section of this overview to the discussion of color however aesthetics of visualizations go far beyond color alone during the visualization process give thought and attention to the details annotations font size font type legend placement axis widths tick mark spacing for publication quality graphics many journals may have recommendations for particular font sizes or types to use and may specify the location inside the axis or outside the axis for tick marks helvetica and arial are often preferred fonts when creating visualizations in addition there are several details that can be used to improve the overall interpretation of your visualization fig 3 these include enclosure e g to highlight data points that meet a certain p value arrows e g to show directional connections annotation e g to explain or label an unusual or exceptional data value or sets of data values and transparency e g when elements overlap overall attention to these small details can be used to improve the overall aesthetics of your visualization 3 2 preserving individual values versus transformation or aggregation often the analysis of a large dataset begins with visualization of raw untransformed unaggregated data on the path to presentation and publications this data is often repackaged in different ways within visualizations this re packaging often includes the use of transformations and the use of aggregations transformations depending on the visualization tool may be applied to the data to the scale or to the coordinate system wickham 2010 when applying transformations to scales or coordinate systems clarity and communication is key this requires attention to and use of tick marks legends and even the figure caption visualizations may also rely on statistical transformations that aggregate or alter data in some way this includes data binning as is done when plotting distributions or density data jittering data smoothing or categorial or other groupings applied to datasets while transformations and aggregations are a necessary part of visualizing large datasets they can also alter the perception of the data and the visualization one existing tension in the visualization of large datasets is whether or not it is important to show all values in a given visualization or whether these values should be aggregated this decision depends on a few factors particularly the size of the dataset consideration 1 and the approach to aggregation consideration 2 but should also be viewed in the context of which approach produces a clear visualization that enables viewers to perceive the overarching plot message several plot types including scatter plots spatial scatter plots and parallel coordinate plots are used to enable readers to quantitatively perceive all values within a dataset humans have a remarkable ability to lump or categorize visual information so often preserving information while highlighting the main or macro pattern is key for effective visualization tufte 1990 as stated by tufte 1990 clutter and confusion are failures of design not attributes of information yet displaying all raw values may overwhelm or obscure trends variation or groups when it comes to large datasets showing all values may not be possible for high volume datasets e g a long timeseries or for many raw values for these situations aggregation is often necessary however it is important to keep in mind that aggregation can subsume extensive variability in raw values which can challenge interpretation of veracity in this section we highlight two considerations when making the decision regarding whether to aggregate or preserve raw data 3 2 1 consideration 1 can raw values be distinguished preserving the visualization of all points is particularly challenging for large datasets as the information contained in the plot may become obscured fig 4 fig 5 for instance plotting many sites or locations or plotting dense datasets can produce overlapping values that may be poorly visualized to combat this the most commonly used strategy is to plot the shape outline with an empty interior fig 4a fig 5a while this strategy may be effective for intermediately sized data the intended outcome of ensuring that all values can be visually interpreted can be difficult as the number of values to be visualized increases as an alternative there are several ways to preserve visibility of all data points in figures displaying large datasets plotters can vary the size of attributes transparency e g kelleher and wagener 2011 or create inset figures where individual points can be distinguished from one another however transparency may not be a solution for displaying density across large volumes of data fig 4b plotting that does not enable the viewer to distinguish all points or values should be avoided as this approach may obscure outliers density or the interpretation of overarching relationships within a dataset preserving raw values encoded as points or lines can be especially useful when the goal is to highlight outliers or a particular subset of observations within a particular dataset from a data science perspective outliers are often an important source of information using a strong color contrast or changing size or shape enables perception of this group or set of outliers as compared to the rest of the dataset fig 4d such an approach can also be used with subplots or facets to highlight multiple sub groups and to emphasize how they relate to the larger dataset as we show in fig 4c aggregation can be useful for conveying where values are concentrated such as the conclusion from fig 4c that most streamflow records occur in moderately sized rivers with record lengths of between 30 and 60 years however as shown in fig 4d when this information is aggregated the individual data points are lost instead our plotting of outliers shows how streamflow record length varies with watershed drainage area aiding in the conclusion that larger watersheds typically have longer record lengths it can be especially challenging to visualize raw values when all data points are plotted along a single axis e g boxplots or violin plots parallel coordinate plots jittering data values which creates slight offsets can be helpful when points are used as an encoding attribute when lines are used as an encoding attribute e g parallel coordinate plots de cluttering strategies may include use of transparency or bundling raseman et al 2019 3 2 2 consideration 2 aggregation to emphasize patterns enabling perception of all values may not be possible for visualization of large datasets in this case aggregation may be used to summarize values aggregation can enter the visualization process either after a plot type is selected prior to selecting a plot type or as part of the iteration when selecting a plot type to use approaches to data aggregation will depend on the type of data you are using and in what way you seek to aggregate when aggregating a dataset for visualizations you must first decide how you would like your output data to be organized this requires considering how you will group your values quantitatively or categorically second you must decide what statistic you will use to transform many values to one value within your groups aggregation may occur during plot creation such as with a density based plot fig 4c but often happens prior to plot generation with the goal of condensing data to be visualized in these contexts aggregation may be used to address technical challenges encountered when trying to plot a large volume dataset and or may be an approach to simplify the plot itself and the overarching message such as when summarizing spatio temporal datasets in these cases the choice of a statistic for aggregation will depend on the overall plot message for instance frequency is used to highlight density statistics available for aggregation include but are not limited to the frequency or count mean median maximum minimum and variance of a dataset during this process decisions regarding how to group data are especially important sometimes these groups may be evident within the dataset such as countries cities watersheds or species while others may require choices in these instances we encourage transparency to describe such choices and justification in the figure caption when working with spatially distributed data additional decisions are required during aggregation aggregation requires the selection of a window or footprint size and shape as we chose to do in fig 5b it may be easy to assume a certain footprint size e g municipalities counties or other geographic boundaries or more challenging in some cases e g geolocated reports of flooding area of hurricane cover we note that the subject of how best to represent and visualize a footprint is also an interesting and open ended question these selections can bias the interpretations gathered from a particular dataset and should be clearly indicated in the figure caption similar decisions are encountered when using non spatial bivariate plots aimed at highlighting density as a third dimension plots that aim to highlight density have commonly used transparency e g kelleher and wagener 2011 raseman et al 2019 but this approach falls short for very large and or very dense datasets fig 4c one option that can be used to visualize density in large datasets is the use of color to indicate density fig 4c fig 5c or to show density groups that highlight the fraction of a dataset across the figure space see example from harrison 2017 3 2 3 consideration 3 when possible show raw values and aggregated information one of the most common ways to visually contextualize or compare large datasets to use a plot that shows distributions these types of plots represent succinct ways to summarize large volume datasets while preserving the dataset statistical properties of the many plot types that exist for showing distributions two of the most common are bar plots and box plots fig 6 a krzywinski and altman 2014 mcgill et al 1978 tukey 1977 however there is growing evidence that shows both of these plot types can be misleading matejka and fitzmaurice 2017 weissgerber et al 2015 this confusion arises because different dataset distributions may contain similar or even equivalent summary statistics given bar plots and box plots primarily show summary statistics medians interquartile ranges and 95th and 5th percentiles for box plots and median or mean plus standard error or confidence intervals for bar plots two similar plots may incorrectly suggest that dataset distributions are equivalent this problem is even more pronounced with bar plots that use a bar to represent the mean or median of the data and lines to indicate standard error or confidence intervals weissgerber et al 2019 three alternative plots for large datasets that preserve distribution shape are density plots violin plots and a new combined approach termed raincloud plots allen et al 2019 density plots can be strong alternatives to boxplots when the goal of a visualization is to show volume but not variety e g multiple groups overlaid density plots can summarize density for a small number of groups however it may become hard to distinguish between groups for more than three to four categories wickham 2010 depending on the degree of difference between the distributions as the number of series comparisons grows subplots should be used to break out individual groups one alternative to density plots for comparing multiple groupings with large volumes are violin plots which are essentially mirrored density plots fig 6b the myriad of violin plot iterations also enables encoding summary statistics alongside the distribution to preserve both types of information however there is an argument to be made that violin plots may include redundant information through mirroring allen et al 2019 raincloud plots are a different type of approach that combine visualization of the distribution showing an aggregated distribution and individual data values allen et al 2019 while these three approaches represent endmembers in the visualization of distributions many other iterations of these types of plots exist for instance one iteration is to combine a barplot and violin plot fig 6c hintze and nelson 1998 enabling interpretation and comparison of summary statistics and overall distribution in addition to the variant shown in fig 6c other variants include beeswarm plots eklund 2015 2016 a re envisioning of the dot plot wilkinson 1999 and beanplots kampstra 2008 2014 one question that may arise when considering plotting distributions if the goal of a plot is to highlight the distribution of the data should we just be plotting the raw data the answer here is an emphatic no estimating distributions and statistics from raw data is notoriously challenging bobko and karren 1979 spence et al 2016 3 3 decision making in the context of dimensionality large datasets are often high dimensional either in terms of the variables they contain or in terms of how those variables are categorically or quantitatively grouped therefore selecting the number of dimensions to display within a given plot is often challenging with so many potential encoding attributes to add spatial location shape width size and color to name a few it is easy to overcomplicate at the same time as the volume and variety of data encapsulated within scientific visualizations grows plot complexity in terms of dimensionality volume of data encoded and composition is certainly growing though simplicity should still be the ultimate goal of any visualization this does not have to be in conflict with employing a visualization that exceeds three dimensions that shows an exceptional volume of data or that combines multiple subplots into a single visualization 3 3 1 consideration 1 balance the number of dimensions you show with overall plot simplicity decision making surrounding the choice of a plot the number of dimensions to display and a key plot message are inherently linked giving thought to how these pieces work together from an early stage is therefore important to creating an effective visualization when making decisions regarding the number of dimensions you seek to display it is important to remember that encoding attributes inherently limit us to just a few dimensions two continuous variables for positions on a bivariate plot one continuous or categorical dimension for color and or size and one categorical dimension for shape therefore many plot types support displaying anywhere between two and five dimensions though some plots such as the parallel coordinate plot and the rose plot can display many more dimensions however as the old adage goes just because you can doesn t mean you should additionally the goal with any plot should be to avoid redundancy as shown in fig 6c color is redundant with labeling on the x axis in fig 7 and fig 8 we explore the catchment attributes and meteorology for large sample studies camels addor et al 2017a addor et al 2017b dataset using a series of figures moving from two dimensional fig 7a c and three dimensional fig 7b visualizations to higher dimensional visualizations such as parallel coordinate plots fig 8 in figs 7 and 8 below adding higher dimensions and showing variety across hydrological signatures fig 8 presents a clearer picture of how watershed behavior is organized across the us fig 8a and to what extent behavior is similar within a larger basin fig 8c however as additional elements are added to the plot such as shown in fig 8b it becomes difficult to extract useful patterns and to compare across multiple dimensions 3 3 2 consideration 2 is the number of groups or series in a single visualization manageable and discernible while our plotting is often limited by dimensions it is not inherently limited by quantitative or categorical groupings these groupings are regularly used when visualizing large datasets to emphasize comparisons between quantitative or qualitative groups comparison is at the heart of understanding trends or differences in data visualization must make comparison between groups easy to interpret tufte et al 1990 grouping is an approach for reducing dimensionality that enables assessment of similarities and differences across dataset subsets when plotting large datasets we often use grouping with colors symbols or sometimes both to show organization within a complex multi dimensional and or large volume visualizations however when the focus is on comparison of these different groups it can be easy to overwhelm when the number of groups shown plotted concurrently not side by side begins to exceed three wickham 2010 in these types of plots aimed at showing many groups all values may be plotted within the same visualization or may be separated into subplots or insets the latter can be an effective way to highlight a subset of the data in the context of the broader dataset in fig 9 we show examples of hydrologic simulations produced from four different models and compared to observed streamflow for one watershed when plotted together the timeseries are hard to distinguish from one another fig 9a even on a logarithmically transformed axis fig 9b separating each of these comparisons into subplots more clearly illuminates the periods when simulated streamflow is in agreement with observed values fig 9c in line with ensuring the number of groups or series to be compared are interpretable is giving thought to how these groups are organized within a visualization when you must specify the order of such groups e g in a heatmap a parallel coordinate plot or a distribution based plot the choice of how to order components of your plot matters this ordering should be done intuitively such as from small drainage areas to large when comparing watersheds e g fig 6 by magnitudes in rank plots fig 7c or potentially by mean values in heat maps 3 3 3 consideration 3 make complexity work for you one of the best ways to simplify large volumes and varieties of data is by using synthesis plots here we define a synthesis plot as any type of plot that combines multiple graphical approaches and encoding attributes by this definition many of the best visualizations today combine multiples of plot types to convey key messages by nature synthesis plots are complex summarizing multidimensional datasets with multiple encoding attributes points lines color arrows they may incorporate symbols often make heavy use of color and include strong labeling and text throughout exceptional examples of such plots include circos diagrams fig 10 a sankey diagrams table based diagrams treemaps fig 10b and ordered bar charts while these visualizations are complicated what often makes them successful is that the creators give narrative to these plots tufte et al 1990 through captions labeling and either verbal e g during a presentation or described in an animation or written descriptions e g captions or manuscript or article text if all else fails your caption should be able to explain your figure if it can t your figure is probably too complicated and needs revision complexity does not necessarily detract from interpretability synthesis plots represent an exceptional opportunity for visual communication 3 4 challenges and opportunities in the use of color color is central to the creation of scientific visualizations zeller and rogers 2020 while color can mislead the reader when interpreting figures ware et al 2008 samsel et al 2018 removing color as an encoding attribute is not always feasible when using color there are several approaches that can be used to make visualizations clear and easy to interpret 3 4 1 consideration 1 use care with color based pattern plots and color based comparisons color is often used in data visualization to differentiate between groups outliers and across scales although color can be useful in the right context it is difficult to differentiate between colors of the same intensity or saturation samsel et al 2018 ware 2008 additionally assigning a scale to a color gradient or understanding how far apart two colors are on a scale are difficult for readers to interpret i e gradients force the readers to do visual math in this vein using color contrast can distort the reader s view of the data displayed when not used properly in visualization samsel et al 2018 for example in gradients of data ratios of hues red to green are about equal but hues orange to blue are about 1 2 for an equal gradient of data this color inequality is particularly important when areas are displayed because our brains will try to bring the complementary colors into balance and misjudge the aerial extent of a color samsel et al 2018 and are not appropriate for displaying categorical variables i e categories of vegetable should not be displayed on a white to red color gradient when it comes to visualizations that rely on color to support interpretation the widely used heat map is particularly difficult to assess because colors can look different depending on what colors are surrounding them albers 1975 in addition colors like red can dominate other colors leading to interpretation that there is more or extensive red in relation to other colors fig 6a often referred to as contrast of extension alternatively it is difficult to compare colors with increasing distance or blank space between the colors to top all of these issues off color blindness is a serious limitation in interpreting figures including but not limited to heatmaps because color can mislead readers we suggest that color is used carefully and after other alternatives in big data visualization there are many other encoding attributes that can be used for differentiation between groups or scale alternatives include shape line weight size and length 3 4 2 consideration 2 is it useful to add color to highlight an overall pattern or to a distinct group of variables using color to convey an additional dimension within visualizations of large volumes of data can be an effective way to convey additional information typically the intent of using color in these types of visualizations is to either 1 convey an overarching pattern e g figs 4c and 5b c fig 7b and c fig 8a and b or 2 to display outliers or a categorical group fig 4d fig 8c however a plot that tries to convey both patterns and outliers may easily overwhelm perception of key messages or interpretation of the visualization the major challenge to using color to visualize patterns for a large dataset is that there are many aspects of a visualization that can obscure our ability to discern a given pattern albers 1975 samsel et al 2018 care should be given to ensure that all values can be distinguished importantly the choice of a color bar should enable interpretation of the perceived pattern an alternative to using color may be to use another type of differentiator e g shape that is easier to interpret an alternative to visualizing patterns is to use color to highlight distinct groups of variables e g details this type of visualization may be employed to identify a statistical grouping such as outliers a categorical grouping such as different types of values e g watersheds within the same sub basin fig 8c or a research based outcome such as groupings from hierarchical clustering or the like as opposed to a typical pattern based plot this type of plot may simplify the number of colors being quantitatively perceived across a large volume of data 3 4 3 consideration 3 when using color choose color schemes or gradients that allow for contrast in hue value and saturation color theory represents an extensive study of how colors interact how best to mix colors and how best to use colors within visualizations of any kind albers 1975 the basis of color theory is the color wheel which is used to identify colors that may work together to make visuals appealing and to provide contrast rhyne 2012 as defined by color theory there are three aspects of color hue color wavelength e g blue or green saturation depth of the color and value grayness of the color these aspects of color may be used to generate a color scheme or gradient for a particular plot many different combinations of colors that incorporate color theory already exist a complementary color scheme corresponds to colors that exist on opposite sides of the color wheel an alternative to a complementary color scheme is a split complementary color scheme which uses a base color along with two colors on opposites sides of the first color complement i e the color directly across from the base color on the color wheel an analogous color scheme is when three colors are adjacent to each other and a triadic color scheme is when three colors are equally spaced around the color wheel color schemes employing complementary color schemes as end members of a gradient are often called divergent color schemes frequently with white or a neutral color in the middle schemes with colors of a similar hue or within an analogous color palette are commonly called sequential color schemes i e light green grass green dark green the best way to make or select a color scheme that is easier to differentiate and distinguish is to leverage the three different aspects of color hue saturation and value by employing at least two aspects of color such as saturation and value visualizations can appear more interpretable ware 2008 likewise combining colors of different saturation or value with changes in hue can make colors more distinguishable particularly for viewers with color blindness colors appropriate for visualization are frequently difficult and overwhelming to choose for those who may be new to the principles of color theory there are several web based resources to assist with selecting a gradient or categorical color scheme table 1 includes a list of these resources that can be used for developing a color palette 3 4 4 consideration 4 avoid rainbow color scales and limit the number of categories to enable interpretation while this recommendation is widely known it still bears repeating despite their broad proliferation rainbow color ramps use constant saturation and value only varying in hue borland and taylor 2007 therefore it is very difficult for colorblind individuals to distinguish between colors on a rainbow scale in addition the gradient includes all colors on the visible spectrum making it hard for viewers to perceive which colors correspond to a positive or negative value this aspect also makes it challenging to interpret which part of the scale the color on the gradient represents borland and taylor 2007 to emphasize the problems with this particular color scale we include a particularly horrific example in fig 11 a to avoid the rainbow color scale pick a type of contrast that best fits the visualization samsel et al 2018 cool to warm contrasts such as blue to red or yellow are often good for scales with gradients from low to high additionally using single color gradients is a strong approach for producing easy to discern gradients humans are biased to prefer balanced gradients with either cool to warm or complementary contrast in a gradient albers 1975 limiting colors to seven categories or less helps the viewer interpret gradient or categorical color scale e g fig 5c fig 11 3 4 5 consideration 5 think beyond just colorblindness colorblindness is not the only visual disability that affects the interpretation of visualizations low vision individuals may find it difficult to read or interpret details in a visualization often increasing the contrast of the colors used in the visualization or increasing font size can make the figure more accessible to those with low vision power and jürgensen 2010 in addition rasters are inaccessible to the visually impaired because the components of the figure are hard to separate choi et al 2019 often those with visual impairments will use computer reading software to get information from a manuscript to provide more information make sure the caption in the figure is accessible to reading programs and fully describes the graph and the results accessibility can be increased by considering creative choices to explain visualizations such as by adding supplemental movies or audio that describe the study or the visualizations power and jürgensen 2010 finally there are new frontiers opening up to make visualizations more accessible for instance machine learning approaches are being tested to identify individual elements of a visualization to pass to a reading software choi et al 2019 3 4 6 consideration 6 use color to tell a story go beyond best practices for a single visualization throughout a presentation or a text whether a manuscript interactive video blog post or other published article colors not only tell the story of one visualization but the larger narrative of the manuscript to keep the message and narrative consistent it is important to keep a consistent color scheme throughout particularly for visualizations that use the same components e g variables parameters groups as discussed above picking colors that are easy to distinguish that visually represent the figure s trends and that can be interpreted by all audiences is the most important aspect of choosing a color palette for a presentation or manuscript in particular it s important to select color palettes that do not violate or challenge cultural or archetypal expectations and that support and correspond to the data being shown e g fig 10b versus fig 10c for example do not use warm colors for water or snowfall or cool colors for fire frequency when using a dataset that is known to have an associated color palette employ this same color scheme e g fig 11 displaying land cover data with a previously created and widely used color palette if choosing a color palette fills you with dread the easiest direction is to pick an already existing color palette that is good for color blind persons and printing grey scale i e viridis there are also really fun ways to dream up color schemes such as using movie color palettes see twitter account cinemapalettes or movies in color https moviesincolor com 4 from static to interactive the fourth v of big data velocity refers to the speed and temporal resolution at which data is collected both of which are accelerating and are a challenge to visualize and represent we can work with this type of data through two methods summarizing such information in static formats or creative visualization techniques such as animation in this particular piece we focus less on velocity as a visualization need though note that the importance of visualizing velocity will likely continue to grow in the near future common approaches to visualize velocity in static formats include visualizing time using color or grouping values and using time as an organizing dimension for instance heat maps or barcode plots are commonly used to convey temporally resolute datasets at one or many locations other plotting options include the use of spatial snapshots e g showing temporal plots for a particular point slice or volume or temporal snapshots e g showing spatial plots for a particular time however it can be quite challenging to convey velocity in static visualizations at current more and more journals offer the option to upload animations as supplementary material journals are also beginning to develop visualization based submissions e g hpeye within the journal hydrologic processes for those interested in such options table 2 lists several packages in r and python that can be used for creating animations or interactive graphics while the scientific literature largely draws on static visualizations interactive visualizations are becoming increasingly common for science communication to a range of technical and non technical audiences though the focus of our article is primarily static visualizations we briefly summarize considerations for interactive visualizations interactive visualizations can be used for a variety of purposes they may be helpful in the data exploration phase or may accompany peer reviewed manuscripts as most journals do not support interactive formats it is important to remember that creating a static version of the figure is often necessary for a manuscript however such interactive visualizations are an option for supplemental or accompanying websites interactive graphs lead to more complexity and additional decisions while there are many types of interactive visualizations the most common examples involve 1 changing choosing a dataset or a unit of analysis in a visualization 2 filtering or querying a dataset in the visualization 3 toggling visualization features such as variables or colors 4 combining or separating visualizations 5 interactive annotations that provide more information and 6 zooming in and out or changing the level of detail walsh 2020 approaches to parse interactive graphic components are now being described through grammars of interaction e g vega https vega github io vega vega lite https vega github io vega lite which provide a framework for approaching the creation of interactive visualizations in addition many common scientific programming languages are facilitating the creation of interactive web apps e g r shiny https shiny rstudio com with these common types of interactive visualizations come pitfalls the following are a brief list of recommendations to consider when creating interactive visualizations 1 ensure that the static version is explanatory as most people will not interact with the figure 2 consider whether interactive features can be engaged and used on all types of devices e g individual points can be difficult to interact with on touch devices and 3 make the interactive components of the visualization well paced and manageable as opposed to too cumbersome or slow to maintain interest with your audience walsh 2020 it is likely that the tools and recommendations for best practices concerning interactive visualizations will continue to evolve especially given the growing interest in the creation and use of interactive visualizations across the scientific community 5 conclusions conveying large datasets within publications or presentations is challenging especially when it comes to visualizing large amounts of data the visualization of large complex datasets requires rethinking our common assumptions and approaches for creating plots in particular when visualizing large datasets researchers must make many decisions before arriving at a final plot these decisions include whether to show all values or to aggregate how to convey multiple categories or comparisons and how to display multiple dimensions all in succinct easily interpretable ways our introductory overview provides several recommendations such as choice of plot type encoding attributes and groupings to aid in the creation of clear visualizations thinking carefully about these choices can enhance visualization quality and messaging however whether or not our recommendations apply in any particular case will ultimately depend on the overarching message conveyed by each visualization and the size and character of the associated dataset though we often focus on encoding attributes and plot choice when approaching visualizations equally important is to give narrative to our plots using notations legends and a clear caption above all when creating plots of large datasets iteration is key be sure to ask for input along the way ensuring that the key takeaways intended for a particular visualization are clear to others scientific visualization is something that we all have very strong feelings about we therefore emphasize that our recommendations are just that not hard and fast rules that should be unilaterally applied in all scenarios be creative thoughtful and intentional with your designs and use your best judgment along the way declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors acknowledge funding from the cuse grant program at syracuse university to ck and a national science foundation award 1940006 to ab all data used in the creation of scientific visualizations are publicly available and cited in the text all code for visualizations are included in a github repository at https github com abraswell bigdataviz the article was improved by thoughtful comments from two anonymous reviewers appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105113 
25796,the weihe river basin wrb in a monsoon transition zone of east asia interacts with multiple weather systems and is susceptible to floods and droughts we developed a framework based on the empirical orthogonal function analysis complete ensemble empirical mode decomposition with adaptive noise analysis and moving average based spearman rank correlation to identify the spatial patterns of the dry wet conditions from 1470 to 2016 decompose the dry wet index into the leading components with a period between 2 and 3 years and 100 years and discover their driving forces on multiple time scales results show that wrb can be divided into four distinguishable hydroclimatic subregions located in the southeast west central and north the east asian summer monsoon and south asian summer monsoon impact this region mainly on the inter annual and inter decadal scales while the impacts of enso pdo ao and nao are dominant on the multi decadal and centennial scales graphical abstract image 1 keywords dry wet conditions multiple time scales floods and droughts east asian summer monsoon south asian summer monsoon 1 introduction a monsoon system acts as an important part of the global climate system and anomalous summer monsoon activities can trigger floods and droughts hao et al 2016 ninomiya 1999 every monsoon system has a limited influential area and usually creates two types of hydroclimatic zones namely humid and arid regions the monsoon transition zone corresponding to the climate transition zone is a region where cold dry and warm wet air masses frequently encounter as a result its climate exhibits significant fluctuation with a high amplitude and large spatial variability on multiple time scales therefore flood and drought possibly co occur in this area where dry wet conditions differ considerably in space these phenomena greatly influence the prevention and control of floods and droughts in this area chao et al 2019 2020 huang et al 2019 huo et al 2020 li et al 2011 luo et al 2018 mu et al 2021 zhang 2004 zhang et al 2021 moreover the monsoon transition zone is mainly composed of semi arid regions which have fragile ecosystems and are extremely susceptible to floods and droughts guan et al 2019 zhang et al 2019a zhu et al 2020 therefore exploring the characteristics of dry wet variation in the monsoon transition zone is quite important the dry wet conditions of a region are complicatedly affected by various factors such as monsoon system atmospheric oscillation solar activities and geographic features hong et al 2010 tian et al 2020 wang et al 2019b solar activities are an external driving factor while atmospheric oscillation and monsoon system are internal factors influencing the dry wet conditions bhalme and mooley 1980 fang et al 2019 higgins et al 2000 hurrell et al 2003 liu et al 2017 mohmmed et al 2018 zhang et al 2007 2008 zhao and han 2005 barlow et al 2001 revealed the significant relationships between the three primary modes of pacific sea surface temperature variability and the american warm season hydroclimate li et al 2021 found that global warming el niño southern oscillation enso and local effects are all significant driving factors controlling the spatiotemporal variability of precipitation extremes over the yangtze river basin besides many studies were carried out to reveal the relationships between regional dry wet conditions and their driving factors across the world han et al 2019 shi et al 2016 wang et al 2019c zaroug et al 2014 zhang et al 2015 zhao et al 2012 however most of these studies in the monsoon transition zones are focused on the time scales between annual and centennial scales very few studies investigated the dry wet conditions and their driving forces beyond the centennial scale in addition the impacts of different driving factors on the dry wet conditions in monsoon transition zone have substantially spatial differential patterns and intensities on different time scales which are rarely explored in the literature china s precipitation is mainly influenced by monsoons westerly and plateau circulation systems while their impacts are closely related to the northern edge of east asian summer monsoon zone zhang et al 2019c the regions to the south of the northern edge of east asian summer monsoon zone are mainly controlled by the monsoon system while those to the north are mainly dominated by the westerly system however the northern edge of the east asian summer monsoon zone will oscillate between south and north with the varying intensity of east asian monsoon every year ma et al 2011 the oscillation zone corresponds to the monsoon transition zone due to the oscillation of the edge the monsoon transition zone is subjected to the coupling effect of east asian monsoon system and mid latitude westerly system which leads to a great spatial variability of dry wet conditions the weihe river basin located in northwestern inland of china is the largest sub basin of the yellow river and located within the east asian monsoon transition zone yang et al 2019 at present the boundary of the east asian monsoon zone is still under a great debate shi et al 2009 thus no consensus is reached on the climatic zoning classification of the weihe river basin in addition the dry wet conditions in the weihe river basin have large spatiotemporal variability making it very hard to be predicted based on the previous studies the factors influencing the dry wet conditions of the weihe river basin include the asian summer monsoon systems such as east asian summer monsoon easm and south asian summer monsoon sasm atmospheric oscillations such as enso pacific decadal oscillation pdo arctic oscillation ao and north atlantic oscillation nao and other natural driving factors like solar activities bi et al 2013 liu et al 2017 wan et al 2018 zhao et al 2015 in most of the previous studies the easm and sasm systems in the asian summer monsoon system are not isolated from each other easm and sasm together serve as the two main components of the asian summer monsoon system huan and li 2018 the two monsoons differ largely in their circulation characters and impacting areas and are physically separated and closely correlated to each other tao 1987 the spatial patterns and intensities of the impacts of the easm and sasm systems on the dry wet conditions in the weihe river basin remain to be further explored the objectives of this study are 1 to reveal the spatial patterns of dry wet conditions in the weihe river basin 2 to reconstruct the dry wet index series of the weihe river basin from 1470 to 2016 and derive its periods trends and changes and 3 to identify the driving factors of the dry wet conditions and reveal the spatial patterns of their influences on multiple time scales this study is valuable for better projecting future changes in this region and its surrounding areas 2 study area and data 2 1 study area our study area is the weihe river basin fig 1 the weihe river located in the east of the northwest inland of china originates from niaoshu mountain in weiyuan county gansu province and finally merges into the yellow river at tongguan county shaanxi province fig 1 it flows through three provinces in china including gansu ningxia and shaanxi located within 33 5 37 5 n in latitude and 103 5 110 5 e in longitude this basin has a total area of 134 800 km2 and is characterized by distinctive seasonality with abundant precipitation and high temperature in summer and little precipitation and low temperature in winter the average annual precipitation is about 600 mm 60 of which occurs in the flood season from june to september and the annual temperature ranges from 7 8 c to 13 5 c chang et al 2014 the weihe river basin is situated in the monsoon transition zone which is subjected to the coupled effect of monsoon and westerly circulation systems moreover it shows complex terrain therefore its dry wet conditions and variation delivers a great spatial variability 2 2 data to quantify the dry wet conditions of the weihe river basin over a long time span we adopted the dryness wetness index dwi from the yearly charts of dryness wetness in china for the last 500 year period hereinafter called the charts published by the chinese academy of meteorological sciences 1981 in the charts dry wet conditions during 1470 1979 in china are quantified as the dwi and classified into five classes i e 1 2 3 4 and 5 1 2 3 4 and 5 indicate very wet wet normal dry and very dry respectively china has continuous and consistent historical records about agricultural conditions weather and disasters such as floods and droughts which are valuable for determining the historical dry wet conditions therefore the recorded agricultural conditions and flood and drought events can reflect the historical dry wet conditions the dwi series from 1470 to 1950 are obtained from more than 2200 local annals and a large number of other historical writings that qualitatively describe the dry wet conditions while the dwi series from 1951 to 1979 are obtained from measured annual precipitation amount table 1 the dwi of each year in the charts was determined cautiously after full consideration of geographical features for example the flood caused by external river inflow was not considered as a flood resulted from the wet condition while a good harvest in the arid area means a wet condition since there are measured precipitation data from 1951 to 1979 the dwi values during this period were directly calculated using the observed precipitation data in order to keep the frequency of dry wet conditions before 1951 the same as that after 1951 the dwi values after 1951 were determined using the same method as shown in table 1 that was used to compute the dwi values prior 1951 there are 120 sites with recorded dry wet conditions from 1470 to 1979 across china fig 1 the data from the 120 sites were interpolated to produce yearly dry wet index contour maps by the chinese academy of meteorological sciences based on these maps we derived the 0 5 0 5 gridded dry wet index data for the weihe river basin from 1470 to 1960 in this research we reconstructed the dwi during 1470 2016 for these years after 1960 we calculated the dry wet index based on the daily 0 5 0 5 observation based gridded precipitation data from china meteorological administration http data cma cn data cdcdetail datacode surf cli chn pre day grid 0 5 html to quantify the influence of monsoons the easm index easmi and the monsoon hadley circulation index mhi are selected to quantify easm and sasm respectively table 2 the easmi is derived from the unified dynamic normalized seasonality dns monsoon index defined by li and zeng 2002 which has a good performance in depicting the easm and has been adopted by the national oceanic and atmospheric administration noaa to monitor the easm strength li et al 2010 the mhi is defined by goswami et al 1999 the two indices are broadly used and proved to be reliable wang and fan 1999 wang et al 2008 average monthly data during 1979 2014 from the reanalysis data of https climatedataguide ucar edu climate data era interim the european centre for medium range weather forecasts ecmwf were used to calculate the monsoon indices above solar activities are quantified by the sunspot number ssn provided by the solar influences data analysis center sidc the ssn data of each year during 1700 2016 were retrieved through http sidc oma be silso datafiles table 2 the pdo index pdoi was provide by university of washington http research jisao washington edu pdo the multivariate el niño southern oscillation index mei wolter and timlin 1998 was selected to quantify enso considering that mei contains more information compared to the other indices kiem and franks 2001 the mei data were downloaded from http www esrl noaa gov psd enso mei table html the ao index aoi was downloaded from http www atmos colostate edu ao data ao index html although ao and nao are closely correlated with each other evidence shows that the teleconnection signals of ao and nao show a great difference in some regions therefore the impacts of nao are also investigated in the study the nao index naoi was downloaded from https climatedataguide ucar edu climate data hurrell north atlantic oscillation nao index station based basic information of the above indices is summarized in table 2 these large scale oscillation indices actually quantify the strength of driving factors that influence the dry wet conditions in the weihe river basin therefore we also called them the driving factor indices dfis in this study 3 methodology 3 1 subregion division since the weihe river basin spans an area of 134 800 km2 it is important to investigate the spatial variability of dry wet conditions and their potentially differential responses to the driving factors therefore it is important to divide the weihe river basin into certain subregions based on the spatial patterns and temporal variation of dry wet conditions it is generally regarded that extreme precipitation events are the main causes of floods while persistent low rainfall is the main cause of droughts liu et al 2017 tao and zhang 2020 wang et al 2019a zhang et al 2019b therefore the other aspects of precipitation rather than annual precipitation total should also be considered especially these characteristics related to extreme hydrometeorological events seven precipitation indices were adopted to quantify various aspects of precipitation such as intensity frequency duration and amount table 3 the seven precipitation indices are max 1 day precipitation amount rx1day max 5 day precipitation amount rx5day number of mild precipitation days r10 number of heavy precipitation days r25 consecutive dry days cdd consecutive wet days cwd and annual precipitation total prcptot detailed definitions of these indices are summarized in table 3 in this study we applied the rclimdex 1 0 software developed by zhang and yang 2004 to compute these precipitation indices in each grid cell of the weihe river basin with a spatial resolution of 0 5 0 5 the empirical orthogonal function eof analysis was performed to obtain the spatial patterns of the seven precipitation indices details of the eof analysis are referred to perry and niemann 2007 based on the results of the eof analysis the spatial patterns and their variations of the dry wet condition can be revealed we then divide the weihe river basin into hydroclimatic subregions based on the eof results terrain and geography 3 2 extension of the dwi index gridded annual dwi values with a spatial resolution of 0 5 0 5 from 1470 to 1960 were directly digitized and obtained from the charts see section 2 2 to extend the series to 2016 we further calculated the gridded dwi values from 1961 to 2016 from the 0 5 0 5 gridded precipitation data provided by china meteorological administration using the same dwi classification method summarized in table 1 as those used to produce the charts through this method we seamless produced cohesive gridded dwi time series from 1470 to 2016 to minimize the impacts of different station sample sizes before and after 1961 we conduct the further analyses on the subregion scale rather than on a grid cell scale 3 3 complete ensemble empirical mode decomposition with adaptive noise to detect the components and their associated periods and trends of the dwi time series we applied the complete ensemble empirical mode decomposition with adaptive noise ceemdan method torres et al 2011 the ceemdan method was first proposed by torres et al 2011 as an improved version of the ensemble empirical mode decomposition eemd the main principle of eemd is to decompose a time series into multiple intrinsic mode functions imfs that contain the local characteristics of the raw time series on different time scales and a residue that reveals the long term trend of the raw time series huang et al 1998 wu and huang 2009 however torres et al 2011 pointed out several limitations of the eemd method despite that it can overcome the mode mixing by adding white noises the limitations include that the residual noises cannot be completely eliminated and the modes which are decomposed from the raw time series are not unique to overcome these limitations torres et al 2011 proposed the ceemdan method which can eliminate the residual noises of the imf components and more efficiently decompose the raw time series at the minimum computation cost advantages of this method have been verified by some studies wang et al 2018 zhang et al 2017 the periods of the dwis were detected through the power spectrum analysis percival and walden 1993 the ceemdan function of the ncar command language ncl was used to carry out the ceemdan analysis which can be downloaded from https www ncl ucar edu document functions built in ceemdan shtml 3 4 attribution of the variation of dry wet conditions on multiple time scales to attribute the variation of dry wet conditions on multiple time scales we developed an integrated attribution framework fig 2 the attribution consists of two methods the correlation analysis based on the ceemdan analysis left of fig 2 and the correlation analysis based on the moving average method right of fig 2 the former is to perform the spearman rank correlation analysis to calculate the correlations between the time series of the decomposed components imfs with different periods of the dwi series and the time series of the dfis the ceemdan analysis is considered to be able to divide the dwi into modes components with different periods which reflect the influences of various driving factors on different time scales as a result the spearman rank correlation analysis can identify the specific driving factors that influence the dry wet variation on a specific time scale owing to the low variance contribution rates of the imfs with a main period greater than 100 years they are recomposed with the residue to form the recomposed time series which stands for the dry wet variation of the weihe river basin on the centennial scale however the correlation analysis based on the ceemdan method ignores the interactions between different components of the dwi series therefore we further applied the correlation analysis based on the moving average to supplement the ceemdan based correlation analysis right of fig 2 the basic principle of the moving average method similar to the low pass filter is to eliminate the influences of these components with a short period and random fluctuations and detect the long term trend of the original series given a time series and a sliding window with a fixed size the first output element of the moving average is obtained by averaging the initial elements of the time series in sliding window then the sliding window shifts towards the end of the time series with one element interval as a step one step movement of the sliding window will produce one output element more details on the method can be found in anderson 2011 in the moving average method the components with a period shorter than the size of the sliding window of a time series are eliminated therefore when multiple moving average operations with different sliding window sizes are applied certain components will be filtered out the interactions between the remaining components of the dwi are kept for the following spearman rank correlation analysis which can overcome the shortcomings of the correlation analysis based on the ceemdan method due to the data limitation these dfis differ in their time lengths see table 2 to make the correlation analysis comparable we chose the common time period i e 1979 2016 to analyze the correlation of the dwi series and its components with the series of the dfis 4 results and discussion 4 1 spatial patterns of precipitation characteristics and hydroclimatic division of the weihe river basin fig 3 shows the spatial patterns of the two leading eof modes of the seven precipitation indices the seven precipitation indices represent different characteristics of precipitation across the weihe river basin rx1day and rx5day quantify the intensities of precipitation extremes while r10 and r25 measure the frequencies of precipitation events cwd and cdd denote the duration of precipitation days and non precipitation days respectively while prcptot quantifies annual precipitation amount table 3 it is clear that the two leading modes of rx1day fig 3a b have similar spatial gradients to those of the corresponding leading modes of cwd fig 3i j similarly the spatial gradients of the two leading modes of rx5day fig 3c d resemble those of the corresponding leading modes of r25 fig 3g h moreover the spatial gradients of the two leading modes of r10 fig 3e f are very close to those of the corresponding leading modes of prcptot fig 3m n in fact the eof1 and eof2 of the six indices rx1day rx5day r10 r25 cwd and prcptot are spatially highly correlated with each other fig 3 all of the six indices are actually indicators of wet conditions higher values of these indices indicate more frequent long lasting or heavier precipitation which can enhance the possibility of floods in contrast the spatial patterns of the eof1 and eof2 of cdd fig 3k i are totally different from those of the eof1 and eof2 of the other six indices the eof1 and eof2 of cdd have a clear west to east spatial gradient fig 3k i while the eof1 and eof2 of the other six indices show an apparent southeast to northwest gradient fig 3 cdd quantifies the number of consecutive dry days serving as an indicator of dryness difference in the spatial patterns of the leading modes of the wetness related indices and dryness related index suggest that the mechanisms or driving factors on the dry wet conditions across this region are likely different and that response to the driving factors differs in space more importantly there are apparent spatial clusters across the leading modes of the seven indices fig 3 indicating that the weihe river basin shows regional patterns in other words the weihe river basin can potentially be divided into subregions that show different dry wet characteristics to derive these subregions we first normalized the values of the eof1 and eof2 of the seven indices into the same range respectively we then combined the fourteen normalized eof modes to produce a map through a linear combination finally we conducted a cluster analysis based on the combined map with the aid of topography as a result we found that the weihe river basin can be divided into four subregions fig 4 subregion 1 is the guanzhong plain located in the southeast of the weihe river basin while subregion 2 is the longxi region with high elevation that is located to the west of the liupanshan mountain and neighbors the qinghai tibetan plateau fig 4 subregion 3 is the hilly loess region with elevation ranging between 400 m and 3900 m fig 4 while subregion 4 is the loess plateau with high elevation the four subregions are visually distinguishable in the eof1 maps of the six indices except cdd see fig 3 highlighting the apparent difference between the four subregions it is interesting to observe that the eof values in subregion 3 are clearly different from those in the other three subregions fig 3 implying the uniqueness of this region in fact subregion 3 is a zone that well reflects the dry wet conditions of the whole weihe river basin which was also reported in a previous study bi et al 2013 bi et al 2013 found that the wetness and dryness in the weihe river basin is mainly controlled by the climate in the central zone of the basin i e subregion 3 in this study in the eof2 maps of the six indices except cdd the signs of values in subregion 1 are opposite to those in subregion 2 and subregion 4 implying that subregion 1 have different dry wet conditions and precipitation characteristics relative to subregions 2 and 4 fig 3 the above results suggest that subregion 3 is the transition zone between subregion 1 subregion 2 and subregion 4 to further identify the driving forces of dry wet conditions in these subregions we analyzed the correlations of the expansion coefficients ecs series of the two leading eofs of the seven precipitation indices with the corresponding dfis series the results show that easmi mhi ao1 nao1 and mei are significantly correlated with at least one leading mode of one of the seven precipitation indices fig 5 in contrast pdoi and ssn do not show a significant correlation with any of the leading modes of the seven precipitation indices fig 5 in other words the dry wet conditions in this region are collectively controlled by easm sasm ao nao and enso while pdo and ssn have weak impacts however the impacts of easm sasm ao nao and enso show apparent differential impacts across this region the easmi series exhibits a significant positive correlation with the 2nd leading mode of rx1day r10 and prcptot fig 5 moreover the signs of the eof2 values of rx1day r10 and prcptot in subregions 1 and 3 are opposite to those in subregions 2 and 4 fig 3b f and n indicating that the strength of easm has polarized impacts on precipitation extremes and total across the weihe river basin the above results are totally understandable when easm is stronger it can carry moist air from the pacific ocean and push it moving further to the north and west causing more precipitation in the north and west i e subregions 2 and 4 and correspondingly less precipitation in the south and east namely subregions 1 and 3 the mhi series has a significant positive correlation with the 1st leading mode of cdd fig 5 the signs of the eof1 values of cdd across the whole region are negative fig 3k these results indicate that a stronger sasm leads to more precipitation across the whole region and correspondingly reduces the consecutive dry days in other words sasm tends to have a unanimous impact on precipitation across this region in terms of mei it is significantly negatively correlated with the 1st leading mode of rx5day and r25 furthermore the signs of the eof1 values of rx5day and r25 are all positive across this region fig 3c and g these results indicate that a stronger enso leads to less rx5day and r25 across the whole region naoi and aoi are significantly negatively correlated with the 2nd leading mode of rx1day fig 5 in contrast easmi has a significant positive correlation with the 2nd leading mode of rx1day fig 5 these results indicate that ao nao influence the spatial pattern of eof2 of max 1 day precipitation amount in an opposite way comparing to easm in summary easm sasm ao nao and enso show apparent impacts on the dry wet conditions across the weihe river basin but pdo and ssn do not show significant impacts on the dry wet conditions of this region easm ao nao and enso mainly impact the precipitation extremes in this region while sasm controls the dryness extreme across the whole region the impacts of easm ao and nao show spatially differential impacts while enso show spatially unanimous impacts 4 2 periods trends and abrupt changes of the dwi series the dwi series of the four subregions from 1470 to 2016 is shown in fig 6 according to the pettitt test 1979 the dwi series of subregion 1 subregion 3 and subregion 4 are statistically non stationary fig 6a c and d an abrupt change occurred from the dry phases to the wet phases around 1641 in both subregions 1 and 3 fig 6a and c while an abrupt change from the dry phases to the wet phases happened around 1657 in subregion 4 since human activities were not substantial in 1640s and 1650s the abrupt changes of the dwi are likely caused by the long term variability within the climate system of the weihe river basin we further applied the ceemdan analysis to decompose the dwi series of the four subregions into a set of imf modes according to the quasi period distribution the dwi series can be divided into six modes with respective quasi periods of 2 3 years 7 10 years 16 22 years 27 33 years 60 80 years and 100 years in all of the four subregions fig 7 the first five leading modes can explain more than 95 of the variance of dwi fig 7 while the 1st leading modes of the four subregions which have a quasi period between 2 and 3 years can explain 50 of variance of the dwi series the quasi periods of the leading modes are different between the four subregions fig 7 the residues represent the long term trends of the dwi series of the four subregions from 1470 to 2016 as shown in fig 7 the long term trends can explain less than 3 5 of variance in the dwi series in all of the four subregions although the summed variance contributions of the modes with a period of more than 100 years i e imf6 9 are small less than 10 in the four subregions these modes are important for explaining the long term variability of the dry wet conditions to further investigate the variability and trends of the dwi series on different time scales we further recomposed the dwi series by excluding the imf modes stepwise for example we excluded the imf1 mode from the dwi series to produce a recomposed series named rs imf1 we excluded both imf1 and imf2 modes from the dwi series to produce a recomposed series which is named as rs imf1 2 we excluded the nine imf modes from the dwi series to produce the residual series that is named as rs imf1 9 based on the pettitt test we derived the statistically significant abrupt change points for all recomposed series for the four subregions as shown in fig 8 the change points of all recomposed series in subregion 1 are the same as or very close to those in subregion 3 suggesting that the dry wet oscillations and variation are nearly consistent in these two subregions regarding subregion 4 the recomposed dwi of the subregion 4 by excluding the components with short periods 2 3 and 7 10 years i e rs imf1 2 has the same change point as those of subregion 1 and 3 fig 8 these results indicate that the components with a period of 2 3 and 7 10 years in subregion 1 and 3 exhibit different variability from those in the subregion 4 the rs imf1 5 series is obtained by combining imf6 imf7 imf8 imf9 and residue which all have a period of 100 years or above the abrupt change points of rs imf1 5 in the four subregions are nearly the same and close to those of the original dwi series therefore it is reasonable to say that rs imf1 5 reflects the non stationarity and long term variability of the dry wet conditions in these regions furthermore rs imf1 9 is the residue of the dwi series and represents the long term variability of the dwi as shown in fig 8 all rs imf1 9 series in the four subregions have a negative trend indicating that all of the four regions have a negative trend in dwi from 1470 to 2016 in other words the whole weihe river basin has a long term wetting trend during 1470 2016 4 3 correlations between the imf components of dwi and the dfis fig 9 shows the spearman rank correlation coefficients between the various dwi components with a period less than 100 years and the dfis for the four subregions all in fig 9 represents the original dwi series there are significant positive correlations between the easmi and the raw dwi in both subregion 1 fig 9a and subregion 3 fig 9c moreover the significant positive correlations are also detected between the easmi and the imf components with a period of 2 3 years in subregion 1 fig 9a and subregion 3 fig 9c additionally easmi also shows a positive correlation with the original dwi series and its imf1 component in both subregion 2 fig 9c and subregion 4 fig 9d in contrast all of the other dfis such as ssn aoi naoi and mei do not show significant correlation with the dwi series and their imf1 components for all of the four regions fig 9 these results suggest that easm is the main factor that influences the dry wet conditions in subregion 1 and subregion 3 on the time scale of 2 3 years this result is also consistent with these results in figs 3 and 5 which shows that a stronger easm corresponds to a drier condition in subregions 1 and 3 different from easmi mhi is significant positively correlated with the imf components with a period of 27 33 years and 60 80 years in subregion 1 fig 9a and with the imf component with a period of 60 80 years in subregion 2 fig 9b aoi pdoi and mei are significant positively correlated with the imf components with a period of 60 80 years in subregion 1 fig 9a and subregion 3 fig 9c nao also shows a significant positive correlation with the imf component with a period of 60 80 years in subregion 3 fig 9c these results suggest that ao nao pdo and enso activities mainly influence subregions 1 and 3 on the time scales of 60 80 years ssn exhibits a significant positive correlation with the imf component with a period of 16 22 years in subregion 2 fig 9b suggesting that the dry wet variability on the time scale of 16 22 years is mainly impacted by ssn in subregion 2 interestingly none of the seven dfis shows a significant correlation with the dwi series and its imf components in subregion 4 fig 9d as mentioned above the recomposed series rs imf1 5 of the dwi series represents the centennial scale variability of the dry wet conditions we further analyzed the correlation of the rs imf1 5 series with the seven dfis the results show that neither of easmi and mhi is significantly correlated with rs imf1 5 for all of the four subregions fig 10 implying that the dominant monsoon systems around this region have little influence on the dry wet variability of the weihe river basin on the centennial scale in contrast mei exhibits a significant negative correlation with the rs imf1 5 series of subregions 1 2 and 3 fig 10 aoi exhibits a significant negative correlation with the rs imf1 5 series of subregions 1 and 3 fig 10 in addition there is a significant negative correlation between pdoi and rs imf1 5 in subregions 1 and 3 fig 10 while ssn exhibits a significant negative correlation with rs imf1 5 of subregion 2 and a significant positive correlation with rs imf1 5 of subregion 1 fig 10 in summary the above results suggest that the two dominant monsoon systems i e easm and sasm around the study area mainly control the dry wet conditions of the weihe river basin on the relatively short time scales i e 2 3 years 27 33 years and 60 80 years while enso ao nao and pdo mainly influence the dry wet conditions of the weihe river basin on the long time scales i e 60 80 years and 100 years 4 4 correlations between the dwi and the dfis based on sliding window method we further investigated the correlations between the dwi and monsoon indices easmi and mhi based on sliding window method in the four subregions when the size of sliding window ssw equals 1 the easmi exhibits significant positive correlations with the dwi of subregions 1 and 3 fig 11 a and b when ssw ranges from 3 to 6 the easmi exhibits negative correlations with dwi of subregion 2 3 and 4 fig 11a when ssw ranges from 7 to 16 the easmi exhibits negative correlations with dwi of subregion 1 2 and 3 and a positive correlation with dwi of subregion 4 when ssw ranges from 17 to 22 the easmi is positively correlated with dwi of subregions 1 and 3 as ssw changes the importance of the components with different periods also varies when ssw equals 1 the component with a period 2 3 years is the major component which is positively correlated with easmi fig 9 when ssw ranges from 3 to 16 the importance of the components with a period of 7 11 years and 16 22 years increase while that of the component with a period of 2 3 years decreases both or one of components with a period of 7 11 years and 16 22 years in the four subregions exhibit negative correlations with easmi which leads to the negative correlations between dwi and easmi when the ssw ranges from 2 to 18 mhi is significantly positively correlated with the dwi in subregions 1 2 and 3 fig 11b however the curve of subregion 4 in fig 11b is inconsistent with those of the other subregions dwi of subregion 4 is nearly not correlated with mhi according to figs 9 and 10 the mhi exhibits positive correlations with the dwi of subregion 1 2 and 3 on multi decadal scale fig 12 shows the spearman rank correlation coefficients of dwis of the four subregions with the mei pdoi aoi naoi ssn and dwis using different ssws when ssw ranges from 12 to 36 mei pdoi aoi and naoi exhibit positive correlations with the dwi of the four subregions fig 12a d however when ssw ranges from 40 to 50 mei aoi and naoi are negatively correlated with the dwi of the four subregions fig 12a c and d the correlation curves based on the moving window method of subregions 2 and 4 clearly differ from those of subregions 1 and 3 pdoi exhibits significant negative correlations with the dwi of subregion 1 and 3 and positive correlations with the dwi of subregions 2 and 4 fig 12b the correlations between ssn and dwi show different patterns in the four subregions fig 12f ssn exhibits positive correlations with dwi of subregions 1 and 3 and negative correlation with dwi of subregions 2 when ssw ranges from 5 to 50 the dwi of subregion 4 exhibits little correlation with ssn 4 5 discussion the weihe river basin is located on the monsoon transition zone which means the coupled effects of the monsoons mid latitude westerly and plateau circulation systems on precipitation must be considered when we study the dry wet conditions the imf component with a period of 2 3 years is the main component of dwi and contains over 50 of variance of dwi in each of the four subregions fig 7 significant positive correlations between easmi and the imf component with a period of 2 3 years in subregions 1 and 3 were found fig 9 which means easm is the main factor that controls the dry wet variability in subregions 1 and 3 on the biennial and triennial scales a stronger easm generally leads to a drier condition in subregions 1 and 3 this result is consistent with the finding of huang et al 2006 who reported that quasi biennial oscillation in the thermal state of the tropical western pacific is closely associated with that of water vapor transport by summer monsoon flow over east asia as a result it leads to the oscillation with a period of 2 3 years in the summer monsoon rainfall of china the 2 3 years variability of dwi in subregions 2 and 4 however are nearly not controlled by easm li et al 2015 pointed out that there is more less precipitation over the eastern parts of northwest china during the rainy season when spring sensible heat flux becomes stronger weaker over the eastern part of the tibetan plateau and that precipitation controlled by the plateau climate system have a 3 year periodic behavior therefore the biennial and triennial oscillations of the dry wet conditions in subregions 2 and 4 are mainly influenced by the plateau circulation system on the inter decadal scale the dwi of subregion 4 is significantly correlated with the easmi fig 11a however the correlation between dwi and easmi in subregion 4 is opposite to those in subregions 1 2 and 3 which is consistent with the spatial pattern in fig 3b the result reflects that the dry wet condition has a spatially seesaw pattern on the inter decadal scale which is also supported by the findings of zhang et al 2019c they suggested that the phenomenon occurs mainly because precipitation in the east of the northwestern china is mainly controlled by the summer monsoon system while that in the west of the northwestern china is mainly dominated by the westerly system the two systems show an antiphase relationship on the inter decadal scale our results reveal that subregions 1 and 3 are mainly controlled by the easm system while subregion 4 is dominated by the westerly system like subregion 4 subregion 2 is nearly not affected by easm on the biennial and triennial scales however the correlation between dwi and easmi in subregion 2 is similar to those in subregions 1 and 3 on the inter decadal scale which are different from that in subregion 4 it implies that the dry wet conditions in subregions 1 2 and 3 are affected by other factors on the inter decadal scale several previous studies have found that there are certain antiphase relationships between easm and sasm on inter annual and inter decadal scales caused by the enso activities and others hong et al 2006 shi et al 2009 shukla and paolino 1983 sun and ying 1999 wang et al 2003 zhang 2001 fig 11a and b reveal that the correlations of dwi with easmi in subregions 1 2 and 3 on inter decadal scales are opposite to the correlations of dwi with mhi in subregion 1 2 and 3 it indicates that sasm influences the dry wet conditions in subregions 1 2 and 3 on inter decadal scales a stronger sasm leads to a drier condition in subregions 1 2 and 3 on the inter decadal scale in summary subregions 1 and 3 are mainly controlled by easm and sasm subregion 2 is mainly controlled by sasm and the plateau circulation system subregion 4 is mainly controlled by the westerly and plateau circulation system on the multi decadal scale mei pdoi aoi and naoi all exhibit positive correlations with the dwis of the four subregions figs 9 and 12 however on the centennial scale mei pdoi aoi and naoi exhibit negative correlations with the dwis fig 10 which are consistent with those results showed in fig 12 when ssw is larger than 40 on the centennial scale pdoi exhibits significant correlations with the dwis of subregions 1 and 3 while no correlations were detected between pdoi and the dwis of subregions 2 and 4 qian and zhou 2014 suggested that the phase shift of pdo triggers the change of the east asian land sea thermal contrast and further induces the variation of easm therefore the impacts of pdo on the dry wet conditions of the weihe river basin are essentially interconnected with those of easm solar activities quantified by ssn exhibit different influences on the dry wet conditions of four subregions figs 10 and 12 on the multi decadal and centennial scales a stronger solar activity leads to a wetter condition in subregion 2 but a drier condition in subregion 1 the large difference between the two adjacent subregions reveals the sensitive and strong responses of the dry wet conditions in the monsoon transition zone to solar activity it has been found that solar activity can affect the rainband in the asian monsoon transition zone on the inter decadal scale herman and goldberg 1978 wang et al 2005 zhao et al 2012 zhao et al 2012 pointed out that the solar activity modulates the decadal variability of summer precipitation by strengthening or weakening monsoon circulation when the solar activity is stronger easm also becomes stronger which makes subregion 1 becomes drier and subregion 2 become wetter subregion 3 is located on the transition zone between the zones strongly controlled by easm and rarely controlled by easm the large spatial heterogeneity of the dry wet conditions featuring the monsoon transition zone is clearly shown in subregion 3 fig 3 the dry wet conditions of the areas near subregion 1 are neutralized by the dry wet condition of the areas near subregion 2 therefore the dwi of subregion 3 is not correlated with the ssn in this study we applied the advanced statistical methods to quantify the relative contributions of different climatic oscillations e g enso pdo ao and nao and monsoon systems namely easm and sasm to the dynamics of the dry wet conditions in the weihe river basin over the recent 547 years the statistical methods provide a relatively objective way to distinguish the relative roles of the different climate patterns on controlling the dry wet conditions in the weihe river basin however these methods can t completely distinguish the interactive effects of the different climate patterns besides the different climatic oscillations are usually interrelated with each to some extent it is out of the scope of the statistical methods and this study to quantify the interconnections of the different climatic oscillations and their interactive effects on the dry wet dynamics in the weihe river basin to completely address this issue physics based simulations need be conducted in the future 5 conclusions in this study we investigated the dry wet conditions of the weihe river basin which is an important monsoon transition zone in china and is influenced by the monsoons westerly plateau circulation system and regional to global atmospheric oscillations from 1470 to 2016 we reconstructed the dry wet conditions based on historical writings and station observations from 1470 to 2016 we further analyzed the variability of dry wet conditions of this region on multiple time scales from biennial to centennial seven precipitation indices were chosen to describe the precipitation characteristics in this region and were analyzed by the empirical orthogonal function eof method to reveal the spatiotemporal patterns of dry wet conditions in the weihe river basin on these analyses we found that the weihe river basin can be divided into four unique distinguishable hydroclimatic subregions with subregion 1 located in the guanzhong plain southeast subregion 2 located in the longxi region west subregion 3 located in the hilly loess region central and subregion 4 located in the loess plateau north we then derived the periods trends and abrupt changes of the dry wet index using the ceemdan method power spectrum analysis and pettitt test subsequently spearman rank correlation analysis and moving average were further applied to detect the driving factors of dry wet condition variability on the inter annual inter decadal multi decadal and centennial scales finally the relationships between driving factors were discussed in the weihe river basin the dwi series consists of different components with a period of 2 3 7 11 16 22 27 33 60 80 and 100 years there is an abrupt change from the dry phase to wet phase around 1641 in subregions 1 and 3 and around 1657 in subregion 4 all of the four subregions show a long term wetting trend over the 1470 2016 period subregion 1 and 3 are mainly controlled by easm and sasm subregion 2 is mainly controlled by sasm and plateau circulation system subregion 4 is mainly controlled by the westerly and plateau circulation system on the inter annual scale a stronger easm generally leads to a drier condition in subregions 1 and 3 on the inter decadal scale a stronger sasm usually causes a drier condition in subregions 1 2 and 3 ao nao makes a great contribution to the spatial patterns of the intensity of extreme precipitations enso remarkably influences the precipitation extremes of the whole region from two perspectives intensity and frequency on the multi decadal scale when enso pdo ao and nao become stronger the weihe river basin overall becomes drier however on the centennial scale stronger enso pdo ao and nao can lead to a wetter condition in the weihe river basin the spatial patterns of influences of pdo on the dry wet conditions are coincident with that of easm solar activity exhibits completely different influences on the dry wet conditions of four subregions on the multi decadal and centennial scales stronger solar activity usually leads to a wetter condition in subregion 2 but a drier condition in subregion 1 the large difference between the two adjacent subregions reveals the sensitive and strong responses of the dry wet condition in monsoon transition zone to solar activities this paper presents a novel study on identifying the spatial patterns and temporal variability of the dry wet conditions for the weihe river basin on multiple time scales it also developed a solid integrated framework to decompose the multi centennial dry wet index series into components with different periods and quantify the contributions of monsoon systems large scale atmospheric oscillations westerly circulation system plateau circulation systems and solar activities the methods in this study are generic and can be easily applied by other similar studies more important this study fills in the knowledge gap on the spatiotemporal variability of the dry wet conditions across the weihe river basin and their driving forces on multiple time scales potentially providing a valuable knowledge for better projecting further changes in this region and its surrounding areas credit authorship contribution statement xinyu chen conceptualization data curation investigation methodology software formal analysis visualization writing original draft quan quan conceptualization investigation methodology software formal analysis writing review editing funding acquisition ke zhang conceptualization investigation software methodology formal analysis writing original draft writing review editing funding acquisition jiahua wei software writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement this work was supported by the national key research and development program of china 2016yfa0601503 2017yfc0403600 and 2016yfc0402701 the belt and road special foundation of the state key laboratory of hydrology water resources and hydraulic engineering 2019491411 hydraulic science and technology plan foundation of shaanxi province 2019slkj b1 fundamental research funds for the central universities of china b200204038 natural science foundation of jiangsu province bk20180022 key research development program of ningxia hui automonous region china 2018beg02010 and six talent peaks project in jiangsu province ny 004 the first author is grateful to prof shengzhi huang for his helpful advice 
25796,the weihe river basin wrb in a monsoon transition zone of east asia interacts with multiple weather systems and is susceptible to floods and droughts we developed a framework based on the empirical orthogonal function analysis complete ensemble empirical mode decomposition with adaptive noise analysis and moving average based spearman rank correlation to identify the spatial patterns of the dry wet conditions from 1470 to 2016 decompose the dry wet index into the leading components with a period between 2 and 3 years and 100 years and discover their driving forces on multiple time scales results show that wrb can be divided into four distinguishable hydroclimatic subregions located in the southeast west central and north the east asian summer monsoon and south asian summer monsoon impact this region mainly on the inter annual and inter decadal scales while the impacts of enso pdo ao and nao are dominant on the multi decadal and centennial scales graphical abstract image 1 keywords dry wet conditions multiple time scales floods and droughts east asian summer monsoon south asian summer monsoon 1 introduction a monsoon system acts as an important part of the global climate system and anomalous summer monsoon activities can trigger floods and droughts hao et al 2016 ninomiya 1999 every monsoon system has a limited influential area and usually creates two types of hydroclimatic zones namely humid and arid regions the monsoon transition zone corresponding to the climate transition zone is a region where cold dry and warm wet air masses frequently encounter as a result its climate exhibits significant fluctuation with a high amplitude and large spatial variability on multiple time scales therefore flood and drought possibly co occur in this area where dry wet conditions differ considerably in space these phenomena greatly influence the prevention and control of floods and droughts in this area chao et al 2019 2020 huang et al 2019 huo et al 2020 li et al 2011 luo et al 2018 mu et al 2021 zhang 2004 zhang et al 2021 moreover the monsoon transition zone is mainly composed of semi arid regions which have fragile ecosystems and are extremely susceptible to floods and droughts guan et al 2019 zhang et al 2019a zhu et al 2020 therefore exploring the characteristics of dry wet variation in the monsoon transition zone is quite important the dry wet conditions of a region are complicatedly affected by various factors such as monsoon system atmospheric oscillation solar activities and geographic features hong et al 2010 tian et al 2020 wang et al 2019b solar activities are an external driving factor while atmospheric oscillation and monsoon system are internal factors influencing the dry wet conditions bhalme and mooley 1980 fang et al 2019 higgins et al 2000 hurrell et al 2003 liu et al 2017 mohmmed et al 2018 zhang et al 2007 2008 zhao and han 2005 barlow et al 2001 revealed the significant relationships between the three primary modes of pacific sea surface temperature variability and the american warm season hydroclimate li et al 2021 found that global warming el niño southern oscillation enso and local effects are all significant driving factors controlling the spatiotemporal variability of precipitation extremes over the yangtze river basin besides many studies were carried out to reveal the relationships between regional dry wet conditions and their driving factors across the world han et al 2019 shi et al 2016 wang et al 2019c zaroug et al 2014 zhang et al 2015 zhao et al 2012 however most of these studies in the monsoon transition zones are focused on the time scales between annual and centennial scales very few studies investigated the dry wet conditions and their driving forces beyond the centennial scale in addition the impacts of different driving factors on the dry wet conditions in monsoon transition zone have substantially spatial differential patterns and intensities on different time scales which are rarely explored in the literature china s precipitation is mainly influenced by monsoons westerly and plateau circulation systems while their impacts are closely related to the northern edge of east asian summer monsoon zone zhang et al 2019c the regions to the south of the northern edge of east asian summer monsoon zone are mainly controlled by the monsoon system while those to the north are mainly dominated by the westerly system however the northern edge of the east asian summer monsoon zone will oscillate between south and north with the varying intensity of east asian monsoon every year ma et al 2011 the oscillation zone corresponds to the monsoon transition zone due to the oscillation of the edge the monsoon transition zone is subjected to the coupling effect of east asian monsoon system and mid latitude westerly system which leads to a great spatial variability of dry wet conditions the weihe river basin located in northwestern inland of china is the largest sub basin of the yellow river and located within the east asian monsoon transition zone yang et al 2019 at present the boundary of the east asian monsoon zone is still under a great debate shi et al 2009 thus no consensus is reached on the climatic zoning classification of the weihe river basin in addition the dry wet conditions in the weihe river basin have large spatiotemporal variability making it very hard to be predicted based on the previous studies the factors influencing the dry wet conditions of the weihe river basin include the asian summer monsoon systems such as east asian summer monsoon easm and south asian summer monsoon sasm atmospheric oscillations such as enso pacific decadal oscillation pdo arctic oscillation ao and north atlantic oscillation nao and other natural driving factors like solar activities bi et al 2013 liu et al 2017 wan et al 2018 zhao et al 2015 in most of the previous studies the easm and sasm systems in the asian summer monsoon system are not isolated from each other easm and sasm together serve as the two main components of the asian summer monsoon system huan and li 2018 the two monsoons differ largely in their circulation characters and impacting areas and are physically separated and closely correlated to each other tao 1987 the spatial patterns and intensities of the impacts of the easm and sasm systems on the dry wet conditions in the weihe river basin remain to be further explored the objectives of this study are 1 to reveal the spatial patterns of dry wet conditions in the weihe river basin 2 to reconstruct the dry wet index series of the weihe river basin from 1470 to 2016 and derive its periods trends and changes and 3 to identify the driving factors of the dry wet conditions and reveal the spatial patterns of their influences on multiple time scales this study is valuable for better projecting future changes in this region and its surrounding areas 2 study area and data 2 1 study area our study area is the weihe river basin fig 1 the weihe river located in the east of the northwest inland of china originates from niaoshu mountain in weiyuan county gansu province and finally merges into the yellow river at tongguan county shaanxi province fig 1 it flows through three provinces in china including gansu ningxia and shaanxi located within 33 5 37 5 n in latitude and 103 5 110 5 e in longitude this basin has a total area of 134 800 km2 and is characterized by distinctive seasonality with abundant precipitation and high temperature in summer and little precipitation and low temperature in winter the average annual precipitation is about 600 mm 60 of which occurs in the flood season from june to september and the annual temperature ranges from 7 8 c to 13 5 c chang et al 2014 the weihe river basin is situated in the monsoon transition zone which is subjected to the coupled effect of monsoon and westerly circulation systems moreover it shows complex terrain therefore its dry wet conditions and variation delivers a great spatial variability 2 2 data to quantify the dry wet conditions of the weihe river basin over a long time span we adopted the dryness wetness index dwi from the yearly charts of dryness wetness in china for the last 500 year period hereinafter called the charts published by the chinese academy of meteorological sciences 1981 in the charts dry wet conditions during 1470 1979 in china are quantified as the dwi and classified into five classes i e 1 2 3 4 and 5 1 2 3 4 and 5 indicate very wet wet normal dry and very dry respectively china has continuous and consistent historical records about agricultural conditions weather and disasters such as floods and droughts which are valuable for determining the historical dry wet conditions therefore the recorded agricultural conditions and flood and drought events can reflect the historical dry wet conditions the dwi series from 1470 to 1950 are obtained from more than 2200 local annals and a large number of other historical writings that qualitatively describe the dry wet conditions while the dwi series from 1951 to 1979 are obtained from measured annual precipitation amount table 1 the dwi of each year in the charts was determined cautiously after full consideration of geographical features for example the flood caused by external river inflow was not considered as a flood resulted from the wet condition while a good harvest in the arid area means a wet condition since there are measured precipitation data from 1951 to 1979 the dwi values during this period were directly calculated using the observed precipitation data in order to keep the frequency of dry wet conditions before 1951 the same as that after 1951 the dwi values after 1951 were determined using the same method as shown in table 1 that was used to compute the dwi values prior 1951 there are 120 sites with recorded dry wet conditions from 1470 to 1979 across china fig 1 the data from the 120 sites were interpolated to produce yearly dry wet index contour maps by the chinese academy of meteorological sciences based on these maps we derived the 0 5 0 5 gridded dry wet index data for the weihe river basin from 1470 to 1960 in this research we reconstructed the dwi during 1470 2016 for these years after 1960 we calculated the dry wet index based on the daily 0 5 0 5 observation based gridded precipitation data from china meteorological administration http data cma cn data cdcdetail datacode surf cli chn pre day grid 0 5 html to quantify the influence of monsoons the easm index easmi and the monsoon hadley circulation index mhi are selected to quantify easm and sasm respectively table 2 the easmi is derived from the unified dynamic normalized seasonality dns monsoon index defined by li and zeng 2002 which has a good performance in depicting the easm and has been adopted by the national oceanic and atmospheric administration noaa to monitor the easm strength li et al 2010 the mhi is defined by goswami et al 1999 the two indices are broadly used and proved to be reliable wang and fan 1999 wang et al 2008 average monthly data during 1979 2014 from the reanalysis data of https climatedataguide ucar edu climate data era interim the european centre for medium range weather forecasts ecmwf were used to calculate the monsoon indices above solar activities are quantified by the sunspot number ssn provided by the solar influences data analysis center sidc the ssn data of each year during 1700 2016 were retrieved through http sidc oma be silso datafiles table 2 the pdo index pdoi was provide by university of washington http research jisao washington edu pdo the multivariate el niño southern oscillation index mei wolter and timlin 1998 was selected to quantify enso considering that mei contains more information compared to the other indices kiem and franks 2001 the mei data were downloaded from http www esrl noaa gov psd enso mei table html the ao index aoi was downloaded from http www atmos colostate edu ao data ao index html although ao and nao are closely correlated with each other evidence shows that the teleconnection signals of ao and nao show a great difference in some regions therefore the impacts of nao are also investigated in the study the nao index naoi was downloaded from https climatedataguide ucar edu climate data hurrell north atlantic oscillation nao index station based basic information of the above indices is summarized in table 2 these large scale oscillation indices actually quantify the strength of driving factors that influence the dry wet conditions in the weihe river basin therefore we also called them the driving factor indices dfis in this study 3 methodology 3 1 subregion division since the weihe river basin spans an area of 134 800 km2 it is important to investigate the spatial variability of dry wet conditions and their potentially differential responses to the driving factors therefore it is important to divide the weihe river basin into certain subregions based on the spatial patterns and temporal variation of dry wet conditions it is generally regarded that extreme precipitation events are the main causes of floods while persistent low rainfall is the main cause of droughts liu et al 2017 tao and zhang 2020 wang et al 2019a zhang et al 2019b therefore the other aspects of precipitation rather than annual precipitation total should also be considered especially these characteristics related to extreme hydrometeorological events seven precipitation indices were adopted to quantify various aspects of precipitation such as intensity frequency duration and amount table 3 the seven precipitation indices are max 1 day precipitation amount rx1day max 5 day precipitation amount rx5day number of mild precipitation days r10 number of heavy precipitation days r25 consecutive dry days cdd consecutive wet days cwd and annual precipitation total prcptot detailed definitions of these indices are summarized in table 3 in this study we applied the rclimdex 1 0 software developed by zhang and yang 2004 to compute these precipitation indices in each grid cell of the weihe river basin with a spatial resolution of 0 5 0 5 the empirical orthogonal function eof analysis was performed to obtain the spatial patterns of the seven precipitation indices details of the eof analysis are referred to perry and niemann 2007 based on the results of the eof analysis the spatial patterns and their variations of the dry wet condition can be revealed we then divide the weihe river basin into hydroclimatic subregions based on the eof results terrain and geography 3 2 extension of the dwi index gridded annual dwi values with a spatial resolution of 0 5 0 5 from 1470 to 1960 were directly digitized and obtained from the charts see section 2 2 to extend the series to 2016 we further calculated the gridded dwi values from 1961 to 2016 from the 0 5 0 5 gridded precipitation data provided by china meteorological administration using the same dwi classification method summarized in table 1 as those used to produce the charts through this method we seamless produced cohesive gridded dwi time series from 1470 to 2016 to minimize the impacts of different station sample sizes before and after 1961 we conduct the further analyses on the subregion scale rather than on a grid cell scale 3 3 complete ensemble empirical mode decomposition with adaptive noise to detect the components and their associated periods and trends of the dwi time series we applied the complete ensemble empirical mode decomposition with adaptive noise ceemdan method torres et al 2011 the ceemdan method was first proposed by torres et al 2011 as an improved version of the ensemble empirical mode decomposition eemd the main principle of eemd is to decompose a time series into multiple intrinsic mode functions imfs that contain the local characteristics of the raw time series on different time scales and a residue that reveals the long term trend of the raw time series huang et al 1998 wu and huang 2009 however torres et al 2011 pointed out several limitations of the eemd method despite that it can overcome the mode mixing by adding white noises the limitations include that the residual noises cannot be completely eliminated and the modes which are decomposed from the raw time series are not unique to overcome these limitations torres et al 2011 proposed the ceemdan method which can eliminate the residual noises of the imf components and more efficiently decompose the raw time series at the minimum computation cost advantages of this method have been verified by some studies wang et al 2018 zhang et al 2017 the periods of the dwis were detected through the power spectrum analysis percival and walden 1993 the ceemdan function of the ncar command language ncl was used to carry out the ceemdan analysis which can be downloaded from https www ncl ucar edu document functions built in ceemdan shtml 3 4 attribution of the variation of dry wet conditions on multiple time scales to attribute the variation of dry wet conditions on multiple time scales we developed an integrated attribution framework fig 2 the attribution consists of two methods the correlation analysis based on the ceemdan analysis left of fig 2 and the correlation analysis based on the moving average method right of fig 2 the former is to perform the spearman rank correlation analysis to calculate the correlations between the time series of the decomposed components imfs with different periods of the dwi series and the time series of the dfis the ceemdan analysis is considered to be able to divide the dwi into modes components with different periods which reflect the influences of various driving factors on different time scales as a result the spearman rank correlation analysis can identify the specific driving factors that influence the dry wet variation on a specific time scale owing to the low variance contribution rates of the imfs with a main period greater than 100 years they are recomposed with the residue to form the recomposed time series which stands for the dry wet variation of the weihe river basin on the centennial scale however the correlation analysis based on the ceemdan method ignores the interactions between different components of the dwi series therefore we further applied the correlation analysis based on the moving average to supplement the ceemdan based correlation analysis right of fig 2 the basic principle of the moving average method similar to the low pass filter is to eliminate the influences of these components with a short period and random fluctuations and detect the long term trend of the original series given a time series and a sliding window with a fixed size the first output element of the moving average is obtained by averaging the initial elements of the time series in sliding window then the sliding window shifts towards the end of the time series with one element interval as a step one step movement of the sliding window will produce one output element more details on the method can be found in anderson 2011 in the moving average method the components with a period shorter than the size of the sliding window of a time series are eliminated therefore when multiple moving average operations with different sliding window sizes are applied certain components will be filtered out the interactions between the remaining components of the dwi are kept for the following spearman rank correlation analysis which can overcome the shortcomings of the correlation analysis based on the ceemdan method due to the data limitation these dfis differ in their time lengths see table 2 to make the correlation analysis comparable we chose the common time period i e 1979 2016 to analyze the correlation of the dwi series and its components with the series of the dfis 4 results and discussion 4 1 spatial patterns of precipitation characteristics and hydroclimatic division of the weihe river basin fig 3 shows the spatial patterns of the two leading eof modes of the seven precipitation indices the seven precipitation indices represent different characteristics of precipitation across the weihe river basin rx1day and rx5day quantify the intensities of precipitation extremes while r10 and r25 measure the frequencies of precipitation events cwd and cdd denote the duration of precipitation days and non precipitation days respectively while prcptot quantifies annual precipitation amount table 3 it is clear that the two leading modes of rx1day fig 3a b have similar spatial gradients to those of the corresponding leading modes of cwd fig 3i j similarly the spatial gradients of the two leading modes of rx5day fig 3c d resemble those of the corresponding leading modes of r25 fig 3g h moreover the spatial gradients of the two leading modes of r10 fig 3e f are very close to those of the corresponding leading modes of prcptot fig 3m n in fact the eof1 and eof2 of the six indices rx1day rx5day r10 r25 cwd and prcptot are spatially highly correlated with each other fig 3 all of the six indices are actually indicators of wet conditions higher values of these indices indicate more frequent long lasting or heavier precipitation which can enhance the possibility of floods in contrast the spatial patterns of the eof1 and eof2 of cdd fig 3k i are totally different from those of the eof1 and eof2 of the other six indices the eof1 and eof2 of cdd have a clear west to east spatial gradient fig 3k i while the eof1 and eof2 of the other six indices show an apparent southeast to northwest gradient fig 3 cdd quantifies the number of consecutive dry days serving as an indicator of dryness difference in the spatial patterns of the leading modes of the wetness related indices and dryness related index suggest that the mechanisms or driving factors on the dry wet conditions across this region are likely different and that response to the driving factors differs in space more importantly there are apparent spatial clusters across the leading modes of the seven indices fig 3 indicating that the weihe river basin shows regional patterns in other words the weihe river basin can potentially be divided into subregions that show different dry wet characteristics to derive these subregions we first normalized the values of the eof1 and eof2 of the seven indices into the same range respectively we then combined the fourteen normalized eof modes to produce a map through a linear combination finally we conducted a cluster analysis based on the combined map with the aid of topography as a result we found that the weihe river basin can be divided into four subregions fig 4 subregion 1 is the guanzhong plain located in the southeast of the weihe river basin while subregion 2 is the longxi region with high elevation that is located to the west of the liupanshan mountain and neighbors the qinghai tibetan plateau fig 4 subregion 3 is the hilly loess region with elevation ranging between 400 m and 3900 m fig 4 while subregion 4 is the loess plateau with high elevation the four subregions are visually distinguishable in the eof1 maps of the six indices except cdd see fig 3 highlighting the apparent difference between the four subregions it is interesting to observe that the eof values in subregion 3 are clearly different from those in the other three subregions fig 3 implying the uniqueness of this region in fact subregion 3 is a zone that well reflects the dry wet conditions of the whole weihe river basin which was also reported in a previous study bi et al 2013 bi et al 2013 found that the wetness and dryness in the weihe river basin is mainly controlled by the climate in the central zone of the basin i e subregion 3 in this study in the eof2 maps of the six indices except cdd the signs of values in subregion 1 are opposite to those in subregion 2 and subregion 4 implying that subregion 1 have different dry wet conditions and precipitation characteristics relative to subregions 2 and 4 fig 3 the above results suggest that subregion 3 is the transition zone between subregion 1 subregion 2 and subregion 4 to further identify the driving forces of dry wet conditions in these subregions we analyzed the correlations of the expansion coefficients ecs series of the two leading eofs of the seven precipitation indices with the corresponding dfis series the results show that easmi mhi ao1 nao1 and mei are significantly correlated with at least one leading mode of one of the seven precipitation indices fig 5 in contrast pdoi and ssn do not show a significant correlation with any of the leading modes of the seven precipitation indices fig 5 in other words the dry wet conditions in this region are collectively controlled by easm sasm ao nao and enso while pdo and ssn have weak impacts however the impacts of easm sasm ao nao and enso show apparent differential impacts across this region the easmi series exhibits a significant positive correlation with the 2nd leading mode of rx1day r10 and prcptot fig 5 moreover the signs of the eof2 values of rx1day r10 and prcptot in subregions 1 and 3 are opposite to those in subregions 2 and 4 fig 3b f and n indicating that the strength of easm has polarized impacts on precipitation extremes and total across the weihe river basin the above results are totally understandable when easm is stronger it can carry moist air from the pacific ocean and push it moving further to the north and west causing more precipitation in the north and west i e subregions 2 and 4 and correspondingly less precipitation in the south and east namely subregions 1 and 3 the mhi series has a significant positive correlation with the 1st leading mode of cdd fig 5 the signs of the eof1 values of cdd across the whole region are negative fig 3k these results indicate that a stronger sasm leads to more precipitation across the whole region and correspondingly reduces the consecutive dry days in other words sasm tends to have a unanimous impact on precipitation across this region in terms of mei it is significantly negatively correlated with the 1st leading mode of rx5day and r25 furthermore the signs of the eof1 values of rx5day and r25 are all positive across this region fig 3c and g these results indicate that a stronger enso leads to less rx5day and r25 across the whole region naoi and aoi are significantly negatively correlated with the 2nd leading mode of rx1day fig 5 in contrast easmi has a significant positive correlation with the 2nd leading mode of rx1day fig 5 these results indicate that ao nao influence the spatial pattern of eof2 of max 1 day precipitation amount in an opposite way comparing to easm in summary easm sasm ao nao and enso show apparent impacts on the dry wet conditions across the weihe river basin but pdo and ssn do not show significant impacts on the dry wet conditions of this region easm ao nao and enso mainly impact the precipitation extremes in this region while sasm controls the dryness extreme across the whole region the impacts of easm ao and nao show spatially differential impacts while enso show spatially unanimous impacts 4 2 periods trends and abrupt changes of the dwi series the dwi series of the four subregions from 1470 to 2016 is shown in fig 6 according to the pettitt test 1979 the dwi series of subregion 1 subregion 3 and subregion 4 are statistically non stationary fig 6a c and d an abrupt change occurred from the dry phases to the wet phases around 1641 in both subregions 1 and 3 fig 6a and c while an abrupt change from the dry phases to the wet phases happened around 1657 in subregion 4 since human activities were not substantial in 1640s and 1650s the abrupt changes of the dwi are likely caused by the long term variability within the climate system of the weihe river basin we further applied the ceemdan analysis to decompose the dwi series of the four subregions into a set of imf modes according to the quasi period distribution the dwi series can be divided into six modes with respective quasi periods of 2 3 years 7 10 years 16 22 years 27 33 years 60 80 years and 100 years in all of the four subregions fig 7 the first five leading modes can explain more than 95 of the variance of dwi fig 7 while the 1st leading modes of the four subregions which have a quasi period between 2 and 3 years can explain 50 of variance of the dwi series the quasi periods of the leading modes are different between the four subregions fig 7 the residues represent the long term trends of the dwi series of the four subregions from 1470 to 2016 as shown in fig 7 the long term trends can explain less than 3 5 of variance in the dwi series in all of the four subregions although the summed variance contributions of the modes with a period of more than 100 years i e imf6 9 are small less than 10 in the four subregions these modes are important for explaining the long term variability of the dry wet conditions to further investigate the variability and trends of the dwi series on different time scales we further recomposed the dwi series by excluding the imf modes stepwise for example we excluded the imf1 mode from the dwi series to produce a recomposed series named rs imf1 we excluded both imf1 and imf2 modes from the dwi series to produce a recomposed series which is named as rs imf1 2 we excluded the nine imf modes from the dwi series to produce the residual series that is named as rs imf1 9 based on the pettitt test we derived the statistically significant abrupt change points for all recomposed series for the four subregions as shown in fig 8 the change points of all recomposed series in subregion 1 are the same as or very close to those in subregion 3 suggesting that the dry wet oscillations and variation are nearly consistent in these two subregions regarding subregion 4 the recomposed dwi of the subregion 4 by excluding the components with short periods 2 3 and 7 10 years i e rs imf1 2 has the same change point as those of subregion 1 and 3 fig 8 these results indicate that the components with a period of 2 3 and 7 10 years in subregion 1 and 3 exhibit different variability from those in the subregion 4 the rs imf1 5 series is obtained by combining imf6 imf7 imf8 imf9 and residue which all have a period of 100 years or above the abrupt change points of rs imf1 5 in the four subregions are nearly the same and close to those of the original dwi series therefore it is reasonable to say that rs imf1 5 reflects the non stationarity and long term variability of the dry wet conditions in these regions furthermore rs imf1 9 is the residue of the dwi series and represents the long term variability of the dwi as shown in fig 8 all rs imf1 9 series in the four subregions have a negative trend indicating that all of the four regions have a negative trend in dwi from 1470 to 2016 in other words the whole weihe river basin has a long term wetting trend during 1470 2016 4 3 correlations between the imf components of dwi and the dfis fig 9 shows the spearman rank correlation coefficients between the various dwi components with a period less than 100 years and the dfis for the four subregions all in fig 9 represents the original dwi series there are significant positive correlations between the easmi and the raw dwi in both subregion 1 fig 9a and subregion 3 fig 9c moreover the significant positive correlations are also detected between the easmi and the imf components with a period of 2 3 years in subregion 1 fig 9a and subregion 3 fig 9c additionally easmi also shows a positive correlation with the original dwi series and its imf1 component in both subregion 2 fig 9c and subregion 4 fig 9d in contrast all of the other dfis such as ssn aoi naoi and mei do not show significant correlation with the dwi series and their imf1 components for all of the four regions fig 9 these results suggest that easm is the main factor that influences the dry wet conditions in subregion 1 and subregion 3 on the time scale of 2 3 years this result is also consistent with these results in figs 3 and 5 which shows that a stronger easm corresponds to a drier condition in subregions 1 and 3 different from easmi mhi is significant positively correlated with the imf components with a period of 27 33 years and 60 80 years in subregion 1 fig 9a and with the imf component with a period of 60 80 years in subregion 2 fig 9b aoi pdoi and mei are significant positively correlated with the imf components with a period of 60 80 years in subregion 1 fig 9a and subregion 3 fig 9c nao also shows a significant positive correlation with the imf component with a period of 60 80 years in subregion 3 fig 9c these results suggest that ao nao pdo and enso activities mainly influence subregions 1 and 3 on the time scales of 60 80 years ssn exhibits a significant positive correlation with the imf component with a period of 16 22 years in subregion 2 fig 9b suggesting that the dry wet variability on the time scale of 16 22 years is mainly impacted by ssn in subregion 2 interestingly none of the seven dfis shows a significant correlation with the dwi series and its imf components in subregion 4 fig 9d as mentioned above the recomposed series rs imf1 5 of the dwi series represents the centennial scale variability of the dry wet conditions we further analyzed the correlation of the rs imf1 5 series with the seven dfis the results show that neither of easmi and mhi is significantly correlated with rs imf1 5 for all of the four subregions fig 10 implying that the dominant monsoon systems around this region have little influence on the dry wet variability of the weihe river basin on the centennial scale in contrast mei exhibits a significant negative correlation with the rs imf1 5 series of subregions 1 2 and 3 fig 10 aoi exhibits a significant negative correlation with the rs imf1 5 series of subregions 1 and 3 fig 10 in addition there is a significant negative correlation between pdoi and rs imf1 5 in subregions 1 and 3 fig 10 while ssn exhibits a significant negative correlation with rs imf1 5 of subregion 2 and a significant positive correlation with rs imf1 5 of subregion 1 fig 10 in summary the above results suggest that the two dominant monsoon systems i e easm and sasm around the study area mainly control the dry wet conditions of the weihe river basin on the relatively short time scales i e 2 3 years 27 33 years and 60 80 years while enso ao nao and pdo mainly influence the dry wet conditions of the weihe river basin on the long time scales i e 60 80 years and 100 years 4 4 correlations between the dwi and the dfis based on sliding window method we further investigated the correlations between the dwi and monsoon indices easmi and mhi based on sliding window method in the four subregions when the size of sliding window ssw equals 1 the easmi exhibits significant positive correlations with the dwi of subregions 1 and 3 fig 11 a and b when ssw ranges from 3 to 6 the easmi exhibits negative correlations with dwi of subregion 2 3 and 4 fig 11a when ssw ranges from 7 to 16 the easmi exhibits negative correlations with dwi of subregion 1 2 and 3 and a positive correlation with dwi of subregion 4 when ssw ranges from 17 to 22 the easmi is positively correlated with dwi of subregions 1 and 3 as ssw changes the importance of the components with different periods also varies when ssw equals 1 the component with a period 2 3 years is the major component which is positively correlated with easmi fig 9 when ssw ranges from 3 to 16 the importance of the components with a period of 7 11 years and 16 22 years increase while that of the component with a period of 2 3 years decreases both or one of components with a period of 7 11 years and 16 22 years in the four subregions exhibit negative correlations with easmi which leads to the negative correlations between dwi and easmi when the ssw ranges from 2 to 18 mhi is significantly positively correlated with the dwi in subregions 1 2 and 3 fig 11b however the curve of subregion 4 in fig 11b is inconsistent with those of the other subregions dwi of subregion 4 is nearly not correlated with mhi according to figs 9 and 10 the mhi exhibits positive correlations with the dwi of subregion 1 2 and 3 on multi decadal scale fig 12 shows the spearman rank correlation coefficients of dwis of the four subregions with the mei pdoi aoi naoi ssn and dwis using different ssws when ssw ranges from 12 to 36 mei pdoi aoi and naoi exhibit positive correlations with the dwi of the four subregions fig 12a d however when ssw ranges from 40 to 50 mei aoi and naoi are negatively correlated with the dwi of the four subregions fig 12a c and d the correlation curves based on the moving window method of subregions 2 and 4 clearly differ from those of subregions 1 and 3 pdoi exhibits significant negative correlations with the dwi of subregion 1 and 3 and positive correlations with the dwi of subregions 2 and 4 fig 12b the correlations between ssn and dwi show different patterns in the four subregions fig 12f ssn exhibits positive correlations with dwi of subregions 1 and 3 and negative correlation with dwi of subregions 2 when ssw ranges from 5 to 50 the dwi of subregion 4 exhibits little correlation with ssn 4 5 discussion the weihe river basin is located on the monsoon transition zone which means the coupled effects of the monsoons mid latitude westerly and plateau circulation systems on precipitation must be considered when we study the dry wet conditions the imf component with a period of 2 3 years is the main component of dwi and contains over 50 of variance of dwi in each of the four subregions fig 7 significant positive correlations between easmi and the imf component with a period of 2 3 years in subregions 1 and 3 were found fig 9 which means easm is the main factor that controls the dry wet variability in subregions 1 and 3 on the biennial and triennial scales a stronger easm generally leads to a drier condition in subregions 1 and 3 this result is consistent with the finding of huang et al 2006 who reported that quasi biennial oscillation in the thermal state of the tropical western pacific is closely associated with that of water vapor transport by summer monsoon flow over east asia as a result it leads to the oscillation with a period of 2 3 years in the summer monsoon rainfall of china the 2 3 years variability of dwi in subregions 2 and 4 however are nearly not controlled by easm li et al 2015 pointed out that there is more less precipitation over the eastern parts of northwest china during the rainy season when spring sensible heat flux becomes stronger weaker over the eastern part of the tibetan plateau and that precipitation controlled by the plateau climate system have a 3 year periodic behavior therefore the biennial and triennial oscillations of the dry wet conditions in subregions 2 and 4 are mainly influenced by the plateau circulation system on the inter decadal scale the dwi of subregion 4 is significantly correlated with the easmi fig 11a however the correlation between dwi and easmi in subregion 4 is opposite to those in subregions 1 2 and 3 which is consistent with the spatial pattern in fig 3b the result reflects that the dry wet condition has a spatially seesaw pattern on the inter decadal scale which is also supported by the findings of zhang et al 2019c they suggested that the phenomenon occurs mainly because precipitation in the east of the northwestern china is mainly controlled by the summer monsoon system while that in the west of the northwestern china is mainly dominated by the westerly system the two systems show an antiphase relationship on the inter decadal scale our results reveal that subregions 1 and 3 are mainly controlled by the easm system while subregion 4 is dominated by the westerly system like subregion 4 subregion 2 is nearly not affected by easm on the biennial and triennial scales however the correlation between dwi and easmi in subregion 2 is similar to those in subregions 1 and 3 on the inter decadal scale which are different from that in subregion 4 it implies that the dry wet conditions in subregions 1 2 and 3 are affected by other factors on the inter decadal scale several previous studies have found that there are certain antiphase relationships between easm and sasm on inter annual and inter decadal scales caused by the enso activities and others hong et al 2006 shi et al 2009 shukla and paolino 1983 sun and ying 1999 wang et al 2003 zhang 2001 fig 11a and b reveal that the correlations of dwi with easmi in subregions 1 2 and 3 on inter decadal scales are opposite to the correlations of dwi with mhi in subregion 1 2 and 3 it indicates that sasm influences the dry wet conditions in subregions 1 2 and 3 on inter decadal scales a stronger sasm leads to a drier condition in subregions 1 2 and 3 on the inter decadal scale in summary subregions 1 and 3 are mainly controlled by easm and sasm subregion 2 is mainly controlled by sasm and the plateau circulation system subregion 4 is mainly controlled by the westerly and plateau circulation system on the multi decadal scale mei pdoi aoi and naoi all exhibit positive correlations with the dwis of the four subregions figs 9 and 12 however on the centennial scale mei pdoi aoi and naoi exhibit negative correlations with the dwis fig 10 which are consistent with those results showed in fig 12 when ssw is larger than 40 on the centennial scale pdoi exhibits significant correlations with the dwis of subregions 1 and 3 while no correlations were detected between pdoi and the dwis of subregions 2 and 4 qian and zhou 2014 suggested that the phase shift of pdo triggers the change of the east asian land sea thermal contrast and further induces the variation of easm therefore the impacts of pdo on the dry wet conditions of the weihe river basin are essentially interconnected with those of easm solar activities quantified by ssn exhibit different influences on the dry wet conditions of four subregions figs 10 and 12 on the multi decadal and centennial scales a stronger solar activity leads to a wetter condition in subregion 2 but a drier condition in subregion 1 the large difference between the two adjacent subregions reveals the sensitive and strong responses of the dry wet conditions in the monsoon transition zone to solar activity it has been found that solar activity can affect the rainband in the asian monsoon transition zone on the inter decadal scale herman and goldberg 1978 wang et al 2005 zhao et al 2012 zhao et al 2012 pointed out that the solar activity modulates the decadal variability of summer precipitation by strengthening or weakening monsoon circulation when the solar activity is stronger easm also becomes stronger which makes subregion 1 becomes drier and subregion 2 become wetter subregion 3 is located on the transition zone between the zones strongly controlled by easm and rarely controlled by easm the large spatial heterogeneity of the dry wet conditions featuring the monsoon transition zone is clearly shown in subregion 3 fig 3 the dry wet conditions of the areas near subregion 1 are neutralized by the dry wet condition of the areas near subregion 2 therefore the dwi of subregion 3 is not correlated with the ssn in this study we applied the advanced statistical methods to quantify the relative contributions of different climatic oscillations e g enso pdo ao and nao and monsoon systems namely easm and sasm to the dynamics of the dry wet conditions in the weihe river basin over the recent 547 years the statistical methods provide a relatively objective way to distinguish the relative roles of the different climate patterns on controlling the dry wet conditions in the weihe river basin however these methods can t completely distinguish the interactive effects of the different climate patterns besides the different climatic oscillations are usually interrelated with each to some extent it is out of the scope of the statistical methods and this study to quantify the interconnections of the different climatic oscillations and their interactive effects on the dry wet dynamics in the weihe river basin to completely address this issue physics based simulations need be conducted in the future 5 conclusions in this study we investigated the dry wet conditions of the weihe river basin which is an important monsoon transition zone in china and is influenced by the monsoons westerly plateau circulation system and regional to global atmospheric oscillations from 1470 to 2016 we reconstructed the dry wet conditions based on historical writings and station observations from 1470 to 2016 we further analyzed the variability of dry wet conditions of this region on multiple time scales from biennial to centennial seven precipitation indices were chosen to describe the precipitation characteristics in this region and were analyzed by the empirical orthogonal function eof method to reveal the spatiotemporal patterns of dry wet conditions in the weihe river basin on these analyses we found that the weihe river basin can be divided into four unique distinguishable hydroclimatic subregions with subregion 1 located in the guanzhong plain southeast subregion 2 located in the longxi region west subregion 3 located in the hilly loess region central and subregion 4 located in the loess plateau north we then derived the periods trends and abrupt changes of the dry wet index using the ceemdan method power spectrum analysis and pettitt test subsequently spearman rank correlation analysis and moving average were further applied to detect the driving factors of dry wet condition variability on the inter annual inter decadal multi decadal and centennial scales finally the relationships between driving factors were discussed in the weihe river basin the dwi series consists of different components with a period of 2 3 7 11 16 22 27 33 60 80 and 100 years there is an abrupt change from the dry phase to wet phase around 1641 in subregions 1 and 3 and around 1657 in subregion 4 all of the four subregions show a long term wetting trend over the 1470 2016 period subregion 1 and 3 are mainly controlled by easm and sasm subregion 2 is mainly controlled by sasm and plateau circulation system subregion 4 is mainly controlled by the westerly and plateau circulation system on the inter annual scale a stronger easm generally leads to a drier condition in subregions 1 and 3 on the inter decadal scale a stronger sasm usually causes a drier condition in subregions 1 2 and 3 ao nao makes a great contribution to the spatial patterns of the intensity of extreme precipitations enso remarkably influences the precipitation extremes of the whole region from two perspectives intensity and frequency on the multi decadal scale when enso pdo ao and nao become stronger the weihe river basin overall becomes drier however on the centennial scale stronger enso pdo ao and nao can lead to a wetter condition in the weihe river basin the spatial patterns of influences of pdo on the dry wet conditions are coincident with that of easm solar activity exhibits completely different influences on the dry wet conditions of four subregions on the multi decadal and centennial scales stronger solar activity usually leads to a wetter condition in subregion 2 but a drier condition in subregion 1 the large difference between the two adjacent subregions reveals the sensitive and strong responses of the dry wet condition in monsoon transition zone to solar activities this paper presents a novel study on identifying the spatial patterns and temporal variability of the dry wet conditions for the weihe river basin on multiple time scales it also developed a solid integrated framework to decompose the multi centennial dry wet index series into components with different periods and quantify the contributions of monsoon systems large scale atmospheric oscillations westerly circulation system plateau circulation systems and solar activities the methods in this study are generic and can be easily applied by other similar studies more important this study fills in the knowledge gap on the spatiotemporal variability of the dry wet conditions across the weihe river basin and their driving forces on multiple time scales potentially providing a valuable knowledge for better projecting further changes in this region and its surrounding areas credit authorship contribution statement xinyu chen conceptualization data curation investigation methodology software formal analysis visualization writing original draft quan quan conceptualization investigation methodology software formal analysis writing review editing funding acquisition ke zhang conceptualization investigation software methodology formal analysis writing original draft writing review editing funding acquisition jiahua wei software writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement this work was supported by the national key research and development program of china 2016yfa0601503 2017yfc0403600 and 2016yfc0402701 the belt and road special foundation of the state key laboratory of hydrology water resources and hydraulic engineering 2019491411 hydraulic science and technology plan foundation of shaanxi province 2019slkj b1 fundamental research funds for the central universities of china b200204038 natural science foundation of jiangsu province bk20180022 key research development program of ningxia hui automonous region china 2018beg02010 and six talent peaks project in jiangsu province ny 004 the first author is grateful to prof shengzhi huang for his helpful advice 
25797,large scale implementation of geologic carbon storage gcs to help reduce atmospheric greenhouse gas emissions requires stakeholder confidence that injected co2 will remain contained and that potential subsurface environmental risks are acceptably small and manageable the u s department of energy s national risk assessment partnership nrap has developed an open source integrated assessment model nrap open iam to help address questions about a potential gcs site s ability to effectively contain injected co2 and protect groundwater and other overlying environmentally sensitive receptors nrap open iam allows a user to 1 incorporate relevant site geologic and injection scenario data 2 characterize important site features and events 3 couple fast prediction models of various system components of the engineered geologic system and 4 execute stochastic dynamic simulation of whole gcs system performance leakage risk assessment and uncertainty quantification nrap open iam is available on gitlab https gitlab com nrap openiam and is accompanied by multiple application examples and detailed user and developer guides keywords geological carbon storage open source framework integrated assessment model quantitative risk assessment national risk assessment partnership nrap open iam 1 introduction geologic carbon storage gcs is a promising approach for reducing anthropogenic carbon dioxide co2 emissions and mitigating the effects of climate change ipcc 2005 national petroleum council 2019 in gcs co2 recovered from the emissions of industrial sources e g from ethanol production conversion of fossil fuels to electricity and other energy products or captured from the atmosphere fasihi et al 2019 keith et al 2006 is compressed to a dense phase fluid transported to an approved storage facility and injected deep underground for safe long term storage while gcs is considered technically feasible remaining hurdles to the successful large scale deployment of gcs are the costs and energy demand associated with co2 capture and stakeholder concerns about the potential risk of fluid leakage through natural e g fractures and faults and man made e g compromised wellbores pathways located in the vicinity of a storage complex pawar et al 2015 the storage complex is defined as a subsurface geological system including a storage reservoir and primary and possibly secondary seal s extending laterally to the defined limits of the co2 storage operation s canadian standards association group 2012 co2 and brine that migrate upward from the storage reservoir s into potable groundwater aquifers may pose a risk to human health and the environment in addition co2 that migrates upward into the atmosphere negates the utility of the gcs operation apps et al 2010 keating et al 2016 zheng et al 2009 these risks must be appropriately characterized assessed and managed at each gcs site to ensure the safety and success of the project while numerous risk assessment methods are used to characterize potential e g leakage risks at gcs sites condor et al 2011 there has been a noticeable trend towards quantitative approaches bourne et al 2014 pawar et al 2015 dean and tucker 2017 quantitative leakage risk assessment relies on physics based models that forecast long term site behavior celia et al 2011 metcalfe et al 2013 stauffer et al 2009 zhang et al 2007 parameters of these models are assumed to be stochastic and to have a predefined probability distribution the models are run for multiple realizations of the parameters values to predict the range of hypothetical risk scenarios at the site and account for the potentially significant uncertainties associated with gcs operations which are often difficult and expensive to characterize rutqvist 2012 developing stochastic models of a gcs system is challenging pawar et al 2015 white and foxall 2016 the physical processes associated with gcs involve non isothermal transient flows of multicomponent multiphase fluids and reactive geochemistry these complex phenomena are often highly non linear and take place in three dimensional porous and fractured media over large scales which is computationally expensive to simulate over the past decade the development of computational tools that facilitate quantitative leakage risk assessment for gcs has been a major focus of the national risk assessment partnership nrap a multi year collaborative research partnership between five national laboratories los alamos national laboratory lawrence livermore national laboratory lawrence berkeley national laboratory pacific northwest national laboratory and the national energy technology laboratory sponsored by the u s department of energy u s doe in 2016 nrap released their first leakage risk forecasting tool for gcs site operators the nrap integrated assessment model for carbon storage nrap iam cs pawar et al 2016 nrap iam cs is built on the systems modeling approach established by co2 pens stauffer et al 2009 which separates a gcs operation into its key components e g reservoir leakage pathway receptor and simulates the physical processes within each component separately component models are linked in a one way forward manner with the outputs from one component informing the inputs of the next the component models within nrap iam cs were developed with computational efficiency in mind and are either reduced order approximations of high fidelity models lookup tables of high fidelity model outputs or analytical semi analytical models this approach allows for a sophisticated representation of the complex processes that occur within each component of a gcs operation but eases computational demand which is necessary to enable stochastic quantification of potential risks while the release of nrap iam cs marked a substantial improvement in gcs system modeling the program was developed with proprietary software that limited the ability of users to create new or tailor existing component models to represent the unique characteristics of their gcs site additionally the nrap team identified several new features and capabilities that could be added to the model to aid with risk based decision making at gcs sites and improve the decision support functionality of the tool for these reasons a new version of the integrated assessment model nrap open source integrated assessment model nrap open iam was developed as its name implies nrap open iam was intentionally developed to be open source and customizable by its users researchers site operators regulators and other gcs stakeholders the open source nature of nrap open iam allows users to create fit for purpose functionality that captures the unique aspects of their gcs site and addresses specific site performance questions of interest nrap open iam is written in the python programming language and requires no special dependencies beyond popular and widely used python libraries e g numpy scipy or matplotlib in this study we describe the structure and function of nrap open iam highlight the new capabilities and features included in the tool and demonstrate the use and applicability of these new features for gcs 2 nrap open iam 2 1 core functionality nrap open iam is written in the python 3 programming which provides 1 cross platform capabilities 2 an extensive number of packages for data handling analysis and visualization and 3 flexibility to include libraries and code written in other programming languages that may be used for the development of embedded component models e g fortran c etc the nrap open iam tool is built upon the functionality of the model analysis toolkit matk python package harp 2015 https matk lanl gov which forms the core structure and analysis capabilities of the tool matk provides basic classes for component and system models sampling latin hypercube sampling lhs sampling with a particular distribution and calibration methods e g non linear least squares minimization and curve fitting additionally matk provides serial and parallel concurrent model execution capabilities to nrap open iam running simulations concurrently can significantly decrease simulation times and make the code ideal for use on computing clusters in addition to workstations and laptops various ensemble analyses e g sensitivity and correlation that benefit from concurrent simulation execution are available for users in nrap open iam the analyses can provide a detailed understanding of the effect of model input uncertainties on model outputs within complex integrated assessment models 2 2 model framework nrap open iam models retain the general framework of models in nrap iam cs the model framework is designed to facilitate the construction of gcs system models in nrap open iam gcs systems are divided into individual component models that simulate physical processes in each of the key elements of a gcs operation fig 1 there are four major component model types in nrap open iam 1 geologic stratigraphy all component models that comprise the total gcs system model in nrap open iam are dependent upon and linked to the parameters set in the geologic stratigraphy component model the component model allows users to define parameters describing the strata of the gcs system including the depth and thickness of the reservoir overlying shale and aquifer layers 2 a reservoir functions as the base of any nrap open iam system model reservoir component models generate arrays representing changing pressure and fluid saturation at the top of the reservoir reservoir cap rock interface over the spatial domain throughout the site performance time period of interest 3 leakage pathways simulate the upward migration of co2 and or brine out of the storage reservoir as a function of leakage pathway characteristics e g path length and effective permeability and the time varying pressure and fluid saturation output at the corresponding location in the reservoir component multiple leakage pathway component models can be placed in an nrap open iam model at user specified locations within the reservoir model domain most of the leakage pathway components included in the nrap open iam are wellbore components wellbore is a hole drilled in the ground for the exploration and or extraction of natural resources such as water gas and oil 4 receptors receive the co2 or brine flow rates from leakage component models as input to calculate the magnitude of leakage into receptors of concern e g potable groundwater aquifers and or the atmosphere aquifer receptor component models calculate the volume of groundwater in the aquifer impacted over time by leakage and account for important hydrologic and geochemical interactions as a function of user defined groundwater aquifer characteristics the atmospheric receptor component model calculates co2 dispersion in the atmosphere from one or more co2 leakage sources component models in nrap open iam are coupled so that the outputs of one component provides inputs to the other components in a way that captures the relationships and dependencies in the physical gcs system 2 3 component models the nrap team has developed a set of general purpose base component models that are included with nrap open iam fig 1 base component models were developed for each component model type reservoir leakage pathway and receptor these models are interchangeable and their input parameters are adjustable which enables the construction of a wide variety of system models that represent a multitude of geologic settings ideal for gcs we highlight the general functionality of the base component models here their more detailed description can be found in the user guide https gitlab com nrap openiam blob master user guide pdf two reservoir component models included with the nrap open iam distribution are the simple reservoir and lookup table reservoir we want to emphasize that nrap open iam is not a reservoir simulation tool however the simple reservoir component model can be used to simulate pressure and co2 saturation during co2 injection into a reservoir of homogeneous permeability and constant thickness the simple reservoir component model uses the semi analytical model described by celia et al 2011 this component model is useful for preliminary rapid estimation of reservoir response in site screening scenarios but is not recommended for scenarios where higher spatial and temporal fidelity is needed users that wish to incorporate more site specific and complex reservoir response information in their gcs system model can do so with the lookup table reservoir component model this component model utilizes lookup tables from outputs created with a high fidelity reservoir simulator e g cmg eclipse fehm petrel stomp tough2 lookup tables provide a means for fast calculation of reservoir response in contrast to the full physics numerical models from which they are derived the leakage pathway component models included with nrap open iam are open wellbore pan et al 2011 pan and oldenburg 2014 pan and oldenburg 2017 cemented wellbore harp et al 2016 and multisegmented wellbore celia et al 2011 nogues et al 2012 nordbotten et al 2005 2009 nordbotten and celia 2006 the open wellbore component model calculates co2 and brine leakage along a completely uncemented legacy well cemented wellbore and multisegmented wellbore component models calculate co2 and brine leakage along fully cemented wells both the cemented wellbore and multisegmented wellbore component models consider the extent to which intermediate formations i e porous and permeable formations overlying the primary sealing caprock but underlying the lowermost drinking water aquifer attenuate unwanted fluid migration that might otherwise reach receptors of concern however the cemented wellbore and multisegmented wellbore component models have different configurations sets of assumptions and parameters which makes them applicable to different scenarios if needed users can omit a leakage pathway component model from their simulation and instead specify a leakage flow rate into a receptor s at any location in the model domain this allows consideration of leakage scenarios in which details about the leakage pathway and or reservoir are unavailable both aquifer and atmosphere receptor component models are included with nrap open iam currently there are four aquifer component models carbonate aquifer keating et al 2016 deep alluvium aquifer mansoor et al 2018 futuregen 2 0 shallow aquifer and above zone monitoring interval azmi bacon et al 2019 these aquifer component models consider different aquifer types that have unique geochemical compositions each aquifer component model simulates the flow of leaked co2 and or brine within the aquifer and accounts for the appropriate geochemical reactions that take place when co2 and or brine is introduced aquifer component models calculate the volume of groundwater impacted in the aquifer and allow users to evaluate impact ph tds heavy metals organics one atmospheric receptor component model is included with nrap open iam this model uses the wind speed leak location s and leak magnitude to calculate co2 dispersion in the near surface atmosphere zhang et al 2016 some gcs sites may have features that cannot be appropriately represented by the base component models included with nrap open iam under these circumstances developers within the nrap open iam user community are encouraged to create component models that represent the unique elements of their site the nrap open iam developer s guide walks users through the creation of a component model https gitlab com nrap openiam blob master developer guide pdf users are free to create new component models using whichever approach e g analytical semi analytical lookup table reduced order modeling they deem appropriate 2 4 model setup nrap open iam models are created by assembling a set of component models into a gcs system that best represents the gcs site of interest the base component models provided with nrap open iam can be used or new ones can be developed by users to represent the unique features and geological conditions at their site all component models are designed to be modular interchangeable and can be combined by users in any manner they desire as long as the general structure of the system model is retained reservoir leakage pathway receptor and identified in the stratigraphy component model for example a user may create a system model that consists of the lookup table reservoir open wellbore and carbonate aquifer component models if they have high fidelity reservoir simulation outputs and are interested in characterizing leakage risks along an open wellbore s from the reservoir and into a carbonate aquifer after component models have been selected and their relationship to one another is defined users specify the input parameters of each component model to be representative of the physical properties of the gcs site component model parameters are typically hydrologic or geochemical properties of the reservoir leakage pathway or receptor if the properties of the system are well characterized parameters can be set as single deterministic values however if the properties of the system are uncertain distributions can be used as parameter inputs several probabilistic distributions are packaged with nrap open iam that users can select from for component model parameters 2 5 model simulation inputs and outputs simulations in nrap open iam can be run in either a deterministic or stochastic manner all model parameters must be specified for simulations which means that if any of the model parameters are not defined the code will assign them a default value stochastic simulations can employ the latin hypercube sampling approach to sample uncertain parameters from the desired distributions nrap open iam calculates the leakage rate of co2 and or brine through the leakage pathways into intermediate formations and receptors during the simulation additional computations are performed in the receptor component models aquifer component models use the co2 and or brine leakage rates to determine the volume of an aquifer impacted by leakage and atmosphere component models use the co2 leakage rate to determine the dispersion of co2 in the atmosphere these outputs are primarily useful for quantifying the leakage risks over time at a proposed or ongoing gcs site and determining the acceptability of those leakage risks pawar et al 2016 however they can also be used to inform other decisions that must be made during the lifecycle of a gcs project some of the new features in nrap open iam discussed in the following section interpret model results for area of review delineation post injection site care and closure period support nrap open iam models are intended to be continuously run over the lifetime of a gcs project as real monitoring data is collected in the field users can use this data to further constrain the distributions of uncertain parameters and update them in the model chen et al 2020 leakage projections for prospective co2 sites are initially made with limited data gathered during the site characterization phase of the project consequently the key properties of the gcs system are often highly uncertain once injection begins the site monitoring network provides gcs operators with additional information about the properties of the geologic system in which they are injecting co2 incorporation of these data further constrains uncertain parameters which improves model forecasts monitoring data can also be used to evaluate the concordance of or agreement between predicted and observed model behavior oldenburg 2018 new functionality in nrap open iam allows users to directly incorporate monitoring data e g reservoir pressure to update a model and or to perform concordance evaluations the process of continuously updating an nrap open iam model to constrain uncertainty and demonstrate concordance is essential to build confidence in system performance and inform decisions 2 6 decision support functionality nrap open iam includes several new features that aid the risk based decision making process for gcs fig 2 these features are built on the base functionality of the tool and involve the interpretation of model outputs or incorporation of monitoring data to directly inform key aspects of gcs site permitting and operation in particular they can aid during the following processes 1 area of review aor delineation determining the aor of the proposed gcs project is a key aspect of the site permitting process in the united states us the u s environmental protection agency u s epa which regulates gcs injection wells as class vi wells under their underground injection control program defines the aor as the region surrounding the storage project where underground sources of drinking water usdws may be endangered by the injection activity if leakage were to occur 40 c f r 146 84 the maximum extent of the co2 plume defines the minimum extent of the aor however the pressure front in the storage reservoir typically extends beyond the maximum extent of the co2 plume the pressure front is defined as the minimum pressure needed to push reservoir fluids upward into the deepest usdw through a hypothetical open wellbore u s environmental protection agency 2013 therefore the aor is a combined maximum extent of the co2 plume and pressure front nrap open iam contains the functionality necessary to estimate both the co2 plume and pressure front extents and thus supports aor calculations python script examples are included with nrap open iam that demonstrate this process additional scripts are also included that show how nrap open iam can be used to perform probabilistic risk based aor calculations in the fashion of bacon et al 2020 and white et al 2020 2 concordance evaluation over the lifetime of a gcs project operators must ensure that site operations conform with regulatory requirements demonstrations of concordance between predicted and observed co2 plume and pressure front behavior in the reservoir are typically a requirement for regulatory conformance chadwick and noy 2015 doughty and oldenburg 2020 harp et al 2019 users can input reservoir monitoring data chen et al 2020 into nrap open iam to evaluate the degree of concordance between observed and predicted reservoir behavior 3 post injection site care and closure support operations at gcs sites continue after co2 injection stops until the site is considered safe to close this timeframe is referred to as the post injection period by the u s epa class vi regulations mandate that the post injection period continues for 50 years or an alternative approved time period until operators demonstrate that co2 plume behavior has conformed to projected expectations and underground sources of drinking water are no longer endangered u s environmental protection agency 2016 examples are included with nrap open iam that show users how to project leakage risks and detectability through the post injection period model outputs can be used to identify and support post injection monitoring durations that are justified by projected leakage risks 3 illustrative use case example use cases example applications described above have been developed to illustrate how the nrap open iam tool can be used to help address risk assessment performance and decision making questions at gcs sites details of these use cases are provided below and in the supplementary materials 3 1 quantifying gcs site containment effectiveness and leakage risk this use case illustrates the application of nrap open iam for leakage risk estimates during the injection period the scenario describes a storage complex with limited information about the status of three abandoned wells the study tries to estimate the potential impact of a worst case scenario in which the abandoned wells are open wellbores with the possibility of the greatest leakage and consequently the largest impact on the environment in this scenario three potential open wellbores serve as direct conduits between the storage reservoir and the atmosphere a relatively unlikely scenario assuming that open wellbores do not have hydraulic connections to other underground resources this use case does not consider co2 leakage into underground sources of drinking water or other underground resources overlying the target reservoir although it is unlikely that all three wells would leak at the same time the use case demonstrates how to perform a scoping study for an extreme catastrophic event the component models used in this use case include a stratigraphy component model with three shale layers and two aquifers a simple reservoir component model an open wellbore component model and an atmospheric component model more details are provided in table 1 component parameters not specified in the table are assumed to take on the default values assigned by nrap open iam three stochastic parameters used in the simulations are represented with their notations referred to in the figures below to evaluate a range of impacts several parameters as described in table 1 are varied for the uncertainty quantification analysis the atmospheric model setup defines a critical concentration a threshold concentration limit above which co2 becomes hazardous and estimates a critical distance radius of the co2 plume from the leaking well within which co2 is at or above the critical concentration a circular area surrounding the well within the critical distance defines the critical region the metric used to quantify the impact of co2 leakage through the open wellbores is the probability of locating markers within the critical distance from a co2 leakage source the markers or locations of interest indicate home or business locations where people are present for each point of the model domain the probability of a given location being within the critical region in case of a leak is calculated as a ratio of the number of realizations for which the given location is within the critical region to the total number of realizations the results are based on an analysis of 20 000 simulations utilizing a latin hypercube sampling approach fig 3 a shows the model output with critical regions surrounding each of the three wells after one week of leakage from one of the realizations the plot shows that the pre defined markers are not located within the critical regions during the time frame of the analysis fig 3b shows the probability of each point within a model domain being within a critical region in this case the area with normalized co2 concentration greater than 0 01 it illustrates that under some scenarios there is a non zero probability that some of the markers can end up within a critical distance from the leaking well results of the additional sensitivity analysis performed for the parameters of the model are shown in fig 4 fig 4a illustrates first order sensitivity coefficients calculated for one of the model outputs critical distance from the first leaking well after one week of leakage first order coefficients measure contribution of a given parameter e g reservoir permeability to the variance of a particular output averaged over variations in other parameters and calculated as a ratio of variance of the output due to the uncertainty in a given parameter to the total variance of the output according to the analysis the critical distance output is most sensitive to the wind speed fig 4b illustrates first order sensitivity coefficients calculated for the selected model outputs at the same time we want to note that the main purpose of figs 3 and 4 is to illustrate the analysis and some of the visualization capabilities of nrap open iam and not to draw any specific conclusions the nrap open iam script for this example is provided in the nrap open iam release package https gitlab com nrap openiam blob master examples scripts iam sys reservoir openwell atmosphere 3locs lhs py additional use cases illustrating the application and utility of nrap open iam are presented in the supplementary materials the authors want to provide a quick glimpse at the work that was already published and point interested readers to the references mentioned there for more details 4 summary nrap open iam is an open source integrated assessment model developed by the u s doe s national risk assessment partnership to characterize gcs risks the nrap open iam incorporates developments from its predecessor nrap iam cs into an open source framework facilitating community involvement in software development and adds capabilities in uncertainty reduction and risk management these new capabilities can help inform monitoring design assess model concordance to measured field data and evaluate mitigation alternatives a variety of component models including semi analytical or lookup table and regression based models derived from full physics process models enable fast probabilistic risk assessment nrap open iam allows for easy incorporation of user developed roms example use cases suggest the practical utility of nrap open iam to support risk assessment uncertainty quantification and stakeholder risk management decisions nrap open iam development continues based on interactions between nrap open iam developers and users nrap open iam provides a quantitative approach for gcs risk assessment based on the long term performance of storage sites the tool equips the user with a suite of analytical tools for performing inverse and ensemble related analysis including model calibration uncertainty quantification and sensitivity analysis it enables conformance evaluation which is an important regulatory aspect of a gcs operation although the tool is developed for gcs risk management the overall framework and methodology could be adapted for risk management applications in other complex engineered geologic systems such as waste disposal unconventional fossil resource extraction enhanced geothermal energy production and geologic energy storage ongoing work seeks to field test and validate nrap open iam and the underpinning methods and analytical capabilities to improve model functionality and continue to develop additional risk management functionality to better support gcs stakeholder needs national risk assessment partnership 2021 these efforts are a necessary step to ensure the robustness and practical utility of nrap open iam the nrap open iam development team is also working on a software quality assurance document to be distributed with nrap open iam which would provide more details about the testing and benchmarking performed for each component in the tool the authors hope that this article provides a useful explanation of the nrap open iam and spurs interest and engagement in the nrap project and its computational tools by gcs practitioners academicians software developers and other stakeholders 5 computer code availability the nrap open iam tool and examples can be downloaded at https gitlab com nrap openiam a copy of the tool can also be obtained through netl s energy data exchange website https edx netl doe gov dataset nrap open source iam by a request through an e mail to nrap netl doe gov nrap open iam has a 3 clause bsd license bsd 3 clause 5 1 software quality assurance several techniques are used to control and assure the quality of nrap open iam the nrap open iam source code is managed with version control software git installation of the tool and its test suite is automatically evaluated after each push to the online software repository and errors are raised if the functionality of the code is impacted by changes to the code the accuracy of each component model in nrap open iam has been evaluated based on the most appropriate method for a given component for example components represented by reduced order models or reduced physics models are benchmarked with high fidelity full physics simulations lookup table based components are compared to the original simulation data from which the tables were constructed disclaimer this project was funded by the united states department of energy national energy technology laboratory in part through a site support contract neither the united states government nor any agency thereof nor any of their employees nor the support contractor nor any of their employees makes any warranty express or implied or assumes any legal liability or responsibility for the accuracy completeness or usefulness of any information apparatus product or process disclosed or represents that its use would not infringe privately owned rights reference herein to any specific commercial product process or service by trade name trademark manufacturer or otherwise does not necessarily constitute or imply its endorsement recommendation or favoring by the united states government or any agency thereof the views and opinions of authors expressed herein do not necessarily state or reflect those of the united states government or any agency thereof declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement this work was completed as part of the national risk assessment partnership nrap project we would like to acknowledge the support of the u s department of energy office of fossil energy s carbon storage program the director for the division of carbon capture and storage research and development mr mark ackiewicz carbon storage program manager mr darin damiani acting carbon storage technology manager mr mark mckoy former carbon storage technology manager ms traci rodosta and nrap project manager ms m kylee underwood the authors would also like to acknowledge nicolas huerta susan carroll brandon schwartz rajesh pawar inci demirkanli and signe white for their constructive reviews of the nrap open iam tool and contributions from researchers across the nrap technical team the authors also would like to thank members of the nrap tool user community for their time and effort to test and apply the nrap tools and provide constructive feedback that is critical to driving continued improvement of nrap open iam and other nrap tools finally we would like to thank the editor in chief dr daniel ames and four anonymous reviewers for their careful reading of our manuscript and their insightful comments and suggestions appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105114 software availability software name nrap open iam developer the u s department of energy s national risk assessment partnership nrap year of the first official release 2018 alpha prototype hardware requirements pc system requirements windows linux mac program language python 3 program size 84 3 mb availability https gitlab com nrap openiam license bsd 3 clause text available here https gitlab com nrap openiam blob master license documentation user and developer guides on gitlab repository https gitlab com nrap openiam blob master user guide pdf and https gitlab com nrap openiam blob master developer guide pdf examples are described in the user guide and provided in the repository https gitlab com nrap openiam tree master examples 
25797,large scale implementation of geologic carbon storage gcs to help reduce atmospheric greenhouse gas emissions requires stakeholder confidence that injected co2 will remain contained and that potential subsurface environmental risks are acceptably small and manageable the u s department of energy s national risk assessment partnership nrap has developed an open source integrated assessment model nrap open iam to help address questions about a potential gcs site s ability to effectively contain injected co2 and protect groundwater and other overlying environmentally sensitive receptors nrap open iam allows a user to 1 incorporate relevant site geologic and injection scenario data 2 characterize important site features and events 3 couple fast prediction models of various system components of the engineered geologic system and 4 execute stochastic dynamic simulation of whole gcs system performance leakage risk assessment and uncertainty quantification nrap open iam is available on gitlab https gitlab com nrap openiam and is accompanied by multiple application examples and detailed user and developer guides keywords geological carbon storage open source framework integrated assessment model quantitative risk assessment national risk assessment partnership nrap open iam 1 introduction geologic carbon storage gcs is a promising approach for reducing anthropogenic carbon dioxide co2 emissions and mitigating the effects of climate change ipcc 2005 national petroleum council 2019 in gcs co2 recovered from the emissions of industrial sources e g from ethanol production conversion of fossil fuels to electricity and other energy products or captured from the atmosphere fasihi et al 2019 keith et al 2006 is compressed to a dense phase fluid transported to an approved storage facility and injected deep underground for safe long term storage while gcs is considered technically feasible remaining hurdles to the successful large scale deployment of gcs are the costs and energy demand associated with co2 capture and stakeholder concerns about the potential risk of fluid leakage through natural e g fractures and faults and man made e g compromised wellbores pathways located in the vicinity of a storage complex pawar et al 2015 the storage complex is defined as a subsurface geological system including a storage reservoir and primary and possibly secondary seal s extending laterally to the defined limits of the co2 storage operation s canadian standards association group 2012 co2 and brine that migrate upward from the storage reservoir s into potable groundwater aquifers may pose a risk to human health and the environment in addition co2 that migrates upward into the atmosphere negates the utility of the gcs operation apps et al 2010 keating et al 2016 zheng et al 2009 these risks must be appropriately characterized assessed and managed at each gcs site to ensure the safety and success of the project while numerous risk assessment methods are used to characterize potential e g leakage risks at gcs sites condor et al 2011 there has been a noticeable trend towards quantitative approaches bourne et al 2014 pawar et al 2015 dean and tucker 2017 quantitative leakage risk assessment relies on physics based models that forecast long term site behavior celia et al 2011 metcalfe et al 2013 stauffer et al 2009 zhang et al 2007 parameters of these models are assumed to be stochastic and to have a predefined probability distribution the models are run for multiple realizations of the parameters values to predict the range of hypothetical risk scenarios at the site and account for the potentially significant uncertainties associated with gcs operations which are often difficult and expensive to characterize rutqvist 2012 developing stochastic models of a gcs system is challenging pawar et al 2015 white and foxall 2016 the physical processes associated with gcs involve non isothermal transient flows of multicomponent multiphase fluids and reactive geochemistry these complex phenomena are often highly non linear and take place in three dimensional porous and fractured media over large scales which is computationally expensive to simulate over the past decade the development of computational tools that facilitate quantitative leakage risk assessment for gcs has been a major focus of the national risk assessment partnership nrap a multi year collaborative research partnership between five national laboratories los alamos national laboratory lawrence livermore national laboratory lawrence berkeley national laboratory pacific northwest national laboratory and the national energy technology laboratory sponsored by the u s department of energy u s doe in 2016 nrap released their first leakage risk forecasting tool for gcs site operators the nrap integrated assessment model for carbon storage nrap iam cs pawar et al 2016 nrap iam cs is built on the systems modeling approach established by co2 pens stauffer et al 2009 which separates a gcs operation into its key components e g reservoir leakage pathway receptor and simulates the physical processes within each component separately component models are linked in a one way forward manner with the outputs from one component informing the inputs of the next the component models within nrap iam cs were developed with computational efficiency in mind and are either reduced order approximations of high fidelity models lookup tables of high fidelity model outputs or analytical semi analytical models this approach allows for a sophisticated representation of the complex processes that occur within each component of a gcs operation but eases computational demand which is necessary to enable stochastic quantification of potential risks while the release of nrap iam cs marked a substantial improvement in gcs system modeling the program was developed with proprietary software that limited the ability of users to create new or tailor existing component models to represent the unique characteristics of their gcs site additionally the nrap team identified several new features and capabilities that could be added to the model to aid with risk based decision making at gcs sites and improve the decision support functionality of the tool for these reasons a new version of the integrated assessment model nrap open source integrated assessment model nrap open iam was developed as its name implies nrap open iam was intentionally developed to be open source and customizable by its users researchers site operators regulators and other gcs stakeholders the open source nature of nrap open iam allows users to create fit for purpose functionality that captures the unique aspects of their gcs site and addresses specific site performance questions of interest nrap open iam is written in the python programming language and requires no special dependencies beyond popular and widely used python libraries e g numpy scipy or matplotlib in this study we describe the structure and function of nrap open iam highlight the new capabilities and features included in the tool and demonstrate the use and applicability of these new features for gcs 2 nrap open iam 2 1 core functionality nrap open iam is written in the python 3 programming which provides 1 cross platform capabilities 2 an extensive number of packages for data handling analysis and visualization and 3 flexibility to include libraries and code written in other programming languages that may be used for the development of embedded component models e g fortran c etc the nrap open iam tool is built upon the functionality of the model analysis toolkit matk python package harp 2015 https matk lanl gov which forms the core structure and analysis capabilities of the tool matk provides basic classes for component and system models sampling latin hypercube sampling lhs sampling with a particular distribution and calibration methods e g non linear least squares minimization and curve fitting additionally matk provides serial and parallel concurrent model execution capabilities to nrap open iam running simulations concurrently can significantly decrease simulation times and make the code ideal for use on computing clusters in addition to workstations and laptops various ensemble analyses e g sensitivity and correlation that benefit from concurrent simulation execution are available for users in nrap open iam the analyses can provide a detailed understanding of the effect of model input uncertainties on model outputs within complex integrated assessment models 2 2 model framework nrap open iam models retain the general framework of models in nrap iam cs the model framework is designed to facilitate the construction of gcs system models in nrap open iam gcs systems are divided into individual component models that simulate physical processes in each of the key elements of a gcs operation fig 1 there are four major component model types in nrap open iam 1 geologic stratigraphy all component models that comprise the total gcs system model in nrap open iam are dependent upon and linked to the parameters set in the geologic stratigraphy component model the component model allows users to define parameters describing the strata of the gcs system including the depth and thickness of the reservoir overlying shale and aquifer layers 2 a reservoir functions as the base of any nrap open iam system model reservoir component models generate arrays representing changing pressure and fluid saturation at the top of the reservoir reservoir cap rock interface over the spatial domain throughout the site performance time period of interest 3 leakage pathways simulate the upward migration of co2 and or brine out of the storage reservoir as a function of leakage pathway characteristics e g path length and effective permeability and the time varying pressure and fluid saturation output at the corresponding location in the reservoir component multiple leakage pathway component models can be placed in an nrap open iam model at user specified locations within the reservoir model domain most of the leakage pathway components included in the nrap open iam are wellbore components wellbore is a hole drilled in the ground for the exploration and or extraction of natural resources such as water gas and oil 4 receptors receive the co2 or brine flow rates from leakage component models as input to calculate the magnitude of leakage into receptors of concern e g potable groundwater aquifers and or the atmosphere aquifer receptor component models calculate the volume of groundwater in the aquifer impacted over time by leakage and account for important hydrologic and geochemical interactions as a function of user defined groundwater aquifer characteristics the atmospheric receptor component model calculates co2 dispersion in the atmosphere from one or more co2 leakage sources component models in nrap open iam are coupled so that the outputs of one component provides inputs to the other components in a way that captures the relationships and dependencies in the physical gcs system 2 3 component models the nrap team has developed a set of general purpose base component models that are included with nrap open iam fig 1 base component models were developed for each component model type reservoir leakage pathway and receptor these models are interchangeable and their input parameters are adjustable which enables the construction of a wide variety of system models that represent a multitude of geologic settings ideal for gcs we highlight the general functionality of the base component models here their more detailed description can be found in the user guide https gitlab com nrap openiam blob master user guide pdf two reservoir component models included with the nrap open iam distribution are the simple reservoir and lookup table reservoir we want to emphasize that nrap open iam is not a reservoir simulation tool however the simple reservoir component model can be used to simulate pressure and co2 saturation during co2 injection into a reservoir of homogeneous permeability and constant thickness the simple reservoir component model uses the semi analytical model described by celia et al 2011 this component model is useful for preliminary rapid estimation of reservoir response in site screening scenarios but is not recommended for scenarios where higher spatial and temporal fidelity is needed users that wish to incorporate more site specific and complex reservoir response information in their gcs system model can do so with the lookup table reservoir component model this component model utilizes lookup tables from outputs created with a high fidelity reservoir simulator e g cmg eclipse fehm petrel stomp tough2 lookup tables provide a means for fast calculation of reservoir response in contrast to the full physics numerical models from which they are derived the leakage pathway component models included with nrap open iam are open wellbore pan et al 2011 pan and oldenburg 2014 pan and oldenburg 2017 cemented wellbore harp et al 2016 and multisegmented wellbore celia et al 2011 nogues et al 2012 nordbotten et al 2005 2009 nordbotten and celia 2006 the open wellbore component model calculates co2 and brine leakage along a completely uncemented legacy well cemented wellbore and multisegmented wellbore component models calculate co2 and brine leakage along fully cemented wells both the cemented wellbore and multisegmented wellbore component models consider the extent to which intermediate formations i e porous and permeable formations overlying the primary sealing caprock but underlying the lowermost drinking water aquifer attenuate unwanted fluid migration that might otherwise reach receptors of concern however the cemented wellbore and multisegmented wellbore component models have different configurations sets of assumptions and parameters which makes them applicable to different scenarios if needed users can omit a leakage pathway component model from their simulation and instead specify a leakage flow rate into a receptor s at any location in the model domain this allows consideration of leakage scenarios in which details about the leakage pathway and or reservoir are unavailable both aquifer and atmosphere receptor component models are included with nrap open iam currently there are four aquifer component models carbonate aquifer keating et al 2016 deep alluvium aquifer mansoor et al 2018 futuregen 2 0 shallow aquifer and above zone monitoring interval azmi bacon et al 2019 these aquifer component models consider different aquifer types that have unique geochemical compositions each aquifer component model simulates the flow of leaked co2 and or brine within the aquifer and accounts for the appropriate geochemical reactions that take place when co2 and or brine is introduced aquifer component models calculate the volume of groundwater impacted in the aquifer and allow users to evaluate impact ph tds heavy metals organics one atmospheric receptor component model is included with nrap open iam this model uses the wind speed leak location s and leak magnitude to calculate co2 dispersion in the near surface atmosphere zhang et al 2016 some gcs sites may have features that cannot be appropriately represented by the base component models included with nrap open iam under these circumstances developers within the nrap open iam user community are encouraged to create component models that represent the unique elements of their site the nrap open iam developer s guide walks users through the creation of a component model https gitlab com nrap openiam blob master developer guide pdf users are free to create new component models using whichever approach e g analytical semi analytical lookup table reduced order modeling they deem appropriate 2 4 model setup nrap open iam models are created by assembling a set of component models into a gcs system that best represents the gcs site of interest the base component models provided with nrap open iam can be used or new ones can be developed by users to represent the unique features and geological conditions at their site all component models are designed to be modular interchangeable and can be combined by users in any manner they desire as long as the general structure of the system model is retained reservoir leakage pathway receptor and identified in the stratigraphy component model for example a user may create a system model that consists of the lookup table reservoir open wellbore and carbonate aquifer component models if they have high fidelity reservoir simulation outputs and are interested in characterizing leakage risks along an open wellbore s from the reservoir and into a carbonate aquifer after component models have been selected and their relationship to one another is defined users specify the input parameters of each component model to be representative of the physical properties of the gcs site component model parameters are typically hydrologic or geochemical properties of the reservoir leakage pathway or receptor if the properties of the system are well characterized parameters can be set as single deterministic values however if the properties of the system are uncertain distributions can be used as parameter inputs several probabilistic distributions are packaged with nrap open iam that users can select from for component model parameters 2 5 model simulation inputs and outputs simulations in nrap open iam can be run in either a deterministic or stochastic manner all model parameters must be specified for simulations which means that if any of the model parameters are not defined the code will assign them a default value stochastic simulations can employ the latin hypercube sampling approach to sample uncertain parameters from the desired distributions nrap open iam calculates the leakage rate of co2 and or brine through the leakage pathways into intermediate formations and receptors during the simulation additional computations are performed in the receptor component models aquifer component models use the co2 and or brine leakage rates to determine the volume of an aquifer impacted by leakage and atmosphere component models use the co2 leakage rate to determine the dispersion of co2 in the atmosphere these outputs are primarily useful for quantifying the leakage risks over time at a proposed or ongoing gcs site and determining the acceptability of those leakage risks pawar et al 2016 however they can also be used to inform other decisions that must be made during the lifecycle of a gcs project some of the new features in nrap open iam discussed in the following section interpret model results for area of review delineation post injection site care and closure period support nrap open iam models are intended to be continuously run over the lifetime of a gcs project as real monitoring data is collected in the field users can use this data to further constrain the distributions of uncertain parameters and update them in the model chen et al 2020 leakage projections for prospective co2 sites are initially made with limited data gathered during the site characterization phase of the project consequently the key properties of the gcs system are often highly uncertain once injection begins the site monitoring network provides gcs operators with additional information about the properties of the geologic system in which they are injecting co2 incorporation of these data further constrains uncertain parameters which improves model forecasts monitoring data can also be used to evaluate the concordance of or agreement between predicted and observed model behavior oldenburg 2018 new functionality in nrap open iam allows users to directly incorporate monitoring data e g reservoir pressure to update a model and or to perform concordance evaluations the process of continuously updating an nrap open iam model to constrain uncertainty and demonstrate concordance is essential to build confidence in system performance and inform decisions 2 6 decision support functionality nrap open iam includes several new features that aid the risk based decision making process for gcs fig 2 these features are built on the base functionality of the tool and involve the interpretation of model outputs or incorporation of monitoring data to directly inform key aspects of gcs site permitting and operation in particular they can aid during the following processes 1 area of review aor delineation determining the aor of the proposed gcs project is a key aspect of the site permitting process in the united states us the u s environmental protection agency u s epa which regulates gcs injection wells as class vi wells under their underground injection control program defines the aor as the region surrounding the storage project where underground sources of drinking water usdws may be endangered by the injection activity if leakage were to occur 40 c f r 146 84 the maximum extent of the co2 plume defines the minimum extent of the aor however the pressure front in the storage reservoir typically extends beyond the maximum extent of the co2 plume the pressure front is defined as the minimum pressure needed to push reservoir fluids upward into the deepest usdw through a hypothetical open wellbore u s environmental protection agency 2013 therefore the aor is a combined maximum extent of the co2 plume and pressure front nrap open iam contains the functionality necessary to estimate both the co2 plume and pressure front extents and thus supports aor calculations python script examples are included with nrap open iam that demonstrate this process additional scripts are also included that show how nrap open iam can be used to perform probabilistic risk based aor calculations in the fashion of bacon et al 2020 and white et al 2020 2 concordance evaluation over the lifetime of a gcs project operators must ensure that site operations conform with regulatory requirements demonstrations of concordance between predicted and observed co2 plume and pressure front behavior in the reservoir are typically a requirement for regulatory conformance chadwick and noy 2015 doughty and oldenburg 2020 harp et al 2019 users can input reservoir monitoring data chen et al 2020 into nrap open iam to evaluate the degree of concordance between observed and predicted reservoir behavior 3 post injection site care and closure support operations at gcs sites continue after co2 injection stops until the site is considered safe to close this timeframe is referred to as the post injection period by the u s epa class vi regulations mandate that the post injection period continues for 50 years or an alternative approved time period until operators demonstrate that co2 plume behavior has conformed to projected expectations and underground sources of drinking water are no longer endangered u s environmental protection agency 2016 examples are included with nrap open iam that show users how to project leakage risks and detectability through the post injection period model outputs can be used to identify and support post injection monitoring durations that are justified by projected leakage risks 3 illustrative use case example use cases example applications described above have been developed to illustrate how the nrap open iam tool can be used to help address risk assessment performance and decision making questions at gcs sites details of these use cases are provided below and in the supplementary materials 3 1 quantifying gcs site containment effectiveness and leakage risk this use case illustrates the application of nrap open iam for leakage risk estimates during the injection period the scenario describes a storage complex with limited information about the status of three abandoned wells the study tries to estimate the potential impact of a worst case scenario in which the abandoned wells are open wellbores with the possibility of the greatest leakage and consequently the largest impact on the environment in this scenario three potential open wellbores serve as direct conduits between the storage reservoir and the atmosphere a relatively unlikely scenario assuming that open wellbores do not have hydraulic connections to other underground resources this use case does not consider co2 leakage into underground sources of drinking water or other underground resources overlying the target reservoir although it is unlikely that all three wells would leak at the same time the use case demonstrates how to perform a scoping study for an extreme catastrophic event the component models used in this use case include a stratigraphy component model with three shale layers and two aquifers a simple reservoir component model an open wellbore component model and an atmospheric component model more details are provided in table 1 component parameters not specified in the table are assumed to take on the default values assigned by nrap open iam three stochastic parameters used in the simulations are represented with their notations referred to in the figures below to evaluate a range of impacts several parameters as described in table 1 are varied for the uncertainty quantification analysis the atmospheric model setup defines a critical concentration a threshold concentration limit above which co2 becomes hazardous and estimates a critical distance radius of the co2 plume from the leaking well within which co2 is at or above the critical concentration a circular area surrounding the well within the critical distance defines the critical region the metric used to quantify the impact of co2 leakage through the open wellbores is the probability of locating markers within the critical distance from a co2 leakage source the markers or locations of interest indicate home or business locations where people are present for each point of the model domain the probability of a given location being within the critical region in case of a leak is calculated as a ratio of the number of realizations for which the given location is within the critical region to the total number of realizations the results are based on an analysis of 20 000 simulations utilizing a latin hypercube sampling approach fig 3 a shows the model output with critical regions surrounding each of the three wells after one week of leakage from one of the realizations the plot shows that the pre defined markers are not located within the critical regions during the time frame of the analysis fig 3b shows the probability of each point within a model domain being within a critical region in this case the area with normalized co2 concentration greater than 0 01 it illustrates that under some scenarios there is a non zero probability that some of the markers can end up within a critical distance from the leaking well results of the additional sensitivity analysis performed for the parameters of the model are shown in fig 4 fig 4a illustrates first order sensitivity coefficients calculated for one of the model outputs critical distance from the first leaking well after one week of leakage first order coefficients measure contribution of a given parameter e g reservoir permeability to the variance of a particular output averaged over variations in other parameters and calculated as a ratio of variance of the output due to the uncertainty in a given parameter to the total variance of the output according to the analysis the critical distance output is most sensitive to the wind speed fig 4b illustrates first order sensitivity coefficients calculated for the selected model outputs at the same time we want to note that the main purpose of figs 3 and 4 is to illustrate the analysis and some of the visualization capabilities of nrap open iam and not to draw any specific conclusions the nrap open iam script for this example is provided in the nrap open iam release package https gitlab com nrap openiam blob master examples scripts iam sys reservoir openwell atmosphere 3locs lhs py additional use cases illustrating the application and utility of nrap open iam are presented in the supplementary materials the authors want to provide a quick glimpse at the work that was already published and point interested readers to the references mentioned there for more details 4 summary nrap open iam is an open source integrated assessment model developed by the u s doe s national risk assessment partnership to characterize gcs risks the nrap open iam incorporates developments from its predecessor nrap iam cs into an open source framework facilitating community involvement in software development and adds capabilities in uncertainty reduction and risk management these new capabilities can help inform monitoring design assess model concordance to measured field data and evaluate mitigation alternatives a variety of component models including semi analytical or lookup table and regression based models derived from full physics process models enable fast probabilistic risk assessment nrap open iam allows for easy incorporation of user developed roms example use cases suggest the practical utility of nrap open iam to support risk assessment uncertainty quantification and stakeholder risk management decisions nrap open iam development continues based on interactions between nrap open iam developers and users nrap open iam provides a quantitative approach for gcs risk assessment based on the long term performance of storage sites the tool equips the user with a suite of analytical tools for performing inverse and ensemble related analysis including model calibration uncertainty quantification and sensitivity analysis it enables conformance evaluation which is an important regulatory aspect of a gcs operation although the tool is developed for gcs risk management the overall framework and methodology could be adapted for risk management applications in other complex engineered geologic systems such as waste disposal unconventional fossil resource extraction enhanced geothermal energy production and geologic energy storage ongoing work seeks to field test and validate nrap open iam and the underpinning methods and analytical capabilities to improve model functionality and continue to develop additional risk management functionality to better support gcs stakeholder needs national risk assessment partnership 2021 these efforts are a necessary step to ensure the robustness and practical utility of nrap open iam the nrap open iam development team is also working on a software quality assurance document to be distributed with nrap open iam which would provide more details about the testing and benchmarking performed for each component in the tool the authors hope that this article provides a useful explanation of the nrap open iam and spurs interest and engagement in the nrap project and its computational tools by gcs practitioners academicians software developers and other stakeholders 5 computer code availability the nrap open iam tool and examples can be downloaded at https gitlab com nrap openiam a copy of the tool can also be obtained through netl s energy data exchange website https edx netl doe gov dataset nrap open source iam by a request through an e mail to nrap netl doe gov nrap open iam has a 3 clause bsd license bsd 3 clause 5 1 software quality assurance several techniques are used to control and assure the quality of nrap open iam the nrap open iam source code is managed with version control software git installation of the tool and its test suite is automatically evaluated after each push to the online software repository and errors are raised if the functionality of the code is impacted by changes to the code the accuracy of each component model in nrap open iam has been evaluated based on the most appropriate method for a given component for example components represented by reduced order models or reduced physics models are benchmarked with high fidelity full physics simulations lookup table based components are compared to the original simulation data from which the tables were constructed disclaimer this project was funded by the united states department of energy national energy technology laboratory in part through a site support contract neither the united states government nor any agency thereof nor any of their employees nor the support contractor nor any of their employees makes any warranty express or implied or assumes any legal liability or responsibility for the accuracy completeness or usefulness of any information apparatus product or process disclosed or represents that its use would not infringe privately owned rights reference herein to any specific commercial product process or service by trade name trademark manufacturer or otherwise does not necessarily constitute or imply its endorsement recommendation or favoring by the united states government or any agency thereof the views and opinions of authors expressed herein do not necessarily state or reflect those of the united states government or any agency thereof declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement this work was completed as part of the national risk assessment partnership nrap project we would like to acknowledge the support of the u s department of energy office of fossil energy s carbon storage program the director for the division of carbon capture and storage research and development mr mark ackiewicz carbon storage program manager mr darin damiani acting carbon storage technology manager mr mark mckoy former carbon storage technology manager ms traci rodosta and nrap project manager ms m kylee underwood the authors would also like to acknowledge nicolas huerta susan carroll brandon schwartz rajesh pawar inci demirkanli and signe white for their constructive reviews of the nrap open iam tool and contributions from researchers across the nrap technical team the authors also would like to thank members of the nrap tool user community for their time and effort to test and apply the nrap tools and provide constructive feedback that is critical to driving continued improvement of nrap open iam and other nrap tools finally we would like to thank the editor in chief dr daniel ames and four anonymous reviewers for their careful reading of our manuscript and their insightful comments and suggestions appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105114 software availability software name nrap open iam developer the u s department of energy s national risk assessment partnership nrap year of the first official release 2018 alpha prototype hardware requirements pc system requirements windows linux mac program language python 3 program size 84 3 mb availability https gitlab com nrap openiam license bsd 3 clause text available here https gitlab com nrap openiam blob master license documentation user and developer guides on gitlab repository https gitlab com nrap openiam blob master user guide pdf and https gitlab com nrap openiam blob master developer guide pdf examples are described in the user guide and provided in the repository https gitlab com nrap openiam tree master examples 
25798,ecohydrological processes are often evaluated by studying the fate of stable water isotopes however isotopic fractionation during evaporation is often ignored or simplified in current models resulting in simulation errors that may be propagated into practical applications of stable isotope tracing in this study we adapted and tested the hydrus 1d model a numerical model widely used to simulate variably saturated water flow and solute transport in porous media by including an option to simulate isotope fate and transport while accounting for evaporation fractionation the numerical results obtained by the adapted model were in excellent agreement with existing analytical solutions additional plausibility tests and field evaluation further demonstrated the adapted model s accuracy a simple particle tracking algorithm was also implemented to calculate soil water s transit times and further validate the modified model s results transit times calculated by the particle tracking module ptm were similar to those estimated by the isotope peak displacement method validating the applicability of the ptm the developed model represents a comprehensive tool to numerically investigate many important research problems involving isotope transport processes in the critical zone keywords water stable isotopes isotope transport modeling hydrus 1d evaporation fractionation water transit time particle tracking algorithm 1 introduction evaporation fractionation is characterized by the retainment of heavier isotopes in the liquid phase and the preferential affinity of lighter isotopes in the vapor phase e g gonfiantini et al 2018 due to evaporation fractionation s unique characteristics stable water isotopes 2h and 18o are good indicators for studying many ecohydrological processes in the critical zone gehrels et al 1998 sprenger et al 2016a such as partitioning evapotranspiration kool et al 2014 xiao et al 2018 and identifying the sources of crop water uptake i e sourcing e g corneo et al 2018 ma and song 2016 wang et al 2019 at the soil vegetation atmosphere interface models that can accurately simulate the transport and fractionation of isotopes are necessary to properly interpret isotopic data in the critical zone the concept of water transit or travel time tt defined as the time elapsed between water entering and leaving a reservoir provides a useful insight into many ecohydrological issues such as partitioning recharge and discharge sources evaluating the role of mobile and immobile waters and inferring temporal origins of root water uptake e g allen et al 2019 brinkmann et al 2018 mcdonnell 2014 sprenger et al 2016b the traditional isotope based method for estimating tts is by inversely estimating lumped isotope transport parameters assuming time invariant tt distributions ttds e g maloszewski et al 2006 stumpp and maloszewski 2010 timbe et al 2014 or storage selection sas functions rinaldo et al 2015 however the lumped models overgeneralize the isotope transport mechanisms some of them cannot truly describe the isotope transport or ttds under transient conditions while others can account for the time variance of ttds but can only describe the mixing and partitioning of isotopes jury et al 1986 sprenger et al 2016a physics based isotope transport models are needed to fully describe the spatio temporal evolution of isotope concentrations under field conditions kim et al 2016 such flow and transport models usually rely on the richards and convection dispersion equations respectively when evaporation fractionation can be neglected one can simulate the fate and transport of isotopes in soils as standard solutes for example stumpp et al 2012 used the modified hydrus 1d model with isotopic information to analyze the effects of the vegetation cover and fertilization measures on water flow and solute transport in lysimeters this modified model is available at https www pc progress com en default aspx h1d lib isotope sprenger et al 2016b used this modified model to infer soil water residence times at different depths brinkmann et al 2018 applied the same model to estimate the residence time distribution of soil water and identify the temporal origin of water taken up by picea abies and fagus sylvatica this modified version of the hydrus 1d model by stumpp et al 2012 allows isotopes to leave the soil profile at the soil surface without considering the fractionation effect during evaporation this is implemented by assuming that the isotope concentration of the evaporation flux is the same as that of soil water at the soil surface however ignoring the evaporative enrichment as done in this modified hydrus 1d leads to underestimating 2h and 18o concentrations in the topsoil which may be more significant in regions with higher evaporative losses sprenger et al 2018 additionally transit times calculations as done in these studies e g sprenger et al 2016b brinkmann et al 2018 are based on isotope transport simulations and require labor intensive and time consuming high precision isotope measurements to calibrate the model the inaccurate sampling or modeling of soil water isotopes especially in case of physical nonequilibrium i e immobile water content dual porosity permeability type solute transport can easily lead to large errors in transit time calculations e g sprenger et al 2018 tetzlaff et al 2014 the temporal evolution of evaporation fractionation was first studied and modeled for the free water surface craig and gordon 1965 the craig gordon model has been the cornerstone of isotope hydrology since it was proposed in 1965 after that zimmermann et al 1967 applied this model to saturated soil under steady state evaporation conditions barnes and allison 1983 extended this work to isothermal steady state evaporation conditions in unsaturated soils barnes and allison 1984 further extended this work to the nonisothermal steady state conditions in unsaturated soils with a defined soil temperature profile barnes and allison 1983 1984 also provided analytical solutions for the transport of isotopes with evaporation fractionation under steady state conditions however to describe and predict the spatial and temporal evolution of isotope concentrations under field evaporation conditions a model capable of describing transient conditions is required shurbaji and phillips 1995 proposed the first numerical model odwish that considered evaporation fractionation this model coupled heat transport and water flow equations in the soil proposed by philip and de vries 1957 and introduced a transition factor into the isotope transport equation this transition factor combines the influence of hydrology and isotope parameters it changes slowly with depth except for quick changes in the evaporative zone which is conducive to obtaining a unique isotope profile shape however the upper boundary condition must be determined by measuring temperatures and humidities at the soil surface and the evaporation front the evaporation front is located at a depth above which the water vapor flux becomes dominant compared to the liquid flux generally it corresponds to the peak in the isotope concentration profile braud et al 2005a since such data are rarely available a model that interacts with the atmosphere is needed to address the surface energy budget mathieu and bariac 1996 proposed a simplified model moise for constant potential evaporation and a predefined soil temperature profile this model still lacked the option of evaluating the surface energy budget soderberg et al 2012 melayah et al 1996a fully coupled the transport of heat water and isotopes with surface energy budget calculations the results showed that the model was very sensitive to the initial isotope profile and small changes in liquid water convective transport better knowledge of isotope transport coefficients in porous media e g mobile immobile phases should improve its prediction ability melayah et al 1996b braud et al 2005a corrected some inconsistencies in the derivations of melayah et al 1996a and several isotope transport models such as sispat isotope braud et al 2005a and soil litter iso haverd and cuntz 2010 have been developed based on this modified theory the soil litter iso model was based on ross explicit numerical solution of the richards equation ross 2003 resulting in significantly improved computational efficiency compared with the sispat isotope model this allowed isotope calculations to be performed for soil profiles with vegetation using coarser spatial discretization and larger time steps haverd and cuntz 2010 however these models did not consider the impacts of physical nonequilibrium flow e g immobile water or preferential flow on isotope transport and concentrations mueller et al 2014 and sprenger et al 2018 used the swis model soil water isotope simulator to model stable isotopes for uniform and nonequilibrium mobile and bulk soil water flow in the vadose zone respectively this model considered evaporation fractionation but neglected vapor flow despite the successes of isotope transport modeling with evaporation fractionation the current isotope transport models table 1 are not widely used some of them are no longer maintained e g mathieu and bariac 1996 melayah et al 1996 are quite complex to deploy e g braud et al 2005a haverd and cuntz 2010 or still only implement a simple treatment of evaporation fractionation e g shurbaji and phillips 1995 stumpp et al 2012 which may be some of the reasons why they are not commonly used the standard version of hydrus 1d can simulate volatile solutes transport in soils by allowing solute transport by convection and dispersion in the liquid phase and diffusion in the soil air thus the model is quite widely used to simulate transport processes of many emerging organic chemicals such as pesticides and fumigants e g spurlock et al 2013ab brown et al 2019 the governing equations for volatile solute transport see eqs 6 55 and 6 56 in radcliffe and šimůnek 2018 are similar to those for the isotope transport by braud et al 2005a the relationship between the liquid and vapor solute concentrations is described in hydrus 1d by henry s law assuming an instantaneous distribution of a solute between the liquid and air phases this volatile solute transport model in hydrus 1d can be adapted to simulate the transport of stable isotopes by modifying the upper boundary condition considering fractionation and reinterpreting henry s coefficient in the governing solute transport equation particle tracking algorithms represent an alternative and more straightforward way of calculating ttds e g šimůnek 1991 asadollahi et al 2020 while still considering transient water flow since the particle tracking algorithm e g šimůnek 1991 can be technically based solely on water balance calculations without requiring isotopic measurements it needs much less input information than the stable water isotope transport models such algorithms can thus have broad applicability and can act as an excellent supplement to the traditional isotope transport based methods for calculating transit times however it is still highly recommended to observe isotopic data identifying accurate model parameters and travel times e g groh et al 2018 mattei et al 2020 sprenger et al 2015 and verifying model determined tts the objectives of this study thus are 1 to adapt the current hydrus 1d model to simulate water flow and transport of stable water isotopes while considering multiple types of evaporation fractionation situations and soil conditions i e isothermal non isothermal conditions equilibrium and nonequilibrium flow with and without vapor flow 2 to verify the new model using analytical solutions and plausibility tests 3 to implement a simple water flow based particle tracking algorithm and 4 to evaluate the capability of the new isotope transport and particle tracking modules using a field dataset the new isotope transport and particle tracking modules provide hydrus 1d users with a comprehensive tool for assessing transit times simulating continuous dynamic changes in soil water isotope concentrations and numerically investigating many fundamental research problems involving sourcing and timing of soil water 2 definition of the isotope concentrations following braud et al 2005a the concentration c i kg m 3 of the isotope i can be defined as 1 c i m i v m i m t m t v n i m i n i m i n w m w n i m i n w m w v m i m w r i ρ where m i kg is the mass of the isotope i either in the liquid or vapor phase v m3 is the volume of water m t kg is the total mass of water m i and m w kg mol are the molar masses of water including the isotope i and ordinary water respectively n i and n w mol are the numbers of moles of water including the isotope i and ordinary water respectively r i is the isotope ratio of the isotope i i e n i n w and ρ kg m3 is the density of water either in the liquid ρ w or vapor ρ v phases see appendix a in this equation we assumed that n i m i n w m w to get the last term the relationship between the isotope ratio r and isotopic composition δ is 2 δ i r i r std r std 1000 where r i and r s t d are the isotope ratios in the water sample and the standard sample the vienna standard mean ocean water vsmow 0 r s t d 155 76 10 6 for hdo and r s t d 2005 2 10 6 for h2 18o according to gonfiantini 1978 r i refers to the 18o 16o or 2h 1h ratios that can be deduced from eq 1 as follows 3 r i m w m i c i ρ note that in this study the term isotope ratio refers to r isotopic composition refers to δ and isotope concentration refers to c kg m 3 the results will be presented throughout the manuscript in δ notation even though numerical computations may be performed using the c r or δ notations 3 craig gordon model 1965 the separation of heavy and light isotopes between reservoirs or reactants and products is called isotopic fractionation gat 2010 kendall and mcdonnell 2012 isotopic fractionation can be divided into equilibrium fractionation chemical thermodynamic fractionation and kinetic fractionation physical diffusion fractionation according to the processes that cause this change equilibrium fractionation occurs during chemical reactions at equilibrium exchange reactions the heavy isotopes are concentrated in substances with the highest bond force constants i e the preferential affinity of the lighter isotope for the vapor phase fry 2006 kinetic fractionation is caused by the differences in the diffusion rates of water molecules through the air i e preferential diffusion of the lighter isotope gat 2010 evaporation fractionation between the soil and free atmosphere includes both equilibrium and kinetic fractionations craig 1961 craig and gordon 1965 calculated the isotope evaporation flux at the liquid vapor interface based on these two types of fractionations this model considers three layers fig 1 a a liquid vapor interface where condensation evaporation and equilibrium fractionation occur b a diffusive sublayer where molecular diffusion dominates and thus kinetic fractionation occurs and c a turbulently mixed sublayer where mixing dominates and thus no fractionation occurs gat 2010 horita et al 2008 the water vapor and isotope evaporation fluxes between the water surface and the bottom of the free atmosphere are described by ohm s law or fick s law as an analog of the concentration gradient and transport resistance braud et al 2005a 2009 gat 2010 the evaporation flux for water vapor e kg m2 s is 4 e ρ s a t v t s h r s h a r a where r a s m is the sum of the resistances r m r t of water vapor to diffusive flow in the diffusive r m and turbulent r t sublayers ρ s a t v is the density of the saturated water vapor kg m3 see appendix a h r s is the relative humidity of the soil air phase at the surface and h a is the relative humidity of the atmosphere at the air temperature t a k normalized to the relative humidity of the atmosphere h a at the interface temperature t s k h r s and h a can be calculated as follows 5 h r s exp m g h s r u t s 6 h a h a ρ s a t v t a ρ s a t v t s where g is the gravitational acceleration lt 2 m is the molecular weight of water kg mol 0 018015 r u is the universal gas constant j mol k 8 314 h s is the matric potential at the soil surface l and t s and t a are the temperatures of the soil surface and atmosphere k respectively the corresponding evaporation flux for water isotopes e i kg m2 s is 7 e i c i s v c i a v r i c i s v c i a v α i k r a ρ s a t v t s α i k r a m i m w h r s α i r l h a r a e α i k m i m w h r s α i r l h a r a h r s h a the isotope ratio of the evaporation flux r e is 8 r e e i e h r s α i r l h a r a h r s h a α i k m i m w where c i s v and c i a v are the isotope concentrations of the surface water vapor and atmosphere kg m 3 r i s m is the sum of the resistances r m i r t i of water isotopes to diffusive flow in the diffusive r m i and turbulent r t i sublayers r v r l are the isotope ratios of the water vapor and remaining liquid water at the soil surface respectively r a is the isotope ratio of the atmosphere α i is the equilibrium fractionation factor and α i k is the kinetic fractionation factor note that α i is defined here as the ratio of vapor to liquid phase isotope ratios and it is thus smaller than 1 the equations used to compute α i for 2h and 18o isotopes as a function of temperature t k can be found in majoube 1971 and horita and wesolowski 1994 the equations by majoube 1971 were used in this study 9 α i o 18 o 16 exp 2 0667 10 3 0 4156 t 1 137 10 3 t 2 10 α i h 2 h 1 exp 52 612 10 3 76 248 t 24 844 10 3 t 2 the kinetic fractionation factor α i k is calculated as mathieu and bariac 1996 11 α i k d v d i v n k where n k is the kinetic fractionation coefficient and d v and d i v are the molecular diffusion coefficients of light and heavy water isotopes in free air l2t 1 respectively the diffusion ratio d v d i v can be calculated from graham s law of gas diffusion 12 d v d i v m i m w 0 029 m w m i 0 029 1 2 where the number 0 029 represents the mean molecular weight of air kg mol for 18o m w 0 018 kg mol and m i 0 020 kg mol and thus d v d i v 1 0324 while for 2h m w 0 018 kg mol and m i 0 019 kg mol and thus d v d i v 1 0166 which are the values used in our study in addition to these theoretical values much research has been conducted to measure these values readers are referred to horita et al 2008 for more details for example merlivat 1978 measured d v d i v 2h 1 0251 and d v d i v 18o 1 0285 the kinetic fractionation coefficient n k is associated with considerable uncertainty depending on evaporation conditions different equations have been used to calculate this value readers can refer to braud et al 2005b horita et al 2008 and quade et al 2018 for more details table s1 shows the equations used in this study the equilibrium fractionation enrichment ε and the kinetic fractionation enrichment ε k can be calculated as follows gat 2010 13 ε 1000 1 α i 14 ε k 1000 α i k 1 h r s h a this equation can be further simplified to get the widely used kinetic fractionation enrichment equation horita et al 2008 15 ε k 1000 d v d i v n k 1 h r s h a 1000 n k d v d i v 1 h r s h a according to gonfiantini 1986 the total fractionation factor α i t o t a l can be simplified and expressed as follows 16 α i t o t a l 1 α i ε k 1000 the isotope ratio of the evaporation flux r e is then calculated using its linear relationship with the isotope ratio of the liquid phase r l 17 r e r l α i t o t a l 4 numerical models the current isotope transport models can be generally divided into two groups the first group includes numerical models for evaporation fractionation without vapor flow these models can be used in relatively humid areas where the evaporation front is close to the ground surface and vapor flow in the soil profile can thus be neglected there is no fractionation within the soil due to the lack of the vapor phase or its consideration the second group includes numerical models for evaporation fractionation with vapor flow these models are intended for more arid zones where the evaporation front can occur deeper in the soil profile and vapor flow in the soil profile should thus be considered under such conditions both equilibrium and kinetic fractionations must be considered within the soils braud et al 2005a mathieu and bariac 1996 for the calculation of relevant water flow and heat transport parameters the readers are referred to the hydrus 1d manual šimůnek et al 2008 here we only focus on the calculation of isotope related parameters 4 1 evaporation fractionation in a system that neglects vapor flow when vapor flow can be neglected e g in humid zones the one dimensional uniform soil water movement in hydrus 1d can be described using the richards equation which assumes that the air phase plays a negligible role in water flow and water flow due to thermal gradients can be neglected šimůnek et al 2008 the governing equation for water flow then is 18 θ l t z k l h h z c o s γ s where θ l is the liquid volumetric water content l3l 3 t is time t h is the water pressure head l z is the spatial coordinate l positive upward γ is the angle between the flow direction and the vertical axis k l h is the isothermal hydraulic conductivity of the liquid phase lt 1 and s is the sink term l3l 3 t 1 since there is no fractionation within the soil the governing equation for the isotope transport is the same as the classical advection dispersion equation 19 θ l c i l t z d i l c i l z q l c i l z s c i l where c i l corresponds to isotope concentrations of soil water kg m 3 q l is the liquid water flux lt 1 and d i l is the effective dispersion coefficient of the isotope i in soil water l2t 1 evaporation fractionation which does not appear in eq 19 is considered using the upper boundary condition since eq 19 is a linear equation linear conversions of concentration do not affect the numerical results therefore not only the c notation but also the r or δ notations can be used to define isotope concentrations in the numerical model compared with traditional solute transport models which leave all solutes behind in the soil during evaporation the isotope transport models allow isotopes to leave with evaporation stumpp et al 2012 did not consider fractionation and assumed that the isotope concentration of the evaporation flux is the same as that of the soil water at the soil surface here the isotope ratio of the evaporation flux is instead evaluated using two methods the first method uses the craig gordon model eq 8 which requires the atmosphere s relative humidity temperature and isotope ratio as additional inputs the second approach follows the gonfiantini 1986 model eqs 16 and 17 which requires only the atmosphere s relative humidity as an additional input the isotope ratio of the evaporation flux is then automatically used in hydrus to calculate the isotope evaporation flux at the upper boundary corresponding to the water flux 4 2 evaporation fractionation in a system that considers vapor flow 4 2 1 water flow vapor flow in the soil profile should be considered in many arid zones nonisothermal liquid and vapor flow in hydrus 1d is described as follows saito et al 2006 zheng et al 2020 20 θ t h t z k l h h z c o s γ k l t t z k v h h z k v t t z s 21 q l k l h h z c o s γ k l t t z 22 q v k v h h z k v t t z where θ t is the total volumetric water content l3l 3 being the sum θ t θ l θ v of the volumetric liquid water content θ l and the volumetric water vapor content θ v l3l 3 both expressed in terms of equivalent water contents i e θ v ρ v θ s θ l ρ w where θ s is the saturated water content l3l 3 k l t is the thermal hydraulic conductivity of the liquid phase l2k 1t 1 k v h is the isothermal vapor hydraulic conductivity lt 1 k v t is the thermal vapor hydraulic conductivity l2k 1t 1 and q v is the vapor flux lt 1 the right hand side of eq 20 represents isothermal liquid flow gravitational liquid flow thermal liquid flow isothermal vapor flow and thermal vapor flow respectively since several terms are a function of temperature this equation should be solved simultaneously with the heat transport equation to account for temporal and spatial changes in soil temperature properly 4 2 2 heat transport the governing equation for heat transport is šimůnek et al 2008 23 c p θ l t t l 0 θ v t z λ θ l t z c w q l t z c v q v t z l 0 q v z c w s t where λ θ l is the coefficient of the apparent thermal conductivity of the soil mlt 3k 1 and c p θ l c w and c v are the volumetric heat capacities ml 1t 2k 1 of the porous medium the liquid phase and vapor phase respectively l 0 is the volumetric latent heat of vaporization of liquid water ml 1t 2 the right hand side of eq 23 represents the conduction of sensible heat the first term convection of sensible heat by liquid water the second term and water vapor the third term and convection of latent heat by vapor flow the fourth term and energy uptake by plant roots the fifth term respectively 4 2 3 isotope transport following the theory of the sispat isotope model braud et al 2005a the total isotope flux is the sum of isotope fluxes in the liquid phase q i l and the vapor phase q i v while both fluxes include convection and diffusion terms assuming instantaneous equilibrium between the liquid and vapor phases the liquid and vapor isotopic ratios can be related by an equilibrium fractionation factor mathieu and bariac 1996 melayah et al 1996a the governing equations for isotope transport then are 24 θ l n s o i l θ l β i c i l t z q i l q i v s c i l 25 θ l n s o i l θ l β i c i l t z c i l q l d i l c i l z β i q v c i l d i v β i c i l z s c i l 26 θ l n s o i l θ l β i c i l t z q l β i q v d i v β i z c i l d i l d i v β i c i l z s c i l that is 27 θ i c i l t z d i l v c i l z q i l v c i l s c i l 28 θ i θ l n s o i l θ l β i 29 q i l v q l β i q v d i v β i z 30 d i l v d i l d i v β i 31 c i v β i c i l 32 c i v m i m w r i v ρ v m i m w α i r i l ρ v α i ρ v ρ w c i l where n s o i l is the soil porosity l3l 3 β i is the ratio of the isotope concentration in the vapor phase and the isotope concentration in the liquid phase and c i l r i l and c i v r i v are isotope concentrations ratios in soil water vapor kg m 3 respectively the effective dispersion coefficients of the isotope i in soil water vapor d i l d i v l2t 1 are given as follows 33 d i l d i l o τ w θ l λ q l 34 d i v n s o i l θ l τ g d v d i v d v n k where τ w and τ g are tortuosity coefficients in the liquid and vapor phases respectively λ is dispersivity l and d i l o is the molecular diffusion coefficient of the isotope i in free water l2t 1 see appendix a 4 2 4 modifications on hydrus 1d this subsection lists all implemented changes into the standard hydrus 1d model to simulate the fate and transport of stable water isotopes to expand the capabilities of the hydrus 1d model and to be consistent with previous verification studies with other models e g the plausibility tests and comparisons with the analytical solution of barnes and alison 1984 a new upper boundary condition bc for water flow was implemented into the atmospheric boundary in hydrus 1d to simulate evaporation from bare soils actual evaporation e kg m2 s is calculated in this bc as a function of potential evaporation e p kg m2 s and the difference in relative humidities between the atmosphere and the soil surface similarly as done in other studies mathieu and bariac 1996 melayah et al 1996 braud et al 2005a this is a more convenient way of estimating actual evaporation at the upper boundary however if sufficient information is available it is better to use the surface energy balance to estimate actual evaporation 35 e e p h r s h a 1 h a the standard version of hydrus 1d can simulate the transport of volatile solutes by also considering solute transport via diffusion in the vapor phase the governing equations for volatile solute transport see eqs 6 55 and 6 56 in radcliffe and šimůnek 2018 are very similar to those for isotope transport the solute transport equation solved in hydrus 1d considers convective and diffusion dispersion transport in the liquid phase and diffusion transport in the vapor phase it does not consider convective transport in the vapor phase to consider the vapor convection term in solute transport two additional transport terms β i q v and d i v β i z in eq 29 had to be included in the governing solute transport equation of hydrus 1d hydrus 1d considers the relationship between the liquid and vapor solute concentration that assumes instantaneous linear distribution of a solute between the liquid and vapor phases henry s law 36 c i v k h c i l where k h is the henry coefficient which can be temperature dependent hydrus 1d assumes that temperature dependency can be expressed using the arrhenius equation to model the isotope transport using the current volatile solute boundary condition in hydrus one can replace the original henry coefficient k h with the ratio of the isotope concentration in the vapor phase and the isotope concentration in the liquid phase β i ρ v ρ w α i since the density of water vapor ρ v is a function of relative humidity of soil air phase i e the soil matric potential while equilibrium fractionation factor α i is a function of soil temperature the henry coefficient for isotope transport is in general a function of both depth z and temperature t the standard hydrus 1d uses the stagnant boundary layer bc for volatile solutes this bc considers the convective solute flux with evaporation and the diffusion solute flux by gaseous diffusion through a stagnant boundary layer on the soil surface jury et al 1983 this upper boundary condition was modified to implement the craig gordon model to account for both equilibrium and kinetic fractionations at the interface between the soil surface and the atmosphere eq 7 4 3 particle tracking module ptm to calculate soil water travel times the particle tracking algorithm from šimůnek 1991 was implemented into hydrus 1d the algorithm is based on the water balance calculations with the development of soil water profiles fully described by solving the richards equation fig 2 the first monitored particle below the soil surface is at depth z z 0 at time t t 0 the amount of water w 0 l is between this particle and the soil surface z 0 37 w 0 0 z 0 θ z t 0 d z during the time interval t 0 t 1 the amount of water n l passes through the soil surface 38 n t 0 t 1 e t i t d t where e t lt 1 is actual evaporation and i t lt 1 is actual infiltration from precipitation or irrigation during the same interval the layers in the root zone between the soil surface and the monitored particles are depleted by root water uptake s t l 39 s t t 0 t 1 0 z p t s z t d z d t where z p t is the particle depth l at time t and s z t is the sink extraction term l3l 3 t 1 at time t 1 there is thus between the soil surface and the monitored particle the following quantity of water w 1 l enriched by infiltration and reduced by evaporation and root water uptake 40 w 1 w 0 n s t the monitored particle is now located at a depth of z z 1 41 0 z 1 θ z t 1 d z 0 z 0 θ z t 0 d z t 0 t 1 e t i t d t t 0 t 1 0 z p t s z t d z d t by repeatedly solving this equation for the time sequence t 0 t 1 t n we obtain a sequence of depths z 0 z 1 z n i e we obtain the trajectory of the observed particle the calculation of the location of the second and further particles can be performed analogously now however the amount of water is balanced between the next two particles located at z a and z b between these particles the amount of water w 0 at time t 0 and the amount of water w 1 at time t 1 are 42 w 0 z a t 0 z b t 0 θ z t 0 d z 43 w 1 z a t 1 z b t 1 θ z t 1 d z during the time interval t 0 t 1 the amount of water between the two particles is depleted by the transpiration amount s t 44 s t t 0 t 1 z a t z b t s z t d z d t according to eq 40 the resulting equation now has the form 45 z a t 1 z b t 1 θ z t 1 d z z a t 0 z b t 0 θ z t 0 d z t 0 t 1 z a t z b t s z t d z d t the algorithm itself proceeds as follows from the particles known position at the beginning of the time interval the pre solved development of the moisture profile and the actual values of infiltration evaporation and transpiration the first monitored particle s new position is calculated using eq 41 new positions of all other particles are then calculated using eq 45 on the surface and at the bottom of the soil profile new particles may be created or may leave the soil profile depending on the moisture profile s actual development by calculating particles trajectories the movement of inert substances not subject to dispersion can be modeled the initial position of particles can be defined geometrically at specified depths or based on mass balance calculations by water storage similarly the release of new particles at the boundary can be defined chronologically at specified times or meteorologically rainfall events or depths the newly implemented particle tracking module requires two input parameters w stand and w prec the w stand parameter represents the water storage which separates neighboring particles in the soil profile at the beginning of the simulation therefore the particles are not geometrically evenly distributed when the soil profile s initial water content is not uniform the w prec parameter is the amount of water that passes through the soil surface before a new particle is released this means that particles are released at the soil surface only under wet conditions under dry conditions the surface flux is directed out of the soil profile and thus new particles will not be released 5 numerical implementations the same graphical user interface gui used in hydrus 1d is used to select and execute the model the hydrus software uses the finite element method for spatial discretization and the finite difference method for temporal discretization for consistency with the numerical model the sispat isotope model used for the verification the galerkin type finite element method fem and an implicit finite difference scheme were used to solve the richards and advection dispersion equations for water flow and isotope transport in this study however the upstream weighting fem for space weighting and the crank nicholson scheme for time weighting are also available at each time step the isotope transport is calculated after the water flow and heat transport equations have been solved first this provides the isotope transport routine with nodal values of soil temperature soil matric potential and water content at both old and new time levels to constitute the storage and transport coefficients for isotope transport in eqs 27 32 details about the numerical solutions of subsurface water flow and heat and solute transport can be found in the hydrus 1d manual šimůnek et al 2008 and braud 2000 to adequately capture the isotope concentration at the soil surface similar to the sispat isotope model the isotope transport equation s solution requires a fine resolution of the vertical unsaturated soil profile close to the soil surface three discretization schemes i e coarse medium and fine fig s1 were selected in the following verification examples to explore the impact of spatial discretization on the modeling results the first scheme uses 101 nodes uniformly distributed in the soil profile i e with a spatial step of 1 cm the second scheme uses 288 nodes with spatial steps gradually increasing from the bottom to the top being twice as large at the bottom 0 46 cm than the top 0 23 cm the third scheme follows the same spatial discretization as used by braud et al 2005a with 288 nodes fig s1 the spatial steps increase from 1 μm at the surface to 1 mm at a depth of about 1 cm and 5 mm at 5 cm they remain 5 mm between depths of about 5 95 cm and then gradually decrease to 1 mm at the bottom only the modeling results obtained using the fine spatial discretization are presented in the main text the results obtained using medium and coarse spatial discretizations can be found in the supplementary material while the initial time step of 25 s was used in this study time steps vary during the simulation they are automatically adjusted by the model depending on the number of iterations required by the water flow scheme to converge adaptive time discretization since the adaptive time discretization was used the temporal resolution is expected to have only a minor effect on the results and is not discussed in this study it must be emphasized that the accuracy of the numerical solution of isotope transport equations is very sensitive to those of water flow and heat transport equations the water flow iteration process continues until absolute changes in water contents pressure heads at all nodes in the unsaturated saturated zone between two successive iterations are less than prescribed tolerances we used 10 7 for both water content and pressure head m tolerances when heat transport is also considered water flow and heat transport equations are solved simultaneously since they affect each other two choices are provided in this case depending on whether the nodal water flux balance smaller than a prescribed tolerance 10 16 m s is used as a convergence criterion for water flow and heat transport the former iteration criterion without the nodal water flux balance is more numerical efficient and more applicable for systems that neglect vapor flow the latter convergence criterion with the nodal water flux balance is more accurate and recommended for a system that considers vapor flow note that iterations are not needed in standard hydrus 1d for solute transport when the governing solute transport equation is linear in this study the difference in the isotope flux at the upper boundary between two successive iterations smaller than a prescribed tolerance 10 16 kg m2 s was added as a convergence criterion for isotope transport the above iterative criteria are important prerequisites for obtaining accurate numerical solutions it is worth mentioning that the new hydrus isotope transport model is faster than the sispat isotope model when no heat transport is considered because fewer iterations are required by the water flow scheme to converge 6 model verification and evaluation 6 1 verification of the numerical solutions first we verified the numerical model that considers evaporation fractionation without vapor flow against the analytical solution of zimmermann et al 1967 for isothermal saturated soils under steady evaporation second the numerical model that considers evaporation fractionation with vapor flow was then verified against the analytical solution of barnes and allison 1984 for nonisothermal unsaturated soils under steady evaporation third mathieu and bariac 1996 designed six plausibility tests for isothermal unsaturated soils to check whether the model produces plausible results as equilibrium and kinetic fractionations were sequentially switched on in the model braud et al 2005a and haverd and cuntz 2010 used these tests to verify the sispat isotope and soil litter iso models respectively we repeated these tests with the hydrus 1d isotope model to see whether the new model produced expected shapes of isotope profiles we considered a 1 m deep soil profile of yolo light clay from philip 1957 in all verification examples basic soil hydraulic thermal and solute transport parameters are given in braud et al 2005a and shown in table 2 for consistency with previous studies we combined the van genuchten vg water retention model van genuchten 1980 with the burdine 1953 and brooks and corey bc hydraulic conductivity model brooks and corey 1964 46 θ l θ r θ s θ r 1 1 α h n m 47 m 1 2 n 48 k θ k s θ l θ r θ s θ r η where θ s and θ r are saturated and residual water content respectively m n and α are the shape parameters of the retention curve k s is the saturated hydraulic conductivity and η is the shape parameter of the conductivity curve equations from de vries 1963 and chung and horton 1987 already available in hydrus were used to describe the volumetric heat capacity and thermal conductivity respectively the tortuosity coefficients in the liquid and vapor phases τ w and τ g are evaluated in hydrus using the model of millington and quirk 1961 or moldrup et al 1997 in all verification examples τ w and τ g were set to 0 67 and λ was set to 0 to be consistent with previous studies to evaluate our model s accuracy this choice is justified because convective and hydrodynamic dispersion processes are negligible compared with the diffusion process under evaporation conditions auriault and adler 1995 6 1 1 comparison with the analytical solution of zimmermann et al 1967 zimmermann et al 1967 conducted experiments and provided an analytical solution for the isotope transport in a homogeneous saturated soil column with the initial isotope ratio isotopic composition r δ evaporating at a steady rate e s into the atmosphere of constant humidity h a air temperature t a and isotope ratio isotopic composition δ i a v r i a v under isothermal conditions at a soil temperature t z table 3 provides all relevant parameter values under steady state conditions the stable isotope profile can be explained by the balance between the upward convective flux evaporation and the downward diffusion flux of the isotope 49 e s r i l r d i l d r i l d z where r i l is the isotope ratio at depth z z is equal to zero at the soil surface and it is positive downwards the above equation can be solved to get 50 r i l r r 0 r exp z z l where r 0 is the isotope ratio at the soil surface and z l is the characteristic length given by 51 z l d i l e s if one reports the isotope ratio in eq 50 in the δ notation using eq 2 we can get 52 δ i l δ δ 0 δ exp z z l where δ 0 δ i l are isotopic compositions at the soil surface and at depth z respectively the isotopic composition at the soil surface δ 0 can be calculated using a variant of the craig gordon model as follows barnes and allison 1983 53 α i 1 δ 0 h a 1 δ i a v 1 h a α i k 1 δ the analytical solution for 18o is 54 δ o 18 31 9 e x p 16 949 z 1000 and for 2h is 55 δ h 2 67 e x p 16 667 z 1000 in the hydrus numerical simulation transport parameters were the same as those in the analytical solution both the upper and lower bcs were set to a constant water pressure head for water flow the soil water pressure head was assumed to be 1 cm at the surface and 109 15 cm at the bottom this bc allowed for a permanent water supply at the bottom of the soil column and kept the soil saturated while maintaining the steady evaporation rate e s both the upper and lower bcs were set to solute flux bcs for isotope transport the surface solute flux in this example referred to the evaporation flux for water isotopes e i calculated by the craig gordon model eq 7 the bottom isotope flux was calculated assuming that the isotope ratio isotopic composition of supply water was the same as the initial values r δ no heat transport was considered in this example fig 3 a and b shows an excellent agreement between the numerical and analytical solutions using a fine spatial discretization fig s2 shows a comparison between the analytical and numerical solutions results using different spatial discretizations the maximum differences between the analytical and numerical solutions in the 18o isotopic composition profiles were 0 21 coarse 0 20 medium and 0 20 fine the maximum differences between the analytical and numerical solutions in the 2h isotopic composition profiles were 0 46 coarse 0 43 medium and 0 43 fine we may conclude that the isotope transport module can produce in this example accurate isotope profiles using all considered spatial discretization schemes water that has experienced evaporation fractionation plots below the global local meteoric water line gmwl lmwl in dual isotope plots the occurrence of kinetic fractionation results in an evaporation line with a slope much smaller than those of gmwl lmwl sprenger et al 2016a the line conditioned excess lc excess is the difference between the δ h 2 from a water sample and a linear transformation of the δ h 2 from a given gmwl lmwl landwehr and coplen 2006 the more negative it is the stronger the kinetic fractionation is sprenger et al 2017 the dual isotope plot fig 3c has a slope of about 2 09 which is much smaller than that 8 20 of the global meteoric water line gmwl the lc excess profile calculated by eq a6 shows the opposite trend to the isotopic composition profiles and is negative in the entire soil profile fig 3d these results suggest that kinetic fractionation also occurs this is reasonable given the fact that kinetic fractionation factor α i k is not equal to one table 3 6 1 2 comparison with the analytical solution of barnes and allison 1984 barnes and allison 1984 proposed an analytical solution for evaporation from unsaturated soil under steady and nonisothermal conditions conditions were the same as for the steady state saturated case above except that the initial pressure head was set to 0 in the entire soil profile nonisothermal conditions were considered and evaporation occurred at a different rate table 4 gives the values of all parameters required in this problem under steady state conditions i e at 250 days of the simulation the stable isotope profile can be explained by the balance between the upward convective flux evaporation and the downward diffusion flux of the isotope both in the liquid and vapor phases 56 e s ρ w q l q v ρ w q l d v d h r ρ s a t v d z 57 e s r q i l q i v ρ w q l r i ρ w d i l d r i l d z d i v d h r ρ s a t v r i v d z where d v is the effective dispersion coefficient of the light isotope in soil water vapor e s is the steady state evaporation rate and h r is the relative humidity of the soil air phase at a certain depth h r can be calculated by eq 5 while the matric potential h s and temperature t s at the soil surface should be replaced by corresponding values at a certain soil depth if we expand the derivative form of the vapor flux d v d h r ρ s a t v d z in eq 56 we can easily find that it describes the sum of the isothermal k v h h z and nonisothermal k v t t z terms in eq 22 if we expand the derivative form of the isotope diffusion flux in the soil water vapor d i v d h r ρ s a t v r i v d z in eq 57 we can easily find out that it describes the sum of the convection β i q v c i l and diffusion d i v β i c i l z terms in eq 25 that is to say vapor convection within the soil is also a diffusive process haverd and cuntz 2010 if we define characteristic lengths z l and z v as follows 58 z l ρ w d l e s 59 z v d v ρ s a t v e s we can then get 60 ρ w q v e s d v d h r ρ s a t v d z e s h r z v d l n h r ρ s a t v d z 61 q i v e s d i v d h r ρ s a t v r i v d z e s h r z v r i l α i α i k d l n h r ρ s a t v α i r i l d z 62 q i l e s ρ w q l r i l ρ w d i l d r i l d z e s ρ w q l r i l e s z l σ i r i l d l n r i l d z combining these equations gives 63 h r z v r i l 1 α i α i k d ln h r ρ s a t v d z α i α i k d ln α i r i l d z r i l r z l σ i d r i l d z where d l is the effective dispersion coefficient of the light isotope in soil water and σ i is a constant depending on the isotope species see appendix a according to the relationship between r and δ values shown in eq 2 the analytical solution can be further simplified and given by the differential equation as follows 64 d δ i l d z z l h r z v 1 δ i l δ h r z v z l h r z v 1 α i k α i d d z ln h r ρ s a t v α i k α i this is a semi analytical solution it can only be solved once we prescribe the isotopic composition of soil water at the surface and solve the water flow and heat transport equations which will provide soil temperatures pressure heads and water contents in the hydrus numerical simulation transport parameters were the same as those in the analytical solution the constant pressure head equal to 0 was adopted in the numerical simulation as the lower bc for soil water flow the new water flow bc which calculates actual evaporation as a function of potential evaporation e p and the difference in humidities between the air and the soil surface eq 35 was used at the upper atmospheric boundary the solute flux was used as the lower bc for isotope transport automatically calculated from its isotopic composition equal to δ and the bottom water flux the stagnant bc for volatile solutes was used at the upper boundary for isotope transport the surface solute flux referred to the evaporation flux for water isotopes e i calculated by the craig gordon model eq 7 the temperature bc was used for heat transport at both boundaries fig 4 e and f shows an excellent agreement between the analytical and numerical solutions using a fine spatial discretization despite a slight overestimation of the peak isotopic composition by hydrus 1d fig s3 compares the analytical and numerical solutions obtained using different spatial discretizations the maximum differences at the evaporation front between the analytical and numerical solutions in the 18o isotopic composition profiles were 24 88 coarse discretization 3 74 medium and 0 88 fine the maximum differences between the analytical and numerical solutions in the 2h isotopic composition profiles were 34 68 coarse 8 40 medium and 3 67 fine this means that in this example the isotope transport module can produce relatively well isotope profiles as long as an appropriate spatial discretization is used the isotopic composition profiles have maximum values at a depth of 2 cm which corresponds with the water content matric potential temperature profiles inflection points fig 4a b and 4c this is also the evaporation front location where the upward soil water flux changes from the liquid to vapor flux fig 4d the dual isotope plots fig 4g have slopes of about 2 66 and 1 62 in the vapor and liquid dominant zones vdz ldz respectively which are far smaller than those of the gmwl the lc excess profile shows the opposite trend to the isotopic composition profiles and is negative in the entire soil profile fig 4h these results suggest that kinetic fractionation also occurs this is reasonable since the kinetic fractionation factor α i k is not equal to one when n k is one table 4 6 1 3 plausibility tests the soil was initially saturated and under hydrostatic conditions the soil water pressure head was equal to 0 01 m at the top and linearly increased to 0 99 m at the bottom the initial isotopic composition δ and soil temperature t z in the soil column were uniform i e the same at all depths water was evaporating from the soil column into an atmosphere with temperature t a relative humidity h a and isotopic composition δ i a v all relevant parameters are summarized in table 5 mathieu and bariac 1996 melayah et al 1996 braud et al 2005a in the hydrus numerical simulation zero water and isotope fluxes were adopted as the lower bcs the new water flow bc which calculates actual evaporation as a function of potential evaporation e p and the difference in humidities between the air and the soil surface eq 35 was used at the upper atmospheric boundary the stagnant bc for volatile solutes was used at the upper boundary for isotope transport the surface solute flux referred to the evaporation flux for water isotopes e i calculated by the craig gordon model eq 7 no heat transport was considered in this example the plausibility test conditions are listed in table 6 the impacts of four parameters on isotopic composition profiles were considered including the equilibrium fractionation factor α i the kinetic fractionation factor α i k which affects the molecular diffusion coefficient of the isotope i in free air d i v the molecular diffusion coefficient of the isotope i in free water d i l and the isotopic composition of atmospheric vapor δ i a v equations from majoube 1971 and mathieu and bariac 1996 were used to calculate the equilibrium α i and kinetic fractionation factor α i k respectively for tests in which they were not equal to 1 the molecular diffusion coefficients of the isotope i in free water air d i l o d i v were calculated by eq a3 a5 these values were then used to calculate the effective dispersion coefficients for the isotope i in soil water vapor d i l d i v based on eqs 33 and 34 the steady vertical isotopic composition profiles at the end of the 250 day simulation are shown in fig 5 test 1 equilibrium and kinetic fractionation factors α i α i k are set to one and molecular diffusion coefficient of the isotope i in free water d i l o is set to zero in other words evaporation fractionation and diffusion in the liquid phase are neglected the isotopic composition of the atmospheric water vapor is set equal to that of the initial soil water δ this results in uniform isotopic compositions in soil water throughout the soil profile equal to δ as expected test 2 test 2 is the same as test 1 except that the isotopic composition of the atmospheric water vapor is set to a low value δ i a v isotope diffusion in soil water vapor due to the concentration gradient between the free atmosphere and soil results in increased isotopic compositions of liquid and vapor phases within the soil as depth increases given the linear relationship between them eq 32 the isotopic composition of surface soil water is close to that of the atmospheric water vapor δ i a v and increases gradually with depth to its initial value δ test 3 test 3 is the same as test 1 except that equilibrium isotopic fractionation is turned on i e α i is not equal to one equilibrium fractionation between soil water and soil water vapor moves lighter water molecules from the liquid phase into the vapor phase which causes isotopic enrichment of the remaining soil water however this enrichment rate is different between regions above and below 5 cm i e the evaporation front as seen in fig 5a and b due to different vapor fluxes above 5 cm the vapor flux is approximately constant with depth and thus the effect of equilibrium fractionation does not differ too much with depth this results in a slow transition from the isotopic composition of soil water towards the surface value below 5 cm the isotopic composition of soil water increases rapidly towards the 5 cm depth due to the increased upward vapor flux fig 5b test 4 test 4 is the same as test 3 except that the isotopic composition of the atmospheric water vapor is reset to a low value δ i a v this shifts the isotopic composition of surface soil water close to δ i a v similarly as in test 2 this surface effect combined with increasing enrichment from the soil bottom towards the soil surface as discussed in test 3 leads to the simulated maximum of the isotopic composition profile test 5 test 5 is the same as test 4 except that diffusion in the liquid phase is turned on i e d i l 0 is not equal to zero since diffusion in the liquid phase causes spreading or dispersion of the solute front radcliffe and šimůnek 2018 this test produces a smaller peak of the isotopic composition profile test 6 test 6 is the same as test 5 except that the kinetic fractionation at the surface is turned on i e α i k is not equal to one and the molecular diffusion coefficient of the isotope i in free air is set to its real value d i v smaller than d v as seen in eq 12 the smaller molecular diffusion coefficient in free air results in increased kinetic fractionation by decreasing the removal of heavy isotopes through the vapor flux this increases isotopic enrichment in the remaining soil water leading to a larger peak of the isotopic composition profile than in test 5 as for dual isotope plots test 6 has slopes far smaller than that of the global meteoric water line gmwl in both liquid ldz and vapor dominant vdz zones fig s4d the line conditioned excess lc excess profile shows the opposite trends to the isotopic composition profiles and is always negative fig s5 these suggest that kinetic fractionation also occurs this is reasonable given the fact that the kinetic fractionation factor α i k is not equal to one in test 6 table 6 for tests 3 5 the dual isotope plots of both the ldz and vdz figs s4a s4b and s4c have slopes of about 6 55 7 80 which are much closer to that of the gmwl this is reasonable since the kinetic fractionation factor is equal to one in tests 3 5 table 6 and thus only equilibrium fractionation occurs these slopes are not exactly equal to that of gmwl especially for the vdz where the exchange with the atmosphere is more significant however the lc excess values in tests 3 5 are almost a constant low value about 10 throughout the soil profile compared to much more negative values in test 6 fig s5 this again verifies that only equilibrium fractionation occurs in tests 3 5 overall the slopes of dual isotope plots with kinetic fractionation are much smaller than those without consideration figs 3c and 4g and fig s4 the lc excesses at the surface layer about 0 50 cm are much more negative than in other depths figs 3d and 4h and fig s5 this indicates that the fractionation at the surface layer is more significant these conclusions are also consistent with those in sprenger et al 2016a therefore the isotope transport module is accurate also from the perspectives of dual isotope plots and lc excess values 6 2 evaluation against the experiment data 6 2 1 the transport of isotopes the dataset is from stumpp et al 2012 available https www pc progress com en default aspx h1d lib isotope the field experiment was conducted in a humid region located at the research area of the hblfa raumberg gumpenstein in gumpenstein austria with a mean annual temperature of 6 9 c and mean annual precipitation of 1035 mm the cylindrical lysimeter with a depth of 1 5 m and a surface area of 1 m2 was embedded in a rainfed agricultural field planted with winter wheat and fertilized with liquid cattle slurry the lysimeter 3 in stumpp et al 2012 the isotopic composition of precipitation and lysimeter seepage water were measured on the event and weekly intervals respectively from may 2002 to february 2007 1736 days in total the temporal distribution of precipitation evapotranspiration temperature air humidity during the simulation period are provided in fig s6 more details about other data collection and measurements can be found in stumpp et al 2012 the final optimized soil hydraulic and solute transport parameters reported in stumpp et al 2012 table s2 were used in the numerical simulations reported below the atmospheric with a surface layer and seepage face boundary conditions were used for water flow at the upper and lower boundaries respectively the temperature bc was used for heat transport at both boundaries the solute flux and zero concentration gradient bcs were used for isotope transport at the upper and lower boundaries respectively the isotope ratio of the evaporation flux was automatically used in hydrus to calculate the isotope evaporation flux at the upper boundary corresponding to the water flux to investigate the sensitivity of the simulation results to the upper boundary conditions for isotope transport the relevant parameters r e r a of different evaporation fractionation models stumpp et al 2012 the craig gordon model and the gonfiantini model were adjusted and implemented their impacts on the simulation results under different assumptions with and without fractionation were discussed since kinetic fractionation can be neglected in humid zones horita et al 2008 only equilibrium fractionation was considered in this example i e n k 0 fig 6 shows the comparison between 18o isotopic compositions of the lysimeter seepage water simulated by stumpp et al 2012 and using the gonfiantini and craig gordon models for a system that neglects vapor flow the nash sutcliffe efficiency nse and determination coefficient r 2 are shown in table 7 the water samples from the lysimeter seepage water plot on the lmwl fig 5 of stumpp et al 2012 indicating negligible fractionation therefore the measured data are closest to the simulations that do not consider fractionation as stumpp et al 2012 did in this model the isotope ratio of the evaporation flux r e is the same as that of the surface soil water r l i e r e r l in the gonfiantini model r e is α i times of r l as can be seen in fig 6 the measured values are close to the values simulated by the gonfiantini model in case of no fractionation i e α i 1 n k 0 r e r l which produces the same results as stumpp et al 2012 for most of the simulation period in the end during about 1150 1500 days the measured values are close to those simulated considering equilibrium fractionation i e α i α i n k 0 r e α i r l to obtain a better agreement between the simulation results and measurements using the craig gordon model the early atmospheric isotope ratio r a should correspond to eq s9 i e r a α i 1 h a r l h a while the late r a should correspond to eq s7 i e r a α i r l therefore in the craig gordon model method an approximate estimate of r a using eq s10 was used for the entire simulation period to calculate r e under equilibrium fractionation assumption i e α i α i n k 0 r a α i 1 h a h a α i r l 2 h a more details can be found in the supplementary material the craig gordon model has obtained relatively satisfactory simulation results nse 0 19 r 2 0 30 compared to the gonfiantini model nse 0 52 r 2 0 25 in the case of equilibrium fractionation the significant differences between the values simulated by the gonfiantini and craig gordon models emphasize the considerable impact of r a on the simulation results due to its effect on r e however the model performance is worse than when fractionation is neglected nse 0 24 r 2 0 37 this also indirectly validates the assumption of stumpp et al 2012 not to consider evaporation fractionation in their analysis of data from this humid zone however this does not rule out errors due to an inaccurate estimation of r a used in the simulation we note that the final optimized soil hydraulic and solute transport parameters reported in stumpp et al 2012 were used in the numerical simulations this parameter set was estimated based on the assumption that there was no fractionation which may not be optimal when fractionation is present this may also explain the best agreement of the stump et al 2012 simulation with the measurements however even under the no fractionation assumption this agreement is not very good likely due to some uncontrollable factors in the field experiments the isotopic compositions and overall temporal variation trends simulated using the gonfiantini or craig godron models considering fractionation are consistent with measured data and the stumpp et al 2012 simulation without considering fractionation this is because evaporation fractionation will not change isotopic composition trends when evaporation is much smaller than the sum of precipitation and soil water storage and the equilibrium fractionation factor is close to 1 however the selection of the atmospheric isotope ratio r a can affect the fluctuation amplitude of the isotope time series by affecting r e the isotopic composition of discharge simulated by all models remains the same during the first 150 days because only water initially in the profile and thus not affected by the upper bc treatment reaches the bottom during this time water infiltrating at the beginning of the simulation starts arriving at the bottom after about 150 days when isotopic compositions simulated by different models with different assumptions start deviating from this point forward differences in simulated discharge isotopic compositions reflect different treatments of the upper bc 6 2 2 particle tracking the input parameters w stand and w prec discussed in section 4 3 of the particle tracking module ptm were set equal to 2 and 10 cm respectively fig 7 shows the spatial temporal distribution of particles during the 5 year simulation there are 48 particles in total among which 18 particles p1 p18 were initially in the soil profile while the next 26 particles p19 p44 were released at the soil surface passed through the lysimeter and left at the bottom finally the last 4 particles p45 p48 were released at the soil surface and remained in the soil profile at the end of the simulation the particle trajectories suddenly drop during periods with many rainfall events and slowly decrease or even rise during periods with limited rainfall fig 7 particles move downward faster during wet seasons and slow down during dry seasons particles move down sharply after heavy rainfalls reflecting piston flow s typical characteristics particles released right before the wet season move down faster than those released right before the dry season the transit times of these particles and corresponding velocities were calculated given in table s3 and shown in fig 7c the mean recharge transit time and velocity are 276 1 days and 6 0 mm day respectively these values are slightly different from those calculated by stumpp et al 2012 using the peak displacement method the mean recharge transit time 250 days and velocity 6 0 mm day of soil water were estimated by stumpp et al 2012 by comparing the convective shift in the isotope peaks of the input precipitation during 2005 2006 and output lysimeter discharge and considering dispersion effects this difference may also be due to different rainfall events selected for these calculations stumpp et al 2012 selected precipitation events from 2005 to 2006 since only during this period there was pronounced and distinct correspondence between the isotopic peaks in precipitation and lysimeter discharge since particles move faster during the period with many precipitation events the peak displacement method is likely to overestimate the flow velocity compared to particle tracking overall the two approaches results are similar which shows the particle tracking model s applicability however the peak displacement method is not applicable when there are no pronounced peaks or a distinct correspondence between the input and output peaks the particle tracking module can be used under such circumstances and overcome this shortcoming of the peak displacement method thus expanding the possibility of calculating transit times to verify the new particle tracking module we conducted simple mass balance calculations based on the results of the numerical solution of the richards equation in hydrus 1d the amount of water in the soil profile when the particle leaves the transport domain w t final should be equal to the amount of water applied at the soil surface infiltration since its release reduced by evaporation and root water uptake from the soil between the particle and the soil surface 65 w t f i n a l t i n i t t f i n a l i t e t 0 z p t s z t d z d t where t init and t final are times when the particle is released at the soil surface and when it leaves at the soil profile bottom respectively the mass balance calculations carried out according to 65 are given in table s4 indicating an almost perfect match and validating the particle tracking module 6 3 discussion and future work modeling water flow and solute transport in the critical zone requires an accurate estimation of soil hydraulic and solute transport parameters combining different types of observed data to calibrate water flow and solute transport models has been found to improve model parameterization for example sprenger et al 2015 demonstrated that a combination of stable isotope profiles and soil moisture time series allowed for a better model calibration for solute transport water flow and root water uptake parameters groh et al 2018 determined the soil hydraulic parameters and the longitudinal dispersivity for multiple lysimeters using two step and bi objective optimization strategies they concluded that the bi objective strategy combining water content matric potential and tracer data was the best parameter estimation strategy mattei et al 2020 showed that it is possible to use only water content and stable isotope profiles measured at one time to accurately calibrate the model for groundwater recharge estimation however using stable isotope data at different soil depths at different times can improve the model calibration mattei et al 2020 the new isotope transport module in hydrus 1d can simulate continuous space time dynamics of stable water isotope concentrations of soil water whether the impact of consideration of evaporation fractionation will propagate into the inversion of soil hydraulic and solute transport parameters is unknown future work will include analyzing field datasets collected in arid regions or laboratory experiments of braud et al 2009 where evaporation fractionation plays a vital role and depth dependent observations are available sensitivity analyses and parameter inversions will be conducted to evaluate the new isotope transport model further the assumption of well mixed water in the soil is in contrast with the recent two water world tww hypothesis that water in the soil should be split into two pools that are isotopically different berry et al 2018 brooks et al 2010 mcdonnell 2014 the first pool the mobile soil water pool often replenishes groundwater and has isotopic composition close to that of the infiltrating water the second pool the immobile soil water pool is supposed to be composed of tightly bound water enriched by evaporation that resides in the soil s capillary space some of which can be used by plants many isotopic measurements support the hypothesis of the widespread existence of tww and demonstrate that some sampling methods e g suction lysimeters are likely to obtain the mobile water while others e g centrifugation cryogenic or toluene distillation are prone to sample all soil water e g figueroa johnson et al 2007 geris et al 2015 goldsmith et al 2012 knighton et al 2019 oerter and bowen 2017 zhao et al 2016 the tww hypothesis was formulated based on two assumptions vargas et al 2017 the first is that there is no mass exchange between the mobile and immobile waters the second is that plant water uptake does not discriminate against 2h or 18o and thus does not affect the isotopic composition of soil water however recent studies have shown that mass exchange between mobile and immobile waters e g oshun et al 2016 vargas et al 2017 and isotopic fractionation may occur during plant water uptake e g barbeta et al 2019 poca et al 2019 on the other hand there is accumulating evidence showing that the equilibrium isotope fractionation between pore water and water vapor within the soil is significantly different from that between liquid and water vapor for free water surface due to complex hydrophilic interactions between soil pore surface and water molecules e g chen et al 2016 gaj and mcdonnell 2019 lin and horita 2016 lin et al 2018 oerter et al 2014 however how waters of different mobility alter the isotopic composition of soil water is little understood and seldom accounted for in isotope transport modeling sprenger et al 2018 the standard version of hydrus 1d can consider a series of physical nonequilibrium flow and transport models e g dual porosity and dual permeability and the same conceptualization can be applied to simulate isotope transport these nonequilibrium flow and transport models will be used in our future studies to evaluate the impacts of physical nonequilibrium on isotope transport modeling and transit time calculations for tww systems 7 summary and conclusions this study presents a model which can simultaneously solve the coupled equations describing the movement of water heat and stable isotopes it is based on the hydrus 1d model to which the isotope transport and particle tracking modules were added the comparisons with analytical solutions plausibility tests under saturated unsaturated and isothermal nonisothermal conditions and field validation demonstrate the model s accuracy and robustness transit times calculated by the particle tracking module ptm are similar to those evaluated by the peak displacement method which validates the use of the water flow based ptm as an alternative tool to isotope transport based methods as compared with existing isotope models our approach enables many thousands of current hydrus users to efficiently operate the new model while using various advanced hydrus software features including flexible dynamic boundary conditions equilibrium and nonequilibrium water flow parameter optimization routines and the well designed user friendly gui šimůnek et al 2016 while also providing higher computational efficiency for example the sispat model always calculates both water flow and heat transport even when the soil system is isothermal our model simulates only water flow for isothermal systems improving numerical efficiency the new particle tracking module provides the hydrus 1d users with an additional tool for assessing transit times the developed model represents a comprehensive tool to numerically investigate many important research problems involving isotope transport processes and establishes a more solid foundation for applying stable isotope tracing in the critical zone declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this research was financially supported by the multistate w4188 program we appreciate editors and reviewers for their constructive comments on this manuscript appendix b supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix b supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105118 appendix a the density of the saturated water vapor ρ s a t v kg m3 depending on temperature t k is calculated as follows a1 ρ s a t v 10 3 exp 31 3716 6014 79 t 7 92495 10 3 t t the density of the water vapor ρ v is the product of the density of the saturated water vapor ρ s a t v kg m3 and relative humidity h r a2 ρ v h r ρ s a t v the molecular diffusion coefficient of the isotope i in free water d i l o l2t 1 is expressed as a function of temperature t k a3 d i l o σ i 10 9 exp 535400 t 2 1393 3 t 2 1876 where σ i is a constant depending on the isotope species 0 98331 for hdo and 0 96691 for h2 18o the molecular diffusion of the isotope i in free air d i v l2t 1 is expressed as a function of temperature t k a4 d i v d v b i a5 d v 2 12 10 5 t 273 16 2 where b i is the ratio of the molecular diffusion coefficients of light and heavy water isotopes in free air 1 0166 for hdo and 1 0324 for h2 18o as discussed in eq 12 for the verification examples in this study the gmwl defined by rozanski et al 1993 was used the lc excess was calculated as follows a6 l c e x c e s s δ h 2 8 2 δ o 18 11 27 1 15 
25798,ecohydrological processes are often evaluated by studying the fate of stable water isotopes however isotopic fractionation during evaporation is often ignored or simplified in current models resulting in simulation errors that may be propagated into practical applications of stable isotope tracing in this study we adapted and tested the hydrus 1d model a numerical model widely used to simulate variably saturated water flow and solute transport in porous media by including an option to simulate isotope fate and transport while accounting for evaporation fractionation the numerical results obtained by the adapted model were in excellent agreement with existing analytical solutions additional plausibility tests and field evaluation further demonstrated the adapted model s accuracy a simple particle tracking algorithm was also implemented to calculate soil water s transit times and further validate the modified model s results transit times calculated by the particle tracking module ptm were similar to those estimated by the isotope peak displacement method validating the applicability of the ptm the developed model represents a comprehensive tool to numerically investigate many important research problems involving isotope transport processes in the critical zone keywords water stable isotopes isotope transport modeling hydrus 1d evaporation fractionation water transit time particle tracking algorithm 1 introduction evaporation fractionation is characterized by the retainment of heavier isotopes in the liquid phase and the preferential affinity of lighter isotopes in the vapor phase e g gonfiantini et al 2018 due to evaporation fractionation s unique characteristics stable water isotopes 2h and 18o are good indicators for studying many ecohydrological processes in the critical zone gehrels et al 1998 sprenger et al 2016a such as partitioning evapotranspiration kool et al 2014 xiao et al 2018 and identifying the sources of crop water uptake i e sourcing e g corneo et al 2018 ma and song 2016 wang et al 2019 at the soil vegetation atmosphere interface models that can accurately simulate the transport and fractionation of isotopes are necessary to properly interpret isotopic data in the critical zone the concept of water transit or travel time tt defined as the time elapsed between water entering and leaving a reservoir provides a useful insight into many ecohydrological issues such as partitioning recharge and discharge sources evaluating the role of mobile and immobile waters and inferring temporal origins of root water uptake e g allen et al 2019 brinkmann et al 2018 mcdonnell 2014 sprenger et al 2016b the traditional isotope based method for estimating tts is by inversely estimating lumped isotope transport parameters assuming time invariant tt distributions ttds e g maloszewski et al 2006 stumpp and maloszewski 2010 timbe et al 2014 or storage selection sas functions rinaldo et al 2015 however the lumped models overgeneralize the isotope transport mechanisms some of them cannot truly describe the isotope transport or ttds under transient conditions while others can account for the time variance of ttds but can only describe the mixing and partitioning of isotopes jury et al 1986 sprenger et al 2016a physics based isotope transport models are needed to fully describe the spatio temporal evolution of isotope concentrations under field conditions kim et al 2016 such flow and transport models usually rely on the richards and convection dispersion equations respectively when evaporation fractionation can be neglected one can simulate the fate and transport of isotopes in soils as standard solutes for example stumpp et al 2012 used the modified hydrus 1d model with isotopic information to analyze the effects of the vegetation cover and fertilization measures on water flow and solute transport in lysimeters this modified model is available at https www pc progress com en default aspx h1d lib isotope sprenger et al 2016b used this modified model to infer soil water residence times at different depths brinkmann et al 2018 applied the same model to estimate the residence time distribution of soil water and identify the temporal origin of water taken up by picea abies and fagus sylvatica this modified version of the hydrus 1d model by stumpp et al 2012 allows isotopes to leave the soil profile at the soil surface without considering the fractionation effect during evaporation this is implemented by assuming that the isotope concentration of the evaporation flux is the same as that of soil water at the soil surface however ignoring the evaporative enrichment as done in this modified hydrus 1d leads to underestimating 2h and 18o concentrations in the topsoil which may be more significant in regions with higher evaporative losses sprenger et al 2018 additionally transit times calculations as done in these studies e g sprenger et al 2016b brinkmann et al 2018 are based on isotope transport simulations and require labor intensive and time consuming high precision isotope measurements to calibrate the model the inaccurate sampling or modeling of soil water isotopes especially in case of physical nonequilibrium i e immobile water content dual porosity permeability type solute transport can easily lead to large errors in transit time calculations e g sprenger et al 2018 tetzlaff et al 2014 the temporal evolution of evaporation fractionation was first studied and modeled for the free water surface craig and gordon 1965 the craig gordon model has been the cornerstone of isotope hydrology since it was proposed in 1965 after that zimmermann et al 1967 applied this model to saturated soil under steady state evaporation conditions barnes and allison 1983 extended this work to isothermal steady state evaporation conditions in unsaturated soils barnes and allison 1984 further extended this work to the nonisothermal steady state conditions in unsaturated soils with a defined soil temperature profile barnes and allison 1983 1984 also provided analytical solutions for the transport of isotopes with evaporation fractionation under steady state conditions however to describe and predict the spatial and temporal evolution of isotope concentrations under field evaporation conditions a model capable of describing transient conditions is required shurbaji and phillips 1995 proposed the first numerical model odwish that considered evaporation fractionation this model coupled heat transport and water flow equations in the soil proposed by philip and de vries 1957 and introduced a transition factor into the isotope transport equation this transition factor combines the influence of hydrology and isotope parameters it changes slowly with depth except for quick changes in the evaporative zone which is conducive to obtaining a unique isotope profile shape however the upper boundary condition must be determined by measuring temperatures and humidities at the soil surface and the evaporation front the evaporation front is located at a depth above which the water vapor flux becomes dominant compared to the liquid flux generally it corresponds to the peak in the isotope concentration profile braud et al 2005a since such data are rarely available a model that interacts with the atmosphere is needed to address the surface energy budget mathieu and bariac 1996 proposed a simplified model moise for constant potential evaporation and a predefined soil temperature profile this model still lacked the option of evaluating the surface energy budget soderberg et al 2012 melayah et al 1996a fully coupled the transport of heat water and isotopes with surface energy budget calculations the results showed that the model was very sensitive to the initial isotope profile and small changes in liquid water convective transport better knowledge of isotope transport coefficients in porous media e g mobile immobile phases should improve its prediction ability melayah et al 1996b braud et al 2005a corrected some inconsistencies in the derivations of melayah et al 1996a and several isotope transport models such as sispat isotope braud et al 2005a and soil litter iso haverd and cuntz 2010 have been developed based on this modified theory the soil litter iso model was based on ross explicit numerical solution of the richards equation ross 2003 resulting in significantly improved computational efficiency compared with the sispat isotope model this allowed isotope calculations to be performed for soil profiles with vegetation using coarser spatial discretization and larger time steps haverd and cuntz 2010 however these models did not consider the impacts of physical nonequilibrium flow e g immobile water or preferential flow on isotope transport and concentrations mueller et al 2014 and sprenger et al 2018 used the swis model soil water isotope simulator to model stable isotopes for uniform and nonequilibrium mobile and bulk soil water flow in the vadose zone respectively this model considered evaporation fractionation but neglected vapor flow despite the successes of isotope transport modeling with evaporation fractionation the current isotope transport models table 1 are not widely used some of them are no longer maintained e g mathieu and bariac 1996 melayah et al 1996 are quite complex to deploy e g braud et al 2005a haverd and cuntz 2010 or still only implement a simple treatment of evaporation fractionation e g shurbaji and phillips 1995 stumpp et al 2012 which may be some of the reasons why they are not commonly used the standard version of hydrus 1d can simulate volatile solutes transport in soils by allowing solute transport by convection and dispersion in the liquid phase and diffusion in the soil air thus the model is quite widely used to simulate transport processes of many emerging organic chemicals such as pesticides and fumigants e g spurlock et al 2013ab brown et al 2019 the governing equations for volatile solute transport see eqs 6 55 and 6 56 in radcliffe and šimůnek 2018 are similar to those for the isotope transport by braud et al 2005a the relationship between the liquid and vapor solute concentrations is described in hydrus 1d by henry s law assuming an instantaneous distribution of a solute between the liquid and air phases this volatile solute transport model in hydrus 1d can be adapted to simulate the transport of stable isotopes by modifying the upper boundary condition considering fractionation and reinterpreting henry s coefficient in the governing solute transport equation particle tracking algorithms represent an alternative and more straightforward way of calculating ttds e g šimůnek 1991 asadollahi et al 2020 while still considering transient water flow since the particle tracking algorithm e g šimůnek 1991 can be technically based solely on water balance calculations without requiring isotopic measurements it needs much less input information than the stable water isotope transport models such algorithms can thus have broad applicability and can act as an excellent supplement to the traditional isotope transport based methods for calculating transit times however it is still highly recommended to observe isotopic data identifying accurate model parameters and travel times e g groh et al 2018 mattei et al 2020 sprenger et al 2015 and verifying model determined tts the objectives of this study thus are 1 to adapt the current hydrus 1d model to simulate water flow and transport of stable water isotopes while considering multiple types of evaporation fractionation situations and soil conditions i e isothermal non isothermal conditions equilibrium and nonequilibrium flow with and without vapor flow 2 to verify the new model using analytical solutions and plausibility tests 3 to implement a simple water flow based particle tracking algorithm and 4 to evaluate the capability of the new isotope transport and particle tracking modules using a field dataset the new isotope transport and particle tracking modules provide hydrus 1d users with a comprehensive tool for assessing transit times simulating continuous dynamic changes in soil water isotope concentrations and numerically investigating many fundamental research problems involving sourcing and timing of soil water 2 definition of the isotope concentrations following braud et al 2005a the concentration c i kg m 3 of the isotope i can be defined as 1 c i m i v m i m t m t v n i m i n i m i n w m w n i m i n w m w v m i m w r i ρ where m i kg is the mass of the isotope i either in the liquid or vapor phase v m3 is the volume of water m t kg is the total mass of water m i and m w kg mol are the molar masses of water including the isotope i and ordinary water respectively n i and n w mol are the numbers of moles of water including the isotope i and ordinary water respectively r i is the isotope ratio of the isotope i i e n i n w and ρ kg m3 is the density of water either in the liquid ρ w or vapor ρ v phases see appendix a in this equation we assumed that n i m i n w m w to get the last term the relationship between the isotope ratio r and isotopic composition δ is 2 δ i r i r std r std 1000 where r i and r s t d are the isotope ratios in the water sample and the standard sample the vienna standard mean ocean water vsmow 0 r s t d 155 76 10 6 for hdo and r s t d 2005 2 10 6 for h2 18o according to gonfiantini 1978 r i refers to the 18o 16o or 2h 1h ratios that can be deduced from eq 1 as follows 3 r i m w m i c i ρ note that in this study the term isotope ratio refers to r isotopic composition refers to δ and isotope concentration refers to c kg m 3 the results will be presented throughout the manuscript in δ notation even though numerical computations may be performed using the c r or δ notations 3 craig gordon model 1965 the separation of heavy and light isotopes between reservoirs or reactants and products is called isotopic fractionation gat 2010 kendall and mcdonnell 2012 isotopic fractionation can be divided into equilibrium fractionation chemical thermodynamic fractionation and kinetic fractionation physical diffusion fractionation according to the processes that cause this change equilibrium fractionation occurs during chemical reactions at equilibrium exchange reactions the heavy isotopes are concentrated in substances with the highest bond force constants i e the preferential affinity of the lighter isotope for the vapor phase fry 2006 kinetic fractionation is caused by the differences in the diffusion rates of water molecules through the air i e preferential diffusion of the lighter isotope gat 2010 evaporation fractionation between the soil and free atmosphere includes both equilibrium and kinetic fractionations craig 1961 craig and gordon 1965 calculated the isotope evaporation flux at the liquid vapor interface based on these two types of fractionations this model considers three layers fig 1 a a liquid vapor interface where condensation evaporation and equilibrium fractionation occur b a diffusive sublayer where molecular diffusion dominates and thus kinetic fractionation occurs and c a turbulently mixed sublayer where mixing dominates and thus no fractionation occurs gat 2010 horita et al 2008 the water vapor and isotope evaporation fluxes between the water surface and the bottom of the free atmosphere are described by ohm s law or fick s law as an analog of the concentration gradient and transport resistance braud et al 2005a 2009 gat 2010 the evaporation flux for water vapor e kg m2 s is 4 e ρ s a t v t s h r s h a r a where r a s m is the sum of the resistances r m r t of water vapor to diffusive flow in the diffusive r m and turbulent r t sublayers ρ s a t v is the density of the saturated water vapor kg m3 see appendix a h r s is the relative humidity of the soil air phase at the surface and h a is the relative humidity of the atmosphere at the air temperature t a k normalized to the relative humidity of the atmosphere h a at the interface temperature t s k h r s and h a can be calculated as follows 5 h r s exp m g h s r u t s 6 h a h a ρ s a t v t a ρ s a t v t s where g is the gravitational acceleration lt 2 m is the molecular weight of water kg mol 0 018015 r u is the universal gas constant j mol k 8 314 h s is the matric potential at the soil surface l and t s and t a are the temperatures of the soil surface and atmosphere k respectively the corresponding evaporation flux for water isotopes e i kg m2 s is 7 e i c i s v c i a v r i c i s v c i a v α i k r a ρ s a t v t s α i k r a m i m w h r s α i r l h a r a e α i k m i m w h r s α i r l h a r a h r s h a the isotope ratio of the evaporation flux r e is 8 r e e i e h r s α i r l h a r a h r s h a α i k m i m w where c i s v and c i a v are the isotope concentrations of the surface water vapor and atmosphere kg m 3 r i s m is the sum of the resistances r m i r t i of water isotopes to diffusive flow in the diffusive r m i and turbulent r t i sublayers r v r l are the isotope ratios of the water vapor and remaining liquid water at the soil surface respectively r a is the isotope ratio of the atmosphere α i is the equilibrium fractionation factor and α i k is the kinetic fractionation factor note that α i is defined here as the ratio of vapor to liquid phase isotope ratios and it is thus smaller than 1 the equations used to compute α i for 2h and 18o isotopes as a function of temperature t k can be found in majoube 1971 and horita and wesolowski 1994 the equations by majoube 1971 were used in this study 9 α i o 18 o 16 exp 2 0667 10 3 0 4156 t 1 137 10 3 t 2 10 α i h 2 h 1 exp 52 612 10 3 76 248 t 24 844 10 3 t 2 the kinetic fractionation factor α i k is calculated as mathieu and bariac 1996 11 α i k d v d i v n k where n k is the kinetic fractionation coefficient and d v and d i v are the molecular diffusion coefficients of light and heavy water isotopes in free air l2t 1 respectively the diffusion ratio d v d i v can be calculated from graham s law of gas diffusion 12 d v d i v m i m w 0 029 m w m i 0 029 1 2 where the number 0 029 represents the mean molecular weight of air kg mol for 18o m w 0 018 kg mol and m i 0 020 kg mol and thus d v d i v 1 0324 while for 2h m w 0 018 kg mol and m i 0 019 kg mol and thus d v d i v 1 0166 which are the values used in our study in addition to these theoretical values much research has been conducted to measure these values readers are referred to horita et al 2008 for more details for example merlivat 1978 measured d v d i v 2h 1 0251 and d v d i v 18o 1 0285 the kinetic fractionation coefficient n k is associated with considerable uncertainty depending on evaporation conditions different equations have been used to calculate this value readers can refer to braud et al 2005b horita et al 2008 and quade et al 2018 for more details table s1 shows the equations used in this study the equilibrium fractionation enrichment ε and the kinetic fractionation enrichment ε k can be calculated as follows gat 2010 13 ε 1000 1 α i 14 ε k 1000 α i k 1 h r s h a this equation can be further simplified to get the widely used kinetic fractionation enrichment equation horita et al 2008 15 ε k 1000 d v d i v n k 1 h r s h a 1000 n k d v d i v 1 h r s h a according to gonfiantini 1986 the total fractionation factor α i t o t a l can be simplified and expressed as follows 16 α i t o t a l 1 α i ε k 1000 the isotope ratio of the evaporation flux r e is then calculated using its linear relationship with the isotope ratio of the liquid phase r l 17 r e r l α i t o t a l 4 numerical models the current isotope transport models can be generally divided into two groups the first group includes numerical models for evaporation fractionation without vapor flow these models can be used in relatively humid areas where the evaporation front is close to the ground surface and vapor flow in the soil profile can thus be neglected there is no fractionation within the soil due to the lack of the vapor phase or its consideration the second group includes numerical models for evaporation fractionation with vapor flow these models are intended for more arid zones where the evaporation front can occur deeper in the soil profile and vapor flow in the soil profile should thus be considered under such conditions both equilibrium and kinetic fractionations must be considered within the soils braud et al 2005a mathieu and bariac 1996 for the calculation of relevant water flow and heat transport parameters the readers are referred to the hydrus 1d manual šimůnek et al 2008 here we only focus on the calculation of isotope related parameters 4 1 evaporation fractionation in a system that neglects vapor flow when vapor flow can be neglected e g in humid zones the one dimensional uniform soil water movement in hydrus 1d can be described using the richards equation which assumes that the air phase plays a negligible role in water flow and water flow due to thermal gradients can be neglected šimůnek et al 2008 the governing equation for water flow then is 18 θ l t z k l h h z c o s γ s where θ l is the liquid volumetric water content l3l 3 t is time t h is the water pressure head l z is the spatial coordinate l positive upward γ is the angle between the flow direction and the vertical axis k l h is the isothermal hydraulic conductivity of the liquid phase lt 1 and s is the sink term l3l 3 t 1 since there is no fractionation within the soil the governing equation for the isotope transport is the same as the classical advection dispersion equation 19 θ l c i l t z d i l c i l z q l c i l z s c i l where c i l corresponds to isotope concentrations of soil water kg m 3 q l is the liquid water flux lt 1 and d i l is the effective dispersion coefficient of the isotope i in soil water l2t 1 evaporation fractionation which does not appear in eq 19 is considered using the upper boundary condition since eq 19 is a linear equation linear conversions of concentration do not affect the numerical results therefore not only the c notation but also the r or δ notations can be used to define isotope concentrations in the numerical model compared with traditional solute transport models which leave all solutes behind in the soil during evaporation the isotope transport models allow isotopes to leave with evaporation stumpp et al 2012 did not consider fractionation and assumed that the isotope concentration of the evaporation flux is the same as that of the soil water at the soil surface here the isotope ratio of the evaporation flux is instead evaluated using two methods the first method uses the craig gordon model eq 8 which requires the atmosphere s relative humidity temperature and isotope ratio as additional inputs the second approach follows the gonfiantini 1986 model eqs 16 and 17 which requires only the atmosphere s relative humidity as an additional input the isotope ratio of the evaporation flux is then automatically used in hydrus to calculate the isotope evaporation flux at the upper boundary corresponding to the water flux 4 2 evaporation fractionation in a system that considers vapor flow 4 2 1 water flow vapor flow in the soil profile should be considered in many arid zones nonisothermal liquid and vapor flow in hydrus 1d is described as follows saito et al 2006 zheng et al 2020 20 θ t h t z k l h h z c o s γ k l t t z k v h h z k v t t z s 21 q l k l h h z c o s γ k l t t z 22 q v k v h h z k v t t z where θ t is the total volumetric water content l3l 3 being the sum θ t θ l θ v of the volumetric liquid water content θ l and the volumetric water vapor content θ v l3l 3 both expressed in terms of equivalent water contents i e θ v ρ v θ s θ l ρ w where θ s is the saturated water content l3l 3 k l t is the thermal hydraulic conductivity of the liquid phase l2k 1t 1 k v h is the isothermal vapor hydraulic conductivity lt 1 k v t is the thermal vapor hydraulic conductivity l2k 1t 1 and q v is the vapor flux lt 1 the right hand side of eq 20 represents isothermal liquid flow gravitational liquid flow thermal liquid flow isothermal vapor flow and thermal vapor flow respectively since several terms are a function of temperature this equation should be solved simultaneously with the heat transport equation to account for temporal and spatial changes in soil temperature properly 4 2 2 heat transport the governing equation for heat transport is šimůnek et al 2008 23 c p θ l t t l 0 θ v t z λ θ l t z c w q l t z c v q v t z l 0 q v z c w s t where λ θ l is the coefficient of the apparent thermal conductivity of the soil mlt 3k 1 and c p θ l c w and c v are the volumetric heat capacities ml 1t 2k 1 of the porous medium the liquid phase and vapor phase respectively l 0 is the volumetric latent heat of vaporization of liquid water ml 1t 2 the right hand side of eq 23 represents the conduction of sensible heat the first term convection of sensible heat by liquid water the second term and water vapor the third term and convection of latent heat by vapor flow the fourth term and energy uptake by plant roots the fifth term respectively 4 2 3 isotope transport following the theory of the sispat isotope model braud et al 2005a the total isotope flux is the sum of isotope fluxes in the liquid phase q i l and the vapor phase q i v while both fluxes include convection and diffusion terms assuming instantaneous equilibrium between the liquid and vapor phases the liquid and vapor isotopic ratios can be related by an equilibrium fractionation factor mathieu and bariac 1996 melayah et al 1996a the governing equations for isotope transport then are 24 θ l n s o i l θ l β i c i l t z q i l q i v s c i l 25 θ l n s o i l θ l β i c i l t z c i l q l d i l c i l z β i q v c i l d i v β i c i l z s c i l 26 θ l n s o i l θ l β i c i l t z q l β i q v d i v β i z c i l d i l d i v β i c i l z s c i l that is 27 θ i c i l t z d i l v c i l z q i l v c i l s c i l 28 θ i θ l n s o i l θ l β i 29 q i l v q l β i q v d i v β i z 30 d i l v d i l d i v β i 31 c i v β i c i l 32 c i v m i m w r i v ρ v m i m w α i r i l ρ v α i ρ v ρ w c i l where n s o i l is the soil porosity l3l 3 β i is the ratio of the isotope concentration in the vapor phase and the isotope concentration in the liquid phase and c i l r i l and c i v r i v are isotope concentrations ratios in soil water vapor kg m 3 respectively the effective dispersion coefficients of the isotope i in soil water vapor d i l d i v l2t 1 are given as follows 33 d i l d i l o τ w θ l λ q l 34 d i v n s o i l θ l τ g d v d i v d v n k where τ w and τ g are tortuosity coefficients in the liquid and vapor phases respectively λ is dispersivity l and d i l o is the molecular diffusion coefficient of the isotope i in free water l2t 1 see appendix a 4 2 4 modifications on hydrus 1d this subsection lists all implemented changes into the standard hydrus 1d model to simulate the fate and transport of stable water isotopes to expand the capabilities of the hydrus 1d model and to be consistent with previous verification studies with other models e g the plausibility tests and comparisons with the analytical solution of barnes and alison 1984 a new upper boundary condition bc for water flow was implemented into the atmospheric boundary in hydrus 1d to simulate evaporation from bare soils actual evaporation e kg m2 s is calculated in this bc as a function of potential evaporation e p kg m2 s and the difference in relative humidities between the atmosphere and the soil surface similarly as done in other studies mathieu and bariac 1996 melayah et al 1996 braud et al 2005a this is a more convenient way of estimating actual evaporation at the upper boundary however if sufficient information is available it is better to use the surface energy balance to estimate actual evaporation 35 e e p h r s h a 1 h a the standard version of hydrus 1d can simulate the transport of volatile solutes by also considering solute transport via diffusion in the vapor phase the governing equations for volatile solute transport see eqs 6 55 and 6 56 in radcliffe and šimůnek 2018 are very similar to those for isotope transport the solute transport equation solved in hydrus 1d considers convective and diffusion dispersion transport in the liquid phase and diffusion transport in the vapor phase it does not consider convective transport in the vapor phase to consider the vapor convection term in solute transport two additional transport terms β i q v and d i v β i z in eq 29 had to be included in the governing solute transport equation of hydrus 1d hydrus 1d considers the relationship between the liquid and vapor solute concentration that assumes instantaneous linear distribution of a solute between the liquid and vapor phases henry s law 36 c i v k h c i l where k h is the henry coefficient which can be temperature dependent hydrus 1d assumes that temperature dependency can be expressed using the arrhenius equation to model the isotope transport using the current volatile solute boundary condition in hydrus one can replace the original henry coefficient k h with the ratio of the isotope concentration in the vapor phase and the isotope concentration in the liquid phase β i ρ v ρ w α i since the density of water vapor ρ v is a function of relative humidity of soil air phase i e the soil matric potential while equilibrium fractionation factor α i is a function of soil temperature the henry coefficient for isotope transport is in general a function of both depth z and temperature t the standard hydrus 1d uses the stagnant boundary layer bc for volatile solutes this bc considers the convective solute flux with evaporation and the diffusion solute flux by gaseous diffusion through a stagnant boundary layer on the soil surface jury et al 1983 this upper boundary condition was modified to implement the craig gordon model to account for both equilibrium and kinetic fractionations at the interface between the soil surface and the atmosphere eq 7 4 3 particle tracking module ptm to calculate soil water travel times the particle tracking algorithm from šimůnek 1991 was implemented into hydrus 1d the algorithm is based on the water balance calculations with the development of soil water profiles fully described by solving the richards equation fig 2 the first monitored particle below the soil surface is at depth z z 0 at time t t 0 the amount of water w 0 l is between this particle and the soil surface z 0 37 w 0 0 z 0 θ z t 0 d z during the time interval t 0 t 1 the amount of water n l passes through the soil surface 38 n t 0 t 1 e t i t d t where e t lt 1 is actual evaporation and i t lt 1 is actual infiltration from precipitation or irrigation during the same interval the layers in the root zone between the soil surface and the monitored particles are depleted by root water uptake s t l 39 s t t 0 t 1 0 z p t s z t d z d t where z p t is the particle depth l at time t and s z t is the sink extraction term l3l 3 t 1 at time t 1 there is thus between the soil surface and the monitored particle the following quantity of water w 1 l enriched by infiltration and reduced by evaporation and root water uptake 40 w 1 w 0 n s t the monitored particle is now located at a depth of z z 1 41 0 z 1 θ z t 1 d z 0 z 0 θ z t 0 d z t 0 t 1 e t i t d t t 0 t 1 0 z p t s z t d z d t by repeatedly solving this equation for the time sequence t 0 t 1 t n we obtain a sequence of depths z 0 z 1 z n i e we obtain the trajectory of the observed particle the calculation of the location of the second and further particles can be performed analogously now however the amount of water is balanced between the next two particles located at z a and z b between these particles the amount of water w 0 at time t 0 and the amount of water w 1 at time t 1 are 42 w 0 z a t 0 z b t 0 θ z t 0 d z 43 w 1 z a t 1 z b t 1 θ z t 1 d z during the time interval t 0 t 1 the amount of water between the two particles is depleted by the transpiration amount s t 44 s t t 0 t 1 z a t z b t s z t d z d t according to eq 40 the resulting equation now has the form 45 z a t 1 z b t 1 θ z t 1 d z z a t 0 z b t 0 θ z t 0 d z t 0 t 1 z a t z b t s z t d z d t the algorithm itself proceeds as follows from the particles known position at the beginning of the time interval the pre solved development of the moisture profile and the actual values of infiltration evaporation and transpiration the first monitored particle s new position is calculated using eq 41 new positions of all other particles are then calculated using eq 45 on the surface and at the bottom of the soil profile new particles may be created or may leave the soil profile depending on the moisture profile s actual development by calculating particles trajectories the movement of inert substances not subject to dispersion can be modeled the initial position of particles can be defined geometrically at specified depths or based on mass balance calculations by water storage similarly the release of new particles at the boundary can be defined chronologically at specified times or meteorologically rainfall events or depths the newly implemented particle tracking module requires two input parameters w stand and w prec the w stand parameter represents the water storage which separates neighboring particles in the soil profile at the beginning of the simulation therefore the particles are not geometrically evenly distributed when the soil profile s initial water content is not uniform the w prec parameter is the amount of water that passes through the soil surface before a new particle is released this means that particles are released at the soil surface only under wet conditions under dry conditions the surface flux is directed out of the soil profile and thus new particles will not be released 5 numerical implementations the same graphical user interface gui used in hydrus 1d is used to select and execute the model the hydrus software uses the finite element method for spatial discretization and the finite difference method for temporal discretization for consistency with the numerical model the sispat isotope model used for the verification the galerkin type finite element method fem and an implicit finite difference scheme were used to solve the richards and advection dispersion equations for water flow and isotope transport in this study however the upstream weighting fem for space weighting and the crank nicholson scheme for time weighting are also available at each time step the isotope transport is calculated after the water flow and heat transport equations have been solved first this provides the isotope transport routine with nodal values of soil temperature soil matric potential and water content at both old and new time levels to constitute the storage and transport coefficients for isotope transport in eqs 27 32 details about the numerical solutions of subsurface water flow and heat and solute transport can be found in the hydrus 1d manual šimůnek et al 2008 and braud 2000 to adequately capture the isotope concentration at the soil surface similar to the sispat isotope model the isotope transport equation s solution requires a fine resolution of the vertical unsaturated soil profile close to the soil surface three discretization schemes i e coarse medium and fine fig s1 were selected in the following verification examples to explore the impact of spatial discretization on the modeling results the first scheme uses 101 nodes uniformly distributed in the soil profile i e with a spatial step of 1 cm the second scheme uses 288 nodes with spatial steps gradually increasing from the bottom to the top being twice as large at the bottom 0 46 cm than the top 0 23 cm the third scheme follows the same spatial discretization as used by braud et al 2005a with 288 nodes fig s1 the spatial steps increase from 1 μm at the surface to 1 mm at a depth of about 1 cm and 5 mm at 5 cm they remain 5 mm between depths of about 5 95 cm and then gradually decrease to 1 mm at the bottom only the modeling results obtained using the fine spatial discretization are presented in the main text the results obtained using medium and coarse spatial discretizations can be found in the supplementary material while the initial time step of 25 s was used in this study time steps vary during the simulation they are automatically adjusted by the model depending on the number of iterations required by the water flow scheme to converge adaptive time discretization since the adaptive time discretization was used the temporal resolution is expected to have only a minor effect on the results and is not discussed in this study it must be emphasized that the accuracy of the numerical solution of isotope transport equations is very sensitive to those of water flow and heat transport equations the water flow iteration process continues until absolute changes in water contents pressure heads at all nodes in the unsaturated saturated zone between two successive iterations are less than prescribed tolerances we used 10 7 for both water content and pressure head m tolerances when heat transport is also considered water flow and heat transport equations are solved simultaneously since they affect each other two choices are provided in this case depending on whether the nodal water flux balance smaller than a prescribed tolerance 10 16 m s is used as a convergence criterion for water flow and heat transport the former iteration criterion without the nodal water flux balance is more numerical efficient and more applicable for systems that neglect vapor flow the latter convergence criterion with the nodal water flux balance is more accurate and recommended for a system that considers vapor flow note that iterations are not needed in standard hydrus 1d for solute transport when the governing solute transport equation is linear in this study the difference in the isotope flux at the upper boundary between two successive iterations smaller than a prescribed tolerance 10 16 kg m2 s was added as a convergence criterion for isotope transport the above iterative criteria are important prerequisites for obtaining accurate numerical solutions it is worth mentioning that the new hydrus isotope transport model is faster than the sispat isotope model when no heat transport is considered because fewer iterations are required by the water flow scheme to converge 6 model verification and evaluation 6 1 verification of the numerical solutions first we verified the numerical model that considers evaporation fractionation without vapor flow against the analytical solution of zimmermann et al 1967 for isothermal saturated soils under steady evaporation second the numerical model that considers evaporation fractionation with vapor flow was then verified against the analytical solution of barnes and allison 1984 for nonisothermal unsaturated soils under steady evaporation third mathieu and bariac 1996 designed six plausibility tests for isothermal unsaturated soils to check whether the model produces plausible results as equilibrium and kinetic fractionations were sequentially switched on in the model braud et al 2005a and haverd and cuntz 2010 used these tests to verify the sispat isotope and soil litter iso models respectively we repeated these tests with the hydrus 1d isotope model to see whether the new model produced expected shapes of isotope profiles we considered a 1 m deep soil profile of yolo light clay from philip 1957 in all verification examples basic soil hydraulic thermal and solute transport parameters are given in braud et al 2005a and shown in table 2 for consistency with previous studies we combined the van genuchten vg water retention model van genuchten 1980 with the burdine 1953 and brooks and corey bc hydraulic conductivity model brooks and corey 1964 46 θ l θ r θ s θ r 1 1 α h n m 47 m 1 2 n 48 k θ k s θ l θ r θ s θ r η where θ s and θ r are saturated and residual water content respectively m n and α are the shape parameters of the retention curve k s is the saturated hydraulic conductivity and η is the shape parameter of the conductivity curve equations from de vries 1963 and chung and horton 1987 already available in hydrus were used to describe the volumetric heat capacity and thermal conductivity respectively the tortuosity coefficients in the liquid and vapor phases τ w and τ g are evaluated in hydrus using the model of millington and quirk 1961 or moldrup et al 1997 in all verification examples τ w and τ g were set to 0 67 and λ was set to 0 to be consistent with previous studies to evaluate our model s accuracy this choice is justified because convective and hydrodynamic dispersion processes are negligible compared with the diffusion process under evaporation conditions auriault and adler 1995 6 1 1 comparison with the analytical solution of zimmermann et al 1967 zimmermann et al 1967 conducted experiments and provided an analytical solution for the isotope transport in a homogeneous saturated soil column with the initial isotope ratio isotopic composition r δ evaporating at a steady rate e s into the atmosphere of constant humidity h a air temperature t a and isotope ratio isotopic composition δ i a v r i a v under isothermal conditions at a soil temperature t z table 3 provides all relevant parameter values under steady state conditions the stable isotope profile can be explained by the balance between the upward convective flux evaporation and the downward diffusion flux of the isotope 49 e s r i l r d i l d r i l d z where r i l is the isotope ratio at depth z z is equal to zero at the soil surface and it is positive downwards the above equation can be solved to get 50 r i l r r 0 r exp z z l where r 0 is the isotope ratio at the soil surface and z l is the characteristic length given by 51 z l d i l e s if one reports the isotope ratio in eq 50 in the δ notation using eq 2 we can get 52 δ i l δ δ 0 δ exp z z l where δ 0 δ i l are isotopic compositions at the soil surface and at depth z respectively the isotopic composition at the soil surface δ 0 can be calculated using a variant of the craig gordon model as follows barnes and allison 1983 53 α i 1 δ 0 h a 1 δ i a v 1 h a α i k 1 δ the analytical solution for 18o is 54 δ o 18 31 9 e x p 16 949 z 1000 and for 2h is 55 δ h 2 67 e x p 16 667 z 1000 in the hydrus numerical simulation transport parameters were the same as those in the analytical solution both the upper and lower bcs were set to a constant water pressure head for water flow the soil water pressure head was assumed to be 1 cm at the surface and 109 15 cm at the bottom this bc allowed for a permanent water supply at the bottom of the soil column and kept the soil saturated while maintaining the steady evaporation rate e s both the upper and lower bcs were set to solute flux bcs for isotope transport the surface solute flux in this example referred to the evaporation flux for water isotopes e i calculated by the craig gordon model eq 7 the bottom isotope flux was calculated assuming that the isotope ratio isotopic composition of supply water was the same as the initial values r δ no heat transport was considered in this example fig 3 a and b shows an excellent agreement between the numerical and analytical solutions using a fine spatial discretization fig s2 shows a comparison between the analytical and numerical solutions results using different spatial discretizations the maximum differences between the analytical and numerical solutions in the 18o isotopic composition profiles were 0 21 coarse 0 20 medium and 0 20 fine the maximum differences between the analytical and numerical solutions in the 2h isotopic composition profiles were 0 46 coarse 0 43 medium and 0 43 fine we may conclude that the isotope transport module can produce in this example accurate isotope profiles using all considered spatial discretization schemes water that has experienced evaporation fractionation plots below the global local meteoric water line gmwl lmwl in dual isotope plots the occurrence of kinetic fractionation results in an evaporation line with a slope much smaller than those of gmwl lmwl sprenger et al 2016a the line conditioned excess lc excess is the difference between the δ h 2 from a water sample and a linear transformation of the δ h 2 from a given gmwl lmwl landwehr and coplen 2006 the more negative it is the stronger the kinetic fractionation is sprenger et al 2017 the dual isotope plot fig 3c has a slope of about 2 09 which is much smaller than that 8 20 of the global meteoric water line gmwl the lc excess profile calculated by eq a6 shows the opposite trend to the isotopic composition profiles and is negative in the entire soil profile fig 3d these results suggest that kinetic fractionation also occurs this is reasonable given the fact that kinetic fractionation factor α i k is not equal to one table 3 6 1 2 comparison with the analytical solution of barnes and allison 1984 barnes and allison 1984 proposed an analytical solution for evaporation from unsaturated soil under steady and nonisothermal conditions conditions were the same as for the steady state saturated case above except that the initial pressure head was set to 0 in the entire soil profile nonisothermal conditions were considered and evaporation occurred at a different rate table 4 gives the values of all parameters required in this problem under steady state conditions i e at 250 days of the simulation the stable isotope profile can be explained by the balance between the upward convective flux evaporation and the downward diffusion flux of the isotope both in the liquid and vapor phases 56 e s ρ w q l q v ρ w q l d v d h r ρ s a t v d z 57 e s r q i l q i v ρ w q l r i ρ w d i l d r i l d z d i v d h r ρ s a t v r i v d z where d v is the effective dispersion coefficient of the light isotope in soil water vapor e s is the steady state evaporation rate and h r is the relative humidity of the soil air phase at a certain depth h r can be calculated by eq 5 while the matric potential h s and temperature t s at the soil surface should be replaced by corresponding values at a certain soil depth if we expand the derivative form of the vapor flux d v d h r ρ s a t v d z in eq 56 we can easily find that it describes the sum of the isothermal k v h h z and nonisothermal k v t t z terms in eq 22 if we expand the derivative form of the isotope diffusion flux in the soil water vapor d i v d h r ρ s a t v r i v d z in eq 57 we can easily find out that it describes the sum of the convection β i q v c i l and diffusion d i v β i c i l z terms in eq 25 that is to say vapor convection within the soil is also a diffusive process haverd and cuntz 2010 if we define characteristic lengths z l and z v as follows 58 z l ρ w d l e s 59 z v d v ρ s a t v e s we can then get 60 ρ w q v e s d v d h r ρ s a t v d z e s h r z v d l n h r ρ s a t v d z 61 q i v e s d i v d h r ρ s a t v r i v d z e s h r z v r i l α i α i k d l n h r ρ s a t v α i r i l d z 62 q i l e s ρ w q l r i l ρ w d i l d r i l d z e s ρ w q l r i l e s z l σ i r i l d l n r i l d z combining these equations gives 63 h r z v r i l 1 α i α i k d ln h r ρ s a t v d z α i α i k d ln α i r i l d z r i l r z l σ i d r i l d z where d l is the effective dispersion coefficient of the light isotope in soil water and σ i is a constant depending on the isotope species see appendix a according to the relationship between r and δ values shown in eq 2 the analytical solution can be further simplified and given by the differential equation as follows 64 d δ i l d z z l h r z v 1 δ i l δ h r z v z l h r z v 1 α i k α i d d z ln h r ρ s a t v α i k α i this is a semi analytical solution it can only be solved once we prescribe the isotopic composition of soil water at the surface and solve the water flow and heat transport equations which will provide soil temperatures pressure heads and water contents in the hydrus numerical simulation transport parameters were the same as those in the analytical solution the constant pressure head equal to 0 was adopted in the numerical simulation as the lower bc for soil water flow the new water flow bc which calculates actual evaporation as a function of potential evaporation e p and the difference in humidities between the air and the soil surface eq 35 was used at the upper atmospheric boundary the solute flux was used as the lower bc for isotope transport automatically calculated from its isotopic composition equal to δ and the bottom water flux the stagnant bc for volatile solutes was used at the upper boundary for isotope transport the surface solute flux referred to the evaporation flux for water isotopes e i calculated by the craig gordon model eq 7 the temperature bc was used for heat transport at both boundaries fig 4 e and f shows an excellent agreement between the analytical and numerical solutions using a fine spatial discretization despite a slight overestimation of the peak isotopic composition by hydrus 1d fig s3 compares the analytical and numerical solutions obtained using different spatial discretizations the maximum differences at the evaporation front between the analytical and numerical solutions in the 18o isotopic composition profiles were 24 88 coarse discretization 3 74 medium and 0 88 fine the maximum differences between the analytical and numerical solutions in the 2h isotopic composition profiles were 34 68 coarse 8 40 medium and 3 67 fine this means that in this example the isotope transport module can produce relatively well isotope profiles as long as an appropriate spatial discretization is used the isotopic composition profiles have maximum values at a depth of 2 cm which corresponds with the water content matric potential temperature profiles inflection points fig 4a b and 4c this is also the evaporation front location where the upward soil water flux changes from the liquid to vapor flux fig 4d the dual isotope plots fig 4g have slopes of about 2 66 and 1 62 in the vapor and liquid dominant zones vdz ldz respectively which are far smaller than those of the gmwl the lc excess profile shows the opposite trend to the isotopic composition profiles and is negative in the entire soil profile fig 4h these results suggest that kinetic fractionation also occurs this is reasonable since the kinetic fractionation factor α i k is not equal to one when n k is one table 4 6 1 3 plausibility tests the soil was initially saturated and under hydrostatic conditions the soil water pressure head was equal to 0 01 m at the top and linearly increased to 0 99 m at the bottom the initial isotopic composition δ and soil temperature t z in the soil column were uniform i e the same at all depths water was evaporating from the soil column into an atmosphere with temperature t a relative humidity h a and isotopic composition δ i a v all relevant parameters are summarized in table 5 mathieu and bariac 1996 melayah et al 1996 braud et al 2005a in the hydrus numerical simulation zero water and isotope fluxes were adopted as the lower bcs the new water flow bc which calculates actual evaporation as a function of potential evaporation e p and the difference in humidities between the air and the soil surface eq 35 was used at the upper atmospheric boundary the stagnant bc for volatile solutes was used at the upper boundary for isotope transport the surface solute flux referred to the evaporation flux for water isotopes e i calculated by the craig gordon model eq 7 no heat transport was considered in this example the plausibility test conditions are listed in table 6 the impacts of four parameters on isotopic composition profiles were considered including the equilibrium fractionation factor α i the kinetic fractionation factor α i k which affects the molecular diffusion coefficient of the isotope i in free air d i v the molecular diffusion coefficient of the isotope i in free water d i l and the isotopic composition of atmospheric vapor δ i a v equations from majoube 1971 and mathieu and bariac 1996 were used to calculate the equilibrium α i and kinetic fractionation factor α i k respectively for tests in which they were not equal to 1 the molecular diffusion coefficients of the isotope i in free water air d i l o d i v were calculated by eq a3 a5 these values were then used to calculate the effective dispersion coefficients for the isotope i in soil water vapor d i l d i v based on eqs 33 and 34 the steady vertical isotopic composition profiles at the end of the 250 day simulation are shown in fig 5 test 1 equilibrium and kinetic fractionation factors α i α i k are set to one and molecular diffusion coefficient of the isotope i in free water d i l o is set to zero in other words evaporation fractionation and diffusion in the liquid phase are neglected the isotopic composition of the atmospheric water vapor is set equal to that of the initial soil water δ this results in uniform isotopic compositions in soil water throughout the soil profile equal to δ as expected test 2 test 2 is the same as test 1 except that the isotopic composition of the atmospheric water vapor is set to a low value δ i a v isotope diffusion in soil water vapor due to the concentration gradient between the free atmosphere and soil results in increased isotopic compositions of liquid and vapor phases within the soil as depth increases given the linear relationship between them eq 32 the isotopic composition of surface soil water is close to that of the atmospheric water vapor δ i a v and increases gradually with depth to its initial value δ test 3 test 3 is the same as test 1 except that equilibrium isotopic fractionation is turned on i e α i is not equal to one equilibrium fractionation between soil water and soil water vapor moves lighter water molecules from the liquid phase into the vapor phase which causes isotopic enrichment of the remaining soil water however this enrichment rate is different between regions above and below 5 cm i e the evaporation front as seen in fig 5a and b due to different vapor fluxes above 5 cm the vapor flux is approximately constant with depth and thus the effect of equilibrium fractionation does not differ too much with depth this results in a slow transition from the isotopic composition of soil water towards the surface value below 5 cm the isotopic composition of soil water increases rapidly towards the 5 cm depth due to the increased upward vapor flux fig 5b test 4 test 4 is the same as test 3 except that the isotopic composition of the atmospheric water vapor is reset to a low value δ i a v this shifts the isotopic composition of surface soil water close to δ i a v similarly as in test 2 this surface effect combined with increasing enrichment from the soil bottom towards the soil surface as discussed in test 3 leads to the simulated maximum of the isotopic composition profile test 5 test 5 is the same as test 4 except that diffusion in the liquid phase is turned on i e d i l 0 is not equal to zero since diffusion in the liquid phase causes spreading or dispersion of the solute front radcliffe and šimůnek 2018 this test produces a smaller peak of the isotopic composition profile test 6 test 6 is the same as test 5 except that the kinetic fractionation at the surface is turned on i e α i k is not equal to one and the molecular diffusion coefficient of the isotope i in free air is set to its real value d i v smaller than d v as seen in eq 12 the smaller molecular diffusion coefficient in free air results in increased kinetic fractionation by decreasing the removal of heavy isotopes through the vapor flux this increases isotopic enrichment in the remaining soil water leading to a larger peak of the isotopic composition profile than in test 5 as for dual isotope plots test 6 has slopes far smaller than that of the global meteoric water line gmwl in both liquid ldz and vapor dominant vdz zones fig s4d the line conditioned excess lc excess profile shows the opposite trends to the isotopic composition profiles and is always negative fig s5 these suggest that kinetic fractionation also occurs this is reasonable given the fact that the kinetic fractionation factor α i k is not equal to one in test 6 table 6 for tests 3 5 the dual isotope plots of both the ldz and vdz figs s4a s4b and s4c have slopes of about 6 55 7 80 which are much closer to that of the gmwl this is reasonable since the kinetic fractionation factor is equal to one in tests 3 5 table 6 and thus only equilibrium fractionation occurs these slopes are not exactly equal to that of gmwl especially for the vdz where the exchange with the atmosphere is more significant however the lc excess values in tests 3 5 are almost a constant low value about 10 throughout the soil profile compared to much more negative values in test 6 fig s5 this again verifies that only equilibrium fractionation occurs in tests 3 5 overall the slopes of dual isotope plots with kinetic fractionation are much smaller than those without consideration figs 3c and 4g and fig s4 the lc excesses at the surface layer about 0 50 cm are much more negative than in other depths figs 3d and 4h and fig s5 this indicates that the fractionation at the surface layer is more significant these conclusions are also consistent with those in sprenger et al 2016a therefore the isotope transport module is accurate also from the perspectives of dual isotope plots and lc excess values 6 2 evaluation against the experiment data 6 2 1 the transport of isotopes the dataset is from stumpp et al 2012 available https www pc progress com en default aspx h1d lib isotope the field experiment was conducted in a humid region located at the research area of the hblfa raumberg gumpenstein in gumpenstein austria with a mean annual temperature of 6 9 c and mean annual precipitation of 1035 mm the cylindrical lysimeter with a depth of 1 5 m and a surface area of 1 m2 was embedded in a rainfed agricultural field planted with winter wheat and fertilized with liquid cattle slurry the lysimeter 3 in stumpp et al 2012 the isotopic composition of precipitation and lysimeter seepage water were measured on the event and weekly intervals respectively from may 2002 to february 2007 1736 days in total the temporal distribution of precipitation evapotranspiration temperature air humidity during the simulation period are provided in fig s6 more details about other data collection and measurements can be found in stumpp et al 2012 the final optimized soil hydraulic and solute transport parameters reported in stumpp et al 2012 table s2 were used in the numerical simulations reported below the atmospheric with a surface layer and seepage face boundary conditions were used for water flow at the upper and lower boundaries respectively the temperature bc was used for heat transport at both boundaries the solute flux and zero concentration gradient bcs were used for isotope transport at the upper and lower boundaries respectively the isotope ratio of the evaporation flux was automatically used in hydrus to calculate the isotope evaporation flux at the upper boundary corresponding to the water flux to investigate the sensitivity of the simulation results to the upper boundary conditions for isotope transport the relevant parameters r e r a of different evaporation fractionation models stumpp et al 2012 the craig gordon model and the gonfiantini model were adjusted and implemented their impacts on the simulation results under different assumptions with and without fractionation were discussed since kinetic fractionation can be neglected in humid zones horita et al 2008 only equilibrium fractionation was considered in this example i e n k 0 fig 6 shows the comparison between 18o isotopic compositions of the lysimeter seepage water simulated by stumpp et al 2012 and using the gonfiantini and craig gordon models for a system that neglects vapor flow the nash sutcliffe efficiency nse and determination coefficient r 2 are shown in table 7 the water samples from the lysimeter seepage water plot on the lmwl fig 5 of stumpp et al 2012 indicating negligible fractionation therefore the measured data are closest to the simulations that do not consider fractionation as stumpp et al 2012 did in this model the isotope ratio of the evaporation flux r e is the same as that of the surface soil water r l i e r e r l in the gonfiantini model r e is α i times of r l as can be seen in fig 6 the measured values are close to the values simulated by the gonfiantini model in case of no fractionation i e α i 1 n k 0 r e r l which produces the same results as stumpp et al 2012 for most of the simulation period in the end during about 1150 1500 days the measured values are close to those simulated considering equilibrium fractionation i e α i α i n k 0 r e α i r l to obtain a better agreement between the simulation results and measurements using the craig gordon model the early atmospheric isotope ratio r a should correspond to eq s9 i e r a α i 1 h a r l h a while the late r a should correspond to eq s7 i e r a α i r l therefore in the craig gordon model method an approximate estimate of r a using eq s10 was used for the entire simulation period to calculate r e under equilibrium fractionation assumption i e α i α i n k 0 r a α i 1 h a h a α i r l 2 h a more details can be found in the supplementary material the craig gordon model has obtained relatively satisfactory simulation results nse 0 19 r 2 0 30 compared to the gonfiantini model nse 0 52 r 2 0 25 in the case of equilibrium fractionation the significant differences between the values simulated by the gonfiantini and craig gordon models emphasize the considerable impact of r a on the simulation results due to its effect on r e however the model performance is worse than when fractionation is neglected nse 0 24 r 2 0 37 this also indirectly validates the assumption of stumpp et al 2012 not to consider evaporation fractionation in their analysis of data from this humid zone however this does not rule out errors due to an inaccurate estimation of r a used in the simulation we note that the final optimized soil hydraulic and solute transport parameters reported in stumpp et al 2012 were used in the numerical simulations this parameter set was estimated based on the assumption that there was no fractionation which may not be optimal when fractionation is present this may also explain the best agreement of the stump et al 2012 simulation with the measurements however even under the no fractionation assumption this agreement is not very good likely due to some uncontrollable factors in the field experiments the isotopic compositions and overall temporal variation trends simulated using the gonfiantini or craig godron models considering fractionation are consistent with measured data and the stumpp et al 2012 simulation without considering fractionation this is because evaporation fractionation will not change isotopic composition trends when evaporation is much smaller than the sum of precipitation and soil water storage and the equilibrium fractionation factor is close to 1 however the selection of the atmospheric isotope ratio r a can affect the fluctuation amplitude of the isotope time series by affecting r e the isotopic composition of discharge simulated by all models remains the same during the first 150 days because only water initially in the profile and thus not affected by the upper bc treatment reaches the bottom during this time water infiltrating at the beginning of the simulation starts arriving at the bottom after about 150 days when isotopic compositions simulated by different models with different assumptions start deviating from this point forward differences in simulated discharge isotopic compositions reflect different treatments of the upper bc 6 2 2 particle tracking the input parameters w stand and w prec discussed in section 4 3 of the particle tracking module ptm were set equal to 2 and 10 cm respectively fig 7 shows the spatial temporal distribution of particles during the 5 year simulation there are 48 particles in total among which 18 particles p1 p18 were initially in the soil profile while the next 26 particles p19 p44 were released at the soil surface passed through the lysimeter and left at the bottom finally the last 4 particles p45 p48 were released at the soil surface and remained in the soil profile at the end of the simulation the particle trajectories suddenly drop during periods with many rainfall events and slowly decrease or even rise during periods with limited rainfall fig 7 particles move downward faster during wet seasons and slow down during dry seasons particles move down sharply after heavy rainfalls reflecting piston flow s typical characteristics particles released right before the wet season move down faster than those released right before the dry season the transit times of these particles and corresponding velocities were calculated given in table s3 and shown in fig 7c the mean recharge transit time and velocity are 276 1 days and 6 0 mm day respectively these values are slightly different from those calculated by stumpp et al 2012 using the peak displacement method the mean recharge transit time 250 days and velocity 6 0 mm day of soil water were estimated by stumpp et al 2012 by comparing the convective shift in the isotope peaks of the input precipitation during 2005 2006 and output lysimeter discharge and considering dispersion effects this difference may also be due to different rainfall events selected for these calculations stumpp et al 2012 selected precipitation events from 2005 to 2006 since only during this period there was pronounced and distinct correspondence between the isotopic peaks in precipitation and lysimeter discharge since particles move faster during the period with many precipitation events the peak displacement method is likely to overestimate the flow velocity compared to particle tracking overall the two approaches results are similar which shows the particle tracking model s applicability however the peak displacement method is not applicable when there are no pronounced peaks or a distinct correspondence between the input and output peaks the particle tracking module can be used under such circumstances and overcome this shortcoming of the peak displacement method thus expanding the possibility of calculating transit times to verify the new particle tracking module we conducted simple mass balance calculations based on the results of the numerical solution of the richards equation in hydrus 1d the amount of water in the soil profile when the particle leaves the transport domain w t final should be equal to the amount of water applied at the soil surface infiltration since its release reduced by evaporation and root water uptake from the soil between the particle and the soil surface 65 w t f i n a l t i n i t t f i n a l i t e t 0 z p t s z t d z d t where t init and t final are times when the particle is released at the soil surface and when it leaves at the soil profile bottom respectively the mass balance calculations carried out according to 65 are given in table s4 indicating an almost perfect match and validating the particle tracking module 6 3 discussion and future work modeling water flow and solute transport in the critical zone requires an accurate estimation of soil hydraulic and solute transport parameters combining different types of observed data to calibrate water flow and solute transport models has been found to improve model parameterization for example sprenger et al 2015 demonstrated that a combination of stable isotope profiles and soil moisture time series allowed for a better model calibration for solute transport water flow and root water uptake parameters groh et al 2018 determined the soil hydraulic parameters and the longitudinal dispersivity for multiple lysimeters using two step and bi objective optimization strategies they concluded that the bi objective strategy combining water content matric potential and tracer data was the best parameter estimation strategy mattei et al 2020 showed that it is possible to use only water content and stable isotope profiles measured at one time to accurately calibrate the model for groundwater recharge estimation however using stable isotope data at different soil depths at different times can improve the model calibration mattei et al 2020 the new isotope transport module in hydrus 1d can simulate continuous space time dynamics of stable water isotope concentrations of soil water whether the impact of consideration of evaporation fractionation will propagate into the inversion of soil hydraulic and solute transport parameters is unknown future work will include analyzing field datasets collected in arid regions or laboratory experiments of braud et al 2009 where evaporation fractionation plays a vital role and depth dependent observations are available sensitivity analyses and parameter inversions will be conducted to evaluate the new isotope transport model further the assumption of well mixed water in the soil is in contrast with the recent two water world tww hypothesis that water in the soil should be split into two pools that are isotopically different berry et al 2018 brooks et al 2010 mcdonnell 2014 the first pool the mobile soil water pool often replenishes groundwater and has isotopic composition close to that of the infiltrating water the second pool the immobile soil water pool is supposed to be composed of tightly bound water enriched by evaporation that resides in the soil s capillary space some of which can be used by plants many isotopic measurements support the hypothesis of the widespread existence of tww and demonstrate that some sampling methods e g suction lysimeters are likely to obtain the mobile water while others e g centrifugation cryogenic or toluene distillation are prone to sample all soil water e g figueroa johnson et al 2007 geris et al 2015 goldsmith et al 2012 knighton et al 2019 oerter and bowen 2017 zhao et al 2016 the tww hypothesis was formulated based on two assumptions vargas et al 2017 the first is that there is no mass exchange between the mobile and immobile waters the second is that plant water uptake does not discriminate against 2h or 18o and thus does not affect the isotopic composition of soil water however recent studies have shown that mass exchange between mobile and immobile waters e g oshun et al 2016 vargas et al 2017 and isotopic fractionation may occur during plant water uptake e g barbeta et al 2019 poca et al 2019 on the other hand there is accumulating evidence showing that the equilibrium isotope fractionation between pore water and water vapor within the soil is significantly different from that between liquid and water vapor for free water surface due to complex hydrophilic interactions between soil pore surface and water molecules e g chen et al 2016 gaj and mcdonnell 2019 lin and horita 2016 lin et al 2018 oerter et al 2014 however how waters of different mobility alter the isotopic composition of soil water is little understood and seldom accounted for in isotope transport modeling sprenger et al 2018 the standard version of hydrus 1d can consider a series of physical nonequilibrium flow and transport models e g dual porosity and dual permeability and the same conceptualization can be applied to simulate isotope transport these nonequilibrium flow and transport models will be used in our future studies to evaluate the impacts of physical nonequilibrium on isotope transport modeling and transit time calculations for tww systems 7 summary and conclusions this study presents a model which can simultaneously solve the coupled equations describing the movement of water heat and stable isotopes it is based on the hydrus 1d model to which the isotope transport and particle tracking modules were added the comparisons with analytical solutions plausibility tests under saturated unsaturated and isothermal nonisothermal conditions and field validation demonstrate the model s accuracy and robustness transit times calculated by the particle tracking module ptm are similar to those evaluated by the peak displacement method which validates the use of the water flow based ptm as an alternative tool to isotope transport based methods as compared with existing isotope models our approach enables many thousands of current hydrus users to efficiently operate the new model while using various advanced hydrus software features including flexible dynamic boundary conditions equilibrium and nonequilibrium water flow parameter optimization routines and the well designed user friendly gui šimůnek et al 2016 while also providing higher computational efficiency for example the sispat model always calculates both water flow and heat transport even when the soil system is isothermal our model simulates only water flow for isothermal systems improving numerical efficiency the new particle tracking module provides the hydrus 1d users with an additional tool for assessing transit times the developed model represents a comprehensive tool to numerically investigate many important research problems involving isotope transport processes and establishes a more solid foundation for applying stable isotope tracing in the critical zone declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this research was financially supported by the multistate w4188 program we appreciate editors and reviewers for their constructive comments on this manuscript appendix b supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix b supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105118 appendix a the density of the saturated water vapor ρ s a t v kg m3 depending on temperature t k is calculated as follows a1 ρ s a t v 10 3 exp 31 3716 6014 79 t 7 92495 10 3 t t the density of the water vapor ρ v is the product of the density of the saturated water vapor ρ s a t v kg m3 and relative humidity h r a2 ρ v h r ρ s a t v the molecular diffusion coefficient of the isotope i in free water d i l o l2t 1 is expressed as a function of temperature t k a3 d i l o σ i 10 9 exp 535400 t 2 1393 3 t 2 1876 where σ i is a constant depending on the isotope species 0 98331 for hdo and 0 96691 for h2 18o the molecular diffusion of the isotope i in free air d i v l2t 1 is expressed as a function of temperature t k a4 d i v d v b i a5 d v 2 12 10 5 t 273 16 2 where b i is the ratio of the molecular diffusion coefficients of light and heavy water isotopes in free air 1 0166 for hdo and 1 0324 for h2 18o as discussed in eq 12 for the verification examples in this study the gmwl defined by rozanski et al 1993 was used the lc excess was calculated as follows a6 l c e x c e s s δ h 2 8 2 δ o 18 11 27 1 15 
25799,to improve hspf hydrological and water quality simulation a new svm surrogate modeling method was investigated and a comprehensive framework for hydrological parameter sensitivity optimization and uncertainty analysis was established for qinglong river watershed hebei province china svm surrogate model was set up based on pairs of parameter sets and nash sutcliffe efficiency coefficients it was concluded that 1 svm surrogate model performs well in both reliability and efficiency 2 sensitivity of eleven parameters was evaluated agwrc is extremely sensitive parameters agwetp deepfr basetp are sensitive parameters and uzsn lzsn lzetp intfw cepsc irc infilt are not influential parameters 3 recommended parameter intervals were lzsn 2 0 5 82 infilt 0 21 0 47 agwrc 0 85 0 87 deepfr 0 001 0 17 basetp 0 001 0 09 agwetp 0 0011 0 13 cepsc 0 01 0 29 uzsn 0 05 1 20 irc 0 3 0 62 lzetp 0 34 0 85 intfw 1 0 5 77 and the optima were obtained respectively 4 posterior distributions of eleven parameters were obtained graphical abstract image 1 keywords hydrological simulation program fortran hspf surrogate model support vector machine svm parameter sensitivity parameter optimization parameter uncertainty 1 introduction hydrological model plays an important role in exploring the process of the natural water cycle goncu and albek 2010 among numerous computer models applying to predict the behavior of environmental systems johnson et al 2003 nasr et al 2007 hydrological simulation program fortran hspf is commonly used for the continuous dynamic steady state hydrological and water quality modeling due to the powerful water quality modeling ability hspf performs well in the simulation of point source and non point source pollutants total maximum daily load tmdl development in water bodies ahmadisharaf and benham 2020 assessment of land use and climate change impacts on watersheds kim and chung 2014 liu and tong 2011 thus hspf is an essential tool of water resource distribution and watershed management adapting to different time and space scales of mixed agricultural and urban watersheds xu et al 2007 the current challenge for hspf modeling mainly arose from parameter complexity or uncertainty fonseca et al 2014 which leads to studies on parameter sensitivity analysis parameter optimization and parameter uncertainty evaluation beven and freer 2001 gallagher and doherty 2007 among all the hspf parameters the hydrological parameters are the most crucial because sediment and water quality simulation are based on the hydrological simulation in addition water quality indicators such as bod nh3 n tn and tp are monitored on a monthly basis so their actual data are rougher than those of the hydrological parameters hence the emphasis on hspf parameters studies was placed on the hydrological parameters van griensven et al 2006 zheng and keller 2007 parameter sensitivity analysis aims to quantitatively identify the critical parameters that affect the simulation accuracy provide guidance for calibration optimization monitoring and testing of hydrological and water quality parameters fonseca et al 2014 tang et al 2007 it consists of single factor sensitivity analysis and global sensitivity analysis the single factor sensitivity analysis only examines the effect of changes in a single parameter on the model results matthews et al 2007 and it cannot adequately describe the spatial distribution of model parameters and the interaction between parameters so global sensitivity analysis is proposed ciriello et al 2019 conventional global analysis methods include morris screening method campolongo et al 2007 and sobol s method tang et al 2007 since they are the multiple linear regression models based on mathematical statistic there are more deviations when the relationship between model parameters is nonlinear gaussian radial basis function rbf meta model can establish a multi parameter nonlinear regression model according to several samples shang et al 2020 however it is challenging to get the analytical expression of this model and the weight of each parameter parameter optimization aims to tune the optimal values or intervals of the parameters because of the equifinality phenomenon the goal of parameter optimization shifts from the optima to the optimal interval beven and freer 2001 whelan et al 2019 the parameter optimization method can be divided into two categories single objective and multi objective optimization methods sce ua method zhang and shao 2018 rosenbrock method kang et al 2011 artificial bee colony algorithm kang et al 2011 response surface method liu et al 2018 these are widely used single objective methods there are also some multi objective optimization methods the most representative one is neural network however it is hard to avoid the local minimum issues luo et al 2021 and curse of dimensionality jiang and chen 2020 parameter uncertainty evaluation aims to calculate the uncertainties or risks of parameters for risk based decision making ahmadisharaf and benham 2020 it can be divided into two categories generalized likelihood uncertainty estimation glue method beven and binley 1992 2014 and bayesian method kennedy and o hagan 2001 because of its simplicity glue method is more widely used in both sensitivity analysis and uncertainty analysis of model parameters xie et al 2019 however it is low efficient to conduct a lot of simulations for glue analysis based on the hspf platform because each simulation is time consuming montanari 2005 while parameter sensitivity analysis optimization and uncertainty evaluation are the systematic engineering for hspf modeling and simulation a comprehensive framework is required for the hydrological and environmental modelers to complete the modeling task kim and chung 2014 zhang and shao 2018 however given the complexity of the hspf model it is difficult to directly integrate the above three parts into a comprehensive framework in practice tzoraki and nikolaidis 2007 moreover it is a tough task to obtain the optimal values or intervals of parameters depending on the numerical solution of hspf model itself diaz ramirez et al 2012 therefore as an alternative a new surrogate model for hspf prototype was developed here which can focus on some parts of the original hspf model establish a simplified model between hydrological parameters of hspf and its simulation results make the research more efficient support vector machine svm is a machine learning technique which is good at solving non linear problems and is characterized by its free form strong robustness and high precision zhou et al 2021 therefore svm was used to establish the surrogate model and analyze the parameter sensitivity in this case we combined the svm surrogate model with a large sample monte carlo simulation the weight of each parameter was obtained and the sensitivity of the parameter was estimated in an efficient way and then a comprehensive framework for hspf parameter sensitivity analysis optimization and uncertainty evaluation was applied to the case study of qinglong river watershed multiple quadratic regression response surface was adopted to compare the results and analyze the reliability of svm surrogate model the rest of this paper is organized as follows section 1 introduces the study area and the model data used section 2 elaborates the methodology of the research section 2 1 describes the latin hypercube sampling lhs which is used for generating the parameter sets inputted to the hspf and the samples for the establishment of the svm surrogate model in section 2 2 and section 2 3 the svm surrogate model is used as the interpolation function for monte carlo simulation and then the distribution of each parameter is estimated so as the sensitivity optimal interval and satisfactory value of each parameter section 3 introduces the case study in qinglong river watershed results of the svm surrogate model are reported and discussed in section 3 1 and the analysis of parameter sensitivity is conducted in section 3 2 while section 3 3 and section 3 4 optimized the selected parameters and analyzed their uncertainty combining svm surrogate model with glue algorithm these results were compared with those produced by a multiple quadratic regression model in section 3 5 in order to verify the effectiveness of the satisfactory parameters simulation results of bod nh3 n tn and tp are shown in section 3 6 finally the main results are summarized in section 4 2 study area and data 2 1 study area qinglong river watershed is located in hebei and liaoning provinces between 118 69 and 119 63 east longitude and 40 103 41 165 north latitude northern china it covers an area of 6340 km2 with an altitude of 350 650 m whose vegetation coverage exceeds 80 as shown in fig 1 this watershed has an average annual rainfall of 500 700 mm in the east asian monsoon climate zone with large inter annual changes and uneven distribution during the year located at the lower reach of the watershed taolinkou reservoir is the most critical water sources of qinhuangdao city hebei province agricultural non point source pollution rural wastewater and garbage pollution and industrial wastewater are the primary pollution sources of qinglong river and taolinkou reservoir liu et al 2018 the hydrological and water quality simulation of qinglong river and taolinkou reservoir is of great significance for calculation of the environmental capacity and assessment of the potential risks arising from climate change and human activities 2 2 model and data hspf model is a semi distributed hydrological and water quality model developed by the us environmental protection agency usepa embedded in the basins system platform it integrates multiple hydrological and water quality modules and analysis tools nasr et al 2007 different types of data are required for hspf modeling as shown in table 1 particularly the observed data onto streamflow and pollutant concentration are used for model calibration and verification chung and lee 2009 2 3 hydrological parameters for study considering results of literature review albek et al 2004 chung and lee 2009 xu et al 2007 and hspf pest automatic calibration kim et al 2007 eleven hydrological parameters were selected for further study usepa 2000 as shown in table 2 3 methodology a comprehensive method was proposed to investigate the hspf model efficiently and precisely latin hypercube sampling and support vector machine were used to establish the surrogate model of hspf model and multiple quadratic regression model was used for comparison and demonstration combined with monte carlo simulation and glue algorithm parameter sensitivity analysis parameter optimization and uncertainty analysis were carried out based on svm surrogate platform the roadmap of the study is shown as fig 2 3 1 svm surrogate modeling method hspf model has been successfully applied in many watersheds ahmadisharaf and benham 2020 liu and tong 2011 but there are still some challenges in the parameter investigation one of them is the time consuming model simulation which makes it difficult to obtain a large number of samples directly ciriello et al 2019 moreover hspf model has many hydrological and water quality parameters and the parameters have interactive relations lee et al 2020 liu et al 2019 pang et al 2018 according to the fact that it is hard to intuitively summarize the functional expression between parameters and the simulation results of the model it is challenging to select calibrate and optimize the parameters matthews et al 2007 on the other hand svm shows many unique advantages in solving small sample regression nonlinear and high dimensional pattern recognition problems wu et al 2008 firstly the svm model is established by support vectors instead of all samples result in good performance at few shot learning which means it has good classification regression and generalization abilities for fewer samples wang et al 2020 secondly numerous kernel functions in svm can be used flexibly to solve various regression problems especially nonlinear regression the appropriate svm kernel function can map the complex regression functions in low dimensional space to high dimensional space so it is very effective for solving the classification and regression problems of high dimensional features talukdar et al 2020 in addition the structural characteristics of svm ensure that the svm model has a unique minimum value in the process of optimization solution chang and lin 2011 wu et al 2008 to cope with these hspf modeling challenges we proposed the svm surrogate model the main idea of surrogate modeling is to describe the physical process of the original model approximately through reasonable experimental design and fitting functions razavi et al 2012 considering the small sample regression and generalization capability of svm the svm model is suitable for hspf surrogate modeling the process of surrogate modeling based on svm is shown as follows 1 hspf parameter set sampling based on latin hypercube sampling lhs as a multidimensional stratified sampling method lhs was initially developed to reduce the sample size and the computational cost required in the monte carlo simulation by dividing each dimension variable into non overlapping n intervals with the same probability and selecting a point in each interval of each dimension randomly lhs achieves the full interval coverage with a smaller sample standard deviation than monte carlo simulation abyani and bahaari 2020 lhs can achieve higher sampling accuracy using smaller sampling scale so it is widely used to select samples for surrogate modeling steps of lhs algorithm are as follows 1 divide interval of each parameter into n equal parts randomly generate a value within each cell i n i 1 n according to the prior distribution of each parameter 2 generate random values only once for each parameter and for each interval miao et al 2019 the parameter sets generated by lhs were divided into two groups one was used as a training set to develop the svm surrogate model and the other was used as a test set to verify the reliability of the surrogate model 2 svm surrogate modeling method for hspf svm model is essentially a response surface model it is used for dealing with the conversion relationship between input and output of complex systems for svm modeling the kernel function is used to develop the relationships between parameter sets and the likelihood function values hence selection of kernel function is crucial the commonly used svm kernel functions include linear kernel function polynomial kernel function gaussian radial basis kernel function gaussian rbf and sigmoid kernel function chang and lin 2011 for high dimensional nonlinear regression problem gaussian rbf is commonly used gaussian rbf can approximate arbitrary nonlinear function has good generalization ability and fast learning convergence speed and has been successfully applied in nonlinear function approximation time series analysis data classification system modeling and so on jain 2020 the svm response surface can be established on the basis of the training set and selection of kernel function therefore it served as the surrogate model of the original hspf model because of its simplicity and reliability the parameter sensitivity optimization and uncertainty evaluation can be realized in higher cost effective manner 3 2 parameter sensitivity analysis based on svm interpolation and regression on the basis of svm surrogate model it is convenient to conduct monte carlo simulations then a number of parameter sets and nses can be obtained for the analysis of parameter sensitivity gaussian rbf does not have an intuitive function expression to display the weight of each parameter in the multiple regression model thereby the svm surrogate model is used as the interpolation function for monte carlo simulation this method dramatically reduces the calculation consumption caused by hspf simulation and parameter sensitivity analysis combines the advantages of monte carlo simulation and regression based simulation sensitivity analysis steps are as follows 1 input a large number of random parameter groups into the svm surrogate model to obtain their simulated likelihood values 2 draw a scatterplot to observe the response of each parameter to the likelihood values in the distribution interval 3 quantify the sensitivity of each parameter to the likelihood values via the gradient obtained by the linear fitting of the parameter and the likelihood value 3 3 parameter optimization and uncertainty evaluation based on glue algorithm generalized likelihood uncertainty estimation provides a reliable algorithm for quantifying the uncertainty in complex models the basic idea of glue is equifinality that is there does not exist an optimal parameter set but multiple parameter sets can achieve the same satisfactory result ahmadisharaf and benham 2020 by drawing the posterior distribution histogram of each parameter and their simulated likelihood values satisfying the threshold condition value the optimal interval of the parameters can be obtained thus glue based on svm surrogate model was used to calibrate parameters and analyze parameter uncertainty steps of the glue method jia and culver 2008 based on svm surrogate model are as follows 1 selection of appropriate likelihood function and likelihood threshold normally nash sutcliffe efficiency nse water balance coefficient relative error re and so on can be used as the likelihood functions nse can reflect the degree of agreement between the simulated runoff and observed runoff is generally used to verify the quality of hydrological model simulation results its range is 1 the overall results can be judged as reliable when nse is greater than 0 nse is close to 1 indicating good modeling quality and high reliability moriasi et al 2015 the selection of threshold is subjective based on the requirement of model accuracy the likelihood threshold can be flexibly selected according to relevant experience 2 determination of model parameter range and selection of parameter prior distribution the physical range of the model parameters has been given in the hspf model as shown in table 2 due to the lack of prior information the prior distribution cannot be accurately described so the uniform distribution is usually used as the prior distribution 3 according to the prior distribution of parameters the svm surrogate model is used to randomly sample a large number of parameter groups within the parameter range and calculate their likelihood values e g nse monte carlo sampling is used as the sampling method during uniform distribution 4 if the likelihood value is above the likelihood threshold its corresponding parameter set would be regarded as a feasible parameter set plot the frequency histogram of all feasible parameter groups then the posterior distribution of each parameter can be obtained analyze the posterior distribution and the dense distribution interval is taken as the optimization interval 5 sort the likelihood values from large to small the parameter sets with the largest likelihood value are input into the hspf model for testing and the best set of simulation results is selected as the recommended value of parameters 6 calculate the uncertainty range of model prediction under different confidence levels e g 90 or 95 level when a 90 confidence interval is obtained the lower limit of the confidence interval is 5 quantile value after likelihood ranking and the upper limit is 95 quantile value the uncertainty of the model simulation is obtained from the proportion of the observed values included in the confidence interval 4 results and discussion 4 1 results of svm surrogate modeling first of all the hspf model of qinglong river watershed was established and run applying the pest auto calibrated parameters optimized by liu et al 2018 and then the simulated data and the observed runoff data of taolinkou reservoir section were used for calculation of nse secondly lhs algorithm was carried out for the eleven selected hydrological parameters on the matlab platform e g lhsdesign function assuming all parameters are uniformly distributed 120 sampling results of eleven parameter sets were obtained and used for the subsequent 120 hspf simulations respectively thus 120 nse values were also obtained for forty simulations these data sets were then used to develop the surrogate model of hspf in this paper libsvm toolbox developed by professor lin zhiren was used for svm regression chang and lin 2011 and the gaussian rbf was selected to establish the svm surrogate model whose hyperparameters was trained by cross validation program finally the svm surrogate model was built with these 120 parameter sets and their nse values the first 100 groups of simulated data were used as the training set and the rest twenty groups were used as the test set after 806 iterations mean square error mse of the model training set was 0 0139 the correlation coefficient r2 was 88 51 and mse of the test set was 0 0073 r2 was 95 18 it reflected that svm surrogate model largely increases computation efficiency while keeping good model performance thus can be used as an alternative to hspf the hyperparameters for the training of svm was selected in fig 3 c was 6 9644 gamma was 0 0474 the regressed nse in train set and test set generated by svm surrogate model was compared with nse simulated by the hspf model as shown in fig 4 4 2 results of sensitivity analysis based on svm surrogate model 30 000 parameter groups were input into the svm surrogate model and the corresponding nses were obtained as shown in fig 4 these pairs of parameter set and nse were used for the following sensitivity analysis to demonstrate the validity of the svm surrogate model a total of 120 parameter sets and nse derived from hspf model were compared with those generated from the svm surrogate model as shown in fig 5 and table 3 it can be found that the trend of data generated by svm surrogate model was similar to that of the hspf model according to fig 5 the gradient of data generated by the hspf model and the svm surrogate model of each parameter were obtained respectively as shown in table 3 it can be seen that the data simulated by svm surrogate model forms visible boundaries the proportion of the hspf simulation value beyond the boundaries was considered as an error all values are 3 33 and below which can be considered credible for most of the model parameters the gradient of the samples generated by svm surrogate model is consistent with those of the hspf model it reflected that the results of sensitivity analysis based on svm surrogate model were reliable comparing the nse simulated by hspf and them generated by svm and their respective linear trend lines was drawn it was found that the gradients of these two data sets were opposite when it comes to infilt agwetp and irc the possible reasons were as follows firstly the insensitivity of the parameter itself leads to the svm simulation error secondly the accuracies of the linear trend line in excel were doubtful r2 of infilt agwetp and irc are 0 029 0 234 and 0 012 respectively indicating poor fitting accuracy however considering the distribution trend and error of most parameters the svm surrogate model can be used as a substitute for the hspf model for sensitivity analysis in view of the complexity of hydrological model parameters the uniform distribution is generally chosen as the prior distribution whose gradient is 0 the gradient of some parameters deviated significantly from the prior uniform distribution indicating high sensitivity the absolute value of gradient was taken as the judgment standard of parameter sensitivity and then the 11 parameters were divided into three categories the extremely sensitive parameters gradient 1 were agwrc the sensitive parameters 0 1 gradient 1 were deepfr agwetp basetp and the insensitive parameters 0 gradient 0 1 were lzsn infilt cepsc uzsn irc lzetp intfw 4 3 results of parameter optimization based on svm surrogate model 30 000 parameter groups and their regression nse generated by svm surrogate model in section 3 2 was used in glue algorithm glue method generated posterior probability distribution of parameters which was used for the optimization of parameter intervals here nse was used as the likelihood function of the glue method through the experience of hydrology calibration and related research moriasi et al 2015 the appropriate threshold of likelihood value was selected as 0 6 a total of 3012 sample points accounting for 10 04 of the total sample the posterior distribution of each parameter was analyzed by drawing histogram in fig 6 a k and the cumulative frequency of each parameter was combined in fig 6 l fig 6 shows when the regressed nse value generated by the svm surrogate model is greater than 0 6 the distribution frequency of each parameter in its value range the dense interval with high parameter distribution frequency was regarded as the optimal ranges for each parameter which is higher than the average distribution frequency and the optimum of each parameter was also obtained by sorting the likelihood values from large to small details are shown in table 4 the model performance can be evaluated as satisfactory when its nse is greater than 0 45 moriasi et al 2015 the optimized parameter set was input into the hspf for testing and the nse value from 2011 to 2013 was 0 78 which proved that the optimized parameter set was satisfactory for hydrological simulation 4 4 results of parameter uncertainty analysis based on the svm surrogate model in terms of uncertainty analysis the threshold of likelihood value was adjusted to 0 at 90 confidence level 5 quantile and 95 quantile of simulated runoff in qinglong river watershed was selected compared with the measured runoff the uncertainty of svm surrogate model was analyzed as shown in fig 7 it can be seen from fig 7 that the uncertainty of the model is significantly positively related to the amount of rainfall the larger the runoff the stronger the uncertainty on the daily runoff scale 10 6 of the measured runoff is included in the uncertainty range and on the monthly runoff scale 19 4 of the measured value is within the uncertainty range we analyzed that a large part of the reason is that the hspf did not simulate the base flow accurately enough while it was relatively sensitive to heavy rainfall and high flow both the trend of the runoff hydrograph and the nse value showed that the results obtained by the svm surrogate model can reflect the runoff variation law of the qinglong river watershed satisfactorily 4 5 comparison with multiple quadratic regression response surface model as the most commonly used regression response method multiple quadratic regression model was used to demonstrate the reliability of svm surrogate model design expert10 software was applied to perform sensitivity analysis and parameter optimization for the above eleven hydrological parameters 188 parameter sets were used for calculation f value and p value are two commonly used statistical parameters in an analysis of variance the f value is the ratio of the variances of two samples at a given degree of freedom the f value was obtained by the analysis of variance table which tested whether the linear relationship of the regression equation is significant through the significance level in general significance levels above 0 05 are significant the parameter sensitivity was determined by their p value the p value is the probability of extreme results when the null hypothesis is true it can indicate whether the data is compatible with a particular statistical model who represents the degree of reliability of the result the parameter was judged as sensitive when the p value was less than 0 05 and extremely sensitive when the p value was less than 0 001 the analysis results are shown in table 5 it can be seen from the p value of the multiple quadratic regression model that the response surface was extremely sensitive and the model fitted well the extremely sensitive parameters identified were infilt and uzsn the sensitive parameters were agwrc and intfw and other parameters were insensitive 2d and 3d contour maps were used to optimize the parameter interval in the multiple quadratic regression model and the optima of the 11 parameters were obtained the results were compared with those obtained by the svm surrogate model as shown in table 6 the calculated r2 of the multiple quadratic regression model was 83 59 while the svm surrogate model was 91 845 the average interval reduction rate of the response surface method was 50 45 and the svm surrogate model was 50 55 the optimal parameter groups obtained by the response surface method and the svm surrogate model were input into hspf respectively with 2011 as the calibration period and 2012 2013 as the verification period the three year average nse value of the response surface method was 0 65 and the average nse value of the svm surrogate model from 2011 to 2013 was 0 78 comparison between response surface method and svm surrogate model for optimal parameter groups simulation results is shown in fig 8 from the three aspects of r2 interval reduction rate and nse value it can be inferred that the regression effect of the svm surrogate model is superior to the multiple quadratic regression response surface model it s worth noting that 1 the optima of infilt and irc generated by svm surrogate model do not fall within their optimal range the possible reasons are as follows 1 the optimization interval of the svm surrogate model is the probability interval where the parameters are most densely distributed judged by the histogram when the nse is optimal however the equifinality phenomenon often appears in models indicates that different parameter values may lead to the same simulation results and the optimum may not necessarily appear in the densest interval of the probability distribution 2 the current parameter analysis is the analysis of the effect of a single parameter on the simulation results but there is often an interaction between various parameters of the model the optimum of a single parameter may not be the optima of a combination of numerous parameters this also provides the next step for the optimization of hspf model parameters study the influence of multiple parameter interactions on the model 2 after adding two parameters under the same watershed and input conditions the optimization interval and value of each parameter obtained by multiple quadratic regression response surface model are different from the interval and value obtained in the study did by liu et al 2019 there are also differences in the sensitive and insensitive parameters identified the optimal parameter interval and parameter sensitivity obtained in the study did by fonseca et al 2014 pang et al 2018 and ahmadisharaf and benham 2020 are also different from those obtained by the svm surrogate model it was proved that sensitivity and optimization results cannot be transferred between different watersheds so each study watershed needs to be analyzed separately van griensven et al 2006 the above phenomenon further confirms the idea of glue that is the hspf model does not have a so called optimal combination of parameter values each parameter has interval uncertainty and parameter uncertainty also affected by the interaction between parameters 4 6 results of water quality simulation based on sensitivity analysis calibration and uncertainty analysis results of the eleven hydrological parameters mentioned above bod nh3 n tn and tp were simulated and verified by the actual data as shown in fig 9 it was found that the analysis of the hydrological parameters based on the svm surrogate model is effective to water quality simulation a comprehensive platform is necessary to complete the above modeling task svm based surrogate model as the parallel model or a module of hspf model focus on the critical aspect of watershed modeling so it provides modelers a solution to improve the efficiency of modeling and to deal with the increasing model complexity for the subsequent application of the hspf model in the qinglong river watershed the following aspects can be deepened watershed ecology and water environment are affected by human and natural factors in terms of human factors china s urbanization process and policy regulation will inevitably cause changes in the water ecological environment of the watershed and land use nobre et al 2020 in terms of natural factors global warming and series of climate change are likely to impact terrestrial and aquatic ecosystems via numerous physical and biological mechanisms liu et al 2020 these changes are closely related to the hydrological and water quality simulation so the work to be done in the future is to predict the response of basin hydrological and water quality to related changes by combining urbanization land use and climate change models provide a more robust technical framework for basin water management 5 conclusions in this work combined with the glue algorithm the svm surrogate model of hspf was used for hspf parameter sensitivity analysis optimization and uncertainty evaluation lhs was used for generating hspf parameter sets and then the corresponding nse were obtained after hspf simulations svm surrogate model was developed based on the pairs of parameter sets and nses and was verified by results of the multiple quadratic regression model it was showed that svm surrogate model is reliable and efficient which is suitable for hspf surrogate modeling we identified that among 11 hydrological parameters in this case the extremely sensitive parameters were agwrc sensitive parameters were agwetp basetp deepfr and insensitive parameters were uzsn lzsn lzetp intfw cepsc irc infilt glue method was used for optimization and uncertainty analysis and then the posterior distributions of eleven parameters were carried out the optimization intervals of hspf model parameters obtained by svm surrogate model were as follows lzsn 2 0 5 82 infilt 0 21 0 47 agwrc 0 85 0 87 deepfr 0 001 0 17 basetp 0 001 0 09 agwetp 0 0011 0 13 cepsc 0 01 0 29 uzsn 0 05 1 20 irc 0 3 0 62 lzetp 0 34 0 85 intfw 1 0 5 77 the recommended value of each parameter was 2 23 0 13 0 87 0 13 0 05 0 13 0 18 0 76 0 75 0 74 4 58 respectively declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was funded by open project of state key laboratory of urban water resource and environment harbin china grant no es201104 the dataset used in this study are available online at https figshare com articles dataset supplementary materials xlsx 13348001 in addition we are very grateful to the editors and anonymous reviewers for their insightful suggestions and comments on this paper appendix a supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105126 
25799,to improve hspf hydrological and water quality simulation a new svm surrogate modeling method was investigated and a comprehensive framework for hydrological parameter sensitivity optimization and uncertainty analysis was established for qinglong river watershed hebei province china svm surrogate model was set up based on pairs of parameter sets and nash sutcliffe efficiency coefficients it was concluded that 1 svm surrogate model performs well in both reliability and efficiency 2 sensitivity of eleven parameters was evaluated agwrc is extremely sensitive parameters agwetp deepfr basetp are sensitive parameters and uzsn lzsn lzetp intfw cepsc irc infilt are not influential parameters 3 recommended parameter intervals were lzsn 2 0 5 82 infilt 0 21 0 47 agwrc 0 85 0 87 deepfr 0 001 0 17 basetp 0 001 0 09 agwetp 0 0011 0 13 cepsc 0 01 0 29 uzsn 0 05 1 20 irc 0 3 0 62 lzetp 0 34 0 85 intfw 1 0 5 77 and the optima were obtained respectively 4 posterior distributions of eleven parameters were obtained graphical abstract image 1 keywords hydrological simulation program fortran hspf surrogate model support vector machine svm parameter sensitivity parameter optimization parameter uncertainty 1 introduction hydrological model plays an important role in exploring the process of the natural water cycle goncu and albek 2010 among numerous computer models applying to predict the behavior of environmental systems johnson et al 2003 nasr et al 2007 hydrological simulation program fortran hspf is commonly used for the continuous dynamic steady state hydrological and water quality modeling due to the powerful water quality modeling ability hspf performs well in the simulation of point source and non point source pollutants total maximum daily load tmdl development in water bodies ahmadisharaf and benham 2020 assessment of land use and climate change impacts on watersheds kim and chung 2014 liu and tong 2011 thus hspf is an essential tool of water resource distribution and watershed management adapting to different time and space scales of mixed agricultural and urban watersheds xu et al 2007 the current challenge for hspf modeling mainly arose from parameter complexity or uncertainty fonseca et al 2014 which leads to studies on parameter sensitivity analysis parameter optimization and parameter uncertainty evaluation beven and freer 2001 gallagher and doherty 2007 among all the hspf parameters the hydrological parameters are the most crucial because sediment and water quality simulation are based on the hydrological simulation in addition water quality indicators such as bod nh3 n tn and tp are monitored on a monthly basis so their actual data are rougher than those of the hydrological parameters hence the emphasis on hspf parameters studies was placed on the hydrological parameters van griensven et al 2006 zheng and keller 2007 parameter sensitivity analysis aims to quantitatively identify the critical parameters that affect the simulation accuracy provide guidance for calibration optimization monitoring and testing of hydrological and water quality parameters fonseca et al 2014 tang et al 2007 it consists of single factor sensitivity analysis and global sensitivity analysis the single factor sensitivity analysis only examines the effect of changes in a single parameter on the model results matthews et al 2007 and it cannot adequately describe the spatial distribution of model parameters and the interaction between parameters so global sensitivity analysis is proposed ciriello et al 2019 conventional global analysis methods include morris screening method campolongo et al 2007 and sobol s method tang et al 2007 since they are the multiple linear regression models based on mathematical statistic there are more deviations when the relationship between model parameters is nonlinear gaussian radial basis function rbf meta model can establish a multi parameter nonlinear regression model according to several samples shang et al 2020 however it is challenging to get the analytical expression of this model and the weight of each parameter parameter optimization aims to tune the optimal values or intervals of the parameters because of the equifinality phenomenon the goal of parameter optimization shifts from the optima to the optimal interval beven and freer 2001 whelan et al 2019 the parameter optimization method can be divided into two categories single objective and multi objective optimization methods sce ua method zhang and shao 2018 rosenbrock method kang et al 2011 artificial bee colony algorithm kang et al 2011 response surface method liu et al 2018 these are widely used single objective methods there are also some multi objective optimization methods the most representative one is neural network however it is hard to avoid the local minimum issues luo et al 2021 and curse of dimensionality jiang and chen 2020 parameter uncertainty evaluation aims to calculate the uncertainties or risks of parameters for risk based decision making ahmadisharaf and benham 2020 it can be divided into two categories generalized likelihood uncertainty estimation glue method beven and binley 1992 2014 and bayesian method kennedy and o hagan 2001 because of its simplicity glue method is more widely used in both sensitivity analysis and uncertainty analysis of model parameters xie et al 2019 however it is low efficient to conduct a lot of simulations for glue analysis based on the hspf platform because each simulation is time consuming montanari 2005 while parameter sensitivity analysis optimization and uncertainty evaluation are the systematic engineering for hspf modeling and simulation a comprehensive framework is required for the hydrological and environmental modelers to complete the modeling task kim and chung 2014 zhang and shao 2018 however given the complexity of the hspf model it is difficult to directly integrate the above three parts into a comprehensive framework in practice tzoraki and nikolaidis 2007 moreover it is a tough task to obtain the optimal values or intervals of parameters depending on the numerical solution of hspf model itself diaz ramirez et al 2012 therefore as an alternative a new surrogate model for hspf prototype was developed here which can focus on some parts of the original hspf model establish a simplified model between hydrological parameters of hspf and its simulation results make the research more efficient support vector machine svm is a machine learning technique which is good at solving non linear problems and is characterized by its free form strong robustness and high precision zhou et al 2021 therefore svm was used to establish the surrogate model and analyze the parameter sensitivity in this case we combined the svm surrogate model with a large sample monte carlo simulation the weight of each parameter was obtained and the sensitivity of the parameter was estimated in an efficient way and then a comprehensive framework for hspf parameter sensitivity analysis optimization and uncertainty evaluation was applied to the case study of qinglong river watershed multiple quadratic regression response surface was adopted to compare the results and analyze the reliability of svm surrogate model the rest of this paper is organized as follows section 1 introduces the study area and the model data used section 2 elaborates the methodology of the research section 2 1 describes the latin hypercube sampling lhs which is used for generating the parameter sets inputted to the hspf and the samples for the establishment of the svm surrogate model in section 2 2 and section 2 3 the svm surrogate model is used as the interpolation function for monte carlo simulation and then the distribution of each parameter is estimated so as the sensitivity optimal interval and satisfactory value of each parameter section 3 introduces the case study in qinglong river watershed results of the svm surrogate model are reported and discussed in section 3 1 and the analysis of parameter sensitivity is conducted in section 3 2 while section 3 3 and section 3 4 optimized the selected parameters and analyzed their uncertainty combining svm surrogate model with glue algorithm these results were compared with those produced by a multiple quadratic regression model in section 3 5 in order to verify the effectiveness of the satisfactory parameters simulation results of bod nh3 n tn and tp are shown in section 3 6 finally the main results are summarized in section 4 2 study area and data 2 1 study area qinglong river watershed is located in hebei and liaoning provinces between 118 69 and 119 63 east longitude and 40 103 41 165 north latitude northern china it covers an area of 6340 km2 with an altitude of 350 650 m whose vegetation coverage exceeds 80 as shown in fig 1 this watershed has an average annual rainfall of 500 700 mm in the east asian monsoon climate zone with large inter annual changes and uneven distribution during the year located at the lower reach of the watershed taolinkou reservoir is the most critical water sources of qinhuangdao city hebei province agricultural non point source pollution rural wastewater and garbage pollution and industrial wastewater are the primary pollution sources of qinglong river and taolinkou reservoir liu et al 2018 the hydrological and water quality simulation of qinglong river and taolinkou reservoir is of great significance for calculation of the environmental capacity and assessment of the potential risks arising from climate change and human activities 2 2 model and data hspf model is a semi distributed hydrological and water quality model developed by the us environmental protection agency usepa embedded in the basins system platform it integrates multiple hydrological and water quality modules and analysis tools nasr et al 2007 different types of data are required for hspf modeling as shown in table 1 particularly the observed data onto streamflow and pollutant concentration are used for model calibration and verification chung and lee 2009 2 3 hydrological parameters for study considering results of literature review albek et al 2004 chung and lee 2009 xu et al 2007 and hspf pest automatic calibration kim et al 2007 eleven hydrological parameters were selected for further study usepa 2000 as shown in table 2 3 methodology a comprehensive method was proposed to investigate the hspf model efficiently and precisely latin hypercube sampling and support vector machine were used to establish the surrogate model of hspf model and multiple quadratic regression model was used for comparison and demonstration combined with monte carlo simulation and glue algorithm parameter sensitivity analysis parameter optimization and uncertainty analysis were carried out based on svm surrogate platform the roadmap of the study is shown as fig 2 3 1 svm surrogate modeling method hspf model has been successfully applied in many watersheds ahmadisharaf and benham 2020 liu and tong 2011 but there are still some challenges in the parameter investigation one of them is the time consuming model simulation which makes it difficult to obtain a large number of samples directly ciriello et al 2019 moreover hspf model has many hydrological and water quality parameters and the parameters have interactive relations lee et al 2020 liu et al 2019 pang et al 2018 according to the fact that it is hard to intuitively summarize the functional expression between parameters and the simulation results of the model it is challenging to select calibrate and optimize the parameters matthews et al 2007 on the other hand svm shows many unique advantages in solving small sample regression nonlinear and high dimensional pattern recognition problems wu et al 2008 firstly the svm model is established by support vectors instead of all samples result in good performance at few shot learning which means it has good classification regression and generalization abilities for fewer samples wang et al 2020 secondly numerous kernel functions in svm can be used flexibly to solve various regression problems especially nonlinear regression the appropriate svm kernel function can map the complex regression functions in low dimensional space to high dimensional space so it is very effective for solving the classification and regression problems of high dimensional features talukdar et al 2020 in addition the structural characteristics of svm ensure that the svm model has a unique minimum value in the process of optimization solution chang and lin 2011 wu et al 2008 to cope with these hspf modeling challenges we proposed the svm surrogate model the main idea of surrogate modeling is to describe the physical process of the original model approximately through reasonable experimental design and fitting functions razavi et al 2012 considering the small sample regression and generalization capability of svm the svm model is suitable for hspf surrogate modeling the process of surrogate modeling based on svm is shown as follows 1 hspf parameter set sampling based on latin hypercube sampling lhs as a multidimensional stratified sampling method lhs was initially developed to reduce the sample size and the computational cost required in the monte carlo simulation by dividing each dimension variable into non overlapping n intervals with the same probability and selecting a point in each interval of each dimension randomly lhs achieves the full interval coverage with a smaller sample standard deviation than monte carlo simulation abyani and bahaari 2020 lhs can achieve higher sampling accuracy using smaller sampling scale so it is widely used to select samples for surrogate modeling steps of lhs algorithm are as follows 1 divide interval of each parameter into n equal parts randomly generate a value within each cell i n i 1 n according to the prior distribution of each parameter 2 generate random values only once for each parameter and for each interval miao et al 2019 the parameter sets generated by lhs were divided into two groups one was used as a training set to develop the svm surrogate model and the other was used as a test set to verify the reliability of the surrogate model 2 svm surrogate modeling method for hspf svm model is essentially a response surface model it is used for dealing with the conversion relationship between input and output of complex systems for svm modeling the kernel function is used to develop the relationships between parameter sets and the likelihood function values hence selection of kernel function is crucial the commonly used svm kernel functions include linear kernel function polynomial kernel function gaussian radial basis kernel function gaussian rbf and sigmoid kernel function chang and lin 2011 for high dimensional nonlinear regression problem gaussian rbf is commonly used gaussian rbf can approximate arbitrary nonlinear function has good generalization ability and fast learning convergence speed and has been successfully applied in nonlinear function approximation time series analysis data classification system modeling and so on jain 2020 the svm response surface can be established on the basis of the training set and selection of kernel function therefore it served as the surrogate model of the original hspf model because of its simplicity and reliability the parameter sensitivity optimization and uncertainty evaluation can be realized in higher cost effective manner 3 2 parameter sensitivity analysis based on svm interpolation and regression on the basis of svm surrogate model it is convenient to conduct monte carlo simulations then a number of parameter sets and nses can be obtained for the analysis of parameter sensitivity gaussian rbf does not have an intuitive function expression to display the weight of each parameter in the multiple regression model thereby the svm surrogate model is used as the interpolation function for monte carlo simulation this method dramatically reduces the calculation consumption caused by hspf simulation and parameter sensitivity analysis combines the advantages of monte carlo simulation and regression based simulation sensitivity analysis steps are as follows 1 input a large number of random parameter groups into the svm surrogate model to obtain their simulated likelihood values 2 draw a scatterplot to observe the response of each parameter to the likelihood values in the distribution interval 3 quantify the sensitivity of each parameter to the likelihood values via the gradient obtained by the linear fitting of the parameter and the likelihood value 3 3 parameter optimization and uncertainty evaluation based on glue algorithm generalized likelihood uncertainty estimation provides a reliable algorithm for quantifying the uncertainty in complex models the basic idea of glue is equifinality that is there does not exist an optimal parameter set but multiple parameter sets can achieve the same satisfactory result ahmadisharaf and benham 2020 by drawing the posterior distribution histogram of each parameter and their simulated likelihood values satisfying the threshold condition value the optimal interval of the parameters can be obtained thus glue based on svm surrogate model was used to calibrate parameters and analyze parameter uncertainty steps of the glue method jia and culver 2008 based on svm surrogate model are as follows 1 selection of appropriate likelihood function and likelihood threshold normally nash sutcliffe efficiency nse water balance coefficient relative error re and so on can be used as the likelihood functions nse can reflect the degree of agreement between the simulated runoff and observed runoff is generally used to verify the quality of hydrological model simulation results its range is 1 the overall results can be judged as reliable when nse is greater than 0 nse is close to 1 indicating good modeling quality and high reliability moriasi et al 2015 the selection of threshold is subjective based on the requirement of model accuracy the likelihood threshold can be flexibly selected according to relevant experience 2 determination of model parameter range and selection of parameter prior distribution the physical range of the model parameters has been given in the hspf model as shown in table 2 due to the lack of prior information the prior distribution cannot be accurately described so the uniform distribution is usually used as the prior distribution 3 according to the prior distribution of parameters the svm surrogate model is used to randomly sample a large number of parameter groups within the parameter range and calculate their likelihood values e g nse monte carlo sampling is used as the sampling method during uniform distribution 4 if the likelihood value is above the likelihood threshold its corresponding parameter set would be regarded as a feasible parameter set plot the frequency histogram of all feasible parameter groups then the posterior distribution of each parameter can be obtained analyze the posterior distribution and the dense distribution interval is taken as the optimization interval 5 sort the likelihood values from large to small the parameter sets with the largest likelihood value are input into the hspf model for testing and the best set of simulation results is selected as the recommended value of parameters 6 calculate the uncertainty range of model prediction under different confidence levels e g 90 or 95 level when a 90 confidence interval is obtained the lower limit of the confidence interval is 5 quantile value after likelihood ranking and the upper limit is 95 quantile value the uncertainty of the model simulation is obtained from the proportion of the observed values included in the confidence interval 4 results and discussion 4 1 results of svm surrogate modeling first of all the hspf model of qinglong river watershed was established and run applying the pest auto calibrated parameters optimized by liu et al 2018 and then the simulated data and the observed runoff data of taolinkou reservoir section were used for calculation of nse secondly lhs algorithm was carried out for the eleven selected hydrological parameters on the matlab platform e g lhsdesign function assuming all parameters are uniformly distributed 120 sampling results of eleven parameter sets were obtained and used for the subsequent 120 hspf simulations respectively thus 120 nse values were also obtained for forty simulations these data sets were then used to develop the surrogate model of hspf in this paper libsvm toolbox developed by professor lin zhiren was used for svm regression chang and lin 2011 and the gaussian rbf was selected to establish the svm surrogate model whose hyperparameters was trained by cross validation program finally the svm surrogate model was built with these 120 parameter sets and their nse values the first 100 groups of simulated data were used as the training set and the rest twenty groups were used as the test set after 806 iterations mean square error mse of the model training set was 0 0139 the correlation coefficient r2 was 88 51 and mse of the test set was 0 0073 r2 was 95 18 it reflected that svm surrogate model largely increases computation efficiency while keeping good model performance thus can be used as an alternative to hspf the hyperparameters for the training of svm was selected in fig 3 c was 6 9644 gamma was 0 0474 the regressed nse in train set and test set generated by svm surrogate model was compared with nse simulated by the hspf model as shown in fig 4 4 2 results of sensitivity analysis based on svm surrogate model 30 000 parameter groups were input into the svm surrogate model and the corresponding nses were obtained as shown in fig 4 these pairs of parameter set and nse were used for the following sensitivity analysis to demonstrate the validity of the svm surrogate model a total of 120 parameter sets and nse derived from hspf model were compared with those generated from the svm surrogate model as shown in fig 5 and table 3 it can be found that the trend of data generated by svm surrogate model was similar to that of the hspf model according to fig 5 the gradient of data generated by the hspf model and the svm surrogate model of each parameter were obtained respectively as shown in table 3 it can be seen that the data simulated by svm surrogate model forms visible boundaries the proportion of the hspf simulation value beyond the boundaries was considered as an error all values are 3 33 and below which can be considered credible for most of the model parameters the gradient of the samples generated by svm surrogate model is consistent with those of the hspf model it reflected that the results of sensitivity analysis based on svm surrogate model were reliable comparing the nse simulated by hspf and them generated by svm and their respective linear trend lines was drawn it was found that the gradients of these two data sets were opposite when it comes to infilt agwetp and irc the possible reasons were as follows firstly the insensitivity of the parameter itself leads to the svm simulation error secondly the accuracies of the linear trend line in excel were doubtful r2 of infilt agwetp and irc are 0 029 0 234 and 0 012 respectively indicating poor fitting accuracy however considering the distribution trend and error of most parameters the svm surrogate model can be used as a substitute for the hspf model for sensitivity analysis in view of the complexity of hydrological model parameters the uniform distribution is generally chosen as the prior distribution whose gradient is 0 the gradient of some parameters deviated significantly from the prior uniform distribution indicating high sensitivity the absolute value of gradient was taken as the judgment standard of parameter sensitivity and then the 11 parameters were divided into three categories the extremely sensitive parameters gradient 1 were agwrc the sensitive parameters 0 1 gradient 1 were deepfr agwetp basetp and the insensitive parameters 0 gradient 0 1 were lzsn infilt cepsc uzsn irc lzetp intfw 4 3 results of parameter optimization based on svm surrogate model 30 000 parameter groups and their regression nse generated by svm surrogate model in section 3 2 was used in glue algorithm glue method generated posterior probability distribution of parameters which was used for the optimization of parameter intervals here nse was used as the likelihood function of the glue method through the experience of hydrology calibration and related research moriasi et al 2015 the appropriate threshold of likelihood value was selected as 0 6 a total of 3012 sample points accounting for 10 04 of the total sample the posterior distribution of each parameter was analyzed by drawing histogram in fig 6 a k and the cumulative frequency of each parameter was combined in fig 6 l fig 6 shows when the regressed nse value generated by the svm surrogate model is greater than 0 6 the distribution frequency of each parameter in its value range the dense interval with high parameter distribution frequency was regarded as the optimal ranges for each parameter which is higher than the average distribution frequency and the optimum of each parameter was also obtained by sorting the likelihood values from large to small details are shown in table 4 the model performance can be evaluated as satisfactory when its nse is greater than 0 45 moriasi et al 2015 the optimized parameter set was input into the hspf for testing and the nse value from 2011 to 2013 was 0 78 which proved that the optimized parameter set was satisfactory for hydrological simulation 4 4 results of parameter uncertainty analysis based on the svm surrogate model in terms of uncertainty analysis the threshold of likelihood value was adjusted to 0 at 90 confidence level 5 quantile and 95 quantile of simulated runoff in qinglong river watershed was selected compared with the measured runoff the uncertainty of svm surrogate model was analyzed as shown in fig 7 it can be seen from fig 7 that the uncertainty of the model is significantly positively related to the amount of rainfall the larger the runoff the stronger the uncertainty on the daily runoff scale 10 6 of the measured runoff is included in the uncertainty range and on the monthly runoff scale 19 4 of the measured value is within the uncertainty range we analyzed that a large part of the reason is that the hspf did not simulate the base flow accurately enough while it was relatively sensitive to heavy rainfall and high flow both the trend of the runoff hydrograph and the nse value showed that the results obtained by the svm surrogate model can reflect the runoff variation law of the qinglong river watershed satisfactorily 4 5 comparison with multiple quadratic regression response surface model as the most commonly used regression response method multiple quadratic regression model was used to demonstrate the reliability of svm surrogate model design expert10 software was applied to perform sensitivity analysis and parameter optimization for the above eleven hydrological parameters 188 parameter sets were used for calculation f value and p value are two commonly used statistical parameters in an analysis of variance the f value is the ratio of the variances of two samples at a given degree of freedom the f value was obtained by the analysis of variance table which tested whether the linear relationship of the regression equation is significant through the significance level in general significance levels above 0 05 are significant the parameter sensitivity was determined by their p value the p value is the probability of extreme results when the null hypothesis is true it can indicate whether the data is compatible with a particular statistical model who represents the degree of reliability of the result the parameter was judged as sensitive when the p value was less than 0 05 and extremely sensitive when the p value was less than 0 001 the analysis results are shown in table 5 it can be seen from the p value of the multiple quadratic regression model that the response surface was extremely sensitive and the model fitted well the extremely sensitive parameters identified were infilt and uzsn the sensitive parameters were agwrc and intfw and other parameters were insensitive 2d and 3d contour maps were used to optimize the parameter interval in the multiple quadratic regression model and the optima of the 11 parameters were obtained the results were compared with those obtained by the svm surrogate model as shown in table 6 the calculated r2 of the multiple quadratic regression model was 83 59 while the svm surrogate model was 91 845 the average interval reduction rate of the response surface method was 50 45 and the svm surrogate model was 50 55 the optimal parameter groups obtained by the response surface method and the svm surrogate model were input into hspf respectively with 2011 as the calibration period and 2012 2013 as the verification period the three year average nse value of the response surface method was 0 65 and the average nse value of the svm surrogate model from 2011 to 2013 was 0 78 comparison between response surface method and svm surrogate model for optimal parameter groups simulation results is shown in fig 8 from the three aspects of r2 interval reduction rate and nse value it can be inferred that the regression effect of the svm surrogate model is superior to the multiple quadratic regression response surface model it s worth noting that 1 the optima of infilt and irc generated by svm surrogate model do not fall within their optimal range the possible reasons are as follows 1 the optimization interval of the svm surrogate model is the probability interval where the parameters are most densely distributed judged by the histogram when the nse is optimal however the equifinality phenomenon often appears in models indicates that different parameter values may lead to the same simulation results and the optimum may not necessarily appear in the densest interval of the probability distribution 2 the current parameter analysis is the analysis of the effect of a single parameter on the simulation results but there is often an interaction between various parameters of the model the optimum of a single parameter may not be the optima of a combination of numerous parameters this also provides the next step for the optimization of hspf model parameters study the influence of multiple parameter interactions on the model 2 after adding two parameters under the same watershed and input conditions the optimization interval and value of each parameter obtained by multiple quadratic regression response surface model are different from the interval and value obtained in the study did by liu et al 2019 there are also differences in the sensitive and insensitive parameters identified the optimal parameter interval and parameter sensitivity obtained in the study did by fonseca et al 2014 pang et al 2018 and ahmadisharaf and benham 2020 are also different from those obtained by the svm surrogate model it was proved that sensitivity and optimization results cannot be transferred between different watersheds so each study watershed needs to be analyzed separately van griensven et al 2006 the above phenomenon further confirms the idea of glue that is the hspf model does not have a so called optimal combination of parameter values each parameter has interval uncertainty and parameter uncertainty also affected by the interaction between parameters 4 6 results of water quality simulation based on sensitivity analysis calibration and uncertainty analysis results of the eleven hydrological parameters mentioned above bod nh3 n tn and tp were simulated and verified by the actual data as shown in fig 9 it was found that the analysis of the hydrological parameters based on the svm surrogate model is effective to water quality simulation a comprehensive platform is necessary to complete the above modeling task svm based surrogate model as the parallel model or a module of hspf model focus on the critical aspect of watershed modeling so it provides modelers a solution to improve the efficiency of modeling and to deal with the increasing model complexity for the subsequent application of the hspf model in the qinglong river watershed the following aspects can be deepened watershed ecology and water environment are affected by human and natural factors in terms of human factors china s urbanization process and policy regulation will inevitably cause changes in the water ecological environment of the watershed and land use nobre et al 2020 in terms of natural factors global warming and series of climate change are likely to impact terrestrial and aquatic ecosystems via numerous physical and biological mechanisms liu et al 2020 these changes are closely related to the hydrological and water quality simulation so the work to be done in the future is to predict the response of basin hydrological and water quality to related changes by combining urbanization land use and climate change models provide a more robust technical framework for basin water management 5 conclusions in this work combined with the glue algorithm the svm surrogate model of hspf was used for hspf parameter sensitivity analysis optimization and uncertainty evaluation lhs was used for generating hspf parameter sets and then the corresponding nse were obtained after hspf simulations svm surrogate model was developed based on the pairs of parameter sets and nses and was verified by results of the multiple quadratic regression model it was showed that svm surrogate model is reliable and efficient which is suitable for hspf surrogate modeling we identified that among 11 hydrological parameters in this case the extremely sensitive parameters were agwrc sensitive parameters were agwetp basetp deepfr and insensitive parameters were uzsn lzsn lzetp intfw cepsc irc infilt glue method was used for optimization and uncertainty analysis and then the posterior distributions of eleven parameters were carried out the optimization intervals of hspf model parameters obtained by svm surrogate model were as follows lzsn 2 0 5 82 infilt 0 21 0 47 agwrc 0 85 0 87 deepfr 0 001 0 17 basetp 0 001 0 09 agwetp 0 0011 0 13 cepsc 0 01 0 29 uzsn 0 05 1 20 irc 0 3 0 62 lzetp 0 34 0 85 intfw 1 0 5 77 the recommended value of each parameter was 2 23 0 13 0 87 0 13 0 05 0 13 0 18 0 76 0 75 0 74 4 58 respectively declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was funded by open project of state key laboratory of urban water resource and environment harbin china grant no es201104 the dataset used in this study are available online at https figshare com articles dataset supplementary materials xlsx 13348001 in addition we are very grateful to the editors and anonymous reviewers for their insightful suggestions and comments on this paper appendix a supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105126 
